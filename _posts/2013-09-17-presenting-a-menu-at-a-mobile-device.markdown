---

title: Presenting a menu at a mobile device
abstract: Systems and methods for presenting a menu at a computing device are provided. In some aspects, an indication of a user touching a hardware menu button of the computing device is received, where the hardware menu button is separate and distinct from a touchscreen of the computing device. A menu on the touchscreen is presented in response to the user touching the hardware menu button, where the menu includes multiple menu elements, and where each menu element has a corresponding command. An indication of the user touching the touchscreen at a starting point within a dragging-start region of the touchscreen is received. An indication of the user dragging a touching device along the touchscreen from the starting point to a termination point is received, where the termination point is associated with a termination menu element. A signal is provided for execution of the corresponding command of the termination menu element.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09317183&OS=09317183&RS=09317183
owner: GOOGLE INC.
number: 09317183
owner_city: Mountain View
owner_country: US
publication_date: 20130917
---
This application claims priority under 35 U.S.C. 119 e and the benefit of U.S. Provisional Application No. 61 868 035 filed Aug. 20 2013 and entitled PRESENTING A MENU AT A MOBILE DEVICE the entire disclosure of which is incorporated herein by reference.

The subject technology generally relates to user interfaces for computers and in particular relates to presenting a menu at a mobile device.

Some mobile devices for example some models of mobile phones or tablet computers feature a touchscreen and a menu button separate and distinct from the touch screen which when selected causes the touchscreen to display a menu associated with a home screen of the mobile device or an application executing on the mobile device.

In some aspects the disclosed subject matter relates to a method for presenting a menu at a computing device. The method includes receiving at a first time an indication of a user touching a hardware menu button of the computing device where the hardware menu button is separate and distinct from a touchscreen of the computing device. The method includes presenting in response to the user touching the hardware menu button a menu on the touchscreen where the menu includes multiple menu elements and wherein each menu element has a corresponding command. The method includes receiving at a second time an indication of the user touching the touchscreen at a starting point within a dragging start region of the touchscreen. The method includes receiving an indication of the user dragging a touching device along the touchscreen from the starting point to a termination point where the termination point is associated with a termination menu element. The method includes signaling for execution of the corresponding command of the termination menu element.

In some aspects the disclosed subject matter relates to a non transitory computer readable medium encoded with executable instructions. The instructions include code for receiving an indication of a user touching a hardware menu button of the computer where the hardware menu button is separate and distinct from a touchscreen of the computer. The instructions include code for presenting in response to the user touching the hardware menu button a menu on the touchscreen where the menu includes multiple menu elements and where each menu element has a corresponding command. The instructions include code for receiving an indication of the user touching the touchscreen at a starting point within a dragging start region of the touchscreen. The instructions include code for receiving an indication of the user dragging a touching device along the touchscreen along a dragging path from the starting point to a termination point where the termination point is associated with a termination menu element. The instructions include code for determining whether the termination point is closer than a threshold length to an edge of the touchscreen. The instructions include code for signaling for execution of the corresponding command of the termination menu element in a case where the termination point is not closer than the threshold length to the edge of the touchscreen. The instructions include code for foregoing signaling for execution of the corresponding command of the termination menu element in a case where the termination point is closer than the threshold length to the edge of the touchscreen

In some aspects the disclosed subject matter relates to a computing device. The computing device includes a touchscreen a hardware menu button separate and distinct from the touchscreen one or more processors and a memory. The memory includes instructions. The instructions include code for receiving a touch up event or a touch down event at the hardware menu button. The instructions include code for presenting in response to the touch up event or the touch down event at the hardware menu button a menu on the touchscreen where the menu includes multiple menu elements and where each menu element has a corresponding command. The instructions include code for receiving multiple touch events at the touch screen the multiple touch events corresponding to a dragging path from a starting point to a termination point where the starting point is within a dragging start region of the touchscreen and where the termination point is associated with a termination menu element. The instructions include code for signaling for execution of the corresponding command of the termination menu element.

It is understood that other configurations of the subject technology will become readily apparent from the following detailed description where various configurations of the subject technology are shown and described by way of illustration. As will be realized the subject technology is capable of other and different configurations and its several details are capable of modification in various other respects all without departing from the scope of the subject technology. Accordingly the drawings and detailed description are to be regarded as illustrative in nature and not as restrictive.

The detailed description set forth below is intended as a description of various configurations of the subject technology and is not intended to represent the only configurations in which the subject technology may be practiced. The appended drawings are incorporated herein and constitute a part of the detailed description. The detailed description includes specific details for the purpose of providing a thorough understanding of the subject technology. However it will be clear and apparent that the subject technology is not limited to the specific details set forth herein and may be practiced without these specific details. In some instances certain structures and components are shown in block diagram form in order to avoid obscuring the concepts of the subject technology.

As set forth above some mobile devices feature a touchscreen and a menu button separate and distinct from the touch screen which when selected causes the touchscreen to display a menu. In order to select an icon on the menu a user must lift his her finger from the menu button and place the finger on a position of the touchscreen corresponding to the icon. This lifting of the finger is time consuming and counterintuitive for some users. As the foregoing illustrates a new approach for presenting a menu at a mobile device and selecting a menu item may be desirable.

The subject technology relates to presenting a menu at a mobile device. According to some implementations the mobile device has a hardware menu button and a touchscreen. The hardware menu button is separate and distinct from the touchscreen and is not included on the touchscreen. In response to a user of the mobile device touching the menu button the mobile device presents a menu on the touchscreen. The menu is associated with a home screen of the mobile device or an application executing on the mobile device. The menu includes multiple icons and each icon has a corresponding command that is executed in response to selecting the icon. The user touches the touchscreen in a dragging start region of the touchscreen which may be for example a region within a threshold distance e.g. 1 centimeter of the bottom of the touchscreen and drags a touching device e.g. a finger or a stylus from a starting point in the dragging start region to a termination point associated with a termination icon. The termination icon is one of the icons on the menu. The user finishes dragging the touching device at the termination point and the mobile device executes the corresponding command of the termination icon.

As used herein the phrase hardware menu button may include a menu button that is separate and distinct from a touchscreen and is not presented on the touchscreen. The phrase software menu button may refer to a menu button that is presented on the touchscreen for example via an application featuring a menu button.

The camera operates by taking photograph s or video s and storing the taken photograph s or video s in a memory of the mobile device or transmitting the taken photograph s via a network for example to a photograph sharing service or a video calling service. The speaker operates by providing audio output for example playing music or providing audio associated with a telephone call. The touchscreen operates by presenting visual data to a user and receiving touch input from the user. The user may provide touch input for the touchscreen using a finger or a stylus. The menu button is a hardware menu button. The menu button when pressed causes the mobile device to display a menu. The displayed menu may be associated with information presented at the touchscreen immediately before requesting the menu. For example if a banking application is displaying data at the touchscreen immediately before the menu button is pressed the menu displayed when the menu button is pressed may be associated with the banking application. The home button when pressed causes the mobile device to display a home screen. The home screen may include on screen buttons to access some applications or webpage s . The back button when pressed causes the touchscreen to display information that was previously displayed at the touchscreen. For example if a user switches from viewing a first document to viewing a second document in a document processor application pressing the back button may cause the document processor application to display the first document again. In some cases the mobile device interface is a flat surface extending to include at least the touch screen and the menu button .

As illustrated in the menu button is adjacent to the bottom side of the touchscreen . However the menu button may be located anywhere in the mobile device interface . For example the menu button may be adjacent to the top side the left side or the right side of the touchscreen .

As shown the touchscreen displays a menu e.g. a menu for a document manager application executing on the mobile device . The user may have caused the menu to be displayed by pressing the menu button for example at touch point . The document manager menu includes icons . including new icon . open icon . save icon . copy icon . and paste icon .. Each of the icons . where k is a number between 1 and 5 has a corresponding command e.g. create new document open document save document copy or paste that is executed when the icon is selected. While the menu is illustrated as including five icons . the subject technology may be implemented with any number of icon s in the menu for example one icon or multiple icons. Also the subject technology may be implemented with any menu not necessarily a menu of a document manager application. While the subject technology is described herein in terms of icons being presented on a menu in some implementations any other menu element s associated with command s may be used in place or instead of the icon s . In other words an icon is an example of a menu element.

After pressing the menu button the user may drag a touching device e.g. a finger or a stylus across the touchscreen to select one of the icons .. For example the user may touch the menu button at touch point via the touching device and then drag the touching device across the touchscreen causing the touchscreen to register touch events at touch points A. . The first in time touch event from among the touch events at touch points A. corresponds to touch point A. which is within a dragging start region of the touchscreen . While seven touch points A. are illustrated the subject technology may be implemented with any number of touch points on the touchscreen .

According to some implementations if after pressing the menu button the user touches the touchscreen within the dragging start region the mobile device determines that the user is initiating a dragging gesture e.g. corresponding to the touch points A. . If after pressing the menu button the user touches the touchscreen outside the dragging start region at a position corresponding to an icon . where k is a number between 1 and 5 the mobile device signals for execution or executes a corresponding command of the icon .on the menu touched by the user. The position of the dragging start region on the touchscreen may be determined using various approaches. For example the dragging start region may occupy a bottom portion of the touchscreen a threshold distance from the bottom side of the touchscreen . The threshold distance may be a constant distance e.g. always one centimeter or a variable distance determined based on an amount of time elapsed between when the user pressed the menu button and when the user first touched the touchscreen after pressing the menu button .

As illustrated in the dragging gesture associated with the touch points A. is initiated within the dragging start region at touch point A. and is terminated at touch point A. on the save icon .. As a result after the dragging gesture associated with the touch points A. is terminated a command corresponding to an icon . e.g. a save document command at the termination touch point A. is executed after the dragging gesture is terminated. While the dragging gesture is illustrated in as including seven touch points the dragging gesture may have any number of touch points.

In some implementations the dragging start region is a region of the touchscreen proximate to the menu button . Thus if after pressing the menu button the user first touches the touchscreen within the dragging start region there is a high probability that the user is initiating a dragging gesture to select an icon . where k is a number between 1 and 5 on the menu . Alternatively if after pressing the menu button the user first touches the touchscreen outside of the dragging start region there is a high probability that the user is selecting an icon corresponding to the touch point rather than initiating a dragging gesture.

As illustrated in the dragging gesture corresponding to the touch points B. involves the user moving the touching device e.g. the finger or the stylus to termination touch point B. which is closer than a threshold length e.g. 0.3 centimeters to an edge of the touchscreen . The edge of the touchscreen may include one two three or four of the top side the bottom side the left side or the right side of the touchscreen . According to some implementations as a result of the touching device being moved to the termination touch point B. closer than the threshold length to the edge of the touchscreen the mobile device may determine that the user intends to cancel the dragging gesture and a command corresponding to an icon at the termination touch point B. may not be executed by the mobile device. Alternatively the command corresponding to the icon at the termination touch point B. may be executed and the termination touch point B. being closer than the threshold length to the edge of the touchscreen may be ignored.

The edge of the touchscreen may include one or more of the following i the bottom side of the touchscreen ii the top side of the touchscreen iii the left side of the touchscreen iv the right side of the touchscreen v the bottom and top sides of the touchscreen vi the bottom and left sides of the touchscreen vii the bottom and right sides of the touchscreen viii the top and left sides of the touchscreen ix the top and right sides of the touchscreen x the left and right sides of the touchscreen xi the bottom top and left sides of the touchscreen xii the bottom top and right sides of the touchscreen xiii the bottom left and right sides of the touchscreen xiv the top left and right sides of the touchscreen or xv the bottom top left and right sides of the touchscreen .

Also as illustrated in the dragging gesture corresponding to the touch points B. involves the user turning the touching device e.g. the finger or the stylus to first move away from the menu button and then move back toward the menu button. The turn may be greater than a threshold angle e.g. greater than 90 degrees . According to some implementations as a result of the turn being greater than the threshold angle the mobile device may determine that the user intends to cancel the dragging gesture and the command corresponding to the icon at the termination touch point B. may not be executed by the mobile device. Alternatively the command corresponding to the icon at the termination touch point B. may be executed. The turn and the turning angle may be ignored or not processed by the mobile device.

While the mobile device is illustrated in and is illustrated as being a mobile phone the subject technology is not limited to mobile phones. The subject technology may be implemented on any mobile device including for example a mobile phone a tablet computer a personal digital assistant PDA an electronic music player a laptop computer a desktop computer etc.

As shown the mobile device includes a processing unit a network interface a touchscreen a menu button a home button a back button and a memory . The processing unit includes one or more processors. The processing unit may include a central processing unit CPU a graphics processing unit GPU or any other processing unit. The processing unit is configured to execute computer instructions that are stored in a computer readable medium for example the memory . The network interface is configured to allow the mobile device to transmit and receive data in a network e.g. the Internet a cellular network a local area network LAN a wide area network WAN a WiFi network etc. The network interface may include one or more network interface controllers NICs . The touchscreen displays information from the mobile device to a user of the mobile device and to receives touch input via a touching device from the user. The touchscreen may receive single touch input or multi touch input. The menu button is a hardware menu button. The menu button when pressed causes the touchscreen to display a menu. The menu may be a default menu associated with a home screen of the mobile device or a menu associated with an application executing on the mobile device and may include icon s that have corresponding command s . The home button when pressed causes the mobile device to display a home screen view at the touchscreen . The back button when pressed causes the mobile device to display at the touchscreen information previously displayed at the touchscreen for example a previously displayed webpage in a web browser application. The touchscreen the menu button the home button and the back button may be separate and distinct from one another or may occupy separate and non overlapping portions of an interface e.g. interface of the mobile device. The touchscreen may correspond to the touchscreen . The menu button home button and back button may correspond to the menu button home button and back button respectively.

The memory stores data and or instructions. The memory may be one or more of a cache unit a storage unit an internal memory unit or an external memory unit. As illustrated the memory includes a touchscreen driver a button driver applications . and a menu presentation module .

The touchscreen driver causes the processing unit to receive touch input e.g. single touch data or multi touch data for instance touches corresponding to the touch points A. or B. via the touchscreen or provide visual output e.g. the menu and the icons . via the touchscreen .

The button driver causes the processing unit to receive input e.g. button press data which may correspond to a touch down event or a touch up event from one or more of the buttons or and to respond to the input from the buttons. In some examples the button driver operates by issuing an interrupt to other processes being executed by the processing unit to allow the processing unit to respond to the input from the button s or .

The applications . may include any applications executing on the mobile device for example a web browser application document manager application an online banking application a social networking application a game playing application etc. One or more of the applications . may have an associated menu which is presented if the menu button is pressed while the application is executing and displaying data on the touchscreen .

The menu presentation module when executed causes the processing unit to present a menu at the touchscreen . According to some implementations when executing the menu presentation module the processing unit receives at a first time via the button driver an indication of a user pressing the menu button . The indication of the user pressing the menu button may be a touch down event or a touch up event of the menu button . The processing unit presents in response to the user pressing the hardware menu button a menu e.g. menu on the touchscreen . The menu includes multiple icons e.g. icons . and each icon has a corresponding command. The processing unit receives at a second time via the touchscreen driver an indication of the user touching the touchscreen at a starting point e.g. touch point A. within a dragging start region e.g. dragging start region of the touchscreen. For example the processing unit may receive a touch event signaled by the touch screen and the touchscreen driver at the starting point. The processing unit receives via the touchscreen driver an indication of the user dragging a touching device along the touchscreen from the starting point to a termination point e.g. touch point A. where the termination point is associated with a termination icon e.g. icon . of the menu. The indication of the user dragging the touching device may include multiple touch events e.g. at touch points A. signaled by the touch screen and the touchscreen driver . The multiple touch events may correspond to a dragging path of the touching device. The processing unit signals for execution of the corresponding command of the termination icon. The signal for the execution of the corresponding command may be provided to an application a server etc.

The position on the touchscreen of the dragging start region may be determined based on an amount of time elapsed between the first time and the second time. For example the dragging start region may include a region of the touchscreen within a threshold distance from a bottom side of the touch screen. Alternatively the dragging start region may include a region of the touchscreen within the threshold distance from a side of the touchscreen e.g. a top side a bottom side a left side or a right side corresponding to the position of the menu button . In another alternative the dragging start region may include a region of the touchscreen within the threshold distance from the position of the menu button . The position of the menu button may be determined by the processing unit via an application programming interface API or may be stored in the memory .

In some implementations the threshold distance is constant e.g. always 0.8 centimeters . Alternatively the threshold distance may be determined based on an amount of time elapsed between the first time and the second time. The threshold distance may decrease as an amount of time elapsed increases due to the fact that the larger the amount of time elapsed the more slowly and the more precisely the user is likely to be moving the touching device. A linear mathematical relationship may exist between the threshold distance and the amount of time elapsed. According to some examples the linear mathematical relationship may exist between the threshold distance and the amount of time elapsed may be expressed according to equation 1 . 0 where 0 1.2 1.6 where 0 0.5 0.4 where t 0.5. 1 

In equation 1 d represents the threshold distance in centimeters and t represents the amount of time elapsed in seconds. According to equation 1 the threshold distance is initially at time t 0 set to 1.2 centimeters and is linearly decreased with respect to the amount of time elapsed until the threshold distance reaches 0.4 centimeters at an amount of time elapsed of 0.5 seconds. After the amount of time elapsed exceeds 0.5 seconds the threshold distance is set to 0.4 centimeters where the threshold distance remains until the menu is closed or a touch event is received on the touchscreen via the touchscreen driver . According to other examples the relationship between the threshold distance and the amount of time elapsed may be different from that expressed in equation 1 .

As shown in the process begins at step where a mobile device e.g. mobile device receives a touch down event at a hardware menu button e.g. menu button of the mobile device. The touch down event may be received when the user pushes down on the hardware menu button with a touching device.

In step the mobile device presents in response to the touch down event a menu on a touchscreen e.g. touchscreen of the mobile device. The menu includes multiple icons e.g. icons . . Each icon has a corresponding command.

In step the mobile device receives a touch up event at the hardware menu button of the mobile device. The touch up event may be received when the user lifts the touching device off the hardware menu button.

In step the mobile device receive a starting touch event at the touchscreen. The starting touch event is associated with a starting point e.g. touch point A. or B. . The starting touch event may be received in response to the user touching the touchscreen at the starting point.

In step the mobile device stops the timer and determines an amount of time elapsed since starting the timer in response to the touch event. In some implementations at technique different from the timer may be used to determine the amount of time elapsed. For example a time on a clock e.g. a network clock or a local clock at the mobile device may be stored in step and in step and the time difference between the two stored times may be determined in step .

In step the mobile device determines a dragging start region of the touchscreen based on the amount of time elapsed. For example the dragging start region may correspond to a region of the touchscreen within a threshold distance from a bottom side of the touchscreen where the threshold distance is determined based on the amount of time elapsed. Techniques for determining the dragging start region are described in greater detail above in conjunction with . After step the process continues to step of .

As shown in in step the mobile device determines whether the starting point is within the dragging start region. If the starting point is within the dragging start region the process continues to step . If the starting point is not within the dragging start region the process continues to step .

In step if the starting point is not within the dragging start region the mobile device signals for execution of the corresponding command of an icon at the starting point. The signal for the execution of the corresponding command may be provided to an application a server etc. After step the process ends.

In step if the starting point is within the dragging start region the mobile device receives multiple touch events at the touchscreen. The multiple touch events correspond to a dragging path from the starting point to a termination point. The multiple touch events may be received in response to the user moving a touching device along the touchscreen from the starting point to the termination point via the dragging path. A distance between touch events in the multiple touch events may correspond to a moving speed of the touching device or a processing speed of the touchscreen or the touchscreen driver.

In step the mobile device determines whether the termination point is closer than a threshold length e.g. 0.2 centimeters 0.3 centimeters 0.35 centimeters etc. to an edge of the touchscreen. If the termination point is closer than the threshold length to the edge of the touchscreen the process continues to step . If the termination point is further or not closer than the threshold length from the edge of the touchscreen the process continues to step .

In step if the termination point is closer than the threshold length to the edge of the touchscreen the mobile device foregoes signaling for execution of the corresponding command of an icon at the termination point. The mobile device may conclude that the user moved the touching device closer than the threshold distance to the edge of the touchscreen because the user is no longer interested in executing the corresponding command or the user changed his her mind about executing the corresponding command. After step the process ends.

In step if the termination point is further or not closer than the threshold length from the edge of the touchscreen the mobile device signals e.g. to a local application or to a server for execution of the corresponding command of the icon at the termination point. After step the process ends.

As shown in the steps of the process may be executed in a certain order and in series. However the steps of the process may be executed in any order not necessarily the order specified above. In some examples two or more of the steps may be executed in parallel. In some examples one or more of the steps may be skipped or not executed. For example some implementations of the process proceed directly from step to step skipping steps and .

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the system memory and the permanent storage device .

From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of the subject technology. The processing unit s can be a single processor or a multi core processor in different implementations.

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some implementations of the subject technology use a mass storage device for example a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other implementations use a removable storage device for example a floppy disk flash drive and its corresponding disk drive as the permanent storage device . Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some implementations the processes of the subject technology are stored in the system memory the permanent storage device or the read only memory . For example the various memory units include instructions for presenting a menu at a mobile device in accordance with some implementations. From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of some implementations.

The bus also connects to the input and output device interfaces and . The input device interface enables the user to communicate information and select commands to the electronic system. Input devices used with input device interface include for example alphanumeric keyboards and pointing devices also called cursor control devices . Output device interfaces enables for example the display of images generated by the electronic system . Output devices used with output device interface include for example printers and display devices for example cathode ray tubes CRT or liquid crystal displays LCD . Some implementations include devices for example a touch screen that functions as both input and output devices.

Finally as shown in bus also couples electronic system to a network not shown through a network interface . In this manner the electronic system can be a part of a network of computers for example a local area network LAN a wide area network WAN or an Intranet or a network of networks for example the Internet. Any or all components of electronic system can be used in conjunction with the subject technology.

The above described features and applications can be implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives RAM chips hard drives EPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage or flash storage for example a solid state drive which can be read into memory for processing by a processor. Also in some implementations multiple software technologies can be implemented as sub parts of a larger program while remaining distinct software technologies. In some implementations multiple software technologies can also be implemented as separate programs. Finally any combination of separate programs that together implement a software technology described here is within the scope of the subject technology. In some implementations the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

A computer program also known as a program software software application script or code can be written in any form of programming language including compiled or interpreted languages declarative or procedural languages and it can be deployed in any form including as a stand alone program or as a module component subroutine object or other unit suitable for use in a computing environment. A computer program may but need not correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.

These functions described above can be implemented in digital electronic circuitry in computer software firmware or hardware. The techniques can be implemented using one or more computer program products. Programmable processors and computers can be included in or packaged as mobile devices. The processes and logic flows can be performed by one or more programmable processors and by one or more programmable logic circuitry. General and special purpose computing devices and storage devices can be interconnected through communication networks.

Some implementations include electronic components for example microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media can store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code for example is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some implementations are performed by one or more integrated circuits for example application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some implementations such integrated circuits execute instructions that are stored on the circuit itself.

As used in this specification and any claims of this application the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification and any claims of this application the terms computer readable medium and computer readable media are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

To provide for interaction with a user implementations of the subject matter described in this specification can be implemented on a computer having a display device e.g. a cathode ray tube CRT or liquid crystal display LCD monitor for displaying information to the user and a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input. In addition a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user for example by sending web pages to a web browser on a user s client device in response to requests received from the web browser.

The subject matter described in this specification can be implemented in a computing system that includes a back end component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a front end component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification or any combination of one or more such back end middleware or front end components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN an inter network e.g. the Internet and peer to peer networks e.g. ad hoc peer to peer networks .

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other. In some aspects of the disclosed subject matter a server transmits data e.g. an HTML page to a client device e.g. for purposes of displaying data to and receiving user input from a user interacting with the client device . Data generated at the client device e.g. a result of the user interaction can be received from the client device at the server.

It is understood that any specific order or hierarchy of steps in the processes disclosed is an illustration of example approaches. Based upon design preferences it is understood that the specific order or hierarchy of steps in the processes may be rearranged or that all illustrated steps be performed. Some of the steps may be performed simultaneously. For example in certain circumstances multitasking and parallel processing may be advantageous. Moreover the separation of various system components illustrated above should not be understood as requiring such separation and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

Various modifications to these aspects will be readily apparent and the generic principles defined herein may be applied to other aspects. Thus the claims are not intended to be limited to the aspects shown herein but is to be accorded the full scope consistent with the language claims where reference to an element in the singular is not intended to mean one and only one unless specifically so stated but rather one or more. Unless specifically stated otherwise the term some refers to one or more. Pronouns in the masculine e.g. his include the feminine and neuter gender e.g. her and its and vice versa. Headings and subheadings if any are used for convenience only and do not limit the subject technology.

A phrase for example an aspect does not imply that the aspect is essential to the subject technology or that the aspect applies to all configurations of the subject technology. A disclosure relating to an aspect may apply to all configurations or one or more configurations. A phrase for example an aspect may refer to one or more aspects and vice versa. A phrase for example a configuration does not imply that such configuration is essential to the subject technology or that such configuration applies to all configurations of the subject technology. A disclosure relating to a configuration may apply to all configurations or one or more configurations. A phrase for example a configuration may refer to one or more configurations and vice versa.

