---

title: Systems and methods of eye tracking control on mobile device
abstract: Methods and systems to facilitate eye tracking control on mobile devices are provided. An image of a portion of a user is received at an eye tracking device, where the image includes reflections caused by light emitted on the user from one or more light sources located within the eye tracking device. One or more eye features associated with an eye of the user is detected using the reflections. Point of regard information is determined using the one or more eye features, where the point of regard information indicates a location on a display of a computing device coupled to the eye tracking device at which the user was looking when the image of the portion of the user was taken. The point of regard information is sent to an application capable of performing a subsequent operation using the point of regard information.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09612656&OS=09612656&RS=09612656
owner: Facebook, Inc.
number: 09612656
owner_city: Menlo Park
owner_country: US
publication_date: 20131125
---
This application claims a priority benefit of U.S. Provisional Application No. 61 730 407 filed Nov. 27 2012 entitled Systems and Methods of Eye Tracking Control on Mobile Device which is incorporated herein by reference in its entirety.

The present disclosure generally relates to mobile devices and more specifically to systems and methods for facilitating eye tracking control on mobile devices.

Example systems and methods to facilitate eye tracking control on mobile devices are described. In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of example embodiments. It will be evident however to one skilled in the art that the present technology may be practiced without these specific details.

A user of a mobile device may interact with and control objects and applications displayed on the mobile device through the user s eye movement. An image of the user s eyes and or face captured by a front facing camera on or coupled to the mobile device may be analyzed using computer vision algorithms such as for example eye tracking algorithms and gaze detection algorithms. For example the captured images may be processed to extract information relating to one or more features of the user s eyes and or face. The mobile device may then use the extracted information to determine the location of the user s eyes and estimate the location on the display at which the user is looking For example the mobile device may be able to estimate at which icon on the display the user is looking The estimation of where the user is looking may be used to direct one or more objects applications and the like to perform a particular operation. For example the user may direct and control the movement of an object on the screen depending on where the user is looking on the display of the mobile device including controlling scrolling functions the movement of objects in a virtual game and the like.

A docking device may be coupled to the mobile device in any manner such as through a USB port on the mobile device micro USB port on the mobile device and the like. While the docking device of is depicted at the bottom of the mobile device one of ordinary skill in the art will appreciate that the docking device may be located at any suitable location relative to the mobile device . The docking device may include a camera module and one or more light emitting diodes LEDs . For explanatory purposes LEDs are depicted and described throughout the disclosure. However one of ordinary skill in the art will appreciate that any appropriate light emitting source may be used e.g. infrared laser .

The docking device may include any number of infrared LEDs that may be placed in a suitable location in any manner within the docking device e.g. tilted at an angle such that it points toward the user s face . In some embodiments the docking device may have either three or six LEDs e.g. for batteries with voltage 5V . However any number of LEDs and any arrangement may be used with or without voltage divider circuitry. In a specific embodiment the one or more LEDs may have any one or more of the following features or characteristics 1 Watt or 3 Watts emitting at 850 nm emitting light at approximately 30 40 degrees field of emission placed in pairs e.g. to bring the voltage level to an appropriate voltage e.g. from 3.7V to 1.85V based on the current the LEDs need blinks in the same frequency as the camera records and the like.

In some embodiments the docking device may also include a suitable type of infrared pass filter e.g. active mechanical high pass band pass etc. . In some embodiments a high pass filter that blocks light below 800 nm and allows light above 800 nm is used. In some embodiments the infrared pass filter may only allow light between 800 900 nm to enter the one or more cameras of the camera module .

The camera module may include one or more front facing cameras placed in any suitable location in any manner within the docking device e.g. tilted at an angle such that it points toward the user s face and may be used to capture images of the user s eyes and or face. The one or more cameras may be placed at an appropriate distance from the LEDs to optimize the proper capture of the infrared light. In some embodiments a camera on the mobile device is used in combination with camera module in stereo mode. The camera module may include any one or more of the following a black and white e.g. monochrome or color e.g. RGB CMOS sensor running at an appropriate frame per second rate e.g. minimum high definition at 30 frames per second a lens without an infrared block filter and with an appropriate field of view e.g. approximately 35 degrees and depth of field e.g. approximately 40 80 cm and the like. The one or more cameras in the camera module may be positioned such that the one or more cameras are tilted upward e.g. toward a user s face .

The images captured by the camera may need to be rotated. The eye tracking software can use sensors on the mobile device e.g. accelerometer magnetometer etc. to detect the orientation of the mobile device and rotate the image accordingly so that it can be properly processed.

The LEDs emit light that is focused and centered toward the eyes of the user. The infrared light from the LEDs is reflected in the pupil and on the cornea of the user and recorded by the cameras in the camera module . The LEDs may be synchronized with the one or more cameras so that the LEDs are on only when the one or more cameras are grabbing an image. In some embodiments to improve the image quality the visible light below 800 nm is filtered out using an infrared pass filter. The field of view and depth of view of the lenses of the one or more cameras in the camera module may allow the user to move around thereby accommodating for head pose variance of the user. The eye tracking control software may analyze the images taken by the camera module to provide x y coordinates of where the user is looking on the display of the mobile device . The x y coordinates may be used for any number of applications e.g. scrolling moving objects selecting icons playing games etc. .

The LEDs and the camera module may be turned on and or off in any manner such as by utilizing an external slider an on off dedicated button on the side or on the back of either the mobile device or the docking device controlled by an application or a digital button on the screen controlled by movement or shaking of the mobile device and or the docking device controlled by voice commands on screen capacitive buttons touch pad s bio signals e.g. EMG EEG etc. and the like. As such in some embodiments the eye tracking components may consume power only while the LEDs and the camera are turned on e.g. when the user is using the eye tracking features .

In some embodiments the eye tracking features are optimized when the camera is located at the bottom of the mobile device e.g. with respect to the perspective of the user . The user may rotate the mobile device coupled to the docking device to properly orient the camera module such that it is located at the bottom of the mobile device . In some embodiments using the accelerometer and or magnetometer of the mobile device the LEDs the pass filter and or the camera may be turned on and or off depending on the orientation of the mobile device and the docking device e.g. turn off the LEDs and the camera when the mobile device and the docking device are rotated such that the camera module is located at the top of the mobile device with respect to the perspective of the user .

The LEDs and the camera may be turned off when the user s face is not recognized for a predetermined amount of time e.g. 5 10 seconds and may turn on again when the user s face is detected and recognized.

The LEDs and the camera modules on the mobile devices may be located in any one of a number of configurations on the mobile devices. shows the LEDs and the camera module being located at the bottom of the mobile device . shows the LEDs being located on one side of the mobile device while the camera module is located on the opposite side of the mobile device . shows the LEDs and the camera module being located on the same side of the mobile device .

The software architecture may be divided into different layers. The bottom layer would correspond to the hardware e.g. the camera s the infrared illumination etc. . A camera layer may be in charge of communicating with the camera s in order to perform camera operations such as for example starting the camera grabbing images controlling the camera properties and the like. This layer may also synchronize the one or more cameras and the infrared emitters so that the lights are turned on when there is an image being captured and off the rest of the time e.g. strobing .

The camera layer may deliver images to the eye tracking layer. In the eye tracking layer images may be processed to find features like face location eye region location pupil center pupil size location of the corneal reflections eye corners iris center iris size and the like. These features are used in the gaze estimation stage which may be in charge of calculating the point of regard of the user which may be the location on the display where the user is looking The gaze estimation stage may also calculate the optical and visual axes of the user s eyes.

The API layer may be used for communication between the eye tracking layer and applications that use eye gaze information e.g. OS API games that employ eye gaze information etc. . The API may send data calculated by the eye tracking layer such as coordinates of the point of regard three dimensional 3 D location of the user s eyes pupil size and the like. The API may also accept commands from an application to the eye tracking layer e.g. to start and or stop the eye tracking engine query for specific information etc. . An application may connect to the eye tracker s API and use eye gaze information for any suitable purpose e.g. control an app or a game record eye data for visual behavior studies etc. .

A calibration process may be conducted the first time the user uses the eye tracking functionality in order to calculate personal parameters e.g. vertical and horizontal offset between optical and visual axes . These personal parameters and the information of the face and eyes are then employed to estimate where the user is looking on the screen through a gaze estimation algorithm.

In operation an image of a portion of a user is received at the mobile device. The image includes reflections e.g. corneal reflections caused by light emitted on the user.

In operation eye features e.g. pupil location size corneal reflection location eye corners iris location etc. of the user are detected using the reflections received.

In operation a point of regard is determined for the user using the eye features detected. Optical and or visual axes may also be determined. The determination of the point of regard may account for the location of the one or more camera and the LEDs with respect to the screen.

In operation the point of regard information is sent to an application capable of using the point of regard information in a subsequent operation.

Certain embodiments are described herein as including logic or a number of components modules or mechanisms. Modules may constitute either software modules e.g. code embodied on a machine readable medium or in a transmission signal or hardware modules. A hardware module is a tangible unit capable of performing certain operations and may be configured or arranged in a certain manner. In example embodiments one or more computer systems e.g. a standalone client or server computer system or one or more hardware modules of a computer system e.g. a processor or a group of processors may be configured by software e.g. an application or application portion as a hardware module that operates to perform certain operations as described herein.

In various embodiments a hardware module may be implemented mechanically or electronically. For example a hardware module may comprise dedicated circuitry or logic that is permanently configured e.g. as a special purpose processor such as a field programmable gate array FPGA or an application specific integrated circuit ASIC to perform certain operations. A hardware module may also comprise programmable logic or circuitry e.g. as encompassed within a general purpose processor or other programmable processor that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a hardware module mechanically in dedicated and permanently configured circuitry or in temporarily configured circuitry e.g. configured by software may be driven by cost and time considerations.

Accordingly the term hardware module should be understood to encompass a tangible entity be that an entity that is physically constructed permanently configured e.g. hardwired or temporarily configured e.g. programmed to operate in a certain manner and or to perform certain operations described herein. Considering embodiments in which hardware modules are temporarily configured e.g. programmed each of the hardware modules need not be configured or instantiated at any one instance in time. For example where the hardware modules comprise a general purpose processor configured using software the general purpose processor may be configured as respective different hardware modules at different times. Software may accordingly configure a processor for example to constitute a particular hardware module at one instance of time and to constitute a different hardware module at a different instance of time.

Hardware modules can provide information to and receive information from other hardware modules. Accordingly the described hardware modules may be regarded as being communicatively coupled. Where multiple of such hardware modules exist contemporaneously communications may be achieved through signal transmission e.g. over appropriate circuits and buses that connect the hardware modules. In embodiments in which multiple hardware modules are configured or instantiated at different times communications between such hardware modules may be achieved for example through the storage and retrieval of information in memory structures to which the multiple hardware modules have access. For example one hardware module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled. A further hardware module may then at a later time access the memory device to retrieve and process the stored output. Hardware modules may also initiate communications with input or output devices and can operate on a resource e.g. a collection of information .

The various operations of example methods described herein may be performed at least partially by one or more processors that are temporarily configured e.g. by software or permanently configured to perform the relevant operations. Whether temporarily or permanently configured such processors may constitute processor implemented modules that operate to perform one or more operations or functions. The modules referred to herein may in some example embodiments comprise processor implemented modules.

Similarly the methods described herein may be at least partially processor implemented. For example at least some of the operations of a method may be performed by one or more processors or processor implemented modules. The performance of certain of the operations may be distributed among the one or more processors not only residing within a single machine but deployed across a number of machines. In some example embodiments the processor or processors may be located in a single location e.g. within a home environment an office environment or as a server farm while in other embodiments the processors may be distributed across a number of locations.

The one or more processors may also operate to support performance of the relevant operations in a cloud computing environment or as a software as a service SaaS . For example at least some of the operations may be performed by a group of computers as examples of machines including processors these operations being accessible via a network e.g. the Internet and via one or more appropriate interfaces e.g. Application Program Interfaces APIs .

Example embodiments may be implemented in digital electronic circuitry or in computer hardware firmware software or in combinations of them. Example embodiments may be implemented using a computer program product e.g. a computer program tangibly embodied in an information carrier e.g. in a machine readable medium for execution by or to control the operation of data processing apparatus e.g. a programmable processor a computer or multiple computers.

A computer program can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module subroutine or other unit suitable for use in a computing environment. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.

In example embodiments operations may be performed by one or more programmable processors executing a computer program to perform functions by operating on input data and generating output. Method operations can also be performed by and apparatus of example embodiments may be implemented as special purpose logic circuitry e.g. a FPGA or an ASIC .

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other. In embodiments deploying a programmable computing system it will be appreciated that that both hardware and software architectures require consideration. Specifically it will be appreciated that the choice of whether to implement certain functionality in permanently configured hardware e.g. an ASIC in temporarily configured hardware e.g. a combination of software and a programmable processor or a combination of permanently and temporarily configured hardware may be a design choice. Below are set out hardware e.g. machine and software architectures that may be deployed in various example embodiments.

Example computer system includes a processor e.g. a central processing unit CPU a graphics processing unit GPU or both a main memory and a static memory which communicate with each other via a bus . Computer system may further include a video display device e.g. a liquid crystal display LCD or a cathode ray tube CRT . Computer system also includes an alphanumeric input device e.g. a keyboard a user interface UI navigation device e.g. a mouse or touch sensitive display a disk drive unit a signal generation device e.g. a speaker and a network interface device .

Disk drive unit includes a machine readable medium on which is stored one or more sets of instructions and data structures e.g. software embodying or utilized by any one or more of the methodologies or functions described herein. Instructions may also reside completely or at least partially within main memory within static memory and or within processor during execution thereof by computer system main memory and processor also constituting machine readable media.

While machine readable medium is shown in an example embodiment to be a single medium the term machine readable medium may include a single medium or multiple media e.g. a centralized or distributed database and or associated caches and servers that store the one or more instructions or data structures. The term machine readable medium shall also be taken to include any tangible medium that is capable of storing encoding or carrying instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present technology or that is capable of storing encoding or carrying data structures utilized by or associated with such instructions. The term machine readable medium shall accordingly be taken to include but not be limited to solid state memories and optical and magnetic media. Specific examples of machine readable media include non volatile memory including by way of example semiconductor memory devices e.g. Erasable Programmable Read Only Memory EPROM Electrically Erasable Programmable Read Only Memory EEPROM and flash memory devices magnetic disks such as internal hard disks and removable disks magneto optical disks and CD ROM and DVD ROM disks.

Instructions may further be transmitted or received over a communications network using a transmission medium. Instructions may be transmitted using network interface device and any one of a number of well known transfer protocols e.g. HTTP . Examples of communication networks include a local area network LAN a wide area network WAN the Internet mobile telephone networks Plain Old Telephone POTS networks and wireless data networks e.g. WiFi and WiMAX networks . The term transmission medium shall be taken to include any intangible medium that is capable of storing encoding or carrying instructions for execution by the machine and includes digital or analog communications signals or other intangible media to facilitate communication of such software.

Although an embodiment has been described with reference to specific example embodiments it will be evident that various modifications and changes may be made to these embodiments without departing from the broader spirit and scope of the technology. Accordingly the specification and drawings are to be regarded in an illustrative rather than a restrictive sense. The accompanying drawings that form a part hereof show by way of illustration and not of limitation specific embodiments in which the subject matter may be practiced. The embodiments illustrated are described in sufficient detail to enable those skilled in the art to practice the teachings disclosed herein. Other embodiments may be utilized and derived therefrom such that structural and logical substitutions and changes may be made without departing from the scope of this disclosure. This Detailed Description therefore is not to be taken in a limiting sense and the scope of various embodiments is defined only by the appended claims along with the full range of equivalents to which such claims are entitled.

Such embodiments of the inventive subject matter may be referred to herein individually and or collectively by the term invention merely for convenience and without intending to voluntarily limit the scope of this application to any single invention or inventive concept if more than one is in fact disclosed. Thus although specific embodiments have been illustrated and described herein it should be appreciated that any arrangement calculated to achieve the same purpose may be substituted for the specific embodiments shown. This disclosure is intended to cover any and all adaptations or variations of various embodiments. Combinations of the above embodiments and other embodiments not specifically described herein will be apparent to those of skill in the art upon reviewing the above description.

