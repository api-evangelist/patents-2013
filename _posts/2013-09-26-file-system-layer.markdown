---

title: File system layer
abstract: In one aspect, a method includes using a file system layer configured to interact with a plurality of volumes and enabling an application to interact with any of the plurality of volumes using a single file system format provided by the file system layer. At least two of the plurality of volumes have different file system formats.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09442938&OS=09442938&RS=09442938
owner: EMC Corporation
number: 09442938
owner_city: Hopkinton
owner_country: US
publication_date: 20130926
---
This application is a continuation in part application of and claims priority to U.S. patent application Ser. No. 13 886 892 filed May 3 2013 titled SCALABLE INDEX STORE which is incorporated herein. This application is also a continuation in part application of and claims priority to U.S. patent application Ser. No. 13 886 915 filed May 3 2013 titled SCALABLE OBJECT STORE which is incorporated herein.

Computer systems may include different resources used by one or more host processors. Resources and host processors in a computer system may be interconnected by one or more communication connections. These resources may include for example data storage devices such as those included in the data storage systems manufactured by EMC Corporation. These data storage systems may be coupled to one or more servers or host processors and provide storage services to each host processor. Multiple data storage systems from one or more different vendors may be connected and may provide common data storage for one or more host processors in a computer system.

A host processor may perform a variety of data processing tasks and operations using the data storage system. For example a host processor may perform basic system I O operations in connection with data requests such as data read and write operations.

Host processor systems may store and retrieve data using a storage device containing a plurality of host interface units disk drives and disk interface units. Such storage devices are provided for example by EMC Corporation of Hopkinton Mass. and disclosed in U.S. Pat. No. 5 206 939 to Yanai et al. U.S. Pat. No. 5 778 394 to Galtzur et al. U.S. Pat. No. 5 845 147 to Vishlitzky et al. and U.S. Pat. No. 5 857 208 to Ofek. The host systems access the storage device through a plurality of channels provided therewith. Host systems provide data and access control information through the channels to the storage device and storage device provides data to the host systems also through the channels. The host systems do not address the disk drives of the storage device directly but rather access what appears to the host systems as a plurality of logical disk units logical devices or logical volumes. The logical disk units may or may not correspond to the actual physical disk drives. Allowing multiple host systems to access the single storage device unit allows the host systems to share data stored therein. In a common implementation a Storage Area Network SAN is used to connect computing devices with a large number of storage devices. Management and modeling programs may be used to manage these complex computing environments.

Two components having connectivity to one another such as a host and a data storage system may communicate using a communication connection. In one arrangement the data storage system and the host may reside at the same physical site or location.

Techniques exist for providing a remote mirror or copy of a device of the local data storage system so that a copy of data from one or more devices of the local data storage system may be stored on a second remote data storage system. Such remote copies of data may be desired so that in the event of a disaster or other event causing the local data storage system to be unavailable operations may continue using the remote mirror or copy.

In another arrangement the host may communicate with a virtualized storage pool of one or more data storage systems. In this arrangement the host may issue a command for example to write to a device of the virtualized storage pool. In some existing systems processing may be performed by a front end component of a first data storage system of the pool to further forward or direct the command to another data storage system of the pool.

Such processing may be performed when the receiving first data storage system does not include the device to which the command is directed. The first data storage system may direct the command to another data storage system of the pool which includes the device.

The front end component may be a host adapter of the first receiving data storage system which receives commands from the host. In such arrangements the front end component of the first data storage system may become a bottleneck in that the front end component processes commands directed to devices of the first data storage system and additionally performs processing for forwarding commands to other data storage systems of the pool as just described.

Often cloud computer may be performed with a data storage system. As it is generally known cloud computing typically refers to the use of remotely hosted resources to provide services to customers over one or more networks such as the Internet. Resources made available to customers are typically virtualized and dynamically scalable. Cloud computing services may include any specific type of application. Some cloud computing services are for example provided to customers through client software such as a Web browser. The software and data used to support cloud computing services are located on remote servers owned by a cloud computing service provider. Customers consuming services offered through a cloud computing platform need not own the physical infrastructure hosting the actual service and may accordingly avoid capital expenditure on hardware systems by paying only for the service resources they use and or a subscription fee. From a service provider s standpoint the sharing of computing resources across multiple customers aka tenants improves resource utilization. Use of the cloud computing service model has been growing due to the increasing availability of high bandwidth communication making it possible to obtain response times from remotely hosted cloud based services similar to those of services that are locally hosted.

Cloud computing infrastructures often use virtual machines to provide services to customers. A virtual machine is a completely software based implementation of a computer system that executes programs like an actual computer system. One or more virtual machines may be used to provide a service to a given customer with additional virtual machines being dynamically instantiated and or allocated as customers are added and or existing customer requirements change. Each virtual machine may represent all the components of a complete system to the program code running on it including virtualized representations of processors memory networking storage and or BIOS Basic Input Output System . Virtual machines can accordingly run unmodified application processes and or operating systems. Program code running on a given virtual machine executes using only virtual resources and abstractions dedicated to that virtual machine. As a result of such encapsulation a program running in one virtual machine is completely isolated from programs running on other virtual machines even though the other virtual machines may be running on the same underlying hardware. In the context of cloud computing customer specific virtual machines can therefore be employed to provide secure and reliable separation of code and data used to deliver services to different customers.

In one aspect a method includes using a file system layer configured to interact with a plurality of volumes and enabling an application to interact with any of the plurality of volumes using a single file system format provided by the file system layer. At least two of the plurality of volumes have different file system formats.

In another aspect an apparatus includes electronic hardware circuitry configured to provide a file system layer configured to interact with a plurality of volumes and enable an application to interact with any of the plurality of volumes using a single file system format provided by the file system layer. At least two of the plurality of volumes have different file system formats.

In a further aspect an article includes a non transitory computer readable medium that stores computer executable instructions. The instructions cause a machine to provide a file system layer configured to interact with a plurality of volumes and enable an application to interact with any of the plurality of volumes using a single file system format provided by the file system layer. At least two of the plurality of volumes have different file system formats.

Described herein is a file system layer. The file system layer may be accessed by an application and from the perspective of the application sees only a single file system format even though the application can use the file system layer to interact with devices e.g. storage volumes with different types of file system formats.

Conventionally object systems may not be scalable. Usually and object system may not offer file access. Typically a file system may not offer object access. Usually a file system may not switch between file and object access.

In certain embodiments the current disclosure may enable storage of a large table or index of key strings along with their corresponding value bytes on file shares across multiple devices.

In some embodiments the current disclosure may enable an indexing service in a VM image that may be installed on a machine. In certain embodiments a set of such nodes may form an indexing service layer. In an embodiment a set of file shares may be created on NAS devices and registered with the ViPR indexing service for storage of data.

In certain embodiments nodes may form a fault tolerant layer over NAS devices. In at least some embodiments any number of nodes may be removed or added at any time without affecting the availability of an indexing system. In other embodiments any number of ViPR nodes may be unavailable but the transaction processing for any part of the data may continue to give a functioning node. In most embodiments each node may be connected to a number of file shares. In certain embodiments each node may be able to read and write data from any of file shares. In other embodiments each node may also accept transaction for any part of the data for any file share.

In most embodiments the current disclosure enables a horizontally scalable architecture. In certain embodiments if file shares run out of space new file shares can be created and registered with nodes. In certain embodiments the indexing system may start placing new incoming writes in the new file shares. In further embodiments any number of nodes may be dynamically added in the system to increase the transaction processing capacity of the system.

In certain embodiments a Virtual Storage Pool may be a definition of the characteristics of a file share device. In most embodiments each file share registered with a data service may be associated with a Virtual Storage Pool.

In some embodiments the desired Virtual Storage Pool characteristic for an index may be specified during its creation. In at least some embodiments the data belonging to the index may be stored on the file shares that are associated with the Virtual Storage Pool. In other embodiments if multiple file shares are associated with the Virtual Storage Pool the data of the index may be spread across all the file shares. In certain embodiments the index may be associated with a Virtual Storage Pool. In an embodiment a Virtual Storage Pool may form logically disjoint sets in which data set is divided.

In some embodiments a Virtual Storage Pool may be divided into one or more Partitions. In certain embodiments partitioning may be done based on consistent hashing. In at least some embodiments a hash number of a key may be found by deriving the SHA 256 value of the key string. In other embodiments each partition may be identified by the divisor remainder pair of the hash space. In a particular embodiment if a partition has divisor identifier 4 and remainder identifier 1 then it may contain all the keys whose hash value when divided by 4 gives remainder 1. In most embodiments the partition identifiers may be scoped within the Virtual Storage Pool so each of Virtual Storage Pool can have same partition identifiers. In certain embodiments a partition may be responsible for storage of data associated with the keys that fall in its hash.

In most embodiments the number of partitions in the system may change dynamically depending on the resources in the system. In certain embodiments if ViPR nodes and NAS devices are added in the system then better load balancing may be achieved by automatically increasing the number of partitions in the system. In some embodiments better load balancing may be achieved by a partition split process.

In at least some embodiments the number of partitions may automatically be decreased when the resources become constrained. In one embodiment partitions may be decreased when more file shares are created on existing devices or when a number of ViPR nodes are removed from the system.

In alternative embodiments a partition with identifiers divisor 4 remainder 1 may split into two partitions with identifiers divisor 8 remainder 1 and divisor 8 remainder 5. In other embodiments two partitions with identifiers divisor 4 remainder 1 and divisor 4 remainder 3 may merge into one partition with identifier divisor 2 remainder 1.

In most embodiments nodes may host a database. In some embodiments the database may be Cassandra. In certain embodiments the database data may be stored in local disks on nodes. In further embodiments the database may be for storing system s metadata and not for any of the index s data. In at least some embodiments nodes may host a small instance of a lock service or locking service. In at least one embodiment the locking service may be Zookeeper. In most embodiments the locking service may provide the lock service for the nodes to coordinate with other nodes.

In most embodiments the information about each partition in the system may be stored as an entry in a database. In certain embodiments the entry may have the identifier for the partition Virtual Storage Pool divisor remainder . In some embodiments the entry may have the node identifier to specify which node is currently responsible for the partition. In at least one embodiment the entry may have a location on the file share where the metadata record of the partition is stored. In at least some embodiments a location may be identified by the file share identifier the relative path of file under the file share the offset in the file where the record begins and the length of the record.

In an embodiment the metadata record of a partition may be stored in a file on the file share. In certain embodiments the metadata record may contains the information about the latest B tree of the partition and position in the journal file. In some embodiments the journal file may be used as a redo log for holding the data that hasn t been included in the B tree yet. In other embodiments the location in the metadata record for the journal may contain the file share id the relative path of file under the file share and the offset in the file. In most embodiments the journal file may be on any file share which need not be same file share where the B tree files and metadata record files are for that partition.

In certain embodiments data transactions for partitions may be logged into the journal. In most embodiments once enough entries are accumulated in journal the entries may be inserted into a B tree and the journal position may be advanced. In some embodiments in the case the node responsible for the partition crashes another node which picks up the responsibility may replay the transactions from the last journal position recorded in the metadata record.

In an embodiment a B tree structure may be maintained to store the keys belonging to the partition and corresponding values. In other embodiments the pages of the B tree may be stored in the files on the file shares. In some embodiments the location of pages in the tree may be identified by file share id the relative path of file under the file share and offset in the file. In other embodiments the B tree may be spread across multiple file shares. In further embodiments the B tree structure may support multiversion concurrency control and read snapshot isolation. In at least one embodiment the existing pages may not be modified and modifications may be written as new pages in the file.

In an embodiment a partition may have files for metadata record B tree and journal. In certain embodiments the B tree and journal may span multiple files. In other embodiments each structure s location may be reached via a chain of pointers starting from the partition entry in Cassandra. In most embodiments the partition structure may not be bound to a fixed location. In a particular embodiment if a file share capacity is getting full the journal writes and B tree modifications may be moved to another file share without break in continuity or consistency.

In most embodiments when a node gets a transaction for a key it may calculate a hash value of the key. In certain embodiments the node may query the database to find into which partition the key falls. In some embodiments the partition information may be cached for future transactions. In alternative embodiments a node may send the transaction to the node responsible for the key to execute the transaction. In other embodiments if the cached information about the partition responsibility was stale the destination node may return a specific error code which may cause the source node to query the database and refresh the information to the latest state.

In an embodiment if a node discovers that the responsibility division of the partitions is uneven the node may take the responsibility from another node. In some embodiments the consistent hashing scheme for partitioning may result in random and even distribution of the load. In at least some embodiments the number of partitions may be the criteria for measuring even split of responsibility among the nodes.

In most embodiments nodes periodically check the database for the partitions that the node is responsible for to see if the node is still the owner. In another embodiment if a node wishes to take over ownership of a partition the node may register itself as the owner in the database. In at least some embodiments the node may wait for a periodic refresh interval for the original owner node to find out that the original node is not the owner anymore and stop serving the transactions for the partition. In most embodiments if a node is not able to reach the database it may stop serving the transactions for the partition until the node can successfully validate that it is the owner. In further embodiments if a node cannot reach the owner node for some time the node may assume that the owner node is down and may take responsibility for the partition.

In some embodiments an object system may be built on top of an indexing system. In certain embodiments an object system may provide object semantics for creating objects reading objects reading and writing metadata associated with the object. In further embodiments the object system may support byte range update on the object contents and atomic append to the object data. In most embodiments the object system may support REST protocols and the related features of S3 Atmos and Swift. In further embodiments an object service or object system may provide a single namespace that may span across multiple file shares.

In certain embodiments objects may be grouped in one or more buckets. In most embodiments a bucket may support operations such as listing of all the objects in the bucket. In some embodiments the list of object names in a bucket may be stored in an indexing system. In a particular embodiment a SHA 256 of the bucket name may be used for deriving a hash id of the partition where the list is stored. In at least some embodiments when an object is created an entry may be made in the indexing system for the bucket id and object name. In other embodiments the listing of bucket operations may go through the entries in the indexing for the bucket id.

In an embodiment each change or mutation to an object may be stored as a separate transaction. In most embodiments storing each change as a separate transaction may provide a journal of changes to the object without overwriting the previous state. In certain embodiments recording a separate object may enable snapshot read isolation. In further embodiments querying the object at a given point in time may see the same consistent state of object throughout the read duration as it was when it started reading.

In other embodiments the data associated with a change or mutation in an object may be written directly into a file on the fileshare. In certain embodiments the location of the data may be stored in the indexing system as an update entry. In a particular embodiment a given object may have many update entries in the index each with location of the data on the file system. In at least some embodiments a reader may need to go through all the update entries of an object to get the current state of the object. In some embodiments the system may consolidate the update entries of an object when there are no readers. In alternative embodiments SHA 256 of namespace bucket object name may be used for deriving the hash id of the partition where the update entries for the object are stored.

In certain embodiments multiple transactions for atomically appending the data to the object may be issued. In some embodiments the update sequencing on the server side of the indexing system may order the append transactions and may provide the atomicity.

In an embodiment the file access feature may provide ability to access the object data through the native file system interface of the NAS device by mounting the fileshare. In certain embodiments the user may send a request to get file access for a bucket. In some embodiments the system may return the full file path for each of the objects in the bucket. In other embodiments modifications made through the file interface on those objects may be reflected in the object data. In at least some embodiments during file access modifications to the object through REST interface may be prevented. In alternative embodiments when a user is done with file access the REST interface may be accessible. In at least one embodiment internally the system may consolidate the update entries and data of an object and may place them into a single file before giving the file out for file access.

Refer now to the simplified embodiment of . In the example embodiment of indexing system has locking service nodes and and database . Database has storage pool . Storage pool has gold storage and bronze storage . Site has array and . Each array has two levels of storage such as Gold Bronze or Platinum. For example array has gold and bronze service levels. Each node and is connected through network to each storage array and . Each of the arrays may be stored in database as belonging to one or more storage pools based on the Class of Services offered by that storage array. Each node and has access to an object system and an index system. In certain embodiments the object system may be for storing objects. In some embodiments the index system may be for storing the location of the stored objects for scalable access.

Refer now to the example embodiments of . In array has been added to site step . Storage Array has been registered with nodes and step . In this way subsequent arrays may be added to site and registered with the nodes and indexing system.

Refer now to the example embodiments of which illustrate committing journal entries to a B tree. When mutations are received to objects in an indexing system the mutations are recorded in journal on storage array step .

After journal has reached a certain size the transactions in journal are committed to B tree . B Tree is stored on arrays and . As B Tree is a tree each node of the tree may be stored on a different array with a pointer pointing to the next node.

Refer now to the example embodiments of which illustrate moving the recording of mutations in a B tree and journal to a different array from a first tree. Partition metadata has oldB tree oldJournal B Tree and Journal . It has been determined that file array is full . Recording mutations to metadata to oldB tree and oldJournal are stopped. New mutations to metadata are recorded in B tree and journal on array step .

Refer now to the example embodiments of which illustrate a new object with a requested Class of service gold being recorded in an indexing system. Object A is being broken up and stored on arrays and based on a request for a gold class of service for this object. A request for creation and storing of object is received step . Object system via node determines in which storage pool object is to be stored step . Object system via node determines on which array or arrays the object is to be stored step . Object system via node writes object data to the array . Object system via node finds the location of data written on the array step . The hash for object is calculated step . A partition for object is determined and stored in indexing system via node step . A node for object is determined by indexing system via node step . An array is determined for object by indexing system via node step . Object is sent to array step . The object write is recorded in the journal step .

Refer now to the example embodiments of which illustrate a new object with a requested class of service of bronze being recorded in an indexing system.

Object B is being broken up and stored on arrays and based on a request for a bronze class of service for this object. A request for creation and storing of object is received step . Object system via node determines in which storage pool object is to be stored step . Object system via node determines on which array or arrays the object is to be stored step . Object system via node writes object data to the array . Object system via node finds the location data written on the array step . The hash for object is calculated step . A partition for object is determined and stored in indexing system via node step . A node for object is determined by indexing system via node step . An array is determined for object by indexing system via node step . Object is sent to array step . The object write is recorded in the journal step .

Refer now to the example embodiments of which illustrate a read to an object stored in an indexing system. Indexing system receives read to object A .

Node receives read request and calculates hash of object . Node determines partition that has object metadata and determines node handling that partition . Node obtains object metadata from node handling partition determines array or arrays where object data is stored from the object metadata and sends read to array or arrays .

Refer now to the example embodiments of which illustrate handling a node failure. Indexing system receives a read to object A and determines which node is to handle reads to this object step . Indexing system realizes there is a failure in node the previously determined node step . Indexing system assigns new node to handle the read for object A step . Node determines array has the information for the read step . Node sends the read to the array step .

Refer now to the example embodiments of which illustrate handling an append to an object. Indexing system receives read to object A . Indexing system determines on which one or more arrays object A is stored step . Indexing system writes the object data to the array step . Indexing system finds the location the data is written on the array step . Indexing system calculates the hash for object A step . Indexing system determines the partitions on which object A is stored step . Indexing system determines the node handling the partition step . Node determines the array step . Node sends the read to the array step .

Refer now to the example embodiments of which illustrate a node determining an uneven partition allocation and taking control of a partition. Node determines that node has an uneven allocation of partitions step . Node takes control of one of node s partitions to service read to object A step .

Refer now to the example embodiment of which illustrates an object system layered over an indexing system. Object system has semantics bucket objects and and indexing system . Semantics has the ability to create objects read objects read metadata and write metadata . Buckets and contain objects and are classifiers for objects. Object system is connected to storage location by network . Storage location has arrays and .

Refer now to the example embodiments of which illustrate creating a bucket. Object system receives a request to create bucket financials . Object system creates bucket financials step .

Refer now to the example embodiments of . Object system receives a request to add an object to a bucket where the object has not yet been created.

Object system via node determines in which storage pool object is to be stored step . Object system via node determines on which array or arrays the object is to be stored step . Object system via node writes object data to an array or arrays . Object system via node finds the location of data written on the array step . The hash for object is calculated step . A partition for object is determined and stored in indexing system via node step . A node for object is determined by indexing system via node step . An array is determined for object by indexing system via node step . Object is sent to array step . The object write is recorded in the journal step . The bucket name is added by node to the indexing system step .

Refer now to the example embodiments of . Indexing system writes the object data to the array step . Indexing system finds the location the data is written on the array step . Indexing system calculates the hash for object A step . Indexing system determines the partitions on which object A is stored step . Indexing system determines the node handling the partition step . Node determines the array step . The data that is changed is recorded in journal step .

Refer now to the example embodiments of which show responding to a status request for an object. Object system receives a request for status for object w step . Object system gets the locations in the indexing system that correspond to the object requested step . Node reads the entries from the indexing system to return the object status step .

Refer now to the example embodiments of which show file access to an object system. Object system receives a request for file system access step . The object system calculates the paths for the file system step . The object system consolidates the object entries step . The object system returns the file paths step . The file paths are mounted and read write access for the file system may be enabled step . In some embodiments read write access to the file system may be enabled and access to the objects may not be permitted. In other embodiments read access to both the file system and the object system may be enabled. In still further embodiments read write access to the object system may be enabled and no access to the file system may be enabled.

In further embodiments the data storage arrays may of the block type file type or object type. In some embodiments the object system may span across block file and object arrays. In other embodiments the indexing system may span across file block and object arrays. In further embodiments the object system may span across public accounts. In other embodiments the indexing system may span across public accounts. In some embodiments the current disclosure may enable an object to be stored and received from a public cloud such as Amazon s S3 or Microsoft s Azure. In other embodiments any type of array may be used and the current disclosure may enable coordination across the arrays regardless of type.

For example refer now to the example embodiment of which illustrates different types of storage systems over laid with an object system. Object system communicates with cloud and site over network . Cloud is a public cloud and information may be stored in and retrieved from the public cloud using object system . Site has block arrays and object arrays and file arrays and . Object system enables objects to be stored and retrieved any array and cloud . As well Object system also enables file access to objects stored in the arrays and cloud. In certain embodiments the cloud may be a private cloud. In other embodiments the cloud may be a public cloud.

In further embodiments an orchestration API may be part of a larger API or coordination API. In some embodiments an orchestration API may request input from a large API or Orchestration engine. In other embodiments an orchestration API may request input from a user. In still further embodiments an orchestration API may be one of a set of other orchestration APIs wherein each of the set of orchestration APIs offer different orchestration functionality. In of these embodiments the set of orchestration APIs may be combined with an overall Orchestration or Engine layer which may coordinate requests between the set of orchestration APIs.

Referring to a system configuration may be used by an application using a web browser not shown to access a file system layer from the Internet . The files system is connected to an object layer which interacts with volumes . In one example the volume uses an Isilion file system the volume uses a VNX file system and the volume supports a network file system NFS volume. The application can interact with the volumes using only one file system format provided by the system layer .

Referring to as described above an indexing system is used in conjunction with an object layer to provide access to the volumes. File systems are mapped as trees using key value pairs. For example a series of key value pairs are saved in array table or database. The key value pairs include a UUID universal unique identifier . That is each level of a directory tree includes a UUID and a key value pair. For example a key value pair UUIDparent.ND.childName UUIDchild includes a key UUIDparent.ND.childName and a value UUIDchild. The key value pair may represent a directory and subdirectory. Another key value the key value pair includes a key UUIDchild.OU.sequence and a value object segment location. The UUIDchild.OU.sequence corresponds to an update to the object. For example each update has a sequence number which is increasing and an object can have a sequence of updates. The object segment location represents a location in one of the volumes where the data is stored.

Referring to an example of a process to lookup a path is a process . Process gets the UUID of the root directory . Process determines if there is another level to the directory tree and if there is another level gets the UUID of the in the next level and repeats processing block . If there is no level left process returns the UUID of the path .

Referring to an example of a process to use the file system layer to create a file or directory is a process . For example the file foo bar 1.jpg is created. Process gets the UUID by looking up the path for example using the process . For example the UUID of foo bar is obtained by lookup of the path foo bar using the process .

Process inserts the entry . For example the entry UUID foo bar .ND.1.jpg UUID 1.jpg is added. Process creates an object with a key . For example an object with key UUID 1.jpg is created.

Referring to an example of a process to use the file system layer to delete a file or directory is a process . For example a file foo bar 1.jpg is deleted. Process gets the UUID by looking up the path for example using the process . For example UUID foo bar is obtained by looking up the path foo bar.

Process checks whether it is empty if it is a directory and removes the object UUID . For example process checks that the directory has no any children inside including files and sub directory and the object UUID 1.jpg is removed.

Process removes the entry . For example the entry that ties UUID foo bar .ND.1.jpg to UUID 1.jpg is removed. In some examples processing block is optional or not used.

Referring to an example of a process to use the file system layer to list a directory is a process . For example the directory foo bar is listed. Process gets the UUID by looking up the path for example using the process . For example UUID foo bar is obtained by looking up the path foo bar. Process lists the children in the directory by using the key UUID . For example the key UUID foo bar is used. For each child in the list process checks whether it is valid by looking at its system metadata .

Referring to an example of a process to use the file system layer to read or write a file is a process . Process get UUID by looking up the path for example using the process . For example UUID 1.jpg is obtained by looking up the path foo bar 1.jpg.

Referring to an example of a process to use the file system layer to rename a file or directory is a process . Process gets the UUID of the source parent UUID srcParent UUID of the destination parent UUID dstParent and UUID of the file UUID .

Process inserts an entry to the destination parent of UUID dstParent .ND.fileName UUID updates the system metadata of the parent and file name for the object UUID and removes the source entry UUID srcParent .NDfileName UUID .

Referring to in one example a computer includes a processor a volatile memory a non volatile memory e.g. hard disk and the user interface UI e.g. a graphical user interface a mouse a keyboard a display touch screen and so forth . The non volatile memory stores computer instructions an operating system and data . In one example the computer instructions are executed by the processor out of volatile memory to perform all or part of the processes described herein e.g. processes to .

The processes described herein e.g. processes to are not limited to use with the hardware and software of they may find applicability in any computing or processing environment and with any type of machine or set of machines that is capable of running a computer program. The processes described herein may be implemented in hardware software or a combination of the two. The processes described herein may be implemented in computer programs executed on programmable computers machines that each includes a processor a non transitory machine readable medium or other article of manufacture that is readable by the processor including volatile and non volatile memory and or storage elements at least one input device and one or more output devices. Program code may be applied to data entered using an input device to perform any of the processes described herein and to generate output information.

The system may be implemented at least in part via a computer program product e.g. in a non transitory machine readable storage medium such as for example a non transitory computer readable medium for execution by or to control the operation of data processing apparatus e.g. a programmable processor a computer or multiple computers . Each such program may be implemented in a high level procedural or object oriented programming language to communicate with a computer system. However the programs may be implemented in assembly or machine language. The language may be a compiled or an interpreted language and it may be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program may be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network. A computer program may be stored on a non transitory machine readable medium that is readable by a general or special purpose programmable computer for configuring and operating the computer when the non transitory machine readable medium is read by the computer to perform the processes described herein. For example the processes described herein may also be implemented as a non transitory machine readable storage medium configured with a computer program where upon execution instructions in the computer program cause the computer to operate in accordance with the processes. A non transitory machine readable medium may include but is not limited to a hard drive compact disc flash memory non volatile memory volatile memory magnetic diskette and so forth but does not include a transitory signal per se.

The processes described herein are not limited to the specific examples described. For example the processes to are not limited to the specific processing order of respectively. Rather any of the processing blocks of may be re ordered combined or removed performed in parallel or in serial as necessary to achieve the results set forth above.

The processing blocks for example in the processes to associated with implementing the system may be performed by one or more programmable processors executing one or more computer programs to perform the functions of the system. All or part of the system may be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array and or an ASIC application specific integrated circuit . All or part of the system may be implemented using electronic hardware circuitry that include electronic devices such as for example at least one of a processor a memory a programmable logic device or a logic gate.

Elements of different embodiments described herein may be combined to form other embodiments not specifically set forth above. Other embodiments not specifically described herein are also within the scope of the following claims.

