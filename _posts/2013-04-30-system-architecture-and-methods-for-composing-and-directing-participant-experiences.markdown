---

title: System architecture and methods for composing and directing participant experiences
abstract: The present invention contemplates a variety of improved methods and systems for providing an experience platform, as well as sentio or experience codecs, and experience agents for supporting the experience platform. The experience platform may be provided by a service provider to enable an experience provider to compose and direct a participant experience. The service provider monetizes the experience by charging the experience provider and/or the participants for services. The participant experience can involve one or more experience participants. The experience provider can create an experience with a variety of dimensions and features. As will be appreciated, the following description provides one paradigm for understanding the multi-dimensional experience available to the participants. There are many suitable ways of describing, characterizing and implementing the experience platform contemplated herein.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08903740&OS=08903740&RS=08903740
owner: Net Power and Light, Inc.
number: 08903740
owner_city: San Francisco
owner_country: US
publication_date: 20130430
---
The present application is a Continuation of U.S. patent application Ser. No. 13 136 869 entitled SYSTEM ARCHITECTURE AND METHODS FOR EXPERIENTIAL COMPUTING filed on Aug. 12 2011 and U.S. patent application Ser. No. 13 367 146 filed Feb. 6 2012 entitled SYSTEM ARCHITECTURE AND METHODS FOR COMPOSING AND DIRECTING PARTICIPANT EXPERIENCES which claims priority to U.S. Provisional Application No. 61 373 193 entitled SYSTEM ARCHITECTURE AND METHODS FOR COMPOSING AND DIRECTING PARTICIPANT EXPERIENCES filed on Aug. 12 2010 all of which are incorporated in their entirety herein by this reference.

The present invention contemplates a variety of improved methods and systems for providing a general purpose tool to enable a wide variety of applications referred to experiential computing.

Some of the attributes of experiential computing are 1 pervasive it assumes multi screen multi device multi sensor computing environments both personal and public this is in contrast to personal computing paradigm where computing is defined as one person interacting with one device such as a laptop or phone at any given time 2 the applications focus on invoking feelings and emotions as opposed to consuming and finding information or data processing 3 multiple dimensions of input and sensor data such as physicality 4 people connected together live synchronously multi person social real time interaction allowing multiple people interact with each other live using voice video gestures and other types of input.

The experience platform may be provided by a service provider to enable an experience provider to compose and direct a participant experience. The service provider monetizes the experience by charging the experience provider and or the participants for services. The participant experience can involve one or more experience participants. The experience provider can create an experience with a variety of dimensions and features. As will be appreciated the following description provides one paradigm for understanding the multi dimensional experience available to the participants. There are many suitable ways of describing characterizing and implementing the experience platform contemplated herein.

In general services are defined at an API layer of the experience platform. The services are categorized into dimensions. The dimension s can be recombined into layers. The layers form to make features in the experience.

By way of example the following are some of the dimensions that can be supported on the experience platform.

Video is the near or substantially real time streaming of the video portion of a video or film with near real time display and interaction.

Audio is the near or substantially real time streaming of the audio portion of a video film karaoke track song with near real time sound and interaction.

Live is the live display and or access to a live video film or audio stream in near real time that can be controlled by another experience dimension. A live display is not limited to single data stream.

Encore is the replaying of a live video film or audio content. This replaying can be the raw version as it was originally experienced or some type of augmented version that has been edited remixed etc.

Graphics is a display that contains graphic elements such as text illustration photos freehand geometry and the attributes size color location associated with these elements. Graphics can be created and controlled using the experience input output command dimension s see below .

Input Output Command s are the ability to control the video audio picture display sound or interactions with human or device based controls. Some examples of input output commands include physical gestures or movements voice sound recognition and keyboard or smart phone device input s .

Interaction is how devices and participants interchange and respond with each other and with the content user experience video graphics audio images etc. displayed in an experience. Interaction can include the defined behavior of an artifact or system and the responses provided to the user and or player.

Game Mechanics are rule based system s that facilitate and encourage players to explore the properties of an experience space and other participants through the use of feedback mechanisms. Some services on the experience Platform that could support the game mechanics dimensions include leader boards polling like dislike featured players star ratings bidding rewarding role playing problem solving etc.

Ensemble is the interaction of several separate but often related parts of video song picture story line players etc. that when woven together create a more engaging and immersive experience than if experienced in isolation.

Auto Tune is the near real time correction of pitch in vocal and or instrumental performances. Auto Tune is used to disguise off key inaccuracies and mistakes and allows singer players to hear back perfectly tuned vocal tracks without the need of singing in tune.

Auto Filter is the near real time augmentation of vocal and or instrumental performances. Types of augmentation could include speeding up or slowing down the playback increasing decreasing the volume or pitch or applying a celebrity style filter to an audio track like a Lady Gaga or Heavy Metal filter .

Remix is the near real time creation of an alternative version of a song track video image etc. made from an original version or multiple original versions of songs tracks videos images etc.

Viewing 360 Panning is the near real time viewing of the 360 horizontal movement of a streaming video feed on a fixed axis. Also the ability to for the player s to control and or display alternative video or camera feeds from any point designated on this fixed axis.

Layer Composition The first layer composition is described herein with reference to . These figures illustrate as an example a tablet and layers associated therewith. In embodiments layers may be composed using one or a combination of two different models a local device composition and cloud or other such remote layer composition. In some instances the layers may be composited in any way or order to result in a sequence of layers e.g. where the layers are arranged one on top of each other . In such examples layers may be composed using different levels of transparency scaling and such parameters may be reconfigured on the fly. In the case of a local setting as is the case in the layer may have a predefined schema that can be defined by the application but still not limited to such a definition. In some instances the layers or the predefined definitions can be dynamically rearranged. So the first layer is at the bottom as exemplarily illustrated in . The next layer rendered on top of it and may be for example mixed using a GPU of the local computing device or other capabilities of the device. As illustrated herein layer composition happens dynamically on the device. In the cloud layer composition or the remote layer composition as exemplarily illustrated in a similar construction happens on a remote site or device. Such a mechanism may be useful when a local device does not have complete capability to render one or more of the layers. This may be done dynamically when the local device s resources are unable to handle the composition or just be done remotely as preferred by a user. Any permutation of computations as may be envisioned by a person of ordinary skill in the art may also additionally be utilized.

Layer Mobility The next feature is the mobility and the ability of the layer to move across devices and for example be present simultaneously across different consumer devices. In embodiments such mobility allow for flare virtualization across the devices. Layer mobility is further discussed with reference to . In the illustrated embodiment three exemplary devices are shown device device and device . In an exemplary embodiment involving device and device a first layer generated on the first device. Consider a scenario where an experience is generated and functional and then a person associated with device initiates a gesture or other trigger or input method to cause an event to occur. In such a scenario device may talk to device or it may directly talk to a personal service. Personal and consideration service is part of the platform that holds all of the integration configuration data for specific people and knows exactly all the available information about how many devices does it have what networks do they have working on and all the parameters. Accordingly this can be identified dynamically so the personal service is constantly talking to each device and send out the routing mechanism.

Consider for instance that in the above example all three devices are located in the network. Once the device connected to the personal service personal service can send the device or each device this information so in local network they can communicate to each other and they can just simply establish direct links and initiate all this movement and other operations by conveying personal service. Another example is where the devices are located in the same geographical location but just connected to different WIFI or wired networks and there is no direct link. Here the devices would instead communicate through the personal service. Anyway regardless of this mechanism the end result will be the device initiates the signal both to destination device in this case device and the source device. It basically informs the device that they should prepare for layered movement. They those who communicate through the service wishes according service to this layer. As an additional example let s say we have some layer created in the cloud. In this case device number tells device number to accept the specific request which indicate a particular layer and device number will be ready for layer movement. This device can talk to each other to accept and to identify how layer movement is progressing. The devices may synchronize the animation between device and device how the layer disappears on the first and appears on the second device. And finally the layer on device moves to device number . The important part is both devices are connected to the service which is responsible for work under this layer so this communication this transmission can happen in either direction. As far as the layer is concerned the same mechanism can be applied from layer application it just simply communicates to according service in the cloud to send identical or device specific stream which contains this layer to another device. The layer movement feature is important because if an application is composed of multiple layers layers can be split so one layer can go on the tablet device and another layer can go on the TV device and they can adapt based on the devices and this allows for experiences that can be split across devices.

Layer Computation The next feature is the computation model of the layer where all the computation for this layer happens and the data exchange mechanism for this layer. Reference is made to which introduces an exemplary architecture and structure. The first layer in this example may be complete configured in the cloud this is the top left corner . In this case layer can be completely generated within the cloud. It can be a remote application generated on the cloud server and the game or application working completely there and fully computer generated and work in normal mode. Using remote delivery mechanisms the game or application may then be captured and delivered to the cloud. In other examples it is understood that the application can be generated locally using capabilities the devices has. In examples where the layer can be generated simultaneously part of this layer can be generated on the cloud and part of the layer can be generated locally on the device. Generated in the cloud may include any computational resource remotely or locally available. So let s say you have two devices in your physical location mobile phone and powerful computer and you are using mostly your phone and layer can be generate part of the layer specific to the phone can be generated on the phone like control button touch events and touch controls. But the other part can be generated let s say we consider map application and powerful machine can render 3D map and just have the information sent to the device. Cloud implies another computational unit regardless of its location and other parameters. The interesting part of this is a hybrid layer under part of the layer generated locally and part of layer generated remotely. In embodiments this can be region based which is simple model so part of the layer just local and part of the layer generate and delivered to the regions. It could be based on the mask so let s say we have generated two pictures and they can mix with different types of transparency and work very well even when there is a lot of movement. In this case a computational mask based approach may be utilized so one part of the layer generated locally and the other part in the cloud and are then mixed together in the device.

Data layers may be sent as data streams and event streams that can be for example transmitted through the cloud. In embodiments each layer transmits to a remote computing node all the data and communicates only to that node. In some examples each layer and further in some examples each instance of such layers may work exactly on the same model so as to offer a central point of communication that is publicly accessible through good bandwidths and other network capabilities. This exemplary embodiment offers a single point of routing. In other embodiments a peering network may be used where devices talk with each other for such communication offerings. In a third embodiment a possible approach is combination of the two previous approaches e.g. in a scenario where we have three persons located and each of the persons has three devices. Each device is located for example in the same wi fi network so the devices can communicate locally but when it comes to communication across the local network the cloud model may be adopted.

Layer Outputs now illustrates i o input and output for layers. So layers can accept inputs from other layers or from people and generic outputs so they can be re combined. The illustrated example describes input output cases. illustrates a multi device multi layer embodiment. Let s consider an exemplary embodiment where we have television and mobile phone and we have a game which is regularly on TV. Let s say it s a first shooter game. This game may be controlled using your mobile phone so you can rotate 3D pan in 360 degrees use a camera etc. When a gesture or any other type of input is provide for example let s say it initiates a movement in the game. And also you can see alternatively for you from your mobile phone. So let s say you can see the map of the game on the mobile phone while you see the main view on the big screen TV. This is a specific example but let s consider how it can be done through the Layer Model. We have game or specific layer executed somewhere either remotely or locally on the device on one device. And we have another device which talks and gives to the personal service or configuration service which gets this information routes and schemes and it spans this direct connection between two devices. The layers accept both from two devices so a user can play using the native device e.g. the mobile device or could also play using the TV assuming TV has input mechanisms such as a keyboard . The user can play from that keyboard ans also the other device since the input information to this layer depending on the layer of the routes and schemes. Accordingly in embodiments the information may be directly sent to the device or through the cloud depending on the layer computational model. This second device can send data to direct it to the local device where layer originated or directed to the cloud part which layer computation or to them both.

In this illustrated scenario the conveyed idea is that even if the game or application or layer wasn t designed for that such type of an input it can be meta schema implemented to be able to work with such type of input. So without any modification to the layer or application agent and with just a configuration file which basically establishes the map between the input and parameters of the unit to the application specific actions the required objectives are easily met. In another example we have a PC based game and it knows how to react to the mouse movement but then we add an accelerated method from the mobile phone and we can just simply map the rotation of the view of the main character in the game to accelerated motion of the phone. So when a user rotates the phone it will rotate the main view. This illustrates how this schema is important to extend input capabilities for initial application layers.

Turning back to the experience platform includes a plurality of devices and a data center . The devices may include devices such as an iPhone an android a set top box a desktop computer and a netbook . At least some of the devices may be located in proximity with each other and coupled via a wireless network. In certain embodiments a participant utilizes multiple devices to enjoy a heterogeneous experience such as using the iPhone to control operation of the other devices. Multiple participants may also share devices at one location or the devices may be distributed across various locations for different participants.

Each device has an experience agent . The experience agent includes a sentio codec and an API. The sentio codec and the API enable the experience agent to communicate with and request services of the components of the data center . The experience agent facilitates direct interaction between other local devices. Because of the multi dimensional aspect of the experience the sentio codec and API are required to fully enable the desired experience. However the functionality of the experience agent is typically tailored to the needs and capabilities of the specific device on which the experience agent is instantiated. In some embodiments services implementing experience dimensions are implemented in a distributed manner across the devices and the data center . In other embodiments the devices have a very thin experience agent with little functionality beyond a minimum API and sentio codec and the bulk of the services and thus composition and direction of the experience are implemented within the data center .

Data center includes an experience server a plurality of content servers and a service platform . As will be appreciated data center can be hosted in a distributed manner in the cloud and typically the elements of the data center are coupled via a low latency network. The experience server servers and service platform can be implemented on a single computer system or more likely distributed across a variety of computer systems and at various locations.

The experience server includes at least one experience agent an experience composition engine and an operating system . In one embodiment the experience composition engine is defined and controlled by the experience provider to compose and direct the experience for one or more participants utilizing devices . Direction and composition is accomplished in part by merging various content layers and other elements into dimensions generated from a variety of sources such as the service provider the devices the content servers and or the service platform .

The content servers may include a video server an ad server and a generic content server . Any content suitable for encoding by an experience agent can be included as an experience layer. These include well know forms such as video audio graphics and text. As described in more detail earlier and below other forms of content such as gestures emotions temperature proximity etc. are contemplated for encoding and inclusion in the experience via a sentio codec and are suitable for creating dimensions and features of the experience.

The service platform includes at least one experience agent a plurality of service engines third party service engines and a monetization engine . In some embodiments each service engine or has a unique corresponding experience agent. In other embodiments a single experience can support multiple service engines or . The service engines and the monetization engines can be instantiated on one server or can be distributed across multiple servers. The service engines correspond to engines generated by the service provider and can provide services such as audio remixing gesture recognition and other services referred to in the context of dimensions above etc. Third party service engines are services included in the service platform by other parties. The service platform may have the third party service engines instantiated directly therein or within the service platform these may correspond to proxies which in turn make calls to servers under control of the third parties.

Monetization of the service platform can be accomplished in a variety of manners. For example the monetization engine may determine how and when to charge the experience provider for use of the services as well as tracking for payment to third parties for use of services from the third party service engines .

The sentio codec is a combination of hardware and or software which enables encoding of many types of data streams for operations such as transmission and storage and decoding for operations such as playback and editing. These data streams can include standard data such as video and audio. Additionally the data can include graphics sensor data gesture data and emotion data. Sentio is Latin roughly corresponding to perception or to perceive with one s senses hence the nomenclature sensio codec. 

The sentio codec can be designed to take all aspects of the experience platform into consideration when executing the transfer protocol. The parameters and aspects include available network bandwidth transmission device characteristics and receiving device characteristics. Additionally the sentio codec can be implemented to be responsive to commands from an experience composition engine or other outside entity to determine how to prioritize data for transmission. In many applications because of human response audio is the most important component of an experience data stream. However a specific application may desire to emphasize video or gesture commands.

The sentio codec provides the capability of encoding data streams corresponding to many different senses or dimensions of an experience. For example a device may include a video camera capturing video images and audio from a participant. The user image and audio data may be encoded and transmitted directly or perhaps after some intermediate processing via the experience composition engine to the service platform where one or a combination of the service engines can analyze the data stream to make a determination about an emotion of the participant. This emotion can then be encoded by the sentio codec and transmitted to the experience composition engine which in turn can incorporate this into a dimension of the experience. Similarly a participant gesture can be captured as a data stream e.g. by a motion sensor or a camera on device and then transmitted to the service platform where the gesture can be interpreted and transmitted to the experience composition engine or directly back to one or more devices for incorporation into a dimension of the experience.

In addition to the above mentioned examples various other modifications and alterations of the invention may be made without departing from the invention. Accordingly the above disclosure is not to be considered as limiting and the appended claims are to be interpreted as encompassing the true spirit and the entire scope of the invention.

