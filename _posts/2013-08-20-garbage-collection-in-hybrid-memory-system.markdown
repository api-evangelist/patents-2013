---

title: Garbage collection in hybrid memory system
abstract: A hybrid memory system includes a primary memory and a secondary memory. A garbage collection operation is performed on the hybrid memory system. A read operation comprising reading data from a first cluster of a plurality of clusters is performed. Responsive to a determination that the read operation failed, the first cluster is unmapped without writing the data to a second cluster and the first cluster continues to be used for subsequent data storage. Responsive to a determination that the read operation did not fail, data is written to the second cluster.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09507719&OS=09507719&RS=09507719
owner: SEAGATE TECHNOLOGY LLC
number: 09507719
owner_city: Cupertino
owner_country: US
publication_date: 20130820
---
According to some embodiments a memory device includes a hybrid controller configured to manage data transfers between a host processor and a secondary memory. The secondary memory is configured to serve as a cache for a primary memory. The controller is configured to perform a read operation comprising reading data from a first cluster of a of a plurality of clusters of the secondary memory. Responsive to a determination that the read operation failed the controller is configured to unmap the first cluster without writing the data to a second cluster and continuing to use the first cluster for subsequent data storage. Responsive to a determination that the read operation did not fail the controller is configured to write the data to the second cluster.

Some embodiments involve a method of performing garbage collection in a hybrid memory system that includes a primary memory and a secondary memory. A read operation is performed the read operation comprising reading data from a first cluster of a plurality of clusters. Responsive to a determination that the read operation failed the first cluster is unmapped without writing the data to a second cluster and continuing to use the first cluster for subsequent data storage. Responsive to a determination that the read operation did not fail the data is written to the second cluster.

Some embodiments include a controller system for a hybrid memory system the controller comprising a hybrid controller configured data transfers between the host processor and a flash memory the flash memory configured to serve as a cache for a magnetic disk. The controller is configured to perform a read operation comprising reading data from a first cluster of a of a plurality of clusters of the secondary memory. Responsive to a determination that the read operation failed the controller is configured to unmap the first cluster without writing the data to a second cluster and continuing to use the first cluster for subsequent data storage. Responsive to a determination that the read operation did not fail the controller is configured to write the data to the second cluster.

These and other features and aspects of the various embodiments disclosed herein can be understood in view of the following detailed discussion and the accompanying drawings.

Some memory devices use at least two types of memory in a hybrid or tiered memory system where at least one type of memory is used as a primary memory and at least one other type of memory is used as a secondary memory that may operate as a cache. The primary memory may have greater storage capacity but slower access times than the secondary memory for example. In this arrangement the secondary memory can serve as a read cache and or a write cache for the primary memory. One example of such a tiered memory device is a hybrid drive in which the primary memory may comprise nonvolatile memory such as magnetic disk magnetic tape and or optical disk and the secondary memory may comprise solid state flash memory and or the secondary memory may be a nonvolatile or volatile memory with or without battery backup. Note that the terms primary memory main memory and secondary memory are used herein for identification of components used in a hybrid memory system and to denote differences in memory e.g. usage capacity performance memory class or type etc. and not necessarily order or preference. Furthermore although examples provided herein refer to the primary memory as magnetic disk and to secondary memory as flash memory the disclosed approaches are applicable to any types of primary and secondary memory.

Embodiments described herein relate to systems and methods for garbage collection in a hybrid memory system that includes a primary memory e.g. a magnetic disk and a secondary memory e.g. a nonvolatile flash. Some embodiments take into account that a hybrid system may continue to use a memory unit in the secondary memory if a read of the secondary memory fails. There may be less concern for losing the data in the secondary memory because a second copy of the data is stored on the primary memory i.e. the magnetic disk.

The host sends memory access requests to the hybrid drive to read or write data. The memory access requests may specify a host LBA range used for the operation of the memory access request. For example a memory access request from the host may request that a host LBA range be written to the hybrid drive and or a memory access request may request that a host LBA range be read from the hybrid drive . The memory access requests received from the host are managed by the hybrid controller to cause data to be written to and or read from the hybrid drive with optimal efficiency. The second cache in this example may optionally be a type of read cache referred to as read only in that only data marked for read operations by the host are placed in the second cache . In such a configuration data marked for writing to the main memory are sent to the main storage either directly or via the first cache . The controller controls data flow between the host interface and the various memories of the hybrid drive . The controller also includes modules for maintaining memory functionality including a garbage collection unit for coordinating garbage collection processes for the first cache second cache and main memory.

According to some embodiments the hybrid memory device also referred to herein as a hybrid drive may be implemented using a controller configured as a hierarchy of abstraction layers. Pairs of the abstraction layers are communicatively coupled through application programming interfaces APIs . The organization of the hybrid controller into abstraction layers to some extent allows each layer to work relatively independently and or can reduce potential conflicts that arise from processing multiple threads of execution. For purposes of discussion some examples provided below are based on the use of a magnetic disk as the main memory dynamic random access memory as the first or primary cache and solid state flash memory as the second or secondary cache. It will be apparent to those skilled in the art that the various memory components are not restricted to these types of memory and may be implemented using a wide variety of memory types.

In some configurations the cache may be configured as a secondary cache and may be faster and smaller than the main storage . The cache is a primary cache and may be faster and smaller than the secondary cache . For example current read write requests from the host may be processed first via the primary cache e.g. identified by the data s logical block address . This enables host commands to complete quickly should the requested data be stored in the primary cache . For host read requests if there is a miss in the primary cache i.e. the requested data is not present in the primary cache the requested data may be searched for in the secondary cache . If not found in either requested data may be accessed via the main storage .

Some of the data stored in the primary cache may either be copied or moved to the secondary cache as new requests come in. The copying movement from primary cache to secondary cache may also occur in response to other events e.g. a background scan. Both copying and moving involve placing a copy of data associated with an LBA range in the secondary cache and moving may further involve freeing up some the LBA range in the primary cache for other uses e.g. storing newly cached data.

The host processor communicates with the hybrid memory device also referred to herein as hybrid drive through a host interface . As previously discussed the main memory includes a memory space that corresponds to a number of memory sectors each sector addressable using a unique a logical block address LBA . The sectors of the main memory are directly accessible by the host using the LBAs and thus the corresponding LBAs of the main memory are referred to herein as host LBAs.

The host sends memory access requests to the hybrid memory device for example the host may request that data be written to and or read from the hybrid memory device. The host interface is configured to transfer memory access requests from the host to other components of the hybrid memory device and to transfer data between the host and the hybrid memory device.

The hybrid controller illustrated in includes number of layers wherein each layer communicates to its nearest neighboring layer s e.g. through a set of requests. For example each layer may only communicate to its nearest neighboring layer s without communicating to other layers. As an example the layer may only communicate directly to layer and the host interface without communicating directly with the layer or to the flash . As an operation such as a memory access request from the host is being carried out each layer is configured to pass control to the next lower layer as the operation is implemented.

The example illustrated in includes three layers which are described in terms applicable to the use of flash memory as a cache. It will be appreciated that these terms are not restrictive and if other types of memory were used as the secondary memory if desired different terminology could be used to reflect the type of secondary memory. Nevertheless the basic functions of the layers can be similar regardless of the type of memory used for primary and or secondary memory and or the terminology used to describe the layers.

The layers illustrated in include the flash cache interface FCI layer the flash cache control and transfer management FCTM layer and the programmable state machine PSM layer . Requests and or data may be passed between the layers as indicated by arrows from a higher layer to the next lower layer starting with the FCI layer and proceeding to the PSM layer which interacts directly with the flash memory . The layered architecture of the hybrid controller described herein allows for handling host memory access requests which can be serviced from either the magnetic memory or one of the caches The layered structure used in conjunction with the flash cache can be configured to achieve specified rates and response times for servicing memory access requests.

The FCI layer decides whether a host read request should be serviced from the primary magnetic memory or from one of the caches . The FCI layer implements processes to determine which data should be promoted to the flash secondary cache and or the primary cache based on various criteria to achieve optimal workload for the hybrid memory device. The flash content and transfer management FCTM layer maintains a mapping e.g. a fully associative mapping as discussed below of the host LBAs to a memory space corresponding to the flash memory space arranged in clusters. The FCTM layer interacts with programmable state machine PSM layer and performs tasks such as optimal scheduling of promotion requests among dies of the flash referred to as die scheduling wear leveling garbage collection and so forth. . The PSM layer programs hardware controllers to generate the required signals to read from and write to the flash for example.

In some cases one or more of the layers of the hybrid controller may be implemented by circuitry and or by one or more processors e.g. such as reduced instruction set computer RISC processors available from ARM. In some cases each layer may be implemented by a separate processor. The processes discussed herein are implementable in hardware interconnected electronic components that carry out logic operations and or by a processor implementing software instructions e.g. stored in a computer readable medium and or by any combination of hardware and software.

In some implementations the hybrid memory device includes a flash cache e.g. as a secondary cache that undergoes garbage collection to make room for incoming data. The hybrid controller is configured to carry out garbage collection operations for the flash cache .

As described previously the HDD host LBA space and flash physical address space are partitioned into units of clusters that may be sized to contain a whole number of sectors of data. Protection Information PI and error correction data may be stored in the cluster. The flash geometry determines the cluster size. In flash geometry according to some embodiments a cluster is defined to contain one or more map units MU . A MU can be defined to be the maximum amount of data that can be transferred to or from the flash in a single flash operation. For example the MU for a dual plane flash may be two pages and for quad plane may be four pages. illustrates a way to partition the flash into clusters for a four die dual plane system. Each cluster spans two planes and two MUs. There are two garbage collection units GCUs per die. Clusters and GCUs are intentionally defined not to span die in order to be able to handle full die failures.

Flash memory cells must be erased by applying a relatively high voltage to the cells before being written or programmed. For a number of reasons these erasures are often performed on blocks of data also referred to herein as erase units or erasure blocks . An erase unit may include any physical or logical blocks of memory that are treated as a single unit for purposes of erasure. In many implementations erase units are contiguous physical units that are larger than the data storage units e.g. pages that may be individually read or programmed. In such a case when data of an existing page needs to be changed it may be inefficient to erase and rewrite the entire block in which the page resides because other data within the block may not have changed. Instead it may be more efficient to write the changes to empty pages in a new physical location remap the logical to physical mapping for the altered blocks via the controller logic and mark the old physical locations as invalid stale.

After some time numerous data storage units within a memory unit may be marked as stale due to changes in data stored within the block. As a result it may make sense to move any valid data out of the block to a new location erase the block and thereby make the block freshly available for programming. This process of tracking invalid stale data units moving of valid data units from an old block to a new block and erasing the old block is sometimes collectively referred to as garbage collection. Garbage collection may be triggered by any number of events. For example metrics e.g. a count of stale units within a block may be examined at regular intervals and garbage collection may be performed for any blocks for which the metrics exceed some threshold. Garbage collection may also be triggered in response to other events such as read writes host requests current inactivity state device power up down explicit user request device initialization re initialization etc. In some cases garbage collection is triggered when the last outstanding user FCI command completes and there are no other outstanding commands for example

Garbage collection is performed on garbage collection units GCUs which generally refer to physical units that are garbage collected and erased as a contiguous unit. In some flash memory implementations the smallest size of a GCU is one erasure block. It may be possible in some implementations for a garbage collection unit to include multiple erasure blocks and other variations between GCU size and erase unit size are also possible. For the purposes of the following discussion the GCU may be of a predetermined size but need not have any direct correspondence to the size of erasure units upon which garbage collection is performed.

As described above in some storage systems that use flash based memory there is an erase before each program of a GCU. This erase process must first ensure that any valid data currently residing in that GCU is moved to another location. This process of moving the valid data and erasing the GCU may be part of a garbage collection operation. In order to find the optimal candidate for garbage collection garbage collection metrics are maintained for the GCUs. One often used garbage collection metric is a staleness count which reflects the number of stale logical block addresses LBAs residing in a particular GCU. In some cases a table is maintained to track the staleness of all GCUs. Additionally or alternatively a binary max heap may be created with staleness as the comparison function. A binary heap is a tree. The root node of the heap may be the GCU with the maximum staleness. The root node is selected as the candidate GCU. Other characteristics may also be considered when forming a garbage collection metric.

In some cases a GCU is made up of one or more clusters. For example each GCU may contain 128 or 256 clusters. As described above staleness of a GCU is defined as the number of clusters in a GCU that no longer contain valid data. Clusters may become invalid as a result of invalidations from the FCI layer or as a result of evictions. Evictions can be used to maintain a predetermined amount of over provisioning for example. Garbage collection of a GCU involves cluster migration and erase of the GCU.

As described previously a garbage collection process involves the movement of data from a first GCU that is undergoing garbage collection where garbage collected data is being collected from to a second GCU where the data from the first GCU is being collected where the garbage collected data is being collected to . This movement of data may involve a cluster migration process. illustrates a process for cluster migration. The process of migrating clusters for a GCU is accounted for using a cluster migration node. For each valid cluster in the GCU a read cluster operation is performed and the data is read into a holding region. To enable faster garbage collection operations multiple concurrent cluster migrations may be supported e.g. across multiple planes and or across multiple die of the cache. According to various implementations the holding region is as large as the maximum number of concurrent cluster migrations supported. For example in a four die system the maximum number of cluster migrations supported may be eight. The system checks if there are enough resources in the PSM layer to implement the read operation. If there are not enough resources the read operation is suspended until there are enough PSM resources to complete the read operation. A suspend list is used to track clusters waiting for PSM resources and to track clusters waiting for an unmap operation due to the cluster being in use. A separate bitmap may be maintained for cluster in use by a host read command. At the end of the host read if this list is non empty and the cluster in use has completed the unmap is performed. If it is determined that there are enough PSM resources for a read the system calls a function that indicates that the read is complete on the flash.

It is determined if the read of the flash was completed successfully. If the read fails the source cluster is unmapped and migration for this cluster ends. As described above read failures do not cause a block to be defected because there is another copy of the data on the primary memory. If the read passes a program command is issued for the read data to be written to a fresh cluster that is selected from the garbage collection ready list. There may be more than one set of garbage collection ready lists for different types of data. For example there may be one ready list set for user data and one ready list set for system data. Maintaining more than one ready list set may help to maintain a program order and or to control overprovisioning of the memory unit.

The system determines if there are enough resources at the PSM layer to complete the program operation. If there are not enough resources the program operation is suspended until there are enough resources to complete the program. If it is determined that there are enough resources to complete the program operation the program complete function is called . It is determined if the program operation passes or fails. If the program operation passes the source cluster is unmapped and the destination cluster is activated . If the source cluster is in use for an in progress flash cache hit it cannot be unmapped and is sent to the suspend list and has to wait for the user read operation to complete.

According to various implementations the FCTM layer includes an overlap checker configured to determine if there is an overlap between the memory access requests. An overlap may occur if there is an overlap in the host LBA ranges of two memory access requests. In some cases an overlap checker may check for read operations that are rounded out to a cluster boundary since it is possible for requests to non overlapping sectors within the same cluster to occur. Due to this the source cluster may not be immediately unmapped.

It is determined if the source cluster is valid i.e. contains valid data. If the source cluster is valid the cluster is unmapped and the cluster migration is complete . If the source cluster is not valid i.e. does not contain valid data the GCU is inserted into the staleness heap and the wear heap. The staleness heap and the wear heap are data structures that are used to select GCUs for garbage collection according to staleness and wear leveling criteria. If the program fails the GCU is deleted from the staleness heap and the wear heap the cluster is unmapped and the cluster migration is complete . The data migration process ends .

After all valid data is migrated out of the candidate GCU the GCU is erased and the clusters within the GCU are added to the ready list. If the GCU is defective has had a program error previously the clusters in the GCU are defected and moved to a defect list and the GCU is marked as being defective. If the erase operation fails all clusters are moved to the defect list and the GCU is marked defective. The decision as to which ready list user or system for example receives the erased clusters is based on the current length of each of the ready lists. A low water mark value and a high water mark value are defined for each of the ready lists. For example the high water mark value may be 1 GB and a low water marker value may be 512 MB. According to various embodiments the high water mark values and the low water mark values impact behavior such as overall system performance and may be tuned for different products and NAND flash parts from different vendors. If the system ready list is below the low water mark the system ready list will receive the erased clusters. Once the system ready list is above the low water mark the user ready list will receive the erased clusters. Garbage collection may continue until both lists are at least as long as their high water marks.

As described above a read error may not result in defecting of a block because a redundant copy of data is stored in the primary memory. In some cases a read error may be remedied in response to a block being programmed and used again. The chance of the read error being remedied depends on a number of factors such as the relaxation time the number of program erase cycles experienced by the memory unit temperature among others. Defecting a memory unit due to a read error may result in unnecessary loss of capacity particularly when the read error results from a transitory condition that can be remedied. It may be beneficial to be able to disable a defecting process to preserve capacity particularly when redundant data is available.

In response to the time difference being greater than the threshold a retention based defecting process is disabled for the memory unit. A larger time difference may indicate that the memory unit has a large relaxation time. A relaxation time is the time duration between two sequential program or erase events of the same memory unit. A larger relaxation time may indicate that the memory unit has less damage than other memory units. According to various aspects the time difference being greater than the threshold indicates that the chance for the memory unit to have a read failure is low.

In some implementations the threshold may be a function of more than one factor. For example the threshold may be a function of one or more of the retention time the number of program erase cycles experienced by the memory unit and or the temperature of the memory unit. If the threshold is a function of more than one factor a similar process may be used to the process shown in for each of the factors. Alternatively weighting coefficients can be applied to the factors the weighted factors can be combined and compared to a composite threshold. In some cases the process of determining whether to disable a defecting process if the time difference is greater than a threshold see blocks and may be checked periodically. Alternatively or additionally disabling the defecting process may be performed in response to a read error in the memory unit for example. In some cases the process of is carried out in conjunction with a garbage collection operation.

In the event that the time difference is less than the threshold the memory unit may be defected because the chance of a read error occurring is high for example. In some cases if the time difference is less than the threshold and a read error occurs a counter is incremented. If the value in the counter rises above a threshold e.g. 2 or 4 the memory unit may be defected during the next garbage collection process for example.

According to various implementations uses one or more lists to carry out the embodiments described herein. The lists can be used in the scheduling of various tasks and or for assigning a priority to tasks for example. illustrates another example of cluster migration in conjunction with a garbage collection process that utilizes various lists to aide in the garbage collection process. When the garbage collection process is initiated the system checks whether there is valid data in the candidate GCU from the Free List and whether the candidate GCU is defective. If there is no valid data in the candidate GCU and the candidate GCU is defective the system continues on to the next candidate GCU. If there is valid data in the candidate GCU and the GCU is not defective the GCU is moved to the Migration Issue List . Once all of the cluster migration requests for the GCU have been issued but not yet completed the GCU is moved to the Issue Complete List . The GCU is moved to the Migration Complete List once the cluster migration request is complete. If the GCU is determined to be defective the system returns to the Free List to process the next candidate GCU. Once the data migration is complete an erase is issued for the GCU and the GCU is placed in the Erase Issue List . If there are not currently enough resources for the erase the GCU is placed in the Erase Suspended List until the resources are available. If there are enough resources available for the erase the erase is completed and the GCU is placed in the Erase Complete List . Once the erase is complete for the GCU the system returns to the Free List to determine the next candidate GCU for garbage collection. In the event that there is no valid data and the GCU is not defective the cluster migration does not have to be completed and the GCU moves to the Erase Issue List .

The foregoing description of the example embodiments has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the inventive concepts to the precise form disclosed. Many modifications and variations are possible in light of the above teaching. Any or all features of the disclosed embodiments can be applied individually or in any combination are not meant to be limiting but purely illustrative. It is intended that the scope be limited not with this detailed description but rather determined by the claims appended hereto.

