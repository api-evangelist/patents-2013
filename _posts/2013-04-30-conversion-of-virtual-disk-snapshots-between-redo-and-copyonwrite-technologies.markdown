---

title: Conversion of virtual disk snapshots between redo and copy-on-write technologies
abstract: A framework for converting between copy-on-write (COW) and redo-based technologies is disclosed. To take a virtual disk snapshot, disk descriptor files, which include metadata information about data stored in virtual volumes (vvols), are “swizzled” such that the descriptor file for a latest redo log, to which IOs are currently performed, points to the base vvol of a COW-based vvol hierarchy. A disk descriptor file previously associated with the base vvol may also be updated to point to the vvol newly created by the snapshot operation. To revert to an earlier disk state, a snapshot may be taken before copying contents of a snapshot vvol of the COW-based vvol hierarchy to a base vvol of the hierarchy, thereby ensuring that the reversion can be rolled back if it is unsuccessful. Reference counting is performed to ensure that vvols in the vvol hierarchy are not orphaned in delete and revert use cases. Differences between vvols in the COW-based vvol hierarchy are used to clone the hierarchy and to migrate the hierarchy to a redo-based disk hierarchy.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09116737&OS=09116737&RS=09116737
owner: VMware, Inc.
number: 09116737
owner_city: Palo Alto
owner_country: US
publication_date: 20130430
---
As computer systems scale to enterprise levels particularly in the context of supporting large scale data centers the underlying data storage systems frequently employ a storage area network SAN or network attached storage NAS . As is conventionally well appreciated SAN or NAS provides a number of technical capabilities and operational benefits fundamentally including virtualization of data storage devices redundancy of physical devices with transparent fault tolerant fail over and fail safe controls geographically distributed and replicated storage and centralized oversight and storage configuration management decoupled from client centric computer systems management.

SCSI and other block protocol based storage devices such as a storage system shown in utilize a storage system manager which represents one or more programmed storage processors to aggregate the storage units or drives in the storage device and present them as one or more LUNs Logical Unit Numbers each with a uniquely identifiable number. LUNs are accessed by one or more computer systems through a physical host bus adapter HBA over a network e.g. Fibre Channel etc. . Within computer system and above HBA storage access abstractions are characteristically implemented through a series of software layers beginning with a low level device driver layer and ending in an operating system specific file system layers . Device driver layer which enables basic access to LUNs is typically specific to the communication protocol used by the storage system e.g. SCSI etc. . A data access layer may be implemented above device driver layer to support multipath consolidation of LUNs visible through HBA and other data access control and management functions. A logical volume manager typically implemented between data access layer and conventional operating system file system layers supports volume oriented virtualization and management of LUNs that are accessible through HBA . Multiple LUNs can be gathered and managed together as a volume under the control of logical volume manager for presentation to and use by file system layers as a logical device.

It has been recognized that the storage systems described above are not sufficiently scalable to meet the particular needs of virtualized computer systems. For example a cluster of server machines may service as many as 10 000 virtual machines VMs each VM using a multiple number of virtual disks and a multiple number of snapshots each of which may be stored for example as a file on a particular LUN or FS volume. Even at a scaled down estimation of 2 virtual disks and 2 snapshots per VM this amounts to 60 000 distinct disks for the storage system to support if VMs were directly connected to physical disks i.e. 1 virtual disk or snapshot per physical disk . In addition storage device and topology management at this scale are known to be difficult. As a result the concept of datastores in which VMs are multiplexed onto a smaller set of physical storage entities e.g. LUN based VMFS clustered file systems or FS volumes such as described in U.S. Pat. No. 7 849 098 entitled Providing Multiple Concurrent Access to a File System incorporated by reference herein was developed.

In conventional storage systems employing LUNs or FS volumes workloads from multiple VMs are typically serviced by a single LUN or a single FS volume. As a result resource demands from one VM workload will affect the service levels provided to another VM workload on the same LUN or FS volume. Efficiency measures for storage such as latency and input output operations per second or IOPS thus vary depending on the number of workloads in a given LUN or FS volume and cannot be guaranteed. Consequently storage policies for storage systems employing LUNs or FS volumes cannot be executed on a per VM basis and service level agreement SLA guarantees cannot be given on a per VM basis. In addition data services provided by storage system vendors such as snapshot replication encryption and deduplication are provided at a granularity of the LUNs or FS volumes not at the granularity of a VM s virtual disk. As a result snapshots can be created for the entire LUN or the entire FS volume using the data services provided by storage system vendors but a snapshot for a single virtual disk of a VM cannot be created separately from the LUN or the file system in which the virtual disk is stored.

An object based storage system disclosed in U.S. patent application Ser. No. 13 219 358 filed Aug. 26 2011 incorporated by reference herein provides a solution by exporting logical storage volumes that are provisioned as storage objects referred to herein as virtual volumes. These storage objects are accessed on demand by connected computer systems using standard protocols such as SCSI and NFS through logical endpoints for the protocol traffic that are configured in the storage system. Logical storage volumes are created from one or more logical storage containers having an address space that maps to storage locations of the physical data storage units. The reliance on logical storage containers provide users of the object based storage system with flexibility in designing their storage solutions because a single logical storage container may span more than one physical storage system and logical storage containers of different customers can be provisioned from the same physical storage system with appropriate security settings. In addition storage operations such as snapshots cloning etc. of the virtual disks may be offloaded to the storage system using the logical storage volumes.

Storage systems typically employ a copy on write COW approach to take disk snapshots. Under such an approach IOs are performed to a base disk while snapshots are maintained as mainly read only point in time copies of the disk to which contents of base disk may be copied on writes to preserve the state of the disk at the previous times. In contrast the connected computer system may employ redo based disk snapshotting. When a redo based snapshot is taken a redo log is created that points to an immediately prior redo log which itself points to another redo log and so on until a base disk is reached. The chain of redo logs tracks differences between the base disk state and later disk states. As COW and redo based approaches are semantic opposites redo based snapshots are typically not supported by COW based storage systems i.e. redo based snapshots cannot be readily offloaded to such systems. Further the storage pools for snapshots in the COW based storage system may be optimized for read only snapshots and therefore not optimal for writable redo logs.

One or more embodiments disclosed herein provide techniques for taking a snapshot of a virtual disk. The techniques include issuing a command to a storage system to take a first COW snapshot of the virtual disk. After taking the first COW snapshot the virtual disk comprises a logical storage volume hierarchy including at least a base logical storage volume and a snapshot logical storage volume. The techniques further include changing a first disk descriptor file associated with the snapshot logical storage volume and a second disk descriptor file associated with the base logical storage volume to be associated instead with the base logical storage volume and the snapshot logical storage volume respectively.

Further embodiments of the present invention include a computer readable storage medium storing instructions that when executed by a computer system cause the computer system to perform one or more the of the techniques set forth above and a computer system programmed to carry out one or more of the techniques set forth above.

Embodiments presented herein provide a framework for converting between copy on write COW and redo based technologies. To take a virtual disk snapshot disk descriptor files which include metadata information about data stored in virtual volumes vvols are swizzled such that the descriptor file for a latest redo log to which IOs are currently performed points to the base vvol of a COW based vvol hierarchy. This may be accomplished e.g. by updating a metadata vvol ID value to specify the base vvol. A disk descriptor file previously associated with the base vvol may also be updated to point to the vvol newly created by the snapshot operation. As a result IOs to the latest redo log in the redo based computer system are converted to IOs to the base vvol of the COW based hierarchy. In one embodiment in which a virtual machine VM is associated with the virtual disk and is stunned during snapshotting the snapshot operation may be split into two phases an asynchronous preparation phase in which the storage system creates a COW snapshot vvol and continues to allow IOs to the current running point and a synchronous snapshot phase in which the VM is stunned while the COW vvol hierarchy is associated with appropriate disk descriptor files via the swizzling operation described above. Doing so reduces the time needed for a snapshot which may be required to be short to avoid guest IOs failing as a result of stunning the VM for too long.

To revert to an earlier disk state a snapshot may be taken before copying contents of a snapshot vvol of the COW based vvol hierarchy to a base vvol of the hierarchy. Doing so ensures that the reversion can be rolled back if it is unsuccessful. For example in trying to revert a VM associated with two virtual disks to an earlier state the reversion of one of those disks may fail leaving the VM in an inconsistent state. In such a case the revert operation may be rolled back using the snapshot taken beforehand thereby returning the virtual machine to a consistent state.

Reference counting is performed to ensure that vvols in the vvol hierarchy are not orphaned in delete and revert use cases. In such cases the disk descriptor file associated with a base vvol may be deleted but the base vvol object itself not deleted if any snapshot vvols depend on that base vvol. As a result the snapshot vvols do not become orphaned. A reference count is maintained so that the base vvol is deleted if no snapshot vvols point to it any longer.

Differences between vvols in the COW based vvol hierarchy are used to clone the hierarchy and to migrate the hierarchy to a redo based disk hierarchy and vice versa. The migration may try to recreate the logical hierarchy of the COW based vvol hierarchy on a piecewise basis. In one embodiment two vvols may be compared based on COW information stored for a vvol hierarchy to determine differences between the vvols. Alternatively the actual data of the two vvols may be compared.

Distributed storage system manager or a single storage system manager or may create vvols e.g. upon request of a computer system etc. from logical storage containers which represent a logical aggregation of physical DSUs. In general a storage container may span more than one storage system and many storage containers may be created by a single storage system manager or a distributed storage system manager. Similarly a single storage system may contain many storage containers. In storage container created by distributed storage system manager is shown as spanning storage system and storage system whereas storage container and storage container are shown as being contained within a single storage system i.e. storage system and storage system respectively . It should be recognized that because a storage container can span more than one storage system a storage system administrator can provision to its customers a storage capacity that exceeds the storage capacity of any one storage system. It should be further recognized that because multiple storage containers can be created within a single storage system the storage system administrator can provision storage to multiple customers using a single storage system.

In the embodiment of each vvol is provisioned from a block based storage system. In the embodiment of a NAS based storage system implements a file system on top of DSUs and each vvol is exposed to computer systems as a file object within this file system. In addition as will be described in further detail below applications running on computer systems access vvols for IO through PEs. For example as illustrated in dashed lines in vvol and vvol are accessible via PE vvol and vvol are accessible via PE vvol is accessible via PE and PE and vvol is accessible via PE . It should be recognized that vvols from multiple storage containers such as vvol in storage container and vvol in storage container may be accessible via a single PE such as PE at any given time. It should further be recognized that PEs such as PE may exist in the absence of any vvols that are accessible via them.

In the embodiment of storage systems implement PEs as a special type of LUN using known methods for setting up LUNs. As with LUNs a storage system provides each PE a unique identifier known as a WWN World Wide Name . In one embodiment when creating the PEs storage system does not specify a size for the special LUN because the PEs described herein are not actual data containers. In one such embodiment storage system may assign a zero value or a very small value as the size of a PE related LUN such that administrators can quickly identify PEs when requesting that a storage system provide a list of LUNs e.g. traditional data LUNs and PE related LUNs as further discussed below. Similarly storage system may assign a LUN number greater than 255 as the identifying number for the LUN to the PEs to indicate in a human friendly way that they are not data LUNs. As another way to distinguish between the PEs and LUNs a PE bit may be added to the Extended Inquiry Data VPD page page . The PE bit is set to 1 when a LUN is a PE and to 0 when it is a regular data LUN. Computer systems may discover the PEs via the in band path by issuing a SCSI command REPORT LUNS and determine whether they are PEs according to embodiments described herein or conventional data LUNs by examining the indicated PE bit. Computer systems may optionally inspect the LUN size and LUN number properties to further confirm whether the LUN is a PE or a conventional LUN. It should be recognized that any one of the techniques described above may be used to distinguish a PE related LUN from a regular data LUN. In one embodiment the PE bit technique is the only technique that is used to distinguish a PE related LUN from a regular data LUN.

In the embodiment of the PEs are created in storage systems using known methods for setting up mount points to FS volumes. Each PE that is created in the embodiment of is identified uniquely by an IP address and file system path also conventionally referred together as a mount point. However unlike conventional mount points the PEs are not associated with FS volumes. In addition unlike the PEs of the PEs of are not discoverable by computer systems via the in band path unless virtual volumes are bound to a given PE. Therefore the PEs of are reported by the storage system via the out of band path.

In the example of distributed storage system manager has created three storage containers SC SC and SC from DSUs each of which is shown to have spindle extents labeled P through Pn. In general each storage container has a fixed physical size and is associated with specific extents of DSUs. In the example shown in distributed storage system manager has access to a container database that stores for each storage container its container ID physical layout information and some metadata. Container database is managed and updated by a container manager which in one embodiment is a component of distributed storage system manager . The container ID is a universally unique identifier that is given to the storage container when the storage container is created. Physical layout information consists of the spindle extents of DSUs that are associated with the given storage container and stored as an ordered list of . The metadata section may contain some common and some storage system vendor specific metadata. For example the metadata section may contain the IDs of computer systems or applications or users that are permitted to access the storage container. As another example the metadata section contains an allocation bitmap to denote which extents of the storage container are already allocated to existing vvols and which ones are free. In one embodiment a storage system administrator may create separate storage containers for different business units so that vvols of different business units are not provisioned from the same storage container. Other policies for segregating vvols may be applied. For example a storage system administrator may adopt a policy that vvols of different customers of a cloud service are to be provisioned from different storage containers. Also vvols may be grouped and provisioned from storage containers according to their required service levels. In addition a storage system administrator may create delete and otherwise manage storage containers such as defining the number of storage containers that can be created and setting the maximum physical size that can be set per storage container.

Also in the example of distributed storage system manager has provisioned on behalf of requesting computer systems multiple vvols each from a different storage container. In general vvols may have a fixed physical size or may be thinly provisioned such that storage space is dynamically allocated to the vvols when such storage space is actually needed and not necessarily when the vvol is initial created. Further each vvol may have a vvol ID which is a universally unique identifier that is given to the vvol when the vvol is created. For each vvol a vvol database stores for each vvol its vvol ID the container ID of the storage container in which the vvol is created and an ordered list of values within that storage container that comprise the address space of the vvol. Vvol database is managed and updated by volume manager which may be a component of distributed storage system manager . In one embodiment vvol database also stores metadata about the vvol e.g. as a set of key value pairs. Computer systems may update and query such metadata via the out of band path at any time during the vvol s existence. The key value pairs fall into different general categories. For example a first category may include well known keys the definition of certain keys and hence the interpretation of their values are publicly available. One example is a key that corresponds to the virtual volume type e.g. in virtual machine embodiments whether the vvol contains a VM s metadata or a VM s data . Another example is the App ID which is the ID of the application that stored data in the vvol. A second category includes computer system specific keys the computer system or its management module stores certain keys and values as the virtual volume s metadata. A third category includes storage system vendor specific keys these allow the storage system vendor to store certain keys associated with the virtual volume s metadata. One reason for a storage system vendor to use this key value store for its metadata is that all of these keys are readily available to storage system vendor plug ins and other extensions via the out of band channel for vvols. The store operations for key value pairs are part of virtual volume creation and other processes and thus the store operation should be reasonably fast. Storage systems are also configured to enable searches of virtual volumes based on exact matches to values provided on specific keys.

IO manager is a software module also in certain embodiments a component of distributed storage system manager that maintains a connection database that stores currently valid IO connection paths between PEs and vvols. In the example shown in seven currently valid IO sessions are shown. Each valid session has an associated PE ID secondary level identifier SLLID vvol ID and reference count RefCnt indicating the number of different applications that are performing IO through this O session. The process of establishing a valid IO session between a PE and a vvol by distributed storage system manager e.g. on request by a computer system is referred to herein as a bind process. For each bind distributed storage system manager e.g. via IO manager adds an entry to connection database . The process of subsequently tearing down the IO session by distributed storage system manager is referred to herein as an unbind process. For each unbind distributed storage system manager e.g. via IO manager decrements the reference count of the IO session by one. When the reference count of an IO session is at zero distributed storage system manager e.g. via IO manager may delete the entry for that IO connection path from connection database . As previously discussed in one embodiment computer systems generate and transmit bind and unbind requests via the out of band path to distributed storage system manager . Alternatively computer systems may generate and transmit unbind requests via an in band path by overloading existing error paths. In one embodiment the generation number is changed to a monotonically increasing number or a randomly generated number when the reference count changes from 0 to 1 or vice versa. In another embodiment the generation number is a randomly generated number and the RefCnt column is eliminated from connection database and for each bind even when the bind request is to a vvol that is already bound distributed storage system manager e.g. via IO manager adds an entry to connection database .

In the storage system cluster of IO manager processes IO requests IOs from computer systems received through the PEs using connection database . When an IO is received at one of the PEs IO manager parses the IO to identify the PE ID and the SLLID contained in the IO in order to determine a vvol for which the IO was intended. By accessing connection database IO manager is then able to retrieve the vvol ID associated with the parsed PE ID and SLLID. In and subsequent figures PE ID is shown as PE A PE B etc. for simplicity. In one embodiment the actual PE IDs are the WWNs of the PEs. In addition SLLID is shown as S S etc. The actual SLLIDs are generated by distributed storage system manager as any unique number among SLLIDs associated with a given PE ID in connection database . The mapping between the logical address space of the virtual volume having the vvol ID and the physical locations of DSUs is carried out by volume manager using vvol database and by container manager using container database . Once the physical locations of DSUs have been obtained data access layer in one embodiment also a component of distributed storage system manager performs IO on these physical locations.

In the storage system cluster of IOs are received through the PEs and each such IO includes an NFS handle or similar file system handle to which the IO has been issued. In one embodiment connection database for such a system contains the IP address of the NFS interface of the storage system as the PE ID and the file system path as the SLLID. The SLLIDs are generated based on the location of the vvol in the file system . The mapping between the logical address space of the vvol and the physical locations of DSUs is carried out by volume manager using vvol database and by container manager using container database . Once the physical locations of DSUs have been obtained data access layer performs IO on these physical locations. It should be recognized that for a storage system of container database may contain an ordered list of file entries in the Container Locations entry for a given vvol i.e. a vvol can be comprised of multiple file segments that are stored in the file system .

In one embodiment connection database is maintained in volatile memory while vvol database and container database are maintained in persistent storage such as DSUs . In other embodiments databases may be maintained in persistent storage.

Storage capability profiles e.g. SLAs or quality of service QoS may be configured by distributed storage system manager e.g. on behalf of requesting computer systems on a per vvol basis. Therefore vvols with different storage capability profiles may be part of the same storage container. In one embodiment a system administrator defines a default storage capability profile or a number of possible storage capability profiles for newly created vvols at the time of creation of the storage container and stored in the metadata section of container database . If a storage capability profile is not explicitly specified for a new vvol being created inside a storage container the new vvol inherits the default storage capability profile associated with the storage container.

Each application has one or more vvols associated therewith and issues IOs to block device instances of the vvols created by operating system e.g. pursuant to CREATE DEVICE calls by application to operating system . The association between block device names and vvol IDs are maintained in block device database . File system driver converts IOs received from applications to block IOs and provides the block IOs to a virtual volume device driver . In contrast IO operations from application bypass file system driver and are provided directly to virtual volume device driver . This signifies that application accesses its block device directly as a raw storage device e.g. as a database disk a log disk a backup archive and a content repository in the manner described in U.S. Pat. No. 7 155 558 entitled Providing Access to a Raw Data Storage Unit in a Computer System the entire contents of which are incorporated by reference herein. When virtual volume device driver receives a block IO it accesses block device database to reference a mapping between the block device name specified in the IO and the PE ID WWN of PE LUN and SLLID that define the IO connection path to the vvol associated with the block device name. In the example shown herein the block device name archive corresponds to a block device instance of vvol that was created for application and the block device names foo dbase and log correspond to block device instances of vvol vvol and vvol respectively that were created for one or more of applications . Other information that is stored in block device database includes an active bit value for each block device that indicates whether or not the block device is active and a CIF commands in flight value. An active bit of 1 signifies that IOs can be issued to the block device. An active bit of 0 signifies that the block device is inactive and IOs cannot be issued to the block device. The CIF value provides an indication of how many IOs are in flight i.e. issued but not completed. In the example shown herein the block device foo is active and has some commands in flight. The block device archive is inactive and will not accept newer commands. However in the example it is waiting for 2 commands in flight to complete. The block device dbase is inactive with no outstanding commands. Finally the block device log is active but the application currently has no pending IOs to the device. Virtual volume device driver may choose to remove such devices from its database at any time.

In addition to performing the mapping described above virtual volume device driver issues raw block level IOs to data access layer . Data access layer includes device access layer which applies command queuing and scheduling policies to the raw block level IOs and device driver for HBA which formats the raw block level IOs in a protocol compliant format and sends them to HBA for forwarding to the PEs via an in band path. In the embodiment where SCSI protocol is used the vvol information is encoded in the SCSI LUN data field which is an 8 byte structure as specified in SAM 5 SCSI Architecture Model 5 . The PE ID is encoded in the first 2 bytes which is conventionally used for the LUN ID and the vvol information in particular the SLLID is encoded in the SCSI second level LUN ID utilizing a portion of the remaining 6 bytes.

As further shown in data access layer includes an error handling unit for handling IO errors that are received through the in band path from the storage system. In one embodiment the I O manager propagates IO errors received by error handling unit through. Examples of IO error classes include path errors between computer system and the PEs PE errors and vvol errors. The error handling unit classifies all detected errors into aforementioned classes. For example when a path error to a PE is encountered and another path to the PE exists data access layer transmits the IO along a different path to the PE. When the IO error is a PE error error handing unit updates block device database to indicate an error condition for each block device issuing IOs through the PE. When the IO error is a vvol error error handing unit updates block device database to indicate an error condition for each block device associated with the vvol. Error handing unit may also issue an alarm or system event so that further IOs to block devices having the error condition will be rejected.

In some embodiments each VM has one or more vvols associated therewith and issues IOs to block device instances of the vvols created by hypervisor pursuant to CREATE DEVICE calls by VM into hypervisor . The block device database maintains associations between block device names and vvol IDs. IOs from VMs are received by a SCSI virtualization layer which converts them into file IOs understood by a virtual machine file system VMFS driver . VMFS driver then converts the file IOs to block IOs and provides the block IOs to virtual volume device driver . IOs from VM on the other hand are shown to bypass VMFS driver and provided directly to virtual volume device driver signifying that VM accesses its block device directly as a raw storage device e.g. as a database disk a log disk a backup archive and a content repository in the manner described in U.S. Pat. No. 7 155 558.

When virtual volume device driver receives a block IO it accesses block device database to reference a mapping between the block device name specified in the IO and the PE ID and SLLID that define the IO session to the vvol associated with the block device name. In the example shown herein the block device names dbase and log corresponds to block device instances of vvol and vvol respectively that were created for VM and the block device names vmdk vmdkn and snapn correspond to block device instances of vvol vvol and vvol respectively that were created for one or more of VMs . Other information that is stored in block device database includes an active bit value for each block device that indicates whether or not the block device is active and a CIF commands in flight value. An active bit of 1 signifies that IOs can be issued to the block device. An active bit of 0 signifies that the block device is inactive and IOs cannot be issued to the block device. The CIF value provides an indication of how many IOs are in flight i.e. issued but not completed.

In addition to performing the mapping described above virtual volume device driver issues raw block level IOs to data access layer . Data access layer includes device access layer which applies command queuing and scheduling policies to the raw block level IOs and device driver for HBA which formats the raw block level IOs in a protocol compliant format and sends them to HBA for forwarding to the PEs via an in band path. In the embodiment where SCSI protocol is used the vvol information is encoded in the SCSI LUN data field which is an 8 byte structure as specified in SAM 5 SCSI Architecture Model 5 . The first 2 bytes which are conventionally used for the LUN ID encode the PE and the SCSI second level LUN ID encode the vvol information such as SLLID utilizing a portion of the remaining 6 bytes. As further shown in data access layer includes an error handling unit which functions in the same manner as error handling unit .

One of skill in the art will recognize that the various terms layers and categorizations used to describe the components in may be referred to differently without departing from their functionality or the spirit or scope of the invention. For example VMM may be considered separate virtualization components between VM and hypervisor which in such a conception may itself be considered a virtualization kernel component since there exists a separate VMM for each instantiated VM. Alternatively each VMM may be considered to be a component of its corresponding virtual machine since such VMM includes the hardware emulation components for the virtual machine. In such an alternative conception for example the conceptual layer described as virtual hardware platform may be merged with and into VMM such that virtual host bus adapter is removed from i.e. since its functionality is effectuated by host bus adapter emulator .

Each management interface communicates with plug ins . To issue and handle management commands application programming interfaces APIs may be provided. It should be recognized that in one embodiment both plug ins are customized to communicate with storage hardware from a particular storage system vendor. Therefore management server and computer systems will employ different plug ins when communicating with storage hardware for different storage system vendors. In another embodiment a single plug in may interact with a given vendor s management interface. This requires the storage system manager to be programmed to a well known interface e.g. by virtue of being published by the computer system and or the management server .

Management server is further configured with a system manager for managing the computer systems. In one embodiment the computer systems are executing virtual machines and system manager manages the virtual machines running in the computer systems. One example of system manager that manages virtual machines is the vSphere product distributed by VMware Inc. As shown system manager communicates with a host daemon hostd running in computer system through appropriate hardware interfaces at both management server and computer system to receive resource usage reports from computer system and to initiate various management operations on applications running in computer system .

As shown in panel B a copy on write COW based hierarchy includes base disk and snapshot disks . Disks such as the base disk and the snapshot disks may be stored as vvols discussed above. Under the COW approach IOs are performed to base disk i.e. the base disk is the current running point while snapshots are maintained as mainly read only point in time copies. Snapshots are only written to when contents of base disk are copied to snapshots before being changed thereby preserving the state of the virtual disk at previous times in snapshots . This approach is different from the redo based approach discussed above so redo based snapshots cannot be offloaded to COW based storage systems.

Note for linked clone VMs in particular redo and COW based semantics may be bridged via fast cloning techniques such as that provided by Citrix Inc. As used herein a linked clone is a duplicate of a parent VM that uses the same base disk as the parent VM with a chain of redo logs to track the differences between the parent VM and the linked clone VM. Linked cloning permits space savings as blocks are shared by multiple VMs. In order to achieve linked clone functionality in COW based storage devices fast cloning technologies may be used in which blocks are shared by reference counting the blocks and allocating only different blocks.

During the preparation phase the hypervisor or system manager which is to be understood as referring to one or more components of the hypervisor and or system manager receives a request to take a snapshot of a particular VM resolves the vvols associated with the VM according to techniques discussed above and transmits a PREP message notifying a storage system to prepare for the snapshot of the resolved vvols. In response to the PREP notification the distributed storage system manager asynchronously performs pre processing and transmits update message s back to the hypervisor about progress of the same. In one embodiment the pre processing may include taking a snapshot which itself includes creating a COW snapshot vvol while continuing to allow guest OS IOs to the current running point i.e. the vvol to which IOs are being performed.

Once pre processing is complete the distributed storage system manager transmits a READY message notifying the hypervisor or system manager that the storage system is ready to receive a snapshot command. After receiving such a notification the hypervisor or system manager stuns the VM so that the snapshot operation may be performed synchronously. The hypervisor or system manager then creates a disk descriptor file for the snapshot vvol which may include metadata information about the actual extents stored in the vvol such as the location of the vvol the vvol ID etc. Note a virtual disk may generally comprise a disk descriptor file and a vvol object. After creating the disk descriptor file the hypervisor or system manager issues a CREATE snapshot call in response to which the distributed storage system manager may transmit an ACK message acknowledging the CREATE call with successful status. In one embodiment distributed storage system manager may return a temporary vvol ID that represents the snapshot in response to the PREP call. In such a case the CREATE call may be passed this temporary ID and distributed storage system manager may then finalize the snapshot it set out to create when the PREP call was made. The finalized snapshot vvol ID may or may not be the same as the temporary one.

After receiving the acknowledgement the hypervisor or system manager swizzles the newly created disk descriptor file for the snapshot vvol and a disk descriptor file for the base vvol. As discussed the redo based semantics of the hypervisor or system manager and the COW based semantics of the system manager may differ. As a result the newly created disk descriptor file maintained by the hypervisor that points to the newly created snapshot vvol may be the redo log disk descriptor file. This association is incorrect as both reads and writes are performed to the redo log whereas the snapshot vvol is mainly read only and may be stored in a storage pool optimized for read only disks and therefore not optimal for writable redo logs. At the same time the read write base vvol may still be pointed to by a previous disk descriptor associated with a read only base disk or redo log under the hypervisor or system manager s redo based semantics. To correct this association as well as the association of the newly created disk descriptor file with the newly created disk snapshot vvol the disk descriptor files may be swizzled. 

As shown in panel B a snapshot is taken at time which for distributed storage system manager includes creating a snapshot vvol . Snapshot vvol is read only and contents of base disk vvol are copied to snapshot vvol before being changed. As part of the snapshot process the hypervisor or system manager which use redo based semantics updates disk descriptor files pointing to base disk vvol and delta dist vvol such that the disk descriptor file for the redo base disk includes a pointer to the COW snapshot vvol and the disk descriptor file for the latest redo log points to the base vvol . This operation is referred to herein as swizzling the disk descriptor files and may be by e.g. updating metadata vvol ID values in the disk descriptor files to specify the appropriate vvols. At time when the VM writes data to the redo log the data is written to the base vvol as opposed to the snapshot vvol. Illustratively P is written to vvol and a copy of C is made to the snapshot vvol.

As shown in panel C a VM is associated with COW disk hierarchies comprising base disk and snapshots and base disk and snapshots . Panel D shows a reversion to the state of the VM before a snapshot was taken which created snapshots . In COW based systems content of a snapshot which maintains the snapshot state are copied to the base disk in a reversion operation. As shown in panel D content of snapshots are copied to base disks respectively. However for a VM associated with multiple virtual disks such as VM some of those disks may be reverted to prior states while other disk reversions fail leaving the VM in an inconsistent state. To prevent this problem a snapshot of the base vvol may first be taken. Then content of the base vvol may be reverted with snapshot vvol s . If this reversion is unsuccessful content of the base vvol may be then be reverted using the snapshot of the base vvol returning the virtual disk to its state before the unsuccessful reversion operation.

Panel C illustrates an approach to prevent snapshot from becoming orphaned. As shown a disk descriptor file here the .vmdk file is deleted but the base vvol object continues to be stored in the storage system. Thereafter a reversion to the disk state maintained via snapshot may be accomplished by creating a new disk descriptor file and configuring this disk descriptor file to point to the stored base vvol object. Then the contents of snapshot may be copied to the restored base vvol object and the VM made to point to the base vvol object as illustrated in panel D. Note a reference count may be maintained of the number of children that the base vvol has and the based vvol base vvol may be deleted if the reference count decrements to 0 indicating that no snapshot vvols depend on the base vvol anymore.

Panel C shows a migration to a redo based datastore. This migration may try to recreate the logical hierarchy of the COW based vvol hierarchy on a piecewise basis. As shown the VM is migrated by first full cloning the earliest snapshot vvol thereby creating the base disk of the redo hierarchy. Then the hypervisor takes a redo snapshot to create redo log determines a difference between the earliest snapshot vvol and the next snapshot vvol and applies this difference i.e. diff vvol vvol to the redo log . This process of taking a redo snapshot then applying a difference between a snapshot and its parent may be repeated if the vvol hierarchy includes additional snapshot vvols. In addition the hypervisor takes a further redo snapshot creating redo log and applies the difference between the base vvol and the latest snapshot vvol i.e. diff vvol vvol to the redo log . As a result a redo based disk hierarchy of the COW based vvol hierarchy depicted in panel A is created.

At step the distributed storage system manager performs pre processing in preparation for taking the snapshot and transmits message s back to the hypervisor or system manager about progress of the same. In one embodiment the pre processing may include taking a snapshot which itself includes creating a COW snapshot vvol while continuing to allow guest OS IOs to the current running point i.e. the base disk to which IOs are performed. The storage system may periodically transmit update messages to the hypervisor or system manager about progress of the snapshot.

At step distributed storage system manager transmits a message notifying the hypervisor that the storage system is ready to receive a snapshot command i.e. that pre processing is complete. Then the hypervisor stuns the VM at step . Stunning the VM may include e.g. stopping vCPUs of the VM and marking the VM as non executable.

At step hypervisor or system manager creates a new disk descriptor file. As discussed the disk descriptor file may include metadata information about data stored in vvols and virtual disks may generally comprise both a disk descriptor file and a vvol object. For snapshotting purposes the redo based hypervisor or system manager may thus create a new disk descriptor file for a redo log.

At step hypervisor or system manager issues a snapshot call. In response the distributed storage system manager transmits at step a message to the hypervisor acknowledging the create call or notifying the hypervisor that the snapshot was not created successfully. In one embodiment distributed storage system manager may return a temporary vvol ID that represents the snapshot in response to the preparation request of step . In such a case the snapshot call may be passed this temporary ID and distributed storage system manager may then finalize the snapshot it set out to create when the preparation request was made. The finalized snapshot vvol ID may or may not be the same as the temporary one. Note as a result of the pre processing performed at step the time between receiving the snapshot call from the hypervisor and the acknowledgment notification may be short.

At step hypervisor or system manager swizzles disk descriptor files. As discussed the hypervisor or system manager may use redo log semantics which differ from the distributed storage system manager s COW semantics. As a result the created disk descriptor file created at step for the redo log may point to the COW snapshot created at step while a read only redo log or base disk descriptor file in the redo based disk hierarchy may point to a base vvol in the COW vvol hierarchy. These associations are incorrect as the newly created redo log to which I Os are performed corresponds to the newly created COW base vvol to which I Os are performed while the read only base disk or redo log in the redo based disk hierarchy corresponds to the read only snapshot. To correct these associations the hypervisor may update the newly created disk descriptor file pointing to the newly created COW base vvol and the disk descriptor file currently pointing to the COW base vvol such that the newly created disk descriptor file points to the COW base vvol while the disk descriptor file currently pointing to the COW base vvol is changed to point to the newly created COW snapshot vvol. This operation is also referred to herein as swizzling the disk descriptor files and may be accomplished by e.g. updating metadata vvol ID values in the disk descriptor files to specify the appropriate vvols. After such swizzling hypervisor or system manager unstuns the VM returning the VM to normal execution at step .

For a VM associated with multiple virtual disks some of those disks may be reverted to prior states while other reversions fail leaving the VM in an inconsistent state. To prevent this problem hypervisor or system manager transmits at step a message to distributed storage system manager to take a snapshot of the base vvol in a COW based vvol hierarchy. Doing so creates a snapshot vvol that relies upon the base vvol. Then at step content of the base vvol is reverted with snapshot vvols in the hierarchy. In particular distributed storage system manager may copy the contents of the snapshot vvols necessary for the reversion to the base vvol.

At step it is determined whether the reversion was successful. For example distributed storage system manager may make such a determination. If the reversion was successful the method ends. If the reversion was unsuccessful then at step distributed storage system manager reverts the content of the base vvol with the snapshot vvol created at step . Such a reversion may guarantee that the VM is in a consistent state.

If the base vvol does not have any children then at step the hypervisor transmits a message to the distributed storage system manager to delete the base vvol. If the base vvol has children then at step the hypervisor deletes the disk descriptor file and maintains a reference count of the number of children. The base vvol object itself is not deleted as the snapshot vvols still depend on the base vvol and would thus become orphaned if the base vvol object were deleted. However the based vvol may be deleted if the reference count decrements to 0 indicating that no snapshot vvols depend on the base vvol anymore.

At step the hypervisor receives a revert request. At step the hypervisor creates a new disk descriptor file and configures the disk descriptor file to point to the stored base vvol object. Note the snapshot vvols dependent on the base vvol may themselves maintain references to the base vvol in metadata of associated disk descriptor files. At step contents of the snapshot vvol that maintains the snapshot state being reverted to are copied to the base vvol object thereby completing the revert operation.

At step a snapshot is taken of the new base vvol. That is a point in time copy of the new base vvol which is itself a full clone of the original first delta vvol is made. At step the hypervisor or system manager determines whether there are more vvols in the COW vvol hierarchy. Note this may generally exclude a last snapshot which as discussed in greater detail below may be compared to the base vvol.

If more vvols are in the COW vvol hierarchy then at step hypervisor or system manager determines a difference between a next snapshot vvol and the snapshot vvol prior in time to it and applies that difference to the new base vvol. For example the difference may be between snapshot vvols for a first and second snapshot in the vvol hierarchy. Upon applying such a difference to the new base vvol the contents of the new base vvol may then be identical to that of the second snapshot vvol in the original vvol hierarchy.

In one embodiment the hypervisor or system manager may invoke difference API exposed by the storage system to determine the difference between the snapshot vvols and in response the distributed storage system manager may return the difference as a bitmap indicating which extents of the first and second snapshot vvols differ. For example the distributed storage system manager may compare the first and second snapshot vvols based on COW information stored for the vvol hierarchy such as a COW bit in metadata that indicates whether a copy needs to be made before writing to an extent and thus whether a write has already been performed that may make the contents of one vvol differ from that of another. Of course the storage system may instead compare the actual data of the first and second snapshot vvols to determine differences between them.

At step another snapshot of the new base vvol is taken. Continuing the example above where a difference between snapshot vvols for a first and a second snapshot is determined the contents of the new base vvol at this point may be identical to that of the second snapshot vvol in the original vvol hierarchy. As a result a snapshot of the new base vvol which would be a second snapshot vvol in the new vvol hierarchy may be a copy of the second snapshot vvol in the original vvol hierarchy.

Returning to step if there no additional vvols are in the COW vvol hierarchy then the difference between the last snapshot vvol and the base vvol is applied to the base vvol in the new vvol hierarchy at step . Doing so re creates the original base vvol as the new base vvol. Once again hypervisor or system manager may invoke a difference API exposed by distributed storage system manager to determine the difference between vvols and in response distributed storage system manager may return the difference as a bitmap indicating which extents of the vvols differ. In one embodiment distributed storage system manager may make this comparison based on COW information stored for the vvol hierarchy. In an alternative embodiment the storage system may compare the actual data of the original base vvol and second snapshot vvol to determine differences between them.

At step a first snapshot is taken of the base disk in the redo vvol hierarchy. This may include creating a redo log file e.g. another .vmdk format file which has the base disk created at step as its parent. At step hypervisor or system manager determines whether there are more vvols in the COW vvol hierarchy. Note this may generally exclude a last snapshot which as discussed in greater detail below may be compared to the base vvol.

If more vvols are in the COW vvol hierarchy then at step hypervisor or system manager determines a difference between a next snapshot vvol in the COW vvol hierarchy and its parent vvol and applies the difference to a last redo log in the redo based disk hierarchy. For example hypervisor or system manager may determine the difference between first and second snapshot vvols in the vvol hierarchy and apply that difference to a first redo log. As a result content stored in the first redo log of the redo log hierarchy would be the difference between the base disk of the redo based disk hierarchy and the state of the virtual disk at a second later snapshot time. In one embodiment the hypervisor may invoke difference API exposed by the distributed storage system manager to determine the difference between vvols and in response the storage system may return the difference as a bitmap indicating which extents of the vvols differ. Here distributed storage system manager may compare the vvols based on COW information stored for the vvol hierarchy or make a comparison of actual data of the first and second snapshot vvols. Having obtained the difference bitmap hypervisor or system manager may then read those bits which are indicated as different and write those bits to the redo log disk file created at step .

At step another redo snapshot is taken. This may include creating an additional redo log file e.g. another .vmdk format file which has the redo log file created before it as its parent. The method then returns to step again.

At if there no additional vvols are in the COW vvol hierarchy then at step hypervisor or system manager determines a difference between the base vvol of the COW vvol hierarchy and a last snapshot vvol in the COW vvol hierarchy and applies that difference to the second redo snapshot created at step . As a result the contents of the last snapshot include the differences in the states of the virtual disk at the last snapshot time and the current time.

Although discussed above primarily with respect to virtual disks associated with VMs techniques discussed herein are not limited thereto and may be employed virtual disks or generic files such as backup files in computer systems generally. Advantageously embodiments disclosed herein permit a redo based hypervisor management server and or computer system to be used with a COW based storage system. Disk snapshots may be offloaded to the COW based storage system by swizzling disk descriptor files. To revert to an earlier disk state a snapshot may be taken before copying contents of a snapshot vvol of the COW based vvol hierarchy to a base vvol of the hierarchy to ensure ensures that the reversion can be rolled back if unsuccessful. Reference counting is performed to ensure that vvols in the vvol hierarchy are not orphaned in delete and revert use cases. Differences between vvols in the COW based vvol hierarchy may be used to clone the hierarchy and to migrate the hierarchy to a redo based disk hierarchy and vice versa. Further techniques disclosed herein preserve COW space and efficiency optimizations.

The various embodiments described herein may employ various computer implemented operations involving data stored in computer systems. For example these operations may require physical manipulation of physical quantities usually though not necessarily these quantities may take the form of electrical or magnetic signals where they or representations of them are capable of being stored transferred combined compared or otherwise manipulated. Further such manipulations are often referred to in terms such as producing identifying determining or comparing. Any operations described herein that form part of one or more embodiments may be useful machine operations. In addition one or more embodiments also relate to a device or an apparatus for performing these operations. The apparatus may be specially constructed for specific required purposes or it may be a general purpose computer selectively activated or configured by a computer program stored in the computer. In particular various general purpose machines may be used with computer programs written in accordance with the teachings herein or it may be more convenient to construct a more specialized apparatus to perform the required operations.

The various embodiments described herein may be practiced with other computer system configurations including hand held devices microprocessor systems microprocessor based or programmable consumer electronics minicomputers mainframe computers and the like.

One or more embodiments may be implemented as one or more computer programs or as one or more computer program modules embodied in one or more computer readable media. The term computer readable medium refers to any data storage device that can store data which can thereafter be input to a computer system computer readable media may be based on any existing or subsequently developed technology for embodying computer programs in a manner that enables them to be read by a computer. Examples of a computer readable medium include a hard drive network attached storage NAS read only memory random access memory e.g. a flash memory device a CD Compact Discs CD ROM a CD R or a CD RW a DVD Digital Versatile Disc a magnetic tape and other optical and non optical data storage devices. The computer readable medium can also be distributed over a network coupled computer system so that the computer readable code is stored and executed in a distributed fashion.

Although one or more embodiments have been described in some detail for clarity of understanding it will be apparent that certain changes and modifications may be made within the scope of the claims. For example SCSI is employed as the protocol for SAN devices and NFS is used as the protocol for NAS devices. Any alternative to the SCSI protocol may be used such as Fibre Channel and any alternative to the NFS protocol may be used such as CIFS Common Internet File System protocol. Accordingly the described embodiments are to be considered as illustrative and not restrictive and the scope of the claims is not to be limited to details given herein but may be modified within the scope and equivalents of the claims. In the claims elements and or steps do not imply any particular order of operation unless explicitly stated in the claims.

In addition while described virtualization methods have generally assumed that virtual machines present interfaces consistent with a particular hardware system the methods described may be used in conjunction with virtualizations that do not correspond directly to any particular hardware system. Virtualization systems in accordance with the various embodiments implemented as hosted embodiments non hosted embodiments or as embodiments that tend to blur distinctions between the two are all envisioned. Furthermore various virtualization operations may be wholly or partially implemented in hardware. For example a hardware implementation may employ a look up table for modification of storage access requests to secure non disk data.

Many variations modifications additions and improvements are possible regardless the degree of virtualization. The virtualization software can therefore include components of a host console or guest operating system that performs virtualization functions. Plural instances may be provided for components operations or structures described herein as a single instance. Finally boundaries between various components operations and data stores are somewhat arbitrary and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of embodiments described herein. In general structures and functionality presented as separate components in exemplary configurations may be implemented as a combined structure or component. Similarly structures and functionality presented as a single component may be implemented as separate components. These and other variations modifications additions and improvements may fall within the scope of the appended claim s .

