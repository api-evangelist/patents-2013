---

title: Method and system for managing network power policy and configuration of data center bridging
abstract: Certain aspects of a method and system for managing network power policy and configuration of data center bridging may include a network domain that comprises a single logical point of management (LPM) that coordinates operation of one or more devices, such as network interface controllers (NICs), switches, and/or servers in the network domain: The single LPM may be operable to manage one or both of a network power policy and/or a data center bridging (DCB) configuration policy for the network domain.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08914506&OS=08914506&RS=08914506
owner: Broadcom Corporation
number: 08914506
owner_city: Irvine
owner_country: US
publication_date: 20130711
---
This application is a continuation of U.S. patent application Ser. No. 12 848 680 filed Aug. 2 2010 pending which claims priority to provisional application Ser. No. 61 359 644 filed Jun. 29 2010 U.S. Provisional Application Ser. No. 61 304 650 filed Feb. 15 2010 U.S. Provisional Application Ser. No. 61 232 368 filed Aug. 7 2009 and U.S. Provisional Application Ser. No. 61 232 035 filed Aug. 7 2009 which applications are incorporated herein by reference in their entirety.

Certain embodiments of the invention relate to networking. More specifically certain embodiments of the invention relate to a method and system for managing network power policy and configuration of data center bridging.

Information Technology IT management may require performing remote management operations of remote systems to perform inventory monitoring control and or to determine whether remote systems are up to date. For example management devices and or consoles may perform such operations as discovering and or navigating management resources in a network manipulating and or administrating management resources requesting and or controlling subscribing and or unsubscribing operations and executing specific management methods and or procedures. Management devices and or consoles may communicate with devices in a network to ensure availability of remote systems to monitor and or control remote systems to validate that systems may be up to date and or to perform any security patch updates that may be necessary.

With the increasing popularity of electronics such as desktop computers laptop computers and handheld devices such as smart phones and PDA s communication networks and in particular Ethernet networks are becoming an increasingly popular means of exchanging data of various types and sizes for a variety of applications. In this regard Ethernet networks are increasingly being utilized to carry for example voice data and multimedia. Accordingly more and more devices are being equipped to interface with Ethernet networks.

As the number of devices connected to data networks increases and higher data rates are required there is a growing need for new transmission technologies which enable higher data rates. Increased data rates may often result in significant increases in power consumption. In this regard as an increasing number of portable and or handheld devices are enabled for Ethernet communications battery life may be a concern when communicating over Ethernet networks. As networks become increasingly large and complex network management also becomes increasingly complex. Furthermore larger faster and more complex networks become increasingly costly in terms of power consumption.

Energy Efficient Ethernet EEE is an emerging feature for Ethernet devices that is being defined by the IEEE 802.3az task force. The basic goal of EEE is for Ethernet network links to enter power saving mode in instances when the Ethernet link is not being utilized.

Further limitations and disadvantages of conventional and traditional approaches will become apparent to one of skill in the art through comparison of such systems with some aspects of the present invention as set forth in the remainder of the present application with reference to the drawings.

A system and or method is provided for managing network power policy and configuration of data center bridging substantially as shown in and or described in connection with at least one of the figures as set forth more completely in the claims.

These and other features and advantages of the present invention may be appreciated from a review of the following detailed description of the present invention along with the accompanying figures in which like reference numerals refer to like parts throughout.

Certain embodiments of the invention may be found in a system and or method for managing network power policy and configuration of data center bridging. Various aspects of the invention may enable a unified management architecture for managing network power policy and data center bridging DCB configuration in a data center environment. The unified management architecture may be operable to coordinate power management modes and or policies power consumption and DCB configuration on network interface controllers NICs servers and switches to provide domain wide power management and DCB configuration management for the networking components.

The data center may comprise several networking components including networking interface controllers inside servers network switches and or aggregation switches. Power management of the data center may comprise energy and cooling costs and limits on data center power availability from the grid. One or more power consumption schemes may be coordinated between the servers and the network domain and or the data center as a whole for example. During operational and idle conditions the network components power mode may be aligned with the application server and or user needs. During the idle condition the power consumed by the networking components that are not in low power modes may be more significant when compared with other system components consuming minimal power.

The data center may be operable to provide a solution for a single operating system OS virtualization a mix of physical and virtual servers network and storage convergence. The data center may enable a single logical point of management LPM for all network devices within a single management domain. The LPM may enable management of switches NICs servers NIC embedded switches and or soft switches. The LPM may enable simplified management and an automated IT administration role. The LPM may enable elimination of mis configuration and or contention issues. The LPM may also enable a flexible server network storage and or hypervisor integration scheme. The data center may be operable to control virtual LAN VLAN quality of service QoS jumbo frames security power converged network and or storage.

The data center may comprise a plurality of link layer technologies such as Ethernet Fibre Channel and Infiniband for example. Accordingly the data center may utilize one or more data center bridging DCB techniques and or protocols such as Congestion Notification CN Priority Flow Control PFC and or Enhanced Transmission Selection ETS . In this regard the DCB protocol suite may include Pause and or PFC for flow control management per link and or priority class ETS for bandwidth allocation per priority class and or Priority Groups and DCB Exchange DCBx for discovery and negotiation of relevant parameters on a per link basis.

The domains . . . may comprise rack mount networking systems that may house for example computing devices such as servers and networking devices such as switches and or other equipments such as power supplies. In an exemplary embodiment of the invention each domain may comprise servers . . . corresponding NICs . . . a switch and an uninterruptable power supply UPS . The data center is for illustration purposes only and the invention is not limited with regard to the network topology or the particular devices within a network.

The servers . . . of domain may each comprise suitable logic circuitry interfaces and or code that may be operable to provide services to client devices such as PCs mobile devices or other servers. Each of the servers may be operable to for example run one or more applications that process input from the clients and or output information to the clients. Each of the servers may interface to the network via a NIC .

The NICs . . . of each of the domains . . . may comprise suitable logic circuitry interfaces and or code that may be operable to interface the corresponding servers . . . to a corresponding switch .

Each of the switches . . . may comprise suitable logic circuitry interfaces and or code that may be operable to forward packets between corresponding NICs other ones of the switches . . . and other networks and or storage area networks .

Aspects of the invention enable network management of computing devices for example servers and networking devices for example switches via a single LPM. Furthermore both computing devices and networking devices in a network may be managed and or configured via a single management console. In this regard the LPMs . . . may be logically coupled to the various devices of the domains . . . and the management console .

With reference to the exemplary domain for illustration the LPM may enable management and or configuration of the servers . . . the corresponding NICs . . . the switch and the UPS via the management console . In this regard the LPM may expose an application programming interface API of the domain to the management console . In various embodiments of the invention the LPM may be implemented via logic circuitry interfaces and or code in the domain . In this regard resources of the servers . . . resources of the switch and or dedicated resources of the domain itself may be utilized to implement the LPM . The LPM may be operable to translate commands and requests of the management console to a device native. The LPM may be operable to provide a single control point for the domain which may distribute network configuration to other servers . . . and the NICs . . . in the domain .

Each LPM may provide a single control point for all and or various devices in a network domain. Each LPM may gather management and or configuration information from the devices of a network domain and make the information available via the management console . Each LPM may distribute management and or configuration information to the devices of a network domain and the information may be provided by server and or network administrators via the network management console .

Aspects of the invention may enable exchanging information to discover and or configure various devices in the network . In this regard one or more parameters in link partners that communicate over a communication link in the data center may need to be configured to enable reliable communication across the link. Accordingly if there is a configuration mismatch then communication over the link may fail or be sub optimal. For example if there is a parameter mismatch between the server and or NIC and the switch then communication over the corresponding link may fail or be sub optimal. Similarly if there is a configuration mismatch between the switch and the switch the communication over the link may fail. Moreover communication partners that are not link partners but communicate over multiple links multiple hops may also need to have matching configurations to enable reliable communication end to end. For example server may communicate with the server over the links and and thus configuration may match end to end. Accordingly aspects of the invention may enable validating that such configurations do match or are consistent with each other. Furthermore aspects of the invention may enable detecting and or correcting configuration mismatch or inconsistencies among many or in some instances all devices in a domain.

In various embodiments of the invention the validation may be performed via the LPMs and or the single management console . The validation may be automatic or may be initiated by an administrator. In various embodiments of the invention configuration of one networking or computing device in the data center may trigger automatic validation and or configuration of link partners to ensure end to end configuration match. The when how and which link partner performs validating and or updating of configuration parameters may be determined on a parameter by parameter basis.

In accordance with another embodiment of the invention DCB may be configured on a link by link basis. The data center may be operable to provide a service that may extend DCB end to end to ensure matching configuration and proper functionality. In one embodiment of the invention all the links in the domain may use the same configuration. In another embodiment of the invention some links may support one policy and other links may support another policy for example Fibre Channel over Ethernet FCoE at 10 Gb s with lossless links connected to some hosts while other hosts may not use FCoE or may have different bandwidth sharing links.

The network administrator may provide per application policies that may drive the per priority policies for the network. For example one or more priorities may be configured for lossless service while other priorities may be configured for best effort. The policies may comprise identifying the priority or priorities to use for FCoE and or Internet small computer system interface iSCSI . There may be one or more policy profiles configured for links depending on the mix of applications delivered over those links. The policy profiles may include for example minimum and optimal bandwidth allocations per priority or traffic class group. The server administrator may configure which applications are enabled on each NIC or may select a policy profile for each NIC . In instances where two or more policies interfere the LPM may enable determination of best possible configuration such that priorities may be given at least their minimum bandwidth if their optimal bandwidth is not available. The LPM may also report and or send error messages based on results such as when it cannot provide the minimum bandwidth. The switch may be operable to configure an adjacent NIC port for DCB. The LPM may ensure that the DCB features are configured consistently in the domain for example PFC is configured for the same priorities from the NIC to the top of the row.

The DCBx may use the concept of Willing or Not Willing to let the two link partners take roles as to which partner is driving the configuration. In a NIC switch link the switch may assume the master role and drive the configuration. However on a switch to switch link the roles may not be clear and both ports may be configured as Not Willing for example. In instances where there is a switch uplink connected to a switch downlink for example switch to switch the network administrator may configure the uplink ports as Willing where both ports are Not Willing or both are Willing. The DCBx may provide the information which may allow both sides of the link to detect that there is a configuration mismatch. The switch may report the mismatch to the LPM so that the mismatch may be corrected or an alert may be generated.

In accordance with another embodiment of the invention the LPM may be operable to ensure that DCB is set appropriately across a path that hosts use for accessing a resource or another host. An OS configuration for QoS may comprise a local administrator that may configure the OS to assign 802.1 priorities to frames if the host networking stack QoS packet scheduler has been installed and the network driver interface specification NDIS driver is enabled for 802.1Q p VLAN tagging. The local administrator and or server administrator may configure the NIC for DCB operation that is not tied into the host QoS operation.

In accordance with another embodiment of the invention the DCB capabilities of the NIC and or switch may comprise one or more of support for PFC and a number of lossless traffic classes support for ETS and a number of traffic classes and support for quantized congestion notification QCN . The DCB capabilities of the NIC and or switch may further comprise one or more of a number of rate limiters or NICs a number of congestion points CPs iSCSI support FCoE support a maximum frame size per lossless priority and or a maximum frame size for port and maximum for any priority.

In accordance with another embodiment of the invention a DCB policy may comprise details regarding a desired configuration policy that may account for the server administrator goals. The DCB configuration may comprise actual values used for the DCB parameters. In an exemplary embodiment the DCB policy may comprise PFC including a willing bit for PFC type length and value TLV and priorities for which PFC may be enabled ETS including a willing bit for ETS TLV mapping of priority to traffic class and traffic class group TCG table or priority group traffic class group bandwidth allocation and a symmetric policy such that ETS configuration of link partners match. The DCB policy may also comprise QCN support including priorities for which QCN is enabled QCN parameters iSCSI support including priority assigned for iSCSI FCoE support including priority assigned for FCoE a maximum frame size per lossless priority and or a maximum frame size for port and maximum for any priority.

Each of the DCB parameters may comprise supported and enabled bits as well as the configuration when enabled for example the priorities on which PFC is enabled or the traffic class groups and weights for ETS. The data center may ensure that grouping of priorities into traffic class groups is consistent. The data center may also ensure that grouping of priorities into TCG may be performed in adherence to the communicated TLVs. For example priorities related to storage may be grouped in one TCG and share a rate limiting RL parameter if QCN is enabled.

Exemplary PFC mismatches may occur and may comprise switch and NIC PFC enable mismatch switch and NIC PFC enabled priorities mismatch a maximum frame size for PFC priority does not match at link partners a maximum frame size for port may not match at link partners and or PFC not enabled for FCoE. Exemplary PFC mismatches may also comprise PFC not enabled for iSCSI if policy indicates that it is desired PFC is desired but not supported on the port and or a link partner may detect reception of PAUSE on a link where PFC is enabled.

Exemplary ETS mismatches may occur and may comprise switch and NIC ETS enable mismatch priority that is supposed to be strict priority may not be in TCG priority that is supposed to be ETS is in TCG priorities in the same traffic class may not all be in the same traffic class group or a traffic class may be a subset of one traffic class group and or bandwidth allocations may not match LPM configured guidelines. Exemplary ETS mismatches may also comprise TCG or bandwidth mismatch between link partners if ETS configuration is intended to be symmetric ETS shares may not total 100 and or symmetric bandwidth allocation based on the nature of the traffic for example 10 Gb s on a FCoE connection in both directions is desired but may not be configured or supported.

Exemplary QCN mismatches may occur such as switch and NIC QCN enabled mismatch switch and NIC QCN enabled priorities mismatch NIC RL parameters may not conform to LPM configuration for that priority switch CP parameters may not conform to LPM configuration for that priority.

Exemplary DCB global mismatches may occur such as PFC and non PFC priorities sharing a traffic class PFC and non PFC priorities sharing a traffic class group and or different assignment of application to traffic classes on different links in the domain. Exemplary DCB global mismatches may also comprise DCB wide consistency mismatch for example miniscule bandwidth allocation for an active storage priority a behavior may be desired but not supported by a device and or an OS may be configured for independently setting priority bits in frames to be transmitted and DCB may be negotiated by the NIC .

In accordance with another embodiment of the invention lossless behavior may be required for FCoE and may be necessary for some other proprietary protocols. In other cases such as iSCSI lossless links may improve performance but may not be strictly required. To support this policy the data center may enforce lossless behavior as required or as desired on a priority. In instances where the data center detects that lossless behavior is required on a priority but not supported in the NIC the LPM may flag it as a mismatch case or based on policy set by the administrator enable 802.3 PAUSE on the NIC to switch link. On a switch to switch link or a NIC switch mismatch the LPM may report the problem to the administrator. The use of PFC and PAUSE on the same link may not be allowed. In accordance with an embodiment of the invention it may be possible that both PFC and PAUSE have been enabled for the link since PAUSE operation may be negotiated in auto negotiation for the link and PFC configuration may be detected in DCBx link layer discovery protocol LLDP exchanges after the link is in operation. In such an instance once PFC has been enabled requests to send PAUSE frames may not be initiated and only PFC may be used. In instances where PFC is configured for more priorities than the number of lossless classes that the port can support multiple PFC priorities may be grouped into the same traffic class. If the number of traffic classes supported on a port is less than the number of TCGs the TCGs may be configured into the same traffic class. In this case the traffic class may be given the combined bandwidth allocation for the TCGs.

In accordance with another embodiment of the invention the LPM may ensure that priorities have a consistent configuration from the NIC through all the switches in the domain . The LPM may have a desired DCB configuration for the domain . The LPM may attempt to match port configurations to that profile and if a port cannot be configured to match the LPM may be enabled to alert the administrator. The LPM may also have a database comprising a set of DCB profiles and a mapping of systems to profiles for instances based on MAC or IP addresses. When a system is connected the LPM may attempt to apply the configuration for that system s profile to the NIC and its corresponding switch and generate an alert if it cannot be applied. When a virtual machine VM is migrating the LPM may be operable to check that the destination port DCB configuration is consistent with the profile for that VM. In instances where the destination port DCB configuration is not consistent with the profile for that VM and the profiles of other VMs already on the destination NIC and switch ports allow for the required change then the LPM may be operable to change the configuration of the NIC and switch ports as required. In instances where the profiles or the port capabilities may not allow for the change the LPM may be operable to generate an alert.

The NIC may comprise suitable logic interfaces code and or one or more circuits that may be operable to support Energy Efficient Ethernet EEE for example.

The PHY core may comprise suitable logic interfaces code and or one or more circuits that may be operable to receive and or communicate packets via the network interface for example the Ethernet . When the NIC has been idle for a particular period of time the PHY core may transition to a lower power mode for example a low power idle mode as specified by IEEE 802.3az specification. The transition of the PHY core to the low power mode may be transparent to the operating system on the network endpoint. The time period of transitioning from the low power mode to a full power mode may be referred to as the wake time Tw of the PHY core .

The MAC may comprise suitable logic interfaces code and or one or more circuits that may be operable to support the Ethernet 802.3 protocol interface to the PHY core support packet classification and error detection logic for incoming packets and support memory for temporary packet buffering. The MAC may be operable to handle offloading of tasks such as checksum calculations accelerating TCP IP or IPSEC traffic for example. The MAC may be operable to centrally manage power management policies for the NIC . The MAC may comprise a timer . The timer may comprise suitable logic code and or one or more circuits that may be operable to store a particular time period.

The DMA engine may comprise suitable logic interfaces code and or one or more circuits that may be operable to initiate direct memory access DMA read and write requests to the PCI E core .

The PCI E core may comprise suitable logic interfaces code and or one or more circuits that may be operable to generate DMA requests on the PCI E core support PCI E protocol and provide PCI E target support. The PCI E core may comprise a power saving feature for example Active State Power Management ASPM . The ASPM feature of the PCI E core may comprise three power states for example a low power PCI E state L a low resume latency energy saving standby state L and a full power PCI E state L. The low power PCI E state L may be operable to save considerably more power than the full power PCI E state L but may also have a greater impact to performance and responsiveness. When the low power PCI E state L is enabled on a given PCI E core and if the PCI E core has been inactive for a period of time for example 10 5000 microseconds the PCI E core may transition to the low power PCI E state L that may consume much less power than the full power PCI E state L. While in the low power PCI E state L a PCI E clock in the PCI E core may be stopped and a phase locked loop PLL may be powered down to save power in the NIC . However the PCI E core needs to be returned to the full power PCI E state L for a device to start a transfer of data across the PCI E core . The time period of transitioning from the low power PCI E state L to the full power PCI E state L may be referred to as the L to L exit latency of the PCI E core for example. The L to L exit latency may begin when a device wants to initiate a PCI E transaction for example a DMA transfer and may initiate the transition of the PCI E core to the full power PCI E state L. The L to L exit latency may end when the PCI E core has transitioned to the full power PCI E state L.

In operation when a packet is received by the NIC via the network interface for example the Ethernet the data in the packet may enter the NIC at the PHY core and be processed by the MAC . The entire packet may be received in order for the MAC to perform a cyclic redundancy check CRC on the packet to check for errors. In instances where there are no errors with the packet the DMA engine may initiate one or more DMA requests to the PCI E core to transfer the packet to host memory via the PCI E core .

In order to transmit a packet the server may initiate a PCI E write transaction to the NIC . The NIC may be operable to initiate a DMA read over the PCI E core . The data received from the server may be assembled by the NIC in the MAC . The MAC may be operable to transmit the data to the PHY core . The PHY core may be operable to transmit the packets via the network interface for example the Ethernet .

In instances where PCI E core is in a low power PCI E state L and the PHY core is in a low power mode for example low power idle the NIC may want to receive a packet via the network interface for example the Ethernet at the PHY core . The NIC may be operable to speculatively initiate a transition of the PCI E core from the low power PCI E state L to the full power PCI E state L when the PHY core senses that its remote network peer is bringing the network interface back to full power in order to send a packet and before the NIC has received the packet. Accordingly the NIC may be operable to mask at least a portion of the L to L exit latency of the PCI E core .

The PHY core may be operable to communicate a signal to the MAC when the PHY core senses that it is about to receive a packet via the network interface for example the Ethernet . The MAC may be operable to communicate a signal to the PCI E core to initiate a transition from the low power PCI E state L to the full power PCI E state L when the PHY core in the NIC senses that it is about to receive a packet and initiates transition from the low power mode to the full power mode. The communicated signal may be edge triggered or level triggered for example. The MAC may be operable to generate a pulse for example or assert a signal to initiate a transition from the low power PCI E state L to the full power PCI E state L.

The timer may be configured for a particular time period after the start of the transition of the PHY core in the NIC from the low power mode to the full power mode if the L to L exit latency of the transition from the low power PCI E state L to the full power PCI E state L is lesser than the wake time Tw of the transition of the PHY core from the low power mode to the full power mode. The timer may also be configured for a particular time period after the transition of the PHY core in the NIC from the low power mode to the full power mode if the speed of the network interface for example the Ethernet to the NIC is lesser than a threshold value for example lesser than 1 GBps. Notwithstanding the invention may not be so limited and other threshold values may be utilized without limiting the scope of the invention.

In accordance with various embodiments of the invention one or more transitions from the full power PCI E state L to the low power PCI E state L and back to the full power PCI E state L may be prevented by resetting a PCI E core inactivity timer earlier than it would have otherwise been reset. The PCI E core inactivity timer may be utilized to determine when the PCI E core may transition from a full power PCI E state L to the low power PCI E state L due to inactivity. The PCI E core inactivity timer may be reset for example when the PHY core initiates transition from a low power mode to a full power mode. Accordingly one or more transitions from the full power PCI E state L to the low power PCI E state L may be avoided where the PCI E core inactivity timer was about to expire when the PHY core initiates transition from a low power mode to a full power mode and would have expired before a packet had been received and been ready for DMA via the PCI E core . The NIC may be operable to reduce system latency by avoiding the one or more transitions from the full power PCI E state L to the low power PCI E state L.

In accordance with various embodiments of the invention one or more power matching modes for a server a NIC and or a switch may comprise one or more of a best performance mode a normal mode and or a minimal mode of operation. In a best performance mode of operation the server NIC and or switch may be configured to provide the best performance including the networking performance. In a normal mode of operation the server NIC and or switch may operate normally and conserve power during idle or low activity periods and the power management related parameters may be set to provide a balance between the performance and power consumption. In a minimal mode of operation the server NIC and or switch may be configured to save power aggressively. The power management related parameters in the minimal mode may be set to minimize the power consumption. The power matching mechanism may be used for example to coordinate modes and power saving mechanisms on the NICs servers and switches and tie in different power management policies together to provide domain wide power management of networking components.

A NIC may be influenced by both the platform policies for example OS BIOS PCIe bus ASPM and or a Baseband Management Controller BMC as well as the network and its related standards for example IEEE and or Energy Efficient Ethernet EEE . Table 1 describes the corresponding ASPM and EEE policies on the NICs and switches for various power modes.

In accordance with various embodiments of the invention one or more features of the power management policy may comprise a domain dynamic power management and dynamic power capping and reporting. The domain dynamic power management may enable domain or sub domain level power modes that may result in coordinated EEE policies and settings on the NICs servers and switches . The dynamic power capping and reporting may provide domain or sub domain level power maximum and average capping and or reporting capability. For a given power budget the LPM may be used to transparently configure appropriate capping on an average link utilization link speeds and teaming configurations on the NICs servers and switches . The LPM may also be used to notify power management mode mismatches on the NICs servers and switches for a given domain power cap.

The LPM may be used to enable dynamic configuration of power modes for a domain or a sub domain of NICs servers and switches . The NIC Advanced Configuration and Power Interface ACPI power management PCI e ASPM state management EEE ASIC level power management server power management and or switch power management may be combined to provide a domain wide power management solution for the networking components.

The NICs XY servers XY and switches X within a domain X may be configured to enable dynamic power modes. The dynamic power modes may be configured either globally per domain X or based on the power management policies set on each server XY for example. One or more of the power management modes may be set by the administrator on a server XY which may result in a NIC XY or LPM X setting some of the underlying parameters such as EEE timers ASPM parameters and or link speed on the NICs XY and the corresponding switches X. The NICs XY and or switches X may further coordinate to detect system idle conditions or may enable transitioning to a low power mode. A network administrator may enable a specific power mode for a set of NICs XY and or switches X globally using a power saving mechanism if the LPM X is operable to remotely configure the NIC XY and or the BMC to influence the OS power policies.

An OS for example the Windows OS may provide a plurality of different exemplary overall system power policies comprising High Performance Balanced and Power Saver. These system level power policies may be mapped to best performance normal and minimal power modes for example. The OS may support power policy management via Windows management instrumentation WMI that may be used for both local and remote management. The OS may enable customizable power policies using scripting or WMI interfaces. The OS power management architecture may provide functionality for power metering and budgeting such as inventory information capability and characteristics measurement reporting configurable platform power budget and or notifications for changes in configuration and capabilities as well as threshold based notifications.

The server power policy may be mapped to different system power states such as processor P states processor C states device states PCI e ASPM states and or EEE mode. Table 2 illustrates the mapping of the NIC ASPM and EEE for different system configurations.

In accordance with various embodiments of the invention the management console may be operable to communicate with a BMC for managing power supply modes fan control power on off and reset power reporting and or power budgeting for example. The power management may be performed remotely using management consoles as well as locally using graphical user interfaces GUIs and or tools.

The OS may be operable to communicate with a BMC to receive power readings and events when the BMC is directly monitoring and or controlling power supplies and sensors. The BMC may be operable to use ACPI drivers as well as OS level power management policies to enforce power management policies that are exposed by the BMC to the management console . The NIC may be operable to participate in the OS power management architecture by supporting the WMI interface and instrumentation. The OS may be operable to control the ACPI and PCIe ASPM states that may impact the NIC power state. The NIC may be operable to follow the directions from the OS and may also independently initiate transition on the link or follow the link partner commands based on EEE.

The EEE settings may be configured on a per link basis by both link partners. One or more cases may be supported for example view and or change EEE settings on a device as an integral part of the server and or OS power policy view and or change end to end EEE settings of a domain without requiring to view and or change EEE settings on a per device basis policy driven automatic selection of the best set of EEE parameters on a device for a specific power management policy as an integral part of the server and or OS power policy policy driven automatic selection of the best set of EEE parameters on every device along an end to end path for a specific power management policy and provide notification for an inconsistent configuration of EEE settings.

One or more parameters may be provided for the EEE settings for example EEE enable and or disable EEE policy such as aggressive normal or optimized for best performance. In an aggressive policy the EEE timers may be set to allow for an aggressive power management. For example a small timer value may be utilized to wait to detect link idle condition and a high system wait time Tw sys value of the link. In a normal policy the EEE timers may be set to allow for a balance between system performance and power savings. For example a small timer value to wait to detect link idle condition and a high Tw sys value. In an optimized for best performance policy the EEE timers may be set conservatively to trade off power savings in favor of providing the best performance on the system. A low value of Tw sys may be suitable to address the requirements of low latency applications and also use a long wait time before deciding to enter into low power mode. One or more supported modes for EEE may comprise 100Base TX 1000Base T 10 GBase T 1000Base KX 10 GBase KX4 and or 10 GBase KR. The EEE operation may be asymmetric except for 1000Base T which may be symmetric. Each device may unilaterally decide to place its transmit path in low power mode for example. The EEE timers may be set in microseconds and the EEE configuration mismatches such as enable and or disable may be notified.

Various aspects of the invention may provide for capping limiting and or reporting of power consumed by the physical networking components for example NICs and switches within a domain . The server power management may comprise the NIC but may not handle the network facing aspects and may not coordinate with the network operation. The LPM may be operable to control an absolute cap on power consumption and or control an average power consumption that may allow reaching power cost goals for example. The history of power consumed by these components may also be provided to enable the analysis of power consumed over a period of time. In one mode the LPM may use the link speed and dynamic teaming to control power consumption while accounting for the maximum power for each device.

In the average power consumption control mode the LPM may be operable to achieve an average power consumption level per domain . In this mode for a given power budget the LPM may be operable to configure link utilization caps link speeds and or teaming configurations on the NICs servers and switches . The LPM may be operable to notify the administrator when it is unable to guarantee a domain power cap based on the configured power management modes and parameters on the NICs servers and switches . One or more parameters may be used for the power capping limiting and reporting of the NICs servers and switches such as a maximum power consumed an average power consumed or the power consumed over a period of time a maximum allowable power for capping the average power consumed over a period of time a power mode for best performance a normal or minimal mode of operation power thresholds and notifications per threshold such as notify when the power exceeds the threshold and notify when the power falls below the threshold for example.

In accordance with an embodiment of the invention the best matching profile setting of power management mode link utilization cap and link speed may be selected based on user setting which may require the device to cap power consumption at a particular level. There may be a need to adjust link parameters when one link partner is adjusting its setting based on power while ensuring the new setting is still kept within the limits of the power cap. Such a mismatch may be communicated to the link partner to reach a mutually agreed upon setting that may still honor the power capping.

In step it may be determined whether two or more of the adjusted DCB configuration policies are conflicting. In instances where two or more of the adjusted DCB configuration policies are conflicting control passes to step . In step the LPM may be operable to arbitrate between the two or more conflicting DCB configuration policies for example between NICs and switches based on a minimum bandwidth available. In instances where two or more policies interfere or conflict the LPM may be operable to determine the best possible configuration such that priorities may be given at least their minimum bandwidth if their optimal bandwidth is not available. The LPM may also report and or send error messages based on results such as when it cannot provide the minimum bandwidth. Control then passes to step . In instances where there is no conflict between two or more of the adjusted DCB configuration policies control passes to step .

In step it may be determined whether one or more parameters between a switch and a NIC are mismatched. In instances where one or more parameters between a switch and a NIC are mismatched control passes to step . In step the one or more parameters of the switch and the NIC that are mismatched may be determined for example one or more of PFC parameters ETS parameters and or QCN parameters. For example one or more PFC mismatches may occur such as switch and NIC PFC enable mismatch switch and NIC PFC enabled priorities mismatch a maximum frame size for PFC priority does not match at link partners and a maximum frame size for port may not match at link partners. One or more ETS mismatches may occur such as switch and NIC ETS enable mismatch priorities in the same traffic class may not all be in the same traffic class group or a traffic class may be a subset of one traffic class group and bandwidth allocations may not match LPM configured guidelines. One or more QCN mismatches may occur such as switch and NIC QCN enabled mismatch and switch and NIC QCN enabled priorities mismatch. One or more DCB global mismatches may occur such as PFC and non PFC priorities sharing a traffic class or group different assignment of application to traffic classes on different links in the domain and a DCB wide consistency mismatch.

In step the LPM may be operable to adjust or set a new DCB configuration policy of the network domain based on the mismatched parameters between the switch and the NIC . Control then returns to step . In instances where there is no mismatch between one or more parameters between a switch and a NIC control returns to step .

In a best performance mode of operation the server NIC and or switch may be configured to provide the best performance including the networking performance. In a normal mode of operation the server NIC and or switch may operate normally and conserve power during idle or low activity periods and the power management related parameters may be set to provide a balance between the performance and power consumption. In a minimal mode of operation the server NIC and or switch may be configured to save power aggressively. The power management related parameters in the minimal mode may be set to minimize the power consumption.

In step the LPM may provide domain or sub domain level power maximum and average capping and or reporting capability. For a given power budget the LPM may be operable to transparently configure appropriate capping on an average link utilization link speeds a maximum power consumed and or an average power consumed by the one or more devices for example NICs switches and or servers based on the selected mode of operation.

In step the LPM may be operable to adjust one or more of system power states processor P states processor C states said one or more devices states PCI E ASPM states and or EEE mode based on the selected mode of operation. The LPM may be used to enable dynamic configuration of power modes for a domain or a sub domain of NICs servers and switches . Control then returns to step .

In accordance with an embodiment of the invention a method and system for managing network power policy and configuration of data center bridging may comprise a network domain which comprises a single LPM that coordinates operation of one or more devices for example NICs switches and or servers . One or more processors and or circuits in the LPM may be operable to manage one or both of a network power policy and or a data center bridging DCB configuration policy for the network domain . One or more processors and or circuits in the LPM may be operable to adjust the DCB configuration policy of the network domain based on one or more of a particular application a particular link a traffic class group a type of network connection and or connection speed. One or more processors and or circuits in the LPM may be operable to arbitrate between two or more conflicting DCB configuration policies for one or more devices for example between NICs and switches based on a minimum bandwidth available. One or more processors and or circuits in the LPM may be operable to manage the DCB configuration policy for the network domain based on one or more parameters of a switch and a NIC that are mismatched. The one or more parameters of the switch and the NIC that are mismatched may comprise one or more of PFC parameters ETS parameters and or QCN parameters.

One or more processors and or circuits in the LPM may be operable to select one or more of a best performance mode a normal mode and or a minimal mode of operation of the one or more devices for example NICs switches and or servers based on the managed network power policy for the network domain . One or more processors and or circuits in the LPM may be operable to adjust one or more of a link speed a link utilization a maximum power consumed and or an average power consumed by the one or more devices for example NICs switches and or servers based on the selected mode of operation. One or more processors and or circuits in the LPM may be operable to adjust one or more of system power states processor P states processor C states said one or more devices states PCI E ASPM states and or EEE mode based on the selected mode of operation. One or more processors and or circuits in the LPM may be operable to manage one or both of the network power policy and or the DCB configuration policy for a portion of the one or more devices for example NICs switches and or servers in the network domain .

Other embodiments of the invention may provide a non transitory computer readable medium and or storage medium and or a non transitory machine readable medium and or storage medium having stored thereon a machine code and or a computer program having at least one code section executable by a machine and or a computer thereby causing the machine and or computer to perform the steps as described herein for managing network power policy and configuration of data center bridging.

Accordingly the present invention may be realized in hardware or a combination of hardware and software. The present invention may be realized in a centralized fashion in at least one computer system or in a distributed fashion where different elements may be spread across several interconnected computer systems. Any kind of computer system or other apparatus adapted for carrying out the methods described herein may be suited. A typical combination of hardware and software may be a general purpose computer system with a computer program that when being loaded and executed may control the computer system such that it carries out the methods described herein. The present invention may be realized in hardware that comprises a portion of an integrated circuit that also performs other functions.

The present invention may also be embedded in a computer program product which comprises all the features enabling the implementation of the methods described herein and which when loaded in a computer system is able to carry out these methods. Computer program in the present context means any expression in any language code or notation of a set of instructions intended to cause a system having an information processing capability to perform a particular function either directly or after either or both of the following a conversion to another language code or notation b reproduction in a different material form.

While the present invention has been described with reference to certain embodiments it will be understood by those skilled in the art that various changes may be made and equivalents may be substituted without departing from the scope of the present invention. In addition many modifications may be made to adapt a particular situation or material to the teachings of the present invention without departing from its scope. Therefore it is intended that the present invention not be limited to the particular embodiment disclosed but that the present invention will include all embodiments falling within the scope of the appended claims.

