---

title: Hover controlled user interface element
abstract: Example apparatus and methods concern controlling a hover-sensitive input/output interface. One example apparatus includes a proximity detector that detects an object in a hover-space associated with the input/output interface. The apparatus produces characterization data concerning the object. The characterization data may be independent of where in the hover-space the object is located. The apparatus selectively controls the activation, display, and deactivation of user interface elements displayed by the apparatus on the input/output interface as a function of the characterization data and interface state. Selectively controlling the activation, display, and deactivation of the user interface elements includes allocating display space on the input/output interface to the user interface elements when they are needed for an operation on the apparatus and selectively reclaiming space on the input/output interface allocated to the user interface elements when they are not needed for an operation on the apparatus.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09170736&OS=09170736&RS=09170736
owner: 
number: 09170736
owner_city: 
owner_country: 
publication_date: 20130916
---
In a touch screen device like a smart phone or a tablet computer a virtual keyboard may be used to input information. For example on a phone having a touch screen the virtual keyboard may be used to send text messages. When a user is texting the user may also want to view the content in the text application while typing. Unfortunately conventional systems may consume so much display space when providing the virtual keyboard that the space left over to display the content of the application may be limited.

One conventional approach provided a user with a tool to resize the virtual keyboard to make it smaller. However making the keyboard smaller may compromise the typing experience. For example it may be very difficult if even possible at all to see the smaller keys or to type on a smaller keyboard. This may be most noticeable for users with less than perfect vision or with large fingers. Another conventional approach involved compressing a conventional QWERTY keyboard down into a one row keyboard. The one row keyboard was then displayed at the bottom of the device. Once again the typing experience was negatively impacted because less than all the keys were available. Another conventional approach made the virtual keyboard appear or disappear based on heuristics associated with whether the user was touching the screen whether the user had typed in a while whether the user had moved their digit s e.g. finger s thumb s onto or off of the virtual keyboard or other actions. However removing the keyboard may have required cumbersome actions to redisplay the keyboard which once again may have negatively impacted the typing experience. Another conventional approach may have included dismissing e.g. removing the keyboard in response to touching for example a back capacitive button. Unfortunately once the keyboard was dismissed there may not have been a convenient way to retrieve the keyboard without tapping on another user interface element e.g. edit compose field . Complicating matters even further if the user happened to scroll their content after the keyboard was dismissed then the user may have been forced to navigate back to a particular user interface element e.g. edit compose field to retrieve the keyboard. Thus conventionally it has been difficult if even possible at all to seamlessly save screen space while not compromising the typing experience.

Touch sensitive screens have in some apparatus been replaced by hover sensitive screens that rely on proximity detectors. Conventional hover sensitive screens displayed user interface elements based on where an object was located in the hover space. This may have unnecessarily constrained the flexibility of presenting activating or deactivating user interface elements.

This Summary is provided to introduce in a simplified form a selection of concepts that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

Example methods and apparatus are directed toward controlling a user interface on a device having an input output interface that is hover space sensitive. Controlling the user interface may include selectively displaying activating de activating and removing user interface elements based on actions performed in the hover space and the state of an application s or other process using the i o interface. Activation display de activation and removal may depend on identifying a control action performed by an object in a hover space associated with the i o interface. Unlike conventional systems that are position dependent the example methods and apparatus described herein may be position independent depending instead on understanding when a user interface element is or is not needed based on state maintained for user interface elements and location independent control actions performed in the hover space.

Some embodiments may include a hover sensitive i o interface and a proximity detector that detects an object e.g. finger thumb pencil stylus with capacitive tip in a three dimensional volume e.g. hover space . The hover space may be disposed in proximity to the i o interface and in an area accessible to the proximity detector. An embodiment may produce characterization data concerning the object. Unlike conventional systems the characterization data may be independent of where in the hover space the object is located. An embodiment may selectively control the activation display and deactivation of a first user interface element displayed by the apparatus on the i o interface as a function of the characterization data.

Example apparatus and methods facilitate saving screen space on a device having an i o interface that is hover sensitive without compromising the user interaction e.g. typing experience. Hover technology is used to detect when a user interface element e.g. virtual keyboard is needed or is not needed. Hover technology and hover sensitive refer to sensing an object spaced away from e.g. not touching yet in close proximity to a display in an electronic device. Close proximity may mean for example beyond 1 mm but within 1 cm beyond 0.1 mm but within 10 cm or other combinations of ranges. Being in close proximity includes being within a range where a proximity detector can detect and characterize an object in the hover space. The device may be for example a phone a tablet computer a computer or other device.

The device may include a proximity detector that detects when an object e.g. digit pen is close to but not touching the i o interface . The proximity detector may identify the location x y z of an object in the three dimensional hover space . The proximity detector may also identify other attributes of the object including for example the speed with which the object is moving in the hover space the orientation e.g. pitch roll yaw of the object with respect to the hover space the direction in which the object is moving with respect to the hover space or device a gesture being made by the object or other attributes of the object .

In different examples the proximity detector may use active or passive systems. For example the proximity detector may use sensing technologies including but not limited to capacitive electric field inductive Hall effect Reed effect Eddy current magneto resistive optical shadow optical visual light optical infrared IR optical color recognition ultrasonic acoustic emission radar heat sonar conductive and resistive technologies. Active systems may include among other systems infrared or ultrasonic systems. Passive systems may include among other systems capacitive or optical shadow systems. In one embodiment when the proximity detector uses capacitive technology the detector may include a set of capacitive sensing nodes to detect a capacitance change in the hover space . The capacitance change may be caused for example by a digit e.g. finger thumb or other object e.g. pen that comes within the detection range of the capacitive sensing nodes. In another embodiment when the proximity detector uses infrared light the proximity detector may transmit infrared light and detect reflections of that light from an object within the detection range e.g. in the hover space of the infrared sensors. Similarly when the proximity detector uses ultrasonic sound the proximity detector may transmit a sound into the hover space and then measure the echoes of the sounds. In another embodiment when the proximity detector uses a photodetector the proximity detector may track changes in light intensity. Increases in intensity may reveal the removal of an object from the hover space while decreases in intensity may reveal the entry of an object into the hover space .

In general a proximity detector includes a set of proximity sensors that generate a set of sensing fields in the hover space associated with the i o interface . The proximity detector generates a signal when object is detected in the hover space . In one embodiment a single sensing field may be employed. In other embodiments two or more sensing fields may be employed. In one embodiment a single technology may be used to detect or characterize the object in the hover space . In another embodiment a combination of two or more technologies may be used to detect or characterize the object in the hover space .

Determining when the virtual keyboard is or is not desired may including analyzing the presence posture or actions of a user s digit s . If the virtual keyboard is needed the virtual keyboard will be presented in a useful location. How the virtual keyboard is presented including the size position orientation or other attributes may be user configurable or controllable through an application programming interface API or other messaging system. If the virtual keyboard is not needed then the virtual keyboard may be diminished or moved to an out of the way location from which it may be conveniently retrieved.

Conventionally a display manufacturer or programmer may have determined the size shape position and performance of a virtual keyboard. In one embodiment an application programming interface API through which an application can communicate with a process that controls the virtual keyboard may be provided. Thus in one example keyboard performance may be controlled on a per application basis.

Some portions of the detailed descriptions that follow are presented in terms of algorithms and symbolic representations of operations on data bits within a memory. These algorithmic descriptions and representations are used by those skilled in the art to convey the substance of their work to others. An algorithm is considered to be a sequence of operations that produce a result. The operations may include creating and manipulating physical quantities that may take the form of electronic values. Creating or manipulating a physical quantity in the form of an electronic value produces a concrete tangible useful real world result.

It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers and other terms. It should be borne in mind however that these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise it is appreciated that throughout the description terms including processing computing and determining refer to actions and processes of a computer system logic processor or similar electronic device that manipulates and transforms data represented as physical quantities e.g. electronic values .

Example methods may be better appreciated with reference to flow diagrams. For simplicity the illustrated methodologies are shown and described as a series of blocks. However the methodologies may not be limited by the order of the blocks because in some embodiments the blocks may occur in different orders than shown and described. Moreover fewer than all the illustrated blocks may be required to implement an example methodology. Blocks may be combined or separated into multiple components. Furthermore additional or alternative methodologies can employ additional not illustrated blocks.

Method also includes at identifying a control action performed by an object in a hover space associated with the i o interface. Recall that the hover space is a three dimensional volume existing in an area accessible to the i o interface. The three dimensional volume may be positioned conceptually above an i o interface. However above may be a relative term meaning located in a direction from which the i o interface can be viewed and in between the i o interface and a viewer of the i o interface. If the user is standing up and reading on a tablet then the space above the i o interface may actually be horizontally displaced from the device rather than vertically displaced. If the user is lying on their back then the hover space may be below the device. A hover space may have different sized dimensions in different applications depending for example on the type of technology used by a proximity detector that provides the hover sensitivity.

In one embodiment identifying the control action includes receiving a signal from a detection system e.g. proximity detector provided by the device. The detection system may be an active detection system e.g. infrared ultrasonic a passive detection system e.g. capacitive or a combination of systems. The detection system may be incorporated into the device or provided by the device. Unlike conventional systems whose proximity systems may provide position information e.g. x y z co ordinates that are mapped to locations on the i o interface example systems may identify a gesture independent of the position of the virtual keyboard or i o interface. For example a simulated typing gesture may be used to indicate the desire to retrieve the keyboard. The occurrence of the simulated typing gesture rather than its location may control whether to retrieve the virtual keyboard.

Identifying the control action may also include other actions. For example identifying the control action may include determining that a digit has entered the hover space or has left the hover space. A digit appearing in the hover space may indicate that the keyboard is needed while the disappearance of the digit may indicate that the keyboard is no longer needed. Identifying the control action may also include identifying the presence of a digit at a pre determined location in the hover space. The pre determined location may be relative to the i o interface or may be relative to the position of the virtual keyboard. Identifying the control action may also include identifying a gesture e.g. simulated typing swipe that is characterized in relation to where it is performed relative to the position of the virtual keyboard. In one embodiment the control action may actually be inaction. For example identifying the control action may include identifying that a digit has moved less than a threshold amount in the hover space over a threshold period of time. For example if a finger in the hover space has moved less than 2 mm in the past three seconds then this may be an indication that the user has stopped typing and the keyboard is no longer required.

Method also includes at selectively controlling the availability of the virtual keyboard on the i o interface as a function of the control action. Controlling the availability may include making the virtual keyboard visible removing the virtual keyboard changing the appearance of the virtual keyboard changing whether the virtual keyboard is able to interact with objects in the hover space or other actions. In one embodiment controlling the availability of the virtual keyboard at includes making one hundred percent of the virtual keyboard visible on the i o interface and making virtual keys associated with the virtual keyboard active for receiving a virtual keystroke. In another embodiment controlling the availability of the virtual keyboard at may include making less than one hundred percent of the virtual keyboard visible on the i o interface and making virtual keys associated with the virtual keyboard inactive for receiving a virtual keystroke. In another embodiment controlling the availability of the virtual keyboard at may include splitting the virtual keyboard into at least two pieces and then rearranging the at least two pieces. In one example re arranging the two pieces may include positioning a first piece at a first position e.g. left side on the i o interface and positioning a second piece at a second different position e.g. right side on the i o interface. Additionally less than one hundred percent of a piece may be visible on the i o interface.

In another embodiment controlling the availability of the virtual keyboard at may include positioning a portion of the virtual keyboard within a threshold distance of an edge of the input output interface. For example the portion may be placed within 1 mm of the edge within 5 mm of the edge within 10 mm of the edge within 1 pixel of the edge within 10 pixels of the edge within one percent of the width of the viewable area of the i o interface from the edge within ten percent of the width of the viewable area of the i o interface from the edge or at other locations. In yet another embodiment controlling the availability of the virtual keyboard comprises changing the opacity of the virtual keyboard. For example the keyboard may be made more or less opaque depending on whether it is being used. If the keyboard is being used then it may be completely or mostly opaque so that it appears to be on top of other information on the i o interface. But if the keyboard is not being used then it may be completely or mostly transparent so that it appears to be behind other information on the i o interface. While splitting a keyboard changing the opacity of a keyboard and repositioning a keyboard are displayed other techniques for minimizing the amount of space used by a virtual keyboard when the virtual keyboard is not in use may be employed.

Method also includes at selectively configuring the user interface element as a function of the state and the control action. For example the presence and location of a text insertion point may be controlled based on whether the virtual keyboard is being displayed or used. Configuring the user interface element at may include selectively displaying the user interface element on the i o interface at a location that makes an insertion point associated with the user interface element visible during virtual typing performed using the virtual keyboard. The location may depend at least in part on the state.

While and method have described a virtual keyboard more generally a user interface element may be reconfigured based on a characterization of a control action performed in the hover space. For example if a user is going to interact with a device appropriate user interface elements may be presented when the user hovers in the hover space and the user interface elements may be removed when the user stops hovering in the hover space. This may be employed in applications including for example a virtual video cassette recorder VCR where VCR controls are presented or removed based on hovering over a playing movie.

To support this configurability method may include at receiving an inbound message. The inbound message may be received through for example an application programming interface API provided by a process running on the device. In different embodiments the inbound message may also be received using other message passing approaches including for example sockets remote procedure calls interrupts or shared memory. The inbound message may include configuration information.

Therefore method may include at selectively reconfiguring how the availability of the virtual keyboard will be controlled. The reconfiguring may be a function of information in the inbound message and may control for example changing the activation display or deactivation actions associated with different control actions.

When a messaging interface like the API is available then method may also include at selectively providing an outbound message to the process through the API. The outbound message may concern for example an action performed using the virtual keyboard or an action performed using the user interface element. The outbound message may facilitate performing some processing off the device that provides the i o interface.

Method may also include at providing an indication that the virtual keyboard is about to be re configured. Consider that a user s attention may be distracted while using a handheld device e.g. phone tablet . Rather than automatically changing the state of the virtual keyboard method may provide an indication that something is about to happen to provide the user with an opportunity to not have the action occur. The indication may be for example a graphical indication e.g. flashing screen an audible indication e.g. warning buzzer a haptic indication e.g. vibration or combination of actions. The indication may alert the user to an impending action e.g. keyboard being removed and allow the user to take an action e.g. accelerate device to avert the impending action.

While illustrate various actions occurring in serial it is to be appreciated that various actions illustrated in could occur substantially in parallel. By way of illustration a first process could identify and maintain state a second process could characterize control actions that occur in the hover space and a third process could control i o interface display space by selectively activating displaying de activating or removing user interface elements as a function of the state and control action. While three processes are described it is to be appreciated that a greater or lesser number of processes could be employed and that lightweight processes regular processes threads and other approaches could be employed.

In one example a method may be implemented as computer executable instructions. Thus in one example a computer readable storage medium may store computer executable instructions that if executed by a machine e.g. computer cause the machine to perform methods described or claimed herein including methods or . While executable instructions associated with the listed methods are described as being stored on a computer readable storage medium it is to be appreciated that executable instructions associated with other example methods described or claimed herein may also be stored on a computer readable storage medium. In different embodiments the example methods described herein may be triggered in different ways. In one embodiment a method may be triggered manually by a user. In another example a method may be triggered automatically.

It is possible that different users at different locations using different devices may access the hover service through different networks or interfaces. In one example the hover service may be accessed by a mobile device . In another example portions of hover service may reside on a mobile device . Hover service may perform actions including for example configuring how control actions will be interpreted on a hover sensitive device providing appearance location or control information for a user interface element or other action. In one embodiment hover service may perform portions of methods described herein e.g. method method .

Mobile device can include a controller or processor e.g. signal processor microprocessor application specific integrated circuit ASIC or other control and processing logic circuitry for performing tasks including signal coding data processing input output processing power control or other functions. An operating system can control the allocation and usage of the components and support application programs . The application programs can include mobile computing applications e.g. email applications calendars contact managers web browsers messaging applications or other computing applications.

Mobile device can include memory . Memory can include non removable memory or removable memory . The non removable memory can include random access memory RAM read only memory ROM flash memory a hard disk or other memory storage technologies. The removable memory can include flash memory or a Subscriber Identity Module SIM card which is known in GSM communication systems or other memory storage technologies such as smart cards. The memory can be used for storing data or code for running the operating system and the applications . Example data can include user interface element state web pages text images sound files video data or other data sets to be sent to or received from one or more network servers or other devices via one or more wired or wireless networks. The memory can store a subscriber identifier such as an International Mobile Subscriber Identity IMSI and an equipment identifier such as an International Mobile Equipment Identifier IMEI . The identifiers can be transmitted to a network server to identify users or equipment.

The mobile device can support one or more input devices including but not limited to a touchscreen a hover screen a microphone a camera a physical keyboard or trackball . While a touch screen and a physical keyboard are described in one embodiment a screen may be hover sensitive and may display a virtual keyboard. The mobile device may also support output devices including but not limited to a speaker and a display . Other possible input devices not shown include accelerometers e.g. one dimensional two dimensional three dimensional . Other possible output devices not shown can include piezoelectric or other haptic output devices. Some devices can serve more than one input output function. For example touchscreen and display can be combined in a single input output device. The input devices can include a Natural User Interface NUI . An NUI is an interface technology that enables a user to interact with a device in a natural manner free from artificial constraints imposed by input devices such as mice keyboards remote controls and others. Examples of NUI methods include those relying on speech recognition touch and stylus recognition gesture recognition both on screen and adjacent to the screen air gestures head and eye tracking voice and speech vision touch gestures and machine intelligence. Other examples of a NUI include motion gesture detection using accelerometers gyroscopes facial recognition three dimensional 3D displays head eye and gaze tracking immersive augmented reality and virtual reality systems all of which provide a more natural interface as well as technologies for sensing brain activity using electric field sensing electrodes electro encephalogram EEG and related methods . Thus in one specific example the operating system or applications can comprise speech recognition software as part of a voice user interface that allows a user to operate the device via voice commands. Further the device can include input devices and software that allow for user interaction via a user s spatial gestures such as detecting and interpreting gestures to provide input to an application.

A wireless modem can be coupled to an antenna . In some examples radio frequency RF filters are used and the processor need not select an antenna configuration for a selected frequency band. The wireless modem can support two way communications between the processor and external devices. The modem is shown generically and can include a cellular modem for communicating with the mobile communication network and or other radio based modems e.g. Bluetooth or Wi Fi . The wireless modem may be configured for communication with one or more cellular networks such as a Global system for mobile communications GSM network for data and voice communications within a single cellular network between cellular networks or between the mobile device and a public switched telephone network PSTN . Mobile device may also communicate locally using for example near field communication NFC element .

The mobile device may include at least one input output port a power supply a satellite navigation system receiver such as a Global Positioning System GPS receiver an accelerometer or a physical connector which can be a Universal Serial Bus USB port IEEE 1394 FireWire port RS 232 port or other port. The illustrated components are not required or all inclusive as other components can be deleted or added.

Mobile device may include a hover logic that is configured to provide a functionality for the mobile device . For example hover logic may provide a client for interacting with a service e.g. service . Portions of the example methods described herein may be performed by hover logic . Similarly hover logic may implement portions of apparatus described herein.

The proximity detector may detect an object in a hover space associated with the apparatus . The hover space may be for example a three dimensional volume disposed in proximity to the i o interface and in an area accessible to the proximity detector . A user may place a digit in the hover space may make a gesture in the hover space may remove a digit from the hover space or take other actions.

Apparatus may include a first logic that is configured to produce characterization data concerning the object. In one embodiment the characterization data is independent of where the object is located in the hover space . For example the characterization data may describe the size and movement of the object without reporting on its location. In one embodiment the characterization data may describe whether the object is present a direction of travel of the object a velocity of travel of the object an orientation of the object a size of the object or a gesture performed by the object . The characterization data may depend on signals provided by the proximity detector to the first logic .

Apparatus may include a second logic that is configured to selectively control the activation display and deactivation of a first user interface element displayed by the apparatus on the i o interface . The activation display and deactivation may be controlled as a function of the characterization data. In one embodiment selectively controlling the activation display and deactivation of the first user interface element includes allocating display space on the i o interface to the first user interface element when the first user interface element is needed for an operation on the apparatus . Selectively controlling the activation display and deactivation of the first user interface element may also include selectively reclaiming space that was allocated to the first user interface element on the i o interface when the first user interface element is not needed for an operation on the apparatus . The space may be reclaimed in different ways. For example the first user interface element may be moved to the edge of the i o interface may be split and moved to different edges of the i o interface may be reduced to a few keys that are then made more transparent to appear behind an application or in other ways.

In one embodiment the second logic is configured to control the activation display and deactivation of a second user interface element displayed by the apparatus on the i o interface as a function of both a state associated with the first user interface element and an action associated with the object and the first user interface element. The second user interface element may display a result of an interaction between the object and the first user interface element. By way of illustration the first user interface element may be a virtual keyboard that receives virtual keystrokes and the second user interface element may be a text box that inserts text at a text insertion point. In one embodiment the second logic is configured to control the activation display or deactivation of the first user interface element within a threshold amount of time of receiving access to the characterization data. For example the second logic may be configured to control the activation display or deactivation of the first user interface element within a second of receiving access to the characterization data within a tenth of a second of receiving access to the characterization data within a hundredth of a second of receiving access to the characterization data within a millisecond of receiving access to the characterization data or within other periods of time. In one embodiment the amount of time may be user configurable.

In one embodiment controlling the activation display and deactivation of the first user interface element includes displaying one hundred percent of the first user interface element on the i o interface and activating the first user interface element to interact with the object . Controlling the activation display and deactivation of the first user interface element may also include displaying less than one hundred percent of the first interface element on the i o interface and deactivating the first user interface element from interacting with the object . Controlling the activation display and deactivation of the first user interface element may also include dividing the first user interface element into at least two pieces positioning one of the pieces at a first position on the i o interface and positioning another of the pieces at a second different position on the i o interface . Controlling the activation display and deactivation of the first user interface element may also include positioning a portion of the first user interface element within a threshold distance of an edge of the i o interface or changing the transparency of the first user interface element.

Apparatus may include a third logic that reconfigures the first logic or the second logic based on a message received from a user or an application through a messaging interface. The message may describe for example how the user would like a virtual keyboard to be diminished when it is not needed.

Apparatus may include a memory . Memory can include non removable memory or removable memory. Non removable memory may include random access memory RAM read only memory ROM flash memory a hard disk or other memory storage technologies. Removable memory may include flash memory or other memory storage technologies such as smart cards. Memory may be configured to store user interface state information control action characterization data or other data.

Apparatus may include a processor . Processor may be for example a signal processor a microprocessor an application specific integrated circuit ASIC or other control and processing logic circuitry for performing tasks including signal coding data processing input output processing power control or other functions. Processor may be configured to control user interface element presentation and functionality. For example processor may be controlled to selectively allow a user interface element e.g. keyboard to consume more space on i o interface when the user interface element is needed and to selectively diminish the amount of space the user interface element consumes on the i o interface when the user interface element is not needed.

In one embodiment the apparatus may be a general purpose computer that has been transformed into a special purpose computer through the inclusion of the set of logics . The set of logics may be configured to perform input and output. Apparatus may interact with other apparatus processes and services through for example a computer network.

The following includes definitions of selected terms employed herein. The definitions include various examples or forms of components that fall within the scope of a term and that may be used for implementation. The examples are not intended to be limiting. Both singular and plural forms of terms may be within the definitions.

Opacity as used herein refers to how much a first image on a display will obscure a second image on the display where the second image is perceived as being behind the first image. Zero percent opacity means the first image will obscure zero percent of the second image. One hundred percent opacity means the first image will obscure one hundred percent of the second image. Opacity may be defined using for example an alpha channel. The alpha channel is used in alpha compositing which is the process of combining an image with a background to create the appearance of partial or full transparency.

References to one embodiment an embodiment one example and an example indicate that the embodiment s or example s so described may include a particular feature structure characteristic property element or limitation but that not every embodiment or example necessarily includes that particular feature structure characteristic property element or limitation. Furthermore repeated use of the phrase in one embodiment does not necessarily refer to the same embodiment though it may.

 Computer readable storage medium as used herein refers to a medium that stores instructions or data. Computer readable storage medium does not refer to propagated signals. A computer readable storage medium may take forms including but not limited to non volatile media and volatile media. Non volatile media may include for example optical disks magnetic disks tapes and other media. Volatile media may include for example semiconductor memories dynamic memory and other media. Common forms of a computer readable storage medium may include but are not limited to a floppy disk a flexible disk a hard disk a magnetic tape other magnetic medium an application specific integrated circuit ASIC a compact disk CD a random access memory RAM a read only memory ROM a memory chip or card a memory stick and other media from which a computer a processor or other electronic device can read.

 Data store as used herein refers to a physical or logical entity that can store data. A data store may be for example a database a table a file a list a queue a heap a memory a register and other physical repository. In different examples a data store may reside in one logical or physical entity or may be distributed between two or more logical or physical entities.

 Logic as used herein includes but is not limited to hardware firmware software in execution on a machine or combinations of each to perform a function s or an action s or to cause a function or action from another logic method or system. Logic may include a software controlled microprocessor a discrete logic e.g. ASIC an analog circuit a digital circuit a programmed logic device a memory device containing instructions and other physical devices. Logic may include one or more gates combinations of gates or other circuit components. Where multiple logical logics are described it may be possible to incorporate the multiple logical logics into one physical logic. Similarly where a single logical logic is described it may be possible to distribute that single logical logic between multiple physical logics.

To the extent that the term includes or including is employed in the detailed description or the claims it is intended to be inclusive in a manner similar to the term comprising as that term is interpreted when employed as a transitional word in a claim.

To the extent that the term or is employed in the detailed description or claims e.g. A or B it is intended to mean A or B or both . When the Applicant intends to indicate only A or B but not both then the term only A or B but not both will be employed. Thus use of the term or herein is the inclusive and not the exclusive use. See Bryan A. Garner A Dictionary of Modern Legal Usage 624 2d. Ed. 1995 .

Although the subject matter has been described in language specific to structural features or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

