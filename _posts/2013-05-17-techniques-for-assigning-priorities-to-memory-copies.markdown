---

title: Techniques for assigning priorities to memory copies
abstract: One embodiment sets forth a method for guiding the order in which a parallel processing subsystem executes memory copies. A driver creates semaphores for all but the lowest priority included in a plurality of priorities and associates one priority with each copy hardware channel included in the parallel processing subsystem. The driver then aliases prioritized streams to the copy hardware channels based on the priorities. Upon receiving a request to execute a memory copy within one of the streams, the driver inserts commands into the aliased copy hardware channel. These commands use the semaphores to direct the parallel processing subsystem to execute the memory copy based on the priority of the copy hardware channel. Advantageously, by assigning priorities to streams and, subsequently, strategically requesting memory copies within the prioritized streams, an application developer may fine-tune their software application to increase the overall processing efficiency of the software application.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09483423&OS=09483423&RS=09483423
owner: NVIDIA Corporation
number: 09483423
owner_city: Santa Clara
owner_country: US
publication_date: 20130517
---
The present invention generally relates to general purpose computing and more specifically to techniques for assigning priorities to memory copies.

A typical parallel processing subsystem that may include one or more graphics processing units GPUs is capable of very high performance using a relatively large number of small parallel execution threads on dedicated programmable hardware processing units. The specialized design of such parallel processing subsystems usually allows these subsystems to efficiently perform certain tasks such as rendering 3 D scenes or computing the product of two matrices using a high volume of concurrent computational and memory operations

To fully realize the processing capabilities of advanced parallel processing subsystems subsystem functionality may be exposed to application developers through one or more application programming interfaces APIs of calls and libraries. Among other things doing so enables application developers to tailor a software application executing on a central processing unit CPU to optimize the way parallel processing subsystems function. In one approach to developing a software application the software application developer may divide work included in the software application into streams of work components e.g. computational and memory operations . Each stream may be executed concurrently on the parallel processing subsystem. Notably work components included in different streams may run concurrently and may be interleaved. In contrast within each stream a sequence of work components executes in issue order on the parallel processing subsystem.

Different types of parallel processing subsystem resources operate on different types of work components. For example compute engines execute computational work components and copy engines execute memory copies. Parallel processing subsystems are typically configured to receive work components via hardware channels with each hardware channel dedicated to an appropriate type of work component. Acting as a liaison between the API and the host scheduler an API driver aliases the work components submitted in each stream onto one or more available hardware channels. A host scheduler included in the parallel processing subsystem receives the work components conveyed through the hardware channels and subsequently schedules the work components to execute on appropriate resources. In particular the API driver distributes memory copies included in various streams to copy hardware HW channels which are configured to convey memory copies to the host scheduler. Upon receiving memory copies via the copy HW channels the host scheduler distributes the memory copies between one or more copy engines.

In one approach to scheduling memory copies the host scheduler allocates discrete time slices to each copy HW channel. And the host scheduler executes the memory copies within each copy HW channel in issue order. In other words when executing memory copies included in a particular copy HW channel the host scheduler selects the memory copy that was issued least recently. For example suppose that the parallel processing subsystem were to include two copy HW channels and one copy engine. Further suppose that the host scheduler were to initially direct the copy engine to begin executing the memory copies included in the first copy HW channel in issue order. Finally suppose that the copy engine were not able to complete all of the memory copies included in the first copy HW channel before the time slice of the first copy HW channel expired. When the time slice expired the host scheduler would wait for any currently executing memory copy to complete and then begin executing memory copies included in the second copy HW channel in issue order . The host scheduler would continue to switch between the two copy HW channels in a similar manner.

One drawback to the above approach to scheduling memory copies is that executing memory copies in strict issue order subject to the time slice constraints of the copy HW channels may cause software applications to execute with unacceptable latency and throughput. More specifically many software applications include multiple computational operations that can execute in parallel. These computational operations often have dependencies on memory copies which would optimally execute simultaneously. However since the bandwidth to copy from system memory to GPU memory and from GPU memory to system memory is limited one or more memory copies may experience undesirable delays. In particular latency sensitive computational operations may get blocked waiting for related memory copies to execute. For example suppose that a software application were to be performing video decoding and encoding using a pipelined workflow. Further suppose that the parallel processing subsystem were to include a single copy engine. Finally suppose that the copy engine were to be occupied performing memory copies associated with the first few stages processing a fifth frame. The memory copies associated with the processing of a fourth frame by the last stage in the pipeline could be delayed. Consequently the overall latency of the fourth frame could cause jitter in frame rates.

As the foregoing illustrates what is needed in the art is a more effective technique for scheduling memory copies submitted to a parallel processing subsystems for processing.

One embodiment of the present invention sets forth a method for prioritizing a plurality of memory copy operations for execution within a parallel processing subsystem. The method includes receiving a request from a software application to execute a first memory copy operation within a first stream identifying a first copy hardware channel to which the first stream is aliased identifying a first priority that is associated with the first copy hardware channel and inserting a sequence of commands into the first copy hardware channel that causes the parallel processing subsystem to schedule the execution of the first memory copy operation according to the first priority.

Other embodiments of the present invention include without limitation a computer readable storage medium including instructions that when executed by a processing unit cause the processing unit to implement aspects of the techniques described herein as well as a system that includes different elements configured to implement aspects of the techniques described herein.

By implementing the disclosed techniques the API driver enables software applications to guide the execution order of memory copies. By exploiting this functionality to prioritize memory copies software applications may more effectively use the parallel processing subsystem resources such as copy engines and memory bandwidth. Consequently stalls and blockages associated with latency sensitive memory copies may be reduced and the performance of the software applications may be improved.

In the following description numerous specific details are set forth to provide a more thorough understanding of the present invention. However it will be apparent to one of skill in the art that the present invention may be practiced without one or more of these specific details.

In one embodiment the parallel processing subsystem incorporates circuitry optimized for graphics and video processing including for example video output circuitry and constitutes a graphics processing unit GPU . In another embodiment the parallel processing subsystem incorporates circuitry optimized for general purpose processing while preserving the underlying computational architecture described in greater detail herein. In yet another embodiment the parallel processing subsystem may be integrated with one or more other system elements in a single subsystem such as joining the memory bridge CPU and I O bridge to form a system on chip SoC .

In operation the CPU is the master processor of the computer system controlling and coordinating operations of other system components. In particular the CPU issues commands that control the operation of the parallel processing subsystem . Those commands may originate within a software application resident in the system memory and executing on the CPU . A compute unified device architecture CUDA software stack is also resident in the system memory . CUDA is a general purpose computing environment which uses the parallel processing subsystem to perform various computing tasks. The CUDA software stack is a set of programs included in the CUDA that issue and manage general purpose computations that operate on components in the parallel processing subsystem . The software application may generate requests i.e. calls for processing by the CUDA software stack to produce a desired set of results. In alternate embodiments the CUDA software stack may be replaced with any set of software programs that expose and manage parallel processing subsystem functionality. For example the CUDA software stack may be replaced with a different general purpose compute software stack or a graphics software stack. Further the CUDA software stack may be configured to inter operate with one or more additional software stacks.

It will be appreciated that the system shown herein is illustrative and that variations and modifications are possible. The connection topology including the number and arrangement of bridges the number of CPUs and the number of parallel processing subsystems may be modified as desired. For instance in some embodiments system memory is connected to CPU directly rather than through a bridge and other devices communicate with system memory via memory bridge and CPU . In other alternative topologies parallel processing subsystem is connected to I O bridge or directly to CPU rather than to memory bridge . In still other embodiments I O bridge and memory bridge might be integrated into a single chip instead of existing as one or more discrete devices. Large embodiments may include two or more CPUs and two or more parallel processing subsystems . The particular components shown herein are optional for instance any number of add in cards or peripheral devices might be supported. In some embodiments switch is eliminated and network adapter and add in cards connect directly to I O bridge .

Referring again to as well as in some embodiments some or all of PPUs in parallel processing subsystem are graphics processors with rendering pipelines that can be configured to perform various operations related to generating pixel data from graphics data supplied by CPU and or system memory via memory bridge and the second communication path interacting with local parallel processing memory which can be used as graphics memory including e.g. a conventional frame buffer to store and update pixel data delivering pixel data to display device and the like. In some embodiments parallel processing subsystem may include one or more PPUs that operate as graphics processors and one or more other PPUs that are used for general purpose computations. The PPUs may be identical or different and each PPU may have a dedicated parallel processing memory device s or no dedicated parallel processing memory device s . One or more PPUs in parallel processing subsystem may output data to display device or each PPU in parallel processing subsystem may output data to one or more display devices .

Referring back now to as well as each PPU communicates with the rest of computer system via communication path which connects to memory bridge or in one alternative embodiment directly to CPU . The connection of PPU to the rest of computer system may also be varied. In some embodiments parallel processing subsystem is implemented as an add in card that can be inserted into an expansion slot of computer system . In other embodiments the PPU can be integrated on a single chip with a bus bridge such as memory bridge or I O bridge . In still other embodiments some or all elements of PPU may be integrated on a single chip with CPU . In one embodiment communication path is a PCI Express link in which dedicated lanes are allocated to each PPU as is known in the art. Other communication paths may also be used.

PPU generates packets or other signals for transmission on communication path and also receives incoming packets or other signals from communication path directing the incoming packets to appropriate components of PPU . Notably the communication path includes one or more hardware channels not shown for transmission of work components to a host scheduler . The host scheduler assigns each work component to an appropriate resource within the parallel processing subsystem for processing. Notably the host scheduler receives computational operations via one or more compute hardware channels not shown and assigns these computational operations to a compute engine for processing. And the host scheduler receives memory copies via one or more copy hardware channels not shown and assigns these memory copies to a copy engine for processing.

As shown the PPU also includes a semaphore . The semaphore is may be used as a control by one or more existing semaphore mechanisms included in the PPU . In particular as part of the scheduling process the host scheduler is configured to create and access the semaphore . In alternate embodiments semaphores may be configured by different PPU components to facilitate a variety of tasks such as synchronization operations and enforcing exclusive access to an object. The semaphore may be included in any memory accessible by the PPU . Further the semaphore and associated semaphore mechanisms may be implemented using any protocols known in the art. For example the semaphore may be included in the local parallel processing memory and may be manipulated using atomic functions.

Each PPU advantageously implements a highly parallel processing architecture. And each PPU can be programmed to execute processing tasks relating to a wide variety of applications including but not limited to linear and nonlinear data transforms filtering of video and or audio data modeling operations e.g. applying laws of physics to determine position velocity and other attributes of objects image rendering operations e.g. tessellation shader vertex shader geometry shader and or pixel shader programs and so on. The PPUs may transfer data from system memory and or local parallel processing memories into internal on chip memory process the data and write result data back to system memory and or local parallel processing memories where such data can be accessed by other system components including CPU or another parallel processing subsystem .

A PPU may be provided with any amount of local parallel processing memory including no local memory and may use local memory and system memory in any combination. For instance a PPU can be a graphics processor in a unified memory architecture UMA embodiment. In such embodiments little or no dedicated graphics parallel processing memory would be provided and PPU would use system memory exclusively or almost exclusively. In UMA embodiments a PPU may be integrated into a bridge chip or processor chip or provided as a discrete chip with a high speed link e.g. PCI Express connecting the PPU to system memory via a bridge chip or other communication means.

As noted above any number of PPUs can be included in a parallel processing subsystem . For instance multiple PPUs can be provided on a single add in card or multiple add in cards can be connected to communication path or one or more of PPUs can be integrated into a bridge chip. PPUs in a multi PPU system may be identical to or different from one another. For instance different PPUs might have different numbers of processing cores different amounts of local parallel processing memory and so on. Where multiple PPUs are present those PPUs may be operated in parallel to process data at a higher throughput than is possible with a single PPU . Systems incorporating one or more PPUs may be implemented in a variety of configurations and form factors including desktop laptop or handheld personal computers servers workstations game consoles embedded systems and the like.

To efficiently achieve a set of results using the parallel processing subsystem including the parallel processing unit the software application passes CUDA requests to the CUDA software stack . As shown the CUDA software stack includes a CUDA runtime application programming interface API and a CUDA driver . The CUDA runtime API includes calls and libraries that expose the functionality of the parallel processing subsystem including PPU to application developers. And the CUDA driver is configured to translate the CUDA requests received by the CUDA runtime API to lower level commands that execute on components within the parallel processing subsystem . More specifically the CUDA driver may submit one or more streams not shown to the parallel processing subsystem for execution within the PPU . Each stream may include any number including zero of work components such as computational components and memory components in any combination. Within each stream the work components execute in issue order on the PPU . However work components included in different streams may run concurrently and may be interleaved.

In particular the CUDA software stack is configured to expose functionality that enables the software application to efficiently use the PPU resources to process one or more memory copies included a stream. As persons skilled in the art will understand the bandwidth to copy from the system memory to the PP memory and from the PP memory to the system memory is limited. And the PP memory provides a much higher bandwidth to the execution engines such as the compute engine included in the PPU than the system memory provides. Consequently the software application typically issues memory copy CUDA requests to copy data from the system memory to the PP memory before performing computational operations on the data using the PPU . Similarly the software application issues memory copy CUDA requests to copy data from the PP memory to the system memory before performing CPU operations on the data.

Advantageously the CUDA driver is configured to direct the PPU to execute memory copies based on stream priories established by CUDA requests received from the software application . More specifically the CUDA runtime API supports CUDA requests that enable the software application to assign stream priorities to streams. Upon receiving a particular CUDA request to assign a stream priority to a stream the CUDA driver aliases the stream to a copy hardware HW channel based on the stream priority. For example suppose that there were to be two copy HW channels a high priority copy HW channel and a low priority copy HW channel . Further suppose that the stream priority were to be a high stream priority. The CUDA driver would map the stream to the high priority copy HW channel .

The priorities of the copy HW channels are used locally by the CUDA driver and are unknown outside the CUDA driver . Further the functionality that causes the memory copies to execute based on priority is implemented within the CUDA driver and conveyed via the commands the CUDA driver inserts into the copy HW channels . To facilitate the prioritization functionality the CUDA driver creates a semaphore for all but the lowest of the priorities associated with one or more of the copy HW channels . More specifically if the CUDA driver associates a low priority 1 through a high priority P with one or more of the copy HW channels then the CUDA driver creates P 1 semaphores . Subsequently the CUDA driver associates the semaphore with the priority 2 the semaphore with the priority 3 and so on.

The CUDA runtime API may support any number of stream priorities the CUDA driver may internally support any number of priorities and the computer system may be configured with any number of copy HW channels . Further the number of stream priorities may be higher lower or equal to the number of priorities that the CUDA driver internally supports. Similarly the number of priorities that the CUDA driver internally supports may be higher lower or equal to the number of copy HW channels . In the computer system the stream priorities and the priorities that the CUDA driver internally supports both use higher numbers to represent higher priorities. However in alternate embodiments either or both of the stream priorities and the priorities that the CUDA driver internally supports may use lower numbers to represent higher priorities. The CUDA driver may perform the mappings between stream priorities the priorities the CUDA driver internally supports and the copy HW channels in any technically feasible fashion. Further in alternate embodiments the CUDA driver may implement the prioritization functionality detailed herein using synchronization mechanisms that do not require semaphores .

Upon receiving a particular CUDA request to perform a memory copy in a stream the CUDA driver processes the CUDA request before including the memory copy in the copy HW channel to which the stream is aliased. The commands that the CUDA driver inserts into the copy HW channel are received by the host scheduler . The host scheduler then schedules components within the PPU to execute these commands. In particular the host scheduler schedules the copy engine to execute memory copy commands.

In prior art approaches to scheduling memory copies the CUDA driver typically receives CUDA requests to perform memory copies within streams and distributes the memory copies to aliased copy HW channels without considering priorities. And the host scheduler allocates a time slice to each of the copy HW channels. The host scheduler selects the copy HW channel that includes the least recently received operation such as a memory copy. The host scheduler then directs the copy engine to process memory copies included in the selected copy HW channel. The copy engine processes the schedulable i.e. not constrained by dependencies or synchronization operations memory copies included in the selected copy HW channel until the time slice of the selected copy HW channel expires. The host scheduler then repeats this procedure again selecting the copy HW channel that includes the least recently received operation. In contrast to the above prior art approach to scheduling memory copies the CUDA driver is configured to direct the host scheduler to execute memory copies in a more flexible priority based order. More specifically the CUDA driver inserts semaphore operations in conjunction with memory copies into the copy HW channels to influence the behavior of the host scheduler . In particular the semaphore operations guide the host scheduler to schedule higher priority copy HW channels in preference to lower priority copy HW channels and to circumvent the time slice constraints.

As previously disclosed the CUDA driver creates and associates one semaphore with each of the priorities associated with one or more copy HW channels except for the lowest priority. The CUDA driver uses the semaphores to track the number of unexecuted memory copies of each priority included in the copy HW channels . For example if the semaphore associated with priority 2 were 5 then the copy HW channels associated with priority 2 would include exactly 5 unexecuted and schedulable memory copies. And if the semaphore associated with priority 2 were 0 then the copy HW channels associated with priority 2 would include no unexecuted and schedulable memory copies. The CUDA driver uses semaphore operations to maintain the correct count of unexecuted memory copies. And the CUDA driver uses semaphore operations to block the execution of memory copies included in copy HW channels associated with priorities lower than the priorities of non zero semaphores . As persons skilled in the art will understand the copy engine executes memory copies and the host scheduler executes semaphore operations. Further the host scheduler coordinates the memory copies and the semaphore operations.

In operation before inserting a memory copy M100 into a selected copy HW channel the CUDA driver inserts a semaphore acquire 0 operation for each semaphore associated with a priority higher than the priority of the selected copy HW channel . As persons skilled in the art will understand these semaphore operations ensure that all schedulable memory operations included in higher priority copy HW channels are executed before memory copy M100. For example suppose that the CUDA driver receives a memory copy X and inserts the memory copy X into the lowest priority copy HW channel . Further suppose that the CUDA driver then receives a memory copy Y and inserts the memory copy Y into a higher priority copy HW channel . Finally suppose that the copy engine had not started to execute the memory copy X. The semaphore operations would ensure that the memory copy Y would be executed before the memory copy X. 

After inserting the semaphore acquire 0 operations associated with the memory copy M100 the CUDA driver inserts either one or three additional commands into the selected copy HW channel . If the priority of the selected copy HW channel is not the lowest priority then the CUDA driver inserts a semaphore increment operation for the semaphore associated with the priority of the selected copy HW channel . Regardless of the priority of the selected copy HW channel the CUDA driver then inserts the memory copy M100 into the selected copy HW channel . And if the priority of the selected copy HW channel is not the lowest priority then the CUDA driver inserts a semaphore decrement operation for the semaphore associated with the priority of the selected copy HW channel . By bracketing the memory copy M100 in this fashion the CUDA driver updates the semaphore associated with the priority of the selected copy HW channel to reflect the execution status of the memory copy M100 in the count of unexecuted memory copies.

For example suppose that the PPU were to include one copy engine and two copy HW channels . Further suppose that the CUDA driver were to designate one copy HW channel for low priority memory copies and one copy HW channel for high priority memory copies. To request that the PPU perform a high priority memory copy the CUDA driver would insert three commands into the high priority copy HW channel . First the CUDA driver would insert a command to increment the semaphore associated with the high priority copy HW channel . Second the CUDA driver would insert a command to perform the high priority memory copy. Third the CUDA driver would insert a command to decrement the semaphore associated with the high priority copy HW channel . And to request that the PPU perform a low priority memory copy the CUDA driver would insert two commands into the low priority copy HW channel . First the CUDA driver would insert a command to acquire a value of 0 for the semaphore associated with the high priority copy HW channel . Second the CUDA driver would insert a command to execute the low priority memory copy. As disclosed previously herein this approach can be extended to influence execution order between memory copies included in any number of copy HW channels .

In operation suppose that the host scheduler were to receive one or more memory copies in a higher priority copy HW channel while the copy engine were executing a memory copy included in a lower priority copy HW channel . The semaphore operations inserted by the CUDA driver would cause the host scheduler to stop submitting memory copies included in the lower priority copy HW channel to the copy engine irrespective of the time slice. Further the semaphore operations would cause the host scheduler to begin submitting memory copies included the higher priority copy HW channel to the copy engine . And suppose that the time slice of the higher priority copy HW channel were to expire while there were unexecuted and schedulable memory copies included the higher priority copy HW channel . The host scheduler would consult the operations included in the lower priority copy HW channels and discover that the CUDA driver had inserted semaphore operations that blocked the execution of the lower priority copy HW channels . Consequently the host scheduler would resume executing the memory copies included in the higher priority copy HW channel .

Advantageously by guiding the execution order of memory copies the CUDA driver enables application developers to increase the performance of their software application by decreasing latency and increasing throughput. For instance suppose that in a pipelined video decoding and encoding algorithm an application developer were to assign memory copies that are more sensitive to latency to a high priority stream and memory copies that are less sensitive to latency to a low priority stream. Further suppose that the PPU were to include the low priority copy HW channel and the high priority copy HW channel . The CUDA driver would insert memory copies included in the low priority stream in conjunction with synchronization operations into the low priority copy HW channel . Similarly the CUDA driver would insert memory copies included in the high priority stream in conjunction with synchronization operations into the high priority copy HW channel . The commands inserted by CUDA driver into the copy HW channel would influence the copy engine to execute the more latency sensitive memory copies in preference to the less latency sensitive memory copies. Consequently the overall latency of individual frames could be decreased and therefore the likelihood of jitter in the frame rates could also be decreased. As this example illustrates the disclosed techniques enable application developers to more effectively use the PPU resources such as copy engines and memory bandwidth to increase the performance of software applications .

Referring again to as well as the parallel processing subsystem includes PPU . And PPU includes the host scheduler and a single copy engine . Referring back now to the first CUDA request cudaMemCpy Mlow1 requests that a memory copy accessing Mlow1 be included in a low priority stream not shown . Upon receiving CUDA request the CUDA driver inserts two commands into the copy HW channel Clow to which the low priority stream is aliased. The first command sem acq Shigh 0 directs the host scheduler to not process subsequent operations included in the copy HW channel Clow until the semaphore Shigh is zero. Because there are no memory operations included in the copy HW channel Chigh the semaphore Shigh is zero. Consequently the host scheduler processes the second command included in the copy HW channel Clow mem cpy Mlow1 and directs the copy engine to perform the memory copy accessing Mlow1.

While the copy engine is performing the memory copy accessing Mlow1 the CUDA driver receives the remaining CUDA requests through . The second CUDA request cudaMemCpy Mlow2 requests that a memory copy accessing Mlow2 be included in the low priority stream. Upon receiving CUDA request the CUDA driver inserts two commands into the copy HW channel Clow to which the low priority stream is aliased. Again the first command sem acq Shigh 0 directs the host scheduler to not process subsequent operations included in the copy HW channel Clow until the semaphore Shigh is zero. The second command mem cpy Mlow2 directs the host scheduler to process the memory copy accessing Mlow 2. Similarly the third CUDA request cudaMemCpy Mlow3 requests that a memory copy accessing Mlow3 be included in the low priority stream. Upon receiving CUDA request the CUDA driver inserts two more commands into copy HW channel Clow sem acq Shigh 0 and mem cpy Mlow3 . 

The fourth CUDA request cudaMemCpy Mhigh1 requests that a memory copy accessing Mhigh1 be included in a high priority stream. Upon receiving CUDA request the CUDA driver inserts three commands into the copy HW channel Chigh to which the high priority stream is aliased. The first command sem inc Shigh directs the host scheduler to increment the semaphore Shigh. Since the host scheduler does not require the copy engine to execute this command the host scheduler immediately increments the semaphore Shigh. Consequently the value of semaphore sHigh becomes 1. The second command mem cpy Mhigh1 directs the host scheduler to process the memory copy accessing Mhigh1. However the copy engine is still executing the memory copy accessing Mlow1 so the host scheduler cannot immediately process this command or any subsequent commands included in the copy HW channel Chigh . The third command sem dec Shigh directs the host scheduler to decrement the semaphore Shigh. Similarly the fifth CUDA request cudaMemCpy Mhigh2 causes the CUDA driver to insert three more commands into the copy HW channel Chigh sem inc Shigh mem cpy Mlhigh2 and sem dec Shigh . 

After the CUDA driver processes the final CUDA request the copy engine finishes performing the memory copy accessing Mlow1. The host scheduler then processes the next command included in the copy HW channel Clow. The next command is mem acq Shigh 0 . Since the semaphore Shigh is 1 the host scheduler suspends executing commands included in copy HW channel Clow until the semaphore Shigh is 0. The host scheduler then switches to the copy HW channel Chigh. The next unexecuted and schedulable command included in the copy HW channel Chigh is the command mem cpy Mhigh1 . Since the copy engine is now available the host scheduler directs the copy engine to perform the memory copy accessing Mhigh1.

The time slice not shown associated with the copy HW channel Chigh expires while the copy engine is performing the memory copy accessing Mhigh1. Consequently when the copy engine finishes performing the memory copy accessing Mhigh1 the host scheduler switches to the copy HW channel Clow. Since the semaphore Shigh is still 1 the host scheduler continues to suspend executing commands included in copy HW channel Clow. Instead the host scheduler switches back to the copy HW channel Chigh and resumes executing the commands included in the copy HW channel Chigh. The host scheduler executes the third and fourth commands included in the copy HW channel Chigh sem dec Shigh and sem inc Shigh causing the semaphore Shigh to transition to 0 and then immediately back to 1. The next unexecuted and schedulable command included in the copy HW channel Chigh is the command mem cpy Mhigh2 . Since the copy engine is available the host scheduler directs the copy engine to perform the memory copy accessing Mhigh2.

The copy engine finishes performing the memory copy accessing Mhigh2 before the time slice associated with the copy HW channel Chigh expires. Consequently the host scheduler processes the final command included in the copy HW channel sem dec Shigh causing the semaphore Shigh to return to 0. Since there are no remaining commands included in the copy HW channel Chigh the host scheduler switches to the copy HW channel Clow. Because the semaphore Shigh is now 0 the host scheduler acquires the value of the semaphore Shigh of 0 and resumes executing commands included in the copy HW channel Clow. 

Upon receiving a CUDA request cudaMemCpy Mx to perform a memory copy accessing Mx within the stream associated with priority N the CUDA driver inserts two sets of commands into the copy HW channel N Cn. The first set of commands ensures that the memory copy accessing Mx does not execute until there are no unexecuted memory copies included in any higher priority copy HW channels . Notably the first set of commands directs the host scheduler to obtain a value of 0 on each of the semaphores N 1 through P.

The second set of commands ensures that the memory copy accessing Mx executes before any unexecuted memory copies included in any lower priority copy HW channels . More specifically the second set of commands causes the host scheduler to sequentially increment the semaphore N direct the copy engine to execute the memory copy accessing Mx and finally decrement the semaphore N.

As shown a method begins at step where the CUDA runtime API passes a CUDA request to the CUDA driver to execute a memory copy within a specified stream. At step the CUDA driver selects the copy HW channel to which the specified stream is aliased. At step the CUDA driver sets a current priority to the priority of the selected copy HW channel . At step if the CUDA driver determines that the current priority is not the highest priority associated with any of the copy HW channels then the method proceeds to step . At step the CUDA driver increments the current priority. As previously noted in this embodiment higher priorities are associated with higher numbers and one is the lowest priority. In alternate embodiments higher priorities could be associated with lower numbers and consequently the implementation details would be altered accordingly. At step the CUDA driver inserts a semaphore acquire 0 command for the semaphore associated with the current priority into the selected HW channel and the method returns to step .

The CUDA driver cycles through steps through inserting commands into the selected copy HW channel until the CUDA driver has inserted commands associated with each of the priorities that are higher than the priority of the selected copy HW channel . In operation these commands direct the host scheduler to block operations included in the selected copy HW channel until the memory copies included in the copy HW channels associated with higher priorities have been executed. If at step the CUDA driver determines that the current priority is the highest priority associated with any of the copy HW channels then the method proceeds to step .

At step if the CUDA driver determines that the priority of the selected copy HW channels is not the lowest priority of any of the copy HW channels then the method proceeds to step . At step the CUDA driver selects the semaphore associated with the priority of the selected copy HW channel . At step the CUDA driver inserts a semaphore increment command for the selected semaphore into the selected copy HW channel . Advantageously in conjunction with commands included in the lower priority copy HW channels this command increases the number of blockages prohibiting the execution of memory copies included in the lower priority copy HW channels . At step the CUDA driver inserts the memory copy command into the selected copy HW channel . At step the CUDA driver inserts a semaphore decrement command for the selected semaphore into the selected copy HW channel and the method terminates. In conjunction with commands included in the lower priority copy HW channels this command reduces the number of blockages prohibiting the execution of memory copies included in the lower priority copy HW channels .

If at step the priority of the selected copy HW channel is the lowest priority of any of the copy HW channels then the method proceeds to step . At step the CUDA driver inserts the memory copy command into the selected copy HW channel and the method terminates. In alternate embodiments the CUDA driver may guide the ordering of memory copies in any technically feasible manner. For instance the CUDA driver may use other synchronization mechanisms to ensure that memory copies included in higher priority copy HW channels are executed before memory copies included in lower priority copy HW channels.

In sum memory copies may be more effectively implemented by dividing the memory copies between multiple HW channels based on priorities and using semaphores to guide the scheduling of the memory copies. In one embodiment a CUDA driver assigns a priority to each copy HW channel. The CUDA driver then aliases streams to the copy HW channels based on the priorities of the streams in conjunction with the priorities of the copy HW channels. Subsequently upon receiving a request to perform a particular memory copy in a particular stream the CUDA driver selects the copy HW channel to which the stream is aliased. Based on the priority of the selected copy HW channel the CUDA driver inserts one or more semaphore operations into the selected copy HW channel in conjunction with the request to perform the particular memory copy. More specifically the CUDA driver uses semaphores to block memory copies included in lower priority copy HW channels from executing before memory copies included in higher priority copy HW channels. In particular the semaphore commands may instruct the host scheduler to switch from a lower priority copy HW channel to a higher priority HW channel before the time slice associated with the lower priority copy HW channel expires. Similarly the semaphore commands may instruct the host scheduler to continuously execute memory copies in a higher priority copy HW channel even after the time slice associated with the higher priority copy HW channel expires.

Advantageously by guiding the execution order of memory copies the CUDA driver enables application developers to optimally use parallel processing subsystem resources to increase the performance of their software applications. More specifically application developers may tailor software applications by strategically assigning memory copies to prioritized streams based the sensitivity of the memory copies to latency. And the CUDA driver uses semaphores to cause the host scheduler to execute memory copies in priority order overriding strict issue order and time slice constraints. Consequently the disclosed techniques enable application developers to more effectively use the parallel processing subsystem resources such as copy engines and memory bandwidth to reduce stalls and blockages associated with latency sensitive memory copies. And reducing stalls and blockages may increase overall throughput and decrease the execution time of the software application.

While the foregoing is directed to embodiments of the present invention other and further embodiments of the invention may be devised without departing from the basic scope thereof. For example aspects of the present invention may be implemented in hardware or software or in a combination of hardware and software. One embodiment of the invention may be implemented as a program product for use with a computer system. The program s of the program product define functions of the embodiments including the methods described herein and can be contained on a variety of computer readable storage media. Illustrative computer readable storage media include but are not limited to i non writable storage media e.g. read only memory devices within a computer such as CD ROM disks readable by a CD ROM drive flash memory ROM chips or any type of solid state non volatile semiconductor memory on which information is permanently stored and ii writable storage media e.g. floppy disks within a diskette drive or hard disk drive or any type of solid state random access semiconductor memory on which alterable information is stored.

The invention has been described above with reference to specific embodiments. Persons of ordinary skill in the art however will understand that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. The foregoing description and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense.

