---

title: Managed reliability of data storage
abstract: Systems and methods are provided herein that can facilitate the managed reliability of data storage, including management of device remanufacturing and masking from an operating system a failure or predicted failure of a device running on a computer or a networked cluster of computers having access to the device. The systems and methods may facilitate removal of a device by coordinating among computers or controllers in a network cluster the logical removal of a device. At a later time, a coordinated logical re-introduction of the device to the systems or computers from which the device was logically removed can be performed. This can be accomplished via a virtualization system that may include a device function driver (DFD), a device virtualization bus driver (DVBD), and a device management service (DMS).
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09454443&OS=09454443&RS=09454443
owner: SEAGATE TECHNOLOGY LLC
number: 09454443
owner_city: Cupertino
owner_country: US
publication_date: 20130314
---
This application relates to systems devices and methods for managed reliability of data storage devices and systems.

In the following detailed description of the embodiments reference is made to the accompanying drawings which form a part hereof and in which are shown by way of illustration of specific embodiments. It is to be understood that other embodiments may be utilized and structural changes may be made without departing from the scope of the present disclosure.

In some embodiments the systems and methods may facilitate the managed reliability of data storage including management of device remanufacturing and masking from an operating system a failure or predicted failure of a device running on a computer or a networked cluster of computers having access to the device. If there are indications that a device is failing or about to fail the systems may respond to operating system requests for the health status of the device by returning an indication that no failure is predicted. If deemed appropriate the device may be taken offline functionally for servicing without interrupting the operating system or without having to power down the computer or networked cluster of computers having access to the device.

In other embodiments the systems and methods may facilitate removal of a device by coordinating among computers or controllers in a network cluster the logical removal of a device such as for remanufacturing. At a later time such as when remanufacturing is complete the system may coordinate the logical re introduction of the device to the systems or computers from which the device was logically removed. Further once a device has been logically removed from a system it could also be physically removed if desired.

In some embodiments the systems and methods for managed reliability of data storage can include a device function driver DFD a device virtualization bus driver DVBD and a device management service DMS for facilitating the management of devices in a system. Detailed embodiments and variations of systems and methods utilizing a DFD DVBD and a DMS are described below with respect to .

Each node may be a computing device connectable to a network or to another computing device. The computing device could be a server a desktop computer a laptop computer a tablet computer a phone another electronic device a communications device any other structure having a processing capability or any combination thereof. Each node may have a unique network address such as a Data Link Control DLC address or a Media Access Control MAC address.

In some embodiments each node may comprise a server having at least one computer processor as well as a computer memory that can include volatile random access memory RAM and some form of non volatile computer memory. The computer memory could include electrically erasable programmable read only memory also known as EEPROM volatile solid state memory such as Dynamic Random Access Memory also known as DRAM non volatile solid state memory such as Flash memory a hard disk drive an optical disk drive or any combination thereof. The computer memory may be connected through a system bus to the processor and to other system components such as a storage controller .

Each node may further comprise an operating system OS a device function driver DFD a device virtualization bus driver DVBD and a device management service DMS . The OS DFD DVBD and DMS may comprise computer program instructions stored in computer memory and processed by central processing unit CPU to perform the methods and functions described in embodiments herein.

The environments may further comprise an array of data storage devices DSDs . The DSDs may include any device that can transmit or receive data over an interface . For example one or more of the DSDs may be any device capable of data storage such as a hard drive or any other data storage device. The array of DSDs may be various types and varied capacities of data storage devices or the DSDs could include data storage devices that are all of the same type and storage capacity.

Each of the DSDs may be connected to one or more nodes . The nodes may share read access write access or any combination thereof to one or more of the DSDs . For example in some embodiments the array of DSDs may comprise 24 data storage devices. Any combination of the DSDs may be read from or written to by any combination of the nodes via an interface . Stated another way the nodes may transmit data to or receive data from one or more of the DSDs across the interface . The interface may transfer data between the nodes and the DSDs .

The interface can be any type of interface capable of transmitting data between devices. The interface may be in the form of a computer bus interface standard or other interface protocol for transferring data between systems or components such as USB 2.0 USB 3.0 IEEE 1394 SATA SAS Fiber Channel or Ethernet. Other embodiments of the interface may include wireless interfaces such as IEEE 802.11 protocols Bluetooth infrared cellular or satellite mobile telecommunication protocols. Hardware necessary to implement the interface may be added to the nodes and the DSDs .

In one or more embodiments the interface may comprise a bridged interface not shown between the nodes and the DSDs . The bridged interface may comprise a node interface not shown an interface bridge not shown and a device interface not shown . The nodes may transmit data to or receive data from the DSDs across the bridged interface.

In a particular embodiment data may be transferred between the nodes and the DSDs by way of the node interface the interface bridge and the device interface. The node interface and the device interface can represent any means of transmitting data between devices. The node interface and the device interface may be a computer bus interface standard or other interface protocol for transferring data between systems or components such as USB 2.0 USB 3.0 IEEE 1394 Compact Flash SATA eSATA PATA SCSI SAS Fiber Channel PS 2 serial cable HDMI or Ethernet. Other embodiments of the interfaces may include wireless interfaces such as IEEE 802.11 protocols Bluetooth infrared cellular or satellite mobile telecommunication protocols. Hardware necessary to implement the node interface or the device interface may be added to the nodes and the DSDs .

The interface bridge may comprise any method of connecting the node interface and the device interface such as a male to male adapter an interface protocol adapter such as USB to SATA a device such as a server a router a personal computer a drive capable of integration into a daisy chain network arrangement or any type of network. The interface bridge may function as a wireless to wired interface protocol adapter so that one of the node interface or the device interface may be a wired interface protocol and the other may be a wireless interface protocol.

A storage controller may be able to communicate with any or all of the DSDs via an interface and may handle the movement of data as well as other functions such as RAID. Moreover the storage controller of a first node may be capable of managing the movement of data with respect to any or all of the DSDs via the interface of a second node and vice versa.

In some embodiments each of the DSDs may only have one node serving as the DSD s management owner at any given time. The management owner may be responsible for monitoring the health of the DSD and for initiating remanufacturing if the DSD s health is degraded.

In one particular embodiment the nodes and the array of DSDs may be disposed within a single shared enclosure . One or more other devices may have access to the nodes via a network connection . Devices may not know to which node or it is connecting at any given time. Nodes and can be formed as a cluster and node can be a backup for node and vise versa. Each device may be a computer such as a server a desktop computer a laptop computer a tablet computer another electronic device a communications device or the like or any combination thereof.

The virtualization system may comprise a device function driver DFD a device virtualization bus driver DVBD and a device management service DMS . In various embodiments the DFD DVBD and DMS may be software modules comprising computer program code. Referring also to computer program code for the software modules of some embodiments may reside in the node storage controller and in the DSDs . Various functions in implementing embodiments of the virtualization system may be resident in different portions of the environment or may be downloaded to various components as needed. Such downloading may comprise reading computer program code from memory of a storage controller or DSD storing the computer program code and executing the computer program code.

The DFD may be designed for use with a specific operating system . When one or more of the DSDs is connected to a first node via an interface the hardware discovery process e.g. a process implemented by a system configured in accordance with the Plug and Play standard may cause the DFD to be loaded. In some embodiments the DFD may be loaded in response to the DSD s being physically connected to the node and enumerated with a hardware ID. The DFD may create a functional device object FDO on each physical device object PDO not shown representing a physical DSD .

When the current node first node is the management owner of one or more of the DSDs the DFD may be responsible for monitoring the health of the DSD and managing remanufacturing operations on the DSD if deemed necessary.

The DFD may also be responsible for spoofing the DSD s storage capacity to implement trivial overprovisioning. Overprovisioning is the practice of providing more storage capacity in the physical DSD than is visible to the operating system so that limited amounts of remanufacturing do not result in a loss net storage capacity. Trivial overprovisioning is overprovisioning implemented on a per DSD basis by hiding a pre defined percentage of a DSD s storage capacity from the operating system . For example an implementation of trivial overprovisioning of 25 means that a DSD with an initial storage capacity of 100 GB would be reported to the operating system by the DFD as having a storage capacity of 75 GB.

The DVBD may be a root enumerated bus driver that communicates with the DFD and creates a physical device object PDO for each FDO created by the DFD . The PDO may represent a virtual DSD to the operating system. Each virtual DSD may be reported to the operating system as a generic DSD type and may be subsequently controlled by the operating system s corresponding DSD class driver. The DVBD may be responsible for deleting the PDO representing the virtual DSD when the associated physical DSD undergoes remanufacturing. Similarly when remanufacturing is complete the DVBD may be responsible for re creating the PDO representing the virtual DSD to the operating system .

The DMS may be a user mode service that is responsible for tracking how physical DSDs and their corresponding virtual DSDs are configured into one or more abstract DSD groups via the operating system s logical volume manager LVM . The DMS may be responsible for logically removing individual virtual DSDs from the abstract DSD group s before the corresponding physical DSDs undergo remanufacturing. Each virtual DSD may be represented by one or more unnamed partition PDOs comprising a resource pool at the LVM. A virtual storage port driver may allow for selection of one or more of the unnamed partition PDOs from the resource pool to be grouped into an abstract DSD group . Thus an abstract DSD group may be one or more virtual DSDs and therefore one or more physical DSDs represented as a single virtual DSD to a user or administrator of the OS .

The DMS may communicate with the DFD to gather information and learn when a DSD owned by the current node first node will undergo remanufacturing. The DMS of a management owner first node may further communicate with the DMS of a non management owner second node and any other nodes in a cluster to inform the second node of the intention to remanufacture the DSD or to inform the second node of the completion of remanufacturing operations on the DSD. Similarly the DMS of a non management owner first node may communicate with the DMS of a management owner second node such that the non management owner first node may become aware of the intention to remanufacture the DSD or become aware of the completion of remanufacturing operations on the DSD.

In a particular embodiment the DFD of the first node may determine that one of the DSDs requires remanufacturing at . Determining whether the DSD requires remanufacturing may be based on certain characteristics exhibited by the DSD indicating that the DSD is either failing or predicted to fail.

The DFD of the first node may communicate its intent to remanufacture the DSD to the DMS of the first node at . Communication between the DFD of the first node and the DMS of the first node may be carried out using an input and output control IOCTL interface. In some embodiments the IOCTL interface may allow the DMS to send a control message. The control message may include a control code representing an operation for the DFD to perform. For example the DMS may send a control message asking the DFD to return information about the corresponding DSD.

In response to being informed of the DFD s intent to remanufacture the DSD the DMS of the first node may logically remove the DSD from its abstract DSD group at . The DMS of the first node may further broadcast the intent to remove the DSD to the DMS of the second node at . The second node may be using the DSD as a non management owner in its abstract DSD group . Although the illustrative embodiment shown by is described herein as comprising two nodes first node and second node other embodiments may comprise a cluster having more than two nodes. The various nodes and their respective DMS may communicate with each other via a network connection .

The DMS of the second node upon being informed of the intent to remanufacture the DSD may in turn inform the DFD of the second node of the intent to remanufacture the DSD. It is to be understood that although communication among the DFD DVBD DMS DSD s OS and software applications or modules of computer program code may be described herein as certain information provided by the sender to the recipient the communication may include any type of information that would achieve the desired outcome. For example by relaying via various interfaces the intent to remanufacture the DSD the desired outcome is for appropriate measures to be taken to prepare for the remanufacturing. Therefore this series of communication may comprise for example a request for the DFD of the second node to disable its device interface with the DVBD of the second node .

The DFD of the second node may notify the DVBD of the second node of the interface departure at . In response the DVBD of the second node may remove the PDO representing the virtual DSD associated with the physical DSD thereby logically removing the virtual DSD .

The DFD of the second node may communicate with the DMS of the second node to indicate success of the logical removal of the DSD at . The DMS of the second node use one or more application programming interfaces APIs to logically remove the virtual DSD from its abstract DSD group .

The DMS of the second node may communicate with the DMS of the first node to indicate successful removal of the DSD from the second node at . Upon receiving this information from the DMS of the second node the DMS of the first node may relay to the DFD of the first node of an indication that the DFD may thereafter properly cause the virtual DSD to be logically removed from the first node . The DFD of the first node may disable the device interface with the DVBD . The DFD of the first node notifies the DVBD of the first node of the interface departure at . In response the DVBD of the first node may logically remove the PDO representing the virtual DSD associated with the physical DSD.

The DFD may then initiate remanufacturing operations on the DSD at . The DFD may enable its interface with the DVBD when remanufacturing is complete thereby commencing a reversal of the actions described above in order to logically re introduce the virtual DSD to the abstract DSD group of the first node and the abstract DSD group of the second node .

Nodes and interfaces between a node and a DSD have been described above with reference to . In some embodiments one or more of the data storage devices DSDs may be connected to a node via an interface . A storage port miniport driver not shown and or a storage port driver not shown may enumerate the DSD with a certain manufacturer hardware ID via an operating system hardware discovery or Plug and Play process. The DFD may be loaded via the operating system hardware discovery process in response to a storage port miniport driver and or a storage port driver enumerating the DSD and in response to the port driver creating a physical device object PDO not shown representing the physical DSD.

The DFD may create a functional device object FDO for each port driver reported PDO having a certain hardware ID at . Referring also to the DFD may register and enable a device interface for each FDO that it creates. The device interface may be used to notify the device virtualization bus driver DVBD of the arrival of a new DSD. The device interface may also be used to inform the DVBD that a DSD is pending the initiation of remanufacturing or has completed remanufacturing. To inform the DVBD that a DSD is intended to be remanufactured the DFD may disable the device interface . Conversely the DFD may inform the DVBD that a DSD has completed remanufacturing by enabling the device interface .

In some embodiments the method may further involve the DFD deciding whether the current node is the management owner of the DSD at . The DFD may make this management owner determination for each FDO created. Each DSD may only have one node serving as the DSD s management owner at any given time.

To determine whether the current node is the management owner of the DSD the DFD may intercept the completion status of periodic persistent reserve out operations hereinafter referred to as reserve request sent by operating system software to the each DSD that is under the current node s control. Management ownership status be acquired by a node based on the first success indicating response i.e. a successful reserve request intercepted by the DFD . Ownership of the DSD may change and the DFD may track the results of subsequent reserve requests. In some embodiments any time a reserve request is successful the current node may become management owner of the DSD to which the reserve request was directed.

The DFD may further provide a standard upper edge device input and output control IOCTL interface used to receive input and output I O operations destined to a DSD. The DVBD may use this interface to forward to the DFD I O operations via IOCTL messages from the operating system s corresponding device class driver s . In turn the DFD may provide a lower edge IOCTL interface to forward to the underlying PDO representing the physical DSD certain I O operations the DFD receives from the DVBD .

Of the current node is determined to be the management owner of the DSD then the DFD may monitor the health of the DSD at . The management owner of the DSD may also be responsible for ensuring that the operating system only perceives the DSD as healthy. By intercepting the results of health status inquiring IOCTL requests periodically sent by the operating system s corresponding device class driver s to the underlying storage port miniport driver and or the storage port driver the DFD may determine whether failure is predicted at . For example the device class driver s may periodically send an IOCTL request message to the underlying storage port miniport driver and or the storage port driver inquiring whether the DSD is predicted to fail.

The DFD may acquire and monitor one or more DSD parameters. DSD parameters may be acquired in a periodic or continuous manner. Monitoring of parameters may include comparison with optimal usage models or other device parameters and thresholds and may include calculation of rates trends and other representations of system and component activity and condition. Monitoring may employ elements of Self Monitoring and Reporting Technology SMART . SMART is an industry adopted standardized specification for failure warnings which are based on monitoring for excessive internal drive errors such as bit read errors and track seek errors. SMART employs a failure warning algorithm running in a processor that checks whether error rates exceed a threshold value and if such condition exists a warning is sent to the node s CPU . While SMART is a reactive approach to possible drive failure aspects of SMART may be employed in connection with parameters acquired as part of the monitoring the health of the DSD.

In some embodiments the acquired DSD parameters may comprise at least one parameter selected from the following group bit error rate number of uncorrectable errors number of grown in errors number of bad sectors number of spare sectors used number of failed reads temperature humidity other environmental conditions amplitude of a read signal quality of a read signal percent of total capacity used number of reads and number of writes. However any parameter suitable evaluating the health of the DSD may be acquired. The parameter s may be monitored per sector track zone or DSD or any combination thereof.

A current health state value may be calculated based on the acquired DSD parameter s . The current health state value may be compared to a failure indicating state value representing a threshold value above which the DSD is predicted to fail. Calculation of the failure indicating state value may also be based on the acquired DSD parameter s . The failure indicating state value may be predetermined or it may be determined on the fly. In one particular embodiment the current health state value may be compared with the failure indicating state value. Failure may be predicted if the current health state value exceeds the failure indicating state value.

If failure is predicted and the current node is management owner of the DSD then the DFD may note the predicted failure but return to the operating system an indication that no failure is predicted at thereby ensuring that the operating system only sees the DSD as healthy. In some examples the DFD may note the predicted failure via a registry entry for each DSD which can have an entry for its status indexed by each DSD s serial number. If failure is not predicted then the DFD may continue to monitor the health of the DSD if the current node is the DSD s management owner.

The periodic health status inquiring IOCTL request messages may return a health indicator to the operating system . The health indicator may indicate that the DSD to which the health status inquiring IOCTL request message was directed is either healthy i.e. no failure is predicted or unhealthy i.e. failure is predicted . The health indicator may indicate a failure is predicted when the current health state value exceeds the failure indicating state value. The DFD may intercept a first status of the health indicator that indicates a failure is predicted and modify the first status to reflect a second status of the health indicator to indicate that no failure is predicted. The second status may be reported to the operating system . In this manner the DFD may mask from the operating system the predicted failure of the DSD.

The DFD of the management owner may also be responsible for determining whether to initiate remanufacturing operations if the DSD s health is degraded i.e. failure is predicted at . A service state value may also be determined based on the acquired DSD parameter s . The service state value may represent a threshold value above which the DSD is determined to require remanufacturing servicing . The current state value may be compared to the service state value. In one particular embodiment remanufacturing may be initiated in response to the current health state value exceeding service state value.

If the DFD determines that remanufacturing should be initiated on the DSD then the DFD may inform the DMS of its intent to remanufacture the DSD at . To communicate with the DMS the DFD may provide a device input and output control IOCTL interface . In some embodiments the IOCTL interface may allow the DMS to send an IOCTL request message including a control code to the DFD . The control code may represent an operation for the DFD to perform. For example the DMS may send a control code asking the DFD to return information about the corresponding DSD.

In one particular embodiment an IOCTL code used for communicating with the DFD may take the form of IOCTL  DFD NAME  MESSAGE or a similar variation such as IOCTL DFD MESSAGE. Each IOCTL code may be used to send information from the caller to the DFD and to allow the DFD to provide information or notifications to the caller.

The IOCTL DFD MESSAGE control code may use the same message buffer for both input and output. shows a diagram of an illustrative embodiment of a message . The message may include fields providing information regarding the version and the size of the message . This may be done to allow DFD not to have to create a message itself. The fields InCommand and OutCommand each may encode a command. Incommand may be the command being sent from user mode to the DFD . OutCommand may be the command being sent from the DFD to the user mode requestor. Each command may have an associated sequence number which may be used by the requestor to match a command with an associated response. The DMS may set the InSequence and the DFD may preserve the InSequence set by the DMS . The DFD may set the OutSequence .

On some embodiments the IOCTL DFD MESSAGE control code may be designed to work as part of an inverted call system between the DMS and the DFD . The DMS may initially send an IOCTL DFD MESSAGE to the DFD with an InCommand field set to a command indicating No Operation e.g. DFD NAME  COMMAND NOP or the like. The DFD may hold the No Operation command until either 1 the DFD receives a new IOCTL DFD MESSAGE or 2 the DFD wishes to provide information to the DMS .

In the event the DFD receives a new IOCTL DFD MESSAGE the DFD may set the OutCommand field to DFD COMMAND NOP and set the Argument field to a parameter indicating a success of the command e.g. STATUS SUCCESS .

In the event the DFD wishes to provide a notification to the DMS the DFD may set the OutCommand field to the corresponding command indicating the subject matter of the notification. For example the DFD may wish to inform the DMS that a DSD owned by the current node is scheduled to begin remanufacturing. The DFD may fill the OutCommand field with DFD COMMAND REMANUFACTURE set the OutSequence field to a unique value and set the Argument field to a parameter indicating success of the command e.g. STATUS SUCCESS .

Referring back to the device function driver DFD may inform the device management service DMS of the DFD s intent to initiate remanufacturing of the data storage device DSD . The DFD may wait for confirmation from the DMS that remanufacturing may begin at . The DMS of the first node may communicate to the DMS of the second node the intent to initiate remanufacturing so that that the second node may take measures to ensure that the DSD may be remanufactured while remaining physically connected to the nodes .

Upon receiving confirmation from the DMS that the DFD may proceed to initiate remanufacturing the DFD may note in the registry that the first node is the owner of the DSD undergoing remanufacturing at . This registry information may be used in the event a reboot occurs while remanufacturing is in progress. To inform the device virtualization bus driver DVBD that a DSD is pending the initiation of remanufacturing the DFD may disable the device interface at . In response to the device interface being disabled the DVBD may delete the associated PDO thereby logically removing the associated virtual DSD from the node . The DFD may then initiate remanufacturing on the DSD at .

In some embodiments the DFD may monitor the progress of the DSD during the remanufacturing process at . When remanufacturing is complete the DFD may remove the entry it previously created in the registry at .

The DFD may inform the DMS that the DSD has completed remanufacturing and or is available for use at . Further the DFD may enable the device interface triggering the DVBD to logically re introduce the associated virtual DSD to the node .

Returning to step in the event the DFD determines that the current node is not the management owner of the DSD the DFD may determine whether a DSD owned by any other node is expected to start remanufacturing at . The DFD may make this determination based on communication with the DMS via the device input and output control IOCTL interface . The DMS may be in communication with other nodes respective DMS via a status interface .

If any other node is expected to start remanufacturing the DFD may disable the device interface at . In response to the device interface being disabled the DVBD may delete the associated PDO thereby logically removing the associated virtual DSD from the current node .

The DFD may wait for the DMS to confirm that remanufacturing of a DSD owned by another node has ended at . Upon being informed of completion of remanufacturing the DFD may enable the device interface triggering the DVBD to logically re introduce the associated virtual DSD to the node .

In some embodiments one or more of the data storage devices DSDs may be physically connected to a node via an interface . A storage port miniport driver not shown and or a storage port driver not shown may enumerate the DSD with a certain manufacturer s hardware ID via an operating system hardware discovery or Plug and Play process. The DFD may be loaded via the operating system hardware discovery process in response to a storage port miniport driver and or a storage port driver enumerating a DSD and in response to the port driver creating a physical device object PDO not shown representing the physical DSD.

The DFD may create a functional device object FDO for each port reported PDO having a certain hardware ID at . Further at step the DFD may register and enable a device interface for each FDO that it creates.

The DFD may also provide a standard upper edge device input and output control IOCTL interface . The upper edge IOCTL interface may be used to receive IOCTL operations from an OS destined to a DSD. The DVBD may use this interface to forward to the DFD I O operations from the operating system s corresponding device class driver s . In turn the DFD may provide a lower edge IOCTL interface to forward to the underlying PDO I O operations it receives from the DVBD .

In some embodiments the DFD may for each DSD determine whether an operation is querying the capacity of the DSD at . If so the DFD may intercept the query at and change the DSD s reported storage capacity to reflect the target storage capacity of the DSD at . For example the DSD may initially have a storage capacity of 100 GB but with the implementation of trivial overprovisioning of 25 the target storage capacity may be 75 GB. Remanufacturing of the DSD may decrease the DSD s storage capacity and therefore a remanufactured DSD may have an actual capacity less than 100 GB but not necessarily 75 GB. Regardless of the current actual capacity of the DSD the DFD may intercept and return queries for the storage capacity of the DSD with 75 GB the target capacity in this illustrative example.

The DVBD may be a root enumerated bus driver that instantiates physical device objects PDOs each of which may represent a single virtual DSD . During initialization the DVBD may register for device interface change notifications from the device function driver DFD at .

If the DVBD is notified of a device interface change at the DVBD may determine whether the device interface change indicates a device interface arrival i.e. the device interface is enabled or a device interface departure i.e. the device interface is disabled at . If notified of a device interface arrival then the DVBD may retrieve and store a reference pointing to the functional device object FDO corresponding to the enabled device interface at . The DVBD may further create a PDO that represents a virtual data storage device virtual DSD at . The virtual DSD may correspond to the physical data storage device DSD on which the FDO was created. The DVBD may associate the PDO with a hardware ID and a compatible ID at .

If the DVBD is notified of a device interface departure then the DVBD may release the reference pointing to the FDO at . The DVBD may further delete the PDO representing the virtual DSD at and inform the operating system s hardware discovery manager of the DSD s departure at .

Referring also to the DMS may publish an information interface and communicate with the DFD via input and output control IOCTL messages to gather information about the node s DSD configuration and discover and report the initiation of DSD remanufacturing.

In some embodiments the DMS may communicate with the DFD to identify the physical DSDs connected to the node at step . The DMS may correlate the identified DSDs with the node s virtual DSDs at . Further the DMS may use one or more application programming interfaces APIs to obtain DSD resource pool and or abstract DSD group information. For example the DMS may use the operating system s failover cluster APIs and the operating system s abstract DSD group APIs to obtain the DSD resource pool and or the abstract DSD group information. The DMS may also correlate the physical DSDs with the obtained DSD resource pool and or abstract DSD group information at .

The method may further involve the DMS maintaining in the registry a mapping of abstract DSD groups to virtual DSDs and or physical DSDs at . The mapping may be used to re establish the membership of the abstract DSD group s if the node is rebooted during remanufacturing of a DSD. The mapping information may be made available to other local applications via an interface provided by a service contract such as for example a Windows Communication Foundation WCF service contract.

The DMS may be informed by DFD of the intent to remanufacture a DSD at . When so informed the DMS may remove the DSD from its associated abstract DSD group s at . The DMS may further communicate the intent to remanufacture the DSD to the DMS instance s running on one or more other nodes in the cluster not shown if any at . The DMS instance s running on one or more other nodes may confirm receipt of the notification of intent to remanufacture the DSD. Upon receiving this confirmation from the one or more other nodes the DMS may notify the DFD of the confirmation at .

The DMS may be informed of the intent to remanufacture a DSD by a DMS instance running on another node in the cluster not shown at . When so informed the DMS may inform the DFD that the DSD should be logically removed at . The DFD may subsequently disable the device interface for the DSD causing the DVBD to delete the virtual DSD corresponding to the physical DSD.

When informed by either the DFD or a DMS instance running on another node in the cluster that remanufacturing of a DSD is complete the DMS of the current node may virtually re introduce the DSD to its appropriate abstract DSD group .

The method steps described above with reference to each of are presented in a specific order. However it should be apparent to one of skill in the art that additional steps may be included or that some steps may be excluded or performed in a different order than that depicted in each of .

In accordance with various embodiments the methods described herein may be implemented as one or more software programs or modules running on a computer processor or controller. In accordance with other embodiments the methods described herein may be implemented as one or more software programs running on a computing device such as a personal computer or server that is using one or more data storage devices such as a disc drive. Dedicated hardware implementations including but not limited to application specific integrated circuits programmable logic arrays and other hardware devices can likewise be constructed to implement the methods described herein. Further the methods and systems described herein may be implemented as a computer readable storage medium such as a storage device or memory including instructions that when executed by a processor cause the processor to perform the methods.

The illustrations of the embodiments described herein are intended to provide a general understanding of the structure of the various embodiments. The illustrations are not intended to serve as a complete description of all of the elements and features of apparatus and systems that utilize the structures or methods described herein. Many other embodiments may be apparent to those of skill in the art upon reviewing the disclosure. Other embodiments may be utilized and derived from the disclosure such that structural and logical substitutions and changes may be made without departing from the scope of the disclosure. Moreover although specific embodiments have been illustrated and described herein it should be appreciated that any subsequent arrangement designed to achieve the same or similar purpose may be substituted for the specific embodiments shown.

This disclosure is intended to cover any and all subsequent adaptations or variations of various embodiments. Combinations of the above embodiments and other embodiments not specifically described herein will be apparent to those of skill in the art upon reviewing the description. Additionally the illustrations are merely representational and may not be drawn to scale. Certain proportions within the illustrations may be exaggerated while other proportions may be reduced. Accordingly the disclosure and the figures are to be regarded as illustrative and not restrictive.

