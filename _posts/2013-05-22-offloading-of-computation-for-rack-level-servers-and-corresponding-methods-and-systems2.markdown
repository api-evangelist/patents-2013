---

title: Offloading of computation for rack level servers and corresponding methods and systems
abstract: A method for handling multiple networked applications using a distributed server system is disclosed. The method can include providing at least one main processor and a plurality of offload processors connected to a memory bus; and operating a virtual switch respectively connected to the main processor and the plurality of offload processors using the memory bus, with the virtual switch receiving memory read/write data over the memory bus.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09619406&OS=09619406&RS=09619406
owner: Xockets, Inc.
number: 09619406
owner_city: San Jose
owner_country: US
publication_date: 20130522
---
This application claims the benefit of U.S. Provisional Patent Application 61 650 373 filed May 22 2012 the contents of which are incorporated by reference herein.

The present disclosure relates generally to servers and more particularly to offload or auxiliary processing modules that can be physically connected to a system memory bus to process data independent of a host processor of the server.

Networked applications often run on dedicated servers that support an associated state context or session defined application. Servers can run multiple applications each associated with a specific state running on the server. Common server applications include an Apache web server a MySQL database application PHP hypertext preprocessing video or audio processing with Kaltura supported software packet filters application cache management and application switches accounting analytics and logging.

Unfortunately servers can be limited by computational and memory storage costs associated with switching between applications. When multiple applications are constantly required to be available the overhead associated with storing the session state of each application can result in poor performance due to constant switching between applications. Dividing applications between multiple processor cores can help alleviate the application switching problem but does not eliminate it since even advanced processors often only have eight to sixteen cores while hundreds of application or session states may be required.

A method for handling multiple networked applications using a distributed server system can include providing at least one main processor and a plurality of offload processors connected to a memory bus and operating a virtual switch respectively connected to the main processor and the plurality of offload processors using the memory bus with the virtual switch receiving memory read write data over the memory bus.

Networked applications are available that run on servers and have associated with them a state session defined applications . The session nature of such applications allows them to have an associated state and a context when the session is running on the server. Further if such session limited applications are computationally lightweight they can be run in part or fully on the auxiliary or additional processor cores such as those based on the ARM architecture as but one particular example which are mounted on modules connected to a memory bus for example by insertion into a socket for a Dual In line Memory Module DIMM . Such modules can be referred to as a Xocket In line Memory Module XIMM and have multiple cores e.g. ARM cores associated with a memory channel. A XIMM can access the network data through an intermediary virtual switch such as OpenFlow or similar that can identify sessions and direct the network data to the corresponding module XIMM mounted cores where the session flow for the incoming network data can be handled.

As will be appreciated through usage of a large prefetch buffer or low latency memory the session context of each of the sessions that are run on the processor cores of a XIMM can be stored external to the cache of such processor cores. By systematically engineering the transfer of cache context to a memory external to the module processors e.g. RAMs and engineering low latency context switch it is possible to execute several high bandwidth server applications on a XIMM provided the applications are not computationally intensive. The wimpy processor cores of a XIMM can be favorably disposed to handle high network bandwidth traffic at a lower latency and at a very low power when compared to traditional high power brawny cores.

In effect one can reduce problems associated with session limited servers by using the module processor e.g. an ARM architecture processor of a XIMM to offload part of the functionality of traditional servers. Module processor cores may be suited to carry computationally simple or lightweight applications such as packet filtering or packet logging functions. They may also be suited for providing the function of an application cache for handling hot code that is to be serviced very frequently to incoming streams. Module processor cores can also be suited for functions such as video streaming real time streaming that often only require light weight processing.

As an example of partitioning applications between a XIMM with wimpy ARM cores and a conventional brawny core e.g. x86 or Itanium server processor with Intel multicore processor a computationally lightweight Apache web server can be hosted on one or more XIMMs with ARM cores while computationally heavy MySQL and PHP are hosted on x86 brawny cores. Similarly lightweight applications such as a packet filter application cache management and application switch are hosted on XIMM s while x86 cores host control accounting analytics and logging.

According to some embodiments a web server running Apache MySQL PHP AMP can be used to service clients that send requests to the server module from network . The embodiment of can split a traditional server module running AMP across a combination of processors cores which act as separate processing entities. Each of the wimpy processor cores to which can be low power ARM cores in particular embodiments can be mounted on an XIMM with each core being allocated a memory channel . At least of one of the wimpy processor cores to can be capable of running a computationally light weight Apache or similar web server code for servicing client requests which are in the form of HTTP or a similar application level protocol. The Apache server code can be replicated for a plurality of clients to service a huge number of requests. The wimpy cores to can be ideally suited for running such Apache code and responding to multiple client requests at a low latency. For static data that is available locally wimpy cores to can lookup such data from their local cache or a low latency memory associated with them. In case the queried data is not available locally the wimpy cores to can request a direct memory access DMA memory to memory or disk to memory transfer to acquire such data.

The computation and dynamic behavior associated with the web pages can be rendered by PHP or such other server side scripts running on the brawny cores . The brawny cores might also have code scripting libraries for interacting with MySQL databases stored in hard disks present in said server module . The wimpy cores to on receiving queries or user requests from clients transfer embedded PHP MySQL queries to said brawny cores over a connection e.g. an Ethernet type connection that is tunneled on a memory bus such as a DDR bus. The PHP interpreter on brawny cores interfaces and queries a MySQL database and processes the queries before transferring the results to the wimpy cores to over said connection. The wimpy cores to can then service the results obtained to the end user or client.

Given that the server code lacking server side script is computationally light weight and many Web API types are Representational State Transfer REST based and require only HTML processing and on most occasions require no persistent state wimpy cores to can be highly suited to execute such light weight functions. When scripts and computation is required the computation is handled favorably by brawny cores before the results are serviced to end users. The ability to service low computation user queries with a low latency and the ability to introduce dynamicity into the web page by supporting server side scripting make the combination of wimpy and brawny cores an ideal fit for traditional web server functions. In the enterprise and private datacenter simple object access protocol SOAP is often used making the ability to context switch with sessions performance critical and the ability of wimpy cores to save the context in an extended cache can enhance performance significantly.

Each of the wimpy processor cores e.g. ARM cores to can be mounted on an in memory module not shown and each of them can be allocated a memory channel to . At least one of the wimpy processor cores to can be capable of running a tight computationally light weight web server code for servicing applications that need to be transmitted with a very low latency jitter. Example applications such as video audio or voice over IP VoIP streaming involve client requests that need to be handled with as little latency as possible. One particular protocol suitable for the disclosed embodiment is Real Time Transport Protocol RTP an Internet protocol for transmitting real time data such as audio and video. RTP itself does not guarantee real time delivery of data but it does provide mechanisms for the sending and receiving applications to support streaming data.

Brawny processor core s can be connected by bus to switch which may be an OpenFlow or other virtual switch . In one embodiment such a bus can be a front side bus.

In operation server module can handle several client requests and services information in real time. The stateful nature of applications such as RTP video streaming makes the embodiment amenable to handle several queries at a very high throughput. The embodiment can have an engineered low latency context overhead system that enables wimpy cores to to shift from servicing one session to another session in real time. Such a context switch system can enable it to meet the quality of service QoS and jitter requirements of RTP and video traffic. This can provide substantial performance improvement if the overlay control plane and data plane for handling real time applications related traffic is split across a brawny processor and a number of wimpy cores to . The wimpy cores to can be favorably suited to handling the data plane and servicing the actual streaming of data in video audio streaming or RTP applications. The ability of wimpy cores to to switch between multiple sessions with low latency makes them suitable for handling of the data plane.

For example wimpy cores to can run code that quickly constructs data that is in an RTP format by concatenating data that is available locally or through direct memory access DMA from main memory or a hard disk with sequence number synchronization data timestamp etc. and sends it over to clients according to a predetermined protocol. The wimpy cores to can be capable of switching to a new session new client with a very low latency and performing a RTP data transport for the new session. The brawny cores can be favorably suited for overlay control plane functionality.

The overlay control plane can often involve computationally expensive actions such as setting up a session monitoring session statistics and providing information on QoS and feedback to session participants. The overlay control plane and the data plane can communicate over a connection e.g. an Ethernet type connection that is tunneled on a memory bus such as a DDR bus. Typically overlay control can establish sessions for features such as audio videoconferencing interactive gaming and call forwarding to be deployed over IP networks including traditional telephony features such as personal mobility time of day routing and call forwarding based on the geographical location of the person being called. For example the overlay control plane can be responsible for executing RTP control protocol RTCP which forms part of the RTP protocol used to carry VoIP communications and monitors QoS Session Initiation Protocol SIP which is an application layer control signaling protocol for Internet Telephony Session Description Protocol SDP which is a protocol that defines a text based format for describing streaming media sessions and multicast transmissions or other low latency data streaming protocols.

In particular embodiments in some embodiments a rack server module further includes a switch which can provide input out memory management unit IOMMU functions and a switch which may be an OpenFlow or other virtual switch . Brawny processor core s can be connected to switch by bus which can be a front side bus. A traditional server module can also include a switch can provide IOMMU functions .

The following example s provide illustration and discussion of exemplary hardware and data processing systems suitable for implementation and operation of the foregoing discussed systems and methods. In particular hardware and operation of wimpy cores or computational elements connected to a memory bus and mounted in DIMM or other conventional memory socket is discussed.

The computation elements or offload processors can be accessible through memory bus . In this embodiment the module can be inserted into a Dual Inline Memory Module DIMM slot on a commodity computer or server using a DIMM connector providing a significant increase in effective computing power to system . The module e.g. XIMM may communicate with other components in the commodity computer or server via one of a variety of busses including but not limited to any version of existing double data rate standards e.g. DDR DDR2 DDR3 etc. 

This illustrated embodiment of the module contains five offload processors however other embodiments containing greater or fewer numbers of processors are contemplated. The offload processors to can be custom manufactured or one of a variety of commodity processors including but not limited to field programmable grid arrays FPGA microprocessors reduced instruction set computers RISC microcontrollers or ARM processors. The computation elements or offload processors can include combinations of computational FPGAs such as those based on Altera Xilinx e.g. Artix class or Zynq architecture e.g. Zynq and or conventional processors such as those based on Intel Atom or ARM architecture e.g. ARM A . For many applications ARM processors having advanced memory handling features such as a snoop control unit SCU are preferred since this can allow coherent read and write of memory. Other preferred advanced memory features can include processors that support an accelerator coherency port ACP that can allow for coherent supplementation of the cache through an FPGA fabric or computational element.

Each offload processor to on the module may run one of a variety of operating systems including but not limited to Apache or Linux. In addition the offload processors to may have access to a plurality of dedicated or shared storage methods. In this embodiment each offload processor can connect to one or more storage units in this embodiments pairs of storage units and . Storage units to can be of a variety of storage types including but not limited to random access memory RAM dynamic random access memory DRAM sequential access memory SAM static random access memory SRAM synchronous dynamic random access memory SDRAM reduced latency dynamic random access memory RLDRAM flash memory or other emerging memory standards such as those based on DDR4 or hybrid memory cubes HMC .

In this embodiment one of the Zynq computational FPGAs to can act as arbiter providing a memory cache giving an ability to have peer to peer sharing of data via memcached or OMQ memory formalisms between the other Zynq computational FPGAs to . Traffic departing for the computational FPGAs can be controlled through memory mapped I O. The arbiter queues session data for use and when a computational FPGA asks for address outside of the provided session the arbiter can be the first level of retrieval external processing determination and predictors set.

Operation of one embodiment of a module e.g. XIMM using an ARM A9 architecture is illustrated with respect to . Use of ARM A architecture in conjunction with an FPGA fabric and memory in this case shown as reduced latency DRAM RLDRAM can simplify or makes possible zero overhead context switching memory compression and CPI in part by allowing hardware context switching synchronized with network queuing. In this way there can be a one to one mapping between thread and queues. As illustrated the ARM A architecture includes a Snoop Control Unit SCU . This unit allows one to read out and write in memory coherently. Additionally the Accelerator Coherency Port ACP allows for coherent supplementation of the cache throughout the FPGA . The RLDRAM provides the auxiliary bandwidth to read and write the ping pong cache supplement Block and Block during packet level meta data processing.

The following table Table 1 illustrates potential states that can exist in the scheduling of queues threads to XIMM processors and memory such as illustrated in .

Essentially zero overhead context switching is also possible using modules as disclosed in . Because per packet processing has minimum state associated with it and represents inherent engineered parallelism minimal memory access is needed aside from packet buffering. On the other hand after packet reconstruction the entire memory state of the session can be accessed and so can require maximal memory utility. By using the time of packet level processing to prefetch the next hardware scheduled application level service context in two different processing passes the memory can always be available for prefetching. Additionally the FPGA can hold a supplemental ping pong cache that is read and written with every context switch while the other is in use. As previously noted this is enabled in part by the SCU which allows one to read out and write in memory coherently and ACP for coherent supplementation of the cache throughout the FPGA . The RLDRAM provides for read and write to the ping pong cache supplement shown as Block and Block during packet level meta data processing. In the embodiment shown only locally terminating queues can prompt context switching.

In operation metadata transport code can relieve a main or host processor from tasks including fragmentation and reassembly and checksum and other metadata services e.g. accounting IPSec SSL Overlay etc. . As IO data streams in and out L1 cache can be filled during packet processing. During a context switch the lock down portion of a translation lookaside buffer TLB of an L1 cache can be rewritten with the addresses corresponding to the new context. In one very particular implementation the following four commands can be executed for the current memory space.

Bandwidths and capacities of the memories can be precisely allocated to support context switching as well as applications such as Openflow processing billing accounting and header filtering programs.

For additional performance improvements the ACP can be used not just for cache supplementation but hardware functionality supplementation in part by exploitation of the memory space allocation. An operand can be written to memory and the new function called through customizing specific Open Source libraries so putting the thread to sleep and a hardware scheduler can validate it for scheduling again once the results are ready. For example OpenVPN uses the OpenSSL library where the encrypt decrypt functions can be memory mapped. Large blocks are then available to be exported without delay or consuming the L2 cache using the ACP . Hence a minimum number of calls are needed within the processing window of a context switch improving overall performance.

It should be appreciated that in the foregoing description of exemplary embodiments of the invention various features of the invention are sometimes grouped together in a single embodiment figure or description thereof for the purpose of streamlining the disclosure aiding in the understanding of one or more of the various inventive aspects. This method of disclosure however is not to be interpreted as reflecting an intention that the claimed invention requires more features than are expressly recited in each claim. Rather as the following claims reflect inventive aspects lie in less than all features of a single foregoing disclosed embodiment. Thus the claims following the detailed description are hereby expressly incorporated into this detailed description with each claim standing on its own as a separate embodiment of this invention.

It is also understood that the embodiments of the invention may be practiced in the absence of an element and or step not specifically disclosed. That is an inventive feature of the invention may be elimination of an element.

Accordingly while the various aspects of the particular embodiments set forth herein have been described in detail the present invention could be subject to various changes substitutions and alterations without departing from the spirit and scope of the invention.

