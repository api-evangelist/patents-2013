---

title: Wear leveling and management in an electronic environment
abstract: Workloads can be intelligently placed across a group of resources in order to attempt to balance or otherwise manage the level of wear among various components of those resources. Devices such as solid state drives or other NAND-type devices can have a limited number of operations that can be performed before those devices become unreliable, such that it can be desirable to monitor the wear level of each of these devices. As it can be easier to manage resources with similar wear levels for large groups of resources, it can be desirable to attempt to level the relative amount of wear among at least groups of these resources. Attempts can be made to level across a fleet or resources, within pools of resources, and/or within the resources themselves, such as where a server includes multiple devices with potentially different wear levels, such as multiple NAND-type devices.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09450876&OS=09450876&RS=09450876
owner: Amazon Technologies, Inc.
number: 09450876
owner_city: Reno
owner_country: US
publication_date: 20130313
---
As an increasing number of applications and services are being made available over networks such as the Internet an increasing number of content application and or service providers are turning to technologies such as resource sharing or cloud computing. Cloud computing in general is an approach to providing access to electronic resources through services such as Web services where the hardware and or software used to support those services is dynamically scalable to meet the needs of the services at any given time. A user or customer can obtain access to various services through the cloud or across at least one network and thus does not have to purchase and maintain the hardware associated with the services.

In order to spread the workloads of various customers across a set of resources approaches such as load balancing are used to direct certain portions of the workloads to different resources or sets of resources. These workloads can vary significantly in size and type however such that certain workloads can require significantly more resource capacity than others. If certain customers are associated with certain resources and those customers have very resource intensive workloads the resources associated with those customers might perform many more operations than other resources associated with other customers. While this might not be significant for certain types of resources other types of resources such as NAND flash memory will wear out after a certain amount of usage. Significant variations in the distributions in workload thus can cause some of these resources to wear out before others which can cause difficulties in the maintenance of a set of resources and the processing of customer workloads.

Systems and methods in accordance with various embodiments of the present disclosure overcome one or more of the above referenced and other deficiencies in conventional approaches to managing resources and or customer workloads connections in an electronic environment. In particular various embodiments can monitor the relative wear of a set of resources and attempt to intelligently place workloads based at least in part upon the relative wear of those resources. In some embodiments a monitoring component can determine the average wear of non volatile memory having a finite number of read write cycles and thus a limited lifetime for each host machine and can attempt to place workloads to balance the average wear level across a set of host machines. In some embodiments the hosts can be grouped by wear level such that hosts of similar age or use levels can be balanced against each other rather than against different generations of hosts. Grouping hosts by age and or wear level also provides for easier management of a fleet or set of hosts. If the wear level of a particular host changes drastically with respect to the other hosts in a group that host can be switched to another logical group with a similar wear level. The wear level can also be examined with respect to the age of a host or resource to ensure that newer resources are not overloaded due to their relatively low wear level as well as to ensure that alarms are not generated when a new host is added and the variation in wear level across the set of hosts exceeds an otherwise unacceptable amount of wear variation.

Similarly approaches in accordance with various embodiments can monitor memory devices with limited lifetimes e.g. solid state drives or other NAND type devices within a host machine or other such resource as well. A host receiving a workload can attempt to place that workload on memory devices based on wear level such as to minimize wear on any particular device balance wear or maximize wear on a targeted device among other such options. Similarly the age of each memory device can be considered such that replacing a device does not generate an alarm when the variation in wear level exceeds a determined threshold. The age of various memory devices hosts or other such components can also be used to calculate a wear rate of each component as well such that the rate of wear can be balanced and or alarms can be generated when one or more components exceed a determined wear rate threshold.

Various other applications processes and uses are presented below with respect to the various embodiments.

In various embodiments the provider environment may include various types of resources that can be utilized by multiple users or applications for a variety of different purposes. In at least some embodiments all or a portion of a given resource or set of resources might be allocated to a particular user or allocated for a particular task for at least a determined period of time. The sharing of these multi tenant resources from a provider environment is often referred to as resource sharing Web services or cloud computing among other such terms and depending upon the specific environment and or implementation. In this example the provider environment includes a plurality of resources of one or more types. These types can include for example application servers operable to process instructions provided by a user or database servers operable to process data stored in one or more data stores in response to a user request. As known for such purposes the user can also reserve at least a portion of the data storage in a given data store. Methods for enabling a user to reserve various resources and resource instances are well known in the art such that detailed description of the entire process and explanation of all possible components will not be discussed in detail herein.

In at least some embodiments an application executing on the client device that needs to access or utilize a portion of the resources which might be exposed as one or more services to which the application has subscribed can submit a request that is received to an interface layer of the provider environment . The interface layer can include application programming interfaces APIs or other exposed interfaces enabling a user to submit requests such as Web service requests to the provider environment. The interface layer in this example can also include other components as well such as at least one Web server routing components load balancers and the like. When a request to access a resource is received at the interface layer in some embodiments information for the request can be directed to a resource manager or other such system service or component configured to manage user accounts and information resource provisioning and usage and other such aspects. A resource manager receiving the request can perform tasks such as to authenticate an identity of the user submitting the request as well as to determine whether that user has an existing account with the resource provider where the account data may be stored in at least one data store in the provider environment.

An interface layer in at least one embodiment includes a scalable set of customer facing servers that can provide the various APIs and return the appropriate responses based on the API specifications. The interface layer also can include at least one API service layer that in one embodiment consists of stateless replicated servers which process the externally facing customer APIs. The interface layer can be responsible for Web service front end features such as authenticating customers based on credentials authorizing the customer throttling customer requests to the API servers validating user input and marshaling or un marshaling requests and responses. The API layer also can be responsible for reading and writing database configuration data to from the administration data store in response to the API calls. In many embodiments the Web services layer and or API service layer will be the only externally visible component or the only component that is visible to and accessible by customers of the control service. The servers of the Web services layer can be stateless and scaled horizontally as known in the art. API servers as well as the persistent data store can be spread across multiple data centers in a region for example such that the servers are resilient to single data center failures.

In various embodiments the resources can take the form of servers e.g. application servers or data servers and or components installed in those servers. In some embodiments at least a portion of the resources can be virtual resources supported by these and or components. In certain embodiments some of the components used to support customer services comprise devices that are known to wear or degrade over time. For example a server might include one or more memory devices such as a Solid State Drive SSD flash memory or other NAND type device. A NAND device in general is a type of non volatile storage that does not require power to retain data. NAND devices are relatively inexpensive compared to other types of memory such as NOR devices and thus are desirable to use to reduce the cost of providing the resources. Due to the design of these NAND devices however the devices have a finite number of read write cycles before the individually erasable segments become unreliable. The wear on these devices can be tracked and or reported such as by monitoring the number of reads and writes and comparing this to a reported number of read write cycles for the lifetime of the NAND device. Other values can be determined and or tracked as well such as the percentage of remaining cycles the percentage of utilized cycles the number of used or remaining cycles etc. It should be understood that while the present discussion will utilize the term wear similar values such as remaining lifetime number of cycles utilized anticipated capacity or other such values could be utilized as well within the scope of the various embodiments. Further other devices with known or anticipated wear patterns or trends can be utilized as well within the scope of the various embodiments.

In addition to the fact that the devices wear out and may need to occasionally be replaced a further complication arises in the fact that the devices can wear differentially based at least in part upon the differences in reads and writes requested to be performed by each of the devices. In a shared resource multi tenant or cloud environment for example multiple customers might use a set of NAND devices and the workload for the customers might be distributed across the devices using a load balancing algorithm or other such process which can distributed based upon factors such as the recency of a workload sent to a device the number of concurrent connections to a device the number of workloads being currently processed by each device or other such information. It will often be the case however that different requests will correspond to workloads of different sizes. Further different customers will utilize the devices by different amounts such that assigning two different devices to two different customers might result in one NAND device having significantly more wear than the other device. Such an approach thus requires a resource manager or other such entity service or component to monitor the wear and or usage of each of these devices and notify when a device is at or near its lifetime such that it can be taken out of service and replaced.

In many instances it can be easier to manage a fleet of servers or other set of devices that are expected to wear out at approximately the same time rather than having to monitor and replace each device individually at different times. Further the devices that wear out most quickly can generally be the most used devices such that they can be the least desirable to frequently have to take off line.

Accordingly systems and methods in accordance with various embodiments can attempt to balance or level the wear across a set of devices such as a fleet of servers including NAND memory devices. As an example illustrates an example configuration wherein multiple servers are provided in a data plane or data environment of a resource provider. The servers in the data plane can be managed by one or more components of a control plane or control environment which are physically and or logically separated from the servers and other components of the data plane . In this example each server can have one or more host managers or other components that can communicate with a resource manager or workload monitor to provide wear information for the server. In some embodiments a server can provide wear information for each device on or in communication with that server or can provide an average wear value for the devices on the server such as an average wear value for the solid state drives contained in the server. In this way a component such as the resource manager can track the relative wear levels of each of the servers and can provide that information to a component such as a placement manager . The placement manager can be a system service component or module for example that determines which server is to receive a workload for an incoming request.

For example a request might be received over at least one network from a client device for a customer where that request includes an amount of work or a workload to be performed by one or more of the servers . The placement manager can look at the relative wear levels of the servers available to accept the request and can make the determination as to where to place the workload based at least in part upon the wear levels of the available servers. In one embodiment the placement manager can decide to direct the workload to the server with the highest remaining percentage of cycles or lowest wear level etc. . In other embodiments the placement manager might analyze the wear levels to determine the server s with more than a threshold higher wear level than the other servers and might remove that server from consideration for placement of the workload. In one of the servers has undergone excessive usage and has a significantly lower remaining percentage of cycles than the other servers. The placement manager then might remove this server from consideration and use the regular placement algorithm to determine which of the other servers should receive the workload. Such usage can help to level the wear across the set of servers rather than let certain servers undergo more usage and develop hot spots within the server fleet.

As discussed however certain customers and workloads can utilize the resources much more than others. Thus determining where to place workloads based primarily upon factors such as relative wear levels can still result in at least some discrepancy in wear level across the fleet of servers . Accordingly approaches in accordance with various embodiments can utilize a service component or module such as a workload monitor to monitor usage by certain customers applications services or entities for example as well as the amount of usage for certain types of requests or workloads. The workload monitor then can attempt to determine patterns in usage or otherwise predict the usage needed for a received workload.

For example the workload manager might receive information for a request received from a first customer. The workload manager might analyze data stored in a local data store or other such location to attempt to predict the number of cycles amount of I O activity or other such measure of usage that will be needed to process the request based upon past usage by that customer. This can include for example an analysis of data for a number of previous requests a lookup of an average number of cycles or another such value. The placement manager might determine that the workload will require very few cycles and thus might direct the workload to the server with the highest level of wear such that the server can still be utilized while attempting to balance wear levels. Similarly if the placement manager determines that the workload will likely require a large number of cycles the placement manager might direct that workload to one of the servers with the lowest wear levels.

In some embodiments a component such as a resource manager can make one or more API calls or similar requires to an agent running on each of the servers . Each agent can be configured to periodically poll the SSDs flash memory or other NAND type devices on the respective server and through a SMART mechanism or similar process as discussed herein can receive information for the current wear level of each device. As each device manufacturer might report different information regarding wear a process can be utilized to attempt to normalize the wear levels of the various devices to a single concept of wear remaining life etc.

Information other than wear levels can be considered in a placement decision as well. For example two servers might have approximately the same average wear levels but one server might be significantly older than the other. In some embodiments the placement manager might decide to direct the workload to the older server as that server will likely need to be replaced or repaired before the newer server. Similarly if the newer server has the same level of wear as the older server then the newer server has been processing a much higher workload over a recent period of time and it can be desirable to even out the workloads for other purposes such as to minimize a likelihood of server failure due to prolonged operation at full capacity etc.

As discussed factors such as the age of a server might be used in placement decisions as it can be undesirable in at least some situations to send the majority of workloads to a recently added group of servers simply because those servers have the least amount of wear. In the servers are grouped physically geographically and or logically into pools of servers where each pool has approximately the same age or other similar characteristics . As illustrated a newest pool has around 88 of its cycles remaining while the older pools have averages of around 55 and 33 respectively. Separating the servers into pools enables wear leveling within the pools such that groups of servers can have similar wear levels and can be managed accordingly. Thus if the resource provider is planning to retire the servers in the oldest pool the placement manager can direct the heaviest workloads to that pool while still balancing within the pool such that the NAND components will approach the end of their useful lifetime at approximately the same time. Pooling also enables other types of decisions to be made such as the amount of usage that newer or older pools should receive which types of customers or requests to place on each type of device etc. Such an approach also provides for easier management of servers of different ages or generations as servers are typically not added one at a time but in groups and such an approach can allow for wear leveling within each of those groups of servers. Such approaches enable each pool to be managed as a unit having an average wear level etc.

In some embodiments the placement decisions can be made to attempt to reduce the number of pools. In this example there are only three pools with significantly different wear levels. In data centers with thousands of servers for example there might be dozens of pools where some of the pools might only vary by a few percent with respect to average wear. In at least some embodiments the placement manager might attempt to place workloads in order to merge pools as their wear levels converge in order to cause there to be fewer distinct servers to manage. Placement decisions might also be made to move servers to different pools. For example a server might have a significantly taxing workload to process which can cause the server to have significantly more wear than the other servers in its pool. If the wear level of that server is then closer to the wear level of another pool the resource manager or another such component can decide to move that server into the other pool instead of working to attempt to balance the other servers within the original pool.

In addition to attempting to level the wear of servers across a pool fleet or other such grouping it also can be desirable to provide a way to provide a notification or alarm when any of a set of wear related issues arise. For example SSDs can provide information such as SMART attributes that provide information about the wear of the drive. If the wear level of a particular component reaches a minimum threshold or other such value a notification might be generated to a fleet manager or other such entity indicating that the drive will need to be replaced and or removed from operation in the near future. Further if certain components or devices are being utilized at more than a threshold rate or the rate of wear is faster than expected a notification can be generated.

In at least some embodiments a component such as a resource manager can monitor the relative wear levels of servers either across a fleet in a pool or otherwise and can generate a notification when the difference in wear values between servers meets or exceeds a maximum difference. For example going back to the system might monitor the relative wear or other such value on the various servers . As illustrated one of the servers has a 72 remaining cycle value while the other servers are around 88 . If a threshold differential of 5 or 10 is set for example the system can generate an alert and or notification that something unusual is happening someone might be abusing the system or at least that the system is being used in a way that was not intended.

As discussed in some embodiments a component such as a workload monitor might monitor customer workloads for use in predicting future workload sizes and or properties. During this monitoring the workload monitor might also attempt to determine if the actual needs of a current workload exceed or otherwise differ from expectations. For example if a customer typically submits small workloads so a workload for that customer was assigned to a server with a high wear level but the actual workload being processed exceeds the expected amount by at least a determined or threshold amount the system can generate a notification. The notification can be sent to a user or entity or can be sent to a component or service such as a placement manager . In at least some embodiments the placement manager can determine that a workload is significantly larger than expected and can determine to move the remaining portion of that workload to a different server. In some embodiments the workload might not have to be significantly different than expected but if another workload is moved or the system starts falling outside an acceptable equilibrium state for example the placement manager might decide to move workloads to other servers within a fleet the same pool a different pool etc.

In some embodiments various other actions might be taken when a workload is much larger or more resource intensive than expected. For example a notification of excessive usage can trigger a throttling of a customer or customer workload or even a customer behavior. In other embodiments changes might be made to the resources themselves such as to the operating parameters or firmware among other such options. In some embodiments a user who occasionally submits excessive workloads can have a first type of action performed such as to shift the workload to different resources while prolonged or repeated submission might result in other actions such as throttling or additional costs as may be set forth in the terms of the customer account etc.

In some embodiments each resource e.g. application server or data server itself might include multiple individual resources such as multiple solid state drives or NAND devices. For example an application server might include eight or sixteen SSDs for processing customer workloads as well as potentially other flash memory or BIOS devices. Each of these devices can have a wear level as well such that it can be desirable in at least some embodiments to manage the wear levels of the individual devices within a server or other such resource as well.

Such an approach enables more accurate wear leveling as the fleet or pool level determinations can be made based upon factors such as the average wear level of a resource but that resource might include multiple NAND devices or similar components that can have significantly different wear levels if not monitored and or attempted to be leveled. Even though a server might report that it has about a year s worth of wear left on it at current usage one of the individual devices in that server might be about ready to expire. Approaches discussed herein enable workloads to be placed and or moved within a server or resource as well in order to balance wear across the components of that server or target specific devices for usage etc. as discussed elsewhere herein . In some embodiments the leveling within a server can be used to determine to move workloads between servers. For example a criterion such as a wear deviation within a server being met or exceeded might cause a workload to be shifted to a different server. Various other placement determinations can be made as well within the scope of the various embodiments.

In some embodiments there can be a level of abstraction provided to customers such as where a virtualization layer is provided that enables the customer to interact with a virtual resource which is actually a type and or address of resource that the provider of the resources can map to any or a number of different resources in a way that is transparent to the customer. In this way the resource provider can place workloads as appropriate and or beneficial without the customer having to do anything or have any knowledge of the placement. A customer will not be granted native or full access to an SSD. A virtual machine manager VMM such as a hypervisor component can manage a set of virtual machines that are abstractions of the underlying drives providing block I O virtualization. The VMM can manage the I Os and when a particular VMM indicates that it wants to write to an SSD a component can determine where to place that I O. In some embodiments an approach such as RAID 0 can be used to group drives together to form a virtual logical unit number LUN and the drives of the LUN can be exposed as a single larger drive. For example a server with eight SSDs might report that it has two drives of larger capacity where each drive is a set of four drives in a LUN. When a request or I O is received to one of the virtual drives or LUNs an algorithm can be used with a LUN mapping to determine which of the physical drives should receive the request. The wear level of each of the physical devices can be monitored and the mappings or placements can be updated to attempt to obtain wear leveling or other wear patterning across the set of devices. In at least some embodiments the remapping can be performed dynamically where the customer actions can be monitored and remapping performed at runtime etc.

Certain embodiments can analyze the status of various SSDs or other such drives or devices in a machine such as a host machine or server and route work to those drives based at least in part upon the relative wear of the drives. It is possible however that one or more of the drives might be replaced with a new drive such as when one of the existing drives fail. The new drive that is added typically will have no wear such that an allowable range of wear differences across the drives on the device might be exceeded. This excessive variation in wear can result in an alarm or notification being generated when excessive wear differential is a trigger for one or more alarms. Alternatively the a host manager or other such component might route an excessive amount of work to the new drive due to the low level of wear which might significantly reduce the lifetime of the new drive. Thus approaches in accordance with various embodiments can also take into account information such as the relative age of each drive. In some embodiments the system might utilize one or more functions linear exponential or otherwise that attempt to model an average rate of wear or range of wear values such that a determination can be made as to whether the amount wear for a given drive falls within an expected range for its age or length of use. In this way the system can attempt to level wear across the devices over time. When determining whether to place a workload on a device then the system can attempt to predict the amount of work needed and can place the workload on a device based at least in part upon whether the workload will cause the device to fall outside the allowable and or expected wear range for its age. It might be observed that devices such as a type of NAND device wear faster or slower over time for the same load such that a wear model or function can be generated and used to determine whether the amount of wear on a device is acceptable for its age.

It can be at least somewhat challenging however to programmatically distinguish alarming wear patterns versus similarly measured differences on drives that are the result of a different age and or generation of drive s in the server. Drives are naturally divided by server but the drive usage can also be controlled at a sub server level by for example assigning drives by virtual machine and steering workloads of different types or sizes to specific drives to help regulate the wear rate. Thus alarming can be performed at a virtual machine grouping level or other partitioning level rather than only at the server level.

In at least some embodiments the wear or lifetime remaining on a drive can be regularly sampled and or monitored over the history of the drive. This can include for example sampling the number of NAND program erase cycles left on the average drive. Such information not only provides an absolute sense of remaining lifetime but can also be analyzed to determine the rate at which a drive is being worn or the first order derivative as well as fluctuations in the rate of wear on the drive which can indicate bursts of usage or the second order derivative etc. As mentioned however it may not be straightforward to determine the wear or lifetime metric for various drives or devices as different manufacturers provide different metrics to quantify this information. Different ways of characterizing similar information may include the number of bytes written to the drive the average number of bytes written to the drive the average number of program erase cycles for NAND cells the number of bytes or program erase cycles that can be reliably executed in the future etc. With additional information such as the total number of NAND cells or bytes storable the size of erase and program blocks the write amplification the amount of spare area and other related drive characteristics the provided data can be analyzed and converted into a common metric across all drive manufacturers types of drives etc.

At least some embodiments utilize a common metric that can be used to calculate the expected lifetime of a drive based at least in part upon a reference model for high load burn rate. The use of the metric an require information such as the expected write amplification at the provisioning depth average mix of block sizes etc. including assumptions needed to make a conservative baseline by server and or platform type. Alarming on wear outliers within a server can account for information such as the age and or generation of the drives. In one embodiment drives can be grouped by a value such as the computed lifetime remaining. Each grouping can have a maximum amount of variation in remaining lifetime such as no more than about 4 from the least to most worn device within the group. As an example a drive with 600 days of remaining lifetime could be considered in the same group as one with 624 days or one with 576 days but drives with 576 and 624 days could not be in the same group with each other since their relative span is 8 total.

Various approaches or statistical techniques can be used to compute these groupings from a population. One technique is to assume all drives will be in the same wear group and determine whether the drives are all within 4 of each other. If not the drives can be sorted by the amount of remaining wear and an attempt can be made to remove the least worn drive from the group statistics. A computation then can be performed to determine the mean wear and variance across the population of remaining drives. The process can be repeated to determine the most and or least worn drive as well as the drive that upon removal would result in the lowest variance among the remaining drives. That drive then can be removed from that group. The process can be repeated for remaining drives until the drives are within the allowable span. The process can be repeated for other groups as well until each group has at most the allowable span. In the worst case each drive might be in its own group depending upon the number of drives but this can be a special rare case. It can be more common that the result will include a most worn group which might include a set of original drives as well as more less worn drive groups each with a smaller number of drives as may have been replacements due to drive failure or other such events.

Occasionally a large number of drives in a small number of servers might fail. It then might be the case that all but a few drives in each server need to be replaced. If replaced naively the result would be a small group of drives that now are the most worn but do not necessarily represent an abuse scenario. Such an occurrence can in some approaches generate an alarm even though the alarm might not be warranted. Such an occurrence can be relatively unlikely as mass failure of devices when it does occur typically happens during server burn in or soon after entering production. In this case the drives should exhibit very little wear. If it happens after the drives have been worn a policy can be utilized that indicates that for drives already having for example more than 2 wear if more than 15 or another appropriate number of devices have failed in the lifetime of the server all of the drives in the server should be replaced with drives of comparable wear lifetime. The functional drives that have been removed can be binned into groups of comparable wear lifetime and used in other servers during replacement in an effort to minimize the number of wear groups on a given server to one or two at most for example.

If the most worn group or groups has less than half the drives this can represent an outlier regardless of drive age such that one or more remediation steps can be appropriate. Oftentimes such an occurrence would be the result of an abuse scenario where a drive or small subset of drives is somehow targeted for more wear than other drives. It may also happen because of virtualization configuration or other software related mistakes in the service layer resulting in an excessive amount of I O being directed to a subset of the drives. In any case these situations can be addressed through configuration settings or customer management e.g. tagging the customer as an abuser and removing them among other such options. Such a situation also can require attention to address the alarming condition such as by replacing the smaller set of drives either the more or less worn population whichever is smaller with drives of comparable wear to the majority of drives. If drives of comparable wear do not exist all the drives can be replaced with drives of comparable wear such as all new drives with no wear. In a least some embodiments a whitelist can be created that can include entries for systems that alarmed in situations such as those presented above. For systems that were whitelisted the wear can be monitored over time to determine whether the differential is worse than a larger threshold and not raise another alarm until the second threshold is exceeded.

The most worn group can represent the assumed lifetime of the server from an alarming perspective such that any alarms related to the overall server lifetime around NAND is based on the most worn drives. Alarms then can be generated as the amount of life left on the server approaches 0 life left or another such value. These drives can also be avoided if there is any server level control over steering I O workloads e.g. via mapping of logical to physical drives where there is an apparent bias that can be addressed. This can include for example VM placement or other gross segregation assignment of SSDs by a customer AMI or workload etc.

In certain embodiments wear rates or other first order derivatives can be computed for use with another set of alarms where those alarms are triggered when certain drives or servers are being worn out at a higher than expected rate. As an example if any given drive is getting worn out faster than 1 per week an alarm might be generated even if the wear span is not outside the allowable range e.g. 4 across the server. This can help to quickly detect abuse or bug scenarios before the server has become operationally impacted by the differential in wear. The wear rate computation may also be used to inform placement decisions by workload or other such aspects.

As discussed above the various embodiments can be implemented in a wide variety of operating environments which in some cases can include one or more user computers computing devices or processing devices which can be used to operate any of a number of applications. User or client devices can include any of a number of general purpose personal computers such as desktop or laptop computers running a standard operating system as well as cellular wireless and handheld devices running mobile software and capable of supporting a number of networking and messaging protocols. Such a system also can include a number of workstations running any of a variety of commercially available operating systems and other known applications for purposes such as development and database management. These devices also can include other electronic devices such as dummy terminals thin clients gaming systems and other devices capable of communicating via a network.

Various aspects also can be implemented as part of at least one service or Web service such as may be part of a service oriented architecture. Services such as Web services can communicate using any appropriate type of messaging such as by using messages in extensible markup language XML format and exchanged using an appropriate protocol such as SOAP derived from the Simple Object Access Protocol . Processes provided or executed by such services can be written in any appropriate language such as the Web Services Description Language WSDL . Using a language such as WSDL allows for functionality such as the automated generation of client side code in various SOAP frameworks.

Most embodiments utilize at least one network that would be familiar to those skilled in the art for supporting communications using any of a variety of commercially available protocols such as TCP IP FTP UPnP NFS and CIFS. The network can be for example a local area network a wide area network a virtual private network the Internet an intranet an extranet a public switched telephone network an infrared network a wireless network and any combination thereof.

In embodiments utilizing a Web server the Web server can run any of a variety of server or mid tier applications including HTTP servers FTP servers CGI servers data servers Java servers and business application servers. The server s also may be capable of executing programs or scripts in response requests from user devices such as by executing one or more Web applications that may be implemented as one or more scripts or programs written in any programming language such as Java C C or C or any scripting language such as Perl Python or TCL as well as combinations thereof. The server s may also include database servers including without limitation those commercially available from Oracle Microsoft Sybase and IBM .

The environment can include a variety of data stores and other memory and storage media as discussed above. These can reside in a variety of locations such as on a storage medium local to and or resident in one or more of the computers or remote from any or all of the computers across the network. In a particular set of embodiments the information may reside in a storage area network SAN familiar to those skilled in the art. Similarly any necessary files for performing the functions attributed to the computers servers or other network devices may be stored locally and or remotely as appropriate. Where a system includes computerized devices each such device can include hardware elements that may be electrically coupled via a bus the elements including for example at least one central processing unit CPU at least one input device e.g. a mouse keyboard controller touch screen or keypad and at least one output device e.g. a display device printer or speaker . Such a system may also include one or more storage devices such as disk drives optical storage devices and solid state storage devices such as random access memory RAM or read only memory ROM as well as removable media devices memory cards flash cards etc.

Such devices also can include a computer readable storage media reader a communications device e.g. a modem a network card wireless or wired an infrared communication device etc. and working memory as described above. The computer readable storage media reader can be connected with or configured to receive a computer readable storage medium representing remote local fixed and or removable storage devices as well as storage media for temporarily and or more permanently containing storing transmitting and retrieving computer readable information. The system and various devices also typically will include a number of software applications modules services or other elements located within at least one working memory device including an operating system and application programs such as a client application or Web browser. It should be appreciated that alternate embodiments may have numerous variations from that described above. For example customized hardware might also be used and or particular elements might be implemented in hardware software including portable software such as applets or both. Further connection to other computing devices such as network input output devices may be employed.

Storage media and computer readable media for containing code or portions of code can include any appropriate media known or used in the art including storage media and communication media such as but not limited to volatile and non volatile removable and non removable media implemented in any method or technology for storage and or transmission of information such as computer readable instructions data structures program modules or other data including RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disk DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the a system device. Based on the disclosure and teachings provided herein a person of ordinary skill in the art will appreciate other ways and or methods to implement the various embodiments.

The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. It will however be evident that various modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims.

