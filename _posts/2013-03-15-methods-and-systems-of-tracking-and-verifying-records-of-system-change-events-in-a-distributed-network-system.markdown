---

title: Methods and systems of tracking and verifying records of system change events in a distributed network system
abstract: This disclosure has reference to verifying records of system change events in a distributed network system providing cloud services. In one embodiment, the methods and systems observe system update messages sent and received among components of the distributed network system, generate a record of the state of the object in response to the update messages, and compare the record of the state of the object with information from a periodic system status message to verify the accuracy of the periodic system status message. Advantageously, the present embodiments provide increased reliability for system status tracking, resource management, and billing for consumption of resources in distributed network systems. Additional benefits and advantages of the present embodiments will become evident in the following description.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09397902&OS=09397902&RS=09397902
owner: RACKSPACE US, INC.
number: 09397902
owner_city: San Antonio
owner_country: US
publication_date: 20130315
---
This application is a continuation in part of and claims priority to co pending non provisional U.S. patent application Ser. No. 13 752 147 entitled Methods and Systems of Distributed Tracing filed Jan. 28 2013 Ser. No. 13 752 255 entitled Methods and Systems of Generating a billing feed of a distributed network filed Jan. 28 2013 and Ser. No. 13 752 234 entitled Methods and Systems of Function Specific Tracing filed Jan. 28 2013 each of which are incorporated in their entirety herein by reference. This application is related to co pending non provisional U.S. patent application Ser. No. 13 841 446 entitled Methods and Systems of Monitoring Failures in a Distributed Network System filed Mar. 15 2013 and Ser. No. 13 841 552 entitled Methods and Systems of Predictive Monitoring of Objects in a Distributed Network System filed Mar. 15 2013 each of which are incorporated in their entirety herein by reference.

The present disclosure relates generally to cloud computing and more particularly to systems and methods of tracking and verifying records of system change events in a distributed network system providing cloud services.

It is useful for a variety of reasons to track system resource usage system status and system object states. Some distributed network systems generate periodic system status messages. For example a system may generate a daily object state notification based upon an object state which may change from time to time in a given day. In such an example the system object may be a Virtual Machine VM which is built using distributed network resources such as storage drives memory and processing bandwidth and communication bandwidth. It may be useful to track the state of the VM for billing purposes system resource management orphan control etc.

There is currently no means for verifying the accuracy of system state notifications which can lead to billing errors system resource mismanagement and other undesirable errors. System state tracking notifications may be inaccurate for various reasons including missed system update notifications errors or faults during system setup or system update or glitches in system state tracking facilities. These faults can lead to costly billing or system resource management errors. Additionally it may be very difficult to trace the source of the error.

The following disclosure has reference to verifying records of system change events in a distributed network system providing cloud services.

In one embodiment the methods and systems observe system update messages sent and received among components of the distributed network system generate a record of the state of the object in response to the update messages and compare the record of the state of the object with information from a periodic system status message to verify the accuracy of the periodic system status message. Advantageously the present embodiments provide increased reliability for system status tracking resource management and billing for consumption of resources in distributed network systems. Additional benefits and advantages of the present embodiments will become evident in the following description.

According to one embodiment application includes event manager configured to provide system event management services. As will be described in more detail below event management can include verification of system object update records and tracking of system object states. By way of example event manager can observe messages within the distributed application across queues and from particular components of the application. As depicted in event manager interfaces with message service of application . Message service connects various subsystems of the application and message service may be configured to pass messages relative to one or more elements of system .

System may include one or more subsystems such as controllers and services . System may include one or more controllers for the application to be employed in a distributed architecture such as cloud computing services. As depicted in controllers include a compute controller a storage controller auth controller image service controller and network controller . Controllers are described with reference to a cloud computing architecture in . By way of example network controller deals with host machine network configurations and can perform operations for allocating IP addresses configuring VLANs implementing security groups and configuring networks. Each of controllers may interface with one or more services. As depicted in compute controller interfaces with compute pool storage controller may interface with object store auth controller may interface with authentication authorization controller image service controller may interface with image store and network controller may interface with virtual networking devices . Although controllers and services are with reference to an open architecture it should be appreciated that the methods and systems for tracing may be equally applied to other distributed applications.

Referring now to an external view of a cloud computing system is illustrated. Cloud computing system includes event manager and message service . According to one embodiment event manager can observe messages of cloud computing system and verify the accuracy of periodic system status messages issued by various components or objects of the could computing system . According to another embodiment controllers and services of the cloud computing system may include event managers to verify the periodic system status messages provided by each respective controller or service.

The cloud computing system includes a user device connected to a network such as for example a Transport Control Protocol Internet Protocol TCP IP network e.g. the Internet. The user device is coupled to the cloud computing system via one or more service endpoints . Depending on the type of cloud service provided these endpoints give varying amounts of control relative to the provisioning of resources within the cloud computing system . For example SaaS endpoint typically only gives information and access relative to the application running on the cloud storage system and the scaling and processing aspects of the cloud computing system is obscured from the user. PaaS endpoint typically gives an abstract Application Programming Interface API that allows developers to declaratively request or command the backend storage computation and scaling resources provided by the cloud without giving exact control to the user. IaaS endpoint typically provides the ability to directly request the provisioning of resources such as computation units typically virtual machines software defined or software controlled network elements like routers switches domain name servers etc. file or object storage facilities authorization services database services queue services and endpoints etc. In addition users interacting with an IaaS cloud are typically able to provide virtual machine images that have been customized for user specific functions. This allows the cloud computing system to be used for new user defined services without requiring specific support.

It is important to recognize that the control allowed via an IaaS endpoint is not complete. Within the cloud computing system are one or more cloud controllers running what is sometimes called a cloud operating system that work on an even lower level interacting with physical machines managing the contradictory demands of the multi tenant cloud computing system . In one embodiment these correspond to the controllers and services discussed relative to . The workings of the cloud controllers are typically not exposed outside of the cloud computing system even in an IaaS context. In one embodiment the commands received through one of the service endpoints are then routed via one or more internal networks . The internal network couples the different services to each other. The internal network may encompass various protocols or services including but not limited to electrical optical or wireless connections at the physical layer Ethernet Fiber channel ATM and SONET at the MAC layer TCP UDP ZeroMQ or other services at the connection layer and XMPP HTTP AMPQ STOMP SMS SMTP SNMP or other standards at the protocol layer. The internal network is typically not exposed outside the cloud computing system except to the extent that one or more virtual networks may be exposed that control the internal routing according to various rules. The virtual networks typically do not expose as much complexity as may exist in the actual internal network but varying levels of granularity can be exposed to the control of the user particularly in IaaS services.

In one or more embodiments it may be useful to include various processing or routing nodes in the network layers and such as proxy gateway . Other types of processing or routing nodes may include switches routers switch fabrics caches format modifiers or correlators. These processing and routing nodes may or may not be visible to the outside. It is typical that one level of processing or routing nodes may be internal only coupled to the internal network whereas other types of network services may be defined by or accessible to users and show up in one or more virtual networks . Either of the internal network or the virtual networks may be encrypted or authenticated according to the protocols and services described below.

In various embodiments one or more parts of the cloud computing system may be disposed on a single host. Accordingly some of the network layers and may be composed of an internal call graph inter process communication IPC or a shared memory communication system.

Once a communication passes from the endpoints via a network layer or as well as possibly via one or more switches or processing devices it is received by one or more applicable cloud controllers . The cloud controllers are responsible for interpreting the message and coordinating the performance of the necessary corresponding services returning a response if necessary. Although the cloud controllers may provide services directly more typically the cloud controllers are in operative contact with the service resources necessary to provide the corresponding services. For example it is possible for different services to be provided at different levels of abstraction. For example a service may be a compute service that will work at an IaaS level allowing the creation and control of user defined virtual computing resources. In addition to the services discussed relative to a cloud computing system may provide a declarative storage API a SaaS level Queue service a DNS service or a Database service or other application services without exposing any of the underlying scaling or computational resources. Other services are contemplated as discussed in detail below.

In various embodiments various cloud computing services or the cloud computing system itself may require a message passing system. The message routing service is available to address this need but it is not a required part of the system architecture in at least one embodiment. In one embodiment the message routing service is used to transfer messages from one component to another without explicitly linking the state of the two components. Note that this message routing service may or may not be available for user addressable systems in one preferred embodiment there is a separation between storage for cloud service state and for user data including user service state.

In various embodiments various cloud computing services or the cloud computing system itself may require a persistent storage for system state. The data store is available to address this need but it is not a required part of the system architecture in at least one embodiment. In one embodiment various aspects of system state are saved in redundant databases on various hosts or as special files in an object storage service. In a second embodiment a relational database service is used to store system state. In a third embodiment a column graph or document oriented database is used. Note that this persistent storage may or may not be available for user addressable systems in one preferred embodiment there is a separation between storage for cloud service state and for user data including user service state.

In various embodiments it may be useful for the cloud computing system to have a system controller . In one embodiment the system controller is similar to the cloud computing controllers except that it is used to control or direct operations at the level of the cloud computing system rather than at the level of an individual service.

For clarity of discussion above only one user device has been illustrated as connected to the cloud computing system and the discussion generally referred to receiving a communication from outside the cloud computing system routing it to a cloud controller and coordinating processing of the message via a service the infrastructure described is also equally available for sending out messages. These messages may be sent out as replies to previous communications or they may be internally sourced. Routing messages from a particular service to a user device is accomplished in the same manner as receiving a message from user device to a service just in reverse. The precise manner of receiving processing responding and sending messages is described below with reference to the various discussed service embodiments. One of skill in the art will recognize however that a plurality of user devices may and typically will be connected to the cloud computing system and that each element or set of elements within the cloud computing system is replicable as necessary. Further the cloud computing system whether or not it has one endpoint or multiple endpoints is expected to encompass embodiments including public clouds private clouds hybrid clouds and multi vendor clouds.

Each of the user device the cloud computing system the endpoints the cloud controllers and the cloud services typically include a respective information processing system a subsystem or a part of a subsystem for executing processes and performing operations e.g. processing or communicating information . An information processing system is an electronic device capable of processing executing or otherwise handling information such as a computer. shows an information processing system that is representative of one of or a portion of the information processing systems described above.

Referring now to diagram shows an information processing system configured to host one or more virtual machines coupled to a network . The network could be one or both of the networks and described above. An information processing system is an electronic device capable of processing executing or otherwise handling information. Examples of information processing systems include a server computer a personal computer e.g. a desktop computer or a portable computer such as for example a laptop computer a handheld computer and or a variety of other information handling systems known in the art. The information processing system shown is representative of one of or a portion of the information processing systems described above.

The information processing system may include any or all of the following a a processor for executing and otherwise processing instructions b one or more network interfaces e.g. circuitry for communicating between the processor and other devices those other devices possibly located across the network c a memory device e.g. FLASH memory a random access memory RAM device or a read only memory ROM device for storing information e.g. instructions executed by processor and data operated upon by processor in response to such instructions . In some embodiments the information processing system may also include a separate computer readable medium operably coupled to the processor for storing information and instructions as described further below.

In one embodiment there is more than one network interface so that the multiple network interfaces can be used to separately route management production and other traffic. In one exemplary embodiment an information processing system has a management interface at 1 GB s a production interface at 10 GB s and may have additional interfaces for channel bonding high availability or performance. An information processing device configured as a processing or routing node may also have an additional interface dedicated to public Internet traffic and specific circuitry or resources necessary to act as a VLAN trunk.

In some embodiments the information processing system may include a plurality of input output devices which are operably coupled to the processor for inputting or outputting information such as a display device a print device or other electronic circuitry for performing other operations of the information processing system known in the art.

With reference to the computer readable media including both memory device and secondary computer readable medium the computer readable media and the processor are structurally and functionally interrelated with one another as described below in further detail and information processing system of the illustrative embodiment is structurally and functionally interrelated with a respective computer readable medium similar to the manner in which the processor is structurally and functionally interrelated with the computer readable media and . As discussed above the computer readable media may be implemented using a hard disk drive a memory device and or a variety of other computer readable media known in the art and when including functional descriptive material data structures are created that define structural and functional interrelationships between such data structures and the computer readable media and other aspects of the system . Such interrelationships permit the data structures functionality to be realized. For example in one embodiment the processor reads e.g. accesses or copies such functional descriptive material from the network interface the computer readable media onto the memory device of the information processing system and the information processing system more particularly the processor performs its operations as described elsewhere herein in response to such material stored in the memory device of the information processing system . In addition to reading such functional descriptive material from the computer readable medium the processor is capable of reading such functional descriptive material from or through the network . In one embodiment the information processing system includes at least one type of computer readable media that is non transitory. For explanatory purposes below singular forms such as computer readable medium memory and disk are used but it is intended that these may refer to all or any portion of the computer readable media available in or to a particular information processing system without limiting them to a specific location or implementation.

The information processing system includes a hypervisor . The hypervisor may be implemented in software as a subsidiary information processing system or in a tailored electrical circuit or as software instructions to be used in conjunction with a processor to create a hardware software combination that implements the specific functionality described herein. To the extent that software is used to implement the hypervisor it may include software that is stored on a computer readable medium including the computer readable medium . The hypervisor may be included logically below a host operating system as a host itself as part of a larger host operating system or as a program or process running above or on top of a host operating system. Examples of hypervisors include Xenserver KVM VMware Microsoft s Hyper V and emulation programs such as QEMU.

The hypervisor includes the functionality to add remove and modify a number of logical containers associated with the hypervisor. Zero one or many of the logical containers contain associated operating environments . The logical containers can implement various interfaces depending upon the desired characteristics of the operating environment. In one embodiment a logical container implements a hardware like interface such that the associated operating environment appears to be running on or within an information processing system such as the information processing system . For example one embodiment of a logical container could implement an interface resembling an x86 x86 64 ARM or other computer instruction set with appropriate RAM busses disks and network devices. A corresponding operating environment for this embodiment could be an operating system such as Microsoft Windows Linux Linux Android or Mac OS X. In another embodiment a logical container implements an operating system like interface such that the associated operating environment appears to be running on or within an operating system. For example one embodiment of this type of logical container could appear to be a Microsoft Windows Linux or Mac OS X operating system. Another possible operating system includes an Android operating system which includes significant runtime functionality on top of a lower level kernel. A corresponding operating environment could enforce separation between users and processes such that each process or group of processes appeared to have sole access to the resources of the operating system. In a third environment a logical container implements a software defined interface such a language runtime or logical process that the associated operating environment can use to run and interact with its environment. For example one embodiment of this type of logical container could appear to be a Java Dalvik Lua Python or other language virtual machine. A corresponding operating environment would use the built in threading processing and code loading capabilities to load and run code. Adding removing or modifying a logical container may or may not also involve adding removing or modifying an associated operating environment . For ease of explanation below these operating environments will be described in terms of an embodiment as Virtual Machines or VMs but this is simply one implementation among the options listed above.

In one or more embodiments a VM has one or more virtual network interfaces . How the virtual network interface is exposed to the operating environment depends upon the implementation of the operating environment. In an operating environment that mimics a hardware computer the virtual network interface appears as one or more virtual network interface cards. In an operating environment that appears as an operating system the virtual network interface appears as a virtual character device or socket. In an operating environment that appears as a language runtime the virtual network interface appears as a socket queue message service or other appropriate construct. The virtual network interfaces VNIs may be associated with a virtual switch Vswitch at either the hypervisor or container level. The VNI logically couples the operating environment to the network and allows the VMs to send and receive network traffic. In one embodiment the physical network interface card is also coupled to one or more VMs through a Vswitch.

In one or more embodiments each VM includes identification data for use naming interacting or referring to the VM. This can include the Media Access Control MAC address the Internet Protocol IP address and one or more unambiguous names or identifiers.

In one or more embodiments a volume is a detachable block storage device. In some embodiments a particular volume can only be attached to one instance at a time whereas in other embodiments a volume works like a Storage Area Network SAN so that it can be concurrently accessed by multiple devices. Volumes can be attached to either a particular information processing device or a particular virtual machine so they are or appear to be local to that machine. Further a volume attached to one information processing device or VM can be exported over the network to share access with other instances using common file sharing protocols. In other embodiments there are areas of storage declared to be local storage. Typically a local storage volume will be storage from the information processing device shared with or exposed to one or more operating environments on the information processing device. Local storage is guaranteed to exist only for the duration of the operating environment recreating the operating environment may or may not remove or erase any local storage associated with that operating environment.

Between the various virtual machines and virtual devices it may be necessary to have a reliable messaging infrastructure. In various embodiments a message queuing service is used for both local and remote communication so that there is no requirement that any of the services exist on the same physical machine. Various existing messaging infrastructures are contemplated including AMQP ZeroMQ STOMP and XMPP. Note that this messaging system may or may not be available for user addressable systems in one preferred embodiment there is a separation between internal messaging services and any messaging services associated with user data.

In one embodiment the message service sits between various components and allows them to communicate in a loosely coupled fashion. This can be accomplished using Remote Procedure Calls RPC hereinafter to communicate between components built atop either direct messages and or an underlying publish subscribe infrastructure. In a typical embodiment it is expected that both direct and topic based exchanges are used. This allows for decoupling of the components full asynchronous communications and transparent balancing between equivalent components. In some embodiments calls between different APIs can be supported over the distributed system by providing an adapter class which takes care of marshalling and unmarshalling of messages into function calls.

In one embodiment a cloud controller or the applicable cloud service creates two queues at initialization time one that accepts node specific messages and another that accepts generic messages addressed to any node of a particular type. This allows both specific node control as well as orchestration of the cloud service without limiting the particular implementation of a node. In an embodiment in which these message queues are bridged to an API the API can act as a consumer server or publisher.

Turning now to one implementation of a message service is shown. For simplicity of description shows the message service when a single instance is deployed and shared in the cloud computing system but the message service can be either centralized or fully distributed.

In one embodiment the message service keeps traffic associated with different queues or routing keys separate so that disparate services can use the message service without interfering with each other. Accordingly the message queue service may be used to communicate messages between network elements between cloud services between cloud controllers between network elements or between any group of sub elements within the above. More than one message service may be used and a cloud service may use its own message service as required.

For clarity of exposition access to the message service will be described in terms of Invokers and Workers but these labels are purely expository and are not intended to convey a limitation on purpose in some embodiments a single component such as a VM may act first as an Invoker then as a Worker the other way around or simultaneously in each role. An Invoker is a component that sends messages in the system via two operations 1 an RPC Remote Procedure Call directed message and ii an RPC broadcast. A Worker is a component that receives messages from the message system and replies accordingly.

In one embodiment there is a message node including one or more exchanges . In a second embodiment the message system is brokerless and one or more exchanges are located at each client. The exchanges act as internal message routing elements so that components interacting with the message service can send and receive messages. In one embodiment these exchanges are subdivided further into a topic exchange and a direct exchange . An exchange is a routing structure or system that exists in a particular context. In a one embodiment multiple contexts can be included within a single message service with each one acting independently of the others. In one embodiment the type of exchange such as a topic exchange vs. direct exchange determines the routing policy. In a second embodiment the routing policy is determined via a series of routing rules evaluated by the exchange .

The direct exchange is a routing element created during or for RPC directed message operations. In one embodiment there are many instances of a direct exchange that are created as needed for the message service. In a further embodiment there is one direct exchange created for each RPC directed message received by the system.

The topic exchange is a routing element created during or for RPC directed broadcast operations. In one simple embodiment every message received by the topic exchange is received by every other connected component. In a second embodiment the routing rule within a topic exchange is described as publish subscribe wherein different components can specify a discriminating function and only topics matching the discriminator are passed along. In one embodiment there are many instances of a topic exchange that are created as needed for the message service. In one embodiment there is one topic based exchange for every topic created in the cloud computing system. In a second embodiment there are a set number of topics that have pre created and persistent topic exchanges

Within one or more of the exchanges it may be useful to have a queue element . A queue is a message stream messages sent into the stream are kept in the queue until a consuming component connects to the queue and fetches the message. A queue can be shared or can be exclusive. In one embodiment queues with the same topic are shared amongst Workers subscribed to that topic.

In a typical embodiment a queue will implement a FIFO policy for messages and ensure that they are delivered in the same order that they are received. In other embodiments however a queue may implement other policies such as LIFO a priority queue highest priority messages are delivered first or age oldest objects in the queue are delivered first or other configurable delivery policies. In other embodiments a queue may or may not make any guarantees related to message delivery or message persistence.

In one embodiment element is a topic publisher. A topic publisher is created instantiated or awakened when an RPC directed message or an RPC broadcast operation is executed this object is instantiated and used to push a message to the message system. Every publisher connects always to the same topic based exchange its life cycle is limited to the message delivery.

In one embodiment element is a direct consumer. A direct consumer is created instantiated or awakened if an RPC directed message operation is executed this component is instantiated and used to receive a response message from the queuing system. Every direct consumer connects to a unique direct based exchange via a unique exclusive queue identified by a UUID or other unique name. The life cycle of the direct consumer is limited to the message delivery. In one embodiment the exchange and queue identifiers are included the message sent by the topic publisher for RPC directed message operations.

In one embodiment elements elements and are topic consumers. In one embodiment a topic consumer is created instantiated or awakened at system start. In a second embodiment a topic consumer is created instantiated or awakened when a topic is registered with the message system . In a third embodiment a topic consumer is created instantiated or awakened at the same time that a Worker or Workers are instantiated and persists as long as the associated Worker or Workers have not been destroyed. In this embodiment the topic consumer is used to receive messages from the queue and it invokes the appropriate action as defined by the Worker role. A topic consumer connects to the topic based exchange either via a shared queue or via a unique exclusive queue. In one embodiment every Worker has two associated topic consumers one that is addressed only during an RPC broadcast operations and it connects to a shared queue whose exchange key is defined by the topic and the other that is addressed only during an RPC directed message operations connected to a unique queue whose with the exchange key is defined by the topic and the host.

In one embodiment element is a direct publisher. In one embodiment a direct publisher is created instantiated or awakened for RPC directed message operations and it is instantiated to return the message required by the request response operation. The object connects to a direct based exchange whose identity is dictated by the incoming message.

Turning now to one embodiment of the process of sending an RPC directed message is shown relative to the elements of the message system as described relative to . All elements are as described above relative to unless described otherwise. At step a topic publisher is instantiated. At step the topic publisher sends a message to an exchange . At step a direct consumer is instantiated to wait for the response message. At step the message is dispatched by the exchange . At step the message is fetched by the topic consumer dictated by the routing key either by topic or by topic and host . At step the message is passed to a Worker associated with the topic consumer . If needed at step a direct publisher is instantiated to send a response message via the message system . At step the direct publisher sends a message to an exchange . At step the response message is dispatched by the exchange . At step the response message is fetched by the direct consumer instantiated to receive the response and dictated by the routing key. At step the message response is passed to the Invoker.

Turning now to one embodiment of the process of sending an RPC broadcast message is shown relative to the elements of the message system as described relative to . All elements are as described above relative to unless described otherwise. At step a topic publisher is instantiated. At step the topic publisher sends a message to an exchange . At step the message is dispatched by the exchange . At step the message is fetched by a topic consumer dictated by the routing key either by topic or by topic and host . At step the message is passed to a Worker associated with the topic consumer .

In some embodiments a response to an RPC broadcast message can be requested. In that case the process follows the steps outlined relative to to return a response to the Invoker. As the process of instantiating and launching a VM instance in shows requests to a distributed service or application may move through various software components which may be running on one physical machine or may span across multiple machines and network boundaries.

Turning now to an IaaS style computational cloud service a compute service is shown at according to one embodiment. This is one embodiment of a cloud controller with associated cloud service as described relative to . Except as described relative to specific embodiments the existence of a compute service does not require or prohibit the existence of other portions of the cloud computing system nor does it require or prohibit the existence of other cloud controllers with other respective services .

To the extent that some components described relative to the compute service are similar to components of the larger cloud computing system those components may be shared between the cloud computing system and a compute service or they may be completely separate. Further to the extent that controllers nodes servers managers VMs or similar terms are described relative to the compute service those can be understood to comprise any of a single information processing device as described relative to multiple information processing devices a single VM as described relative to a group or cluster of VMs or information processing devices as described relative to . These may run on a single machine or a group of machines but logically work together to provide the described function within the system.

In one embodiment compute service includes an API Server a Compute Controller an Auth Manager an Object Store a Volume Controller a Network Controller and a Compute Manager . These components are coupled by a communications network of the type previously described. In one embodiment communications between various components are message oriented using HTTP or a messaging protocol such as AMQP ZeroMQ or STOMP.

Although various components are described as calling each other or sending data or messages one embodiment makes the communications or calls between components asynchronous with callbacks that get triggered when responses are received. This allows the system to be architected in a shared nothing fashion. To achieve the shared nothing property with multiple copies of the same component compute service further includes distributed data store . Global state for compute service is written into this store using atomic transactions when required. Requests for system state are read out of this store. In some embodiments results are cached within controllers for short periods of time to improve performance. In various embodiments the distributed data store can be the same as or share the same implementation as Object Store .

In one embodiment the API server includes external API endpoints . In one embodiment the external API endpoints are provided over an RPC style system such as CORBA DCE COM SOAP or XML RPC. These follow the calling structure and conventions defined in their respective standards. In another embodiment the external API endpoints are basic HTTP web services following a REST pattern and identifiable via URL. Requests to read a value from a resource are mapped to HTTP GETs requests to create resources are mapped to HTTP PUTs requests to update values associated with a resource are mapped to HTTP POSTs and requests to delete resources are mapped to HTTP DELETEs. In some embodiments other REST style verbs are also available such as the ones associated with WebDay. In a third embodiment the API endpoints are provided via internal function calls IPC or a shared memory mechanism. Regardless of how the API is presented the external API endpoints are used to handle authentication authorization and basic command and control functions using various API interfaces. In one embodiment the same functionality is available via multiple APIs including APIs associated with other cloud computing systems. This enables API compatibility with multiple existing tool sets created for interaction with offerings from other vendors.

The Compute Controller coordinates the interaction of the various parts of the compute service . In one embodiment the various internal services that work together to provide the compute service are internally decoupled by adopting a service oriented architecture SOA . The Compute Controller serves as an internal API server allowing the various internal controllers managers and other components to request and consume services from the other components. In one embodiment all messages pass through the Compute Controller . In a second embodiment the Compute Controller brings up services and advertises service availability but requests and responses go directly between the components making and serving the request. In a third embodiment there is a hybrid model in which some services are requested through the Compute Controller but the responses are provided directly from one component to another.

In one embodiment communication to and from the Compute Controller is mediated via one or more internal API endpoints provided in a similar fashion to those discussed above. The internal API endpoints differ from the external API endpoints in that the internal API endpoints advertise services only available within the overall compute service whereas the external API endpoints advertise services available outside the compute service . There may be one or more internal APIs that correspond to external APIs but it is expected that there will be a greater number and variety of internal API calls available from the Compute Controller .

In one embodiment the Compute Controller includes an instruction processor for receiving and processing instructions associated with directing the compute service . For example in one embodiment responding to an API call involves making a series of coordinated internal API calls to the various services available within the compute service and conditioning later API calls on the outcome or results of earlier API calls. The instruction processor is the component within the Compute Controller responsible for marshaling arguments calling services and making conditional decisions to respond appropriately to API calls.

In one embodiment the instruction processor is implemented as a tailored electrical circuit or as software instructions to be used in conjunction with a hardware processor to create a hardware software combination that implements the specific functionality described herein. To the extent that one embodiment includes computer executable instructions those instructions may include software that is stored on a computer readable medium. Further one or more embodiments have associated with them a buffer. The buffer can take the form of data structures a memory a computer readable medium or an off script processor facility. For example one embodiment uses a language runtime as an instruction processor running as a discrete operating environment as a process in an active operating environment or can be run from a low power embedded processor. In a second embodiment the instruction processor takes the form of a series of interoperating but discrete components some or all of which may be implemented as software programs. In another embodiment the instruction processor is a discrete component using a small amount of flash and a low power processor such as a low power ARM processor. In a further embodiment the instruction processor includes a rule engine as a submodule as described herein.

In one embodiment the Compute Controller includes a message queue as provided by message service . In accordance with the service oriented architecture described above the various functions within the compute service are isolated into discrete internal services that communicate with each other by passing data in a well defined shared format or by coordinating an activity between two or more services. In one embodiment this is done using a message queue as provided by message service . The message service brokers the interactions between the various services inside and outside the Compute Service .

In one embodiment the message service is implemented similarly to the message service described relative to . The message service may use the message service directly with a set of unique exchanges or may use a similarly configured but separate service.

The Auth Manager provides services for authenticating and managing user account role project group quota and security group information for the compute service . In a first embodiment every call is necessarily associated with an authenticated and authorized entity within the system and so is or can be checked before any action is taken. In another embodiment internal messages are assumed to be authorized but all messages originating from outside the service are suspect. In this embodiment the Auth Manager checks the keys provided associated with each call received over external API endpoints and terminates and or logs any call that appears to come from an unauthenticated or unauthorized source. In a third embodiment the Auth Manager is also used for providing resource specific information such as security groups but the internal API calls for that information are assumed to be authorized. External calls are still checked for proper authentication and authorization. Other schemes for authentication and authorization can be implemented by flagging certain API calls as needing verification by the Auth Manager and others as needing no verification.

In one embodiment external communication to and from the Auth Manager is mediated via one or more authentication and authorization API endpoints provided in a similar fashion to those discussed above. The authentication and authorization API endpoints differ from the external API endpoints in that the authentication and authorization API endpoints are only used for managing users resources projects groups and rules associated with those entities such as security groups RBAC roles etc. In another embodiment the authentication and authorization API endpoints are provided as a subset of external API endpoints .

In one embodiment the Auth Manager includes rules processor for processing the rules associated with the different portions of the compute service . In one embodiment this is implemented in a similar fashion to the instruction processor described above.

The Object Store provides redundant scalable object storage capacity for arbitrary data used by other portions of the compute service . At its simplest the Object Store can be implemented one or more block devices exported over the network. In a second embodiment the Object Store is implemented as a structured and possibly distributed data organization system. Examples include relational database systems both standalone and clustered as well as non relational structured data storage systems like MongoDB Apache Cassandra or Redis. In a third embodiment the Object Store is implemented as a redundant eventually consistent fully distributed data storage service.

In one embodiment external communication to and from the Object Store is mediated via one or more object storage API endpoints provided in a similar fashion to those discussed above. In one embodiment the object storage API endpoints are internal APIs only. In a second embodiment the Object Store is provided by a separate cloud service so the internal API used for compute service is the same as the external API provided by the object storage service itself.

In one embodiment the Object Store includes an Image Service . The Image Service is a lookup and retrieval system for virtual machine images. In one embodiment various virtual machine images can be associated with a unique project group user or name and stored in the Object Store under an appropriate key. In this fashion multiple different virtual machine image files can be provided and programmatically loaded by the compute service .

The Volume Controller coordinates the provision of block devices for use and attachment to virtual machines. In one embodiment the Volume Controller includes Volume Workers . The Volume Workers are implemented as unique virtual machines processes or threads of control that interact with one or more backend volume providers to create update delete manage and attach one or more volumes to a requesting VM.

In a first embodiment the Volume Controller is implemented using a SAN that provides a sharable network exported block device that is available to one or more VMs using a network block protocol such as iSCSI. In this embodiment the Volume Workers interact with the SAN to manage and iSCSI storage to manage LVM based instance volumes stored on one or more smart disks or independent processing devices that act as volume providers using their embedded storage . In a second embodiment disk volumes are stored in the Object Store as image files under appropriate keys. The Volume Controller interacts with the Object Store to retrieve a disk volume and place it within an appropriate logical container on the same information processing system that contains the requesting VM. An instruction processing module acting in concert with the instruction processor and hypervisor on the information processing system acts as the volume provider managing mounting and unmounting the volume on the requesting VM. In a further embodiment the same volume may be mounted on two or more VMs and a block level replication facility may be used to synchronize changes that occur in multiple places. In a third embodiment the Volume Controller acts as a block device proxy for the Object Store and directly exports a view of one or more portions of the Object Store as a volume. In this embodiment the volumes are simply views onto portions of the Object Store and the Volume Workers are part of the internal implementation of the Object Store .

In one embodiment the Network Controller manages the networking resources for VM hosts managed by the compute manager . Messages received by Network Controller are interpreted and acted upon to create update and manage network resources for compute nodes within the compute service such as allocating fixed IP addresses configuring VLANs for projects or groups or configuring networks for compute nodes.

In one embodiment the Network Controller may use a shared cloud controller directly with a set of unique addresses identifiers and routing rules or may use a similarly configured but separate service.

In one embodiment the Compute Manager manages computing instances for use by API users using the compute service . In one embodiment the Compute Manager is coupled to a plurality of resource pools each of which includes one or more compute nodes . Each compute node is a virtual machine management system as described relative to and includes a compute worker a module working in conjunction with the hypervisor and instruction processor to create administer and destroy multiple user or system defined logical containers and operating environments VMs according to requests received through the API. In various embodiments the pools of compute nodes may be organized into clusters such as clusters and . In one embodiment each resource pool is physically located in one or more data centers in one or more different locations. In another embodiment resource pools have different physical or software resources such as different available hardware higher throughput network connections or lower latency to a particular location.

In one embodiment the Compute Manager allocates VM images to particular compute nodes via a Scheduler . The Scheduler is a matching service requests for the creation of new VM instances come in and the most applicable Compute nodes are selected from the pool of potential candidates. In one embodiment the Scheduler selects a compute node using a random algorithm. Because the node is chosen randomly the load on any particular node tends to be non coupled and the load across all resource pools tends to stay relatively even.

In a second embodiment a smart scheduler is used. A smart scheduler analyzes the capabilities associated with a particular resource pool and its component services to make informed decisions on where a new instance should be created. When making this decision it consults not only all the Compute nodes across the resource pools until the ideal host is found.

In a third embodiment a distributed scheduler is used. A distributed scheduler is designed to coordinate the creation of instances across multiple compute services . Not only does the distributed scheduler analyze the capabilities associated with the resource pools available to the current compute service it also recursively consults the schedulers of any linked compute services until the ideal host is found.

In one embodiment either the smart scheduler or the distributed scheduler is implemented using a rules engine not shown and a series of associated rules regarding costs and weights associated with desired compute node characteristics. When deciding where to place an Instance rules engine compares a Weighted Cost for each node. In one embodiment the Weighting is just the sum of the total Costs. In a second embodiment a Weighting is calculated using an exponential or polynomial algorithm. In the simplest embodiment costs are nothing more than integers along a fixed scale although costs can also be represented by floating point numbers vectors or matrices. Costs are computed by looking at the various Capabilities of the available node relative to the specifications of the Instance being requested. The costs are calculated so that a good match has lower cost than a bad match where the relative goodness of a match is determined by how closely the available resources match the requested specifications.

In one embodiment specifications can be hierarchical and can include both hard and soft constraints. A hard constraint is a constraint is a constraint that cannot be violated and have an acceptable response. This can be implemented by having hard constraints be modeled as infinite cost requirements. A soft constraint is a constraint that is preferable but not required. Different soft constraints can have different weights so that fulfilling one soft constraint may be more cost effective than another. Further constraints can take on a range of values where a good match can be found where the available resource is close but not identical to the requested specification. Constraints may also be conditional such that constraint A is a hard constraint or high cost constraint if Constraint B is also fulfilled but can be low cost if Constraint C is fulfilled.

As implemented in one embodiment the constraints are implemented as a series of rules with associated cost functions. These rules can be abstract such as preferring nodes that don t already have an existing instance from the same project or group. Other constraints hard or soft may include a node with available GPU hardware a node with an available network connection over 100 Mbps a node that can run Windows instances a node in a particular geographic location etc.

When evaluating the cost to place a VM instance on a particular node the constraints are computed to select the group of possible nodes and then a weight is computed for each available node and for each requested instance. This allows large requests to have dynamic weighting if 1000 instances are requested the consumed resources on each node are virtually depleted so the Cost can change accordingly.

Turning now to a diagram showing one embodiment of the process of instantiating and launching a VM instance is shown as diagram . At time the API Server receives a request to create and run an instance with the appropriate arguments. In one embodiment this is done by using a command line tool that issues arguments to the API server . In a second embodiment this is done by sending a message to the API Server . In one embodiment the API to create and run the instance includes arguments specifying a resource type a resource image and control arguments. A further embodiment includes requester information and is signed and or encrypted for security and privacy. At time API server accepts the message examines it for API compliance and relays a message to Compute Controller including the information needed to service the request. In an embodiment in which user information accompanies the request either explicitly or implicitly via a signing and or encrypting key or certificate the Compute Controller sends a message to Auth Manager to authenticate and authorize the request at time and Auth Manager sends back a response to Compute Controller indicating whether the request is allowable at time . If the request is allowable a message is sent to the Compute Manager to instantiate the requested resource at time . At time the Compute Manager selects a Compute Worker and sends a message to the selected Worker to instantiate the requested resource. At time Compute Worker identifies and interacts with Network Controller to get a proper VLAN and IP address. At time the selected Worker interacts with the Object Store and or the Image Service to locate and retrieve an image corresponding to the requested resource. If requested via the API or used in an embodiment in which configuration information is included on a mountable volume the selected Worker interacts with the Volume Controller at time to locate and retrieve a volume for the to be instantiated resource. At time the selected Worker uses the available virtualization infrastructure to instantiate the resource mount any volumes and perform appropriate configuration. At time selected Worker interacts with Network Controller to configure routing. At time a message is sent back to the Compute Controller via the Compute Manager indicating success and providing necessary operational details relating to the new resource. At time a message is sent back to the API Server with the results of the operation as a whole. At time the API specified response to the original command is provided from the API Server back to the originally requesting entity. If at any time a requested operation cannot be performed then an error is returned to the API Server at time and the API specified response to the original command is provided from the API server at time . For example an error can be returned if a request is not allowable at time if a VLAN cannot be created or an IP allocated at time if an image cannot be found or transferred at time etc. Such errors may be one potential source of mistakes or inconsistencies in periodic system status notifications discussed below.

Having described an example of a distributed application and operation within a distributed network system various embodiments of methods and systems for verification of records of system change events in a distributed network system are described with references to . As used herein a distributed network system may relate to one or more services and components and in particular cloud services. Various embodiments of the methods and systems disclosed herein may permit verification of records of system change events in a distributed network system providing cloud services.

Reselling system may be configured as an intermediary for selling and or providing services of cloud computing system to one or more entities such as customers. Services by reseller system may be based on requests such as customer billable request . Based on received requests for cloud services reseller system may generate one or more customer bills . Similarly reseller system may generate one or more requests such as billable requests for cloud services. Based on requested services buy reseller system cloud computing system may generate one or more reseller bills . According to one embodiment customer bills generated by reseller system may be based on one or more of billing feed and service fees such as reseller bills . It is advantageous to verify the accuracy of records upon which the reseller bills and customer bills are based according to the present embodiments.

Regardless of the component used to fulfill the request received by the public API a notification is generated and sent to notification queue . Event manager may then observe messages or notifications associated with the request received by the public API. In one embodiment event manager may directly access notification queue . Alternatively event manager may observe the notifications as they leave notification queue and are communicated between controllers components by message service . In one embodiment notification queue may be integrated with message service . In another embodiment notification queue may be maintained separate from message service .

In one embodiment notification router may communicate messages from notification queue . In the depicted embodiment notification router is illustrated as an integrated component with event manager . In an alternative embodiment notification router may be integrated with message service . In still a further embodiment notification router may be coupled to message service for the purpose of observing messages communicated between controllers components by message service . Notification router may communicate messages notifications to real time usage queue and also to usage queue .

The messages in usage queue may be used by usage processor to generate a periodic system status message. In one embodiment the periodic system status messages are generated on a daily basis. One of ordinary skill in the art will recognize that other embodiments may exist where the period of generating the periodic system status messages is different. For example the periodic system status messages may be generated hourly weekly bi weekly monthly quarterly yearly etc. In alternative embodiments the usage processor may be implement in a distributed fashion where each of a plurality of hosts in the distributed network system includes a process for generating a host specific periodic system status message and communicate that to usage database for aggregation or for independent analysis.

Real time usage processor may collect usage messages from real time usage queue and construct a record of the state of the object in response to the update messages. The record may be maintained in real time or near real time as compared with the periodic system status messages. In one embodiment the record may be chronologically arranged for example in a timeline such that sources of errors in the periodic system status message can be more effectively identified.

In one embodiment real time usage processor may store the record in usage database . In one embodiment an updated record may be stored in the usage database each time the real time usage processor updates the record. Additionally the usage processor may store the periodic system status message in usage database . In one embodiment both the record and the periodic system status message may be stored in the same usage database . In another embodiment the record may be stored in separate usage databases.

The usage auditor may access both the record of the state of the object and the most recent periodic system status message from the usage database . The usage auditor may then compare the state of the object as described in the periodic system status message with the expected state as defined by the record at the time the periodic usage message was generated. In a further embodiment the usage auditor may determine the state of the object in the record based upon a timestamp included with the periodic system status message. For example the periodic system status message may include a specific date and time and the usage auditor may use that timestamp to align the periodic system status message with the proper point in the record for verification that the periodic system status message is correct.

In one embodiment a usage API may also be provided. For example in the system of reseller system may access usage API to obtain verified usage data or system status information for billing. Alternatively one or more of the system processors utilities management components auditing components or the like may access usage API to obtain verified usage data.

The method continues at block when event manager generates a record of the state of the object in response to the update messages. For example the real time usage processor may generate the record of the state of the object and pass the record to the usage database .

At block the method also includes receiving a periodic system status message comprising information regarding the state of the object. For example the usage auditor may receive the periodic system status message from the usage database . In on embodiment the usage auditor may be configured to query the usage database at a scheduled and regular interval. For example the usage auditor may query the usage database daily at a predetermined time of day. The usage processor may be configured to generate and store the periodic system status message in the usage database .

The method may also include comparing the information from the periodic system status message with the record of the state of the object to verify the accuracy of the periodic system status message as shown at block . The usage auditor may perform the comparison upon receiving both the record and the periodic system status message. For example the usage auditor may compare object properties including processing properties memory properties data storage properties network access properties and various other properties associated with a VM. In other embodiments usage auditor may compare object properties such as a number of images associated with an account a volume of data stored in a data storage object and the like. On of ordinary skill in the art will recognize additional object properties associated with various system objects that may be verified. In one embodiment the usage auditor compares the properties described in the periodic system status message with expected values for those properties based on the record of system change events in the distributed network system. In a further embodiment the usage auditor may identify an error or time of error in response to a discrepancy between the information in the periodic system status message and the time associated with the discrepancy in the record.

In one embodiment the event manager may receive a first state notification that includes information regarding properties of a VM instance. The first state notification may be one embodiment of a periodic system status message. In this embodiment the VM instance is given an identification number 1234 for tracking purposes. The properties included in this embodiment of a state notification include a memory volume and a listing of disks with associated disk volume. One of ordinary skill will recognize that other properties associated with VM 1234 may be included in first state notification including processing bandwidth or number of processing cores associated with VM 1234 network access bandwidth or number of Network Interface Cards NICs associated with VM 1234 and the like.

First state notification may be used as a starting point for generating reconstructed state . Reconstructed state is one embodiment of a record of system change events in a distributed network system. In one embodiment a first reconstructed state record is generated in response to the first state notification . The object properties described in the first state notification are included in record and record is associated with the time line at a time that the first state notification is received i.e. 00 00 AM in this example .

In one embodiment first state notification may form a starting point for reconstructed state . In other embodiments for example where first state notification is not available reconstructed state may be generated in response to system update messages without the benefit of knowing an initial system state.

In the example described in event manager observes a VM resize notification . Resize notification may be observed in response to for example a memory resize request received from public API . In this embodiment the resize notification indicates that the memory allocation associated with VM 1234 has been changes from 1024 MB to 2048 MB. Accordingly reconstructed state may be updated at block to show the updated system state which now includes 2048 MB of memory. Block may be associated with the timeline at the time corresponding to the timestamp in resize notification which is 10 00 AM in this embodiment.

Similarly at 06 00 PM event manager may observe a disk attach notification . Disk attach notification may indicated that a new disk with a size of 50 GB has been associated with VM 1234. Accordingly at block reconstructed state is updated to include the original 80 GB and 10 GB disks as well as a new 50 GB disk. In one embodiment block may be associated with the timeline at the time indicated by the time stamp in disk attach notification which is 06 00 PM in this example.

If it turns out that the customer changes his mind about adding the new 50 GB disk or if a system error occurred or for a variety of other reasons the newly allocated 50 GB disk may be removed and a disk remove notification may be observed by event manager . Accordingly reconstructed state may be updated at block to remove the 50 GB disk leaving only the 80 GB disk and the 10 GB disk originally described in first state notification .

In one embodiment a second state notification may be issued at the end of the day or at the beginning of the next day. The second state notification may be generated by usage processor and stored in usage database . Similarly the reconstructed state as reflected in record may be stored by real time usage processor into usage database . The usage auditor may then receive both the second state notification and the record and compare at block to determine whether the second state notification matches record . If the second state notification matches record then the second state notification is verified and the process repeats for a new day based upon the information in the second state notification . If however the information in second state notification does not match the information in record then an error is identified.

In one embodiment usage auditor may generate an alarm alert or electronic notification that an error was identified. Alternatively usage auditor may trigger another component of event manager to generate the alarm alert or other electronic notification. Embodiments of alarms alerts or electronic notifications include emails text messages blinking lights sirens or sounds log records data tags or flags etc. One of ordinary skill in the art will recognize a variety of alarms alerts or electronic notifications that may be suitable for use with the present embodiments.

Such an embodiment may allow a system administrator to ensure that there are no orphaned VMs on the host. Orphaned VMs may reside on the host and consume valuable host resources but may not be associated with any user accounts. In one embodiment the periodic system update message may for example include a list of ten VMs on the host. In reality however the record may show that there are in fact fifteen VMs consuming system resources on the host. Such errors may occur for various reasons including coding glitches communication errors VM deletion process failures etc.

Orphaned files may become orphaned when they are no longer associated with a user account. Orphaned files are problematic because they continue to use system resources but the cloud storage provider can no longer bill for maintaining them. A file may become orphaned through various software or process errors or glitches.

Another embodiment is illustrated in . The method described in may verify that a list of IP address allocations is accurate. The method may also provide an opportunity to track potentially fraudulent activities related to IP address allocation. In one embodiment the method may also be implemented by event manager .

The method may include observing IP address allocation messages associated with a customer account as shown at block . Alternatively the IP addresses may be tracked on the basis of the server issuing the IP address. The method may also include generating a record of the state of the user account in response to the IP allocation messages as shown at block . Alternatively the record may be generated with reference to a server issuing IP addresses.

As shown at block the method may also include receiving a periodic system status message comprising a list of IP addresses associated with the customer account. In an alternative embodiment the periodic system status message may comprise a list of IP addresses issued by an IP address server or an IP Address Management IPAM process.

The method may further include comparing the periodic system status message with the record to verify the list of IP addresses associated with the user account as shown at block . Alternatively the list of IP addresses may be associated with an IP address server or IPAM process.

The method may also include generating a record of the state of the host or cloud storage device configured to store the VM images in response to the update messages as shown at block . The method may further include receiving a periodic system status message comprising a list of images on the host or cloud storage as shown at block . In one embodiment the method may also include comparing the system status message with the record to verify the image state from the host or cloud storage as shown at block .

In each of the embodiments described in the operations of methods may be carried out by one or more of the components of event manager described in Fig. Each of the various methods described may include further operations. For example the methods may include determining whether a discrepancy exists between the periodic system status message and the record of the state of the object.

In one embodiment generating the record may include recording a timestamp associated with each of a plurality of update messages ordering the update messages chronologically according to the timestamps and generating a timeline of the state of the object in response to the ordered update messages as illustrated in .

The methods may also include determining a time at which a system error occurred resulting in a discrepancy between the record of the state of the object and the periodic system status message in response to information on the timeline. An alert may be generated in response to a determination that the record of the state of the object and the periodic system status message are inconsistent. The alert may include an electronic notification to a system administrator.

The method may also include updating information in one or more system records or databases associated with the object in response to a determination that a discrepancy exists between the periodic system status message and the record of the state of the object.

Each day a new record may be generated in response to the periodic system status update message from the previous day in one embodiment. Indeed the record of the state of the object may be regenerated in response to update messages received after receipt of the periodic system status message. In such embodiments the periodic system status message is a starting point for generating the record of the state of the object.

In various embodiments the periodic system status message may be received hourly daily weekly bi weekly monthly quarterly yearly or at any other interval suitable for use with the present embodiments.

In one embodiment tracking and verifying periodic system status messages is implemented as an electrical circuit or as software instructions to be used in conjunction with a hardware processor to create a hardware software combination that implements the specific functionality described herein. To the extent that one embodiment includes computer executable instructions those instructions may include software that is stored on a computer readable medium. Further one or more embodiments have associated with them a buffer. The buffer can take the form of data structures a memory a computer readable medium or an off script processor facility. For example one embodiment uses a language runtime as an instruction processor running as a discrete operating environment as a process in an active operating environment or can be run from a low power embedded processor. In a second embodiment the instruction processor takes the form of a series of interoperating but discrete components some or all of which may be implemented as software programs. In another embodiment the instruction processor is a discrete component using a small amount of flash and a low power processor such as a low power ARM processor. In a further embodiment the instruction processor includes a rule engine as a submodule as described herein.

Although illustrative embodiments have been shown and described a wide range of modification change and substitution is contemplated in the foregoing disclosure and in some instances some features of the embodiments may be employed without a corresponding use of other features. Accordingly it is appropriate that the appended claims be construed broadly and in a manner consistent with the scope of the embodiments disclosed herein.

