---

title: Wearable user device enhanced display system
abstract: Systems and methods for displaying information on a wearable user device include determining calibration information based on user actions and information from images captured by a camera on a wearable user device. A field of view calibration may then be performed on a display engine in the wearable user device using the calibration information. Graphical information may then be displayed on a display device on the wearable user device according to a user field of view using the display engine in the wearable user. Field of view calibrations may be performed using calibration information that is based on: a user's head range of motion such that displayed graphical information conforms to that head range of motion, calibration objects by themselves that indicate a user's perspective in a user field of view, and calibration objects that allow the size of a user's hands to be determined and used to measure objects.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09448689&OS=09448689&RS=09448689
owner: PayPal, Inc.
number: 09448689
owner_city: San Jose
owner_country: US
publication_date: 20130927
---
This application claims priority to U.S. Provisional Patent Application Ser. No. 61 872 041 filed on Aug. 30 2013 and entitled WEARABLE USER DEVICE ENHANCED DISPLAY SYSTEM the entire disclosure of which is hereby incorporated herein by reference.

The present invention generally relates to wearable user devices and more particularly to a system for providing enhanced display features on a wearable use device.

Wearable user devices such as for example Google Glass available from Google Inc. of Mountain View Calif. may include wearable computing devices that feature a head mounted display device that is included on a frame similar to an eyeglass frame that the user wears on their head such that the display device is viewable in front of at least one eye of the user and between the eye of the user and physical objects the user is viewing. Users of such wearable user devices may interact with the wearable user device to provide instructions by speaking those instructions aloud and the wearable user device may then use voice recognition techniques to interpret those instructions so that they may be executed by for example displaying graphical user interfaces icons and or other information in on the display device such that that information is viewable over the user s field of view. Instructions may also be provided to the wearable user device through a touchpad located on the frame that allows the user to swipe through information e.g. icons displayed on the display device. However conventional display techniques for such information on a wearable user device has limited the functionality of wearable user devices as such conventional techniques do not take into account information about the users field of view.

Embodiments of the present disclosure and their advantages are best understood by referring to the detailed description that follows. It should be appreciated that like reference numerals are used to identify like elements illustrated in one or more of the figures wherein showings therein are for purposes of illustrating embodiments of the present disclosure and not for purposes of limiting the same.

The present disclosure provides systems and methods for enhancing the display of information on a wearable user device by determining calibration information based on for example user actions performed in conjunction with information from images captured by a camera on the wearable user device. The calibration information may be used to perform a field of view calibration on a display engine that provides for the display of graphical information on a display device of the wearable user device. Subsequent to the field of view calibration the display engine may display graphical information on the display device according to a user field of view. One example of a field of view calibration is based upon a range of head motion of the user and the display of graphical information according to the user field of view may include modifying the size of icons displayed to the user on the display device. Another example of a field of view calibration is based upon a primary calibration object and the display of graphical information according to the user field of view may include displaying perspective enhanced graphical information using secondary calibration objects that are in the user field of view. Yet another example of a field of view calibration is based upon a user s hands and a calibration object and the display of graphical information according to the user field of view may include displaying measurement graphical information that includes a determined measurement distance between the user s hands in the user field of view.

Referring now to an embodiment of a method for providing enhanced display features on a wearable user device is illustrated. Generally the method includes determining calibration information at block performing a field of view calibration using the calibration information at block and displaying information according to a user field of view at block . In some of the embodiments discussed below the calibration information is determined based on at least one user action and information from at least one image captured by a camera on the wearable user device. However other calibration techniques may be utilized that will enable the field of view calibration and thus will fall within the scope of the present disclosure. Furthermore while the embodiments discussed below discuss the display of graphical information according to the user field of view the display of information other than purely graphical information is envisioned as falling within the scope of the present disclosure e.g. different types e.g. size font and or other text characteristics of alphanumeric text may fall within the scope of graphical information . Thus while a variety of specific examples are illustrated and described below one of skill in the art in possession of the present disclosure will recognize that a wide variety of modifications to those specific examples will fall within the scope of the present disclosure.

Referring first to a few embodiments for determining calibration information at block of the method are illustrated. In the embodiments illustrated in different ranges of head motion of a user are determined as calibration information and as discussed in further detail below may be utilized for adjusting the display of graphical information to the user based on that user s range of head motion field of view. Referring initially to a user is wearing a wearable user device that includes a display device that is positioned in front of one or more of the user s eyes. As discussed in further detail below the wearable user device includes a non transitory memory having instruction that when executed by one or more hardware processors in the wearable user device cause the one or more hardware processors to provide a display engine that performs the functions of the wearable user device discussed herein.

At block the user may approach a calibrator that in the illustrated embodiments is positioned on a wall and includes a vertical calibrator portion and a horizontal calibrator portion . In response to recognizing the calibrator e.g. using image recognition techniques an instruction received from the user or to a variety of other calibration initiation instructions known in the art the display engine in the wearable user device may provide a calibration screen illustrated in on the display device . In the illustrated embodiment the calibration screen includes a calibration instruction that instructs the user to stand a distance from the calibrator with their head level but any variety of calibration techniques may be provided and result in any appropriate calibration instruction for that calibration technique. The calibration screen also includes a plurality of head orientation indicators including in the illustrated embodiment a head pitch indicator a head yaw indicator and a head roll indicator . In an embodiment a gyroscope or other orientation determination device in the wearable user device operates to determine orientation information related to the user s head and that orientation information may be reported by the display engine in the indicators on the calibration screen .

In an embodiment of block the user stands at a distance from the wall and or calibrator e.g. 5 feet in the illustrated embodiment and positions their head until it is substantially level such that each of the head pitch indicator head yaw indicator and head roll indicator on the calibration screen are zeroed out as illustrated in . In response to receiving the orientation information from the orientation determination device in the wearable user device that results in the display engine providing the zeroed out indicators the display engine may save that zeroed out orientation information as calibration information. In the illustrated embodiment the display engine has provided calibration information lines for the zeroed out orientation information on the calibration screen including a vertical calibration information line and a horizontal calibration information line along with a centering complete indicator that indicates to the user that the centering portion of the calibration procedure is complete. In an embodiment the vertical calibration information line and the horizontal calibration information line are provided based on the user action of the user standing a predetermined distance from the calibrator with their head level along with the images captured of the calibrator by a camera on the wearable user device . Furthermore if the size of the calibrator is known or retrievable by the wearable user device given the known distance the wearable user device may perform a distance calibration similarly as discussed below with reference to and

As discussed in further detail below the zeroing out calibration operations illustrated in provide a number of benefits for the wearable user device enhanced display system described herein. For example the zeroing out calibration options provide a reference point for a viewing height of the user e.g. the horizontal calibration line and can be used in making measurements in the field of view of the user rendering perspective enhanced graphics and determining what is within a viewing range of the user . In some embodiments the calibrator may include measurement indicators such as the lines on the vertical calibrator portion and horizontal calibrator portion of the calibrator that are illustrated in that may include any unit of measurement known in the art e.g. inches centimeters etc. . In other embodiments the calibrator may simply include an image that is recognizable to the display engine and the display engine may use previously received information about the user e.g. a height of the user the calibrator and or other reference information to perform the zeroing out operations discussed above.

Referring now to embodiments for determining calibration information at block of the method are illustrated. The embodiments illustrated in may follow the zeroing out operations discussed above with reference to . In one example the display engine may display a calibration screen that is similar to the calibration screen illustrated in but that instructs the user to pitch their head up and then down as far as they can while standing in front of the calibrator illustrated in in order provide calibration information for head pitch. As illustrated in the user may pitch their head up by moving their head in a direction A as far as they can followed by pitching their head down by moving their head in a direction B as far as they can. In an embodiment the crossed vertical calibration line and horizontal calibration line illustrated in may be provided on the display device such they do not move relative to the display device . This may be done to provide a target for the user to view as they pitch their head through its range of motion such that the eyes of the user remain focused on a point of the display device throughout the head pitch calibration.

Referring now to an embodiment of the calibration screen is illustrated that is provided by the display engine at the first end of a head range of motion of the user . For example the orientation determination device in the wearable user device may provide the display engine with orientation information that the head of the user is performing a pitching motion and as that pitching motion slows to a stop the display engine may determine calibration information that includes the first end of the range of head pitch of the user illustrated in with the user having moved their head in the direction A . In response to determining the first end of the range of head pitch of the user the display engine provides the head pitch indicator maxed out to the up pitch and saves that first end of the range of head pitch as calibration information. In the illustrated embodiment the display engine has provided a horizontal calibration information line for the first end of the range of head pitch on the calibration screen along with a complete indicator that indicates to the user that the first end of the head pitch calibration is complete. In an embodiment the horizontal calibration information line is provided based on the user action of pitching their head up as far as they can along with the images captured of the calibrator by the camera on the wearable user device e.g. at the end of the range of motion .

Referring now to an embodiment of the calibration screen is illustrated that is provided by the display engine at the second end of a head range of motion of the user . For example the orientation determination device in the wearable user device may provide the display engine with orientation information that the head of the user is performing a pitching motion and as that pitching motion slows to a stop the display engine may determine calibration information that includes the second end of the range of head pitch of the user illustrated in with the user having moved their head in the direction B . In response to determining the second end of the range of head pitch of the user the display engine provides the head pitch indicator maxed out to the down pitch and saves that second end of the range of head pitch as calibration information. In the illustrated embodiment the display engine has provided a horizontal calibration information line for the second end of the range of head pitch on the calibration screen along with the complete indicator that indicates to the user that the second end of the head pitch calibration is complete. In an embodiment the horizontal calibration information line is provided based on the user action of pitching their head down as far as they can along with the images captured of the calibrator by a camera on the wearable user device e.g. at the end of the range of motion .

Referring now to embodiments for determining calibration information at block of the method are illustrated. The embodiments illustrated in may follow the zeroing out operations discussed above with reference to . In one example the display engine may display a calibration screen that is similar to the calibration screen illustrated in but that instructs the user to yaw their head right and then left as far as they can while standing in front of the calibrator illustrated in in order provide calibration information for head yaw. As illustrated in the user may yaw their head right by moving their head in a direction C as far as they can followed by yawing their head left by moving their head in a direction D as far as they can. In an embodiment the crossed vertical calibration line and horizontal calibration line illustrated in may be provided on the display device such they do not move relative to the display device . This may be done to provide a target for the user to view as they yaw their head through its range of motion such that the eyes of the user remain focused on a point of the display device throughout the head yaw calibration.

Referring now to an embodiment of the calibration screen is illustrated that is provided by the display engine at the first end of a head range of motion of the user . For example the orientation determination device in the wearable user device may provide the display engine with orientation information that the head of the user is performing a yawing motion and as that yawing motion slows to a stop the display engine may determine calibration information that includes the first end of the range of head yaw of the user illustrated in with the user having moved their head in the direction C . In response to determining the first end of the range of head yaw of the user the display engine provides the head yaw indicator maxed out to the right yaw and saves that first end of the range of head yaw as calibration information. In the illustrated embodiment the display engine has provided a vertical calibration information line for the first end of the range of head yaw on the calibration screen along with the complete indicator that indicates to the user that the first end of the head yaw calibration is complete. In an embodiment the vertical calibration information line is provided based on the user action of yawing their head right as far as they can along with the images captured of the calibrator by a camera on the wearable user device e.g. at the end of the range of motion .

Referring now to an embodiment of the calibration screen is illustrated that is provided by the display engine at the second end of a head range of motion of the user . For example the orientation determination device in the wearable user device may provide the display engine with orientation information that the head of the user is performing a yawing motion and as that yawing motion slows to a stop the display engine may determine calibration information that includes the second end of the range of head yaw of the user illustrated in with the user having moved their head in the direction D . In response to determining the second end of the range of head yaw of the user the display engine provides the head yaw indicator maxed out to the left yaw and saves that second end of the range of head yaw as calibration information. In the illustrated embodiment the display engine has provided a vertical calibration information line for the second end of the range of head yaw on the calibration screen along with the complete indicator that indicates to the user that the second end of the head yaw calibration is complete. In an embodiment the vertical calibration information line is provided based on the user action of yawing their head left as far as they can along with the images captured of the calibrator by a camera on the wearable user device e.g. at the end of the range of motion .

Referring now to embodiments for determining calibration information at block of the method are illustrated. The embodiments illustrated in may follow the zeroing out operations discussed above with reference to . In one example the display engine may display a calibration screen that is similar to the calibration screen illustrated in but that instructs the user to roll their head right and then left as far as they can while standing in front of the calibrator illustrated in in order provide calibration information for head roll. As illustrated in the user may roll their head right by moving their head in a direction E as far as they can followed by rolling their head left by moving their head in a direction F as far as they can. In an embodiment the crossed vertical calibration line and horizontal calibration line illustrated in may be provided on the display device such they do not move relative to the display device . This may be done to provide a target for the user to view as they roll their head through its range of motion such that the eyes of the user remain focused on a point of the display device throughout the head roll calibration.

Referring now to an embodiment of the calibration screen is illustrated that is provided by the display engine at the first end of a head range of motion of the user . For example the orientation determination device in the wearable user device may provide the display engine with orientation information that the head of the user is performing a rolling motion and as that rolling motion slows to a stop the display engine may determine calibration information that includes the first end of the range of head roll of the user illustrated in with the user having moved their head in the direction E . In response to determining the first end of the range of head roll of the user the display engine provides the head roll indicator maxed out to the right roll and saves that first end of the range of head roll as calibration information. In the illustrated embodiment the display engine has provided the horizontal and vertical calibration information lines and discussed above with reference to rotated in a direction G for the first end of the range of head roll on the calibration screen along with the complete indicator that indicates to the user that the first end of the head roll calibration is complete. In an embodiment the vertical calibration information line and the horizontal calibration information line rotated in the direction G are provided based on the user action of rolling their head right as far as they can along with the images captured of the calibrator by a camera on the wearable user device e.g. at the end of the range of motion .

Referring now to an embodiment of the calibration screen is illustrated that is provided by the display engine at the second end of a head range of motion of the user . For example the orientation determination device in the wearable user device may provide the display engine with orientation information that the head of the user is performing a rolling motion and as that rolling motion slows to a stop the display engine may determine calibration information that includes the second end of the range of head roll of the user illustrated in with the user having moved their head in the direction F . In response to determining the second end of the range of head roll of the user the display engine provides the head roll indicator maxed out to the left roll and saves that second end of the range of head roll as calibration information. In the illustrated embodiment the display engine has provided the horizontal and vertical calibration information lines and discussed above with reference to rotated in a direction H for the second end of the range of head roll on the calibration screen along with the complete indicator that indicates to the user that the second end of the head roll calibration is complete. In an embodiment the vertical calibration information line and the horizontal calibration information line rotated in the direction H are provided based on the user action of rolling their head left as far as they can along with the images captured of the calibrator by a camera on the wearable user device e.g. at the end of the range of motion .

Thus following the calibration operations illustrated and described with reference to the display engine in the wearable user device may have stored calibration information that details a range of head motion for the user . As detailed above the range of head motion of the user may be determined by user performing user actions that include pitching yawing and rolling their head along with information from images captured by a camera on the wearable user device of the calibrator . However other techniques may be used to determine the range of head motion of a user.

The method may then proceed to block where a field of view calibration is performed using the calibration information determined in block . Referring now to a schematic view of an embodiment of a field of view calibration is illustrated. The embodiment illustrated in is intended to schematically represent multiple user fields of view that may be determined by the display engine in the wearable user device using range of head motion calibration discussed above. In the illustrated embodiment the field of view calibration includes a normal user field of view and a restricted user field of view both plotted on a vertical and horizontal scale to illustrate the relative differences between those user fields of view. For example the normal user field of view may be determined by performing the field of view calibration on the display engine for a user that has a normal range of head motion i.e. no restrictions in head pitch yaw and or roll movement while the restricted user field of view may be determined by performing the field of view calibration on the display engine for a user that has a restricted range of head motion relative to the normal range of head movement i.e. restrictions in head pitch yaw and or roll movement . As can be seen by comparing the normal user field of view and the restricted user field of view relative to the vertical and horizontal scale the field of view calibration provide the display engine e.g. programs the display engine with information about the limits of the user s ability to view information displayed on the display device by pitching yawing and rolling their head. As discussed in further detail below the display of information on the display device of the wearable user device may change based on a user s range of head motion such that the display engine will display information differently for a user with the normal user field of view relative to a user with the restricted field of view .

The method may then proceed to block where information is displayed according to a user field of view. As discussed in further detail below the user field of view at block may be the user field of view that is based on the range of head motion of the user as illustrated in a user field of view that is included in images captured by a camera on the wearable user device and or combinations thereof. Referring now to a schematic view of the display of graphical information according to a first user field of view is illustrated. In an embodiment the schematic view illustrates the display of information to a user with the restricted field of view discussed above with reference to . The schematic view includes a screen shot that provides a first person view of the screen displayed on the display device to a user with the restricted field of view as well as a top view that schematically illustrates how information is perceived by that user . The screenshot and top view illustrate how the user has a stationary head viewing range i.e. a viewing range provided without the user moving their head that may be extended into extended range and extended range by the user yawing their head left and right. The limits of the extended ranges and are determined according to the calibration information and field of view calibration discussed above.

In an embodiment at block the display engine displays a plurality of icon graphical elements and according to the user field of view that includes the stationary head viewing range and the extended ranges and . The restricted range of head motion which provide the stationary head viewing range and the extended ranges and illustrated in is used by the display engine to determine an icon detail that in the illustrated embodiment is a size of the icons graphical elements . At block the display engine displays the icon graphical elements at a first size such that the icon graphical elements are viewable in the stationary head viewing range the icon graphical element is viewable in the extended range and the icon graphical element is viewable in the extended range .

Referring now to a schematic view of the display of graphical information according to a second user field of view is illustrated. In an embodiment the schematic view illustrates the display of information to a user with the normal field of view discussed above with reference to . The schematic view includes a screen shot that provides a first person view of the screen displayed on the display device to a user with the normal field of view as well as a top view that schematically illustrates how information is perceived by that user . The screenshot and top view illustrate how the user has a stationary head viewing range i.e. a viewing range provided without the user moving their head that may be extended into extended range and extended range by the user yawing their head left and right. The limits of the extended ranges and are determined according to the calibration information and the field of view calibration discussed above.

In an embodiment at block the display engine displays a plurality of icon graphical elements and according to the user field of view that includes the stationary head viewing range and the extended ranges and . The normal range of head motion which provides the stationary head viewing range and the extended ranges and illustrated in may be used by the display engine to determine an icon detail that in the illustrated embodiment is a size of the icons graphical elements . At block the display engine displays the icon graphical elements at a second size such that a portion of the icon graphical elements and as well as the icon graphical elements are viewable in the stationary head viewing range the icon graphical element and a portion of the icon graphical element are viewable in the extended range and the icon graphical element and a portion of the icon graphical element are viewable in the extended range .

Thus depending on the user field of view as determined by the field of view calibration based on the range of head movement of the user icon graphical elements may be displayed at different sizes. For example the embodiments illustrated in illustrate how the display engine may display the same number of icon graphical elements but may change their size depending on the user range of head motion to the calibration information and field of view. While changing icon size based on the user field of view has been illustrated and described above one of skill in the art in possession of the present disclosure will recognize that a wide variety of icon details may be changed based on the user range of head motion field of view while falling within the scope of the present disclosure. Furthermore while only head yaw range of motion has been illustrated and described as being used to determine how to display information at block one of skill in the art in possession of the present disclosure will appreciate that information may be adjusted for display by the display engine based on the head pitch range of motion and the head roll range of motion as well.

Referring next to embodiments for determining calibration information at block of the method are illustrated. In the embodiments illustrated in a primary calibration object is used to determine calibration information and as discussed in further detail below may then be utilized for adjusting the display of graphical information to the user based on secondary calibration objects in the user s field of view. Referring initially to the user may wear the wearable user device that includes the display device that is positioned in front of a user s eye. As discussed in further detail below the wearable user device includes a non transitory memory that includes instructions that when executed by one or more hardware processors in the wearable user device cause the one or more hardware processors to provide a display engine that performs the functions of the wearable user device discussed herein.

At block the user may approach a primary calibration object that in the illustrated embodiment is positioned on the wall and includes a horizontal line a vertical line and a plurality of circular calibration target elements and . In response to recognizing the primary calibration object e.g. using image recognition techniques an instruction received from the user or to a variety of other calibration initiation instructions known in the art the display engine in the wearable user device may provide a calibration screen illustrated in on the display device . In the illustrated embodiment the calibration screen includes a calibration instruction that instructs the user to stand a distance from the primary calibration object and center the primary calibration object in their view.

In an embodiment of block the user performs a user action by standing at the distance from the wall and or primary calibration object e.g. a distance marked by a line on the floor in the illustrated embodiment and views the primary calibration object such that the intersection of the horizontal line and the vertical line that are within the circular target element is at the center of the user s field of view. The camera on the wearable user device captures images of the primary calibration object and provides to images to the display engine for use in determining whether the primary calibration object is centered in the user s view e.g. based on the intersection of the horizontal line and the vertical line that are within the circular target element being at the center of the calibration screen .

Referring now to in response to determining that the primary calibration object is centered in the user s view the display engine may display centering graphics on the display device including horizontal centering lines vertical centering lines and circular centering indicators along with a calibration complete indicator which operate to indicate to the user that they have properly centered the primary calibration object in their view. Furthermore the display engine may save calibration information that includes a known size of primary calibration object and its features e.g. the horizontal line vertical line and plurality of circular target elements and the known distance between the user and the primary calibration object and information from the images captured of the centered primary calibration object e.g. the perceived sizes of the different features of the primary calibration object e.g. the horizontal line vertical line and plurality of circular target elements and .

The method then proceeds to block where a field of view calibration is performed using the calibration information determined at block . Using the calibration information determined according to the embodiment illustrated in a field of view calibration may be performed on the display engine that programs the display engine with information about the size of the primary calibration object and its perceived size when viewed by the user through the wearable user device . As discussed below being programmed with the calibration information determined as discussed above with reference to allows the display engine to provide perspective enhanced graphical information based on secondary calibration objects that are located in the user s field of view.

Referring now to an embodiment of a perspective enhanced graphical information screen is illustrated that includes perspective enhanced graphical information that may be displayed according to a user s field of view at block of the method . In the embodiment illustrated in the perspective enhanced graphical information screen is provided on the display device such that it is viewed by the user over physical objects in the user s field of view. For example a plurality of physical objects e.g. shelving units in the illustrated embodiment on a floor are located opposite the display device from the user s eye with a plurality of secondary calibration objects positioned in view of a camera on the wearable user device e.g. on the sides of the physical objects in the illustrated embodiment . In an embodiment each of the secondary calibration objects may be substantially similar to the primary calibration object discussed above including the same dimensions and the same sized features e.g. the same size horizontal lines vertical lines and circular target elements .

In the embodiment illustrated in user may have provided an instruction to the wearable user device to find a product that is located in one of the shelving unit physical objects . The wearable user device may then operate to determine the location of the requested product e.g. by referencing a database of product locations communicating with the requested product using wireless communication techniques etc. . Furthermore the camera on the wearable user device may capture images of the field of view of the user that includes the physical objects floor and secondary calibration objects . In one example the wearable user device may use the determined location of the product the image of the user s field of view received from the camera a determined location of the user e.g. received from a location determination device in the wearable user device and or other information known in the art to determine a location of the product in the user s field of view and display a product indicator on the perspective enhanced graphical information screen that indicates the location of the product in the field of view of the user as well as a product direction graphical information that provides the user with directions to the product along with a distance to the product e.g. 31 feet in the illustrated embodiment .

In the example illustrated in the product indicator and the product direction graphical information are perspective enhanced graphical information that is provided by the display engine using the secondary calibration objects included in one or more images taken of the field of view of the user . The display engine uses the sizes of the secondary calibration objects in the image of the field of view of the user along with the calibration information determined based on the calibration using the primary calibration object discussed above to calculate how far away those secondary calibration objects are from the user . That calculation allows the display engine to display the perspective enhanced graphical information such as the product identifier which may be displayed as larger as the user physically approaches the product being indicated and the product direction graphical information which is displayed such that it appears to be located on the floor and extending a distance away from the user . As the user moves towards the product identified by the product indicator the display engine may continuously receive images from the camera on the wearable user device and use those images each of which may include the secondary calibration objects to modify the display of the product indicator and the product direction graphical information such that product indicator is enlarged and the product direction graphical information does not appear to the user to change dimensions relative to the physical objects and floor in the field of view of the user . The display engine may also continuously calculate the distance of the user from the product indicated by the product indicator to update the distance between the user and the product that is displayed on the product direction graphical information .

Referring now to an embodiment of a perspective enhanced graphical information screen is illustrated that includes perspective enhanced graphical information that may be displayed according to a user s field of view at block of the method . In the embodiment illustrated in the perspective enhanced graphical information screen is provided on the display device such that it is viewed by the user over physical objects in the user s field of view. For example a plurality of physical objects e.g. product boxes in the illustrated embodiment on a pallet are located opposite the display device from the user s eye. In an embodiment secondary calibration objects not illustrated but similar to the secondary calibration objects discussed above with reference to may be positioned on or near the physical objects such that those secondary calibration objects are in view of a camera on the wearable user device . In an embodiment each of the secondary calibration objects may be substantially similar to the primary calibration object discussed above including the same dimensions and the same sized features e.g. the same size horizontal lines vertical lines and circular target elements .

In the embodiment illustrated in user is viewing the plurality of product box physical objects positioned on the pallet . The wearable user device may operate to automatically determine whether the product box physical objects on the pallet correspond to details about an expected number of product box physical objects in a database in response to the product box physical objects being in the field of view of the user in response to an instruction from the user e.g. an instruction by the user to check product order etc. For example the camera on the wearable user device may capture images of the field of view of the user that includes the physical objects the pallet and the secondary calibration object s . In one example the wearable user device may use the information about the product box physical objects in the database the image of the user s field of view received from the camera and or other information known in the art to determine a number of product box physical objects on the pallet determine whether that number of product box physical objects matches the expected number of product box physical objects in the database and in the illustrated embodiment provide product box graphical indicators if the number of product box physical objects does not match the expected number of product box physical objects in the database.

In the example illustrated in the product box graphical indicators are perspective enhanced graphical information that are provided by the display engine using the secondary calibration objects included in one or more images taken of the field of view of the user . The display engine uses the sizes of the secondary calibration objects in the image of the field of view of the user along with the calibration information determined based on the calibration using the primary calibration object discussed above the sizes of the product box physical objects in the field of view of the user and the expected number of product box physical objects in the database to display the product box graphical indicators in a manner that indicates to the user that product box physical objects may be missing from an order e.g. as wire frame representations of the product box physical objects that should be located on the pallet according to the expected number of product box physical objects in the database . As illustrated the display engine may display the perspective enhanced graphical information such as the product box graphical indicators so that they appear stacked upon the product box physical objects and each other which provides the user with perspective enhanced graphics that allow the user to quickly recognize that product box physical objects are missing from an order. As the user moves towards or around the product box physical objects the display engine may continuously receive images from the camera on the wearable user device and use those images which include the secondary calibration objects to modify the display of the product box graphical indicators such that product box graphical indicators are enlarged rotated and or otherwise reoriented to stationary in their perceived positioning on the product box physical objects located in the field of view of the user . The display engine may also display a warning on the display device that includes an order number along with a number of product box physical objects that the display engine determined were missing from that order.

Referring now to an embodiment of a perspective enhanced graphical information screen is illustrated that includes perspective enhanced graphical information that may be displayed according to a user s field of view at block of the method . In the embodiment illustrated in the perspective enhanced graphical information screen is provided on the display device such that it is viewed by the user over physical objects in the user s field of view. For example a plurality of physical objects and e.g. customers and a shopping cart in the illustrated embodiment in a store are located opposite the display device from the user s eye. In an embodiment shelving units in the store include secondary calibration objects that are in view of a camera on the wearable user device . In an embodiment each of the secondary calibration objects may be substantially similar to the primary calibration object discussed above including the same dimensions and the same sized features e.g. the same size horizontal lines vertical lines and circular target elements .

In the embodiment illustrated in the user is viewing the plurality of customer physical objects and shopping cart physical objects positioned in the store. The wearable user device may operate to recognize customers e.g. using facial recognition techniques through communication with customer devices such as mobile phones and or using a variety of other techniques known in the art and in response retrieve information e.g. from a database in a merchant device about those customers including customer names customer average spending habits customer purchase histories and or a variety of other customer information known in the art. The wearable user device may also operate to communicate with the shopping cart physical objects and in response retrieve information about the products a customer has placed in those shopping cart physical objects e.g. product identifications product prices cumulative price of a plurality of products etc. . Furthermore the camera on the wearable user device may capture images of the field of view of the user that includes the customer physical objects the shopping cart physical objects and the secondary calibration objects . In one example the wearable user device may use the information retrieved from customer physical objects shopping cart physical objects images of the user s field of view received from the camera and or other information known in the art to generate customer information windows and along with shopping cart information window .

In the example illustrated in the customer information windows and the shopping cart information window are perspective enhanced graphical information that are provided by the display engine using the secondary calibration objects included in one or more images taken of the field of view of the user . The display engine uses the sizes of the secondary calibration objects in the image of the field of view of the user along with the calibration information determined based on the calibration using the primary calibration object discussed above and the retrieved customer and shopping cart information to display the customer information windows and the shopping cart information window in a variety of manners that may depend on the distance of customers and shopping carts from the user . As illustrated the display engine may display the perspective enhanced graphical information as the customer information windows and the shopping cart information window such that more information is displayed for a customer or shopping cart when that customer or shopping cart is close to the user. As the user moves relative to customer physical objects and or shopping physical objects the display engine may continuously receive images from the camera on the wearable user device and use those images which include the secondary calibration objects to modify the display of the customer physical objects and or shopping physical objects such that customer physical objects and or shopping physical objects are enlarged shrunk include more information include less information etc.

In the specific example illustrated in the customer information window is displayed for a customer that is closest to the user and thus that customer information window is larger than the other customer information windows and includes information such as a customer name an average amount spent by the customer in a typical shopping trip a time that the customer has spent in the store a previous purchase of the customer a brand preference of the customer and or a variety of other customer information known in the art. The customer information windows and are displayed for customers that are an intermediate distance from the user including a customer that is partially obscured by a shelving unit and includes information such as a customer name and average amount spent by the customer in a typical shopping trip. Thus as can be seen the customer information windows and are smaller than the customer information window based on the display engine determining that the customer for the customer information window is closer to the user than the customers for the customer information windows and

Similarly the customer information window is displayed for a customer that is the furthest distance from the user and includes information about the average amount spent by the customer in a typical shopping trip. Thus as can be seen the customer information window is smaller than the customer information windows and based on the display engine determining that the customer for the customer information window is further from the user than the customers for the customer information windows and . The customer information windows are for customers that have been detected but are outside the user s field of view and include information about the average amount spent by those customers in a typical shopping trip. As can be seen the display engine may have been primary calibration object unable to retrieve data for several customers outside of the users view or those customers may be new customers and thus no customer information is available for those customers e.g. the 22 customers to the left of the user that have an average spending amount of 0 and the 31 customers to the right of the user that have an average spending amount of 0 . The shopping cart information window is displayed for a shopping cart that is the relative far from the user and includes information about the cumulative amount of products currently in the shopping cart.

Referring now to an embodiment of a perspective enhanced graphical information screens and are illustrated that includes perspective enhanced graphical information that may be displayed according to a user s field of view at block of the method at different times as a physical object becomes close to the user . In an embodiment the perspective enhanced graphical information screens and of may be displayed as a customer and shopping cart approach the user in the store in . In the embodiment illustrated in the perspective enhanced graphical information screens and are provided at different times e.g. the perspective enhanced graphical information screen followed by the perspective enhanced graphical information screen followed by the perspective enhanced graphical information screen on the display device such that it is viewed by the user over physical objects in the user s field of view. For example a plurality of physical objects and e.g. a customer and a shopping cart in the illustrated embodiment in a store are located opposite the display device from the user s eye. In an embodiment secondary calibration objects e.g. similar to the secondary calibration objects in are in view of a camera on the wearable user device . In an embodiment each of the secondary calibration objects may be substantially similar to the primary calibration object discussed above including the same dimensions and the same sized features e.g. the same size horizontal lines vertical lines and circular target elements .

In the embodiment illustrated in user is viewing the customer physical object and shopping cart physical object as the customer approaches the user with the shopping cart . As discussed with reference to the wearable user device may operate to recognize customers and in response retrieve information about those customers as well as communicate with shopping carts to retrieve information about the products a customer has placed in those shopping cart physical objects . The camera on the wearable user device may capture images of the field of view of the user that includes the customer physical object the shopping cart physical object and the secondary calibration objects. In one example the wearable user device may use the information retrieved from customer physical object the shopping cart physical object images of the user s field of view received from the camera and or other information known in the art to generate customer information window and shopping cart information window .

However as can be seen from the graphical information screens and in as the customer physical object and the shopping cart physical object move closer to the user the display engine uses the images of the user s field of view received from the camera to determine their relative distance from the user and operates to enlarge the customer information window and shopping cart information window and in the illustrated embodiment provide more information about the customer in the customer information window and more information about the shopping cart contents in the shopping cart information window . For example the graphical information screen illustrates the customer information window and shopping cart information window when the customer physical object and shopping cart physical object are furthest away from the user with the customer information window displaying an average amount the customer typically spends at the store and the shopping cart information window displaying a cumulative amount of the products currently in the shopping cart.

The graphical information screen illustrates the customer information window and shopping cart information window when the customer physical object and shopping cart physical object have moved closer to the user relative to the graphical information screen with the customer information window displaying a customer name and an average amount the customer typically spends at the store and the shopping cart information window displaying a cumulative amount of the products currently in the shopping cart as well as product descriptions for products in the shopping cart. The graphical information screen illustrates the customer information window and shopping cart information window when the customer physical object and shopping cart physical object have moved closer to the user relative to the graphical information screen with the customer information window displaying a customer name an average amount the customer typically spends at the store a time the customer has spent in the store a previous purchase of the customer and a brand preference of the customer and the shopping cart information window displaying a product descriptions for products in the shopping cart and a price of each of those products. Thus the display engine may provide perspective enhanced graphics than change based on the user s field of view and in particular based on perspective details of physical objects in the user s field of view.

While the examples of the provision of perspective enhanced graphical information above rely on specific secondary calibration objects positioned in the user field of view that are related to the primary calibration object detailed in other methods of calibration to provide perspective enhanced graphical information is envisioned as falling within the scope of the present disclosure. In one embodiment the wearable user device may be able to retrieve dimensions for a variety of physical objects in the user s field of view and use those dimensions along with images of the user s field of view to provide perspective enhanced graphical information. For example the wearable user device may be able to retrieve known dimensions for a stop sign and thus a stop sign in a user s field of view may be utilized as the secondary calibration objects discussed above to provide perspective enhanced graphical information. Similarly the wearable user device may be able to retrieve known dimensions for products and thus products in a user s field of view may be utilized as the secondary calibration objects discussed above to provide perspective enhanced graphical information. As such in some situations the wearable user device may be pre calibrated and the calibration discussed above with reference to may be skipped.

Furthermore in addition to the calibration and image information discussed above other external data sources may be utilized to enhance the display features of the wearable user device. For example map data customer data product data and or a variety of other external data available to the wearable user device e.g. over the network may be retrieved and displayed along with or in conjunction with the enhanced graphical information.

Referring next to embodiments for determining calibration information at block of the method are illustrated. In the embodiments illustrated in a calibration object is used along with the hands of the user to determine calibration information and as discussed in further detail below may then be utilized making measurements using the user s hands when the user s hands are in the user s field of view. Referring initially to the user is wearing the wearable user device that includes the display device that is positioned in front of the user s eye. As discussed in further detail below the wearable user device includes a non transitory memory that includes instruction that when executed by one or more hardware processors in the wearable user device cause the one or more hardware processors to provide a display engine that performs the functions of the wearable user device discussed herein.

At block the user may approach a calibration object that in the illustrated embodiment is positioned on the wall and includes a plurality of rectangular measurement elements and hand positioning elements and hand measurement grids . In response to recognizing the calibration object e.g. using image recognition techniques an instruction received from the user or to a variety of other calibration initiation instructions known in the art the display engine in the wearable user device may provide a calibration screen illustrated in on the display device . In the illustrated embodiment the calibration screen includes a calibration instruction that instructs the user to place their hands on the positioning elements of the calibration object .

In an embodiment of block the user performs a user action by placing their hands on the positioning elements of the calibration object and views the calibration object such that the user s hands are in the user s field of view as illustrated in . The camera on the wearable user device captures images of the calibration object and the users hands and provides those images to the display engine for use in determining calibration information such as the size of the user s hands and the perceived distance between the user s hands on the calibration object . In response to determining that the user s hands are properly positioned on the calibration object the display engine may display calibration graphics on the display device adjacent each of the user s hands which operate to indicate to the user that they have properly positioned their hands on the calibration object . Furthermore the display engine may save calibration information that includes a known size of the calibration object and its features e.g. the plurality of rectangular measurement elements the known distance between the positioning elements on the calibration object and information from the images captured of the user s hands on the calibration object .

Referring now to an embodiment of a user s hand positioned on the hand measurement grid on the calibration object is illustrated. As can be seen the user s hand includes a finger and a thumb that are positioned over the hand measurement grid . The measurement grid may include grid cells of known sizes e.g. square inches square centimeters etc. that allow the finger and thumb of the user to be measured accurately including measurements of length and width of the finger and thumb that may be saved as calibration information.

The method then proceeds to block where a field of view calibration is performed using the calibration information determined at block . Using the calibration information determined according to the embodiment illustrated in a field of view calibration may be performed on the display engine that programs the display engine with information about the size of the size of the user s hands finger thumb and the distance between the hands positioned on the positioning elements on the calibration object as perceived by the user . As discussed below being programmed with the calibration information determined as discussed above with reference to allows the display engine to determine measurement distances between the user s hands and display measurement graphical information related to that measurement distance.

Referring now to an embodiment of a measurement graphical information screen is illustrated that includes measurement graphical information that may be displayed according to a user s field of view at block of the method . In the embodiment illustrated in the measurement graphical information screen is provided on the display device such that it is viewed by the user over physical objects in the user s field of view. For example a physical object e.g. a product box in the illustrated embodiment is located opposite the display device from the user s eye along with the user s hands with a finger on each hand extended.

In the embodiment illustrated in the user may wish to measure one or more dimensions of the physical object and may provide an instruction to the wearable user device to begin measurement by speaking such an instruction positioning their hands as illustrated in and or in a variety of other manners. Furthermore the camera on the wearable user device may capture images of the field of view of the user that includes the physical object and user hands with fingers extended. In one example the wearable user device may recognize the instruction to begin measurement and in response provide a measurement indicator that includes a measured distance between the user s fingers a measurement line corresponding to a finger on a first of the user hands and a measurement line corresponding to a finger on a second of the user hands .

In the example illustrated in measurement indicator including the measurement line and the measurement line are measurement graphical information that is provided by the display engine using the user hands and fingers included in one or more images taken of the field of view of the user . The display engine uses the sizes of the user hands and fingers in the image of the field of view of the user along with the calibration information determined based on the calibration using the calibration object discussed above to determine the distance between the fingers on the user hands . That calculation allows the display engine to display the measurement indicator including the measurement line corresponding to a finger on a first of the user hands and the measurement line corresponding to a finger on a second of the user hands .

In an embodiment as the user moves their hands and fingers apart e.g. in opposite directions A and B towards the edges of the physical object the display engine may continuously receive images from the camera on the wearable user device . The display engine may then use those images which include the hands and fingers of the user to determine a measurement distance between the fingers of the user and modify the display of the measurement indicator including the measurement line and the measurement line such that measurements lines and move apart from each other and the measurement indicator indicates the increasing measurement distance determined between the measurement lines and . Thus the user may use the fingers on their hands to determine a measurement of a length of an edge of the physical object .

Referring now to the user may manipulate the fingers and thumbs on their hands to measure more than one dimension on the physical object . As illustrated in the user has moved their hands and fingers apart relative to the embodiment illustrated in as discussed above and then moved their fingers and thumbs such that their fingers on opposite hands are touching the opposite corners on a top edge of the physical object and their thumbs on opposite hands are positioned adjacent the a bottom edge of the physical object that is opposite its top edge. In one example the wearable user device may recognize the instruction to begin measurement of more than one dimension e.g. a voice instruction the above describe hand finger and thumb movements etc. and in response provide the measurement indicator that includes the measured horizontal distance e.g. 11.5 inches in the illustrated embodiment between the user s fingers the measurement line corresponding to a finger on a first of the user hands the measurement line corresponding to a finger on a second of the user hands and in addition a measured vertical distance e.g. 4.2 inches in the illustrated embodiment between the user s fingers a measurement line corresponding to the fingers on the user hands and a measurement line corresponding to the thumbs on the user hands .

In the example illustrated in measurement indicator including the measurement line the measurement line the measurement line and the measurement line are measurement graphical information that is provided by the display engine using the user hands fingers and thumbs included in one or more images taken of the field of view of the user . The display engine uses the sizes of the user hands fingers and thumbs in the image of the field of view of the user along with the calibration information determined based on the calibration using the calibration object discussed above to determine the horizontal distance between the fingers on the user hands as well as the vertical distance between the fingers and thumbs on the user hands . That calculation allows the display engine to display the measurement indicator including the measurement line corresponding to a finger on a first of the user hands and the measurement line corresponding to a finger on a second of the user hands the measurement line corresponding to the fingers on the user hands and a measurement line corresponding to the thumbs on the user hands

In an embodiment as the user moves their hands fingers and thumbs apart e.g. in the opposite directions A and B towards the edges of the physical object as illustrated in extending the finger and thumb on a hand apart etc. the display engine may continuously receive images from the camera on the wearable user device and use those images which include the hands fingers and thumbs of the user to determine the horizontal measurement distance between the fingers of the user and the vertical measurement distance between the fingers and thumbs of the user and modify the display of the measurement indicator including the measurement line the measurement line the measurement line and the measurement line such that measurements lines and move apart from each other the measurement lines and move apart from each other and the measurement indicator indicates the horizontal measurement distance determined between the measurement lines and and the vertical measurement distance between the measurement lines and . Thus the user may use the fingers and thumbs on their hands to determine a measurement of a length and width of the physical object .

The display engine may also determine measurement distances in 3 dimensions using the calibration information determined based on the calibration using the calibration object discussed above along with image captured by the camera on the wearable user device . For example using the physical object illustrated in rather than positioning the hands fingers and thumbs along a common surface of the physical object the user may position the finger and thumb on a first hand around a corner adjacent a top surface of the physical object and position the finger and thumb on a second hand around an opposite corner on the physical object that is adjacent a bottom surface of the physical object . The display engine may then use image recognition techniques to recognize the 3 dimensional nature of the physical object along with the calibration information including the known size of the users hand fingers and thumbs to provide measurement indicators and distances for a length width and height of the physical object .

Referring now to another example of 3 dimensional measurement is illustrated. In this embodiment the user is using their hand to hold a physical object such that their thumb is visible. As discussed above the display engine may determine measurement distances in 3 dimensions using the calibration information determined based on the calibration using the calibration object discussed above along with images captured by the camera in the wearable user device . For example image recognition techniques may be used by the display engine to recognize the edges of the physical object including a length edge a height edge and a width edge . Using that image recognized information and the known size of the thumb of the user a measurement graphical information screen may be provided on the display device that includes a plurality of corner measurement indicators and that are displayed over the user s field of view of the physical object to indicate where the display engine detects the corners of the physical object along with a length measurement e.g. 3.2 inches in the illustrated embodiment a height measurement e.g. 6.1 inches in the illustrated embodiment and a width measurement e.g. 1.0 inches in the illustrated embodiment .

In some embodiments the wearable user device may use the measurements determined to display the measurement graphical information screen discussed above to provide other information to the user . For example in the embodiment illustrated in the display engine has used the length measurement height measurement and width measurement of the physical object as well as the images of the physical object to query a database of products e.g. in a merchant device to determine whether the measurements of the physical object match expected measurements in the database. In the illustrated embodiment a match indicator is being displayed on the measurement graphical information screen that indicates that the physical object being held by the user matches the dimensions of a product stored in the database.

Referring now to a measurement graphical information screen is illustrated that may be displayed on the display device in substantially the same manner as discussed above for the measurement graphical information screen of . In the embodiment illustrated in a physical object e.g. a person is located opposite the display device from the user and the user is using the fingers on each hand to measure a distance on the physical object e.g. a distance between the person s shoulders . The display engine is displaying measurement indicators and for the fingers on each hand of the user and a measurement distance that is determined based on the calibration using the calibration object as detailed above. As discussed with reference to the user may move their hands and fingers relative to each other such that the display of the measurement indicators and are moved and the measurement distance changes until the user is displayed a measurement distance of a desired feature on the physical objet . In some situations the fingers of the user may be touching the physical object to provide the measurement distance displayed. In other embodiments the physical object may be a distance away from the fingers of the user and other calibration techniques discuss herein may be leveraged to determine an accurate measurement of the physical object e.g. the person may include a secondary calibration object around their neck as discussed above with reference to and such that the display engine may determine how far away the person is from the user s fingers and thus how the measurement distance between the user s fingers corresponds to the portion of the person in the user s field of view that is between the user s fingers .

Referring now to a graphical information screen is illustrated being displayed over a user field of view that includes physical objects such as shelving units a floor and a ceiling . The graphical information screen may be provided using calibration information obtained using any of the systems and methods described above. In the illustrated embodiment the wearable user device is being worn by a user that has be tasked with retrieving a product within a predetermined amount of time e.g. a worker in a warehouse retrieving a product requested by a customer . In response the display engine has searched a database for the product and determined a product physical location e.g. in the warehouse along with a user location and provided graphical information on the display device including a perspective enhanced direction graphic that details directions between the user and the product e.g. within the warehouse a product graphic that illustrates the product to the user and a timing graphic that shows the user how long they have to retrieve the product.

In the illustrated embodiment the perspective enhanced direction graphic provides directions e.g. through the warehouse that are based on a known layout of the physical location such that the perspective enhanced direction graphic extend along the floor around the shelving units displayed on the display device as ghost lines over the shelving units and up to another floor that is above the ceiling also displayed on the display device as ghost lines over the ceiling . As can be seen the perspective enhanced direction graphic displayed by the ghost lines over the ceiling provide perspective indicators to indicate that the product being searched for is approximately located on a floor above the current location of the user and directly above that location. The user may then follow the perspective enhanced direction graphic to the product use the product graphic to confirm the product found and watch the timing graphic to ensure that the product is being retrieved in the desired amount of time.

Referring now to a graphical information screen is illustrated being displayed over a user field of view that includes physical objects such as a customer cups a drink machine drink ingredients and storage cabinets . The graphical information screen may be provided using calibration information obtained using any of the systems and methods described above. In the illustrated embodiment the wearable user device is being worn by a user that has been tasked with making a drink for the customer within a predetermined amount of time e.g. an employee in a coffee shop making a coffee requested by a customer . In response the display engine may have used image recognition techniques to identify and track the customer and in response provide a customer indicator on the display device that is displayed over the user s view of the customer wherever the customer is in the user s field of view. In addition the display engine may use the determined identity of the customer e.g. through image recognition techniques from information retrieved when the customer made a purchase etc. to retrieve customer information and provide a customer information window on the display device that includes an order number for the customer a photo of the customer a name of the customer and the details of the order of the customer .

In addition the display engine may use the order information received from the customer to determine order creation details that detail how the customer s order is created which may be used to display portions of the customer information window . Those order creation details may be used by the display engine along with image recognition techniques that identify the cups by type to provide a cup indicator over the appropriate cup for the order of the customer . Those order creation details may also be used by the display engine along with image recognition techniques that identify the drink machine and its components to provide a drink making instruction over the appropriate portion of the drink machine such that the user may operate the drink machine e.g. by using the identified component twice to create the order of the customer . Those order creation details may also be used by the display engine along with image recognition techniques that identify the drink ingredients by type to provide a drink making instruction over the appropriate drink ingredient such that the user may add the appropriate drink ingredient e.g. by providing 4 servings of the identified drink ingredient to create the order of the customer . Those order creation details may also be used by the display engine along with information retrieved e.g. from a database about the contents and locations of items in the cabinets to provide a drink making instruction over the cabinet such that the user may retrieve the appropriate drink ingredient e.g. by retrieving soy milk from the cabinet to create the order of the customer . While not illustrated the order of the steps in making the customer order may be indicated on the graphical information screen by enumerating those steps animating the indicator and instructions and e.g. enlarging the indicator instruction that should currently be performed and or using any other ordering chronology instructions known in the art.

The user may also use a timing graphic displayed on the display device to determine whether the order for the customer is being made in time. The graphical information screen illustrates how the wearable user device may be used to enable an user e.g. like the coffee shop employee in the embodiment provided to operate efficiently without requiring extensive training as the wearable user device operates to train on the fly by displaying in the user s field of view everything that user needs to do to accomplish their job. The intuitive heads up display substantially reduces the training time needed to get an employee up to speed on the details of their job.

Thus systems and methods have been described that provide for the enhanced display of information on a display device by creating a variety of graphical and other display information that is presented in a manner that is intuitive to a user and that augments the user s field of view with graphics and information in a manner that utilizes the users field of view of the physical world to display graphical elements that enable the functionality discussed above.

In some embodiments the functionality of the wearable user device may be extended using other devices external to the wearable user device. For example the user may include the wearable user device as well as a phone a smart watch and or a variety of other user devices known in the art. In such embodiments the wearable user device may be linked e.g. via Bluetooth or other wireless technology with the phone smart watch or other external device to provide the user the ability to interact externally with the wearable user device via the phone smart watch or other external device. For example with reference to the embodiments discussed above the user may use the phone or smart watch to confirm that the user is at an end of a head range of motion during the calibration performed as illustrated in . In addition the user may use the phone or smart watch to interact with the icons displayed as illustrated in . The user may also use the phone or smart watch to confirm that the user has centered the primary calibration object during the calibration performed as illustrated in . The user may also use the phone or smart watch to select a customer information window or shopping cart information window illustrated in . While a few examples have been provided a wide variety of external device uses with the wearable user device will fall within the scope of the present disclosure.

Referring now to an embodiment of a network based system for implementing one or more processes described herein is illustrated. As shown network based system may comprise or implement a plurality of servers and or software components that operate to perform various methodologies in accordance with the described embodiments. Exemplary servers may include for example stand alone and enterprise class servers operating a server OS such as a MICROSOFT OS a UNIX OS a LINUX OS or other suitable server based OS. It can be appreciated that the servers illustrated in may be deployed in other ways and that the operations performed and or the services provided by such servers may be combined or separated for a given implementation and may be performed by a greater number or fewer number of servers. One or more servers may be operated and or maintained by the same or different entities.

The embodiment of the networked system illustrated in includes a wearable user device a plurality of merchant devices a payment service provider device and a plurality of account holder devices in communication over a network . The wearable user device may be the wearable user device discussed above. The merchant devices may be the merchant devices or other devices discussed above that interact with the wearable user device and may be operated by the merchants discussed above. The payment service provider device may be the payment service provider devices discussed above and may be operated by a payment service provider such as for example PayPal Inc. of San Jose Calif. The account provider devices may be the account provider devices discussed above and may be operated by the account providers discussed above such as for example credit card account providers bank account providers savings account providers and a variety of other account providers known in the art.

The wearable user device merchant devices payment service provider device and account provider devices may each include one or more processors memories and other appropriate components for executing instructions such as program code and or data stored on one or more computer readable mediums to implement the various applications data and steps described herein. For example such instructions may be stored in one or more computer readable mediums such as memories or data storage devices internal and or external to various components of the system and or accessible over the network .

The network may be implemented as a single network or a combination of multiple networks. For example in various embodiments the network may include the Internet and or one or more intranets landline networks wireless networks and or other appropriate types of networks.

The wearable user device may be implemented using any appropriate combination of hardware and or software configured for wired and or wireless communication over network . The wearable user device may include one or more browser applications which may be used for example to provide a convenient interface to permit the user to browse information available over the network . For example in one embodiment the browser application may be implemented as a web browser configured to view information available over the Internet.

The wearable user device may also include one or more toolbar applications which may be used for example to provide user side processing for performing desired tasks in response to operations selected by the user. In one embodiment the toolbar application may display a user interface in connection with the browser application.

The wearable user device may further include other applications as may be desired in particular embodiments to provide desired features to the wearable user device . In particular the other applications may include a payment application for payments assisted by a payment service provider through the payment service provider device . The other applications may also include security applications for implementing user side security features programmatic user applications for interfacing with appropriate application programming interfaces APIs over the network or other types of applications. Email and or text applications may also be included which allow the user to send and receive emails and or text messages through the network . The wearable user device includes one or more user and or device identifiers which may be implemented for example as operating system registry entries cookies associated with the browser application identifiers associated with hardware of the wearable user device or other appropriate identifiers such as a phone number. In one embodiment the user identifier may be used by the payment service provider device and or account provider device to associate the user with a particular account as further described herein.

The merchant device may be maintained for example by a conventional or on line merchant conventional or digital goods seller individual seller and or application developer offering various products and or services in exchange for payment to be received conventionally or over the network . In this regard the merchant device may include a database identifying available products and or services e.g. collectively referred to as items which may be made available for viewing and purchase by the user or a customer.

The merchant device also includes a checkout application which may be configured to facilitate the purchase by the payer of items. The checkout application may be configured to accept payment information from the user through the wearable user device the account provider through the account provider device and or from the payment service provider through the payment service provider device over the network .

Referring now to an embodiment of a wearable user device is illustrated. The wearable user device may be the may be the wearable user devices discussed above. The wearable user device includes a frame having a computing chassis that extends from the frame a display device that extends from the computing chassis a microphone located on the computing chassis and a camera located on the computing chassis . One of skill in the art will recognize that the wearable user device is a mobile wearable user device such as for example Google Glass available from Google Inc. of Mountain View Calif. that may provide a user with the functionality discussed above with reference to the method . However a variety of other mobile wearable user devices may be used in the method without departing from the scope of the present disclosure.

Referring now to an embodiment of a computer system suitable for implementing for example the wearable user device and or the merchant device the payment service provider device and or the account provider device is illustrated. It should be appreciated that other devices utilized by users merchants payment service providers and account providers in the payment system discussed above may be implemented as the computer system in a manner as follows.

In accordance with various embodiments of the present disclosure computer system such as a computer and or a network server includes a bus or other communication mechanism for communicating information which interconnects subsystems and components such as a processing component e.g. processor micro controller digital signal processor DSP etc. a system memory component e.g. RAM a static storage component e.g. ROM a disk drive component e.g. magnetic or optical a network interface component e.g. modem or Ethernet card a display component e.g. CRT or LCD an input component e.g. keyboard keypad or virtual keyboard a cursor control component e.g. mouse pointer or trackball and or a location determination component e.g. a Global Positioning System GPS device as illustrated a cell tower triangulation device and or a variety of other location determination devices known in the art . In one implementation the disk drive component may comprise a database having one or more disk drive components.

In accordance with embodiments of the present disclosure the computer system performs specific operations by the processor executing one or more sequences of instructions contained in the memory component such as described herein with respect to the wearable user device and the merchant device s the payment service provider device and or the account provider device s . Such instructions may be read into the system memory component from another computer readable medium such as the static storage component or the disk drive component . In other embodiments hard wired circuitry may be used in place of or in combination with software instructions to implement the present disclosure.

Logic may be encoded in a computer readable medium which may refer to any medium that participates in providing instructions to the processor for execution. Such a medium may take many forms including but not limited to non volatile media volatile media and transmission media. In one embodiment the computer readable medium is non transitory. In various implementations non volatile media includes optical or magnetic disks such as the disk drive component volatile media includes dynamic memory such as the system memory component and transmission media includes coaxial cables copper wire and fiber optics including wires that comprise the bus . In one example transmission media may take the form of acoustic or light waves such as those generated during radio wave and infrared data communications.

Some common forms of computer readable media includes for example floppy disk flexible disk hard disk magnetic tape any other magnetic medium CD ROM any other optical medium punch cards paper tape any other physical medium with patterns of holes RAM PROM EPROM FLASH EPROM any other memory chip or cartridge carrier wave or any other medium from which a computer is adapted to read. In one embodiment the computer readable media is non transitory.

In various embodiments of the present disclosure execution of instruction sequences to practice the present disclosure may be performed by the computer system . In various other embodiments of the present disclosure a plurality of the computer systems coupled by a communication link to the network e.g. such as a LAN WLAN PTSN and or various other wired or wireless networks including telecommunications mobile and cellular phone networks may perform instruction sequences to practice the present disclosure in coordination with one another.

The computer system may transmit and receive messages data information and instructions including one or more programs i.e. application code through the communication link and the network interface component . The network interface component may include an antenna either separate or integrated to enable transmission and reception via the communication link . Received program code may be executed by processor as received and or stored in disk drive component or some other non volatile storage component for execution.

Referring now to an embodiment of a wearable user device is illustrated. In an embodiment the device may be the payer device and . The device includes a communication engine that is coupled to the network and to a display engine that is coupled to a calibration database . The communication engine may be software or instructions stored on a computer readable medium that allows the device to send and receive information over the network . The display engine may be software or instructions stored on a computer readable medium that is operable to determine calibration information and store it in the calibration database perform field of view calibrations display graphical information determine icon details display icons according to determined icon details provide perspective enhanced graphical information determine measurement distances provide measurement graphical information and provide any of the other functionality that is discussed above. While the database has been illustrated as located in the wearable user device one of skill in the art will recognize that it may be connected to the automatic payment engine through the network without departing from the scope of the present disclosure.

Where applicable various embodiments provided by the present disclosure may be implemented using hardware software or combinations of hardware and software. Also where applicable the various hardware components and or software components set forth herein may be combined into composite components comprising software hardware and or both without departing from the scope of the present disclosure. Where applicable the various hardware components and or software components set forth herein may be separated into sub components comprising software hardware or both without departing from the scope of the present disclosure. In addition where applicable it is contemplated that software components may be implemented as hardware components and vice versa.

Software in accordance with the present disclosure such as program code and or data may be stored on one or more computer readable mediums. It is also contemplated that software identified herein may be implemented using one or more general purpose or specific purpose computers and or computer systems networked and or otherwise. Where applicable the ordering of various steps described herein may be changed combined into composite steps and or separated into sub steps to provide features described herein.

The foregoing disclosure is not intended to limit the present disclosure to the precise forms or particular fields of use disclosed. As such it is contemplated that various alternate embodiments and or modifications to the present disclosure whether explicitly described or implied herein are possible in light of the disclosure. For example the above embodiments have focused on merchants and users however a customer or consumer can pay or otherwise interact with any type of recipient including charities and individuals. The payment does not have to involve a purchase but may be a loan a charitable contribution a gift etc. Thus merchant as used herein can also include charities individuals and any other entity or person receiving a payment from a payer. Having thus described embodiments of the present disclosure persons of ordinary skill in the art will recognize that changes may be made in form and detail without departing from the scope of the present disclosure. Thus the present disclosure is limited only by the claims.

