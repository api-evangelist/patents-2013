---

title: Systems and methods for predictive downloading in congested networks
abstract: An embodiment user equipment has a list of predictive data that a user may request, and programming to receive prefetched data based on the list of predictive data at a reduced cost, wherein the reduced cost is lower than a network cost of downloading the data, and to store the prefetched data within the UE for future consumption. An embodiment base station has a list of predictive data a UE may request, a high priority queue for data requested by the UE, and a low priority queue with predictive data corresponding to the list of predictive data. The base station further includes programing to send the requested data and to send the predictive data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09060292&OS=09060292&RS=09060292
owner: Futurewei Technologies, Inc.
number: 09060292
owner_city: Plano
owner_country: US
publication_date: 20130104
---
This application claims the benefit of U.S. Provisional Application No. 61 583 973 filed on Jan. 6 2012 entitled Systems and Methods for Predictive Downloading in Congested Networks which application is hereby incorporated herein by reference.

The present invention relates generally to communications methods and systems and in particular embodiments to a system and method for downloading information in a communications network.

Communications networks provide communications between network elements. For example a communications network such as a wireless communications network allows users to access remote information. The amount of data accessed and downloaded by a user may be significant and may cause delays in response time with respect to the amount of time a user waits for the requested data.

With regard to data accesses particularly downloads it is generally accepted that a user s downloading habits are predictable and the cost of transmitting receiving data changes dramatically over time e.g. systems are often under loaded . Current technologies involving predictive downloading focus on the user application level. Specific applications in user equipment UE may use predictive downloading to prefetch data and minimize response time when a user actually requests data.

These and other problems are generally solved or circumvented and technical advantages are generally achieved by preferred embodiments of the present invention which allows for predictive downloading in predictive networks.

In accordance with an embodiment of the present invention a method includes determining see step a list of predictive data at a user equipment UE receiving see step prefetched data at the UE corresponding to the list of predictive data. The UE is charged a reduced bandwidth for receiving the prefetched data wherein the reduced bandwidth is lower than a network bandwidth used by a network to transmit the prefetched data to the UE. The method further includes storing see step the prefetched data within the UE for future consumption.

In accordance with another embodiment of the present invention user equipment UE includes a non transitory computer readable storage medium storing instructions and a processor which when executing the instructions in the non transitory computer readable storage medium performs the following determine a list of data the UE may request in the future download data based on the list and store the data in the UE. The UE is charged a reduced bandwidth for receiving the prefetched data wherein the reduced bandwidth is lower than a network bandwidth used by a network to transmit the data to the UE.

In accordance with another embodiment of the present invention a method includes transmitting requested data requested by a user equipment UE from a base station BS determining by the BS a list of predictive data the UE may request generating a low priority queue of predictive data based on the list wherein the low priority queue is set a priority level lower than the priority level of the requested data transmitted by the BS and transmitting predictive data from the low priority queue. The method further includes charging the UE a reduced bandwidth for the predictive data wherein the reduced bandwidth is lower than a network bandwidth used to transmit the predictive data to the UE.

In accordance with yet another embodiment of the present invention a base station BS includes a non transitory computer readable storage medium storing instructions and a processor which when executing the instructions in the non transitory computer readable storage medium performs the following create a high priority queue see step for data requested by a user equipment UE at the BS send requested data see step from the high priority queue receive see step a list of potential data the UE may request create a low priority queue see step corresponding with the potential data wherein the low priority queue has a priority level less than the priority of the high priority queue and send see step potential data to the UE. The UE is charged a reduced bandwidth wherein the reduced bandwidth is less than a network bandwidth used to send the potential data to the UE see step .

An advantage of an embodiment is it allows for UEs to receive predictive data without paying for the data unless the data is actually used.

A further advantage of an embodiment is it utilizes periods when a network is underloaded to send predictive data to a UE allowing the network to maximize throughput.

The making and using of the presently preferred embodiments are discussed in detail below. It should be appreciated however that the present invention provides many applicable inventive concepts that can be embodied in a wide variety of specific contexts. The specific embodiments discussed are merely illustrative of specific ways to make and use the invention and do not limit the scope of the invention.

Predictive downloading at the user application level ignores network load and fails to take advantage of times when a network is under loaded to prefetch data and maximize network efficiency. Furthermore under current approaches users are charged by the network for any prefetched data even if the users do not ultimately request the data. Therefore users may be charged for data that they do not actually need. Accordingly a system and method for predictive downloading in congested networks is provided to address these concerns.

Various embodiments are described with respect to preferred embodiments in a specific context namely a wireless network. The invention may also be applied however to other networks such as wireline optical networks or combinations of these networks.

According to an embodiment system predicts what various users will download ahead of time predictive data and then sends this predictive data to an UE when the cost to network is low. For example system may comprise a transparent daemon that analyzes downloading history in the past and predicts data the user may want in the future. This daemon indicates to network what predictive data may be prefetched for the user at low traffic periods hereinafter prefetched data or prefetched object . Network may set a very low queuing priority for prefetched data. Base stations could schedule these queues as large delay tolerant data essentially maximizing throughput in an unfair sense. UE then pays the network for this prefetched data only if it is actually used. This paying mechanism may be achieved for example via UE downloading a key to decrypt the data at the cost of the bandwidth used before to download the prefetched data.

In an embodiment a daemon transparent to the user controls the generation and downloading of this prefetched data. This daemon may exist in several places for example at UE within a specific application s server or at network . The daemon may also exist concurrently within several different places with the possibility of overlap of what tasks the daemon can perform at each location. It is therefore contemplated in alternate embodiments for the various combinations of daemon activity contributing in combination to download prefetched data to UE .

In an embodiment the daemon exists within UE . The daemon monitors a user s past and present download history and generates information based on patterns observed. The daemon may also monitor various user specific statistics such as location battery life time of day cache size available etc. The daemon may create a database that comprises of two parts a predictive data list and copies of prefetched data. The predictive data list keeps track of the user s download history and generates a list of predictive data for the user. For example this first part can keep a list of the user s visited uniform resource locators URLs the number of times an URL was downloaded the frequency of those downloads and the time frame for which downloaded information is valid. The daemon then knows when the user s information has become stale and may prefetch this information before the user asks for it.

The user may also proactively set this predictive data list with a list of interested objects and a refresh rate for these objects. These pro active objects may also be added to the database by an application which operates in the UE eliminating the need for the user to intervene and decide which objects may be prefetched. Some exemplary applications are news websites weather or stock information which may be downloaded at regular intervals or easy to identify videos from websites such as YouTube and Netflix e.g. a user s regularly scheduled TV programs . Provisions for these kinds of example applications are present in many content centric network schemes e.g. CCNx .

The second part of the database copies of prefetched data keeps a copy of the prefetched objects that MAC has downloaded and links these objects back to the predictive data list. Some of these prefetched objects may have many parts e.g. a webpage consists of an Hypertext Markup Language HTML text file and binary picture objects . The prefetched objects may also comprise many sub objects that may be serialized into a single file for easier storage in the database.

An example implementation of the second part of the database is a download cache similar to a web cache. Typically a web cache decreases the load on the network since it allows the user to view the same object multiple times without downloading it every time. In this case download cache allows the network to push the objects to the user s device and stores the objects for later even if the user did not request the objects.

In an alternate embodiment of the second part of the database a virtual firewall and cache may be introduced within UE . This firewall intercepts some or all communications leaving UE and using conventional algorithms redirects data to the cache if applicable. Examining the most commonly hit data a list of predictive data is constructed and a set of prefetched data may be downloaded to the cache.

In an alternate embodiment the daemon exists within network . The traffic generated by a particular UE such as UE may be monitored and compared to other similar traffic patterns or UE s past traffic history. A list of predictive data may then be generated using a predictive algorithm.

In an embodiment the system may be optimized such that network can keep track of parts of a database similar to the database generated by a daemon residing within UE . For example the network may retain a copy of portions of the predictive data list the first part of the database at no cost. Because the user downloads objects from network network already has a list of past downloaded objects. Therefore network has the access to a list of downloaded URLs the frequency of a URL s downloads and the time frame for which downloaded information is valid. Because network has a copy of this part of the database UE need not inform network about which objects are becoming stale and should be prefetched for the user.

For pro active objects that may be a portion of the first part of the database network can collaborate with network services over its backbone to become aware of the objects that the user may want to download in the future such as TV programs. This eliminates or reduces the need for communication between network and UE to obtain this information. For other pro active objects UE may explicitly notify the network with the objects it wishes to download in a pro active fashion. For example network could read a prefetch tag present in HTML5 and some predecessors. In this scheme the application indicates as part of an object request that the object is being prefectched and the above schemes would be applied to that data. Note that a different type of flow control i.e. other than unmodified TCP may be applied to this traffic as the behavior may be quite different.

In an embodiment an optimization network may also keep a copy of portions of the copies of prefetched data the second part of the database . This copy may be shared by multiple users since the URLs stored in part one of the database uniquely identify objects. An advantage of this embodiment is that it allows network to save backbone bandwidth by not fetching the same object multiple times for different users. Note that multicast broadcast messages can be used to deliver data to more than one user at once.

For example a prefetched object may be pushed to user A at time t. At some later time t t being greater than t a different user B requests the same object. Since network already has a copy of this object it can deliver the object to user B more quickly and cheaply than if the object had to download from the Internet. In another example user A may download an object at time t. The network object is then later requested by user B at time t t being greater than t . Network determines that the object downloaded at t is no longer fresh so it downloads a fresh version over its backbone. Network then keeps a copy of the updated object so that the updated object may be prefetched for user A when the network is under loaded. The network thus saves backbone bandwidth by only having to download the updated object once.

In another embodiment the daemon may also exist within an application server. Specific applications such as application in UE could generate predictive lists based on an UE s activity and download prefetched data from its server. For example a video application e.g. Netflix could pre download the next video of a series or subscriber applications e.g. podcasts could automatically download new subscriber data.

The application server may also keep a list of predictive data. This list is based on the user s preferences and previous use both of which are known to the application server. The application server pushes this list and the objects to network . Network may then prefetch the objects for the user when it is under loaded. Alternatively the application server may provide network with information about how fresh certain objects need to be in order to be relevant to a user. The freshness requirements for specific objects may vary. For example weather data should to be refreshed every few hours or more frequently to remain fresh whereas an episode of a TV may remain fresh for days weeks or years. Network may automatically download updated objects and push them to UE .

In another embodiment the application may be a proxy server or other portion of a CDN network within a Relay node. For instance a proxy server would provide a list of data to download representing common objects that have expired that are likely to be requested again. It may then maintain the freshness of its cache longer.

In an embodiment the prefetched data list may also include additional information regarding the data. It may include for example the location of the data requested its probability of use and the freshness of the data. The prefetched data information may also include additional information having to do with data transmission. For example the information may include percentage that has already been received which portions of the information have been received and encryption keys. This information may be kept as a list within the daemon.

In an embodiment individual queues are created for each user organized by each piece of data s priority level quality of service QoS class identifier QCI QCI 0 QCI 1 . . . QCI N . Predictive data is placed in queue with a priority QCI 1 which is a new priority level below all others. Different users may compete for resources to receive data in queues using methods known in the art regarding current networks e.g. highest priority first or a weighted queue scheduling in which different queues are assigned fixed weights in proportional fair scheduling Scheduler coordinates data to be delivered to a user based on service rates set at nodes . Because prefetched data has the lowest priority of the data among all of the users it is sent to users last and only if the network has allocated surplus throughput for the higher priority data. The probability of use and additional information may be used to weight individual users to determine who receives predictive data. Regarding predictive data transmission scheduling may not achieve network fairness as is known in the art such as proportional fairness. Regarding prefetched data the goal is not fairness but rather to maximize or increase the overall rate of network transmissions. While fairness may be achieved for regular data in queues the fairness among data in queue predictive data may not be achieved.

Scheduler may be an utility based scheduler meaning the scheduler calculates a utility for each user and tries to maximize the assigned utility based on sub optimal or optimal scheduling techniques. One type of scheduler is the proportionally fair PF scheduler in which the overall utility function to be maximized is r instant r ave

wherein r instant is the achievable data transfer rate and r ave is the average rate of data transferred over some time window. The utility of sending a particular piece of data may be calculated at node . Node then estimates a fair rate based on the utilities calculated at node while trying to maximize the overall utility function above. Node sets the service rates for different queues and at nodes based on the fair rate estimated at node . Finally scheduler delivers data to the user based on the service rates set in nodes .

The utility for sending a particular piece of predicted data to a user may be calculated based on the equation r instant probability of use scaling constant x wherein x is a value used to represent various factors such as interference generated to users outside of the cell battery consumption by UE in reception electricity consumption by the base station etc. The value of x may change over time depending on an UE s condition. The scaling constant is present for comparison between the data s priority level and others. The scaling constant of predictive data may be low to prevent this type of data from transmitting when other types of data are present.

Because UE s may be in hibernation mode when the network wants to send predictive data the UE may be need to be woken up in order to receive data. The overhead required as well as battery consumed may be factored into the utility function when considering whether to transmit this data or not. For example the calculation of the variable x above may factor in overhead costs and the scheduler will only send predictive data if the utility function for sending that piece of data is positive. In an embodiment r instant is approximated for these users in hibernation mode based on last known channel qualities.

Predictive data may be delivered to queue using a variety of methods. In one embodiment the network may keep a copy of predictive data from previous transmissions so that the data already exists in queue .

In another embodiment large predictive data packets e.g. objects that are several MB or GB in size will not be downloaded at once. In such cases if the application server supports a network coding protocol the data may be downloaded one network coded packet at a time from the application server. If the application server does not support a network coding protocol then the large predictive data will be downloaded by the network using some other protocol. For example it may be downloaded only in portions if partial downloading and resuming is allowed i.e. Rsync etc . Division of data into web objects presents a natural way to divide the large data into more manageable chunks of data. Each object is uniquely identified and smaller in size than the full set of objects. Larger objects may be downloaded by a protocol that has a resume feature. For example the file transfer protocol FTP allows resuming of previously started transmissions with its REST command.

At the network large predictive data packets may be broken down into many smaller packets which are network coded with each other. These smaller packets sit in the upper levels of the network and may be requested by individual base stations as needed. When a base station predicts that it will have available bandwidth the base station may start obtaining the predictive data by sending a request for it. This data is sent to the base station using a very low priority similar to how the data is delivered from the base station to an UE. If the backhaul network is congested this priority based delivery system may prevent the predictive data from increasing congestion significantly. Network coding is only an example implementation and may not be present in various alternate embodiments.

Referring back to in an embodiment a user does not pay for prefetched data but is charged for this prefetched data when it is used. In an alternative embodiment a user only pays a reduced cost for prefetched data but is charged for remainder when the prefetched data is used. In the user may requests access to a piece of prefetched data stored in cache through application . MAC provides application with the data and MAC also processes billing for the used prefetched data with network .

In an embodiment each piece of prefetched data is encrypted with an individual key. MAC must request the key to decrypt the prefetched data prior to sending the data to application . This request would indicate that data s usage and it would cause that user s account to be charged by network for the cost of sending the prefetched data. Network may also check the prefetched data for freshness when a request for a key is made. The network would then be able to verify that the data sent to the UE is still fresh and inform the UE as such.

In an alternative embodiment MAC simply informs network whenever prefetched data is accessed without the use of individual encryption keys. For this method MAC may be a trusted application by network facilitating payment and ensuring that the user is not cheating. One way to implement MAC in a secure manner is to encrypt prefetched data on the network side and share the encryption key only with MAC . One key is used to encrypt all of the prefetched data at the same time. MAC decrypts the part of the data that the user requested using its keys and notifies network that the data was accessed.

In yet another embodiment network assigns each user credits which is stored in a trusted application MAC on UE . Instead of contacting the network every time the data is accessed MAC checks if the user has credits left. If the user has enough credits to access the data the data is sent to the user for consumption. If the user does not have enough credit the MAC contacts network to get more credits for the user. The advantage of this method is that amount of traffic consumed by network to charge for content is decreased.

The bus may be one or more of any type of several bus architectures including a memory bus or memory controller a peripheral bus video bus or the like. The CPU may comprise any type of electronic data processor. The memory may comprise any type of system memory such as static random access memory SRAM dynamic random access memory DRAM synchronous DRAM SDRAM read only memory ROM a combination thereof or the like. In an embodiment the memory may include ROM for use at boot up and DRAM for program and data storage for use while executing programs.

The mass storage device may comprise any type of storage device configured to store data programs and other information and to make the data programs and other information accessible via the bus. The mass storage device may comprise for example one or more of a solid state drive hard disk drive a magnetic disk drive an optical disk drive or the like.

The video adapter and the I O interface provide interfaces to couple external input and output devices to the processing unit. As illustrated examples of input and output devices include the display coupled to the video adapter and the mouse keyboard printer coupled to the I O interface. Other devices may be coupled to the processing unit and additional or fewer interface cards may be utilized. For example a serial interface card not shown may be used to provide a serial interface for a printer.

The processing unit also includes one or more network interfaces which may comprise wired links such as an Ethernet cable or the like and or wireless links to access nodes or different networks. The network interface allows the processing unit to communicate with remote units via the networks. For example the network interface may provide wireless communication via one or more transmitters transmit antennas and one or more receivers receive antennas. In an embodiment the processing unit is coupled to a local area network or a wide area network for data processing and communications with remote devices such as other processing units the Internet remote storage facilities or the like.

While this invention has been described with reference to illustrative embodiments this description is not intended to be construed in a limiting sense. Various modifications and combinations of the illustrative embodiments as well as other embodiments of the invention will be apparent to persons skilled in the art upon reference to the description. It is therefore intended that the appended claims encompass any such modifications or embodiments.

