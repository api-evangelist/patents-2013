---

title: Framework for voice controlling applications
abstract: A system for voice control of applications includes an electronic device that receives speech signals and converts the speech signals into words. A voice navigation module analyzes an application and determines application type and enabled features. A command registration module registers commands based on the determined application type and enabled features. The commands control the application when matched with associated speech. A speech command interpretation module receives the words and detects a speech mode for matching commands with interpreted speech, and executes matched commands for navigating through and controlling the application.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09218052&OS=09218052&RS=09218052
owner: Samsung Electronics Co., Ltd.
number: 09218052
owner_city: Suwon-si
owner_country: KR
publication_date: 20130314
---
One or more embodiments relate generally to applications and in particular to a navigation framework for voice control of applications on an electronic device.

Television applications may be developed using standard web technologies such as hypertext mark up language HTML JavaScript and Flash. Utility of these web based television applications are usually limited to using a remote control device with directional step by step inputs i.e. up down left right and click type selection that is navigated using directional inputs.

In one embodiment a system provides for voice control of applications. One embodiment comprises a system that includes an electronic device that receives speech signals and converts the speech signals into words. In one embodiment a voice navigation module analyzes an application and determines application type and enabled features. In one embodiment a command registration module registers commands based on the determined application type and enabled features. The commands control the application when matched with associated speech. In one embodiment a speech command interpretation module receives the words and detects a speech mode for matching commands with interpreted speech and executes matched commands for navigating through and controlling the application.

One embodiment provides a method for voice control of applications. In one embodiment the method comprises analyzing an application for detecting application type and enabled features. In one embodiment commands based on the detected application type and enabled features are registered. The commands control the application when matched with associated speech. In one embodiment words converted from speech are received and a speech mode for matching commands with interpreted speech is detected. Matched commands for navigating through and controlling the application are executed.

Another embodiment provides a non transitory computer readable medium having instructions which when executed on a computer perform a method comprising analyzing an application for detecting application type and enabled features. In one embodiment commands are registered based on the detected application type and enabled features. The commands control the application when matched with associated speech. In one embodiment words converted from speech are received and a speech mode for matching commands with interpreted speech is detected. Matched commands for navigating through and controlling the application are executed.

These and other aspects and advantages of the one or more embodiments will become apparent from the following detailed description which when taken in conjunction with the drawings illustrate by way of example the principles of the embodiments.

The following description is made for the purpose of illustrating the general principles of the embodiments and is not meant to limit the inventive concepts claimed herein. Further particular features described herein can be used in combination with other described features in each of the various possible combinations and permutations. Unless otherwise specifically defined herein all terms are to be given their broadest possible interpretation including meanings implied from the specification as well as meanings understood by those skilled in the art and or as defined in dictionaries treatises etc.

One or more embodiments relate generally voice control of applications e.g. web applications which may be used with an electronic device e.g. a television device . In one embodiment a voice navigation module analyzes an application and determines application type and enabled features. In one embodiment a command registration module registers commands based on the determined application type and enabled features. The commands control the application when matched with associated speech. In one embodiment a speech command interpretation module receives the words and detects a speech mode for matching commands with interpreted speech and executes matched commands for navigating through and controlling the application.

In one embodiment the electronic device comprises an electronic device capable of data communication over a communication link such as a wireless communication link. Examples of such an electronic device include stationary electronic devices a mobile phone device a mobile tablet device etc. Examples of a stationary electronic device may include televisions projector systems etc. In one embodiment a method provides for voice control of applications for an electronic device. One embodiment comprises analyzing an application for detecting application type and enabled features. In one embodiment commands based on the detected application type and enabled features are registered. The commands control the application when matched with associated speech. In one embodiment words converted from speech are received and a speech mode for matching commands with interpreted speech is detected. Matched commands for navigating through and controlling the application are executed.

Web applications assume that any object on screen may be clicked at any time and it is well suited for click type controls such as voice and gesture. Thus Web application style development is not well suited for directional controls since there is no concept of coordinates between different buttons. Applications on televisions are typically developed in a remote control style e.g. linear movements instead of a web application style making them difficult to convert for complex voice gesture navigation.

In one embodiment a television side e.g. client side framework uses voice recognition technology to navigate web applications on a television electronic device. In one embodiment the framework comprises of a core library as well as modules that account for specific technologies e.g. Flash and specific features e.g. remote control emulation . In one embodiment the framework provides for web type applications e.g. hypertext markup language HTML JavaScript JS Flash Air etc. to traverse multiple objects or search an object with a single recognition command. In one embodiment developer effort is minimized in using the framework to provide for voice control for commands and navigation. In one embodiment features are included in the framework such as conditional dependency injection automatic command registration and smart defaults for assisting developers.

In one embodiment applications are designed for remote control style controls are provided with voice remote controls with the addition of a few extra lines of code. In one embodiment for applications that are a hybrid between remote and cursor styles for control the framework provides for registering clickable objects e.g. on screen objects with voice commands. In one embodiment for applications that fully use cursor event handler style of control the framework provides full voice and gesture control with minimum modification to existing application code.

Any suitable circuitry device system or combination of these e.g. a wireless communications infrastructure including communications towers and telecommunications servers operative to create a communications network may be used to create communications network . Communications network may be capable of providing communications using any suitable communications protocol. In some embodiments communications network may support for example traditional telephone lines cable television Wi Fi e.g. a 802.11 protocol Bluetooth high frequency systems e.g. 900 MHz 2.4 GHz and 5.6 GHz communication systems infrared other relatively localized wireless communication protocol or any combination thereof. In some embodiments communications network may support protocols used by wireless and cellular phones and personal email devices e.g. a Blackberry . Such protocols can include for example GSM GSM plus EDGE CDMA quadband and other cellular protocols. In another example a long range communications protocol can include Wi Fi and protocols for placing or receiving calls using VOIP or LAN. Transmitting device and receiving device when located within communications network may communicate over a bidirectional communication path such as path . Both transmitting device and receiving device may be capable of initiating a communications operation and receiving an initiated communications operation.

Transmitting device and receiving device may include any suitable device for sending and receiving communications operations. For example transmitting device and receiving device may include a television system a device with audio video capabilities tablets and any other device capable of communicating wirelessly with or without the aid of a wireless enabling accessory system or via wired pathways e.g. using traditional telephone wires . The communications operations may include any suitable form of communications including for example voice communications e.g. telephone calls data communications e.g. e mails text messages media messages or combinations of these e.g. video conferences .

In one embodiment all of the applications employed by audio output display input mechanism communications circuitry and microphone may be interconnected and managed by control circuitry . In one example a hand held music player capable of transmitting music to other tuning devices may be incorporated into the electronics device .

In one embodiment audio output may include any suitable audio component for providing audio to the user of electronics device . For example audio output may include one or more speakers e.g. mono or stereo speakers built into electronics device . In some embodiments audio output may include an audio component that is remotely coupled to electronics device . For example audio output may include a headset headphones or earbuds that may be coupled to communications device with a wire e.g. coupled to electronics device with a jack or wirelessly e.g. Bluetooth headphones or a Bluetooth headset .

In one embodiment display may include any suitable screen or projection system for providing a display visible to the user. For example display may include a screen e.g. an LCD screen that is incorporated in electronics device . As another example display may include a movable display or a projecting system for providing a display of content on a surface remote from electronics device e.g. a video projector . Display may be operative to display content e.g. information regarding communications operations or information regarding available media selections under the direction of control circuitry .

In one embodiment input mechanism may be any suitable mechanism or user interface for providing user inputs or instructions to electronics device . Input mechanism may take a variety of forms such as a button keypad dial a click wheel or a touch screen. The input mechanism may include a multi touch screen.

In one embodiment communications circuitry may be any suitable communications circuitry operative to connect to a communications network e.g. communications network and to transmit communications operations and media from the electronics device to other devices within the communications network. Communications circuitry may be operative to interface with the communications network using any suitable communications protocol such as for example Wi Fi e.g. a 802.11 protocol Bluetooth high frequency systems e.g. 900 MHz 2.4 GHz and 5.6 GHz communication systems infrared GSM GSM plus EDGE CDMA quadband and other cellular protocols VOIP or any other suitable protocol.

In some embodiments communications circuitry may be operative to create a communications network using any suitable communications protocol. For example communications circuitry may create a short range communications network using a short range communications protocol to connect to other communications devices. For example communications circuitry may be operative to create a local communications network using the Bluetooth protocol to couple the electronics device with a Bluetooth headset.

In one embodiment control circuitry may be operative to control the operations and performance of the electronics device . Control circuitry may include for example a processor a bus e.g. for sending instructions to the other components of the electronics device memory storage or any other suitable component for controlling the operations of the electronics device . In some embodiments a processor may drive the display and process inputs received from the user interface. The memory and storage may include for example cache Flash memory ROM and or RAM. In some embodiments memory may be specifically dedicated to storing firmware e.g. for device applications such as an operating system user interface functions and processor functions . In some embodiments memory may be operative to store information related to other devices with which the electronics device performs communications operations e.g. saving contact information related to communications operations or storing information related to different media types and media items selected by the user .

In one embodiment the control circuitry may be operative to perform the operations of one or more applications implemented on the electronics device . Any suitable number or type of applications may be implemented. Although the following discussion will enumerate different applications it will be understood that some or all of the applications may be combined into one or more applications. For example the electronics device may include an automatic speech recognition ASR application a dialog application a map application a media application e.g. QuickTime MobileMusic.app or MobileVideo.app . In some embodiments the electronics device may include one or several applications operative to perform communications operations. For example the electronics device may include a messaging application a mail application a voicemail application an instant messaging application e.g. for chatting a videoconferencing application a fax application or any other suitable application for performing any suitable communications operation.

In some embodiments the electronics device may include microphone . For example electronics device may include microphone to allow the user to transmit audio e.g. voice audio for speech control and navigation of applications N during a communications operation or as a means of establishing a communications operation or as an alternate to using a physical user interface. Microphone may be incorporated in electronics device or may be remotely coupled to the electronics device . For example microphone may be incorporated in wired headphones microphone may be incorporated in a wireless headset may be incorporated in a remote control device etc.

In one embodiment the electronics device may include any other component suitable for performing a communications operation. For example the electronics device may include a power supply ports or interfaces for coupling to a host device a secondary input mechanism e.g. an ON OFF switch or any other suitable component.

In one embodiment the voice navigational framework module provides voice command control for applications that do not include any means for voice control of the applications. In one embodiment for applications that are designed for remote control style control e.g. single input presses navigation in step by step moves such as up down left right the voice navigational framework module provides voice command control of the remote controls. In one embodiment for a Javascript library the voice navigational framework module generates events that map voice commands to remote control commands. In another embodiment the voice navigational framework module modifies firmware to allow all applications to enable basic voice controls with a parameter in a file such as the config.xml file.

In one embodiment for applications that are a hybrid between remote control and cursor style controllable e.g. move and click the voice navigational framework module provides a framework to register clickable objects on screen. In one embodiment the mechanism for voice gesture acts in a way such that when an object would typically be clicked in an application a function is called to handle it. In one embodiment the voice navigational framework module provides a single library for a developer to register all the clickable objects on screen. In one embodiment the library may provide the implementations for voice gesture and remote e.g. Voice Navi lib for HTML JS and Voice Navi lib for Flash Air . In some embodiments an application e.g. web application for TV developer does not need to be concerned with multi modal control as they may provide application code once and all designed physical controls will work automatically.

In one embodiment the voice navigational framework module provides additional features on top of an existing software development kit SDK allowing developers to work with sets of commands visually cycle through the list of possible commands and disambiguation of multiple commands. In one embodiment for applications that are designed to fully use cursor event handler styles of controls the voice navigational framework module provides full voice and gesture control with minimum modification to existing application code. In one embodiment instead of requiring developers to register their callbacks the voice navigational framework module provides for analyzing the DOM to automatically register callbacks. In one embodiment the voice navigational framework module provides an advantage that developers are free to develop innovative voice navigation solutions for their web applications.

The voice navigational framework module provides a framework that has a powerful feature set is easy to learn and should encourage developers to adopt it. In one embodiment the voice navigational framework module provides for allowing developers to decide what level of voice navigation is appropriate for their TV application and depending on the level chosen the application may require very little modification or significant modification. In one example for remote control emulation only the existing application may only require a one line addition of a voice navigation library without any other changes to the existing code e.g. voice enabled Pandora Vudu NHL apps etc. . In one embodiment the voice navigational framework module allows developers to register desired voice commands to the voice navigation library e.g. voice enabled Comcast Xfinity app .

In one embodiment if block determines that the application is not a Flash application the flowchart continues to block where the navigation framework module determines whether the application includes any dependencies e.g. device specific dependencies . In one embodiment if it is determined that the application does include dependencies the flowchart continues to block where the dependency injection module is executed. In one embodiment the dependency injection module injects the framework s required software dependencies into the developer code of the application. In one embodiment the injected dependencies include device specific dependencies allowing the same framework to be used across a variety of electronic devices e.g. different types of TV systems devices components etc. . In one embodiment upon the dependency injection module including the framework in the application a self executing function checks for existing libraries and creates local copies for itself if they have not yet been included. In one embodiment once the injection into the application is completed by the dependency injection module the self executing function begins initialization of whichever application modules the developer has chosen without further intervention. If block determines that dependencies are not included in the application the flowchart continues to block where the voice navigation library is ready to be used.

In block the navigation framework module determines the features that are enabled in the application including the voice navigation library . In one embodiment for custom commands desired by a developer in the application in block the command registration module adds the custom commands into a custom command dictionary. In one embodiment for remote control commands in the application in block the command registration module registers the remote control command callback pairs into a multi dimensional data structure . In one embodiment new command callback pairs may be dynamically added to the multi dimensional data structure as a user interface UI shifts between different scenes of the application. In one embodiment command pairs are organized into sets where the sets represent logical partitions of the UI such as menu items and content icons. In one embodiment the command pairs represent sets of abstract commands such as remote control inputs. In one embodiment the sets may be enabled and disabled by a developer as desired for user navigation through the UI.

In one embodiment for cursor gesture commands in the application in block the DOM analysis module performs DOM analysis on the application and analyzes the application for commands and callback functions and registers found commands and callback functions automatically into the command registration module . In one embodiment a developer may specify HTML attributes that will be searched during the analysis using a custom attribute such as voice or an existing one such as ID. In one embodiment the DOM analysis module searches for the custom attributes using for example jQuery and registers their callbacks into the command registration module . The flowchart continues to block upon the completion of the appropriate block or .

In one embodiment in block the library of the application is now ready for receiving commands from a user of the electronic device e.g. electronic device . In one embodiment the application may be launched by using a voice recognition application on the electronic device using the input mechanism e.g. speaking a command for launching the application tapping on a display screen pressing a button using a remote control launching a dialog application etc. In one embodiment speech signals entered through a microphone e.g. microphone are processed by an ASR and input in block for an initial utterance. In block the voice command is received for the application launched on the electronic device. The flowchart then continues to block .

In block the navigation framework module determines the type of mode that the received speech comprises. In one embodiment if it is determined that the mode is for local dictionary only commands the flowchart continues to block for processing by the local dictionary module otherwise if it is determined that the mode is for free speech commands the flowchart continues to block for processing by the free speech module . In one embodiment the mode is determined based on matching the speech.

In one embodiment when the electronic device detects voice input the voice is translated into a string and passed into the navigation framework module . The navigation framework module processes the string for attempting to match the string to a command within the currently active command pair sets. In one embodiment if the current mode is determined to be local dictionary only entries with a static type are searched in block . If the current mode is determined to be free speech in block both static and dynamic entries are searched for matching commands.

In one embodiment for local dictionary mode block processes the speech input and attempts to match a command using the local dictionary module . In block if a match is found the flowchart continues to block . If a match is not found in block the flowchart continues to block . In one embodiment in block the matched commands are executed for the application based on the received speech. The flowchart proceeds to block where it is determined whether a page in the application has changed or not. If the navigation framework module detects that a page in the application has changed since last processing the page of the application the flowchart proceeds back to block for processing otherwise the flowchart proceeds back to block awaiting further speech commands.

In one embodiment in block the free speech command interpretation module is used for processing the received speech using the Regex routing module for searching within the free speech module . In one embodiment dynamic type commands use a regular expression syntax to give developers additional flexibility. In one embodiment dynamic commands are processed by the Regex routing module for matching. In block if a regular expression match is found the flowchart continues to block for processing. If a match is not found in block the flowchart continues to block . In one embodiment in block the phonetic matching module is used to attempt to match the speech input.

In one embodiment the phonetic matching module supports the Regex routing module using fuzzy matching. In one embodiment if no direct match is found for a given vocal command by the Regex routing module a second attempt is made using phonetic matching using the phonetic matching module . In one embodiment phonetic translation processing searches for common linguistic patterns and condenses normal strings into phonetic equivalents. In one embodiment the phonetic equivalents may then be compared to the phonetic equivalent of user registered commands e.g. in the free speech module . In one embodiment the navigation framework module uses a well known algorithm such as double metaphone for processing the phonetic translation. In block if a match is found the flowchart continues to block for processing otherwise the flowchart continues to block for processing.

In one embodiment the user is using another application for entering a string for searching on display . In this embodiment the custom commands are searched as the user has now entered the speech terms search STAR WARS . In one embodiment the user speech invokes the application using the term search which is paired with executing a string search application i.e. application . Using the rest of the speech terms STAR WARS the navigation framework module uses processing e.g. pulse code modulation PCM and a conversion application e.g. Nuance to enter the converted text e.g. Star Wars into the application for execution. In one embodiment the navigation framework module uses command registration of the custom commands to allow a developer to directly specify voice command function callback pairs which requires more development effort than for example remote control emulation but also provides the most flexibility to define unique behaviors from voice commands outside of those that may be automatically extracted. In one embodiment automatic command registration is used by the navigation framework module for analyzing a web application to identify selectable HTML objects and register them into the voice navigation dictionary. In one embodiment as the application refreshes with new pages the navigation framework module will automatically update the voice navigation dictionary with new selectable objects. In one embodiment automatic command registration enables title navigation with minimal code changes required by a developer.

In one embodiment the commands are also categorized by type as static or dynamic commands. In one embodiment the static commands are fixed strings and use the electronic device s embedded ASR for highly accurate matching. In one embodiment dynamic commands are matched to cloud or server side free speech voice recognition results and may be less accurate than using the electronic device s ASR but are more flexible.

In one embodiment as the user navigates through a page of an application the next item to be selected is calculated dynamically using the coordinates of the current item and all selectable items on the screen. In one embodiment the coordinates of icons represent an application page. In one embodiment from the upper left hand corner of the currently selected element the distance between the upper left hand corner of each other selectable element is calculated. In one embodiment the search is restricted to only those objects falling within a cone expanding in each direction the shape of the cone is narrow to favor elements that are well aligned with the current object. In one embodiment from those items that fall into the cones the object with the shortest distance is selected when that direction is pressed.

In one embodiment the after making a single line actionscript call the flash module injects the voice navigation framework module processing through the ExternalInterface API provided by the flash player. In one embodiment to allow the two sides to interoperate the flash module injects additional javascript that searches for the flash object in the flash application page and registers the corresponding actionscript callbacks.

In one embodiment when the flash module is invoked the flash module may also perform Stage Analysis analogous to DOM Analysis to automatically register text strings that are placed onto the actionscript stage. In one embodiment translating spoken strings into bubbled mouse events allows flash applications that were not designed for an electronic device s directional remote input to be easily navigable through voice commands. The example shows the example stages for interoperability between the voice navigation framework module flash delegate and flash module . In one embodiment the example stages comprise initialization injection creation callback entry e.g. setFixedComand Left delegate.callback command entry e.g. Left delegate callback using the command left e.g. delegateCallback Left the object.relayCommand left and emulateEvent Left .

As is known to those skilled in the art the aforementioned example architectures described above according to said architectures can be implemented in many ways such as program instructions for execution by a processor as software modules microcode as computer program product on computer readable media as analog logic circuits as application specific integrated circuits as firmware as consumer electronic devices AV devices wireless wired transmitters wireless wired receivers networks multi media devices etc. Further embodiments of said Architecture can take the form of an entirely hardware embodiment an entirely software embodiment or an embodiment containing both hardware and software elements.

One or more embodiments have been described with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to the embodiments. Each block of such illustrations diagrams or combinations thereof can be implemented by computer program instructions. The computer program instructions when provided to a processor produce a machine such that the instructions which execute via the processor create means for implementing the functions operations specified in the flowchart and or block diagram. Each block in the flowchart block diagrams may represent a hardware and or software module or logic implementing one or more embodiments. In alternative implementations the functions noted in the blocks may occur out of the order noted in the figures concurrently etc.

The terms computer program medium computer usable medium computer readable medium and computer program product are used to generally refer to media such as main memory secondary memory removable storage drive a hard disk installed in hard disk drive. These computer program products are means for providing software to the computer system. The computer readable medium allows the computer system to read data instructions messages or message packets and other computer readable information from the computer readable medium. The computer readable medium for example may include non volatile memory such as a floppy disk ROM flash memory disk drive memory a CD ROM and other permanent storage. It is useful for example for transporting information such as data and computer instructions between computer systems. Computer program instructions may be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks.

Computer program instructions representing the block diagram and or flowcharts herein may be loaded onto a computer programmable data processing apparatus or processing devices to cause a series of operations performed thereon to produce a computer implemented process. Computer programs i.e. computer control logic are stored in main memory and or secondary memory. Computer programs may also be received via a communications interface. Such computer programs when executed enable the computer system to perform the features of the embodiments as discussed herein. In particular the computer programs when executed enable the processor and or multi core processor to perform the features of the computer system. Such computer programs represent controllers of the computer system. A computer program product comprises a tangible storage medium readable by a computer system and storing instructions for execution by the computer system for performing a method of one or more embodiments.

Though the embodiments have been described with reference to certain versions thereof however other versions are possible. Therefore the spirit and scope of the appended claims should not be limited to the description of the preferred versions contained herein.

