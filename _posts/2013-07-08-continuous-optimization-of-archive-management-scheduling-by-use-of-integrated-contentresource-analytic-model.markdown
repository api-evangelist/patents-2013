---

title: Continuous optimization of archive management scheduling by use of integrated content-resource analytic model
abstract: A method and associated system for continuously optimizing data archive management scheduling. A flow network is modeled. The flow network represents data content, software programs, physical devices, and communication capacity of the archive management system in various levels of vertices such that an optimal path in the flow network from a task of at least one archive management task to a worker program of the archive management system represents an optimal initial schedule for the worker program to perform the task.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09158581&OS=09158581&RS=09158581
owner: International Business Machines Corporation
number: 09158581
owner_city: Armonk
owner_country: US
publication_date: 20130708
---
This application is a Continuation application claiming priority to Ser. No. 13 569 620 filed Aug. 8 2012 which is a Continuation of Ser. No. 12 631 247 filed Dec. 4 2009 U.S. Pat. No. 8 276 148 issued Aug. 14 2012.

The present invention discloses a system and associated method for continuously optimizing tasks based on integrated content resource management semantics. Conventional task optimization methods employ management semantics based on either software data content or hardware resources. An enterprise computing environment such as a data center needs continuous and dynamic optimization of management tasks due to service performance requirement vast amount of data and limited time available for maintenance.

According to one embodiment of the present invention a method for continuously optimizing data archive management scheduling comprises a job scheduler receiving inputs from an archive management system wherein the inputs comprise task information replica placement data infrastructure topology data and resource performance data modeling a flow network from the received inputs such that the flow network represents data content software programs physical devices and communication capacity of the archive management system and such that an optimal path of the flow network from a task of at least one archive management task to a worker program of the archive management system represents an optimal initial schedule for the worker program to perform the task a processor of a computer system computing the optimal initial schedule from the optimal path in the flow network transferring the computed optimal initial schedule to the archive management system to perform the optimal initial schedule receiving monitoring results of operations of the transferred optimal initial schedule from the archive management system creating an adjusted schedule as a result of dynamically adjusting the optimal initial schedule based on the received monitoring results and transferring the adjusted schedule to the archive management system to perform the adjusted schedule such that the archive management system generates new monitoring results of operations of the transferred adjusted schedule.

According to one embodiment of the present invention a computer program product comprises a computer readable memory unit that embodies a computer readable program code. The computer readable program code contains instructions that when run by a processor of a computer system implement a method for continuously optimizing data archive management scheduling.

According to one embodiment of the present invention a computer system comprises a processor and a computer readable memory unit coupled to the processor wherein the computer readable memory unit containing instructions that when run by the processor implement a method for continuously optimizing data archive management scheduling.

According to one embodiment of the present invention a process for supporting computer infrastructure said process comprising providing at least one support service for at least one of creating integrating hosting maintaining and deploying computer readable code in a computing system wherein the code in combination with the computing system is capable of performing a method for continuously optimizing data archive management scheduling.

The archive management system comprises a data archive archive management tasks job scheduler inputs and a job scheduler . The archive management system is employed in an enterprise utilizing the data archive to provide data computing services to users of the enterprise. Examples of the enterprise may be inter alia data center etc. Examples of data computing services may be inter alia data archival and provisioning data backup data mining for business intelligence BI etc. In this specification the term content indicates software data component of the archive management system . In management semantics content management is primarily related to retention and utilization of massive amount of data in the data archive .

In this specification the term resource indicates a physical device hardware of the archive management system . In management semantics main purpose of resource management is to maximize availability of the services by keeping the resources fault tolerant. Conventional disaster recovery DR techniques are utilized for various resource configurations pursuant to service requirements such as Recovery Point Objective RPO Recovery Time Objective RTO etc. Examples of the resources may be inter alia servers storage devices communication equipments supporting network bandwidths etc. The servers employed in the archive management system are enterprise level servers supporting sophisticated virtualization management and reliability services. One example of the servers may be inter alia VMware ESX ESXi server infrastructure etc. VMware is a registered trademark of VMware Inc. in the United States The storage devices employed in the archive management system are persistent storage devices that store the data content the virtualization kernel and relevant files.

The data computing services of the enterprise requires extensive amount of maintenance activities which must be performed extra to the actual data computing services required by the users. Because the data computing services must meet a certain performance requirement level set forth in service agreements between the users and the enterprise the maintenance activities are usually performed during a period of lower service demands from the users. However because the enterprise is also required to make the services available to users for extended amount of time it is impractical to set a time window exclusive for maintenance activities without providing services to users. To perform necessary maintenance activities for ever growing amount of data in the data archive within a limited amount of time is imperative to the archive management system .

The archive management tasks are processes that perform archive management maintenance activities as scheduled by the job scheduler . Examples of the archive management maintenance activities may be inter alia provisioning disaster recovery DR maintenance service requirement compliance backup operations for various DR technologies updating indexes of newly generated data content that require content parsing and keyword lookup Hierarchical Storage Management HSM that moves inactive data to low cost storage tiers etc.

The job scheduler inputs comprise task information replica placement data infrastructure topology data and resource performance data.

The task information comprises a respective access path to objects necessary to perform a task a respective operation to be performed on the objects and system attributes that are useful for task scheduling such as object size. The tasks performs functions of inter alia content archiving indexing data mining for Business Intelligence BI purposes by use of available storage server network resources for primary data as well as replicas created for disaster recovery. All tasks require a relatively uniform amount of resources.

The replica placement data comprises addresses of replicas of primary data within the data archive that have been created for the purpose of disaster recovery.

The infrastructure topology data represent how a server is connected to a storage device volume within the data archive which is a path between the server and the storage device volume through elements of the data archive . The infrastructure topology data are collected by a part of the data archive that collects and maintains management data. The infrastructure topology data is represented in a quintuple wherein Hostname is a first identifier for a server wherein Initiator Port WWN is a second identifier for an initial port wherein Fabric WWN is a third identifier for a network fabric wherein Target Port WWN is a fourth identifier for a target port and wherein volume is a fifth identifier for a storage device volume. A World Wide Name WWN or World Wide Identifier WWID is a unique identifier in a Fiber channel or Serial attached Small Computer System Interface SCSI storage network. Each WWN is an eight 8 byte number combining first three 3 bytes derived from an Institute of Electrical and Electronics Engineers IEEE Organizationally Unique Identifier OUI and next five 5 bytes derived from vendor supplied information. WWNs are built into devices similar to Ethernet Media Access Control MAC address. The network fabric indicates a network topology where network nodes are connect with each other via one or more network switches as used in telecommunication field.

The resource performance data comprises performance metrics of resources such as clock speed tasks per second bytes per second etc. Accuracy of resource performance data regarding relative performance between storage devices and servers rather than absolute accuracy of respective performance data affect accuracy of optimal scheduling solutions .

The job scheduler creates a content resource analytic model from the job scheduler inputs and dynamically and continuously generates optimal scheduling solutions by use of the content resource analytic model . See description of infra for details of steps performed by the job scheduler . In this specification the terms scheduling solution schedule and job schedule are used interchangeably.

The content resource analytic model is an analytic model of the archive management system within the job scheduler . The content resource analytic model enables the job scheduler to continuously optimize operations of the archive management tasks and to generate the optimal scheduling solutions comprising an initial schedule and at least one dynamically adjusted schedule.

In conventional system analytics modeling the resource and the content are separately modeled and managed. Conventional resource management semantics focuses on devices of a system which utilizes historic performance statistics of the devices and predicts device saturation as a function of total workload. Conventional content management semantics focuses on software application and relevant data to simplify decision making processes for archive management tasks .

In contrast with conventional system analytics modeling the method of the present invention integrates both resource and content in the content resource analytic model to enable disaster recovery and data protection and to optimize both resource utilization and content centric archive management tasks . The method of the present invention utilizes existing data redundancy and storage paths among present replicas instead of creating new replicas as in conventional data intensive job scheduling. In one embodiment of the present invention the method of the present invention manipulates system parameters for dynamic optimization of the archive management tasks throughout the archive management system for overall performance gain. In another embodiment of the present invention the method of the present invention utilizes application specific parameters for selected maintenance tasks of which performance is critical to the overall performance of the archive management system .

The method of the present invention is utilized to parallelize data operations and computations for archiving compliance business intelligence queries to discover and correlate application content information with block level snapshot and mirroring details to limit archiving workload on the primary data by making it available for other conventional book keeping operations and to leverage replicas residing in both a local site and remote sites.

The flow network E comprises eight 8 levels indicating respective vertex groups. The flow network E further comprises directed edges from LEVEL k to LEVEL k 1 wherein k 0 . . . 6.

A source level LEVEL0 L0 comprises a source vertex of the flow network E. A first level LEVEL1 L1 comprises at least one task vertex that represents an individual task or a task set. Each task vertex in L1 has a respective incoming edge from the source vertex.

A first edge E from the source vertex in LEVEL0 to a task vertex V in LEVEL1 has time capacity t that indicates a unit of time required to perform the task associated with the task vertex V. In one embodiment of the present invention the time capacity t is assigned to 1 uniformly for all tasks in LEVEL1. In another embodiment of the present invention the time capacity t is assigned to a respective processing time associated with each task in LEVEL1 to reflect varying time required to process the respective task.

A second level LEVEL2 L2 comprises at least one volume vertex that represents a respective volume of storage devices. A third level LEVEL3 L3 comprises said at least one volume vertex. A volume is represented by a pair of volume vertices V in LEVEL2 and V in LEVEL3.

A second edge E from the task vertex V in LEVEL1 to a first volume vertex V indicates that the volume stores a latest copy of files necessary to perform the task represented by the task vertex V. The second edge E has a capacity of one 1 that indicates a unit of volume usage of the task represented by the task vertex V.

A third edge E from the first volume vertex V to a second volume vertex V has a volume throughput capacity indicating a number of tasks that the volume represented by the pair of vertices V and V can support.

A fourth level LEVEL4 L4 comprises at least one server vertex that represents a respective server. A fifth level LEVEL5 L5 comprises said at least one server vertex. A server is represented by a pair of server vertices V in LEVEL4 and V in LEVEL5.

A fourth edge E from the second volume vertex V to a first server vertex V indicates that the volume of V and V is physically connected to the server of V and V via a communication channel. The fourth edge E has a communication capacity that indicates throughput of the communication channel between the volume of V and V and the server of V and V. The communication channel may be a fabric a network and combinations thereof. The fourth edge E also has a path information of the communication channel between the volume of V and V and the server of V and V to enable prompt Storage Area Network SAN path construction.

A fifth edge E from the first server vertex V to a second server vertex V has a server throughput capacity indicating computational throughput of the server of V and V which comprises available processing cycles and memory.

A sixth level LEVEL6 L6 comprises at least one worker vertex comprising a worker vertex V that represents a worker and or software application.

A sixth edge E from the second server vertex V to the worker vertex V indicates that the server of V and V hosts the worker represented by the worker vertex . The sixth edge E has a host capacity indicating a number of workers that run on the server of V and V.

A sink level LEVEL7 L7 comprises a sink vertex of the flow network E. Each worker vertex in the sixth level LEVEL6 has an outgoing edge to the sink vertex analogous to a seventh edge E. The seventh edge E from the worker vertex V to the sink vertex has infinite capacity.

In step the archive management system collects information necessary for scheduling and provides the collected information to the job scheduler as the job scheduler inputs. In this specification the terms job scheduler inputs and inputs are used interchangeably. The inputs describe application and data collectively referred to as content and computing and communication resources of the archive management system. The inputs comprise task information replica placement data infrastructure topology data and resource performance data. See description of supra for details of the infrastructure topology data.

See description of steps through of infra for operations performed by the job scheduler in response to step of . After performing step the archive management system proceeds with step .

In one embodiment of the present invention the archive management system is an IBM Document Analysis and REcognition DARE system. The replica placement data is collected by a Disaster Recovery DR Orchestrator and the infrastructure topology data is retrieved from a TotalStorage Productivity Center TPC database. The DR Orchestrator and the TPC database are components of a data archive of the IBM Document Analysis and REcognition DARE system. IBM and TotalStorage are registered trademarks of International Business Machines Corp. in the United States .

In step the archive management system receives an initial schedule from the job scheduler in response to step of infra. Then the archive management system proceeds with step .

A loop of the archive management system comprising step through step is performed for each schedule that the archive management system received from the job scheduler.

In step the archive management system creates at least one actual Storage Area Network SAN path to establish a data and control path between user interface and resources. The archive management system creates said at least one SAN path from paths of the flow network created by the job scheduler in step of infra which indicate physical connectivity of resources. Then the archive management system proceeds with step .

In one embodiment of the present invention the SAN path is created from server storage network resources of the IBM Document Analysis and REcognition DARE system to IBM TotalStorage Productivity Center TPC Web Application Programming Interface API . In the same embodiment a path from LEVEL2 through LEVEL6 in the flow network of supra is transformed into a Volume Server Worker SAN path by use of zoning and masking. User inputs provided through TPC Web API comprise zoning parameters and masking parameters .

In step the archive management system deploys virtual machines to perform a received schedule. The received schedule is the initial schedule received in step in a first iteration of the loop and is an adjusted schedule received in step in subsequent iterations of the loop. Each archive management task in the received schedule is assigned to a respective virtual machine for operation. In one embodiment of the present invention the IBM Document Analysis and REcognition DARE system virtualizes the received schedule by deploying indexing virtual machines to VMware control. The archive management system proceeds with step .

In step the archive management system initiates the received schedule to perform the archive management tasks. Then the archive management system proceeds with step .

In step the archive management system monitors operations of the archive management tasks while performing the archive management tasks according to the received schedule. The archive management system provides monitoring results to the job scheduler. See description of steps through of infra for operations performed by the job scheduler in response to step of . After performing step the archive management system proceeds with step .

In step the archive management system receives a dynamically adjusted schedule from the job scheduler in response to step of infra. Then the archive management system loops back to step to process the adjusted schedule.

In step the job scheduler receives scheduling information as inputs comprising task information replica placement data infrastructure topology data and resource performance data from information collecting components of the archive management system in response to step of supra. The infrastructure topology data is at least one quintuple that represents a respective possible path between a server Hostname and a storage device volume. The resource performance data comprises server clock speed storage device clock speed and data transfer rate between servers and storage devices. Then the job scheduler proceeds with step .

In one embodiment of the present invention the job scheduler invokes a web service call requesting the task information for a set of tasks to be scheduled to the archive management system when a job comprising the set of tasks is ready for processing. See description of supra for elements of the task information.

In step the job scheduler creates a flow network by use of the inputs. The flow network is an embodiment of the content resource analytics model of supra that represents both content and resource of the archive management system. The flow network models uniform and unrelated parallel machines. An optimal mapping in the flow network corresponds to an optimal scheduling solution in terms of time to perform archive management tasks. See steps through of infra for steps creating the flow network. Then the job scheduler proceeds with step .

In one embodiment of the present invention the job scheduler employs three basic assumptions. A first assumption is that each task takes a unit amount of processing time regardless of characteristics of workload for respective tasks which may be either computation bound or data operation bound. The first assumption means that all tasks has uniform throughput. The first assumption prevents max flow computation of step infra from arbitrarily splitting task groups. A second assumption is that all performance input can be aggregated as a static achievable throughput of tasks per unit time. A third assumption is that each task has a binary value indicating whether or not the task can be processed at a replicated volume.

In step the job scheduler computes an initial schedule that is an optimal static mapping in the flow network by use of max flow computation. The optimal static mapping is a path from LEVEL1 to LEVEL6 with a minimum capacity value within the flow network. After step the job scheduler proceeds with step .

Step comprises multiple sub steps. First the job scheduler multiplies a factor of time denoted as T on a respective weight of each edge that represents throughput of resources which is Edge LEVEL2 LEVEL3 Edge LEVEL3 LEVEL4 or Edge LEVEL4 LEVEL5 . Second the job scheduler rounds down each result of the multiplication to a closest integer value and adjust capacity of said each edge. Third the job scheduler performs a series of max flow computations on the flow network with the adjusted capacity of said each edge. If a max flow resulting from a max flow computation is equal to a total number of tasks denoted as N then all N tasks can be completed within time T. Fourth the job scheduler generates candidate schedules from paths of the flow network. Fifth the job scheduler searches the candidate schedules that also can complete all N tasks to find a schedule with a smallest time value T which is an optimal schedule.

In one embodiment of the present invention the job scheduler employs Edmonds Karp algorithm to compute max flows. In the same embodiment the job scheduler employs binary search to search the candidate schedules for the optimal schedule.

In one embodiment of the present invention an upper bound value for time to compute the optimal schedule is the total number of tasks multiplied by a primary volume capacity that is N Capacity based on that all tasks can be performed on a primary volume for a respective task. In another embodiment of the present invention the upper bound for time to compute the optimal schedule is a maximum value among values equal to the total number of tasks multiplied by a volume capacity that is MAX N Capacity which generalizes any volume that may be selected for the tasks.

The job scheduler may optimize step with various techniques. A first optimization technique is task aggregation for which each vertex in LEVEL1 represent a task set instead of an individual task. Capacity of respective edges from LEVEL1 through LEVEL3 comprising all edges LEVEL1 LEVEL2 and edge LEVEL2 LEVEL3 is set to a value equal to the number of tasks in the task set. Each volume in LEVEL2 and LEVEL3 has a respective timestamp such that said each volume keeps up to date copies of all files modified prior to the time of the respective timestamp. With task aggregation and volume timestamps the number of task sets is equal to the number of volumes. When both task sets and volumes are ordered by processing time to complete each task set there is a mapping from the i th task set to only j th volumes for j i wherein i and j are positive integers. Because task aggregation reduces the number of vertices and edges the job scheduler spends less time computing the optimal schedule in step .

A second optimization of step is minimum cost method in which a schedule with a smallest number of worker volume mappings is selected as the optimal schedule. Subsequent to obtaining the smallest time value T the job scheduler runs a min cost max flow algorithm where each incoming edge to a volume vertex in LEVEL2 has a cost value. As a result the job scheduler finds the optimal schedule that satisfy both the smallest time value T and the smallest number of worker volume mappings.

In step the job scheduler provides the initial schedule to the archive management system for deployment. See descriptions of steps through of supra for operations of the archive management system performed in response to step of . Then the job scheduler proceeds with step .

A loop of the job scheduler comprising step through step is performed for each schedule that the job scheduler creates for continuous optimization.

In step the job scheduler receives monitoring results of operations performed by a current schedule. The current schedule is the initial schedule computed in step in a first iteration of the loop and is an adjusted schedule computed in step in subsequent iterations of the loop. Then the job scheduler proceeds with step .

In step the job scheduler adjusts the current schedule per monitoring results received in step to optimize performance. Then the job scheduler proceeds with step .

In contrast with the initial schedule in step that is static and optimal based on the three basic assumptions about behaviors of the archive management system the job scheduler in step utilizes heuristics to dynamically optimize the current schedule according to actual performance monitoring results of the current schedule. The job scheduler assigns a respective capacity to each task in LEVEL1 to reflect individuality of said each task in terms of amount of resource required to perform the task throughput of the task etc based on metrics obtained from the performance monitoring data such as size of files in the task etc. The job scheduler employs heuristics for optimization because monitoring result that represents past behavior of the archive management system is not enough to predict future behavior of the archive management system.

In first iteration of the loop adjusting the initial schedule the job scheduler translates the initial schedule that is static into a dynamic set of operations according to current conditions of the archive management system. The job scheduler puts a group of tasks in a central repository and schedules each task on demand when a worker has resources available. The tasks in the group assigned to the worker are ordered according to a predefined heuristic metric and once the worker acquires necessary resources to perform a task and demands a new task a first task with a highest heuristic metric is scheduled. Examples of heuristic metrics may be inter alia capacity of a task out degree of a task total volume load minimum volume load etc. The capacity of the task is computed dynamically or statically according to metrics comprising file lengths. The out degree of the task indicates a number of volumes that can be employed to perform the task. Wherein a first task has a larger out degree than a second task it is more likely for the job scheduler to choose the first task sooner than the second task. The total volume load denotes a cumulative load on all volumes that may handle the task. The total volume load is same as a sum of in degrees of all volumes connected to the task in the flow network. Wherein a first task has a smaller total volume load than a second task it is more likely for the job scheduler to choose the first task sooner than the second task. The minimum volume load denotes a load on a least loaded volume that may handle the task. The minimum volume load is same as a smallest in degree of all volumes connected to the task in the flow network. Wherein a first task has a smaller minimum volume load than a second task it is more likely for the job scheduler to choose the first task sooner than the second task. Both heuristic metrics and system performance can be updated dynamically according to feedback from the system.

In step the job scheduler provides the adjusted schedule from step to archive management system. Then the job scheduler loops back to step to continuously and dynamically optimize performance of the adjusted schedule.

The flow network is defined as a directed graph in which each edge has a capacity that limits the amount of flow that pass through each edge. Vertices of the flow network comprise a source that has only outgoing flows and a sink that has only incoming flows. Any vertices between the source and the sink have a respective incoming flow and outgoing flow. Flow networks are typically used to model road traffic system fluid in pipes currents in an electrical circuit data traffic travels through a network of nodes etc. In one embodiment of the present invention conventional graph libraries that support computation algorithms such as Edmonds Karp max flow computation are utilizes for the flow network modeling. Examples of conventional graph libraries may be inter alia the Java Universal Network Graph JUNG Framework LEMON open source graph library and the Boost Graph Library BGL etc. Java is a registered trademark of Sun Microsystems Inc. in the United States 

In step the job scheduler creates setup vertices in each level of the flow network model per respective vertex definition for LEVEL1 to LEVEL6 according to the definitions of supra. Then the job scheduler proceeds with step .

In step the job scheduler creates directed edges from vertices of LEVEL k to vertices of LEVEL k 1 per respective edge definition Edge LEVEL k LEVEL k 1 wherein k 1.5. Then the job scheduler proceeds with step .

In step the job scheduler assigns capacity of a respective edge for all edges created in step per capacity definition respective to Edge LEVEL k LEVEL k 1 wherein k 1.5. Then the job scheduler proceeds with step .

In step the job scheduler creates a path in the flow network from tasks in LEVEL1 to workers in LEVEL6 that represent a job schedule to perform the tasks in LEVEL1 by use of volumes in LEVEL2 and LEVEL3 servers in LEVEL4 and LEVEL5 and workers in LEVEL6.

The computer system comprises a processor an input device coupled to the processor an output device coupled to the processor and computer readable memory units comprising memory devices and each coupled to the processor . The input device may be inter alia a keyboard a mouse a keypad a touch screen a voice recognition device a sensor a network interface card NIC a Voice video over Internet Protocol VoIP adapter a wireless adapter a telephone adapter a dedicated circuit adapter etc. The output device may be inter alia a printer a plotter a computer screen a magnetic tape a removable hard disk a floppy disk a NIC a VoIP adapter a wireless adapter a telephone adapter a dedicated circuit adapter an audio and or visual signal generator a light emitting diode LED etc. The memory devices and may be inter alia a cache a dynamic random access memory DRAM a read only memory ROM a hard disk a floppy disk a magnetic tape an optical storage such as a compact disk CD or a digital video disk DVD etc. The memory device includes a computer code which is a computer program that comprises computer executable instructions. The computer code includes inter alia an algorithm used for continuously optimizing data archive management scheduling according to the present invention. The processor executes the computer code . The memory device includes input data . The input data includes input required by the computer code . The output device displays output from the computer code . Either or both memory devices and or one or more additional memory devices not shown in may be used as a computer usable storage medium or a computer readable storage medium or a program storage device having a computer readable program embodied therein and or having other data stored therein wherein the computer readable program comprises the computer code . Generally a computer program product or alternatively an article of manufacture of the computer system may comprise said computer usable storage medium or said program storage device .

Any of the components of the present invention can be deployed managed serviced etc. by a service provider that offers to deploy or integrate computing infrastructure with respect to a process for dynamically building a web interface per data collecting rules of the present invention. Thus the present invention discloses a process for supporting computer infrastructure comprising integrating hosting maintaining and deploying computer readable code into a computing system e.g. computing system wherein the code in combination with the computing system is capable of performing a method for continuously optimizing data archive management scheduling.

In another embodiment the invention provides a business method that performs the process steps of the invention on a subscription advertising and or fee basis. That is a service provider such as a Solution Integrator can offer to create maintain support etc. a process for continuously optimizing data archive management scheduling of the present invention. In this case the service provider can create maintain support etc. a computer infrastructure that performs the process steps of the invention for one or more customers. In return the service provider can receive payment from the customer s under a subscription and or fee agreement and or the service provider can receive payment from the sale of advertising content to one or more third parties.

While shows the computer system as a particular configuration of hardware and software any configuration of hardware and software as would be known to a person of ordinary skill in the art may be utilized for the purposes stated supra in conjunction with the particular computer system of . For example the memory devices and may be portions of a single memory device rather than separate memory devices.

As will be appreciated by one skilled in the art the present invention may be embodied as a system method or computer program product. Accordingly the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore the present invention may take the form of a computer program product embodied in any tangible medium of expression having computer usable program code embodied in the medium.

Any combination of one or more computer usable or computer readable medium s may be utilized. The term computer usable medium or computer readable medium collectively refers to computer usable readable storage medium . The computer usable or computer readable medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus a device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing . . . . Note that the computer usable or computer readable medium could even be paper or another suitable medium upon which the program is printed as the program can be electronically captured via for instance optical scanning of the paper or other medium then compiled interpreted or otherwise processed in a suitable manner if necessary and then stored in a computer memory. In the context of this document a computer usable or computer readable medium may be any medium that can contain or store a program for use by or in connection with an instruction running system apparatus or device.

Computer code for carrying out operations of the present invention may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The computer code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

The present invention is described with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. The term computer program instructions is interchangeable with the term computer code in this specification. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in the computer readable medium that can direct a computer or other programmable data processing apparatus to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instruction means which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operational steps to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that in some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be run substantially concurrently or the blocks may sometimes be run in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

The corresponding structures materials acts and equivalents of all means or step plus function elements in the claims are intended to include any structure material or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present invention has been presented for purposes of illustration and description but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The embodiment was chosen and described in order to best explain the principles of the invention and the practical application and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.

