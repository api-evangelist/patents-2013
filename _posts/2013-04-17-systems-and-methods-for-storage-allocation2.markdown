---

title: Systems and methods for storage allocation
abstract: Resources of an address space are managed in dynamically sized ranges, extents, sets, and/or blocks. The address space may be divided into regions, each corresponding to a different, respective allocation granularity. Allocating a block within a first region of the address space may comprise allocating a particular number of logical addresses (e.g., a particular range, set, and/or block of addresses), and allocating a block within a different region may comprise allocating a different number of logical addresses. The regions may be configured to reduce the metadata overhead needed to identify free address blocks (and/or maintain address block allocations), while facilitating efficient use of the address space for differently sized data structures.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09563555&OS=09563555&RS=09563555
owner: SanDisk Technologies LLC
number: 09563555
owner_city: Plano
owner_country: US
publication_date: 20130417
---
This application is a continuation in part of and claims priority to U.S. patent application Ser. No. 13 424 333 entitled Logical Interfaces for Contextual Storage filed Mar. 19 2012 for David Flynn et al. and which claims priority to U.S. Provisional Patent Application No. 61 454 235 entitled Virtual Storage Layer Supporting Operations Ordering a Virtual Address Space Atomic Operations and Metadata Discovery filed Mar. 18 2011 this application also claims priority to U.S. Provisional Patent Application No. 61 625 647 entitled Systems Methods and Interfaces for Managing a Logical Address Space filed Apr. 17 2012 for David Flynn et al. and to U.S. Provisional Patent Application No. 61 637 165 entitled Systems Methods and Interfaces for Managing a Logical Address Space filed Apr. 23 2012 for David Flynn et al. each of which is incorporated by reference.

This disclosure relates to storage systems and in particular to managing an address space of a storage system.

A computing system may provide a logical address space of a storage device and or system. The logical address space may comprise identifiers used by storage clients to reference storage resources. The computing system may further comprise a logical to physical translation layer configured to map identifiers of the logical address space with the storage location of data associated with the identifiers. The translation layer may comprise any to any mappings between identifiers and physical addresses. The logical address space may be independent of the underlying physical storage resources and may exceed the capacity of the physical storage resources. Storage clients may allocate portions of the logical address space to perform storage operations. Maintaining allocation metadata pertaining to the logical address space may however impose significant overhead.

Disclosed herein are embodiments of methods of managing storage allocation. The disclosed methods may comprise one or more machine executable operations and or steps. The disclosed operations and or steps may be embodied as program code stored on a computer readable storage medium. Accordingly embodiments of the methods disclosed herein may be embodied as a computer program product comprising a computer readable storage medium storing computer usable program code executable to cause a computing device to perform one or more method operations and or steps.

Embodiments of the disclosed method may comprise a computing device providing an address space of a storage device the address space configured such that at least two or more addresses of the address space are associated with a different physical storage capacity and allocating one of the at least two or more addresses to a storage client in response to a storage request. The allocation granularity may pertain to allocation of logical addresses within the address space. Alternatively or in addition the allocation granularity may pertain to data segment size corresponding to one or more logical address of the address space.

The method may further comprise allocating a logical identifier within a first section of the address space corresponding to a first sector size on the storage device and allocating a logical identifier within a different section of the address space corresponding to a different sector size on the storage device.

In some embodiments the method includes allocating storage resources within a selected section of the address space in response to a request from a storage client and selecting the section based on one or more of a size of data associated with the request a size of a data structure associated with the request a size of a storage entity associated with the request a file associated with the request an application associated with the request a parameter of the request a storage client associated with the request an input output I O control ioctrl parameter an fadvise parameter and availability of unallocated logical addresses within the sections. Dividing the address space may comprise partitioning logical addresses within the address space into an identifier portion and an offset portion wherein relative sizes of the identifier portions to the offset portions vary between the sections.

Some embodiments of the method may further comprise moving data stored on the storage device to a different section of the address space. Moving the data may comprise associating the data with a different logical address than a logical address stored with the data on the storage device and or updating persistent metadata of the data to reference the different logical address in response to relocating the data on the storage device.

Disclosed herein are embodiments of an apparatus comprising a translation module configured to manage a logical address space of a storage device a partitioning module configured to segment the logical address space into a plurality of different regions the individual regions having a different respective allocation granularity and an allocation module configured to allocate logical identifiers within the regions in accordance with the allocation granularities of the regions. The apparatus may further include an interface module configured to provide for specifying a region of the logical address space in which to perform one or more of an allocation operation and a storage operation. The interface module may be configured to provide information pertaining to the allocation granularities of one or more of the regions to a storage client. The different respective allocation granularities of the regions may pertain to one or more of a logical identifier block size for allocation operations performed within the respective regions and a data sector size associated with the logical identifiers of the respective regions. The allocation module may be configured to associate logical identifiers within the logical address space with one of a plurality of data sector sizes. In some embodiments the apparatus includes a data read module configured to read a data segment associated with a logical identifier on the storage device wherein a size of the data segment corresponds to a data sector size associated with the logical identifier.

In some embodiments the disclosed apparatus also includes a reallocation module configured to reallocate a set of logical identifiers corresponding to data stored on the storage device to a different set of logical identifiers. The reallocation module may be configured to modify a size of a block of logical identifiers associated with the data and the reallocation module may be configured to move one or more of the logical identifiers to another region of the logical address space. Each region may comprise one or more blocks of logical identifiers within the logical address space. The reallocation module may be configured to combine a plurality of blocks allocated within a first region of the logical address space into a single larger block of logical identifiers within a different region of the logical address space. Alternatively or in addition the reallocation module may be configured to reallocate a block of logical identifiers within a first region of the logical address space as one or more smaller blocks of logical identifiers within a different region of the logical address space.

The apparatus may further include a log storage module configured to store data on the storage device in association with respective logical identifiers corresponding to the data. The reallocation module may be configured to modify the logical identifier associated with a data segment such that the logical identifier associated with the data segment on the storage device is inconsistent with the modified logical identifier. The apparatus may include a translation module configured to reference the data segment associated with the inconsistent logical identifier on the storage device by use of the modified logical identifier. The log storage module may be configured to store the data segment in association with the modified logical identifier on the storage device in response to grooming a storage division comprising the data segment.

Disclosed herein are embodiments of a method comprising associating logical addresses of an address space with respective sector sizes wherein the sector size associated with a logical address corresponds to a physical storage capacity on a storage device corresponding to the logical address determining a sector size of one of the logical addresses in response to a request and performing a storage operation on the storage device in accordance with the determined sector size. The method may further include selecting a sector size for the logical address based on one or more of a file associated with the logical address an application associated with the logical address the storage client associated with the logical address an input output I O control parameter and an fadvise parameter. In some embodiments the method further includes determining an available physical storage capacity of the storage device based on sector sizes of logical addresses of the address space that are associated with valid data on the storage device and or assigning a different respective sector size to each of a plurality of segments of the address space wherein determining the sector size of the logical address comprises associating the logical address with one of the segments.

According to various embodiments a storage layer manages one or more storage devices. The storage device s may comprise non volatile storage devices such as solid state storage device s that are arranged and or partitioned into a plurality of addressable media storage locations. As used herein a media storage location refers to any physical unit of storage e.g. any physical storage media quantity on a storage device . Media storage units may include but are not limited to pages storage divisions erase blocks sectors blocks collections or sets of physical storage locations e.g. logical pages logical erase blocks etc. described below or the like.

The storage layer may be configured to present a logical address space to one or more storage clients. As used herein a logical address space refers to a logical representation of storage resources. The logical address space may comprise a plurality e.g. range of logical identifiers. As used herein a logical identifier LID refers to any identifier for referencing a storage resource e.g. data including but not limited to a logical block address LBA a cylinder head sector CHS address a file name an object identifier an inode a Universally Unique Identifier UUID a Globally Unique Identifier GUID a hash code a signature an index entry a range an extent or the like. The logical address space LIDs and relationships between LIDs and storage resources define a logical interface through which storage clients access storage resources. As used herein a logical interface refers to a handle identifier path process or other mechanism for referencing and or interfacing with a storage resource. A logical interface may include but is not limited to a LID a range or extent of LIDs a reference to a LID e.g. a link between LIDs a pointer to a LID etc. a reference to a virtual storage unit or the like. A logical interface may be used to reference data through a storage interface and or application programming interface API .

The storage layer may maintain storage metadata such as a forward index to map LIDs of the logical address space to media storage locations on the storage device s . The storage layer may provide for arbitrary any to any mappings to physical storage resources. Accordingly there may be no pre defined and or pre set mappings between LIDs and particular media storage locations and or media addresses. As used herein a media address refers to an address of a storage resource that uniquely identifies one storage resource from another to a controller that manages a plurality of storage resources by way of example a media address includes but is not limited to the address of a media storage location a physical storage unit a collection of physical storage units e.g. a logical storage unit a portion of a media storage unit e.g. a logical storage unit address and offset range and or extent or the like. Accordingly the storage layer may map LIDs to physical data resources of any size and or granularity which may or may not correspond to the underlying data partitioning scheme of the storage device s . For example in some embodiments the storage controller is configured to store data within logical storage units that are formed by logically combining a plurality of physical storage units which may allow the storage controller to support many different virtual storage unit sizes and or granularities.

As used herein a logical storage element refers to a set of two or more non volatile storage elements that are or are capable of being managed in parallel e.g. via an I O and or control bus . A logical storage element may comprise a plurality of logical storage units such as logical pages logical storage divisions e.g. logical erase blocks and so on. Each logical storage unit may be comprised of storage units on the non volatile storage elements in the respective logical storage element. As used herein a logical storage unit refers to logical construct combining two or more physical storage units each physical storage unit on a respective solid state storage element in the respective logical storage element each solid state storage element being accessible in parallel . As used herein a logical storage division refers to a set of two or more physical storage divisions each physical storage division on a respective solid state storage element in the respective logical storage element.

The logical address space presented by the storage layer may have a logical capacity which may comprise a finite set or range of LIDs. The logical capacity of the logical address space may correspond to the number of available LIDs in the logical address space and or the size and or granularity of the data referenced by the LIDs. For example the logical capacity of a logical address space comprising 2 32 unique LIDs each referencing 2048 bytes 2 kb of data may be 2 43 bytes. In some embodiments the logical address space may be thinly provisioned. As used herein a thinly provisioned logical address space refers to a logical address space having a logical capacity that exceeds the physical storage capacity of the underlying storage device s . For example the storage layer may present a 64 bit logical address space to the storage clients e.g. a logical address space referenced by 64 bit LIDs which exceeds the physical storage capacity of the underlying storage devices. The large logical address space may allow storage clients to allocate and or reference contiguous ranges of LIDs while reducing the chance of naming conflicts. The storage layer may leverage the any to any mappings between LIDs and physical storage resources to manage the logical address space independently of the underlying physical storage devices. For example the storage layer may add and or remove physical storage resources seamlessly as needed and without changing the logical interfaces used by the storage clients.

The storage layer may be configured to store data in a contextual format. As used herein a contextual format refers to a self describing data format in which persistent metadata is associated with the data on the physical storage media e.g. stored with the data in a packet or other data structure . The persistent metadata provides context for the data with which it is stored. In certain embodiments the persistent metadata uniquely identifies the data with which the persistent metadata is stored. For example the persistent metadata may uniquely identify a sector of data owned by a storage client from other sectors of data owned by the storage client. In a further embodiment the persistent metadata identifies an operation that is performed on the data. In a further embodiment the persistent metadata identifies an order of a sequence of operations performed on the data. In a further embodiment the persistent metadata identifies security controls a data type or other attributes of the data. In certain embodiment the persistent metadata identifies at least one of a plurality of aspects including data type a unique data identifier an operation and an order of a sequence of operations performed on the data. The persistent metadata may include but is not limited to a logical interface of the data an identifier of the data e.g. a LID file name object id label unique identifier or the like reference s to other data e.g. an indicator that the data is associated with other data a relative position or offset of the data with respect to other data e.g. file offset etc. data size and or range and the like. The contextual data format may comprise a packet format comprising a data segment and one or more headers. Alternatively a contextual data format may associate data with context information in other ways e.g. in a dedicated index on the non volatile storage media a storage division index or the like . Accordingly a contextual data format refers to a data format that associates the data with a logical interface of the data e.g. the context of the data . A contextual data format is self describing in that the contextual data format includes the logical interface of the data.

In some embodiments the contextual data format may allow data context to be determined and or reconstructed based upon the contents of the non volatile storage media and independently of other storage metadata such as the arbitrary any to any mappings discussed above. Since the media storage location of data is independent of the logical interface of the data it may be inefficient or impossible to determine the context of data based solely upon the media storage location or media address of the data. Storing data in a contextual format on the non volatile storage media may allow data context to be determined without reference to other storage metadata. For example the contextual data format may allow the logical interface of data to be reconstructed based only upon the contents of the non volatile storage media e.g. reconstruct the any to any mappings between LID and media storage location .

In some embodiments the storage controller may be configured to store data on an asymmetric write once storage media such as solid state storage media. As used herein a write once storage media refers to a storage media that is reinitialized e.g. erased each time new data is written or programmed thereon. As used herein asymmetric storage media refers to storage media having different latencies for different storage operations. Many types of solid state storage media are asymmetric for example a read operation may be much faster than a write program operation and a write program operation may be much faster than an erase operation e.g. reading the media may be hundreds of times faster than erasing and tens of times faster than programming the media . The storage media may be partitioned into storage divisions that can be erased as a group e.g. erase blocks in order to inter alia account for the asymmetric properties of the media. As such modifying a single data segment in place may require erasing the entire erase block comprising the data and rewriting the modified data to the erase block along with the original unchanged data. This may result in inefficient write amplification which may excessively wear the media. Therefore in some embodiments the storage controller may be configured to write data out of place. As used herein writing data out of place refers to writing data to different media storage location s rather than overwriting the data in place e.g. overwriting the original physical location of the data . Modifying data out of place may avoid write amplification since existing valid data on the erase block with the data to be modified need not be erased and recopied. Moreover writing data out of place may remove erasure from the latency path of many storage operations the erasure latency is no longer part of the critical path of a write operation .

The storage controller may comprise one or more processes that operate outside of the regular path for servicing of storage operations the path for performing a storage operation and or servicing a storage request . As used herein the regular path for servicing a storage request or path for servicing a storage operation also referred to as a critical path refers to a series of processing operations needed to service the storage operation or request such as a read write modify or the like. The path for servicing a storage request may comprise receiving the request from a storage client identifying the logical interface of the request e.g. LIDs pertaining to the request performing one or more storage operations on a non volatile storage media and returning a result such as acknowledgement or data. Processes that occur outside of the path for servicing storage requests may include but are not limited to a groomer deduplication and so on. These processes may be implemented autonomously and in the background from servicing storage requests such that they do not interfere with or impact the performance of other storage operations and or requests. Accordingly these processes may operate independent of servicing storage requests.

In some embodiments the storage controller comprises a groomer which is configured to reclaim storage divisions erase blocks for reuse. The write out of place paradigm implemented by the storage controller may result in obsolete or invalid data data that has been erased modified and or overwritten remaining on the storage device. For example overwriting data X with data Y may result in storing Y on a new storage division rather than overwriting X in place and updating the any to any mappings of the storage metadata to identify Y as the valid up to date version of the data. The obsolete version of the data X may be marked as invalid but may not be immediately removed e.g. erased since as discussed above erasing X may involve erasing an entire storage division which is a time consuming operation and may result in write amplification. Similarly data that is no longer is use e.g. deleted or trimmed data may not be immediately removed. The non volatile storage media may accumulate a significant amount of invalid data. A groomer process may operate outside of the critical path for servicing storage operations. The groomer process may reclaim storage divisions so that they can be reused for other storage operations. As used herein reclaiming a storage division refers to erasing the storage division so that new data may be stored programmed thereon. Reclaiming a storage division may comprise relocating valid data on the storage division to a new storage location. The groomer may identify storage divisions for reclamation based upon one or more factors which may include but are not limited to the amount of invalid data in the storage division the amount of valid data in the storage division wear on the storage division e.g. number of erase cycles time since the storage division was programmed or refreshed and so on.

The storage controller may be further configured to store data in a log format. As described above a log format refers to a data format that defines an ordered sequence of storage operations performed on a non volatile storage media. In some embodiments the log format comprises storing data in a pre determined sequence within the media address space of the non volatile storage media e.g. sequentially within pages and or erase blocks of the media . The log format may further comprise associating data e.g. each packet or data segment with respective sequence indicators. The sequence indicators may be applied to data individually e.g. applied to each data packet and or to data groupings e.g. packets stored sequentially on a storage division such as an erase block . In some embodiments sequence indicators may be applied to storage divisions when the storage divisions are reclaimed e.g. erased as described above and or when the storage divisions are first used to store data.

In some embodiments the log format may comprise storing data in an append only paradigm. The storage controller may maintain a current append point within a media address space of the storage device. The append point may be a current storage division and or offset within a storage division. Data may then be sequentially appended from the append point. The sequential ordering of the data therefore may be determined based upon the sequence indicator of the storage division of the data in combination with the sequence of the data within the storage division. Upon reaching the end of a storage division the storage controller may identify the next available storage division the next storage division that is initialized and ready to store data . The groomer may reclaim storage divisions comprising invalid stale and or deleted data to ensure that data may continue to be appended to the media log.

The log format described herein may allow valid data to be distinguished from invalid data based upon the contents of the non volatile storage media and independently of the storage metadata. As discussed above invalid data may not be removed from the storage media until the storage division comprising the data is reclaimed. Therefore multiple versions of data having the same context may exist on the non volatile storage media e.g. multiple versions of data having the same logical interface and or same LID . The sequence indicators associated with the data may be used to distinguish invalid versions of data from the current up to date version of the data the data that is the most recent in the log is the current version and all previous versions may be identified as invalid.

According to various embodiments a logical interface of data stored in a contextual format is modified. The contextual format of the data may be inconsistent with the modified logical interface. As used herein an inconsistent contextual data format refers to a contextual data format that defines a logical interface to data on storage media that is inconsistent with the logical interface of the data. The logical interface of the data may be maintained by a storage layer storage controller or other module. The inconsistency may include but is not limited to the contextual data format associating the data with a different LID than the logical interface the contextual data format associating the data with a different set of LIDs than the logical interface the contextual data format associating the data with a different LID reference than the logical interface or the like. The storage controller may provide access to the data in the inconsistent contextual format and may update the contextual format of the data of the non volatile storage media to be consistent with the modified logical interface. The update may require rewriting the data out of place and as such may be deferred. As used herein a consistent contextual data format refers to a contextual data format that defines the same or an equivalent logical interface as the logical interface of the data which may include but is not limited to the contextual data format associating the data with the same LID s or equivalent LID s as the logical interface the contextual data format associating the LID with the same set of LIDs as the logical interface the contextual data format associating the data with the same reference LID as the logical interface or the like.

According to various embodiments a storage controller and or storage layer performs a method for managing a logical address space comprising modifying a logical interface of data stored in a contextual format on a non volatile storage media wherein the contextual format of the data on the non volatile storage media is inconsistent with the modified logical interface of the data accessing the data in the inconsistent contextual format through the modified logical interface and updating the contextual format of the data on the non volatile storage media to be consistent with the modified logical interface. The logical interface of the data may be modified in response to a request e.g. a request from a storage client . The request may comprise a move clone e.g. copy deduplication or the like. The request may return e.g. be acknowledged by the storage layer before the contextual format of the data is updated on the non volatile storage media. Modifying the logical interface may further comprise storing a persistent note on the non volatile storage media indicative of the modification to the logical interface e.g. associate the data with the modified logical interface . The contextual format of the data may be updated out of place at other media storage locations on the non volatile storage media. Updates to the contextual format may be deferred and or made outside of the path of other storage operations e.g. independent of servicing other storage operations and or requests . For example the contextual format of the data may be updated as part of a grooming process. When reclaiming a storage division data that is in an inconsistent contextual format may be identified and updated as the data is relocated to new media storage locations. Providing access to the data through the modified logical interface may comprise referencing the data in the inconsistent contextual format through one or more reference entry and or indirect entries in an index.

In the following detailed description reference is made to the accompanying drawings which form a part thereof. The foregoing summary is illustrative only and is not intended to be in any way limiting. In addition to the illustrative aspects embodiments and features described above further aspects embodiments and features will become apparent by reference to the drawings and the following detailed description.

The storage layer may be configured to provide storage services to one or more storage clients . The storage clients may include local storage clients operating on the computing device and or remote storage clients accessible via the network and communication interface . The storage clients may include but are not limited to operating systems file systems database applications server applications kernel level processes user level processes applications and the like.

The storage layer comprises and or is communicatively coupled to one or more storage devices A N. The storage devices A N may include different types of storage devices including but not limited to solid state storage devices hard drives SAN storage resources or the like. The storage devices A N may comprise respective controllers A N and non volatile storage media A N. The storage layer may comprise an interface configured to provide access to storage services and or metadata maintained by the storage layer . The interface may be comprise but is not limited to a block I O interface a virtual storage interface a cache interface and the like. Storage metadata may be used to manage and or track storage operations performed through any of the block I O interface virtual storage interface cache interface or other related interfaces.

The cache interface may expose cache specific features accessible through the storage layer . In some embodiments the virtual storage interface presented to the storage clients provides access to data transformations implemented by the non volatile storage device and or the non volatile storage media controller .

The storage layer may provide storage services through one or more interfaces which may include but are not limited to a block I O interface an extended virtual storage interface a cache interface and the like. The storage layer may present a logical address space to the storage clients through one or more interfaces. As discussed above the logical address space may comprise a plurality of LIDs each corresponding to respective media storage locations on one or more of the storage devices A N. The storage layer may maintain storage metadata comprising any to any mappings between LIDs and media storage locations as described above. The logical address space and storage metadata may therefore define a logical interface of data stored on the storage devices A N.

The storage layer may further comprise a log storage module that is configured to store data in a contextual log format. The contextual log data format may comprise associating data with persistent metadata such as the logical interface of the data e.g. LID or the like. The contextual log format may further comprise associating data with respective sequence identifiers on the non volatile storage media A N which define an ordered sequence of storage operations performed on the storage devices A N as described above.

The storage layer may further comprise a storage device interface configured to transfer data commands and or queries to the storage devices A N over a bus which may include but is not limited to a peripheral component interconnect express PCI Express or PCIe bus a serial Advanced Technology Attachment ATA bus a parallel ATA bus a small computer system interface SCSI FireWire Fibre Channel a Universal Serial Bus USB a PCIe Advanced Switching PCIe AS bus a network Infiniband SCSI RDMA or the like. The storage device interface may communicate with the storage devices A N using input output control IO CTL command s IO CTL command extension s remote direct memory access or the like.

The non volatile storage devices A N may comprise non volatile storage media A N which may include but is not limited to NAND flash memory NOR flash memory nano random access memory nano RAM or NRAM magneto resistive RAM MRAM dynamic RAM DRAM phase change RAM PRAM magnetic storage media e.g. hard disk tape optical storage media or the like.

Portions of the storage layer may be implemented by use of one or more drivers kernel level applications user level applications and the like which may be configured to operate within an operating system guest operating system e.g. in a virtualized computing environment or the like. Other portions of the storage layer may be implemented by use of hardware components such as one or more controllers Field Programmable Gate Arrays FPGAs Application Specific Integrated Circuits ASICs and or the like.

The storage layer may present a logical address space to the storage clients through one or more of the interfaces and or of the interface . The storage layer may maintain storage metadata comprising any to any mappings between LIDs in the logical address space and media storage locations on one or more non volatile storage devices A N. The storage layer may further comprise a log storage module configured to store data on the storage device s A N in a contextual log format. The contextual log data format may comprise storing data in association with persistent metadata such as the logical interface of the data. The contextual log format may further comprise associating data with respective sequence identifiers that define an ordered sequence of storage operations performed through the storage layer .

The storage media controller may comprise a storage request receiver module configured to receive storage requests from the storage layer via a bus . The storage request receiver may be further configured to transfer data to from the storage layer and or storage clients via the bus . Accordingly the storage request receiver module may comprise one or more direct memory access DMA modules remote DMA modules bus controllers bridges buffers and so on.

The storage media controller may comprise a data write module that is configured to store data on the non volatile storage media in a contextual format. The requests may include and or reference data to be stored on the non volatile storage media may include logical interface of the data e.g. LID s of the data and so on. The data write module may comprise a contextual write module and a write buffer . As described above the contextual format may comprise storing a logical interface of the data e.g. LID of the data in association with the data on the non volatile storage media . In some embodiments the contextual write module is configured to format data into packets and may include the logical interface of the data in a packet header or other packet field . The write buffer may be configured to buffer data for storage on the non volatile storage media . The data packets may comprise an arbitrary amount data. In some embodiments the write buffer may comprise one or more synchronization buffers to synchronize a clock domain of the storage media controller with a clock domain of the non volatile storage media and or bus . The data write module may be configured to store data in arbitrarily sized structures packets on the non volatile storage media .

The log storage module may be configured to select media storage location s for the data and may provide addressing and or control information to the non volatile storage elements via the bus . In some embodiments the log storage module is configured to store data sequentially in a log format within the media address space of the non volatile storage media. The log storage module may be further configured to groom the non volatile storage media as disclosed above.

Upon writing data to the non volatile storage media the storage media controller may be configured to update storage metadata e.g. a forward index to associate the logical interface of the data e.g. the LIDs of the data with the media address es of the data on the non volatile storage media . In some embodiments the storage metadata may be maintained on the storage media controller for example the storage metadata may be stored on the non volatile storage media on a volatile memory not shown or the like. Alternatively or in addition the storage metadata may be maintained within the storage layer e.g. on a volatile memory of the computing device of . In some embodiments the storage metadata may be maintained in a volatile memory by the storage layer and may be periodically stored on the non volatile storage media .

The storage media controller may further comprise a data read module that is configured to read contextual data from the non volatile storage media in response to requests received via the storage request receiver module . The requests may comprise a LID of the requested data a media address of the requested data and so on. The contextual read module may be configured to read data stored in a contextual format from the non volatile storage media and to provide the data to the storage layer and or a storage client . The contextual read module may be configured to determine the media address of the data using a logical interface of the data and the storage metadata . Alternatively or in addition the storage layer may determine the media address of the data and may include the media address in the request. The log storage module may provide the media address to the non volatile storage elements and the data may stream into the data read module via the read buffer . The read buffer may comprise one or more read synchronization buffers for clock domain synchronization as described above.

The storage media controller may further comprise a multiplexer that is configured to selectively route data and or commands to from the data write module and the data read module . In some embodiments storage media controller may be configured to read data while filling the write buffer and or may interleave one or more storage operations on one or more banks of non volatile storage elements not shown .

The storage media controller may manage the non volatile storage elements as a logical storage element . The logical storage element may be formed by coupling the non volatile storage elements in parallel using the bus . Accordingly storage operations may be performed on the non volatile storage elements concurrently and in parallel e.g. data may be written to and or read from the non volatile storage elements in parallel . The logical storage element may comprise a plurality of logical storage divisions e.g. logical erase blocks each comprising a respective storage division of the non volatile storage elements . The logical storage divisions may comprise a plurality of logical storage units e.g. logical pages each comprising a respective physical storage unit of the non volatile storage elements . The storage capacity of a logical storage unit may be a multiple of the number of parallel non volatile storage elements comprising the logical storage unit for example the capacity of a logical storage element comprised of 2 kb pages on 25 non volatile storage elements is 50 kb. In other embodiments comprising 25 non volatile storage elements having a 8 kb page size the logical page may have a storage capacity of 200 kb.

As disclosed herein the storage controller may be configured to store data within large constructs such as logical storage divisions and or logical storage units formed from plurality non volatile storage elements . The storage controller may therefore be capable of handling data storage operations of different sizes independent of the underlying physical partitioning and or arrangement of the non volatile storage elements . In some embodiments for example the storage layer may be configured to store data in 16 kb segments sectors within logical pages despite the fact that the page size of the underlying non volatile storage elements is only 2 kb.

Although depicts a particular embodiment of a logical storage element the disclosure is not limited in this regard and could be adapted to differently sized logical storage elements comprising any number of non volatile storage elements . The size and number of erase blocks pages planes or other logical and physical divisions within the non volatile storage elements are expected to change over time with advancements in technology it is to be expected that many embodiments consistent with new configurations are possible and are consistent with the embodiments disclosed herein.

As described above the contextual write module may be configured to store data in a contextual format. In some embodiments the contextual format comprises a packet format. depicts one example of a contextual data format packet format . A packet includes data e.g. a data segment that is associated with one or more LIDs. In some embodiments the data segment comprises compressed encrypted and or whitened data. The data segment may be a predetermined size e.g. a fixed data block or segment size or a variable size. The packet may comprise persistent metadata that is stored on the non volatile storage media with the data segment e.g. in a header of the packet format as depicted in . The persistent metadata may include logical interface metadata that defines the logical interface of the data segment . The logical interface metadata may associate the data segment with one or more LIDs LID references e.g. reference entries a range a size and so on. The logical interface metadata may be used to determine the context of the data independently of the storage metadata and or may be used to reconstruct the storage metadata e.g. reconstruct the any to any mappings described above . The persistent metadata may comprise other metadata which may include but are not limited to data attributes e.g. an access control list data segment delimiters signatures links metadata flags described below and the like.

In some embodiments the packet may be associated with log sequence indicator . The log sequence indicator may be persisted on the non volatile storage media e.g. page with the data packet and or on the storage division e.g. erase block of the data packet . Alternatively the sequence indicator may be persisted in a separate storage division. In some embodiments a sequence indicator is applied when a storage division reclaimed e.g. erased when the first or last storage unit is programmed etc. . The log sequence indicator may be used to determine an order of the packet in a sequence of storage operations performed on the non volatile storage media as described above.

Referring back to the contextual write module may be configured to generate data packets of any suitable size. Data packets may be of a fixed size or a variable size. Due to the independence between the logical interface of data and the underlying media storage location of the data the size of the packets generated by the contextual write module may be independent of the underlying structure and or partitioning of the non volatile storage media .

The data write module may further comprise an ECC write module which may be configured to encode the contextual data e.g. data packets into respective error correcting code ECC words or chunks. The ECC encoding may be configured to detect and or correct errors introduced through transmission and storage of data on the non volatile storage media . In some embodiments data packets stream to the ECC write module as un encoded blocks of length N ECC blocks . An ECC block may comprise a single packet multiple packets or a portion of one or more packets. The ECC write module may calculate a syndrome of length S for the ECC block which may be appended and streamed as an ECC chunk of length N S. The values of N and S may be selected according to testing and experience and may be based upon the characteristics of the non volatile storage media e.g. error rate of the media and or performance efficiency and robustness constraints. The relative size of N and S may determine the number of bit errors that can be detected and or corrected in an ECC chunk.

In some embodiments there is no fixed relationship between the ECC input blocks and the packets a packet may comprise more than one ECC block the ECC block may comprise more than one packet a first packet may end anywhere within the ECC block and a second packet may begin after the end of the first packet within the same ECC block. The ECC algorithm implemented by the ECC write module and or ECC read module may be dynamically modified and or may be selected according to a preference e.g. communicated via the bus in a firmware update a configuration setting or the like.

The ECC read module may be configured to decode ECC chunks read from the non volatile storage medium . Decoding an ECC chunk may comprise detecting and or correcting errors therein. The contextual read module may be configured to depacketize data packets read from the non volatile storage media . Depacketizing may comprise removing and or validating contextual metadata of the packet such as the logical interface metadata described above. In some embodiments the contextual read module may be configured to verify that the logical interface information in the packet matches a LID in the storage request.

In some embodiments the log storage module is configured to store contextual formatted data sequentially in a log format. As described above log storage refers to storing data in a format that defines an ordered sequence of storage operation which may comprise storing data at sequential media addresses within the media address space of the non volatile storage media e.g. sequentially within one logical storage units . Alternatively or in addition sequential storage may refer to storing data in association with a sequence indicator such as a sequence number timestamp or the like such as the sequence indicator described above.

The log storage module may store data sequentially at an append point. An append point may be located where data from the write buffer will next be written. Once data is written at an append point the append point moves to the end of the data. This process typically continues until a logical erase block is full. The append point is then advanced to the next available logical erase block . The sequence of writing to logical erase blocks is maintained e.g. using sequence indicators so that if the storage metadata is corrupted or lost the log sequence of storage operations data be replayed to rebuild the storage metadata e.g. rebuild the any to any mappings of the storage metadata .

The logical storage units may be assigned respective media addresses in the example the media addresses range from zero 0 to N. The log storage module may store data sequentially at the append point data may be stored sequentially within the logical page and when the logical page is full the append point advances to the next available logical page in the logical erase block where the sequential storage continues. Each logical erase block A N may comprise a respective sequence indicator. Accordingly the sequential storage operations may be determined based upon the sequence indicators of the logical erase blocks A N and the sequential order of data within each logical erase block A N.

As used herein an available logical page refers to a logical page that has been initialized e.g. erased and has not yet been programmed. Some non volatile storage media can only be reliably programmed once after erasure. Accordingly an available logical erase block may refer to a logical erase block that is in an initialized or erased state. The logical erase blocks A N may be reclaimed by a groomer or other process which may comprise erasing the logical erase block A N and moving valid data thereon if any to other storage locations. Reclaiming logical erase block A N may further comprise marking the logical erase block A N with a sequence indicator as described above.

The logical erase block B may be unavailable for storage due to inter alia not being in an erased state e.g. comprising valid data being out of service due to high error rates or the like and so on. In the example after storing data on the physical storage unit the append point may skip the unavailable logical erase block B and continue at the next available logical erase block C. The log storage module may store data sequentially starting at logical page and continuing through logical page at which point the append point continues at a next available logical erase block as described above.

After storing data on the last storage unit e.g. storage unit N of storage division N the append point wraps back to the first division A or the next available storage division if storage division A is unavailable . Accordingly the append point may treat the media address space as a loop or cycle.

As disclosed above the storage controller may be configured to modify and or overwrite data out of place. Accordingly a storage request to overwrite data A stored at physical storage location with data A may be stored out of place on a different location media address within the physical address space . Storing the data A may comprise updating the storage metadata to associate A with the new media address and or to invalidate the data A at media address . The groomer module may be configured to scan the physical address space to reclaim storage resources comprising invalidated data that no longer needs to be preserved on the storage device such as the obsolete version of data A at media address . The storage metadata may be reconstructed based on contextual log based storage format disclosed herein. In the embodiment the current version of data A may be distinguished from the obsolete data A based on log ordering information on the storage device . Accordingly the reconstructed index may identify the data A at media address as the current valid version of the data and determine that the data A at media address is obsolete and can be removed from the device.

Referring back to the storage controller may comprise a groomer module that is configured to reclaim logical erase blocks as described above. The groomer module may monitor the non volatile storage media and or storage metadata to identify logical erase blocks for reclamation. The groomer module may reclaim logical erase blocks in response to detecting one or more conditions which may include but are not limited to a lack of available storage capacity detecting a percentage of data marked as invalid within a particular logical erase block reaching a threshold a consolidation of valid data an error detection rate reaching a threshold improving data distribution data refresh or the like.

The groomer module may operate outside of the path for servicing storage operations and or requests. Therefore the groomer module may operate as an autonomous background process which may be suspended and or deferred while other storage operations are in process. The groomer may manage the non volatile storage media so that data is systematically spread throughout the logical erase blocks which may improve performance and data reliability and to avoid overuse and underuse of any particular storage locations thereby lengthening the useful life of the solid state storage media e.g. wear leveling etc. . Although the groomer module is depicted in the storage layer the disclosure is not limited in this regard. In some embodiments the groomer module may operate on the storage media controller may comprise a separate hardware component or the like.

In some embodiments the groomer may interleave grooming operations with other storage operations and or requests. For example reclaiming a logical erase block may comprise relocating valid data thereon to another storage location. The groomer read and groomer write bypass modules and may be configured to allow data packets to be read into the data read module and then be transferred directly to the data write module without being routed out of the storage media controller .

The groomer read bypass module may coordinate reading data to be relocated from a reclaimed logical erase block . The groomer module may be configured to interleave relocation data with other data being written to the non volatile storage media via the groomer write bypass . Accordingly data may be relocated without leaving the storage media controller . In some embodiments the groomer module may be configured to fill the remainder of a logical page or other data storage primitive with relocation data which may improve groomer efficiency while minimizing the performance impact of grooming operations.

The storage layer may further comprise a deduplication module which may be configured to identify duplicated data on the storage device . The deduplication module may be configured to identify duplicated data and to modify a logical interface of the data such that one or more LIDs reference the same set of data on the storage device as opposed to referencing separate copies of the data. The deduplication module may operate outside of the path for servicing storage operations and or requests as described above.

As described above the storage controller may maintain an index corresponding to the logical address space . depicts one example of such an index . The index may comprise a one or more entries A N. Each entry A may correspond to a LID or LID range or extent in the logical address space . The entries A N may represent LIDs that have been allocated for use by one or more storage clients . The index may comprise any to any mappings between LIDs and media storage locations on one or more storage devices . For example the entry B binds LIDs 072 083 to media storage locations . An entry D may represent a LID that has been allocated but has not yet been used to store data and as such the LIDs may not be bound to any particular media storage locations e.g. the LIDs 178 192 are unbound . As described above deferring the allocation of physical storage resources may allow the storage controller to more efficiently manage storage resources e.g. prevent premature reservation of physical storage resources so that the storage resources are available to other storage clients . One or more of the entries A N may comprise additional metadata which may include but is not limited to access control metadata e.g. identify the storage client s authorized to access the entry reference metadata logical interface metadata and so on. The index may be maintained by the storage layer e.g. translation module and may be embodied as storage metadata on a volatile memory and or a non transitory machine readable storage media and or .

The index may be configured to provide for fast and efficient entry lookup. The index may be implemented using one or more datastructures including but not limited to a B tree a content addressable memory CAM a binary tree a hash table or other datastructure that facilitates quickly searching a sparsely populated logical address space. The datastructure may be indexed by LID such that given a LID the entry A N corresponding to the LID if any can be identified in a computationally efficient manner.

In some embodiments the index comprises one or more entries not shown to represent unallocated LIDs e.g. LIDs that are available for allocation by one or more storage clients . The unallocated LIDs may be maintained in the index and or in a separate index as depicted in . In some embodiments the index may comprise one or more sub indexes such as a reference index. As described below the reference index may comprise data that is being referenced by one or more other entries A N in the index e.g. indirect references . Although particular examples and datastructures of storage metadata are described herein the disclosure is not limited in this regard the storage layer may be configured to incorporate any type of storage metadata embodied using any suitable datastructure.

The apparatus includes an allocation request module that receives from a requesting device an allocation request to allocate logical capacity. The requesting device may be storage client or any other device or component capable of sending an allocation request. The storage layer may comprise and or be communicatively coupled to one or more storage devices as depicted in . The logical capacity associated with the allocation request may refer to storing data on a particular storage device or on any of a plurality of storage devices A N.

The allocation request may include a logical allocation request or may include a request to store data. A logical allocation request may comprise a request to allocate LIDs to a storage client . A data storage request may comprise a request to store data corresponding to one or more LIDs that are allocated to the storage client which are then bound to media storage locations. As described above binding the LIDs may comprise associating the LIDs with media storage locations comprising the data in an index maintained in the storage metadata e.g. the index . The LIDs may be bound to media storage locations at the time of allocation e.g. the allocation request may comprise a request to store data . Alternatively where the allocation request is separate from a request to store data allocating LIDs to the data may be in a separate step from binding the LIDs to the media storage locations. In some embodiments the request comes from a plurality of storage clients consequently a client identifier may be associated with the request the apparatus may use the client identifier to implement an access control with respect to allocations for that storage client and or with respect to the LIDs available to allocate to the storage client . In addition the client identifier may be used to manage how much physical capacity is allocated to a particular storage client or set of storage clients .

The apparatus includes a logical capacity module that determines if a logical address space of the data storage device includes sufficient unallocated logical capacity to satisfy the allocation request. The logical capacity module may determine if the logical address space has sufficient unbound and or unallocated logical capacity using an index or other datastructure maintaining LID bindings and or LID allocations. In some embodiments the logical capacity module may search a logical to physical map or index maintained in the storage metadata and or an unallocated index described below.

As described above unbound LIDs may refer to LIDs that do not correspond to valid data stored on a media storage location. An unbound LID may be allocated to a client or may be unallocated. In some embodiments the logical to physical map is configured such that there are no other logical to logical mappings between the LIDs in the map and media addresses associated with the LIDs.

In some embodiments the logical capacity module searches the logical to physical index or other datastructure to identify unbound LIDs and identifies unallocated logical space therein. For example if a logical address space includes a range of logical addresses from 0000 to FFFF and the logical to physical map indicates that the logical addresses 0000 to F000 are allocated and bound the logical capacity module may determine that LIDs F001 to FFFF are not allocated. If the LIDs F001 to FFFF are not allocated to another storage client they may be available for allocation to satisfy the allocation request.

In some embodiments the translation module may maintain a plurality of different logical address spaces such as a separate logical address space each storage client . Accordingly each storage client may operate in its own separate logical storage space . The storage layer may therefore comprise separate storage metadata e.g. indexes capacity indicators and so on for each storage client or group of storage clients . Storage clients may be distinguished by an identifier which may include but is not limited to an address e.g. network address credential name context or other identifier. The identifiers may be provided in storage requests and or may be associated with a communication channel or protocol used by the storage client to access the storage layer .

In some embodiments the index or other datastructure may comprise an allocation index or allocation entries configured to track logical capacity allocations that have not yet been bound to media storage locations. For example a LID or other portion of logical capacity may be allocated to a client but may not be associated with data stored on a storage device . Accordingly although the logical capacity maybe allocated it may be unbound and as such may not be included in the logical to physical index. Accordingly when determining the unallocated logical address space the logical capacity module may consult additional datastructures e.g. allocation index allocation entries and or an unallocated index . Alternatively the allocation entry may be included in the logical to physical index e.g. entry D and may comprise an indicator showing that the entry is not bound to any particular media storage locations.

An allocation request may include a request for a certain number of LIDs. The logical capacity module may determine if the available logical capacity e.g. unbound and or unallocated logical capacity is sufficient to meet or exceed the requested amount of logical addresses. In another example if the allocation request specifies a list or range of LIDs to allocate the logical capacity module can determine if the LIDs for all or a portion of the LIDs requested are unallocated or unbound.

The apparatus may further comprise an allocation reply module that communicates a reply to the requesting device indicating whether the request can be satisfied. For example if the logical capacity module determines that the unallocated logical space is insufficient to satisfy the allocation request the allocation reply module may include in the reply that the allocation request failed and if the logical capacity module determines that the unallocated logical space is sufficient to satisfy the allocation request and or the specified LIDs are unallocated the allocation reply module may include in the reply an affirmative response. An affirmative response may comprise a list of allocated LIDs a range of LIDs or the like.

In some embodiments the allocation request is for a specific group of LIDs and the allocation reply module may reply with the requested LIDs. In another embodiment the allocation request is part of a write request. In one case the write request includes specific LIDs and the allocation reply module may reply with the requested LIDs. In another case the write request only includes data or an indication of an amount of data and the allocation reply module may reply by allocating LIDS sufficient for the write request and returning the allocated LIDS. Alternatively if an indication of an amount of data is provided the reply may include LIDs that are unallocated. The allocation reply module may reply before or after the data is written. If the allocation reply module sends a reply after the data is written the reply may be part of a confirmation of writing the data. One of skill in the art will recognize other ways that the allocation reply module may reply in response to the logical capacity module determining if the logical space of the data storage device has sufficient unallocated logical space to satisfy an allocation request.

The storage layer may expose portions of the logical address space maintained by the translation module e.g. index directly to storage clients via the virtual storage interface or other interface . The storage clients may use the virtual storage interface to perform various functions including but not limited to identifying available logical capacity e.g. particular LIDs or general LID ranges determining available physical capacity querying the health of the storage media identifying allocated LIDs identifying LIDs that are bound to media storage locations etc. The interface can expose all or a subset of the features and functionality of the apparatus directly to clients which may leverage the virtual storage interface to delegate management of the logical address space and or LIDs to the storage layer .

The apparatus includes in one embodiment a physical capacity request module a physical capacity allocation module and a physical capacity reply module . The physical capacity request module receives from a requesting device a physical capacity request. The physical capacity request is received at the data storage device and includes a request of an amount of available physical storage capacity in the data storage device and or physical storage capacity allocated to the requesting device . The physical capacity request may include a quantity of physical capacity or may indirectly request physical storage capacity for example by indicating a size of a data unit to be stored. Another indirect physical storage capacity request may include logical addresses of data to be stored which may correlate to a data size. One of skill in the art will recognize other forms of a physical capacity request.

The physical capacity allocation module determines the amount of available physical storage capacity on one or more storage devices and or A N. The amount of available physical storage capacity includes a physical storage capacity of unbound media storage locations. In some embodiments the amount of available physical storage capacity may be budgeted for example only a portion of the physical storage capacity of a storage device may be available to the requesting device. The amount of available physical storage capacity may be budgeted based on a quota associated with each storage client or group of storage clients . The apparatus may enforce these quotas. The allocation of available physical storage device may be determined by configuration parameter s may be dynamically adjusted according to performance and or quality of service policies or the like.

The physical capacity allocation module may determine the amount of available physical storage capacity using an index or other datastructure such as the index described above. Index may identify the media storage locations that comprise valid data e.g. entries A N that comprise bound media storage locations . The available storage capacity may be a total or budgeted physical capacity minus the capacity of the bound media storage locations. Alternatively or in addition an allocation index or other datastructure may maintain an indicator of the available physical storage capacity. The indicator may be updated responsive to storage operations performed on the storage device including but not limited to grooming operations deallocations e.g. TRIM writing additional data physical storage capacity reservations physical storage capacity reservation cancellations and so on. Accordingly the module may maintain a running total of available physical storage capacity that is available on request.

The physical capacity reply module that communicates a reply to the requesting device in response to the physical capacity allocation module determining the amount of available physical storage capacity on the data storage device.

The physical capacity allocation module in one embodiment tracks bound media storage locations unbound media storage locations reserved physical storage capacity unreserved physical storage capacity and the like. The physical capacity allocation module may track these parameters using a logical to physical map a validity map a free media address pool a used media address pool a physical to logical map or other means known to one of skill in the art.

The reply may take many forms. In one embodiment where the physical capacity request includes a request for available physical capacity the reply may include an amount of available physical storage capacity. In another embodiment where the physical capacity request includes a specific amount of physical capacity the reply may include an acknowledgement that the data storage device has the requested available physical storage capacity. One of skill in the art will recognize other forms of a reply in response to a physical capacity request.

The apparatus with a physical capacity request module a physical capacity allocation module and a physical capacity reply module is advantageous for storage devices where a logical to physical mapping is not a one to one mapping. In a typical random access device where read and write requests include one or more LBAs a file server storage client may track physical storage capacity of a storage device by tracking the LBAs that are bound to media storage locations.

For a log storage system where multiple media storage locations can be mapped to a single LID e.g. multiple versions of data mapped to a LID or vice versa e.g. multiple LIDs to a the same media storage locations tracking LIDs may not provide any indication of physical storage capacity. These many to one relationships may be used to support snap shots cloning e.g. logical copies deduplication and or backup. Examples of systems and methods for managing many to one LID to media storage location logical interfaces are disclosed in further detail below. The apparatus may track available physical storage space and may communicate the amount of available physical storage space to storage clients which may allow the storage clients to offload allocation management and physical capacity management to the storage layer .

In some embodiments media storage locations are bound to corresponding LIDs. When data is stored in response to a write request LIDs associated with the data are bound to the media storage location where the data is stored. For a log structured file system where data is stored sequentially the location where the data is stored is not apparent from the LID even if the LID is an LBA. Instead the data is stored at an append point and the address where the data is stored is mapped to the LID. If the data is a modification of data stored previously the LID may be mapped to the current data as well as to a location where the old data is stored. There may be several versions of the data mapped to the same LID.

The apparatus in one embodiment includes an allocation module that allocates the unallocated logical space sufficient to satisfy the allocation request of the requesting device. The allocation module may allocate the unallocated logical space in response to the logical capacity module determining that the logical space has sufficient unallocated logical space to satisfy the allocation request.

In one embodiment the allocation request is part of a pre allocation where logical space is not associated with a specific request to store data. For example a storage client may request using an allocation request logical space and then may proceed to store data over time to the allocated logical space. The allocation module allocates LIDs to the storage client in response to an allocation request and to the logical capacity module determining that the logical space has sufficient unallocated logical space to satisfy the allocation request.

The allocation module may also allocate LIDs based on an allocation request associated with a specific storage request. For example if a storage request includes specific LIDs and the logical capacity module determines that the LIDs are available the allocation module may allocate the LIDs in conjunction with storing the data of the storage request. In another example if the storage request does not include LIDs and the logical capacity module determines that there are sufficient LIDs to for the storage request the allocation module may select and allocate LIDs for the data and the allocation reply module may communicate the allocated LIDs.

The allocation module may be configured to locate unallocated LIDs to satisfy an allocation request. In some embodiments the allocation module may identify unallocated LIDs by receiving a list of requested LIDs to allocate from the storage client and verify that these LIDs are available for allocation. In another example the allocation module may identify unallocated LIDs by searching for unallocated LIDs that meet criteria received in conjunction with the request. The criteria may be LIDs that are associated with a particular storage device A N that are available in a RAID that have some assigned metadata characteristic etc.

In another example the allocation module may identify unallocated LIDs by creating a subset of LIDs that meet criteria received in conjunction with the request identified in a pool of available LIDs. In one instance the LIDs may be a subset of LIDs that have already been allocated to the client . For example if a set or group of LIDs is allocated to a particular user group employer etc. a subset of the LIDs may be allocated. A specific example is if a set of LIDs is allocated to an organization and then a subset of the allocated LIDs is further allocated to a particular user in the organization. One of skill in the art will recognize other ways that the allocation module can identify one or more unallocated LIDs.

The allocation module in one embodiment can expand the LIDs allocated to a storage client by allocating LIDs in addition to LIDs already allocated to the storage client . In addition LIDs allocated to a storage client may be decreased by deallocating certain LIDs so that they return to a pool of unallocated LIDs. In other embodiments subsets of allocated LIDs may be allocated deallocated increased decreased etc. For example LIDs allocated to a user in an organization may be deallocated so that the LIDs allocated to the user are still allocated to the organization but not to the user.

The apparatus in one embodiment includes an allocation query request module an allocation query determination module and an allocation query reply module . The allocation query request module receives an allocation query from some requesting device such as a storage client etc. An allocation query may include a request for information about allocating logical space or associated management of the allocated logical space. For example an allocation query may be a request to identify allocated LIDs identify bound LIDs identify allocated LIDs that are not bound to media storage locations unallocated LIDs or a range of LIDs and the like.

The allocation query may include information about logical allocation logical capacity physical capacity or other information meeting criteria in the allocation query. The information may include metadata status logical associations historical usage flags control etc. One of skill in the art will recognize other allocation queries and the type of information returned in response to the allocation query.

The allocation query includes some type of criteria that allows the allocation query determination module to service the allocation request. The allocation query determination module in one embodiment identifies one or more LIDs that meet the criteria specified in the allocation query. The identified LIDs include allocated LIDs that are bound to media storage locations allocated LIDs that are unbound unallocated LIDs and the like.

The allocation query reply module communicates to the client the results of the query to the requesting device or to another device as directed in the allocation query. The results of the allocation query may include a list of the identified LIDs an acknowledgement that LIDs meeting the criteria were found an acknowledgement that LIDs meeting the criteria in the allocation query were not found bound unbound status of LIDs logical storage capacity or the like. Typically the allocation query reply module returns status information and the information returned may include any information related to managing and allocating LIDs known to those of skill in the art.

The apparatus in another embodiment includes a logical space management module that manages the logical space of the data storage device from within the data storage device. For example the logical space management module may manage the logical space from a storage layer or driver associated with a storage device of the data storage device. The logical space management module may track unbound LIDs and bound LIDs for example in the logical to physical map in an index or in another datastructure. As described above a bound LID refers to a LID corresponding to data a bound LID is a LID associated with valid data stored on a media storage location of the storage device .

The logical space management module in various embodiments may service allocation requests and allocation queries as described above and other functions related to allocation. The logical space management module can also include receiving a deallocation request from a requesting device. The deallocation request typically includes a request to return one or more allocated LIDs to an unallocated state and then communicating to the requesting device or other designated device the successful deallocation. The deallocation request may include a request to return one or more storage locations associated with the LIDs allocated and then communicating to the requesting device or other designated device the successful deallocation. This might be transparent or might require that the deallocation request be extended to include an indication that a logical physical deallocation should accompany the request. Deallocation requests may be asynchronous and tied to the groomer. Thus the deallocation request may be virtual in time until completed. The management of the allocations logical and physical may diverge from the actual available space at any point in time. The management module is configured to deal with these differences.

The logical space management module may also receive a LID group command request from a requesting device and may communicate to the requesting device a reply indicating a response to the LID group command request. The LID group command request may include an action to take on for example two or more LIDs LID group metadata associated with the LID group the data associated with the LID group and the like. For example if several users are each allocated LIDs and the users are part of a group a LID group command may be to deallocate the LIDs for several of the users allocate additional LIDs to each user return usage information for each user etc. The action taken in response to the LID group command may also include modifying the metadata backing up the data backing up the metadata changing control parameters changing access parameters deleting data copying the data encrypting the data deduplicating the data compressing the data decompressing the data etc. One of skill in the art will recognize other logical space management functions that the logical space management module may also perform.

The apparatus in one embodiment includes a mapping module that binds in a logical to physical map e.g. the index bound LIDs to media storage locations. The logical capacity module determines if the logical space has sufficient unallocated logical space using the logical to physical map mapped by the mapping module . The index may be used to track allocation of the bound LIDs the unbound LIDs the allocated LIDs the unallocated LIDs the allocated LID capacity the unallocated LID capacity and the like. In one embodiment the mapping module binds LIDs to corresponding media addresses in multiple indexes and or maps.

In addition a reverse map may be used to quickly access information related to a media address and to link to a LID associated with the media address. The reverse map may be used to identify a LID from a media address. A reverse map may be used to map addresses in a data storage device into erase regions such as erase blocks such that a portion of the reverse map spans an erase region of the storage device erased together during a storage space recovery operation. Organizing a reverse map by erase regions facilitates tracking information useful during grooming operations. For example the reverse map may include which media addresses in an erase region have valid data and which have invalid data. When valid data is copied from an erase region and the erase region erased the reverse map can easily be changed to indicate that the erase region does not include data and is ready for sequential storage of data.

A more detailed discussion of forward and reverse mapping is included in U.S. patent application Ser. No. 12 098 434 titled Apparatus System and Method for Efficient Mapping of Virtual and Media addresses Non Volatile Storage to David Flynn et al. filed Apr. 8 2008 which is incorporated herein by reference. By including any to any mappings between LIDs and media addresses the storage layer efficiently consolidates functions such as thin provisioning allocation functions etc. that have traditionally been handled by other entities. The mapping module may therefore provide an efficient way to eliminate layers of mapping used in traditional systems.

In a thinly provisioned storage system one potential problem is that a storage client may attempt to write data to a storage device only to have the write request fail because the storage device is out of available physical storage capacity. For random access devices where the file server file system tracks available physical storage capacity relying on the one to one mapping of LBAs to PBAs the likelihood of a storage device running out of storage space is very low. The apparatus includes a physical space reservation request module located in the storage layer that receives a request from a storage client to reserve available physical storage capacity on the data storage device i.e. the storage device that is part of the data storage device hereinafter a physical space reservation request . In one embodiment the physical space reservation request includes an indication of an amount of physical storage capacity requested by the storage client .

The indication of an amount of physical storage capacity requested may be expressed in terms of physical capacity. The request to reserve physical storage capacity may also include a request to allocate the reserved physical storage capacity to a logical entity. The indication of an amount of physical storage capacity may be expressed indirectly as well. For example a storage client may indicate a number of logical blocks and the data storage device may determine a particular fixed size for each logical block and then translate the number of logical blocks to a physical storage capacity. One of skill in the art will recognize other indicators of an amount of physical storage capacity in a physical space reservation request.

The physical space reservation request in one embodiment is associated with a write request. In one embodiment the write request is a two step process and the physical space reservation request and the write request are separate. In another embodiment the physical space reservation request is part of the write request or the write request is recognized as having an implicit physical space reservation request. In another embodiment the physical space reservation request is not associated with a specific write request but may instead be associated with planned storage reserving storage space for a critical operation etc. where mere allocation of storage space is insufficient.

In certain embodiments the data may be organized into atomic data units. For example the atomic data unit may be a packet a page a logical page a logical packet a block a logical block a set of data associated with one or more logical block addresses the logical block addresses may be contiguous or noncontiguous a file a document or other grouping of related data.

In one embodiment an atomic data unit is associated with a plurality of noncontiguous and or out of order logical block addresses or other identifiers that the data write module handles as a single atomic data unit. As used herein writing noncontiguous and or out of order logical blocks in a single write operation is referred to as an atomic write. In one embodiment a hardware controller processes operations in the order received and a software driver of the client sends the operations to the hardware controller for a single atomic write together so that the data write module can process the atomic write operation as normal. Because the hardware processes operations in order this guarantees that the different logical block addresses or other identifiers for a given atomic write travel through the data write module together to the nonvolatile memory. The client in one embodiment can back out reprocess or otherwise handle failed atomic writes and or other failed or terminated operations upon recovery once power has been restored.

In one embodiment apparatus may mark blocks of an atomic write with a metadata flag indicating whether a particular block is part of an atomic write. One example of metadata marking is to rely on the log write append only protocol of the nonvolatile memory together with a metadata flag or the like. The use of an append only log for storing data and prevention of any interleaving blocks enables the atomic write membership metadata to be a single bit. In one embodiment the flag bit may be a 0 unless the block is a member of an atomic write and then the bit may be a 1 or vice versa. If the block is a member of an atomic write and is the last block of the atomic write in one embodiment the metadata flag may be a 0 to indicate that the block is the last block of the atomic write. In another embodiment different hardware commands may be sent to mark different headers for an atomic write such as the first block in an atomic write middle member blocks of an atomic write tail of an atomic write or the like.

On recovery from a power loss or other failure of the client or of the storage device in one embodiment the apparatus scans the log on the nonvolatile storage in a deterministic direction for example in one embodiment the start of the log is the tail and the end of the log is the head and data is always added at the head . In one embodiment the power management apparatus scans from the head of the log toward the tail of the log. For atomic write recovery in one embodiment when scanning head to tail if the metadata flag bit is a 0 then the block is either a single block atomic write or a non atomic write block. In one embodiment once the metadata flag bit changes from 0 to 1 the previous block scanned and potentially the current block scanned are members of an atomic write. The power management apparatus in one embodiment continues scanning the log until the metadata flag changes back to a 0 at that point in the log the previous block scanned is the last member of the atomic write and the first block stored for the atomic write.

In one embodiment the nonvolatile memory uses a sequential append only write structured writing system where new writes are appended on the front of the log i.e. at the head of the log . In a further embodiment the storage controller reclaims deleted stale and or invalid blocks of the log using a garbage collection system a groomer a cleaner agent or the like. The storage controller in a further embodiment uses a forward map to map logical block addresses to media addresses to facilitate use of the append only write structure and garbage collection.

The apparatus in one embodiment includes a physical space reservation module that determines if the data storage device i.e. storage device has an amount of available physical storage capacity to satisfy the physical storage space request. If the physical space reservation module determines that the amount of available physical storage capacity is adequate to satisfy the physical space reservation request the physical space reservation module reserves an amount of available physical storage capacity on the storage device to satisfy the physical storage space request. The amount of available physical storage capacity reserved to satisfy the physical storage space request is the reserved physical capacity.

The amount of reserved physical capacity may or may not be equal to the amount of storage space requested in the physical space reservation request. For example the storage layer may need to store additional information with data written to a storage device such as metadata index information error correcting code etc. In addition the storage layer may encrypt and or compress data which may affect storage size.

In one embodiment the physical space reservation request includes an amount of logical space and the indication of an amount of physical storage capacity requested is derived from the requested logical space. In another embodiment the physical space reservation request includes one or more LIDs and the indication of an amount of physical storage capacity requested is derived from an amount of data associated with the LIDs. In one example the data associated with the LIDs is data that has been bound to the LIDs such as in a write request. In another example the data associated with the LIDs is a data capacity allocated to each LID such as would be the case if a LID is an LBA and a logical block size could be used to derive the amount of requested physical storage capacity.

In another embodiment the physical space reservation request is a request to store data. In this embodiment the physical space reservation request may be implied and the indication of an amount of physical storage capacity requested may be derived from the data and or metadata associated with the data. In another embodiment the physical space reservation request is associated with a request to store data. In this embodiment the indication of an amount of physical storage capacity requested is indicated in the physical space reservation request and may be correlated to the data of the request to store data.

The physical space reservation module may also then factor metadata compression encryption etc. to determine an amount of required physical capacity to satisfy the physical space reservation request. The amount of physical capacity required to satisfy the physical space reservation request may be equal to larger than or smaller than an amount indicated in the physical space reservation request.

Once the physical space reservation module determines an amount of physical capacity required to satisfy the physical space reservation request the physical space reservation module determines if one or more storage devices A N either individually or combined have enough available physical storage capacity to satisfy the physical space reservation request. The request may be for space on a particular storage device e.g. A a combination of storage devices A N such as would be the case if some of the storage devices A N are in a RAID configuration or for available space generally. The physical space reservation module may tailor a determination of available capacity to specifics of the physical space reservation request.

Where the physical space reservation request is for space on more than one storage device the physical space reservation module will typically retrieve available physical storage capacity information from each logical to physical map of each storage device or a combined logical to physical map of a group of storage devices A N. The physical space reservation module typically surveys bound media addresses. Note that the physical space reservation module may not have enough information to determine available physical capacity by looking at bound LIDs because there is typically not a one to one relationship between LIDs and media storage locations.

The physical space reservation module reserves physical storage capacity in one embodiment by maintaining enough available storage capacity to satisfy the amount of requested capacity in the physical space reservation request. Typically in a log structured file system or other sequential storage device the physical space reservation module would not reserve a specific media region or media address range in the storage device but would instead reserve physical storage capacity.

For example a storage device may have 500 gigabytes GB of available physical storage capacity. The storage device may be receiving data and storing the data at one or more append points thus reducing the storage capacity. Meanwhile a garbage collection or storage space recovery operation may be running in the background that would return recovered erase blocks to storage pool thus increasing storage space. The locations where data is stored and freed are constantly changing so the physical space reservation module in one embodiment monitors storage capacity without reserving fixed media storage locations.

The physical space reservation module may reserve storage space in a number of ways. For example the physical space reservation module may halt storage of new data if the available physical storage capacity on the storage device decreased to the reserved storage capacity may send an alert if the physical storage capacity on the storage device was reduced to some level above the reserved physical storage capacity or some other action or combination of actions that would preserve an available storage capacity above the reserved physical storage capacity.

In another embodiment the physical space reservation module reserves a media region range of media addresses etc. on the data storage device. For example if the physical space reservation module reserved a certain quantity of erase blocks data associated with the physical space reservation request may be stored in the reserved region or address range. The data may be stored sequentially in the reserved storage region or range. For example it may be desirable to store certain data at a particular location. One of skill in the art will recognize reasons to reserve a particular region address range etc. in response to a physical space reservation request.

In one embodiment the apparatus includes a physical space reservation return module that transmits to the storage client an indication of availability or unavailability of the requested amount of physical storage capacity in response to the physical space reservation module determining if the data storage device has an amount of available physical storage space that satisfies the physical space reservation request. For example if the physical space reservation module determines that the available storage space is adequate to satisfy the physical space reservation request the physical space reservation return module may transmit a notice that the physical space reservation module has reserved the requested storage capacity or other appropriate notice.

If on the other hand the physical space reservation module determines that the storage device does not have enough available physical storage capacity to satisfy the physical space reservation request the physical space reservation return module may transmit a failure notification or other indicator that the requested physical storage space was not reserved. The indication of availability or unavailability of the requested storage space for example may be used prior to writing data to reduce a likelihood of failure of a write operation.

The apparatus in another embodiment includes a physical space reservation cancellation module that cancels all or a portion of reserved physical storage space in response to a cancellation triggering event. The cancellation triggering event may come in many different forms. For example the cancellation triggering event may include determining that data to be written to the storage device and associated with available space reserved by the physical space reservation module has been previously stored by the storage layer .

For example if a deduplication process deduplication module determines that the data has already been stored the data may not need to be stored again since the previously stored data could be mapped to two or more LIDs. In a more basic example if reserved physical storage space is associated with a write request and the write request is executed the cancellation triggering event could be completion of storing data of the write request. In this example the physical space reservation cancellation module may reduce or cancel the reserved physical storage capacity.

If the data written is less than the reserved space the physical space reservation cancellation module may merely reduce the reserved amount or may completely cancel the reserved physical storage capacity associated with the write request. Writing to less than the reserved physical space may be due to writing a portion of a data unit where the data unit is the basis of the request where data associated with a physical space reservation request is written incrementally etc. In one embodiment physical storage space is reserved by the physical storage space reservation module to match a request and then due to compression or similar procedure the storage space of the data stored is less than the associated reserved physical storage capacity.

In another embodiment the cancellation triggering event is a timeout. For example if a physical space reservation request is associated with a write request and the physical space reservation module reserves physical storage capacity if the data associated with the write request is not written before the expiration of a certain amount of time the physical space reservation cancellation module may cancel the reservation of physical storage space. One of skill in the art will recognize other reasons to cancel all or a portion of reserved physical capacity.

The physical space reservation module in one embodiment may increase or otherwise change the amount of reserved physical storage capacity. For example the physical space reservation request module may receive another physical space reservation request which may or may not be associated with another physical space reservation request. Where the physical space reservation request is associated with previously reserved physical storage capacity the physical space reservation module may increase the reserved physical storage capacity. Where the physical space reservation request is not associated with previously reserved physical storage capacity the physical space reservation module may separately reserve physical storage capacity and track the additional storage capacity separately. One of skill in the art will recognize other ways to request and reserve available physical storage capacity and to change or cancel reserved capacity. Standard management should include some kind of thresholds triggers alarms and the like for managing the physical storage capacity providing indicators to the user that action needs to be taken. Typically this would be done in the management system. But either the management system would have to pool the devices under management or said devices would have to be configured programmed to interrupt the manger when a criteria was met preferred .

The apparatus in another embodiment includes a LID binding module that in response to a request from a storage client to write data binds one or more unbound LIDs to media storage locations comprising the data and transmits the LIDs to the storage client . The LID assignment module in one embodiment allows on the fly allocation and binding of LIDs. The request to write data in another embodiment may be a two step process. The LID binding module may allocate LIDs in a first step for data to be written and then in a second step the data may be written along with the allocated LIDs.

In one embodiment the LID allocation module allocates LIDs in a contiguous range. The LID binding module may also allocate LIDs in a consecutive range. Where a logical space is large the LID allocation module may not need to fragment allocated LIDs but may be able to choose a range of LIDs that are consecutive. In another embodiment the LID allocation module binds LIDs that may not be contiguous and may use logical spaces that are interspersed with other allocated logical spaces.

The apparatus in another embodiment includes a DMA module that pulls data from a client in a direct memory access DMA and or a remote DMA RDMA operation. The data is first identified in a request to store data such as a write request and then the storage layer executes a DMA and or RDMA to pull data from the storage client to a storage device . In another embodiment the write request does not use a DMA or RDMA but instead the write request includes the data. Again the media storage locations of the data are bound to the corresponding LIDs.

In one embodiment the apparatus includes a deletion module . In response to a request to delete data from the data storage device in one embodiment the deletion module removes the mapping between storage space where the deleted data was stored and the corresponding LID. The deletion module may also unbind the one or more media storage locations of the deleted data and also may deallocate the one or more logical addresses associated with the deleted data.

Step may comprise receiving an allocation request from a storage client . The allocation request may be received through the interface of the storage layer . The logical capacity module determines if a logical address space includes sufficient unallocated logical capacity to satisfy the allocation request where the determination includes a search of a logical to physical map e.g. index or other datastructure . The logical to physical map includes bindings between LIDs of the logical space and corresponding media storage locations comprising data of the bound LIDs wherein a bound LID differs from the one or more media storage locations addresses bound to the LID. The allocation reply module communicates a reply to the requesting device and the method ends.

The physical capacity allocation module determines the amount of available physical storage capacity on the data storage device where the amount of available physical storage capacity includes a physical storage capacity of unbound storage locations in the data storage device. The physical capacity reply module communicates a reply to the requesting device in response to the physical capacity allocation module determines the amount of available physical storage capacity on the data storage device and the method ends.

The physical space reservation module determines if the data storage device has available physical storage capacity to satisfy the physical storage space request. If the physical space reservation module determines that the data storage device has available physical storage capacity to satisfy the physical storage space request the physical space reservation module reserves physical storage capacity adequate to service the physical space reservation request and the physical space reservation return module transmits to the requesting storage client an indication that the requested physical storage space is reserved.

The physical allocation module maintains enough available physical storage capacity to maintain the reservation of physical storage capacity until the reservation is used by storing data associated with the reservation or until the reservation is cancelled and the method ends. If the physical space reservation module determines that the data storage device does not have available physical storage capacity to satisfy the physical storage space request the physical space reservation return module transmits to the requesting storage client an indication that the requested physical storage space is not reserved or an indication of insufficient capacity and the method ends.

Step may comprise allocating LIDs to the storage client to service the write request if necessary as disclosed above. Step may further comprise identifying LIDs allocated to the storage client for use in referencing the data of the write request. Step may comprising indicating that the identified LIDs are allocated by the storage client and are currently being used to reference valid data on a storage device. Step may further comprise allocating and or reserving physical storage capacity for the write request by use of the physical capacity allocation module as disclosed above.

Step may comprise servicing the write request by inter alia storing data of the write request onto one or more storage device s . The data may be stored in a contextual log based format as disclosed herein. The data may be stored at one or more physical storage locations which may be referenced by respective media addresses. Step may comprise binding the LIDs identified at step to the media addresses of step . Step may therefore comprise the mapping module binding the media addresses to the LIDs identified at step e.g. binding the LIDs to the media addresses in one or more entries A N of an index . In some embodiments the media addresses may be determined concurrently with or after the data is stored at step .

In some embodiments step further comprises providing an indication of the LIDs used to satisfy the write request the LIDs identified at step to the storage client . The LIDs may be communicated in an acknowledgement message a return value a callback or other suitable mechanism.

The storage layer receives a write request to write data to a storage device where the data is already associated with bound LIDs. In other embodiments the write request is to store the data on more than one storage device in the storage system such as would be the case if the storage devices are RAIDed or if the data is written to a primary storage device and to a mirror storage device . The storage controller stores the data on the storage device and the mapping module maps one or more media storage locations where the data is stored to the bound LIDs e.g. updates the binding between the LIDs and media storage locations in the index . Step may further comprise communicating an indication that the request of step was successfully completed.

The storage entries may further comprise and or reference metadata which may comprise metadata pertaining to the LIDs such as age size LID attributes e.g. client identifier data identifier file name group identifier and so on. Since the metadata is associated with the storage entries which are indexed by LID e.g. address the metadata may remain associated with the storage entry regardless of changes to the location of the underlying storage locations on the non volatile storage device e.g. changes to the storage locations .

The index may be used to efficiently determine whether the non volatile storage device comprises a storage entry referenced in a client request and or to identify a storage location of data on the device . For example the non volatile storage device may receive a request to allocate a particular LID. The request may specify a particular LID a LID and a length or offset e.g. request 3 units of data starting from LID 074 a set of LIDs or the like. Alternatively or in addition the client request may comprise a set of LIDs LID ranges continuous or discontinuous or the like.

The non volatile storage device may determine whether a storage entry corresponding to the requested LIDs is in the index using a search operation. If a storage entry comprising the requested LIDs is found in the index the LID s associated with the request may be identified as being allocated and bound. Accordingly data corresponding to the LID s may be stored on the non volatile storage device . If the LID s are not found in the index the LID s may be identified as unbound but may be allocated . Since the storage entries may represent sets of LIDS and or LID ranges a client request may result in partial allocation. For example a request to allocate 068 073 may successfully allocate LIDs 068 to 071 but may fail to allocate 072 and 073 since these are included in the storage entry . In the event of a partial allocation the entire allocation request may fail the available LIDs may be allocated and other LIDs may be substituted for the failed LIDs or the like.

In the example depicted in the storage entry corresponding to the storage request is in the index storage entry and as such the LIDs associated with the request are identified as allocated and bound. Therefore if the client request is to read data at the specified LIDs data may be read from the storage locations identified in the storage entry and returned to the originator of the request. If the request is to allocate the identified LIDs the allocation request may fail and or substitute LIDs may be allocated as described above .

When new storage entries are added to the index a merge operation may occur. In a merge operation an existing storage entry may be merged with one or more other storage entries. For instance a new storage entry for LIDs 084 088 may be merged with entry . The merge may comprise modifying the LID 1215 of the storage entry to include the new addresses e.g. and or to reference the storage locations to include the storage location on which the data was stored.

Although the storage entries in the index are shown as comprising references to storage locations e.g. addresses the disclosure is not limited in this regard. In other embodiments the storage entries comprise reference or indirect links to the storage locations. For example the storage entries may include a storage location identifier or reference to the reverse map .

As discussed above the reverse map may comprise metadata which may include metadata pertaining to sequential storage operations performed on the storage locations such as sequence indicators e.g. timestamp to indicate an ordered sequence of storage operations performed on the storage device e.g. as well as an age of the storage locations and so on . The metadata may further include metadata pertaining to the storage media such as wear level reliability error rate disturb status and so on. The metadata may be used to identify unreliable and or unusable storage locations which may reduce the physical storage capacity of the non volatile storage device .

The reverse map may be organized according to storage divisions e.g. erase blocks of the non volatile storage device . In this example the entry that corresponds to storage entry is located in erase block n . Erase block n is preceded by erase block n 1 and followed by erase block n 1 the contents of erase blocks n 1 and n 1 are not shown . An erase block may comprise a predetermined number of storage locations. An erase block may refer to an area in the non volatile storage device that is erased together in a storage recovery operation.

The validity indicator may be used to selectively invalidate data. Data marked as invalid in the reverse index may correspond to obsolete versions of data e.g. data that has been overwritten and or modified in a subsequent storage operation . Similarly data that does not have a corresponding entry in the index may be marked as invalid e.g. data that is no longer being referenced by a storage client . Therefore as used herein invalidating data may comprise marking the data as invalid in the storage metadata which may include removing a reference to the media storage location in the index and or marking a validity indicator of the data in the reverse map.

In some embodiments the groomer module described above uses the validity indicators to identify storage divisions e.g. erase blocks for recovery. When recovering or reclaiming an erase block the erase block may be erased and valid data thereon if any may be relocated to new storage locations on the non volatile storage media. The groomer module may identify the data to relocate using the validity indicator s . Data that is invalid may not be relocated may be deleted whereas data that is still valid e.g. still being referenced within the index may be relocated. After the relocation the groomer module or other process may update the index to reference the new media storage location s of the valid data. Accordingly marking data as invalid in the storage metadata may cause data to be removed from the non volatile storage media . The removal of the data however may not occur immediately when the data is marked invalid but may occur in response to a grooming operation or other processes that is outside of the path for servicing storage operations and or requests. Moreover when relocating data the groomer module may be configured to determine whether the contextual format of the data should be updated by referencing the storage metadata e.g. the reverse map and or index .

The validity metadata may be used to determine an available physical storage capacity of the non volatile storage device e.g. a difference between physical capacity or budgeted capacity and the storage locations comprising valid data . The reverse map may be arranged by storage division e.g. erase blocks or erase region to enable efficient traversal of the physical storage space e.g. to perform grooming operations determine physical storage capacity and so on . Accordingly in some embodiments the available physical capacity may be determined by traversing the storage locations and or erase blocks in the reverse map to identify the available physical storage capacity and or is being used to store valid data .

Alternatively or in addition the reverse map or other datastructure may comprise an indicator to track the available physical capacity of the non volatile storage device . The available physical capacity indicator may be initialized to the physical storage capacity or budgeted capacity of the non volatile storage device and may be updated as storage operations are performed. The storage operations resulting in an update to the available physical storage capacity indicator may include but are not limited to storing data on the storage device reserving physical capacity on the storage device canceling a physical capacity reservation storing data associated with a reservation where the size of the stored data differs from the reservation detecting unreliable and or unusable storage locations and or storage division e.g. taking storage locations out of service and so on.

In some embodiments the metadata and or may be configured to reflect reservations of physical storage capacity. As described above in conjunction with a storage client may reserve physical storage capacity for an operation that is to take place over time. Without a reservation the storage client may begin the operation but other clients may exhaust the physical capacity before the operation is complete. In some embodiments the storage client issues a request to reserve physical capacity before beginning the storage operation. The storage layer may update storage metadata e.g. the indexes and or disclosed herein to indicate that the requested portion has been reserved. The reserved portion may not be associated with any particular media storage locations rather the reservation may indicate that the storage layer is to maintain at least enough physical storage capacity to satisfy the reservation. For example the indicator of remaining physical storage capacity may be reduced by the amount of reserved physical storage capacity. Requests subsequent to the reservation may be denied if satisfying the requests would exhaust the remaining physical storage capacity in the updated indicator . In some embodiments a reservation of physical storage capacity may be valid for a pre determined time until released by the storage client until another higher priority request is received or the like. The reservation may expire once the storage client that reserved the physical capacity consumes the reserved physical storage capacity in one or more subsequent storage operations. If the storage operations occur over a series of storage operations as opposed to a single operation the reservation may be incrementally reduced accordingly.

The index may be used to determine an available logical capacity of the logical address space e.g. by traversing the index . The available logical capacity may consider LIDs that are bound using the storage entries as well as LIDs that are allocated but not yet bound using the allocation entries such as .

As shown in in some embodiments the allocation entries may be maintained in the index with the storage entries. Alternatively allocation entries may be maintained in a separate index or other datastructure . When an allocation entry becomes associated with data on the non volatile storage device e.g. as associated with storage locations the allocation entry may be modified and or replaced by a storage entry.

In some embodiments the index or index may comprise an indicator to track the available logical capacity of the logical address space . The available logical capacity may be initialized according to the logical address space presented by the storage device . Changes to the index may cause the available logical capacity indicator to be updated. The changes may include but are not limited to addition of new allocation entries removal of allocation entries addition of storage entries removal of allocation entries or the like.

At step a non volatile storage device may be initialized for use. The initialization may comprise allocating resources for the non volatile storage device e.g. solid state storage device such as communications interfaces e.g. bus network and so on allocating volatile memory accessing solid state storage media and so on. The initialization may further comprise presenting a logical address space to storage clients initializing one or more indexes e.g. the indexes described above in conjunction with and so on.

At step the non volatile storage device may present a logical space to one or more clients. Step may comprise implementing and or providing an interface e.g. API accessible to one or more clients or the like.

At step the non volatile storage device may maintain metadata pertaining to logical allocation operations performed by the method . The logical allocation operations may pertain to operations in the logical address space presented at step and may include but are not limited to allocating logical capacity binding logical capacity to media storage locations and so on. The metadata may include but is not limited to indexes associating LIDs in the logical address space with media storage locations on the non volatile storage device indexes associating storage locations with LIDs e.g. index of allocation entries indicating allocated LIDs having no associated storage location e.g. index of an unallocated index e.g. index of maintaining an indicator of unallocated logical capacity e.g. indicator of and so on.

At step a client request pertaining to a LID in the logical address space may be received. The client request may comprise a query to determine if a particular LID and or logical capacity can be allocated a request to allocate a LID and or logical capacity a request to store data on the non volatile storage device or the like.

At step the metadata maintained at step may be referenced to determine whether the client request can be satisfied. Step may comprise referencing the metadata e.g. indexes and or indicators maintained at step to determine an available logical capacity of the logical address space and or to identify available LIDs or LID range as described above.

At step the method may provide a response to the client request which if the request cannot be satisfied may comprise providing a response to indicate such. Providing the response may comprise one or more of an indicator that the allocation can be satisfied allocating LIDs satisfying the request providing allocated LIDs satisfying the request providing one or more requested LIDs and or one or more additional LIDs e.g. if a portion of a requested set of LIDs can be allocated or the like.

Following step the flow may return to step where the method may update the metadata e.g. indexes indicators and so on according to the allocation operation if any performed at step .

At steps and the method may be initialized present a logical storage space to one or more clients and or maintain metadata pertaining to logical operations performed by the method .

At step the method may maintain metadata pertaining to physical storage operations performed by the method . The storage operations may include but are not limited to reserving physical storage capacity canceling physical storage capacity reservations storing data on the non volatile storage device deallocating physical storage capacity grooming operations e.g. garbage collection error handling and so on physical storage space budgeting and so on. As discussed above metadata maintained at step may include but is not limited to indexes associating LIDs in the logical address space with storage locations on the non volatile storage device indexes associating storage locations with LIDs e.g. index of allocation entries indicating allocated LIDs having no associated storage location e.g. index of an unallocated index e.g. index of maintaining an indicator of unallocated logical address space e.g. indicator of and so on.

At step a client request pertaining to physical storage capacity of the non volatile storage device may be received. The client request may comprise a query to determine if physical storage capacity is available a request to reserve physical storage capacity a request to store data a request to deallocate data e.g. TRIM or the like.

At step the metadata maintained at steps and or may be referenced to determine whether the client request can be satisfied. Step may comprise referencing the metadata at steps and or to determine an available physical storage capacity of the non volatile storage device and or to identify storage locations associated with particular LIDs e.g. in a deallocation request or TRIM as described above.

At step the method may provide a response to the client request which if the request cannot be satisfied may comprise providing a response to indicate such. Providing the response may comprise one or more of indicating that the client request can and or was satisfied reserving physical storage capacity for the client cancelling a physical storage capacity reservation storing data on the non volatile storage device deallocating physical storage capacity or the like.

Referring back to the storage layer may be configured to maintain allocations of the logical address space and or bindings between LIDs and media storage locations using inter alia the storage metadata . The storage layer may be further configured to store data in contextual format as disclosed above the contextual format may comprise associating persistent contextual metadata e.g. logical interface with the data. Accordingly contextual metadata pertaining to the data may be determined independent of the storage metadata . Moreover the storage layer may be configured to store data in a sequential log such that a sequence of storage operations performed through the storage layer can be replayed and or the storage metadata may be reconstructed based upon the contents of the storage device . In some embodiments the storage layer may maintain a large thinly provisioned logical address space which may simplify logical allocation operations for the storage clients e.g. allow the storage clients to operate within large contiguous LID ranges with low probability of LID collisions . The storage layer may be further configured to deter the reservation of media storage locations until needed to prevent premature exhaustion or over reservation of physical storage resources.

The storage layer may expose access to the logical address space and or storage metadata to the storage clients through one or more interfaces . As disclosed herein storage clients may delegate certain functions to the storage layer . Storage clients may leverage the virtual storage interface to perform various operations including but not limited to logical address space management media storage location management e.g. mappings between LIDs and media storage locations such as thin provisioning deferred physical resource reservation crash recovery logging backup e.g. snapshots crash recovery data integrity transactions data move operations cloning deduplication and so on.

In some embodiments storage clients may leverage the contextual log format to delegate crash recovery and or data integrity functionality to the storage layer . For instance after an invalid shutdown and reconstruction operation the storage controller may provide access to the reconstructed storage metadata to storage clients through the interface . The storage clients may therefore delegate crash recovery and or data integrity to the storage layer . File system storage clients may require crash recovery and or data integrity services for certain data such as I node tables file allocation tables and so on. The storage client may have to implement these services itself which may impose significant overhead and or complexity. The storage client may be relieved from this overhead by delegating crash recovery and or data integrity to the storage layer as disclosed herein.

In some embodiments storage clients may also delegate logical allocation operations and or physical storage reservations to the storage layer . A storage client such as a file system may maintain its own metadata to track logical and physical allocations for files the storage client may maintain a set of logical addresses that mirrors the media storage locations of the non volatile storage device . If the underlying storage device provides a one to one mapping between logical block address and media storage locations as with conventional storage devices the block storage layer performs appropriate LBA to media address translations and implements the requested storage operations. If however the underlying non volatile storage device does not support one to one mappings e.g. the underlying storage device is a sequential or write out of place device such as a solid state storage device another redundant set of translations are needed e.g. a Flash Translation Layer or other mapping . The redundant set of translations and the requirement that the storage client maintain logical address allocations may represent a significant overhead and may make allocating contiguous LBA ranges difficult or impossible without time consuming defragmentation operations. The storage client may delegate such allocation functionality to the storage layer . The storage layer may leverage a thinly provisioned logical address space to manage large contiguous LID ranges for the storage client without the need for redundant address translation layers.

The entries in the index may include LIDs that are allocated but that are not associated with media storage locations on a non volatile storage device. Like the index described above inclusion in the index may indicate that a LID is both allocated and associated with valid data on the non volatile storage device . Alternatively the index may be implemented similarly to the index of . In this case the index may comprise entries that are associated with valid data on the non volatile storage device along with entries that are allocated but are not associated with stored data. The entries that are associated with valid data may identify the media storage location of the data as described above. Entries that are not associated with valid stored data e.g. allocation entries such as the entry of may have a NULL media storage location indicator or some other suitable indicator.

In some embodiments the index may comprise security related metadata such as access control metadata or the like. The security related metadata may be associated with each respective entry e.g. entry in the index . When storage requests pertaining to a particular LID are received by the storage layer the storage layer may access and or enforce the security related metadata if any in the corresponding entry. In some embodiments the storage layer delegates enforcement of security related policy enforcement to another device or service such as an operating system access control system or the like. Accordingly when implementing storage operations the storage layer may access security related metadata and verify that the requester is authorized to perform the operating using a delegate. If the delegate indicates that the requester is authorized the storage layer implements the requested storage operations if not the storage layer returns a failure condition.

The storage layer may access the storage metadata such as the index to allocate LIDs in the logical address space to determine a remaining logical capacity of the logical address space to determine the remaining physical storage capacity of the non volatile storage device s and so on. The storage layer may respond to queries for the remaining logical capacity remaining physical storage capacity and the like via the virtual storage interface . Similarly the storage layer may service requests to reserve physical storage capacity on the non volatile storage device . As described above a storage client may wish to perform a sequence of storage operations that occur over time e.g. receive a data stream perform a DMA transfer or the like . The storage client may reserve sufficient logical and or physical storage capacity to perform the sequence of storage operations up front to ensure that the operations can be completed. Reserving logical capacity may comprise allocating LIDs through the storage layer using the virtual storage interface . Physical capacity may be similarly allocated. The storage client may request to reserve physical capacity through the virtual storage interface . If a sufficient amount of physical capacity is available the storage layer acknowledges the request and updates the storage metadata accordingly and as described above in conjunction with .

The storage layer and or storage metadata is not limited to the particular exemplary datastructures described above. The storage metadata may comprise any suitable datastructure or datastructure combination for efficiently tracking logical address space allocations and or associations between LIDs and media storage locations. For example the index may be adapted such that entries in the index comprise and or are linked to respective physical binding metadata. The physical binding metadata may comprise a sub index of associations between LIDs in a particular allocated range and corresponding media storage locations on the non volatile storage medium. Each sub range within the allocated LID comprises an entry associating the sub range with a corresponding media storage location if any .

In some embodiments the storage layer is configured to segment the LIDs in the logical address space into two or more portions. As shown in a LID 1900 is segmented into a first portion and a second portion . In some embodiments the first portion comprises high order bits of the LID 1900 and the second portion comprises low order bits. However the disclosure is not limited in this regard and could segment LIDs using any suitable segmentation scheme.

The first portion may serve as a reference or identifier for a storage entity. As used herein a storage entity refers to any data or data structure that is capable of being persisted to the non volatile storage device accordingly a storage entity may include but is not limited to file system objects e.g. files streams I nodes etc. a database primitive e.g. database table extent or the like streams persistent memory space memory mapped files virtual storage unit VSU logical unit number LUN virtual logical unit number VLUN logical storage unit LSU block storage device or the like.

The second portion may represent an offset into the storage entity. For example the storage layer may reference the logical address space comprising 64 bit LIDs the logical address space may comprise 2 64 unique LIDs . The storage layer may partition the LIDs into a first portion comprising the high order 32 bits of the 64 bit LID and a second portion comprising the low order 32 bits of the LID. The resulting logical address space may be capable of representing 2 32 1 unique storage entities e.g. using the first portion of the LIDs each having a maximum size or offset of 2 32 virtual storage locations e.g. 2 TB for a virtual storage location size of 512 bytes . The disclosure is not limited in this regard however and could be adapted to use any suitable segmentation scheme. For example in implementations that require a large number of small storage entities e.g. database applications messaging applications or the like the first portion may comprise a larger proportion of the LID. For instance the first portion may comprise 42 bits providing 2 42 1 unique identifiers and the second portion may comprise 22 bits providing a maximum offset of 4 GB . Alternatively where larger files are required the segmentation scheme may be similarly modified. Furthermore the storage layer may present larger logical address spaces e.g. 128 bits and so on in accordance with the requirements of the storage clients configuration of the computing device and or configuration of the non volatile storage device . In some embodiments the storage layer segments the logical address space in response to a request from a storage client or other entity.

The storage layer may allocate LIDs based on the first portion . For example in a 64 bit address space when the storage layer allocates a LID comprising a first portion 0000 0000 0000 0000 0000 0000 0000 0100 e.g. first portion logical address 4 the storage layer is effectively allocating a logical address range comprising 2 32 unique LIDs 1956 4 294 967 296 unique LIDS ranging from 

In some embodiments the storage layer uses the segmentation of the LIDs to simplify the storage metadata . In one example the number of bits in the first portion is X and the number of bits in the second portion is Y. The storage layer may determine that the maximum number of unique LIDs that can be allocated is 2 X and that the allocated LIDs can be referenced using only the first portion of the LID e.g. the set of X bits . Therefore the storage layer may simplify the storage metadata index to use entries comprising only the first portion of a LID. Moreover the storage layer may determine that the LIDs are allocated in fixed sized ranges of 2 Y. Accordingly each entry in the storage metadata e.g. index may be of the same extent. Therefore the range portion of the metadata entries may be omitted.

Each entry in the index may be uniquely identified using the first portion eight bits of a LID. Accordingly the entries may be indexed using only the first portion e.g. 8 bits . This simplification may reduce the amount of data required to identify an entry from 64 bits to 8 bits assuming a 64 bit LID with an 8 bit first portion . Moreover the LIDs may be allocated in fixed sized logical ranges e.g. in accordance with the second portion . Therefore each entry may represent the same range of allocated LIDs. As such the entries may omit explicit range identifiers which may save an additional 64 bits per entry .

The storage layer may use the simplified index to maintain LID allocations in the logical address space and or identify LIDs to allocate in response to requests from storage clients . In some embodiments the storage layer maintains a listing of first portions that are unallocated. Since in some embodiments allocations occur in a pre determined way e.g. using only the first portion and within a fixed range 1956 the unallocated LIDs may be expressed in a simple list or map as opposed to an index or other datastructure. As LIDs are allocated they are removed from the datastructure and are replaced when they are deallocated.

Associations between portions of the entry and valid data on the non volatile storage device may be maintained in the index using physical binding metadata as described above . depicts an example of physical binding metadata for use in a segmented logical addressing scheme. For clarity in the example LIDs are segmented such that the first portion comprises 56 bits and the second portion comprises 8 bits the reverse of . The entry is identified using the first portion 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0111 1010. The entries of the index may be simplified to reference only offsets within the entry e.g. within the second portion which comprises 8 bits in the example . Moreover the head entry may omit the top end of the second portion e.g. may omit 1111 1111 since it can be determined that the top most entry will necessarily include the maximal extent of the range defined by the second portion . Similarly the tail entry may omit the bottom end of the second portion e.g. may omit 0000 000 since it can be determined that the bottom most entry will necessarily include the beginning of the range defined by the second portion . Each entry associates a range within the second portion with valid data on the non volatile storage device if any as described above.

As described above storage clients may delegate LID allocation to the storage layer using the virtual storage interface . The delegation may occur in a number of different ways. For example a storage client may query the storage layer via the storage layer interface for any available LID. If a LID is available the storage layer returns an allocated LID to the storage client . Alternatively the storage client may request a particular LID for allocation. The request may comprise the first portion of the LID or an entire LID with an offset . The storage layer may determine if the LID is unallocated and if so may allocate the LID for the client and return an acknowledgement. If the LID is allocated or the LID falls within an allocated range the storage layer may allocate an alternative LID and or may return an error condition. The storage layer may indicate whether particular LIDs are allocated and or whether particular LIDs are bound to media storage locations on the non volatile storage device . The queries may be serviced via the virtual storage interface .

In embodiments in which the storage layer implements segmented LIDs the storage layer may expose the segmentation scheme to the storage clients . For example storage clients may query the storage layer to determine the segmentation scheme currently in use. The storage clients may also configure the storage layer to use a particular LID segmentation scheme adapted to the needs of the storage client .

The storage layer may allocate LIDs using only the first portion of a LID. If the LID is unallocated the storage layer acknowledges the request and the storage client is allocated a range of LIDs in the logical address space corresponding to the first portion and comprising the range defined by the second portion . Similarly when allocating a nameless LID e.g. any available LID selected by the storage layer the storage layer may return only the first portion of the allocated LID. In some embodiments when a client requests a LID using the first portion and the second portion the storage layer extracts the first portion from the requested LID and allocates a LID corresponding to the first portion to the client if possible . Advantageously the disclosed embodiments support such a large number of addresses for the second portion over such a high number of contiguous addresses that storage requests that cross a LID boundary are anticipated to be very rare. In certain embodiments the storage layer may even prevent allocations that cross LID boundaries as used herein a LID boundary is between two contiguous LIDs the first being the last addressable LID in a second portion of a LID and the second being the first addressable LID in a next successive first portion of a LID . If the request crosses a boundary between pre determined LID ranges the storage layer may return an alternative LID range that is properly aligned to the LID segmentation scheme return an error or the like. In other embodiments if the request crosses a boundary between pre determined LID ranges the storage layer may allocate both LIDs if available .

As described above the storage layer may be leveraged by the storage clients for logical allocations physical storage bindings physical storage reservations crash recovery data integrity and the like. is a block diagram depicting a file system storage client leveraging the storage layer to perform file system operations.

The file system storage client accesses the storage layer via the virtual storage interface to allocate LIDs for storage entities such as file system objects e.g. files . In some embodiments when a new file is created the file system storage client queries the storage layer for a LID. The allocation request may be implemented as described above. If the requested LIDs can be allocated the storage layer returns an allocated LID to the file system storage client . The LID may be returned as a LID and an offset indicating an initial size for the file a LID range a first portion of a LID or the like. The example shows the storage layer implementing a segmented LID range and as such the storage layer may return the first portion of a LID 2062 in response to an allocation request.

In some embodiments the file system storage client may implement a fast and efficient mapping between LIDs and storage entities. For example when the first portion of the LID is sufficiently large the file system storage client may hash file names into LID identifiers into hash codes of the same length as the first portion of the LID 2062 . When a new file is created the file system storage client hashes the file name to generate the first portion of the LID 2062 and issues a request to the storage layer to allocate the LID. If the LID is unallocated e.g. no hash collisions have occurred the storage layer may grant the request. The file system storage client may not need to maintain an entry in the file system table for the new file or may only be required to maintain an abbreviated version of a table entry since the LID 2062 can be derived from the file name. If a name collision occurs the storage layer may return an alternative LID which may be derived from the hash code or file name which may obviate the need for the file system table to maintain the entire identifier.

The file system storage client may maintain a file system table to associate file system objects e.g. files with corresponding LIDs in the logical address space of the storage layer . In some embodiments the file system table is persisted on the non volatile storage device at a pre determined LID. Accordingly the file system storage client may delegate crash recovery and or data integrity for the file system table as well as the file system objects themselves to the storage layer .

The file system storage client may reference files using the file system table . To perform storage operations on a particular file the file system storage client may access a file system entry corresponding to the file e.g. using a file name lookup or another identifier such as an I node or the like . The entry comprises a LID of the file which in the example is a first portion of a LID 2062. The file system storage client performs storage operations using the first portion of the LID along with an offset the second portion . The file system storage client may combine the file identifier first portion with an offset to generate a full LID 2070. The LID 2070 may be sent to the storage layer in connection with requests to perform storage operations within the logical address space .

The storage layer performs storage operations using the storage metadata . Storage requests to persist data in the logical address space comprise the storage layer causing the data to be stored on the non volatile storage device in a contextual log based format as disclosed above. The storage layer updates the storage metadata to associate LIDs in the logical address space with media storage locations on the non volatile storage comprising data stored in the storage operation.

Storage operations to access persisted data on the non volatile storage device may comprise the storage client such as the file system storage client requesting the data associated with one or more LIDs 2070 in the logical address space. The file system storage client may identify the LIDs using the file system table or another datastructure. In response to the request the storage layer determines the media storage location of the LIDs 2070 on the non volatile storage device using the storage metadata which is used to access the data.

In some embodiments storage clients such as the file system storage client may deallocate a storage entity. Deallocating a storage entity may comprise issuing a deallocation request to the storage layer via the virtual storage interface . In response to a deallocation request the storage layer removes the deallocated LIDs from the storage metadata and or may mark the deallocated LIDs as unallocated. The storage layer may also invalidate the media storage locations corresponding to the deallocated LIDs in the storage metadata and or the non volatile storage device e.g. using a reverse map as disclosed above . A deallocation may be a hint to a groomer of the non volatile storage device that the media storage locations associated with the deallocated LIDs are available for recovery.

The groomer however may not actually remove the data for some time after the deallocation request issued. Accordingly in some embodiments the virtual storage interface may provide an interface through which storage clients may issue a deallocation directive as opposed to a hint . The deallocation directive may configure the storage layer to return a pre determined value e.g. 0 or NULL for subsequent accesses to the deallocated LIDs or the media storage locations associated therewith even if the data is still available on the non volatile storage device . The pre determined value may continue to be returned until the LIDs are reallocated for another purpose.

In some embodiments the storage layer implements a deallocation directive by removing the deallocated LIDs from the storage metadata and returning a pre determined value in response to requests for LIDs that are not allocated in the storage metadata and or are not bound e.g. are not associated with valid data on the non volatile storage device . Alternatively or in addition in response to a deallocation directive the storage layer may cause the corresponding media storage locations on the non volatile storage device to be erased. The storage layer may provide the file system storage client with an acknowledgement when the erasure is complete. Since erasures make take a significant amount of time to complete relative to other storage operations the acknowledgement may be issued asynchronously.

In some embodiments the name to LID metadata may be included with the storage metadata . For example entries in the index of may be indexed by name in addition to or in place of a LID. The storage layer may persist the name to LID metadata on the non volatile storage device such that the integrity of the metadata is maintained despite invalid shutdown conditions. Alternatively or in addition the name to LID metadata may be reconstructed using the contextual log based data format on the non volatile storage device .

At step storage metadata is maintained. The storage metadata may track allocations of LIDs within the logical address space as well as bindings between LIDs and media storage locations of the non volatile storage device. The metadata may further comprise indications of the remaining logical capacity of the logical address space the remaining physical storage capacity of the non volatile storage device the status of particular LIDs and so on.

In some embodiments the metadata is maintained in response to storage operations performed within the logical address space. The storage metadata is updated to reflect allocations of LIDs by storage clients. When storage clients persist data to allocated LIDs bindings between the LIDs and the media storage locations comprising the data are updated.

At step storage operations are performed using a log based sequence. As described above the storage layer and non volatile storage device may be configured to store data in a log based format such that an ordered sequence of storage operations performed on the storage device can be reconstructed in the event of an invalid shutdown or other loss of storage metadata . The ordered sequence of storage operations allows storage clients to delegate crash recovery data integrity and other functionality to the storage layer .

At step the method segments LIDs of a logical address space into at least a first portion and a second portion. The segmentation of step may be performed as part of a configuration process of the storage layer and or non volatile storage device e.g. when the device is initialized . Alternatively or in addition the segmentation of step may be performed in response to a request from a storage client. The storage client may request a particular type of LID segmentation according to the storage requirements thereof. For example if the storage client has a need to store a large number of relatively small storage entities the storage client may configure the LID segmentation to dedicate a larger proportion of the LID to identification bits and a smaller proportion to offset bits. Alternatively a storage client who requires a relatively small number of very large storage entities may configure the method to implement a different type of segmentation that uses a larger proportion of the LID for offset bits allowing for larger storage entities .

At step the storage layer uses the first portion of the LID to reference storage client allocations e.g. as a reference for storage entities . Step may comprise reconfiguring the storage metadata to allocate LIDs using only the first portion of the LID e.g. the upper X bits of a LID . The size of the first portion may determine the number of unique storage entities that can be expressed in the storage metadata e.g. as 2 X 1 where X is the number of bits in the first portion . Accordingly a first portion comprising 32 bits may support approximately 2 32 unique storage entities. The reconfiguration may simplify the storage metadata since each entry may be identified using a smaller amount of data only the first portion of the LID as opposed to the entire LID .

At step the storage layer uses the second portion of the LID as an offset into a storage entity. The size of the second portion may define the maximum size of a storage entity under the current segmentation scheme . The size of a LID may be defined as the virtual block size times 2 Y where Y is the number of bits in the second portion. As discussed above a virtual block size of 512 and second portion comprise 32 bits results in a maximum storage entity size of 2 TB. Step may comprise reconfiguring the storage metadata to reference LID to media storage location bindings using only the second portion of the LID. This may allow the storage metadata entries e.g. entries in physical binding metadata to be simplified since the bindings can be expressed using a smaller number of bits.

At step the storage layer uses the LID segmentation of step to allocate LIDs comprising contiguous logical address ranges in the logical address space. Step may comprise the storage layer allocating LIDs using only the first portion of the LID e.g. the upper X bits . The allocated LID may comprise a contiguous logical address range corresponding to the number of bits in the second portion as described above.

In some embodiments allocating a LID at step does not cause corresponding logical storage locations to be reserved of bound thereto. The bindings between allocated LIDs and media storage locations may not occur until the storage client actually performs storage operations on the LIDs e.g. stores data in the LIDs . The delayed binding prevents the large contiguous LID allocations from exhausting the physical storage capacity of the non volatile storage device.

At step the storage layer causes data to be stored on the non volatile storage device in a contextual log based format. As described above the contextual log based formatting of the data is configured such that in the event of an invalid shutdown the data and metadata pertaining thereto can be reconstructed.

At step the storage layer reconstructs data stored on the non volatile storage device using the data formatted in the contextual log based format. As described above the log based format may comprise storing LID identifiers with data on the non volatile storage device. The LID identifiers may be used to associate the data with LIDs in the logical address space e.g. reconstruct the storage metadata . Sequence indicators stored with the data on the non volatile storage device are used to determine the most current version of data associated with the same LID since data is written out of place updated data may be stored on the non volatile storage device along with previous obsolete versions. The sequence indicators allow the storage layer to distinguish older versions from the current version. The reconstruction of step may comprise reconstructing the storage metadata determining the most current version of data for a particular LID e.g. identifying the media storage location that comprises the current version of the data and so on.

At step the storage layer provides access to the reconstructed data to storage clients. Accordingly the storage clients may delegate crash recovery and or data integrity functionality to the storage layer which relieves the storage clients from implementing these features themselves. Accordingly the storage clients can be simpler and more efficient.

At step the storage layer accesses storage metadata to determine the status of the requested LID logical capacity physical storage capacity or the like. The access may comprise identifying an entry for the LID in a logical to physical map in an allocation index or the like. If the particular LID falls within an entry in an allocation index and or logical to physical index the storage layer may determine that the LID is allocated and or may determine whether the LID is bound to a media storage location. The access may further comprise traversing a metadata index to identify unallocated LIDs unused media storage locations and so on. The traversal may further comprise identifying allocated or unallocated LIDs to determine current LID allocation or unallocated LID capacity to determine bound physical storage capacity determine remaining physical storage capacity or the like. At step the storage layer returns the status determined at step to the storage client .

At step the storage layer receives a request pertaining to the status of a particular media storage location on a non volatile storage device. The media storage location may be associated with a LID in the logical address space presented by the storage layer . Alternatively the query may be iterative and may pertain to all media storage locations on the non volatile storage device e.g. a query regarding the status of all media storage locations on the device . Similarly the query may pertain to the physical storage capacity of the non volatile storage device such as a query regarding the physical storage capacity that is bound to LIDs in the logical address space e.g. currently occupied available physical storage capacity and so on.

The query of step may be useful in various different contexts. For example in a RAID rebuild operation a second non volatile storage device may be configured to mirror the contents of a first non volatile storage device. The data stored on the first logical storage device may be stored sequentially e.g. in a contextual log based format . As such the first non volatile storage device may comprise invalid data e.g. data was deleted was made obsolete by a sequent storage operation etc. . The query of step may be issued by the second non volatile storage device to determine which media storage locations on the first non volatile storage device exist e.g. are valid and should be mirrored on the second non volatile storage device. Accordingly the query of step may be issued in the form of an iterator configured to iterate over e.g. discover all media storage locations that comprise valid data and the extent of the valid data.

Step comprises accessing storage metadata such as the index or reverse map described above in conjunction with to determine whether the specified media storage location comprises valid data and or to determine the extent or range of valid data in the specified media storage location. At step the storage layer returns the status determined at step to the requester.

In some embodiments methods and are used to implement conditional storage operations. As used herein a conditional storage operation refers to a storage operation that is to occur if one or more conditions are met. A conditional write may comprise a storage client requesting that data be written to a particular set of LIDs. The storage layer may implement the conditional write if the specified LIDs do not exist e.g. are not already allocated to another storage client and the non volatile storage comprises sufficient physical storage capacity to satisfy the request. Similarly a conditional read may comprise a storage client requesting data from a particular set of LIDs. The storage layer may implement the conditional read if the specified LIDs exist and are bound to valid data e.g. are in storage metadata maintained by the storage layer and are bound to media storage locations . In other examples the storage layer provides for nameless reads and writes in which a storage client presents identifier and the storage layer determines the LIDs associated with the identifier and services the storage request accordingly e.g. nameless writes as described above . In this case the storage layer offloads management of identifier to LID mappings for the storage client.

In some embodiments the storage metadata maintained by the storage layer may provide for designating certain portions of the logical address space as being temporary or ephemeral. As used herein an ephemeral address range is an address range that is set to be automatically deleted under certain conditions. The conditions may include but are not limited to a restart operation a shutdown event planned or unplanned expiration of a pre determined time resource exhaustion etc.

Data may be identified as ephemeral in storage metadata maintained by the storage layer in metadata persisted to the solid state storage media or the like. Referring back to an entry in the index forward map may be identified as ephemeral in the metadata thereof. When the storage layer persists the index as part of a shutdown restart or other operation entries that include an ephemeral indicator may be omitted effectively invalidating the corresponding data. Alternatively or in addition the storage layer may designate an a portion of the large logical address space as comprising ephemeral data. Any entries in the ephemeral address range may be designated as ephemeral in the index without additional modifications to entry metadata.

In some embodiments an ephemeral indicator may be included in a media storage location on the non volatile storage media. depicts one example of a contextual data format e.g. packet format which may be used to store a data segment on a non volatile storage media. As described above in some embodiments packets may be subject to further processing before being persisted on a media storage location e.g. packets may be encoded into ECC codewords by an ECC generator as described above .

The packet format may comprise persistent metadata which may include logical interface metadata as described above. The packet format may comprise and or be associated with a sequence indicator which may include but is not limited to a sequence number timestamp or other suitable sequence indicator. The sequence indicator may be included in the persistent metadata e.g. as another field not shown . Alternatively or in addition a sequence indicator may be stored elsewhere on the non volatile storage media . For example a sequence indicator may be stored on a page or virtual page basis on an erase block basis or the like. As described above each logical erase block may be marked with a respective marking and packets may be stored sequentially therein. Accordingly the sequential order of packets may be determined by a combination of the logical erase block sequence indicators e.g. indicators and the sequence of packets within each logical erase block.

The storage layer may be configured to reconstruct the storage metadata e.g. index etc. using the contextual log based formatted data stored on the non volatile storage media . Reconstruction may comprise the storage layer or another process reading packets formatted in the contextual log based format from media storage locations of the solid state storage media . As each packet is read a corresponding entry in the storage metadata e.g. the indexes described above may be created. The LID range associated with the entry is derived from the LID 2516 in the header of the packet. The sequence indicator associated with the data packet may be used to determine the most up to date version of data for a particular LID. As described above the storage layer may write data out of place due to inter alia wear leveling write amplification and other considerations. Accordingly data intended to overwrite an existing LID may be written to a different media storage location than the original data. The overwritten data is invalidated as described above this data however remains on the solid state storage media until the erase block comprising the data is groomed e.g. reclaimed and erased . The sequence identifier may be used to determine which of two or more contextual log based packets corresponding to the same LID comprises the current valid version of the data.

In some embodiments and as illustrated in the header includes an ephemeral indicator . When reconstructing the storage metadata the ephemeral indicator may be used to identify data that should be invalidated e.g. deleted . Invalidating ephemeral data may comprise omitting the LIDs 2514 referenced in the logical interface of the packet marking the data segment as invalid in a reverse index and so on. Similarly if data marked as ephemeral is more up to date than other data per the sequence indicator the original older data may be retained and the ephemeral data may be ignored.

The storage layer may provide an API through which storage clients may designate certain LID ranges or other identifiers as being ephemeral. Alternatively or in addition the storage layer may implement higher level interfaces using ephemeral data. For example a multi step atomic write e.g. multi block atomic write may be implemented by issuing multiple write requests each of which designates the data as being ephemeral. When all of the writes are completed the ephemeral designation may be removed. If a failure occurs during the multi step atomic write data that was previously written can be ignored no roll back is necessary since the data will be removed the next time the device is restarted. A similar approach may be used to provide support for transactions. As used herein a transaction refers to a plurality of operations that are completed as a group. If any one of the transaction operations is not completed the other transaction operations are rolled back. As a transaction are implemented the constituent storage operations may be marked as ephemeral. Successful completion of the transaction comprises removing the ephemeral designation from the storage operations. If the transaction fails the ephemeral data may be ignored.

In some embodiments ephemeral data may be associated with a time out indicator. The time out indicator may be associated with the operation of a storage reclamation process such as a groomer. When the groomer evaluates a storage division e.g. erase block page etc for reclamation ephemeral data therein may be treated as invalid data. As such the ephemeral data may be omitted during reclamation processing e.g. not considered for storage division selection and or not stored in another media storage location during reclamation . In some embodiments ephemeral data may not be treated as invalid until its age exceeds a threshold. The age of ephemeral data may be determined by the sequence indicator associated therewith. When the age of ephemeral data exceeds a pre determined threshold it may be considered to be part of a failed transaction and may be invalidated as described above. The threshold may be set on a per packet basis e.g. in the header may be set globally through an API or setting of the storage layer or the like.

As described above removing an ephemeral designation may comprise updating storage metadata e.g. index to indicate that a particular entry is no longer to be considered to be ephemeral. In addition the storage layer may update the ephemeral indicator stored on the solid state storage media e.g. in persistent metadata of a packet . However if the solid state storage media is write out of place it may not be practical to overwrite or rewrite these indicators. Therefore in some embodiments the storage layer persists a note on the solid state storage media e.g. writes a persistent note to a media storage location of the solid state storage media . As used herein a persistent note refers to a metadata note that is persistently stored on the solid state storage media. Removing the ephemeral designation may comprise persisting a metadata note indicating the removal to the solid state storage media. As depicted in a persistent note may comprise a reference that identifies one or more packets on a media storage location. The reference may comprise any suitable identifying information including but not limited to a logical interface a LID a range a media storage location identifier a sequence indicator or the like. The persistent note may also include a directive which in the example may be a directive to remove an ephemeral designation from the identified packets. Additional details regarding persistent notes are disclosed in U.S. patent application Ser. No. 13 330 554 entitled Apparatus System and Method for Persistent Metadata filed Dec. 19 2011 and which is hereby incorporated by reference.

In some embodiments the logical address space presented by the storage layer may include an ephemeral LID range. As used herein an ephemeral LID range comprises references to ephemeral data e.g. LIDs that are to be auto deleted on restart or another condition . This segmentation may be possible due to the storage layer maintaining a large e.g. sparse logical address space as described above. The storage layer maintains ephemeral data in the ephemeral logical address range as such each entry therein is considered to be ephemeral. An ephemeral indicator may also be included in contextual log based formatted data bound to the LIDs within the ephemeral range.

At step the requested LIDs are allocated as described above unless not already allocated by another storage client . Step may further comprise updating storage metadata to indicate that the LIDs ephemeral which may include but is not limited to setting an indicator in a entry for the LIDs in the storage metadata e.g. index allocating the LIDs in an ephemeral range of the index.

At step the storage client may request one or more persistent storage operations on the ephemeral LIDs of step . The storage operations may comprise a multi block atomic write operations pertaining to a transaction a snapshot operation a clone described in additional detail below or the like. Step may comprise marking contextual log based data associated with the persistent storage operations as ephemeral as described above e.g. in a header of a packet comprising the data .

At step if the method receives a request to remove the ephemeral designation the flow continues to step otherwise the flow continues to step . The request of step may be issued by a storage client and or the request may be part of a higher level API as described above. For example the request may be issued when the constituent operations a transaction or atomic operation are complete.

At step the ephemeral designation applied at steps and are removed. Step may comprise removing metadata indicators from storage metadata folding the ephemeral range into a non ephemeral range of the storage metadata index or the like folding is described in additional detail below . Step may further comprising storing one or more persistent notes on the non volatile storage media that remove the ephemeral designation from data corresponding to the formerly ephemeral data as described above.

At step the method may determine whether the ephemeral data should be removed. If not the flow continues back to step otherwise the flow continues to step . At step the ephemeral data is removed or omitted when the storage metadata is persisted as part of a shutdown or reboot operation . Alternatively or in addition data that is designated as ephemeral on the non volatile storage media may be ignored during a reconstruction process.

At step the method iterates over media storage locations of the storage device. The iteration may comprise accessing a sequence of media storage locations on the non volatile storage medium as described above in conjunction with .

At step for each media storage location the method access data formatted in the contextual log based format described above. The method may reconstruct the storage metadata using information determined from the contextual log based data format on the non volatile storage media . Using the contextual log based data format the method may determine the LIDs associated with the data may determine whether the data is valid e.g. using persistent notes and or sequence indicators as described above and so on. Alternatively step may comprise issuing queries to another storage device to iteratively determine which media storage locations comprise valid data. The iterative query approach described above in conjunction with may be used to mirror a storage device.

In addition at step the method determines whether a particular data packet is designated as being ephemeral. The determination may be based on an ephemeral indicator in a header of the packet. The determination may also comprise determining whether a persistent note that removes the ephemeral designation exists e.g. a persistent note as described above in conjunction with . Accordingly step may comprise the method maintaining the metadata for the packet in a temporary e.g. ephemeral location until the iteration of step completes and the method can determine whether a persistent note removing the ephemeral designation exists.

If step determines that the data is ephemeral the flow continues to step otherwise the flow continues to step . At step the method removes the ephemeral data. Removing the data may comprise omitting LIDs associated with the data from storage metadata e.g. the index described above marking the media storage location as invalid and available to be reclaimed e.g. in the reverse map or the like.

At step the method reconstructs the storage metadata as described above. In some embodiments step may further comprise determining whether the data is valid as described above in conjunction with . If the data is valid the method may be configured to perform further processing. For example if the method is being used to construct a mirror of another storage device step may comprise transferring the valid data to the mirror device.

In some embodiments the storage layer may provide an API to order storage operations performed thereon. For example the storage layer may provide a barrier API to determine the order of operations. As used herein a barrier refers to a primitive that enforces an order of storage operations. A barrier may specify that all storage operations that were issued before the barrier are completed before the barrier and that all operations that were issued after the barrier complete after the barrier. A barrier may mark a point in time in the sequence of operations implemented on the non volatile storage device.

In some embodiments a barrier is persisted to the non volatile storage media as a persistent note. A barrier may be stored on the non volatile storage media and may therefore act as a persistent record of the state of the non volatile storage media at a particular time e.g. a particular time within the sequence of operations performed on the non volatile storage media . The storage layer may issue an acknowledgement when all operations issued previous to the barrier are complete. The acknowledgement may include an identifier that specifies the time e.g. sequence pointer corresponding to the barrier. In some embodiments the storage layer may maintain a record of the barrier in the storage metadata maintained thereby.

Barriers may be used to guarantee the ordering of storage operations. For example a sequence of write requests may be interleaved with barriers. Enforcement of the barriers may be used to guarantee the ordering of the write requests. Similarly interleaving barriers between write and read requests may be used to remove read before write hazards.

Barriers may be used to enable atomic operations similarly to the ephemeral designation described above . For example the storage layer may issue a first barrier as a transaction is started and then issue a second barrier when complete. If the transaction fails the storage layer may roll back the sequence of storage operations between the first and second barriers to effectively undo the partial transaction. Similarly a barrier may be used to obtain a snapshot of the state of the non volatile storage device at a particular time. For instance the storage layer may provide an API to discover changes to the storage media that occurred between two barriers.

In another example barriers may be used to synchronize distributed storage systems. As described above a second storage device may be used to mirror the contents of a first storage device. The first storage device may be configured to issue barriers periodically e.g. every N storage operations . The second storage device may lose communication with the first storage device for a certain period of time. To get back in sync the second storage device may transmit its last barrier to the first storage device and then may mirror only those changes that occurred since the last barrier.

Distributed barriers may also be used to control access to and or synchronize shared storage devices. For example storage clients may be issued a credential that allows access to a particular range of LIDs read only access read write delete etc. . The credentials may be tied to a particular point or range in time e.g. as defined by a barrier . As the storage client interacts with the distributed storage device the credential may be updated. However if a storage client loses contact with the distributed storage device the credential may expire. Before being allowed access to the distributed storage device the client may first be required to access a new set of credentials and or ensure that local data e.g. cached data etc. is updated accordingly.

At step the method enforces the ordering constraints of the barrier. Accordingly step may comprise causing all previously issued storage requests to complete. Step may further comprise queuing all subsequent requests until the previously issued requests complete and the barrier is acknowledged at step .

At step the method determines if the ordering constraints are met and if so the flow continues to step otherwise the flow continues at step .

At step the barrier is acknowledged which may comprise returning a current time e.g. sequence indicator at which the operations issued before the barrier were completed. Step may further comprise storing a persistent note of the barrier on the non volatile storage. At step the method resumes operation on storage requests issued subsequent to the barrier at step . At step the flow ends until a next request for a barrier is received.

In some embodiments the storage layer leverages the logical address space to manage logical copies of data e.g. clones . As used herein a clone or logical cloning operation refers to replicating a range or set of ranges of LIDs within the logical address space and or other addressing system. The cloned range may comprise different set s of LIDs which may be bound to the same media storage locations as the original LIDs source LIDs allowing two or more LIDs and or LID ranges to reference the same data. Clone operations may be used to perform higher level operations such as deduplication snapshots logical copies atomic operations e.g. atomic writes transactions etc. and the like.

Creating a clone may comprise modifying the logical interface of data stored in a non volatile storage device in order to inter alia allow the data to be referenced by use of two or more different LIDs and or LID extents. Accordingly creating a clone of a LID or set of LIDs may comprise allocating new LIDs in the logical address space or dedicated portion thereof and associating the new LIDs with the same media storage location s as the original LIDs in the storage metadata . Creating a clone may therefore comprise adding one or more entries to a forward index configured to associate the new set of LIDs with the data.

As disclosed herein the storage controller may be configured to store data in a contextual format on a storage device . The contextual format may comprise associating data with corresponding persistent metadata that defines and or references inter alia the logical interface of the data. In the embodiment the data stored at media addresses comprises a packet format that includes persistent metadata . The persistent metadata may comprise the logical interface of the data segment logical interface metadata and as such may associate the data segment with the LIDs 1024 2048 of the entry . As disclosed herein the contextual data format may enable the index and or other metadata to be reconstructed from the contents of the storage device in the embodiment the entry may be reconstructed by associating the data stored at media addresses with the LIDs 1024 2048 identified in the persistent metadata of the packet . Although depicts a single packet the disclosure is not limited in this regard. In some embodiments the data of the entry may be stored in multiple different packets each comprising respective persistent metadata e.g. a separate packet for each media storage location etc. .

Creating a clone of the entry may comprise allocating one or more LIDs in the logical address space and binding the new LIDs to the same data segment as the entry e.g. the data segment at media storage location . Creating the clone may therefore comprise modifying the storage metadata without requiring the underlying data segment to be copied and or replicated.

The modified logical interface B of the data may be inconsistent with the contextual format of the data segment on the storage device . As disclosed above the persistent metadata of the data segment comprises logical interface metadata that associates the data segment with LIDs 1024 2048 of the logical interface A and not LIDs 6144 7168 of the modified logical interface B. The contextual format of the data may be updated to be consistent with the modified logical interface B e.g. updated to associate the data with LIDs 1024 2048 and 6144 7168 as opposed to only LIDs 1024 2048 .

Updating the contextual format of the data segment may comprise updating the persistent metadata on the storage device . If the storage device is a random access write in place storage device the persistent metadata may be updated by overwriting and or updating the persistent metadata without relocating the data segment and or packet . In other embodiments however the storage controller may be configured to append data to a log and or update data out of place on the storage device . In such embodiments updating the contextual format of the data segment may comprise relocating and or rewriting the data segment on the storage device which may be a time consuming processes and may be particularly inefficient if the data segment is large and or the clone comprises a large number and or of LIDs. Therefore in some embodiments the storage layer may defer updating the contextual format of cloned data and or may update the contextual format in one or more background operations. In the meantime the storage layer may be configured to provide access to the data while stored in the inconsistent contextual format .

The storage layer may be configured to acknowledge completion of clone operations before contextual format of the corresponding data is updated. The data may be subsequently rewritten e.g. relocated in the updated contextual format on the storage device in another process which may be outside of the critical path of the clone operation and or other storage operations e.g. in one or more background operations . In some embodiments the data segment is relocated using the groomer or the like. Accordingly storage clients may be able to access the data segment through the modified logical interface B both 1024 2048 and 6144 7168 without waiting for the contextual format of the data segment to be updated to be consistent with the modified logical interface B.

Until the contextual format of the data segment is updated on the non volatile storage media the modified logical interface B of the data segment may exist only in the index . Therefore if the index is lost due to inter alia power failure or data corruption the clone operation may not be reflected in the reconstructed storage metadata the clone operation may not be persistent and or crash safe . In a metadata reconstruction operation the contextual format of the data at 3453 4477 is accessed the logical interface metadata of the persistent metadata indicates that the data is associated only with LIDs 1024 2048 not 1024 2048 and 6144 7168. Therefore only entry will be reconstructed as in and will be omitted moreover subsequent attempts to access the data segment through the modified logical interface B e.g. through 6144 7168 may fail.

In some embodiments a clone operation may further comprise storing a persistent note on the storage device to make a clone operation persistent and or crash safe. The persistent note may comprise an indication of the modified logical interface of the data. In the embodiment the persistent note corresponding to the depicted clone operation may comprise a persistent indicator that associates the data stored at media addresses with both LID ranges 1024 2048 and 6144 7168. During reconstruction of the index the persistent note may indicate that the data segment is associated with both LID ranges such that both entries and can be reconstructed. In some embodiments the storage layer may acknowledge completion of a clone operation in response to updating the metadata e.g. creating the index entry and storing the persistent note on the storage device . The persistent note may be invalidated and or marked for erasure in response updating the contextual format of the data segment to be consistent with the logical interface B e.g. relocating the data segment by the groomer module as disclosed above .

As disclosed above the storage controller may be configured to store the data segment in an updated contextual format that is consistent with the modified logical interface B. In some embodiments the updated contextual format may comprise associating the data segment with the LIDs of both entries and e.g. both LIDs 1024 2048 and 6144 7168 . depicts one embodiment of an updated contextual format packet of the data segment .

As illustrated in the embodiment the logical interface metadata of the updated packet indicates that the data segment is associated with both LID ranges 1024 2048 and 6144 7168 as opposed to only 1024 2048 . The updated contextual data format packet may be written out of place at different media addresses which is reflected in the entries and in the index . In response to updating the contextual format of the packet the corresponding persistent note if any may be invalidated removed and or marked for subsequent removal from the storage device . In some embodiments removing the persistent note may comprise issuing one or more TRIM messages indicating that the persistent note no longer needs to be retained on the storage device . Alternatively or in addition portions of the index may be stored in a persistent crash safe storage location e.g. non transitory storage media and or the storage device . In response to persisting the storage metadata e.g. the entries and the persistent note may be removed even if the contextual format of the data has not yet been updated on the storage device .

Clones may operate in different modes. In a copy on write mode storage operations that occur after creating the clone may cause the clones to diverge from one another e.g. the entries and may refer to different media addresses . depicts one embodiment of a storage operation performed within a cloned range in a copy on write mode. In the embodiment the storage controller has written the data segment in the updated contextual data format packet that is configured to associate the data with both LID ranges 1024 2048 and 6144 7168 as depicted in . A storage client may then issue one or more storage requests to modify and or overwrite data corresponding to the LIDs 6657 7168. In the embodiment the storage request comprises modifying and or overwriting LIDs 6657 7168. In response to the storage request the storage controller may store the new and or modified data on the storage device which may comprise appending a data segment to the log in a contextual format packet . The packet may associate the data segment with the LIDs 6657 7424 as disclosed herein e.g. by use of LID indicators within persistent metadata of the packet . The index may be updated to associate the LIDs 6657 7424 with the data segment which may comprise splitting the entry into an entry configured to continue to reference the unmodified portion of the data in the data segment and an entry that references the new data segment stored at media addresses . In the copy on write mode depicted in the entry corresponding to the LIDs 1024 2048 may continue to reference the data segment at media addresses . Although not depicted in modifications to within the LID range 1024 2048 may result in similar divergent changes affecting the entry . Moreover the storage request s are not limited to modifying and or overwriting data. Other operations may comprise expanding a LID range appending data removing LIDs deleting and or trimming data and or the like.

In some embodiments the storage controller may support other clone modes such as a synchronized clone mode. In a synchronized clone mode changes made within a cloned LID range may be reflected in one or more other corresponding LID ranges. In the embodiment implementing the described storage operations in a synchronized clone mode may comprise updating the entry to reference the new data segment as disclosed herein which may comprise inter alia splitting the entry into an entry configured to associate LIDs 1024 1536 with the original data segment and adding an entry configured to associate the LIDs 1537 2048 with the new data segment .

Referring back to the copy on write embodiment of the storage layer may be further configured to manage clone merge operations. As used herein a range merge or clone merge refers to an operation to combine two or more different sets of LIDs. In the embodiment a range merge operation may comprise merging the entry with the cloned entries and . The storage layer may be configured to implement range merge operations according to a merge policy such as a recency policy in which more recent changes override earlier changes a priority based policy based on the relative priority of storage operations e.g. based on properties of the storage client s applications and or users associated with the storage operations a completion indicator e.g. completion of an atomic storage operation failure of an atomic storage operation or the like fadvise parameters ioctrl parameters and or the like.

The range merge operation illustrated in may result in modifying the logical interface C to portions of the data. The contextual format of the data segment may associate the data with LIDs 6657 7168 rather than LIDs 1537 2048. As disclosed above the storage layer may provide access to the data stored in the inconsistent contextual format. The storage controller may be configured to store the data in an updated contextual format in which the data segment is associated with LIDs 1537 2048 in one or more background operations e.g. grooming operations . In some embodiments the range merge operation may further comprise storing a persistent note on the storage device to associate the data segment with the updated logical interface C e.g. associate the data at media addresses with LIDs 1537 2048 . As disclosed above the persistent note may be used to ensure that the range merge operation is persistent and crash safe. The persistent note may be removed in response to relocating the data segment in a contextual format that is consistent with the logical interface C e.g. associates the data segment with the LIDs 1537 2048 .

The logical clone operations disclosed in conjunction with may be used to implement other logical operations such as a range move operation. Referring back to the clone operation of entry comprises modifying the logical interface associated with the data segment to associate the data segment with the LIDs 1024 2048 of entry and the LIDs 6144 7168 of entry . The cloning operation further includes storing a persistent note indicating the updated logical interface B of the data and rewriting the data segment in accordance with the updated logical interface B in one or more background storage operations e.g. grooming operations .

The same set of operations may be performed to perform a range move operation. As used herein a range move operation refers to modifying the logical interface of one or more data segments to associate the data segments with a different set of LIDs. A range move operation may therefore comprise updating storage metadata e.g. the index to associate the one or more data segments with the updated logical interface storing a persistent note on the storage device comprising the updated logical interface of the data segments and rewriting the data segments in accordance in a contextual format packet that is consistent with the updated logical interface e.g. includes the updated logical interface in the persistent metadata as disclosed herein. Accordingly the storage layer may implement range move operations using the same mechanisms and or processing steps as those disclosed above in conjunction with .

The logical clone operations disclosed in however may impose certain limitations on the storage layer . As disclosed above storing data in a contextual format packet may comprise associating the data with each LID that references the data. In the embodiment the persistent metadata comprises references to both LID ranges 1024 2048 and 6144 7168. Increasing the number references to a data segment may therefore impose a corresponding increase in persistent metadata overhead. In some embodiments the size of the persistent metadata may be limited which may limit the number of references and or clones that can reference a particular data segment. Moreover inclusion of multiple LID references may complicate groomer operations. The number of index entries needed to be updated in a grooming operation may vary in accordance with the number of LIDs that reference the data that is to be relocated. Referring back to relocating the data segment in a grooming and or storage recovery operation may comprise updating two separate index entries and . Relocating a data segment referenced by N different clones e.g. N different LIDs may comprise updating N different index entries. Similarly storing the data segment may comprise writing N entries into the persistent metadata . This variable overhead may reduce the performance of background grooming operations and may limit the number of concurrent clones and or references that can be supported.

In some embodiments the storage layer may comprise and or leverage an intermediate mapping layer to reduce the overhead imposed by clone operations. The intermediate mapping layer may comprise reference entries configured to facilitate efficient cloning operations as well as other operations as disclosed in further detail herein . As used herein a reference entry refers to an entry that only exists while it is being referenced by one or more entries in the logical address space . Accordingly a reference entry does not exist in its own right but only exists as long as it is being referenced by one or more other index entries. In some embodiments reference entries may be immutable. Multiple clones may reference the same set of data through a single reference entry. The contextual format of cloned data data that is referenced by multiple LIDs may be simplified to associate the data with a reference entry which in turn is associated with N other references through other persistent metadata e.g. persistent notes . Relocating cloned data may therefore comprise updating a single mapping between the reference entry and the new media address of the data.

A clone operation may comprise linking one or more LID entries in the logical address space to reference entries in the reference index . The reference entries may comprise the media address es of the cloned data. Accordingly LIDs that are associated with cloned data may reference the cloned data indirectly through the reference index . Such entries may be referred to as indirect entries. As used herein an indirect entry refers to an entry in the index that references and or is linked to a reference entry in the reference index . Indirect entries may be assigned a LID within the logical address space and may be accessible to the storage clients .

As disclosed above after cloning a particular address range storage clients may perform storage operations within one or more of the cloned ranges which may cause the clones to diverge from one another in accordance with the clone mode . In a copy on write mode changes made to a particular clone may not be reflected in the other cloned ranges. In the embodiment changes made to a clone may be reflected in local entries within an indirect entry. As used herein a local entry or local LID refers to a portion of an indirect entry that is directly mapped to one or more media addresses on the storage device . Accordingly local entries and or local LIDs may be configured to reference data that has been changed in a particular clone and or differs from the contents of other clones.

The translation module may be configured to access data associated with cloned data. In some embodiments the translation module is configured to determine the media addresses associated with an indirect entry by use of the corresponding reference entries in the reference index . The translation module may further comprise a cascade lookup module configured to manage indirect entries that comprise local LIDs. The cascade lookup module may be configured to traverse local LIDs of indirect entries first and if the LID is not found within local entries the cascade lookup module may continue searching within the reference entries to which the indirect entry is linked.

The log storage module and groomer module may be configured to manage the contextual format of cloned data. In the embodiment cloned data data that is referenced by two or more LIDs and or LID ranges within the index may be stored in a contextual format that associates the data with the corresponding reference entries. Accordingly the logical interface metadata stored with cloned data segments may correspond to a single reference entry as opposed to identifying each LID and or LID range of the clone. Creating a clone may therefore comprise updating the contextual format of the cloned data in one or more background operations by use of inter alia the groomer module .

The storage layer may provide access to the data at media address through either LID 10 or LID 400 and by reference to the reference entry . In response to a request pertaining to LID 10 or LID 400 the translation module may determine that the corresponding entry in the index is an indirect entry that is associated with an entry in the reference index . In response the cascade lookup module may determine the media address associated with the LID by use of local entries if any and the corresponding reference entry .

The data stored at media address may be stored in a contextual format that is inconsistent with the clone configuration e.g. the data may be associated with LID 10 2 as opposed to the reference entry and or LID 400 . The data may be stored in an updated contextual format in state D in one or more background and or grooming operations. The data may be stored with persistent metadata that associates the data with the reference entry as opposed to the separate LIDs ranges 10 2 and 400 2. Relocating the cloned data may only require updating a single entry in the reference index as opposed to multiple entries corresponding to each LID that references the data e.g. entries and . Moreover any number of LIDs in the index may reference the cloned data without increasing the size of the persistent metadata associated with the cloned data and or complicating the operation of the groomer module .

The reference entry may be assigned identifiers 0Z 1023Z. As disclosed above the identifier s of the reference entry may correspond to a particular portion of the logical address space or may correspond to a different separate namespace. The storage layer links the entries and to the reference entry by use of inter alia metadata and . Alternatively or in addition the indirect entries and may replace media address information with references and or links to the reference entry . The reference entry may not be directly accessible by storage clients via the storage layer and or interface .

The clone operation may further comprise modifying the logical interface D of the data segment the modified logical interface D may allow the data segment to be referenced through the LIDs 1024 2048 of the indirect entry and or the LIDs 6144 7168 of the indirect entry . Although the reference entry may not be used by storage clients to reference the data segment depicts the reference entry as part of the modified logical interface D of the data segment since the reference entry is used to access the data by the translation module through the indirect entries and .

Creating the clone may further comprise storing a persistent note on the storage device . As disclosed above the persistent note may identify the reference entry associated with the data segment . Accordingly the persistent note may associate the media addresses with the identifier s of the reference entry . The clone operation may further comprise storing another persistent note configured to associate the LIDs of entries and LIDs 1024 2048 and 6144 7168 with the reference entry . Alternatively metadata pertaining to the association between entries and and the reference entry may be included in the persistent note . The persistent notes and or may be retained on the storage device until the data segment is relocated in an updated contextual format and or the index and or reference index are persisted. As disclosed above storage of the persistent note s and or may ensure that the clone operation is persistent and crash safe.

The modified logical interface D of the data segment may be inconsistent with the contextual format of the data A the logical interface metadata A of the persistent metadata A may reference LIDs 1024 2048 rather than the identifiers of the reference entry and or the cloned entry . The storage controller may be configured to store the cloned data segment in an updated contextual format B that is consistent with the modified logical interface D the logical interface metadata B of the persistent metadata B may associate the data segment with the reference entry as opposed to separately identifying the LIDs within each cloned range LIDs of entries and . Accordingly the use of the indirect entry allows the logical interface D of the data segment to comprise any number of LIDs independent of size limitations of the contextual data format A B e.g. independent of the number of LIDs that can be included in the logical interface metadata . Moreover additional logical copies of the reference entry may be made without updating the contextual format B of the data such updates may be made by associating the LID ranges with the reference entry in the index and or by use of inter alia persistent notes .

As disclosed above the indirect entries and or may initially reference the data segment through the reference entry . Storage operations performed after creating the clones and or may be reflected by use of local LIDs within the respective entries and or . depicts one embodiment of the result of a storage operation pertaining to LIDs 1024 1052 performed after completing the clone operation of . After completion of the clone operation a storage client may modify data associated with one or more of the clones. In the embodiment a storage client modifies and or overwrites data corresponding to LIDs 1024 1052 of entry which may comprise appending a new data segment to the storage device . The data segment may be stored in a contextual format comprising persistent metadata configured to associate the data segment stored at media addresses with logical interface metadata LIDs 1024 1052 . The storage layer may be configured to associate the data segment with the LIDs 1024 1052 in a local LID entry . The local LID entry may reference the updated data directly as opposed to referencing the data through a reference entry e.g. reference entry .

In response to a request pertaining to data 1024 1052 or sub set thereof the cascade lookup module may search for references to the LIDs in a cascading lookup operation which may comprise searching for references to local LIDs if available followed by the reference entries . In the embodiment the local entry may be used to satisfy requests pertaining to LIDs 1024 1052 media addresses rather than 64432 64460 per the reference entry . Requests for LIDs that are not found in local entries e.g. LIDs 1053 2048 may continue to be serviced through the reference entry . Accordingly the storage layer may use the indirect entry and reference entry to implement a cascade lookup for LIDs pertaining to the clone range 1024 2048. The logical interface E of the data may therefore comprise one or more local entries and or one or more indirect and or reference entries.

In a further embodiment illustrated in a storage client may modify data of the clone through another one of the LIDs of the logical interface E e.g. LIDs 6144 6162 the logical interface delimiters are not shown in to avoid obscuring the details of the illustrated embodiment. The modified data may be referenced using a local entry as disclosed above. Since each of the clones now has its own respective version of the original clone data 0Z 52Z neither clone references that portion of the reference entry . The storage layer may determine that the corresponding clone data and reference identifiers are no longer being referenced and may be removed as depicted in . The clones may continue to diverge until neither nor references any portion of the reference entry at which point the reference entry may be removed.

Although depict local entries and that overlap with the corresponding indirect entries and the disclosure is not limited in this regard. In some embodiments the storage operation of may be reflected by creating the local entry and modifying the indirect entry to reference LIDs 1053 2048. Similarly the operation of may comprise creating the local entry and modifying the indirect entry to reference LIDs 6163 7168.

Referring back to the translation module may be configured to groom the reference index . In some embodiments each entry in the reference index comprises metadata that includes a reference count not shown . The reference count may be incremented as new references or links to the reference entry are added and may be decremented in response to removing references to the entry. In some embodiments reference counts may be maintained for each reference identifier in the reference index . Alternatively reference counts may be maintained for reference entries as a whole. When the reference count of a reference entry reaches 0 the reference entry or a portion thereof may be removed from the reference index . Removing a reference entry or portion of a reference entry may comprise invalidating the corresponding data on the storage device as disclosed herein indicating that the data no longer needs to be retained on the storage device .

In another example the storage layer may remove reference entries using a mark and sweep approach. The storage layer or other process such as the translation module and or groomer may periodically check references to entries in the reference index by inter alia following links to the reference entries from indirect entries or other types of entries in the index . Entries that are not referenced by any entries during the mark and sweep may be removed as disclosed above. The mark and sweep may operate as a background process and may periodically perform a mark and sweep operation to garbage collect reference entries that are no longer in use.

The reference index disclosed in conjunction with may be created on demand e.g. in response to creation of a clone or other indirect data reference . In other embodiments all data may be referenced through intermediate two layer mappings. In such embodiments storage clients may allocate indirect virtual identifiers VIDs in a virtual address space which may be linked to and or reference media addresses through an intermediate mapping layer such as the logical address space of the storage layer . These embodiments may result in an additional mapping layer between storage clients and the storage device s . Storage clients may reference data using VIDs of a virtualized address space that map to logical identifiers of the logical address space which in turn are associated with media addresses on respective storage device s .

The indirection layer may provide access to the virtual address space through the interface . The interface may comprise one or more of a block device interface virtual storage interface cache interface and the like as disclosed herein. The clone module may be configured to manage clone operations within the virtual address space . Although depicts the indirection layer separately from the storage layer the disclosure is not limited in this regard. In some embodiments virtual address space VID index VID translation module and or the clone module may be implemented as part of the storage layer .

The VIDs of the virtual address space may be used to inter alia perform efficient cloning operations. Alternatively or in addition the additional mapping layer may be leveraged to enable logical clone operations on random access write in place storage devices such as hard disks.

Storage clients may perform storage operations in reference to VIDs of the virtual address space . Accordingly storage operations may comprise two or more translation layers. The VID index may comprise a first translation layer between VIDs of the virtual address space and LIDs of the logical address space . The index of the storage layer may implement a second translation layer between the LIDs and media address es on respective storage devices .

The indirection layer may be configured to manage allocations within the virtual address space by use of inter alia the VID metadata VID index and or VID translation module . The VID translation module may be configured to maintain associations between VIDs of the virtual address space and LIDs of the logical address space by use of the VID index . In some embodiments allocating a VID in the virtual address space may comprise allocating one or more corresponding LIDs in the logical address space . Accordingly each VID allocated in the virtual address space may be mapped to one or more LIDs in the logical address space . The mappings may be sparse and or any to any as disclosed herein. The logical address space may not be directly accessible to the storage clients e.g. the logical address space may be used as an intermediate mapping layer . Performing a storage operation through the indirection layer may comprise a identifying the LIDs corresponding to one or more VIDs referenced in the storage operation by use of the VID translation module and or VID index and b implementing the storage operation within the storage layer in reference to the identified LIDs.

In state A the VID index may comprise an entry that represents two VIDs 10 and 11 in the virtual address space . The VID index may be configured to map the VID entry to LIDs within the logical address space using the VID index . In the embodiment the VID index maps the VID entry to the LID entry . The entry may be allocated to a storage client which may perform storage operations in reference to the VIDs. In state A the storage layer may be configured to map the LID entry to one or more media addresses on the storage device media address .

In state B the indirection layer is configured to implement a clone operation. The clone operation may comprise creating a clone of the VID entry . In the clone is identified as VID index entry . The clone operation may further comprise associating the cloned entry with corresponding LID entry in the VID index . The corresponding entry in the index may remain unchanged. Alternatively a reference count or other indicator of the LID entry may be updated to indicate that the entry is being referenced by multiple VID entries. The contextual format of the data stored at media address may be left unchanged e.g. continue to associate the data with the LID entry . The clone operation may further comprise storing a persistent note and or on the storage device to persist the association between VID entry and the LID entry . Alternatively or in addition the clone operation may be made persistent and or crash safe by persisting the VID index .

In state C the data at media address may be relocated to media address . The relocation may occur in a standard grooming operation and not to update the contextual format of the cloned data. Relocating the data may comprise updating a single entry in the index .

The clone implementations disclosed herein may be used to efficiently implement storage operations such as range clone operations range move operations snapshots deduplication atomic writes and the like.

The embodiments for clone operations disclosed herein may be leveraged to manage snapshots of the logical address space or virtual address space . Creating a snapshot of a address range may comprise maintaining an immutable copy of the AR and the corresponding data. As used herein an address range or AR refers to a logical address range a virtual address range or the like.

The embodiments for managing range clone and or range move operations disclosed herein may be leveraged to perform one or more higher level operations such as deduplication operations. Referring back to the storage layer may comprise a deduplication module configured to identify duplicate data on the storage device and or non volatile storage media . Duplicate data may be identified using any suitable mechanism. In some embodiments duplicated data is identified by scanning the contents of the storage device generating signature values for various data segments and comparing data signature values to identify duplicates. The signature values may include but are not limited to cryptographic signatures hash codes cyclic codes and or the like. Signature information may be stored within storage metadata such as the index e.g. in metadata associated with the entries and or may be maintained and or indexed in one or more separate datastructures not shown . The deduplication module may compare data signatures and upon detecting a signature match may perform one or more deduplication operations. The deduplication operations may comprise verifying the signature match e.g. performing a byte by byte data comparison and performing one or more range clone operations to reference the duplicate data within two or more LIDs and or LID ranges.

In response to identifying and or verifying that entries and reference duplicate data the storage layer may be configured to deduplicate the data which may comprise creating one or more range clones. As disclosed above creating a range clone may comprise modifying the logical interface G of the duplicated data segment to associate a single version of the data segment with both sets of LIDs 1024 2048 and 6144 7168.

The range clone may be implemented using any of the clone embodiments disclosed herein including the range clone embodiments of the reference entry embodiments of and or the two layer mapping embodiments of . In the de deduplication embodiment of both LID ranges 1024 2048 and 614407168 may be modified to reference a single version of the data segment the other data segment may be removed and or groomed from the storage device .

The embodiment uses the reference entry implementation of . As such the deduplication operation may comprise creating a reference entry to represent the deduplicated data segment the cloned data . The deduplication operation may further comprise modifying and or converting the entries M and M to indirect entries and which may be mapped to the data segment through the reference entry as disclosed above. The deduplication operations may comprise modifying the logical interface G to reference the data segment through both sets of LIDs 1024 2048 and 6144 7168 as well as the reference entry . The deduplication operations may further comprise storing a persistent note on the non volatile storage media to associate the data segment with the updated logical interface G e.g. associate the data segment with the reference entry and or the linked indirect entries and as disclosed herein. The deduplication operations may further comprise updating the contextual format of the data segment to be consistent with the modified logical interface G as disclosed above. Updating the contextual format may comprise relocating e.g. rewriting the data segment in an updated contextual format to new media storage locations e.g. media storage locations in one or more background operations. The updated contextual format may comprise persistent metadata that includes logical interface metadata to associates the data segment with the reference entry e.g. identifiers 0Z 1023Z .

Although depict cloning and or deduplicating a single entry or range of LIDs the disclosure is not limited in this regard. In some embodiments a plurality of LID ranges may be cloned in a single clone operation. For example referring back to a cloning operation may clone the entry along with all of its child entries. In another example a clone operation may comprise copying the entire contents of the index e.g. all of the entries in the index . This type of clone operation may be used to create a snapshot of a logical address space or a particular LID range . As used herein a snapshot refers to the state of a storage device or set of LIDs at a particular point in time. The snapshot may persist the state of a logical address range despite changes to the original.

At time t A the storage layer may be configured to create a snapshot of the logical address range LAS. As used herein a snapshot of an address range refers to an operation that is configured to maintain the state of the address range at a particular time e.g. freeze the address range . The snapshot operation may comprise preserving the state of the LAS at a particular time. The snapshot operation may further comprise preserving the logical address range while allowing subsequent storage operations to be performed within the logical address range.

As disclosed above the storage layer may be configured store data in an ordered log by use of inter alia the log storage module . The log order of storage operations may be determined using sequence information associated with the data such as sequence indicators on storage divisions of a solid state storage medium e.g. logical storage element of and or sequential storage locations within the physical address space of the storage device as disclosed in conjunction with .

The storage controller may be further configured to maintain other types of ordering and or timing information such as the relative time ordering of data in the log. However in some embodiments the log order of data may not accurately reflect data information. As disclosed above the groomer module may be configured to relocate data on the storage device . Relocating data may comprise reading the data from its original storage location on the storage device and appending the data at a current append point in the log. As such older relocated data may be stored with newer current data in the log.

In some embodiments the log storage module is configured to associate data with timing information which may be used to establish relative timing information of the storage operations performed in the log. In some embodiments the timing information may comprise respective timestamps maintained by the timing module which may be applied to each data packet stored in the log. The timestamps may be stored within persistent metadata of the data packets e.g. in packet headers . Alternatively or in addition the timing module may be configured to track timing information at a higher level of granularity. In some embodiments the timing module maintains one or more global timing indicators an epoch identifier . As used herein an epoch identifier refers to an identifier used to determine relative timing of storage operations performed through the storage layer . The log storage module may be configured to include an indicator of the current epoch identifier in the persistent metadata the epoch indicator may correspond to the epoch in which the data segment was written to the log. The timing module may be configured to increment the global epoch identifier in response to certain events such as the creation of new snapshots user requests and or the like. The epoch indicator of the data segment may remain unchanged through relocation and or other grooming operations. Accordingly the epoch indicator may correspond to the original storage time of the data segment independent of the relative position of the contextual data format packet in the log.

As disclosed above a snapshot operation may comprise preserving the state of a particular logical address range LAS at a particular time. A snapshot operation may therefore comprise preserving data pertaining to the LAS on the storage device . Preserving the data may comprise a identifying data pertaining to a particular timeframe epoch and b preserving the identified data on the storage device e.g. preventing the identified data being removed from the storage device in inter alia grooming operations . Data that needs to be preserved for a particular snapshot may be identified by use of the epoch indicators disclosed above.

In state A time t denoted by epoch indicator e the storage layer may receive a request to implement a snapshot operation through the interface . In response to the request the snapshot module may determine the current value of the epoch identifier maintained by the timing module . The current value of the epoch identifier may be referred to as the current snapshot epoch. In the embodiment the snapshot epoch is 0. The snapshot module may be further configured to cause the timing module to increment the current global epoch indicator e.g. increment the epoch identifier to 1 . Creating the snapshot may further comprising storing a persistent note on the storage device . The persistent note may indicate the current updated epoch indicator and may further indicate that data pertaining to the snapshot epoch is to be preserved. The persistent notes may be using during a metadata reconstruction operation to a determine the current epoch identifier and to b configure the snapshot module and or groomer module to preserve data associated with the snapshot epoch e.

The snapshot module may be further configured to instruct the groomer to preserve data associated with the snapshot epoch. In response the groomer may be configured to a identify data to preserve for the snapshot snapshot data and b prevent the identified data from being removed from the storage device in inter alia grooming operations e.g. storage recovery operations . The groomer module may identify snapshot data in reference to the epoch indicators associated with the data. As disclosed in conjunction with data may be written out of place on the storage device . The most current version of data associated with a particular LID or LID range may be determined based on the order of the corresponding data packets within the log. The groomer may be configured to identify the most current version of data within the snapshot epoch as data that needs to be preserved. Data that has been rendered obsolete by other data in the snapshot epoch may be removed. Referring to the embodiment if the data A and A associated with the same LIDs were both marked with the snapshot epoch the groomer module would identify the most current version of the data in epoch as A and would identify the data A for removal.

In state B the snapshot module may be configured to preserve data pertaining to the snapshot LAS data associated with epoch e while allowing storage operations to continue to be performed during subsequent epochs e.g. epoch e . The storage operations may comprise storing data on the storage device. The data may be stored with an indicator of the current epoch e . The snapshot module may be configured to preserve data that is rendered obsolete and or invalidated by storage operations performed during epoch e and subsequent epochs . Referring back to the embodiment the groomer module may identify data A as data to preserve for the snapshot LAS the data A may be the most current version within epoch e . The snapshot module and or groomer may be configured to preserve the data A even of the corresponding LIDs are trimmed and or deleted during epoch e. Similarly the data A may be preserved in response to overwriting the data with a new version A during epoch e.

The snapshot for LAS data marked with epoch indicator e may be preserved until it is deleted. The snapshot may be deleted in response to a request received with the interface . As indicated in state C the epoch may persist on the storage device even after other intervening epochs epochs e eN have been created and or deleted. Deleting the epoch e may comprise configuring the snapshot module and or groomer module to remove invalid obsolete data associated with the epoch e.

The storage operations performed after creating the snapshot at A may modify the logical address space and specifically the index . The modifications may comprise updating LID to media address bindings in response to appending data to the storage device adding LIDs removing and or trimming LIDs and so on. In some embodiments the snapshot module is configured to preserve the LAS index in a separate storage location such as a separate location in the logical address space in a separate namespace or the like. Alternatively the snapshot module may allow the changes to take place in the index without preserving the original version of the index LAS at time t. The snapshot module may be configured to reconstruct the index for LAS at time t using the data stored in the contextual log based data format on the storage device . The LAS at time t may be reconstructed as disclosed above which may comprise sequentially accessing data stored on the storage device in a log order and creating index entries based on persistent metadata associated with the data packet . In the embodiment the LAS may be reconstructed by referencing data packets that are marked with the epoch indicator e or lower . Data associated with epoch indicators greater than e may be ignored since such data corresponds to operations after creation of the snapshot LAS .

The storage layer may be configured to implement a move operation. The move operation may comprise modifying the logical interface to the data B by inter alia replacing the association between the LIDs 1023 1024 and 1025 and the data at the respective media storage locations and with a new logical interface B for the data that includes a new set of LIDs e.g. 9215 9216 and 9217 . The move operation may be performed in response to a request received via the interface and or as part of a higher level storage operation e.g. a request to rename a file operations to balance and or defragment the index or the like .

The move operation may be implemented in accordance with one or more of the cloning embodiments disclosed above. In some embodiments the move operation may comprise associating the media addresses mapped to LIDs 1023 1024 and 1025 with the destination LIDs 9215 9216 and 9217 which may result in modifying the logical interface B of the data in accordance with the move operation. The move operation may further comprise storing a persistent note on the storage device to ensure that the move operation is persistent and crash safe. The data stored at media addresses and may be re written in accordance with the updated logical interface B in one or more background operations as disclosed above.

As disclosed herein the contextual format of the data on the media addresses and may be inconsistent with the updated logical interface B the contextual format of the data may associate the respective data segments with LIDs 1023 1024 and 1025 as opposed to 9215 9216 and 9217. The persistent note may comprise the updated logical interface for the data so that the storage metadata e.g. index can be correctly reconstructed if necessary.

The storage layer may provide access to the data in the inconsistent contextual format through the modified logical interface B LIDs 9215 9216 and 9217 . The data may be rewritten and or relocated in a contextual format that is consistent with the modified logical interface B subsequent to the move operation outside of the path of the move operation and or other storage operations . In some embodiments the data at media addresses and or may be rewritten by a groomer module in one or more background grooming operations as described above. Therefore the move operation may complete and or return an acknowledgement in response to updating the index is updated and or storing the persistent note .

As illustrated in the index may be updated in response to storing data in the consistent contextual format. The data segment at media storage location may be relocated in a grooming operation which may comprise storing the data in a contextual format that is consistent with the modified logical interface B of the move operation e.g. includes persistent metadata comprising the logical interface that associates the data segment with LID 9215 . The index may be updated to reference the data in the updated contextual format which may comprise modifying the entry for LID 9215 such that it no longer is linked to the reference entry for 1023. The entry for LID 9215 may revert from an indirect node to a standard index entry and the reference entry for LID 1023 may be removed.

Referring to a storage client may modify data associated with LID 9217 which may comprise storing the modified data out of place at media address . The data may be written in a contextual format that is consistent with the modified logical interface B e.g. associates the data with LID 9217 . In response the index may be updated to associate the entry with the media storage location of the modified data e.g. media storage location and to remove the reference entry for LID 1025 as disclosed above.

In some embodiments the reference index may be maintained separately from the index such that the entries therein e.g. entries cannot be directly referenced by storage clients . This segregation of the logical address space may allow storage clients to operate more efficiently. For example rather than stalling operations until data is rewritten and or relocated in the updated contextual format data operations may proceed while the data is rewritten in one or more processes outside of the path for servicing storage operations and or requests. Referring to following the move operation disclosed above a storage client may store data in connection with the LID 1024. The reference entry corresponding to LID 1024 may be included in the reference index due to inter alia the data at not yet being rewritten in the updated contextual format. However since the reference index is maintained separately from the index a name collision may not occur and the storage operation may complete. The index may include a separate entry comprising the logical interface for the data stored at media storage location while continuing to provide access to the data formerly bound to 1024 through the reference index through the logical interface B.

When the entries are no longer linked any entries in the reference index due to inter alia rewriting relocating modifying deleting and or overwriting the data the last of the reference entries may be removed and the entries may no longer be linked to reference entries in the reference index . In addition the persistent note associated with the move operation may be invalidated and or removed from the storage device as disclosed above.

Referring back to the interface of the storage layer may be configured to provide APIs and or interfaces for performing the storage operations disclosed herein. The APIs and or interfaces may be exposed through one or more of the block interface an extended virtual storage interface and or the like. The block interface may be extended to include additional APIs and or functionality use of interface extensions such as fadvise parameters I O control parameters and the like. The interface may provide APIs to perform range clone operations range move operations range merge operations apply attributes and or metadata to ranges e.g. freeze a range manage range snapshots and the like. As disclosed above a range clone operation comprises creating a logical copy of a set of one or more sources LIDs. Range clone operations may be implemented using any of the embodiments disclosed herein including but not limited to the range clone embodiments depicted in including the range merge embodiment of the reference entry embodiments of and or the two layer mapping embodiments of . As disclosed above in conjunction with the disclosed embodiments may be further configured to implement range move operations.

The lower level interfaces disclosed herein may be used to implement higher level operations such as deduplication file level snapshots efficient file copy operations logical file copies address space management mmap checkpoints atomic writes and the like. These higher level operations may also be exposed through the interface of the storage layer .

In other embodiments the clone operation may leverage a reference index e.g. as disclosed in . Before the range clone operation in state C the LIDs of the source file may be directly mapped to the corresponding file data in the index . Creating the range clone in state D may comprise creating a reference entry in the reference index and associating the source file LIDs and destination file LIDs with the reference entry. The range clone operation may further comprise storing a persistent note on the storage device and or updating the contextual format of the file data as disclosed herein.

The storage layer may implement the clone operation using a two layer mapping embodiment e.g. as disclosed in . Initially the source file may correspond to virtual identifiers VIDs in a virtual address space VID index which may be mapped to file data LIDs in the logical address space in the index . Performing the range clone operation may comprise associating the destination file VIDs with the LIDs of the intermediate mapping layer. The range clone operation may further comprise storing a persistent note on the storage device indicating that the destination VIDs are associated with the file data LIDs. Since the file data is already bound to the intermediate file data LIDs the contextual format of the file data may not need to be updated.

The file system may be further configured to leverage the storage layer to checkpoint mmap operations. As used herein an mmap operation refers to an operation in which the contents of files are accessed as pages of memory through standard load and store operations rather than the standard read write interfaces provided by the file system . An msync operation refers to an operation to flush the dirty pages of the file if any to the storage device . The use of mmap operations may make file checkpointing difficult. File operations are performed in memory and an msync is issued when the state has to be saved. However the state of the file after msync represents the current in memory state and the last saved state is lost. If the file system were to crash during an msync the file could be left in an inconsistent state.

In some embodiments the file system is configured to checkpoint the state of an mmap ed file during calls with msync. Checkpointing the file may comprise creating a file level snapshot and or range clone as disclosed above. The file level snapshot may be configured to save the state of the file before the changes are applied. When the msync is issued another clone may be created to reflect the changes applied in the msync operation. As depicted in in state prior to the mmap operation file may be associated with LIDs 10 13 and corresponding media addresses P1 P4. In response to the mmap operation the file system may perform a range clone operation through the interface of the storage layer which may comprise creating a cloned file .. The cloned file . may be associated with a different set of LIDs 40 43 that reference the same data same media addresses P1 P4 . In other embodiments the files may be cloned using a reference entry embodiment and or two layer mapping embodiment as disclosed above.

In response to an msync call the file system may perform another range clone operation through the interface . As illustrated in state C the range clone operation associated with the msync operation may comprise updating the file with the contents of one or more dirty pages media addresses P5 and P6 and cloning the updated file as file .. The file . may reflect the state of the file before the msync operation. Accordingly in the event of a failure the file system may be capable of reconstructing the previous state of the file .

The storage layer may be further configured to implement efficient atomic storage operations. Referring to in some embodiments the storage layer comprises an atomic storage module . As used herein an atomic storage operation refers to a storage operation that is either fully completed or is rolled back as a whole. Accordingly atomic storage operations may not remain in a partially completed state. Implementing atomic storage operations and particularly atomic storage operations comprising multiple steps and or pertaining to multiple different LID ranges or vectors may impose high overhead costs. For example some database systems implement atomic storage operations using multiple sets of redundant write operations. The atomic storage module may leverage the range clone range move and or other operations disclosed herein to increase the efficiency of atomic storage operations.

In some embodiments the interface provides APIs and or interfaces for performing vectored atomic storage operations. A vector may be defined as a data structure such as 

The iov base parameter may reference a memory or buffer location comprising data of the vector iov len may refer to a length or size of the data buffer and dest lid may refer to the destination logical identifier s for the vector e.g. base logical identifier the length of the logical identifier range may be implied and or derived from the input buffer iov len .

The vector write operation above may be configured to gather data from each of the vector data structures referenced by the iov pointer and or specified by the vector count parameter iov cnt and write the data to the destination logical identifier s specified in the respective iovect structures e.g. dest lid . The flag parameter may specify whether the vector write operation should be implemented as an atomic vector operation.

As illustrated above a vector storage request may comprise performing the same operation on each of a plurality of vectors e.g. implicitly perform a write operation pertaining to one or more different vectors . In some embodiments a vector storage request may specify different I O operations for each constituent vector. Accordingly each iovect data structure may comprise a respective operation indicator. In some embodiments the iovect structure may be extended as follows 

The iov flag parameter may specify the storage operation to perform on the vector. The iov flag may specify any suitable storage operation which include but is not limited to a write a read an atomic write a trim or discard request a delete request a format request a patterned write request e.g. request to write a specified pattern a write zero request or an atomic write operation with verification request allocation request or the like. The vector storage request interface described above may be extended to accept vector structures 

The flag parameter may specify whether the vector operations of the vector request are to be performed atomically.

The atomic storage module may be configured to redirect storage operations pertaining to an atomic storage operation to a pre determined range an in process range . The in process range may be a designated portion of the logical address space that is not accessible to the storage clients . Alternatively the in process range may be implemented in a separate address namespace. After the atomic storage operation has been completed within the in process range e.g. all of the constituent I O vectors have been processed the atomic storage module may perform an atomic range move operation to move the data from the in process range to the destination range s . As disclosed above the range move operation may comprise writing a single persistent note to the storage device .

A storage client may issue an atomic write request pertaining to vectors A and B. As illustrated in before the atomic storage operation is performed at state A the LIDs 10 13 of vector A may be bound to media addresses P1 P4 and the LIDs 36 38 of vector B may be bound to media addresses P6 8. As depicted in state B the atomic storage module may be configured to redirect the atomic storage operations to an in process index . As disclosed above the in process index may comprise a designated region of the logical address space and or may be implemented within a separate index and or address namespace. The vector A within the in processes index may correspond to the LIDs 10 13 of vector A and the in process vector B may correspond to the LIDs 36 38 of vector B. The vectors A and B may comprise metadata configured to reference the corresponding vectors A and B in the index . Implementing the atomic storage operations in state B may comprise appending data to the storage device in association with the in process LIDs Z0 Z3 and or Z6 Z6 of the in process vectors A and B. Other storage operations may be performed concurrently with and or interleaved within the atomic vector operations within the in process index .

If the atomic storage operation fails before completion the original data of vectors A and B may be unaffected. During reconstruction the data associated with the in process entries the data at P9 P13 and or P100 P102 may identified as part of an incomplete atomic storage operation due to the association between the data and identifiers within the in process index and the data may be removed.

As illustrated in in state B the atomic storage operation s may be completed within the in process index . Completion of the atomic storage request may comprise performing a range move operation to move the data written to the in process vectors A and B into the logical address space . The range move operation may comprise performing an atomic storage operation to store a persistent note on the storage device to bind the media address P9 P13 to LIDs 10 13 and P100 102 to LIDs 36 38. The range move operation may be implemented in other ways including but not limited to the reference entry embodiments of and or the two layer mapping embodiments of .

Step may comprise modifying a logical interface of data stored in a contextual format on a non volatile storage media. The logical interface may be modified at step in response to performing an operation on the data which may include but is not limited to a clone operation a deduplication operation a move operation or the like. The request may originate from a storage client the storage layer e.g. deduplication module or the like.

Modifying the logical interface may comprise modifying the LID s associated with the data which may include but is not limited to referencing the data using one or more additional LIDs e.g. clone deduplication etc. changing the LID s associated with the data e.g. a move or the like. The modified logical interface may be inconsistent with the contextual format of the data on the non volatile storage media as described above.

Step may further comprise storing a persistent note on the non volatile storage media that identifies the modification to the logical interface. The persistent note may be used to make the logical operation persistent and crash safe such that the modified logical interface e.g. storage metadata of the data may be reconstructed from the contents of the non volatile storage media if necessary . Step may further comprise acknowledging that the logical interface has been modified e.g. returning from an API call returning an explicit acknowledgement or the like . The acknowledgement occur and access through the modified logical interface at step before the contextual format of the data is updated on the non volatile storage media . Accordingly the logical operation may not wait until the data is rewritten and or relocated as discussed below updating contextual format of the data may be deferred and or implemented in a processes that is outside of the critical path of the method and or the path for servicing other storage operations and or requests.

Step may comprise providing access to the data in the inconsistent contextual format through the modified logical interface of step . As described above updating the contextual format of the data to be consistent with the modified contextual interface may comprise rewriting and or relocating the data on the non volatile storage media which may impose additional latency on the operation of step and or other storage operations pertaining to the modified logical interface. Therefore the storage layer may be configured to provide access to the data in the inconsistent contextual format while or before the contextual format of the data is updated. Providing access to the data at step may comprise referencing and or linking to one or more reference entries corresponding to the data via one or more indirect entries as described above.

Step may comprise updating the contextual format of the data on the non volatile storage media to be consistent with the modified logical interface of step . Step may comprise rewriting and or relocating the data to another media storage location on the non volatile storage media and or on another non volatile storage device A N. As described above step may be implemented using a process that is outside of the critical path of step and or other storage requests performed by the storage layer step may be implemented by another autonomous module such as groomer module deduplication module or the like. Accordingly the contextual format of the data may be updated independent of servicing other storage operations and or requests. As such step may comprise deferring an immediate update of the contextual format of the data and updating the contextual format of the data in one or more background processes such as a groomer process. Alternatively or in addition updating the contextual format of the data may occur in response to e.g. along with other storage operations. For example a subsequent request to modify the data may cause the data to be rewritten out of place and in the updated contextual format e.g. as described above in connection with .

Step may further comprise updating storage metadata as the contextual format of the data is updated. As data is rewritten and or relocated in the updated contextual format the storage layer may update the storage metadata e.g. index accordingly. The updates may comprise removing one or more links to reference entries in a reference index and or replacing indirect entries with local entries as described above. Step may further comprise invalidating and or removing a persistent note from the non volatile storage media in response to updating the contextual format of the data and or persisting the storage metadata as described above.

Step comprises selecting a storage division for recovery such as an erase block or logical erase block. As described above the selection of step may be based upon a number of different factors such as a lack of available storage capacity detecting a percentage of data marked as invalid within a particular logical erase block reaching a threshold a consolidation of valid data an error detection rate reaching a threshold improving data distribution data refresh or the like. Alternatively or in addition the selection criteria of step may include whether the storage division comprises data in a contextual format that is inconsistent with a corresponding logical interface thereof as described above.

As discussed above recovering or reclaiming a storage division may comprise erasing the storage division and relocating valid data thereon if any to other storage locations on the non volatile storage media. Step may comprise determining whether the contextual format of data to be relocated in a grooming operation should be updated e.g. is inconsistent with the logical interface of the data . Step may comprise accessing storage metadata such as the indexes described above to determine whether the persistent metadata e.g. logical interface metadata of the data is consistent with the storage metadata of the data. If the persistent metadata is not consistent with the storage metadata e.g. associates the data with different LIDs as described above the flow continues at step otherwise the flow continues at step .

Step may comprise updating the contextual format of the data to be consistent with the logical interface of the data. Step may comprise modifying the logical interface metadata to reference a different set of LIDs and or reference entries as described above.

Step comprises relocating the data to a different storage location in a log format that as described above preserves an ordered sequence of storage operations performed on the non volatile storage media. Accordingly the relocated data in the updated contextual format may be identified as the valid and up to date version of the data when reconstructing the storage metadata if necessary . Step may further comprise updating the storage metadata to bind the logical interface of the data to the new media storage locations of the data remove indirect and or reference entries to the data in the inconsistent contextual format and so on as disclosed herein.

Step may comprise determining and or verifying that the non volatile storage media comprises duplicate data or already comprises data of a write and or modify request . Accordingly step may occur within the path of a storage operation e.g. as or before duplicate data is written to the non volatile storage media and or may occur outside of the path of servicing storage operations e.g. identify duplicate data already stored on the non volatile storage media . Step may comprise generating and or maintaining data signatures in storage metadata and using the signature to identify duplicate data.

In response to identifying the duplicate data at step the storage layer or other module such as the deduplication module may modify a logical interface of a copy of the data such that a single copy may be referenced by two or more sets of LIDs. The modification to the logical interface at step may comprise updating storage metadata and or storing a persistent note on the non volatile storage media as described above. Step may further comprise invalidating and or removing other copies of the data on the non volatile storage media as described above.

The contextual format of the data on the non volatile storage media may be inconsistent with the modified logical interface. Therefore steps and may comprise providing access to the data in the inconsistent contextual format through the modified logical interface and updating the contextual format of the data on the non volatile storage media as described above.

Referring back to the cloning embodiments depicted in in other examples clone operations may be used to perform atomic operations such as multi step writes or transactions. An atomic operation to modify a data in a particular logical address range may comprise creating a clone of the logical address range implementing storage operations within the clone and when the operations complete folding the clone back into the logical address space e.g. overlaying the original logical address range with the clone . As used herein folding a logical address range refers to combining two or more address ranges together e.g. folding a logical address range with a clone thereof . The folding may occur according to one of a plurality of operational modes which may include but are not limited to an overwrite mode in which the contents of one of one logical address range overwrites the contents of another logical address range a merge mode in which the contents of the logical address ranges are merged together e.g. in a logical OR operation or the like.

In another example in which the LID range of the clone was modified e.g. data was appended or deleted from the clone the LID 2814 would be modified in a corresponding way. Accordingly a folding operation may comprise allocation of additional LIDs in the logical address space . Therefore in some embodiments clones may be tied to one another e.g. using entry metadata and or . An extension to a clone such as entry may be predicated on the logical address range being available to the original entry . The link between the entries may be predicated on the mode of the clone as described above. For example if the entries are not to be folded at a later time the clones may not be linked.

As described above clones may be tied together according to an operational mode of the clones. For example changes to a clone may be automatically mirrored in the other clone. This mirroring may be uni directional bi direction or the like. The nature of the tie between clones may be maintained in storage metadata e.g. metadata entries and and or in reference entries . The storage layer may access the metadata entries and or when storage operations are performed within the LID ranges 2815 and or 2825 to determine what if any synchronization operations are to be performed.

In some embodiments data of a clone may be designated as ephemeral as described above. Accordingly if upon reboot or another condition the ephemeral designation is not removed the clone may be deleted e.g. invalidated as described above . is a flow diagram of another embodiment of a method for cloning ranges of a logical address space .

Step may comprise receiving a request to create a clone. The request may be received from a storage client through an interface and or may be part of a higher level API provided by the storage layer . The request may include an operational mode of the clone which may include but is not limited to how the clones are to be synchronized if at all how folding is to occur whether the copy is to be designated as ephemeral and so on.

Step may comprise allowing LIDs in the logical address space to service the request. The allocation of step may further comprise reserving physical storage space to accommodate changes to the clone. The reservation of physical storage space may be predicated on the operational mode of the clone. For instance if all changes are to be synchronized between the clone and the original address range a small portion if any physical storage space may be reserved. Step may further comprise allocating the clone within a designated portion or segment of the logical address space e.g. a range dedicated for use with clones .

Step may comprise updating the logical interface of data of the clone as described above. Step may further comprise storing a persistent note on the non volatile storage media to make the clone persistent and crash safe as described above.

Step may comprise receiving a storage request and determining if a storage request pertains to the original LID range and or the clone of the LID range. If so the flow continues to step otherwise the flow remains on step .

Step may comprise determining what if any operations are to be taken on the other associated LID ranges e.g. synchronize changes allocate logical and or physical storage resources or the like . The determination of step may comprise accessing storage metadata describing the operational mode of the clone and or the nature of the tie if any between the original LIDs and the clone thereof.

Step may comprise performing the operations if any determined at step along with the requested storage operation. If one or more of the synchronization operations cannot be performed e.g. additional logical address space cannot be allocated the underlying storage operation may fail.

At step a request to fold the clone is received. The request may specify an operational mode of the fold and or the operational mode may have been specified when the clone was created at step .

Step comprises folding the clone back into the logical address space of the original logical range. Step may comprise overwriting the contents of the original logical address range with the contents of the clone merging the logical address ranges e.g. in an OR operation or the like. In some embodiments the merging comprises deleting e.g. invalidating the clone which may comprise removing entries of the clone from the storage metadata index removing shared references to media storage locations from a reference count datastructure and the like. Step may further comprise modifying a logical interface of the merged data as described above. The modified logical interface may change the LIDs used to reference the data. The modified logical interface may be inconsistent with the contextual format of the data on the non volatile storage media . Therefore step may further comprise providing access to the data in the inconsistent contextual format and or updating the contextual format of the data as described above.

As disclosed above in some embodiments the storage layer may be configured to segment the logical address logical address space into a plurality of contiguous LID ranges. As illustrated in a LID e.g. address is segmented into a first portion and a second portion . In some embodiments the first portion comprises high order bits of the LID 1900 and the second portion comprises low order bits. The first portion may serve as a reference or identifier and the second portion may represent a range e.g. block size offset within a contiguous range of LIDs. In this manner the storage layer may logically segment or divide the sparse logical address space into segments of contiguous LIDs that can be efficiently allocated as a group. In the embodiment segmenting LIDs into 32 high order and 32 low order bits may result in a logical address space that is capable of representing 2 32 1 unique LID allocation ranges e.g. using the first portion of the LIDs 1952 each of which have a maximum size or offset of 2 32 virtual storage locations e.g. 2 TB for a virtual storage location size of 512 bytes . In other embodiments different segmentation schemes may be used. In embodiments require a large number of small storage entities e.g. database applications messaging applications or the like the first portion may comprise a larger proportion of the LID address range and the second portion e.g. first portion comprising 42 bits providing 2 42 1 unique identifiers . Alternatively where larger storage entitles are used the ratio between the size of the first and second address portions and may be reversed.

The LID segmentation scheme disclosed herein may be used to define an allocation granularity of the logical address space . In the embodiment the allocation granularity is fixed according to the segmentation of the logical addresses each allocation operation in the logical address space comprises allocating X LIDs where X is determined according to the size of the second portion of the logical addresses . The allocation granularity may also determine the number of unique storage entities that can be represented within the logical address space in the embodiment the logical address space is capable of supporting Y unique storage entities Y unique LID ranges of size X where Y is determined according to the size of the first portion .

The fixed allocation granularity may result in wasted storage resources. In embodiments in which each file is allocated a pre determined range of contiguous LIDs e.g. 2 32 1 LIDs a large proportion of the LIDs allocated for small files will likely never be used which may result in increased metadata overhead and or may reduce the number of unique files that can be represented within the logical address space . Similarly large files that do not fit within a single LID allocation range e.g. require more than 2 32 LIDs may have to allocate multiple LID ranges which may result in additional wasted resources. These issues may be compounded in embodiments that have a more limited logical address space e.g. fewer number of bits available to represent LIDs 1900 . In some embodiments for example LIDs may be limited to 48 bits rather than 64 due to inter alia operating system limitations addressing limitation addressing overhead e.g. use of a portion of a LID to represent different virtual storage units and so on.

Accordingly in some embodiments the storage layer may be configured to implement an adaptive and or variable allocation scheme in which different portions of the logical address space are configured to provide a different respective allocation granularity. As used herein allocation granularity refers to the amount of storage resources that are allocated in a single allocation operation. The allocation granularity of a region may refer to the size of LID blocks or ranges allocated in the region. In the embodiment the allocation granularity of the logical address space was determined according to the size of the first and second portions and of the segmented LIDs 1900.

Alternatively or in addition allocation granularity may refer to physical storage allocations and or operations. As disclosed above LIDs in the logical address space may correspond to be bound to physical storage resources such as physical sectors. As used herein a physical sector data sector or sector refers to physical storage capacity capable of storing a particular amount of data. The physical sector size may therefore determine the granularity of data storage operations performed on the storage device the data sector size may determine the smallest granularity of write read operations that can be performed on the storage device . As such storage clients may be configured align storage operations in accordance with a particular data sector size. For example in embodiments comprising a 512 byte sector size storage clients may adapt storage operations to fall within the 512 byte boundaries. In some storage systems the sector size is based on physical characteristics the underlying storage devices a storage device may for example be physically partitioned into sectors or pages having a particular pre determined size. By contrast the storage layer disclosed herein may be capable of storing data within large logical constructs such as logical storage divisions and or logical pages of the logical storage element disclosed above in . As such the storage layer may be capable of performing storage operations according to arbitrarily sized physical sectors that are independent of the underlying partitioning of the storage device and or individual non volatile storage elements . Accordingly in some embodiments the physical sector size implemented by the storage layer may be configurable and or variable. As disclosed in further detail below the physical sector size may vary within different regions of the logical address space a LID in a first allocation region of the logical address space may correspond to a 512 byte sector and a LID in a different allocation region may correspond to a 4 kb sector and so on. Storage clients may be configured to operate in different allocation regions in accordance with a preferred physical sector size.

The storage controller may be further configured to store data in a contextual log based format a packet format . As disclosed above the data write module may be configured to generate packets corresponding to any suitable physical sector size comprising any sized data segment . The size of the packets may be independent of the underlying partitioning and or arrangement of the non volatile storage elements . Therefore the storage layer may be capable of performing storage operations corresponding to any suitable physical sector size and or physical granularity from a few bytes e.g. 256 byte sector sizes to 50 kb or more. The storage layer may be configured to store a packet comprising a 512 data segment within a logical page along a packet comprising a 2 kb data segment B.

In some embodiments the allocation module comprises a partition module configured to partition and or segment the logical address space into two or more allocation regions. The allocation regions may correspond to different allocation granularities. The allocation granularity of a particular region may refer to the allocation of physical storage resources e.g. physical sector size and or logical allocation granularity such as LID allocation block size. The allocation module may further comprise an allocation policy module configured to determine an allocation granularity for storage client storage requests and or storage entities and or to selectively reallocate storage resources. The reallocation module may be configured to reallocate storage resources which may comprise performing one or more of the range clone and or range move operations as disclosed herein.

In the embodiment the LID 3636A in region A is bound to data packet A in the index . The data packet A may comprise a 512 byte data segment A in accordance with the physical allocation granularity of the region A. In some embodiments the persistent metadata of the packets A N comprise respective size indicators A N indicating a size of the corresponding data segments A N. Alternatively or in addition the data segment size may be indicated in the index and or other metadata . The LID 3636B in region B may be bound to data packet B. The data packet may comprise a 2 k data segment B in accordance with the physical sector size of region B. LID 3636N may be bound to data packet N which may comprise a 4 k data segment N in accordance with the physical allocation granularity of region N. The differently sized data packets A N may be stored at arbitrary physical storage locations within the storage device . In some embodiments the data packets A N may be stored within large logical storage units of a logical storage element as disclosed above. Although depicts a particular embodiment of logical address partitioning the disclosure is not limited in this regard and could be adapted to partition the logical address space into any number of different allocation regions A N corresponding to any suitable physical and or logical allocation granularity.

Certain storage clients may operate more efficiently at specific sector sizes. For example an application that processes large amounts of contiguous data may operate most efficiently with large 4 kb sector sizes. Other applications that rely on a large number of relatively small transactions may operate more efficiently using smaller sector sizes. In some embodiments the interface provides mechanisms for specifying a desired sector size for particular storage and or allocation operations. A file system storage client may for example specify that storage operations pertaining to a particular file A be performed at a 2 k sector size. In response the allocation module may allocate LIDs for the file A within the region B of the logical address space . Alternatively or in addition the file system and or other storage clients may query the interface for information pertaining to the available allocation regions A N and or data sector sizes supported by the storage layer . The storage clients may selectively allocate LIDs within the regions A N in accordance with a desired physical allocation granularity sector size . The file system storage client may therefore be configured to allocate LIDs having different sector sizes for different files A N according to the access characteristics of the files A N. As such the file system storage client may be capable of supporting files A N having different respective data sector sizes. In some embodiments users may specify a desired file sector size through inter alia ioctrl parameters an fadvise API and or the like.

Referring back to the log storage module may be configured to provide for storing store data according to the sector size assigned to the data corresponding to the LID associated with the data . The log storage module may determine the sector size in reference to inter alia the storage metadata index and or allocation module . The log storage module may configure the storage device controller data write module to packetize the data in accordance with the sector size for storage within the log on the storage device . The log storage module may be further configured to provide for reading data of various different data sector sizes. In response to a read request pertaining to a particular LID the log storage module may determine the sector size corresponding to the LID as above and may configure the data read module to read the corresponding data packet size.

Although depicts regions A D as being of approximately the same size e.g. the logical address space is equally segmented into four regions A D the disclosure is not limited in this regard. In some embodiments partitioning module may be configured to segment the logical address space into differently sized regions A D and or into different numbers of regions A D. For example the logical address space may be segmented into two regions a first region for large files and a second region for small files and large file region may be allocated a larger proportion of the logical address space than the small file region or vice versa.

Referring back to the each region A D may comprise and or result in a different segmentation of the LIDs 1901A D. The portion of a LID 1901A D comprising the identifier portion of the LID 1952A D versus the offset or range portion of the LID 1954A D may vary depending on the size of the underlying contiguous LID range 3651A D. For example the LIDs 1901A of region A comprise a larger proportion offset bits A as compared to the LIDs 1901D of region D. Conversely the LIDs 1901D of region D comprise a larger number of identifier bits D as compared to the LIDs 1901A of region A. Although not depicted in the LIDs 1901A D may further comprise bits for specifying the region A D of the LID specifying a logical storage unit of the LID and so on. In some embodiments in the four way segmentation of each LID 1901A D may comprise two bits for specifying one of the regions A D. Alternatively or in addition the storage layer may track LID region relationships based upon pre determined LID values or ranges in the index and or other metadata such that no region specifying overhead is needed.

In some embodiments the storage layer may provide access to allocation information through the interface . In some embodiments the interface may be configured to publish information pertaining to the allocation regions of the logical address space indicate the remaining unallocated and or unbound resources within a particular region and or LID block and the like. The interface may be further configured to allow storage clients to specify a desired allocation granularity physical sector size and or the like. In some embodiments for example an allocation request may specify the number of contiguous LIDs requested for allocation. In response the allocation module may allocate the LIDs within the appropriate region. For example the region A may contiguous LID ranges 3561A comprising 65536 LIDs region B may comprise contiguous LID ranges 3651B comprising 16384 LIDs region C may comprise contiguous LID ranges 3651C comprising 4096 LIDs and region D may comprise contiguous LID ranges 3651D comprising 1024 LIDs. In response to a request to allocate 8024 LIDs the storage layer may allocate an available contiguous LID range 3651B within region B. Alternatively the storage layer may allocate a contiguous LID range 3651B in region B and a contiguous LID range 3651C in region C.

In some embodiments the allocation module comprises an allocation policy module that is configured to select a suitable allocation granularity region A D and or physical granularity region A N based on one or more allocation policies which may include but are not limited to availability of contiguous LID ranges in the regions A D whether the LID range is expected to grow information pertaining to the storage client associated with the request information pertaining to an application associated with the request information pertaining to a storage entity associated with the request e.g. file information explicit requests request parameters ioctrl fadvise etc. and or the like. In some embodiments LID allocation requests may specify a particular allocation region e.g. LID region A D . For example a storage client may initially allocate a small LID range but may know that the LID range may be required to grow over time e.g. the storage client may be receiving a stream of data over a network . Accordingly the storage client request an initially small LID allocation but may specify that the LID allocation be serviced in the region A. In some embodiments the allocation module may initially allocate LIDs in the smallest granularity region D and may move storage entities to larger regions as needed. As such even if a storage client requests a larger number of LIDs the allocation module may defer allocation of additional LIDs until needed.

In some embodiments the allocation module may comprise a reallocation module configured to inter alia relocate storage entities between different allocation regions e.g. physical allocation regions A N and or logical allocation regions A D . In one embodiment a file storage entity may be initially managed using LIDs within the region D. However the file may grow to require more than a single contiguous LID range 3651D. In response the storage layer may allocate additional contiguous LID ranges 3651D within the region D. Alternatively reallocation module may determine that the storage entity should be relocated which may comprise a range move operation from the region D to another region A C. As disclosed above in conjunction with the range move operation may comprise a modifying the logical interface of the data corresponding to the storage entity in the index and or other storage metadata b storing persistent note on the storage device associating the data with the updated logical interface c and or rewriting the data in the uploaded logical interface in one or more background operations. Alternatively or in addition the range move operation may comprise modifying a two layer mapping between the logical identifiers one or more intermediate mapping layers as disclosed above in connection with .

The reallocation module determine that the file should be moved from region C to region D of the logical address space by use of inter alia the policy module . The policy module may identify files that should be reallocated moved based on one or more of requests to allocate additional capacity for the file in response to a balancing operation within the logical address space in response to availability issues e.g. lack of availability in the region C in response to a move request from a storage client and or the like. In some embodiments the reallocation module may be configured to periodically balance the logical address space to move relatively large files files comprising a number of contiguous LID ranges into larger regions so that the files may benefit from larger contiguous LID ranges. Similarly files that have not used their allocated capacity for a predetermined time period may be moved into smaller granularity regions.

Moving the file may comprise allocating one or more contiguous LID ranges 3651B in the region B. In the embodiment the reallocation module is configured to move the file from the region C to the region B. Moving the file may comprise allocating a contiguous LID range 3651D in region B and performing a move operation as disclosed above e.g. modifying the logical interface of the file data storing a persistent note on the storage device and or updating the contextual format of the data to be consistent with the logical interface . Moving the file may allow the file to be managed using contiguous LIDs of a single entry in the region B of the index as opposed to multiple entries .

As illustrated in the file has been moved to region B. Following the range move operation the file may grow by a relatively small increment. The increment may require additional capacity beyond the contiguous LID region A allocated to the file in region B. In response the storage layer may allocate additional LIDs in the region B e.g. another contiguous LID range 3651B . However if the increase to LID capacity is relatively small allocating another relatively large contiguous LID range 3651B may be inefficient e.g. result in a large number of unused LIDs . As such the allocation module may allocate LIDs in a different region. In the embodiment the storage layer allocates the additional LIDs in the region D which comprises relatively small contiguous LID ranges 3651D. The LID allocation may be represented in an entry in the index within a region D of the index . The entry may comprise a range of LIDs 3743A allocated to the file along with corresponding physical storage locations e.g. physical addresses 3745A as described above. The file may therefore be managed using two noncontiguous sets of LIDs. In some embodiments the entries and may be linked through respective metadata reference information or the like to indicate that the entries and correspond to the same file . Alternatively a file system and or the storage layer may maintain references to the entries and e.g. an i node or other datastructure .

In another embodiment allocating additional LIDs comprises moving the file into the region A. Referring to the file may be moved in response to a request to expand the file in response the reallocation module may be configured to move the file from the region B to the region A. The move operation may comprise a allocating a contiguous range of LIDs 3651A in the region A represented by entry and b performing a range move operation to modify the logical interface of the file data to the new LIDs 3615A as disclosed herein.

Although depict range move operations to move data to different logical allocation regions within the logical address space the disclosure is not limited in this regard the same range move operations may be used to move data to from different physical allocation regions A N of . Referring to in some embodiments data stored in a plurality of packets A comprising a 512 byte data segments A may be moved to a smaller number of packets B comprising 2 k data segments B within region B in one or more range move operations as disclosed herein. The range move operation may comprise maintaining the data in the smaller packets A until the data is rewritten in a one or more background processes e.g. grooming operations . Storage metadata associated with the data corresponding entries in the index may be configured to indicate that data of the LIDs in region B are stored with smaller physical segment sizes until the data is rewritten in the updated packet format B.

Referring back to the interface of the storage layer may be configured to provide logical and or physical allocation information to storage clients through the interface . In some embodiments the file system may leverage such information to streamline file management operations. The file system may perform journaling operations to inter alia persist metadata pertaining to allocation operations performed for the files A N managed thereby. The journaling operations may comprise storing metadata pertaining to logical and or physical storage allocation operations. In some embodiments the file system may leverage allocation metadata to streamline such operations. The interface may for example provide an indication of the remaining logical capacity of one or more of the files A N. For example the file A may be allocated within region B of the logical address space and as such may be allocated a particular range of LIDs. The file A may only occupy a limited subset of the allocated LIDs. The file system may query the storage layer through the interface to determine the remaining allocated LID capacity for the file A such that subsequent file expansions can be performed without explicit allocation requests. The file system may be further configured to identify an appropriate allocation region A D in accordance with an expected file size.

In some embodiments the reallocation module may be configured to move data to from different regions A N of the logical address space . The reallocation module may move a storage entities files in response to determining that the storage entity is stored at an unsuitable physical granularity. For example a storage client may perform a large number of small write operations to data stored in a large granularity region N. The small write operations may for example comprise modifying 256 bytes of data within large 4 kb data sectors. The reallocation module may be configured to move the data to the region A that has a smaller 512 byte granularity to improve the performance of the small write operations. The move may comprise a range move operation as disclosed above. The range move may further comprise rewritten one or more data packets N comprising 4 kb data segments N as a plurality of data packets A comprising smaller 512 byte data segments A.

Step may comprise receiving an allocation request. The allocation request may be received with the interface of the storage layer . The allocation request may comprise a request to allocate one or more LIDs. Alternatively the allocation request may comprise a request to perform a storage operation e.g. write data the storage device in a nameless write operation or the like . Step may therefore comprise selecting an allocation region for the request by use of inter alia the policy module . The policy module may be configured to select the allocation region based on one or more request parameters file level knowledge e.g. information about the data to be stored in connection with the allocated LIDs application level knowledge e.g. information about the storage client associated with the request data access characteristics and the like request parameters and the like.

Step may comprise allocating storage resources within one of the defined allocation regions. Step may comprise allocating a contiguous range of LIDs within a particular LID allocation region A D. Alternatively or in addition step may comprise allocating LIDs and or storing data at a particular physical granularity e.g. having a particular data sector size in accordance a selected region A N .

Step may comprise associating a LID with a particular data sector size based on inter alia the regions defined at step . Step may be performed in response to receiving a storage request pertaining to the LID such as request to write and or modify data associated with the LID a request to read data associated with the LID and or the like. The sector size may be determined in reference to storage metadata the index the allocation module and or the like.

Step may comprise performing one or more storage operations in accordance with the determined sector size. Step may comprise configuring the data write module to store data packets in accordance with the identified sector data. Alternatively or in addition step may comprise configuring the data read module to read one or more data packets of a particular size as disclosed above.

Step may comprise allocating one or more LIDs to a storage client within a selected region of the logical address space . Step may comprise selecting a region of the logical address space . Selection of the region may be based upon inter alia a size of the request a request parameter e.g. the storage client may request allocation within a particular region and or allocation of a particular range of contiguous LIDs configuration and or preferences of the storage client availability request parameters ioctrl fadvise and or the like. Allocating the one or more LIDs may comprise allocating a contiguous range of LIDs in accordance with the allocation granularity of the selected region of the logical address space . The contiguous range of LIDs allocated at step may therefore comprise logical capacity that exceeds the number of LIDs requested by the storage client . In some embodiments step may comprise allocating one or more noncontiguous LID ranges within one or more of the regions A D as disclosed above.

Step comprises managing the segmented logical address space . Step may comprise moving one or more storage entities e.g. files in response to allocation changes and or balancing operations as disclosed above.

Step may comprise performing one or more storage operations within the selected region and or in accordance with the allocation granularity of the selected region. Step may comprise allocating a particular range of LIDs in accordance with a particular logical allocation region A D storing data within physical sectors of a predetermined size in accordance with a particular physical allocation region A N and or the like.

Step may comprise moving data corresponding to the storage operations performed at step to a different allocation region. Step may comprise determining that the data should be moved. The determination may be based on receiving a request through the interface to move the data. Alternatively or in addition the determination may be based on profiling metadata pertaining to storage operation s such as access characteristics of the data changes in requested allocation size and or the like. For example a file may be moved from a relatively small logical allocation region to a larger logical allocation region in response to continued expansion of the file. In another embodiment a file may be moved from a large logical allocation region to a smaller logical allocation region in response to a reduction in file size. In other embodiments data may be moved in to regions A N having different data sector sizes in accordance with observed data access characteristics. Step may further comprise performing one or more range move operations to move the data to from different portions of the logical address space as disclosed herein.

This disclosure has been made with reference to various exemplary embodiments. However those skilled in the art will recognize that changes and modifications may be made to the exemplary embodiments without departing from the scope of the present disclosure. For example various operational steps as well as components for carrying out operational steps may be implemented in alternate ways depending upon the particular application or in consideration of any number of cost functions associated with the operation of the system e.g. one or more of the steps may be deleted modified or combined with other steps . Therefore this disclosure is to be regarded in an illustrative rather than a restrictive sense and all such modifications are intended to be included within the scope thereof. Likewise benefits other advantages and solutions to problems have been described above with regard to various embodiments. However benefits advantages solutions to problems and any element s that may cause any benefit advantage or solution to occur or become more pronounced are not to be construed as a critical a required or an essential feature or element. As used herein the terms comprises comprising and any other variation thereof are intended to cover a non exclusive inclusion such that a process a method an article or an apparatus that comprises a list of elements does not include only those elements but may include other elements not expressly listed or inherent to such process method system article or apparatus. Also as used herein the terms coupled coupling and any other variation thereof are intended to cover a physical connection an electrical connection a magnetic connection an optical connection a communicative connection a functional connection and or any other connection.

Additionally as will be appreciated by one of ordinary skill in the art principles of the present disclosure may be reflected in a computer program product on a machine readable storage medium having machine readable program code means embodied in the storage medium. Any tangible non transitory machine readable storage medium may be utilized including magnetic storage devices hard disks floppy disks and the like optical storage devices CD ROMs DVDs Blu Ray discs and the like flash memory and or the like. These computer program instructions may be loaded onto a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions that execute on the computer or other programmable data processing apparatus create means for implementing the functions specified. These computer program instructions may also be stored in a machine readable memory that can direct a computer or other programmable data processing apparatus to function in a particular manner such that the instructions stored in the machine readable memory produce an article of manufacture including implementing means that implement the function specified. The computer program instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operational steps to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions that execute on the computer or other programmable apparatus provide steps for implementing the functions specified.

While the principles of this disclosure have been shown in various embodiments many modifications of structure arrangements proportions elements materials and components that are particularly adapted for a specific environment and operating requirements may be used without departing from the principles and scope of this disclosure. These and other changes or modifications are intended to be included within the scope of the present disclosure.

