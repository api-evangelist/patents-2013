---

title: Managing software and hardware forwarding elements to define virtual networks
abstract: Some embodiments provide a set of one or more network controllers that communicates with a wide range of devices, ranging from switches to appliances such as firewalls, load balancers, etc. The set of network controllers communicates with such devices to connect them to its managed virtual networks. The set of network controllers can define each virtual network through software switches and/or software appliances. To extend the control beyond software network elements, some embodiments implement a database server on each dedicated hardware. The set of network controllers accesses the database server to send management data. The hardware then translates the management data to connect to a managed virtual network.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09455901&OS=09455901&RS=09455901
owner: NICIRA, INC.
number: 09455901
owner_city: Palo Alto
owner_country: US
publication_date: 20131031
---
This application claims the benefit of U.S. Provisional Patent Application 61 887 117 entitled Managing Software and Hardware Forwarding Elements to Define Virtual Networks filed Oct. 4 2013. U.S. Provisional Patent Application 61 887 117 is incorporated herein by reference.

Many current enterprises have large and sophisticated networks comprising switches hubs routers servers workstations and other networked devices which support a variety of connections applications and systems. The increased sophistication of computer networking including virtual machine migration dynamic workloads multi tenancy and customer specific quality of service and security configurations require a better paradigm for network control. Networks have traditionally been managed through low level configuration of individual components. Network configurations often depend on the underlying network for example blocking a user s access with an access control list ACL entry requires knowing the user s current IP address. More complicated tasks require more extensive network knowledge forcing guest users port 80 traffic to traverse an HTTP proxy requires knowing the current network topology and the location of each guest. This process is of increased difficulty where the network switching elements are shared across multiple users.

In response there is a growing movement driven by both industry and academia towards a new network control paradigm called Software Defined Networking SDN . In the SDN paradigm a network controller running on one or more servers in a network controls maintains and implements control logic that governs the forwarding behavior of shared network switching elements on a per user basis. Making network management decisions often requires knowledge of the network state. To facilitate management decision making the network controller creates and maintains a view of the network state and provides an application programming interface upon which management applications may access a view of the network state.

Three of the many challenges of large networks including datacenters and the enterprise are scalability mobility and multi tenancy and often the approaches taken to address one hamper the other. Currently instead of a hardware switch an extender or a L2 gateway is used to bridge the physical network to the virtual network. For instance the L2 gateway may be used to connect two managed environments or connect a managed environment and an unmanaged environment. The extender is an x86 box. As such its throughput is less than dedicated hardware. The extender also introduces one extra hop that can hinder performance and latency.

Some embodiments provide a set of one or more network controllers that communicates with a wide range of devices e.g. third party hardware ranging from switches to appliances such as firewalls load balancers etc. The set of network controllers communicates with such devices to connect them to its managed virtual networks. The set of network controllers can define each virtual network through software switches and or software appliances. To extend the control beyond software network elements some embodiments implement a database server on dedicated hardware and control the hardware through the database server using a database protocol. For instance the set of network controllers accesses the database server to send management data. The hardware then translates the management data to connect to a managed virtual network.

In some embodiments the database server is designed to handle transactions and deal with conflicts in having multiple writers e.g. more than one network controllers writing to a database . As an example for a given transaction the database server of some embodiments executes a set of operations on a database. When there are multiple operations the database server may execute each operation in a specified order except that if an operation fails then the remaining operations are not executed. The set of operations is executed as a single atomic consistent isolated transaction. The transaction is committed only if each and every operation succeeds. In this manner the database server provides a means of transacting with a client that maintains the reliability of the database even when there is a failure in completing each operation in the set of operations.

To prevent multiple clients writing to the database at the same time the database server may support various lock operations to lock or unlock the database. In some embodiments each client must obtain a particular lock from the database server before the client can write to a certain table of the database. The database will assign the client ownership of the lock as soon as it becomes available. When multiple clients request the same lock they will receive it in first come first served order. After receiving the lock the client can then write to the database and release the lock by perform an unlock operation.

In some embodiments the database server supports bi directional asynchronous notifications. For example when there is an update to a database table the database server sends a notification regarding an update to a client e.g. executing on a network controller or on the hardware . The notification may include a copy of the table or a subset of the table e.g. a record that was updated. In some embodiments the protocol s update call is used to exchange forwarding state. For instance if the hardware is a switch it can publish its forwarding state by having its database client write e.g. a learned MAC address to a database table. This will in turn cause the database server to push the update to a database client executing on a network controller. The network controller can then notify other network elements e.g. the software switches regarding the update. When a network controller is notified of an update to a software switch the network controller s client accesses the database server executing on the hardware switch to write to a database table. This will in turn cause the database server to push the update to the database client executing on the hardware switch. The hardware switch s software stack can then translate the update to a flow that it understands to process packets.

The database server of some embodiments maintains a database by performing garbage collection operations to remove database entries e.g. records that are not used. In some embodiments the garbage collection is performed at runtime e.g. with each given transaction performed periodically e.g. at set interval as a background task or performed when triggered e.g. when there is a change to the database or a change to a particular table of the database . In some embodiments if a table entry e.g. a record is not part of a root set and the entry has no reference or a reference of a particular importance e.g. a strong reference then the table entry is subject to garbage collection. The garbage collection process prevents the database from continually growing in size over time with unused data.

Some embodiments provide a network controller that manages software and hardware switching elements. The network controller sends management data to the hardware switching element using a protocol to add the hardware switching element to a virtual network. To manage traffic between the hardware and software switching element the network controller exchanges forwarding states with the hardware switching element through the protocol s asynchronous notification. The forwarding state of the software switching element is asynchronously sent from the network controller to the hardware switching element when the software s forwarding state has changed. The forwarding state of the hardware switching element is asynchronously received at the network controller from the hardware switching element when the hardware s forwarding state has changed.

The network controller of some embodiments facilitates in implementing a logical switching element from software and hardware switching elements. In facilitating the network controller sends a first transaction that instructs a database server on the hardware switching element to write to a database a logical forwarding element identifier LFEI that identifies a logical switch. The network controller sends a second transaction that instructs the database server on the hardware switching element to write to the database an address of at least one software switching element that use the LFEI. The hardware switching element uses the address to establish a tunnel between the hardware switching elements and the software switching element. The hardware and software switching elements implements the logical switching element by sending packets over the established tunnel using the LFEI.

Embodiments described herein provide a system for controlling forwarding elements. The system includes a network controller e.g. that operates on a computing device to generate and send forwarding instructions to several forwarding elements in a virtual network including software and hardware forwarding elements. The system includes a service node e.g. that operates on another computing device or the same computing device to 1 receive based on the forwarding instructions each unknown unicast packet from a software or a hardware forwarding element in the plurality of forwarding elements 2 replicate the packet and 3 send the packet to each other hardware forwarding element. The unknown unicast packet is sent to each particular hardware forwarding element in the virtual network so that the particular hardware forwarding element identifies whether a machine connected to the hardware forwarding element s port has the same address as the destination address associated with the packet and output the packet to the port if the addresses are the same. For example to find a matching address the hardware forwarding element may flood some or all of its ports and record the MAC address of the packet that responds to the flood.

In some embodiments the service node receives based on the forwarding instructions from the network controller each multicast packet from a software or a hardware forwarding element. The service node then replicates the packet and sends the packet to each other hardware forwarding element. The multicast packet is sent to each particular hardware forwarding element in the virtual network so that the particular hardware forwarding element identifies whether a machine connected to the hardware forwarding element s port is a part of the virtual network and output the packet to the port if the machine is a part of the virtual network.

The preceding Summary is intended to serve as a brief introduction to some embodiments as described herein. It is not meant to be an introduction or overview of all subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawings but rather are to be defined by the appended claims because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.

Embodiments described herein provide a set of one or more network controllers that communicates with a wide range of devices e.g. third party hardware ranging from switches to appliances such as firewalls load balancers etc. The set of network controllers communicates with such devices to connect them to its managed virtual networks. The set of network controllers can define each virtual network through software switches and or software appliances. To extend the control beyond software network elements some embodiments implement a database server on dedicated hardware and control the hardware through the database server using a database protocol. For instance the set of network controllers accesses the database server to send management data. The hardware then translates the management data to connect to a managed virtual network.

In some embodiments the database server is designed to handle transactions and deal with conflicts in having multiple writers e.g. more than one network controllers writing to a database . As an example for a given transaction the database server of some embodiments executes a set of operations on a database. When there are multiple operations the database server may execute each operation in a specified order except that if an operation fails then the remaining operations are not executed. To prevent multiple clients writing to the database at the same time the database server may support various lock operations to lock or unlock the database.

The database server of some embodiments supports other types of calls e.g. remote procedure calls using the database protocol including an update notification. When there is an update to a database table the database server sends a notification regarding an update to a client e.g. executing on a network controller or on the hardware . The notification may include a copy of the table or a subset of the table e.g. a record that was updated. In some embodiments the protocol s update call is used to exchange management data and forwarding state.

For some embodiments of the invention illustrates an example of how a network controller cluster communicates with several forwarding elements to create virtual networks. Specifically this figure shows the controller cluster exchanging management and configuration data with the forwarding elements to create tunnels. The tunnels extend the virtual network overlay among software or virtual switches and and the physical switch . Two operational stages and are shown in this figure.

In some embodiments the forwarding elements can be configured to route network data e.g. packets between network elements e.g. virtual machines computers that are coupled to the forwarding elements. For instance the software switch can route data to the software switch and can route data to the physical switch . In addition the software switch can route data to the software switch and the physical switch while the physical switch can route data to the two software switches.

The software switch or operates on a host e.g. a hypervisor . As an example the software switch may operate on a dedicated computer or on a computer that performs other non switching operations. The physical switch has dedicated hardware to forward packets. That is different from the software switches and the physical switch has application specific integrated circuits ASICs that are specifically designed to support in hardware forwarding. The term packet is used here as well as throughout this application to refer to a collection of bits in a particular format sent across a network. One of ordinary skill in the art will recognize that the term packet may be used herein to refer to various formatted collections of bits that may be sent across a network such as Ethernet frames TCP segments UDP datagrams IP packets etc.

The network controller cluster manages and configures the forwarding elements to create virtual networks. In some embodiments the network controller cluster communicates with the software switches and using different protocols. Specifically the protocols include an OpenFlow protocol and a database protocol. The OpenFlow protocol is primary used to inspect and modify a set of one or more flow tables analogous to a set of flow tables in the physical switch . The network controller cluster computes flows and pushes them to the software switches and through the OpenFlow channel. The network controller cluster communicates with the software switches and using the database protocol to create and manage overlay tunnels to transport nodes. The network controller cluster might also use this protocol for discovery purposes e.g. discover which virtual machines are hosted at a hypervisor .

For the software switches and the network controller cluster uses these two protocols to implement packet forwarding for logical datapaths. Each logical datapath comprises a series pipeline of logical flow tables each with its own globally unique identifier. The tables include a set of flow entries that specify expressions to match against the header of a packet and actions to take on the packet when a given expression is satisfied. Possible actions include modifying a packet dropping a packet sending it to a given egress port on the logical datapath and writing in memory metadata analogous to registers on the physical switch associated with the packet and resubmitting it back to the datapath for further processing. A flow expression can match against this metadata in addition to the packet s header. The network controller cluster writes the flow entries for each logical datapath to a single software switch flow table at each software switch that participates in the logical datapath.

Different from the software switches and the network controller cluster communicates primarily with the physical switch using with one protocol namely the database protocol. Through the database channel the network controller cluster reads the configurations of the physical switch e.g. an inventory of its physical ports and sends management data to the physical switch. For example a network controller might send instructions to the physical switch to create tunnel ports for a logical switch.

In some embodiments the network controller cluster communicates with the physical switch over the database channel to exchanges forwarding state e.g. L2 and or L3 forwarding state . For instance the physical switch might send an update notification to a network controller regarding a learned MAC address of a machine e.g. desktop computer laptop that is connected to its port. The network controller can then compute a flow and push the flow down to the software switches using the OpenFlow channel. The network controller might also send to the physical switch the MAC addresses of the machines that are coupled to the software switches and .

The forwarding state is exchanged with the physical switch because the flows of the physical switch is ASIC based and not OpenFlow based as in the software switches and . In other words the network controller cluster do not compute flows for the physical switch and push the flows to the physical switch through the database channel. The physical switch computes its own flows based on the forwarding information and how the physical switch computes the flows can vary from one switch vendor to another.

Having described several components of an example operation of these components will now be described by reference to the two stages and that are shown in the figure. The first stage illustrates an example control plane implementation to create tunnels between the forwarding elements . In particular the network controller cluster exchanges management data with the software switches and using the database protocol. The network controller cluster also computes and pushes flows to the software switches and through the OpenFlow channel. The network controller cluster communicates with the physical switch to send management data using the database protocol. The dedicated hardware then translates the management data to define a tunnel between the hardware and each of the software switches and . Furthermore the network controller cluster and the physical switch use the database protocol to exchange forwarding state.

The second stage shows an example data plane view of the virtual network. In this example several tunnels are established between each two forwarding elements. Namely a tunnel is established between the two software switches and . There is also a tunnel between the physical switch and the software switch and between the physical switch and the software switch . In some embodiments these tunnels provide a virtual unicast or multicast link e.g. Ethernet link between the forwarding elements which forward unencapsulated frames to and from attached components such as the machines e.g. VMs or physical links .

In the example described above the network controller cluster defines the physical switch to be a tunnel endpoint. Normally an extender or a gateway is used to connect two managed environments or a managed environment and an unmanaged environment. The extender is an x86 box e.g. that runs the software switch . As the physical switch is managed by network controller cluster the system does not introduce such an extender in between the physical switch and the machines and . Thus there is no one extra hop that can hinder performance and latency and with possible hardware specifications that provide less throughput than with the physical switch s dedicated hardware.

Many more examples of managing hardware and software forwarding elements are described below. Specially Section I show several an example database protocol that is used to extend the control to hardware switches and appliances. This is followed by Section II that describes several example features of a database server that is installed on hardware. Next Section III describes examples of creating virtual networks with hardware and software switches. Section IV then describes examples of exchanging forwarding state with a physical switch. Section V then describes several examples packet flows including known unicast unknown unicast and multicast. Section VI section then describe the environment in which some embodiments of the inventions are implemented. Finally Section VII describes an example of an electronic system that implement some embodiments described herein.

In some embodiments the network controller cluster defines each virtual network through software switches and or software appliances. To extend the control beyond software network elements some embodiments implement a database server on dedicated hardware. The set of network controllers accesses the database server to send management data. The dedicated hardware then translates the management data to connect to a managed virtual network.

In some embodiments the database server is designed to handle transactions and deal with conflicts in having multiple writers e.g. the more than one network controllers writing to a database . The database server is also designed to provide asynchronous notifications. For example when there is an update to a database table the database server sends a notification regarding an update to a client e.g. executing on a network controller or on the hardware . The notification may include a copy of the table or a subset of the table e.g. a record that was updated. In the example of the database server is an Open Virtual Switch OVS database server that accesses the OVS database .

The switch software stack represents several programs that operate on the physical switch . The software stack can include a variety of different programs to configure and manage the switch. This can include management that is in and outside of the scope of the network controller cluster . For instance the software stack may include a program to update its firmware modify switch settings e.g. its administrative password and or reset the switch. The software stack is vendor specific which means that it can change from one vendor to another vendor. In addition different vendors might provide different features that are represented by their corresponding software stack .

In the software stack includes at least one module to program the switch ASIC. The switch ASIC is a component which is specifically designed to support in hardware forwarding. That is it is primarily designed to quickly forward packets. To simplify the description only one switching ASIC is shown. However one of ordinary skill in the art would understand that the physical switch could include a number of ASICs that operate in conjunctions with one another to forward packets.

As shown the database server communicates with multiple clients and . Each client accesses the database through the database server . Each client reads and writes to the database using the database protocol. Each client may also be notified of an update to the database e.g. a table a subset of a table . In some embodiments the database protocol specifies a monitor call which is a request sent from a database client or to the database server to monitor one or more columns of a table and receive updates when there is an update to the one or more columns e.g. a new row value an update to an existing row value etc. . For instance the network controller s client may be notified when the switch s client updates the database and vice versa.

In some embodiments the database server is used to write forwarding state to the database . The physical switch is in a sense publishing its own forwarding tables in the database . As an example the database client on the switch software stack may update the database with MAC addresses of a machine that is connected to its port. This would in turn cause the database server to send a notification regarding the update to the client on a network controller.

Similar to the physical switch the network controller cluster is publishing addresses of other machines e.g. virtual machines that are connected to one or more software switches not shown . When the client on the network controller end makes an update to database the update would in turn generate a notification for the client on the switch software stack . The client may then read the update and a software on the stack may program a Content Addressable Memory CAM or Ternary CAM TCAM in the switch ASIC

In the example described above the network controller communicates with a dedicated hardware namely a hardware switch. is a system diagram that illustrates the network controller cluster communicating with both hardware and software switches. Specifically this figure shows how the network controller cluster exchanges management and configuration data e.g. flows with a software switch using the database protocol and the OpenFlow protocol while exchange management data and forwarding state with the physical switch using the database protocol. In this example the software switch operates on a host . The software switch has a database server an OpenFlow agent and a forwarding module . The software switch of some embodiments is an Open Virtual Switch OVS . In those embodiments the flow agent may be referred to as an OVS daemon and the forwarding module may be referred to as a kernel module.

In some embodiments the host includes hardware hypervisor and one or more virtual machines VMs . The hardware may include typical computer hardware such as processing units volatile memory e.g. random access memory RAM nonvolatile memory e.g. hard disc drives optical discs etc. network adapters video adapters or any other type of computer hardware. The hardware can also include one or more NICs which are typical network interface controllers.

A hypervisor is a software abstraction layer that can run on top of the hardware of the host . There are different types of hypervisors namely Type 1 bare metal which runs directly on the hardware of the host and Type 2 hosted which run on top of the host s operating system. The hypervisor handles various management tasks such as memory management processor scheduling or any other operations for controlling the execution of the VMs. Moreover the hypervisor communicates with the VMs to achieve various operations e.g. setting priorities . In some embodiments the hypervisor is a Xen hypervisor while in other embodiments the hypervisor may be any other type of hypervisor for providing hardware virtualization of the hardware on the host .

In some embodiments the software switch runs on a VM. The VM can be a unique virtual machine which includes a modified Linux kernel e.g. to include the OVS kernel module . The VM of such embodiments is responsible for managing and controlling other VMs running on the hypervisor. In some embodiments the VM includes a user space and the OVS daemon runs as a background process in the user space.

The OVS daemon is a component of the software switch that makes switching decisions. On the other hand the kernel module receives the switching decisions caches them and uses them to process packets. For instance when a packet comes in the kernel module first checks a datapath cache not show to find a matching flow entry. If no matching entry is found the control is shifted to the OVS daemon . The OVS daemon examines one or more flow tables to generate a flow to push down to the kernel module . In this manner when any subsequent packet is received the kernel module can quickly process the packet using the cached flow entry. The kernel module provides a fast path to process each packet. However the switching decisions are ultimately made through the OVS daemon in some embodiments.

A network controller uses the OpenFlow protocol to inspect and modify a set of one or more flow tables managed by the OVS daemon . The network controller cluster computes flows and pushes them to the software switch through this OpenFlow channel. The network controller communicates with the software switch using the database protocol to create and manage overlay tunnels to transport nodes. The network controller might also use this protocol for discovery purposes e.g. discover which virtual machines are hosted at the hypervisor . The OVS daemon also communicates with the database server to access management data e.g. bridge information virtual interfaces information stored in the database .

Different from the software switch a network controller communicates with the physical switch using the database protocol. The database protocol is essentially used to control the physical switch . Through the database channel the network controller reads the configurations of the physical switch e.g. an inventory of its physical ports and sends management data to the physical switch. For example a network controller might send instructions to the physical switch to create tunnel ports for a logical switch. As mentioned above the network controller cluster exchanges forwarding state e.g. L2 and or L3 forwarding state with the physical switch . That is the network controller instructs the physical switch to program its forwarding table using the database protocol.

The ingress ports conceptually represent a set of ports through which the switch receives network data. The ingress ports may include different amounts of ingress ports in different embodiments. As shown the ingress ports can receive network data that is external to the switch which is indicated as incoming packets in this example. When a packet is received through an ingress port the switch may send the packet to the switch ASIC so that the packet can be quickly processed.

The egress ports conceptually represent a set of ports through which the switching sends network data. The egress ports may include different amounts of egress ports in different embodiments. In some embodiments some or all of the egress ports may overlap with some or all of the ingress ports . For instance in some such embodiments the set of ports of the egress ports is the same set of ports as the set of ports of ingress ports . As illustrated in the egress ports receive network data after the switching processes the network data based on the forwarding tables . When the egress ports receive network data e.g. packets the switch sends the network data out of the egress ports which is indicated as outgoing packets in this example based on an entry in the forwarding tables .

In some embodiments the forwarding tables store active flow tables and or flow entries that are used to determine operations for making switching decisions. In this example each flow entry is includes a qualifier and an action. The qualifier defines a set of fields to match against a set of packet header fields. As shown in the flow entries are stored in memory. The memory can be random access memory RAM or some other type of memory such as Content Addressable Memory CAM or Ternary Content Addressable Memory TCAM . For example a vendor may design their Layer 2 switches with CAM for performing Layer 2 switching and or with TCAM for performing Quality of Service QoS functions. The switch architecture may support the ability to perform multiple lookups into multiple distinct CAM and or TCAM regions in parallel. The CAM and TCAM are examples of switching ASICs that some vendors switches leverage for line speed fast switching.

As described above an instance of the database server controls access to the database . The database client accesses the database to read and write management data and forwarding state. In addition a database client on the network controller accesses the database to read and write management data and forwarding state. The database server may send a notification to one database client e.g. on the switch end if the other database client e.g. on the network controlled end updates a table or a subset of a table of the database .

One other distinction to note is that the hardware switch s model is more generalized than that of the software switch s model. In the software switch the network controller has specific knowledge of how forwarding works and takes advantage of it. On the other hand the operations of the hardware switch can vary from one third party vendor to another. Therefore in the hardware model database is more abstract in that it contains the basic information to manage the hardware and exchange forwarding state.

Some embodiments implement a database server on dedicated hardware. In some embodiments the network controller cluster uses a database protocol to accesses this database server. Several example of such database server will now be described in this section by reference to . This section also describes various protocol messages used to communicate with the database server. In some embodiments the database is an Open Virtual Switch OVS database and the protocol is an OVS database protocol.

To prevent multiple clients writing to the database at the same time the database protocol may support various lock operations to lock or unlock the database. One of reasons for using such feature is that they resolve conflicts between multiple writers. For example two different controllers may assume to be masters and attempt to write to a database at the same time. This locking feature resolves that conflicts by making each network controller receive permission before writing to the database. In some embodiments each client must obtain a particular lock from the database server before the client can write to a certain table of the database. The database will assign the client ownership of the lock as soon as it becomes available. When multiple clients request the same lock they will receive it in first come first served order. After receiving the lock the client can then write to the database and release the lock by perform an unlock operation.

To simplify the description only two network controllers and are shown in this figure. However there can be additional network controllers. Each network controller has a database client or that can communicate with the database server to access the database . In some embodiments each client or communicates with the database server through the database communication protocol e.g. a JavaScript Object Notation JSON remote procedure call RPC based protocol .

The first stage shows the client on the network controller making a first call e.g. RPC to the database server . The first call is a lock operation call. In some embodiments the database server will assign the client ownership of the lock as soon as it becomes available. When multiple clients request the same lock they will receive it in first come first served basis. The database server of some embodiments supports an arbitrary number of locks each of which is identified by an e.g. a client defined ID . In some embodiments the precise usage of a lock is determined by the client or . For example the clients and may be programmed to agree that a certain table can only be written by the owner of a certain lock. That is the database server itself does not enforce any restrictions on how locks are used. The database server simply ensures that a lock has at most one owner.

The second stage assumes that the specified lock is available. Accordingly the database server sends a response. The response is locked notification which notifies the client that it has been granted a lock it had previously requested with the lock method call. In some embodiments the notification has the client defined ID.

At the same second stage the client on the network controller makes a call e.g. RPC to the database server requesting the same lock e.g. to modify a same table . However the lock has already been assigned to the client on the network controller . In some embodiments the client has to perform an unlock operation by sending an unlock request to the database server with the lock ID. When the lock is released the database server will then send the locked notification to the next client e.g. the client or that is in line to use the lock. In some embodiments the database protocol supports a call to steal a specified lock.

In the third stage the ownership of the specified lock belongs to the client on the network controller . The client on the network controller cannot access one or more tables of the database . The enforcement of this security policy in some embodiments is defined at the client . In other words the database server is not programmed to stop the client from the accessing one or more of the same tables but the client is programmed to stop itself. One of ordinary skill in the art would understand that the database server could be programmed to enforce its own security policies. For instance the database server can be implemented such that it locks a database table that is being accessed by a client.

The third stage illustrates the client on the network controller exercising the ownership of the lock by sending to the database server a transaction call e.g. in order to insert or update a record in the database . The fourth stage illustrates the database server performing the transaction on the database and returning a result of the transaction. The fifth stage shows the client on the network controller releasing the specified lock. In some embodiments the database server sends a confirmation message not shown of releasing the lock to the database client on the network controller . As the lock is released the sixth stage shows the ownership of the lock being transferred to the client on the network controller .

In the example described above two clients and on the network controllers and attempt to perform transactions on the same database table at the same time. In some embodiments the client on the dedicated hardware must take ownership of the lock to write to a database table. That is when the client or on the network controller or has ownership of the lock the client on the hardware must wait until the lock becomes available.

In some embodiments the database server supports bi directional asynchronous notifications. For example when there is an update to a database table the database server sends a notification regarding an update to a client e.g. executing on a network controller or on the hardware . The notification may include a copy of the table or a subset of the updated table e.g. a record . The bi directional asynchronous notification mechanism is different from polling the database server to determine if there are any changes to one or more tables. The asynchronous notification feature is particularly useful because each client executing on a network controller or on the hardware receives a message when a set of one or more columns of interest has changed.

The database protocol of some embodiments provides a mechanism to receive notifications e.g. asynchronous notification . In some embodiments each client can use the database protocol s monitor or registration request to monitor a database. This request enables the client to replicate tables or subsets of tables within the database by requesting notifications of changes to those tables and by receiving the complete initial state of a table or a subset of a table. With this database call the client can specify one or more table columns to monitor. The columns can be from any number of database tables. The client can also use the monitor cannel request to cancel a previously issued monitor request.

The client of some embodiments can specify different types of operations to monitor such as initial insert delete and modify. The initial parameter specifies that each and every row in the initial table be sent as part of the response to the monitor request. The insert parameter specifies that update notifications be sent for rows newly inserted into the table. The delete parameter specifies that update notifications be sent for rows deleted from the table. The modify parameter specifies that update notifications are sent whenever a row in the table is modified. When there is a change to any one of the monitored fields the database server of some embodiments returns a table update object. This object may include previous row values if the rows values have been modified or deleted.

The first stage illustrates the client on the network controller sending a transaction request to the database server on the hardware . The transaction call includes a request to insert a new record on a particular table. The client on the hardware has previously sent the database server a request to monitor this table.

The second stage shows the database server on the hardware performing the requested operation on the database . In particular the database server inserts a new record in the particular table. The third stage shows the database server sending the result of the transaction to the client on the network controller . After performing the insert operation the database server sends at stage the update notification to the client on the hardware .

One of the main uses of the contents of the hardware s database is to exchange forwarding tables e.g. L2 forwarding tables between the network controller cluster and the hardware switch. The physical switch is publishing its own forwarding tables in the database. One of way this can happens is the database client on the physical switch would make updates to the database contents. This would in turn generate a notification to a client on a network controller.

In some embodiments the protocol s update call is used to exchange forwarding state. For instance when the network controller is notified of an update to a software switch the network controller s client accesses the database server executing on the hardware to write e.g. a MAC address of a VM to a database table. This will in turn cause the database server to push the update to the database client executing on the hardware . The hardware switch s software stack can then translate the update to a flow that it understands to process packets. Example of pushing such addresses to the hardware will be described below by reference to . The update call can also be used for management purposes such as setting up tunnel ports for the hardware . Example of pushing management data to the hardware will be described below by reference to .

Four operational stages are shown in this figure. The first stage illustrates the client on the hardware sending a transaction request to the database server . The transaction call includes a request to insert a new record on a particular table. The client on the network controller has previously sent the database server a request to monitor this table.

The second stage shows the database server on the hardware performing the requested operation on the database . In particular the database server inserts a new record in the particular table. The third stage shows the database server sending the result of the transaction to the client . After performing the insert operation the database server sends at stage the update to the client on the hardware .

In some embodiments the protocol s update call is used to exchange forwarding state. For instance if the hardware is a switch it can publish its forwarding state by having the database client write e.g. a learned MAC address to a database table. This will in turn cause the database server to push the update to the database client executing on the network controller . The network controller can then notify other network elements e.g. the software switches hardware switches regarding the update. An example of publishing addresses of machines attached to a switch s ports will be described below by reference to . The update call can also be used for management purposes such reading an inventory of physical ports on the hardware setting up tunnel ports etc.

In some embodiments the database server is designed to deal with any errors in completing transactions. As an example for a given transaction the database server of some embodiments executes a set of operations on a database. When there are multiple operations the database server may execute each operation in a specified order except that if an operation fails then the remaining operations are not executed. The set of operations is executed as a single atomic consistent isolated transaction. The transaction is committed only if each and every operation succeeds. In this manner the database server provides a means of transacting with a client that maintains the reliability of the database even when there is a failure in completing each operation in the set of operations.

At D15 the process performs the next operation on the database. The process then determines at whether the operation on the database has been successfully completed or has failed. When the operation has failed the process specifies at that there was an error performing the operation. The process then proceeds to which is described below.

When the operation has not failed the process specifies at that the operation has been performed successfully. The process then determines at whether there is another operation to perform. As stated above the transaction can include more than one operation to perform on the database. If there is another operation the process returns to which is described above. If there is no other operation the process commits at the transaction. Here the process is committing the transaction because each and every operation has been successfully performed.

At the process sends the result of the transaction to the client. If the transaction has been committed the result includes for each operation an indication that the operation has been successfully performed or an indication that an operation has failed. These indications are based on what the process specified at operations and which are described above. The indication s value can be simple either a zero or a one Boolean value which indicates failure or success respectively or vice versa.

Some embodiments perform variations on the process . The specific operations of the process may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. Furthermore the process could be implemented using several sub processes or as part of a larger macro process.

In some embodiments the database server which is installed on hardware is performs a garbage collection operation on a database. Garbage collection is the process of maintaining the database by removing data in a database that is no longer used or outdated. In some embodiments the garbage collection is performed at runtime e.g. with each given transaction performed periodically e.g. at set interval as a background task or performed when triggered e.g. when there is a change to the database or a change to a particular table of the database .

In some embodiments if a table entry e.g. a record is not part of a root set and the entry has no reference or a reference of a particular importance e.g. a strong reference then the table entry is subject to garbage collection. The database server of some embodiments allows a table to be defined as a root table. If the table is defined as a root table the table s entries exist independent of any references. The entries in the root table can be thought off as part of the root set in a garbage collector. In referencing other entries the database server of some embodiments allows each entry to refer to another entry in the same database table or a different database table. To uniquely identify different entries each entry may be associated with a unique identifier such as a primary key Universally Unique Identifier UUID etc.

The garbage collection process prevents the database from continually growing in size over time with unused data. For instance a Top of Rack TOR switch may have a database that stores data relating logical switches. The logical switch information can include the addresses e.g. IP addresses of the tunnel endpoints that instantiate the logical switch and other supplemental data. In such cases the garbage collection feature allows those addresses and supplemental data to be deleted from the database when the corresponding logical switch is deleted.

The first stage shows the database prior to deleting an entry from the root table . To simplify the discussion each of the tables shows only three entries. Specifically the root table has three root entries. The table has three entries with the first two entries referencing the first entry of the root table and the third entry referencing the third entry of the same root table. The table has three entries that all reference the first entry of the table . In some embodiments the table and represents child tables. However as mentioned above an entry in a table may reference another entry in the same table. Thus a child entry can be in a same table as a parent entry.

The second stage illustrates the first entry being deleted from the root table . The third stage shows a garbage collection process being performed on the database . In particular the third stage shows that two entries in the table being deleted as a result of the first entry being deleted from the root table . In some embodiments the two entries are deleted from the table because they are not part of a root set and or each of the entries has no references of a particular value e.g. a strong reference . This means that the references in the table to the first entry in the table are not reference of a particular importance. If the references were of strong importance the first entry in the table would not be subject to garbage collection in some embodiments.

The fourth stage shows the continuation of the garbage collection process. Here the process deletes all the entries from the table . These entries are deleted from the table because they are not part of a root set and are not referenced in a particular manner by any child or leaf entries. The fifth stage shows the result of the garbage collection process. Specifically it shows that remaining entries after the garbage collection process which are two entries in the root table and one entry in the table .

The network controller cluster of some embodiments can define a virtual network through software and hardware forwarding elements. That is the network controller cluster provides interfaces for configuration management and asynchronous notification which can be used to create a single logical switch across multiple software and hardware forwarding elements. These forwarding elements can be at one location e.g. at one datacenter or at multiple different location e.g. different datacenters . Several examples of such virtual networks will now be described below by reference to .

Two operational stages and are shown in . The first stage illustrates an example control plane implementation that defines tunnels between the devices and . The tunnels extend the virtual network overlay among hypervisors and and the TOR switch . In this example the network controller cluster exchanges management data with the hypervisors and using the database protocol. The management data includes instructions on how each device should create a tunnel between itself and another device. For instance each of tunnel endpoint may receive addresses IP addresses of other tunnel endpoints addresses of service nodes virtual network identifiers VNIs to use for tunnels etc. In some embodiments the VNI is also referred to as a logical forwarding element identifier LFEI . This is because the identifier is used to effectuate the creation of the logical forwarding element from several managed edge forwarding elements. That is two or more edge managed forwarding elements use the same VNI to create a separate broadcast domain for a set of machines attached their virtual interfaces or ports.

In some embodiments the network controller cluster computes and pushes flows to the hypervisors and through the OpenFlow channel. The network controller cluster exchanges management data with the TOR switch using the database protocol. The database protocol is also used by the controller cluster and the TOR switch to exchange forwarding state.

The second stage shows an example data plane view of the overlay network that is established through the controller cluster and the devices . In particular the network controller cluster has pushed management data to establish a tunnel between the two hypervisors and between the hypervisor and the TOR switch and between the hypervisor and the TOR switch .

The second stage shows Virtual Extensible LAN VXLAN tunnels. However other types of tunnels can be established depending on the encapsulation protocol that is used. As an example the network control cluster can be configured to create different types of tunnels such as Stateless Transport Tunneling STT tunnels Network Virtualization using Generic Routing Encapsulation NVGRE etc.

The previous example illustrated tunnels being established between several edge devices. In some embodiments the tunnels are established to create virtual private networks VPNs for different entities e.g. clients customers tenants . The network controller cluster of some embodiments implements multiple logical switches elements across different types of switching elements including software and hardware switches. This allows multiple different logical forwarding elements to be implemented across any combination of software and or hardware switches.

As shown in the TOR switch has two ports that are connected to machines associated with two different entities. Specifically port 1 which is paired with VLAN 1 is connected to a machine e.g. computer that is associated with a first entity e.g. a client customer or tenant . Port 2 which is paired with VLAN 2 is connected to another machine that is associated with a second entity. The figure conceptually shows that two ports are connected to different machines for different entities as port 1 has a lighter shade of gray than port 2.

In relation to the port and VLAN pairing when a hardware switch receives traffic from a forwarding element e.g. a hypervisor the hardware drops that packet on its physical port. At that point the hardware switch might be trunking multiple VLANs. In some embodiments the system e.g. the network controller cluster keeps track of the VLANs and its mapping to the VNI. For example there might be a logical switch 1 with a VNI 1 and VLAN1 on port 1 is bridged to that logical switch. What this means is that when the physical switch receives a packet for VNI 1 it will essentially drop the packet on its port 1 using the VLAN1. When you are directly attaching a machine e.g. a server computer to an L2 hardware switch there is no L3 tunnel established between the switch and the machine. So instead of the VNI the L2 VLAN is used to distinguish traffic for different entities e.g. tenants customers etc. . In other words this is essentially a translation or mapping mechanism from one side to the other. On one side there is the tunnel IP fabric where you use VNI and on the other side there is the physical L2 fabric where you cannot use VNI because it is connecting directly to the machine.

Similar to the TOR switch each hypervisor or is communicatively coupled to machines that are associated with different entities. This is shown by the virtual interfaces VIFs on the hypervisors and . In some embodiments a VIF is an interface that connects a virtual machine VM to a software switch. The software switch provides connectivity between the VIFs and the hypervisor s physical interfaces PIFs and handle traffic between VIFs collocated on the same hypervisor.

VIF1 on hypervisor and VIF2 on hypervisor are each connected to a VM associated with the first entity. As mentioned above port 1 of the TOR switch is also connected to a machine associated with this first entity. On the other hand VIF3 on hypervisor and VIF4 on hypervisor are connected to VMs of the second entity. Port 2 of the TOR switch is connected to a machine associated with this second entity. conceptually shows that VIFs are connected to different VMs for different entities as the VIFs for the first entity s VM has a lighter shade of gray than the VIFs for the second entity s VMs.

The network controller cluster of some embodiments creates logical switches and by assigning different virtual network identifiers VNIs for different entities e.g. clients customers tenants . This is shown in because each of the logical switches is assigned a unique VNI. Specifically the first entity is assigned a VNI of 1 and the second entity is assigned a VNI of 2.

Network traffic between the separate tunnel endpoints and are encapsulated with the two VNIs. The encapsulation creates two separate broadcast domains for the machines of the two entities. Accordingly the logical switch has logical ports that are associated with the machines associated with the first entity and the logical switch has logical ports that are associated with the machines associated with the second entity.

As shown in the logical switch has a logical port 1 which is associated with VIF1 of the hypervisor logical port 2 which is associated with VIF2 of the hypervisor and a logical tunnel port 1 which is associated with the port 1 and VLAN 1 pair of the TOR switch . These VIFs and the port and VLAN pair are all connected to the machines associated with the first entity. In the logical abstraction there are several logical ports. In some embodiments each port has to have an attachment to send or receive traffic. The attachment is shown adjacent to the port number. For example port 1 and VLAN1 of the physical switch is attached to LTP. Also the names of the ports are symbolic name. In the system each port may be identified by a universally unique identifier UUID and that identifier is unique for each port.

On the other hand the logical switch has a logical port 3 which is associated with VIF3 of the hypervisor logical port 4 which is associated with VIF4 of the hypervisor and a logical tunnel port 2 which is associated with the port 2 and VLAN 2 pair of the TOR switch . These VIFs and the port and VLAN pair are all connected to the machines associated with the second entity.

As mentioned above some embodiments connect a wide range of third party devices ranging from top of rack switches to appliances such as firewalls load balancers etc. to managed virtual networks. In the data plane tunnels are used to extend the virtual network overlay among the hypervisors and third party devices. A variety of tunneling encapsulations may be used VXLAN STT GRE etc. . In the control plane the one or more controllers provide sufficient information to a device e.g. a third party device to enable it to connect a service to a virtual network. The definition of a service in this context is dependent on the third party device. For example it could be a firewall instance or a VLAN to be bridged into a virtual network by a TOR switch.

In some embodiments the exchange of information takes place using a database that resides in the device e.g. third party device . In the case where a physical switch is to be connected to a managed virtual network the switch provides an inventory of its physical ports to a set of network controllers via a database protocol. This provides the set of network controllers with information that allows it to manage physical ports much as it does virtual ports. In other words physical ports can be attached to virtual networks policies can be attached to the ports etc.

When connecting higher level services e.g. firewalls load balancers etc. to virtual networks different mechanisms can be used to create a binding between a service instance and a virtual network. For example the management interface to the device e.g. the third party device could provide this capability using a universally unique identifier UUID of the virtual network provided by the set of network controller using the database protocol described herein to perform the mapping.

From a data plane perspective a device e.g. third party box needs to at least some of the following 1 receive packets on a tunnel from a virtual switch e.g. OVS instance and forward them into the appropriate service context or port on the device 2 forward packets from a given context or port to the appropriate virtual switch instance over a tunnel in some cases the appropriate virtual switch instance may be in a service node 3 apply the appropriate tunnel encapsulation when forwarding packets to virtual switch instances this includes both tunnel type VXLAN GRE etc. and appropriate keys VNIs etc. 4 in some cases replicate packets to multiple virtual instances and in some cases perform some data plane learning.

The database protocol described herein enables the hardware to perform these data plane functions. It also allows information to be fed by the hardware to the set of network controllers to allow the correct forwarding of packets from managed virtual networks to third party devices. Notably a hardware switch TOR switch can notify the set of network controllers of the addresses MAC addresses that have been learned on its physical ports so that the set of network controller can facilitate in forwarding packets destined for those addresses toward the hardware switch.

The database server provides a means to exchange information between a device e.g. third party device and the set of network controllers. The database server resides on the device and has multiple clients a local client and a remote client e.g. a network controller . The network controller and the third party device exchange information by reading and writing to the common database resident on the device. For example a device can notify the network controller that it has a certain set of physical ports available by writing to the database which is then read by the network controller. Similarly one or more network controllers can instruct a device to connect a physical port to a particular logical network by writing an entry into a table containing such mappings.

Accordingly the VNI is a network identifier. Depending on the protocol this is typically some bit of data e.g. 24 bit to allow some amount of e.g. 16 million logical networks. In some embodiments each VNI represents a virtual Layer 2 broadcast domain and routes can be configured for communication based on the VNI. Different protocols may give this identifier different name such as tenant network identifier virtual private network identifier etc.

At the process for a physical port of the hardware writes to the database a mapping that binds a VLAN to the logical switch. The process can perform these operations for multiple physical ports. Accordingly the process determines at whether to do additional binding. If so the process returns to which is described above. Otherwise the process proceeds to . The process writes at to the database addresses e.g. IP addresses of a set of tunnels that uses the same VNI of the logical switch. The process then ends.

Some embodiments perform variations on the process . The specific operations of the process may not be performed in the exact order shown and described. For example the process might read the inventory of physical ports with one transaction and later perform a number of other transactions to write to the database. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. As an example the process might iteratively perform the operations multiple times to connect the hardware to another virtual network. Furthermore the process could be implemented using several sub processes or as part of a larger macro process.

Having described an example process example applications of the process will now be described in the following sub sections.

The following sub section provides an example of using the process to connect some physical servers e.g. desktop computers to a virtual network. In this example a physical switch e.g. a VXLAN capable L2 switch provides the gateway function between the physical servers and the virtual network. The physical switch runs an instance of a database server. One client of the database server is local to the switch and at least one other client runs inside a network controller.

In some embodiments initial configuration of the VXLAN gateway is performed using out of band mechanisms such as the command line interface CLI of the gateway. Such configuration would include the configuration of physical ports providing an IP address of the network controller etc. but no virtual network provisioning. The local database client will populate the database with this configured information making it available to the network controller.

The creation of virtual networks is handled by protocols calls to the network controller. To simply the discussion it is assumed that the virtual network is a VXLAN. Further protocol calls will request that network controller connect physical ports or 802.1q tagged ports to the VXLAN. The network controller of some embodiments then writes the following information into the database on the physical switch.

At this point the VXLAN exists and a set of possibly tagged physical ports is mapped to the VXLAN. Further protocol calls to the network controller are used to connect VMs to the VXLAN. To enable the physical switch to send packets to those VMs the network controller writes MAC addresses of the VM s on the database on the physical switch. For traffic in the other direction from VMs towards the physical switch the client in the physical switch also writes learned MAC addresses of physical servers connected to its ports. Example of such operations will be described below by reference to . In some cases it is also may be necessary to deal with unknown MAC addresses at least for packets destined to the physical world. This is handled as a special case of multicast with the unknown MAC address pointing to a set of one or more tunnels. An example of such special case will be described below by reference to .

In the example described above the VXLAN successfully emulates an L2 switch connecting the VMs to the gateway with no flooding of traffic taking place within the VXLAN to discover the location of MAC addresses. In some embodiments Address Resolution Protocol ARP requests may be flooded via replication at a service node.

The following sub section provides an example of using a variation on the process to connect a device providing higher level services such as a firewall or a load balancer to a logical network that is managed by the controller cluster. In this case the sequence of operations is slightly simplified compared to the preceding example.

The initial configuration of the device e.g. third party device may vary from one device to another. In some embodiments the initial configuration includes providing the device with an IP address of the network controller e.g. through the device s CLI or a management console . The device runs an instance of the database server. One client of the database server is local to the device and at least one other client runs inside a network controller.

The client in the device writes to the database one or more IP addresses on which it can terminate VXLAN tunnels. At this point the network controller is able to create logical networks that connect to the device.

Logical network creation is triggered by a database protocol call to the network controller. The network controller of some embodiments then writes the following information into the database on the device.

In some embodiments the network controller provides mechanisms to bind an instance of a service provided by the device e.g. third party device to the logical network. For example the device CLI could be used to create an instance of a service and bind it to a particular logical network. In some embodiments each logical network in the database has a unique name independent of the tunneling mechanism that is used to create the logical network. This name should be used for bindings rather than the VXLAN specific VNI.

In some embodiments the device writes the MAC address of the service in the MAC to physical locator entries for the logical switch. If the service is implemented in a VM this would be the MAC address of the virtual network interface card VNIC . In some embodiments ARP requests are handled by replication at a service node.

The previous section described example of configuration hardware to add the hardware to a managed virtual network. Once the hardware is configured the network controller of some embodiments exchanges forwarding state with the hardware. As an example a hardware switch can publish its forwarding state by having its database client write a learned MAC address to a database table. This will in turn cause the database server to push the update to a database client executing on a network controller. The network controller can then notify other forwarding elements e.g. software switches hardware switches regarding the update.

In some embodiments uploading the MAC addresses is a continuous process. The hardware switch will keep on learning MAC addresses. For example the hardware switch might have various time mechanisms. If a machine is moved away or silent for a long period time the hardware switch might remove the machine s MAC address. The hardware switch might add the address again if it detects that machine is connected to one of its physical ports. In some embodiments each MAC address is published by the forwarding element e.g. hardware switch hypervisor with the forwarding element s IP address.

One the other side the network controller knows the MAC addresses of the virtual machines connected to software switches. To stress the MAC addresses of the virtual machines are known address and not learned MAC addresses. These known MAC address are being pushed with the network controller writing to the hardware switch s database. One reason they are known rather than learned is that VMs are connected to virtual interfaces VIFs and there is no timeout mechanism associated with the VIFs. The VM is either associated with one VIF or not associated with that VIF. In some embodiments the forwarding state of a software switching element is asynchronously sent from the network controller to the hardware switching element when the software s forwarding state has changed e.g. a VM is attached to a VIF .

When the database is updated with the learned MAC address the database server of some embodiments sends an update notification to a network controller e.g. in a network controller cluster . In some embodiments the network controller has a database client that has been configured to monitor updates to a table or a subset of table using the database protocol. For example the client might have previously sent a monitor or registration request with a set of parameters e.g. the columns being monitored what kind of transactions are being monitored such as initial insert delete modify etc. . The database server of some embodiments sends an update notification that has a copy of the table or a subset of the table e.g. a record that was updated. Several examples of a client on monitoring a database are described above by reference to .

As shown in the TOR switch publishes the MAC address to the network controller . In some embodiments the MAC address is sent with a virtual network identifier VNI of the logical switch and an IP address of the TOR switch . The network controller receives the information and then pushes it down to the hypervisors and . Each of the hypervisors and then updates its respective flow table with a flow that includes the MAC address and action to forward the packet with the same MAC address e.g. in the header s destination MAC address field to the TOR switch . In some embodiments the network controller receives the MAC address computes a flow and sends the flow to each of the hypervisors and . The flow may be sent to each hypervisor using the OpenFlow Protocol. That is instead of the database protocol e.g. the OVDB channel the flow will be sent to the hypervisors using one or more OpenFlow calls. However if a tunnel did not exist between the TOR switch and the hypervisor or then network controller would have to send instructions e.g. the address of the TOR switch VNIs etc. to the hypervisor to create the tunnel. The instructions sent to the hypervisor using the database protocol e.g. OVSDB protocol in some embodiments.

In the example described above the TOR switch publishes a MAC address to the network controller . provides a data flow diagram that shows the hypervisor that publishing a MAC address of a virtual machine VM that is connected to its virtual interface VIF . This flow begins when a new VIF is created and the VM is associated with the VIF.

As shown in the hypervisor is notified of the MAC address of the VM attached to VIF4. In some embodiments when a VM is attached to a VIF the software switch executing on the hypervisor is notified of the MAC address of the VM. The software switch then sends the MAC address to the network controller . In some embodiments the MAC address is sent with one or more of the following the VNI to identify the logical switch a logical port number and IP address of the hypervisor .

Having received the information the network controller sends the information to the hypervisor and the TOR switch . The hypervisor receives the information. The hypervisor then updates its flow table with a flow that includes the MAC address and action to forward the packet with the same MAC address e.g. in the header s destination MAC address field to the hypervisor . In some embodiments the network controller receives the information computes a flow and sends the flow to the hypervisor . The flow may be sent to each hypervisor using the OpenFlow Protocol. That is instead of the database protocol e.g. the OVDB channel the flow will be sent to the hypervisors using one or more OpenFlow calls.

Different from the hypervisor the network controller sends the information to the TOR switch by first creating a new transaction message. The transaction is then sent to the database server on the TOR switch . The database server then updates the database based on the instructions in the message. In some embodiments the database client on the TOR switch is notified of the update. The update is then passed to a program on the switch s software stack. The program uses the update to generate a flow that includes the MAC address and action to forward the packet with the same MAC address e.g. in the header s destination MAC address field to the hypervisor .

The previous section described several examples of the network controller exchanging forwarding state with the dedicated hardware. Several examples of forwarding packets will now be described below by reference to .

In these examples a set of one or more network controllers has computed a set of flows for a software switching to forward packets to other switching elements e.g. software switching element hardware switching element . The set of network controllers has also exchanged forwarding state with one or more hardware switching element. Each hardware switching element has translated the state to a set of flows that it can understand to forward packet.

The flow begins when a packet is received though a physical port of the TOR switch . The physical port is associated with a port and VLAN pairing i.e. port 2 VLAN2 . As shown by the box above the figure number the TOR switch performs a look up operation. For instance when the packet is received through on port 2 and VLAN 2 the destination MAC address is already there in the packet. The TOR switch looks at what port this packet came in and what the VLAN ID is and based on the information it determines that the packet should be tunneled to the hypervisor using the VNI 2. In the example of the hypervisor is identified by its associated IP address.

After performing the look up operation the TOR switch encapsulates the packet using the tunnel protocol and sends the packet over the tunnel to the hypervisor . Specifically the packet is encapsulated using the VNI e.g. the VNI 2 . Here the TOR switch sends the packet to the hypervisor using the IP address of the hypervisor.

When the hypervisor receives the packet it performs a flow lookup operation to output the packet to an appropriate port. Specifically it parses the packet to identify header values such as the destination MAC address. The hypervisor then tries to find a matching flow in its datapath or cache. If the matching flow is found the hypervisor uses the flow to output the packet to a VM. If no matching flow is found the hypervisor generates a flow to push into the datapath to process the packet. In this example the packet is output to VIF4 which is associated with logical port 4 e.g. in .

As shown by the box above the figure number the hypervisor performs a look up operation. For instance when the packet is received through VIF1 the hypervisor looks at this information and other supplemental information e.g. the logical port number associated with the VIF and based on one or more pieces of those information it determines that the packet should be tunneled to the service node using the VNI 2. In the example of the service node is identified by its associated IP address.

When the service node receives the packet it replicates the packet and sends the packet to each of the TOR switches and . In some embodiments the service node send the packet over a tunnel that is established between the service node and each of the two TOR switches and . The packet is sent over the corresponding tunnel using the VNI. As shown the service node forwards the packets to the tunnel endpoints namely the TOR switches and . The TOR switch send the packet out Port 1 and VLAN 1 and the TOR switch sends the packet out of Port 4 VLAN 1.

The flow begins when a packet is received though a physical port of the TOR switch . The physical port is associated with a port and VLAN pairing i.e. port 1 VLAN1 . As shown by the box the TOR switch look at what port this packet came in what the VLAN ID is and whether this is an ARP packet and based on the information it determines that the packet should be tunneled to the service node using the VNI 1.

When the service node receives the packet the service node sends the packet to each of the hypervisors and . In some embodiments the service node sends the packet over a tunnel that is established between the service node and each of the two hypervisors and . The packet is sent over the corresponding tunnel using the VNI. As shown the service node forwards the packets to the tunnel endpoints namely the hypervisors and . The hypervisor send the packet out logical port 1 of logical switch 1 that is associated with VIF1. The hypervisor sends the packet out logical port 2 of logical switch1 that is associated with VIF2.

In the example of the hypervisor receives a packet from a VM and calls a service node . In this example the hypervisor has flow entry to send packet with an unknown MAC address in the packet s header to the service node that is unknown to the service node . As shown the flow begins when a packet is received though the logical port 1. The logical port 1 is associated with VIF1. The hypervisor then encapsulates the packet using the tunnel protocol and sends the packet over a tunnel to the service node . Specifically the packet is encapsulated using the VNI i.e. the VNI 1 for the logical switch 1 . In some embodiments the TOR switch sends the packet to the service node using the IP address of the service node .

When the service node receives the packet the service node sends the packet over a tunnel that is established the service node and the TOR switch . However the service node does not send the packet to the hypervisor . As mentioned above this is because the system assumes that the TOR switch can have unknown unicast behind it while the hypervisor only has known unicast.

The packet is sent over the corresponding tunnel using the VNI. As shown the service node forwards the packets to the tunnel endpoint namely the TOR switch . The hypervisor send the packet out one of its physical ports that is associated with the port and VLAN pairing.

The following section will describe the environment in which some embodiments of the inventions are implemented. In the present application switching elements and machines may be referred to as network elements. In addition a network that is managed by one or more network controllers may be referred to as a managed network in the present application. In some embodiments the managed network includes only managed switching elements e.g. switching elements that are controlled by one or more network controllers while in other embodiments the managed network includes managed switching elements as well as unmanaged switching elements e.g. switching elements that are not controlled by a network controller .

In some embodiments a network controller cluster controls one or more managed switching elements that are located at the edge of a network e.g. tunnel endpoints edge switching elements or edge devices . In addition to controlling edge switching elements the network controller cluster of some embodiments also utilize and control non edge switching elements that are inserted in the network to simplify and or facilitate the operation of the managed edge switching elements. For instance in some embodiments the network controller cluster requires the switching elements that the network controller cluster controls to be interconnected in a hierarchical switching architecture that has several edge switching elements as the leaf nodes in the hierarchical switching architecture and one or more non edge switching elements as the non leaf nodes in this architecture. In some such embodiments each edge switching element connects to one or more of the non leaf switching elements and uses such non leaf switching elements to facilitate the communication of the edge switching element with other edge switching elements.

Some embodiments employ one level of non leaf non edge switching elements that connect to edge switching elements and in some cases to other non leaf switching elements. Other embodiments on the other hand employ multiple levels of non leaf switching elements with each level of non leaf switching elements after the first level serving as a mechanism to facilitate communication between lower level non leaf switching elements and leaf switching elements. In some embodiments the non leaf switching elements are software switching elements that are implemented by storing the switching tables in the memory of a standalone computer instead of an off the shelf switch. As described herein the leaf or non leaf switching elements can be extended beyond software switching elements to hardware switching elements.

As mentioned above some embodiments provide a network controller cluster that communicates with a wide range of devices e.g. third party hardware ranging from switches to appliances such as firewalls load balancers etc. The network controller cluster communicates with such devices to connect them to its managed virtual networks. The network controller cluster can define each virtual network through software switches and or software appliances. The common feature of the different types of equipment or boxes is that each of them has a database server that is controlled through a protocol.

The network controller of some embodiments can be implemented in software as an instance of an application. As illustrated in the network controllers and are instances of a software application. As shown each of the network controllers and includes several software layers a control application layer a virtualization application layer and a networking operating system layer.

In some embodiments the control application layer receives user input that specifies a network switching element. The control application layer may receive the user input in any number of different interfaces such as a graphical user interface GUI a command line interface a web based interface a touchscreen interface etc. In some embodiments the user input specifies characteristics and behaviors of the network switching element such as the number of switching element ports access control lists ACLs network data forwarding port security or any other network switching element configuration options.

The control application layer of some embodiments defines a logical data path set based on user input that specifies a network switching element. As noted above a logical data path set is a set of network data paths through managed switching elements that are used to implement the user specified network switching element. In other words the logical data path set is a logical representation of the network switching element and the network switch s specified characteristics and behaviors.

Some embodiments of the virtualization application layer translate the defined logical data path set into network configuration information for implementing the logical network switching element across the managed switching elements in the network. For example the virtualization application layer of some embodiments translates the defined logical data path set into a corresponding set of data flows. In some of these cases the virtualization application layer may take into account various factors e.g. logical switching elements that are currently implemented across the managed switching elements the current network topology of the network etc. in determining the corresponding set of data flows.

The network operating system layer of some embodiments configures the managed switching elements routing of network data. In some embodiments the network operating system instructs the managed switching elements to route network data according to the set of data flows determined by the virtualization application layer.

In some embodiments the network operating system layer maintains several views of the network based on the current network topology. One view that the network operating system layer maintains is a logical view. The logical view of the network includes the different logical switching elements that are implemented across the managed switching elements in some embodiments. Some embodiments of the network operating system layer maintain a managed view of the network. Such managed views include the different managed switching elements in the network i.e. the switching elements in the network that the network controllers control . In some embodiments the network operating system layer also maintains relationship data that relate the logical switching elements implemented across the managed switching elements to the managed switching elements.

While and other figures in this application may show a set of managed switching elements managed by a network controller some embodiments provide several network controllers also referred to as a cluster of network controllers or a control cluster for managing the set of managed switching elements e.g. virtual switches hardware switches . In other embodiments different control clusters may manage different sets of managed switching elements. Employing a cluster of network controllers in such embodiments to manage a set of managed switches increases the scalability of the managed network and increases the redundancy and reliability of the managed network. In some embodiments the network controllers in a control cluster share e.g. through the network operating system layer of the network controllers data related to the state of the managed network in order to synchronize the network controllers.

As mentioned above the network control system of some embodiments can manage both physical switching elements and software switching elements. illustrates an example of such a network control system. In particular this figure conceptually illustrates a network control system of some embodiments for managing TOR switching element and OVSs running on hosts in the racks of hosts and .

In this example the managed switching element and the OVSs running on the hosts in the racks of hosts and are edge switching elements because they are the last switching elements before end machines in the network. In particular the network controller manages the TOR switching element and the OVSs that are running on the hosts in the rack of hosts and the network controller manage the OVSs that are running on the hosts in the rack of hosts .

The above figures illustrate examples of network controllers that control edge switching elements in a network. However in some embodiments the network controllers can control non edge switching elements as well. illustrates a network control system that includes such network controllers. In particular conceptually illustrates a network control system of some embodiments for managing TOR switching elements and OVS running on hosts in the racks of hosts and .

As shown in the network controllers and manage edge switching elements and non edge switching elements. Specifically the network controller manages the TOR switching elements and and the OVSs running on the hosts in the rack of hosts . The network controller manages TOR switching element and the OVSs running on the hosts in the rack of hosts . In this example the TOR switching element and the OVSs running on the hosts in the racks of hosts and are edge switching elements and the TOR switching elements and are non edge switching elements.

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more computational or processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives random access memory RAM chips hard drives erasable programmable read only memories EPROMs electrically erasable programmable read only memories EEPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the system memory and the permanent storage device .

From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments.

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash memory device etc. and its corresponding drive as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices cameras e.g. webcams microphones or similar devices for receiving voice commands etc. The output devices display images generated by the electronic system or otherwise output data. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD as well as speakers or similar audio output devices. Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself. In addition some embodiments execute software stored in programmable logic devices PLDs ROM or RAM devices.

As used in this specification and any claims of this application the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification and any claims of this application the terms computer readable medium computer readable media and machine readable medium are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. In addition a number of the figures including conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. Furthermore the process could be implemented using several sub processes or as part of a larger macro process. Thus one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details but rather is to be defined by the appended claims.

