---

title: Software defined network controller
abstract: An extensible software defined network (SDN) controller is described that provides an application-aware framework that enable a variety of different user applications to communicate with the controller and that allows the controller to automatically configure devices in a network based on the needs of the applications. For example, the controller includes a plurality of different northbound interfaces that enable a variety of different user applications to communicate with the controller. The controller also includes multiple southbound protocols for configuring and enabling functionality in network devices based on the communications with the user applications.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09450817&OS=09450817&RS=09450817
owner: Juniper Networks, Inc.
number: 09450817
owner_city: Sunnyvale
owner_country: US
publication_date: 20130930
---
This application claims the benefit of U.S. Provisional Application No. 61 799 922 filed Mar. 15 2013 the entire content of which is incorporated by reference herein.

Large scale applications geographically distributed over large areas often process large distributed datasets that require massive data transfer over a wide area network. Service provider networks typically provide an extensive network infrastructure to provide packet based data services to the offered services. The network infrastructure typically includes a vast collection of access nodes aggregation nodes and high speed edge routers interconnected by communication links. These access devices typically execute various protocols and exchange signaling messages to anchor and manage subscriber sessions and communication flows associated with the subscribers.

In general an extensible software defined network SDN controller is described that provides an application aware framework that enables a variety of different user applications to communicate with the controller and that allows the controller to automatically configure devices in a network based on the needs of the applications. For example the controller includes a plurality of different northbound interfaces that enable a variety of different user applications to communicate with the controller. The controller also includes a plurality of southbound interface by which the controller receives topology information from deployed switching and routing infrastructure and that allows various southbound protocols to configure and enable functionality in network devices based on the communications with the user applications.

In example implementations the techniques of this disclosure provide an SDN controller that ties together service modules with standard southbound Application Programming Interfaces APIs and protocols that provide services such as policy service discovery registration stateful topology management path computation and provisioning using a plug in architecture. The SDN controller provides a set of northbound APIs for application development that can be used in both workflow and service creation applications as well as end user client applications.

Some examples of the SDN controller described herein may include domain wide network platforms with multiple programmable APIs including both OpenFlow and Path Computation Element PCE and may leverage and interoperate with existing distributed networking control planes in a hybrid model. This enables the seamless implementation of SDN and the realization of its benefits at performance and scale without breaking anything in the networking environment.

In some example implementations the SDN controller provides a mechanism for bridging the application to network divide allowing an application to signal requirements to the network and the network can be dynamically programmed to support the applications requirements. The SDN controller can greatly simplify the provisioning of complex network capabilities while enabling application and service innovation with predictable performance and better experiences without impacting operations.

The techniques of this disclosure may provide one or more advantages. For example the SDN controller improves operational efficiency as the network is automatically provisioned with no need for human intervention. The SDN controller can also provide greater service agility as networks can be dynamically morphed to support new applications experiences. The SDN controller can also provide capital efficiency i.e. an ability to do more with less because networks do not need to be over built or purpose built for a particular service or application. Rather network resources can be allocated only when needed and reserved for high priority applications and business critical applications.

In some examples a controller that manages a network of one or more network devices comprises one or more databases configured to store network topology information and network state information for the network devices. The controller also comprises one or more network device protocol interfaces wherein each of the network device protocol interfaces is configured to exchange state information with at least one of the network devices wherein the state information comprises at least one of network topology information and network device state information. The controller also comprises one or more application interfaces configured to receive from applications requests for application specific network configurations. The controller also comprises one or more core modules configured to compute respective network configurations to conform the network topology and network device states to satisfy the requests. The controller also comprises one or more core applications configured to generate network device state information to implement computed network configurations and to use the network device protocol interfaces to program the network device state information to the network devices to program the network configuration in the network.

In another example a method comprises storing by one or more databases of a controller that manages a network of one or more network devices network topology information and network state information for the network devices. The method also comprises exchanging with at least one of the network devices and by one or more network device protocol interfaces of the controller state information comprising at least one of network topology information and network device state information. The method also comprises receiving by one or more application interfaces of the controller and from applications requests for application specific network configurations. The method also comprises computing by one or more core modules of the controller respective network configurations to conform the network topology and network device states to satisfy the requests. The method also comprises generating by one or more core applications of the controller network device state information to implement computed network configurations and to use the network device protocol interfaces to program the network device state information to the network devices to program the network configuration in the network.

This disclosure also describes devices computing devices apparatuses and computer readable mediums storing instructions that may be configured to perform the techniques described herein.

The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features objects and advantages of the invention will be apparent from the description and drawings and from the claims.

In the example of service provider network includes a centralized controller that orchestrates various end to end solutions across various network devices of . As described in further detail below the SDN controller delivers a feedback loop mechanism between the network and client applications in both directions. Via controller applications can inform devices in service provider network of certain requested aspects such as service level agreements SLAs or guarantees. The SDN controller brings the application and service provider network together so that service provider network can adapt to the needs of the applications and so that the applications can adapt to the changing service provider network . In this manner controller provides a mechanism for real time application to network collaboration.

Aggregation network provides transport services for network traffic associated with subscribers . Aggregation network typically includes one or more aggregation nodes AG such as internal routers and switches that provide transport services between access nodes AXs and edge routers ERs . Aggregation nodes are nodes which aggregate several access nodes . After authentication and establishment of network access through access network or radio access network any one of subscriber devices may begin exchanging data packets with public network with such packets traversing AXs and AGs . Although not shown aggregation network may include other devices to provide security services load balancing billing deep packet inspection DPI and other services for mobile traffic traversing aggregation network .

As described herein controller operates to provide a central configuration point for configuring AGs of aggregation network provide transport services to transport traffic between AXs and edge routers . AGs may for example operate as label switched routers LSRs that forward traffic along transport label switched paths LSPs defined within aggregation network . Access nodes and edge routers may operate as endpoints for the LSPs to map subscriber traffic into and out of the LSPs. For example edge routers may map network services to individual LSPs within aggregation network while access nodes map the network services to individual end points EPs via the LSPs. Controller traffic engineers the LSPs through aggregation network according to the bandwidth Quality of Service QoS and availability requirements of network services applications as further described below.

As further described below controller includes a path computation module PCM that handles topology computation and path provisioning for the whole of aggregation network . That is the PCM of controller processes topology information for aggregation network performs path computation and selection in real time based on a variety of factors including current load conditions of subscriber traffic and provisions the LSPs within the aggregation network.

AXs and ERs operate at the borders of aggregation network and responsive to controller apply network services such as authorization policy provisioning and network connectivity to network traffic associated with subscribers in communication with access nodes . In the example of for ease of explanation service provider network is shown having two access nodes although the service provider network may typically service thousands or tens of thousands of access nodes.

In this example service provider network includes an access network with an AX and EP that provide subscriber devices A with access to aggregation network . In some examples AX may comprise a router that maintains routing information between subscriber devices A and aggregation network . AX for example typically includes Broadband Remote Access Server BRAS functionality to aggregate output from one or more EPs into a higher speed uplink to aggregation network . Edge router provides an anchor point of active sessions for subscriber devices A. In this sense edge router may maintain session data and operate as a termination point for communication sessions established with subscriber devices A that are currently accessing packet based services of public network via aggregation network .

EP may communicate with AX over a physical interface supporting various protocols. EP may comprise a switch a router a gateway or another terminal that operates as a demarcation point between customer equipment such as subscriber devices B and service provider equipment. In one example EP may comprise a digital subscriber line access multiplexer DSLAM or other switching device. Each of subscriber devices A may utilize a Point to Point Protocol PPP such as PPP over Asynchronous Transfer Mode ATM or PPP over Ethernet PPPoE to communicate with EP . For example using PPP one of subscriber devices may request access to aggregation network and provide login information such as a username and password for authentication by policy server not shown . Other embodiments may use other lines besides DSL lines such as cable Ethernet over a T1 T3 or other access links.

As shown in service provider network may include a radio access network with an access node AX and EP that provide subscriber devices B with access to aggregation network via radio signals. For example EP may be connected to one or more wireless radios or base stations not shown to wirelessly exchange packetized data with subscriber devices B. EP may comprise a switch a router a gateway or another terminal that aggregates the packetized data received from the wireless radios to AX . The packetized data may then be communicated through aggregation network of the service provider by way of AGs and edge routers ERs and ultimately to public network .

Aggregation network provides session management mobility management and transport services to support access by subscriber devices B to public network . Edge router provides an anchor point of active sessions for subscriber devices B. Edge router may maintain session data and operate as a termination point for communication sessions established with subscriber devices B that are currently accessing packet based services of public network via aggregation network .

In some examples one or more of access network and radio access network may comprise an optical access network. For example AX may comprise an optical line terminal OLT connected to one or more EPs or optical network units ONUs via optical fiber cables. In this case AX may convert electrical signals from aggregation network to optical signals using an optical emitter i.e. a laser and a modulator. AX then transmits the modulated optical signals over one or more optical fiber cables to the CPEs which act as termination points of the optical access network. As one example EP converts modulated optical signals received from AX to electrical signals for transmission to subscriber devices A over copper cables. As one example EP may comprise a switch located in a neighborhood or an office or apartment complex capable of providing access to a plurality of subscriber devices A. In other examples such as fiber to the home FTTH EP may comprise a gateway located directly at a single family premise or at an individual business capable of providing access to the one or more subscriber devices A at the premise. In the case of radio access network the EPs may be connected to wireless radios or base stations and convert the modulated optical signals to electrical signals for transmission to subscriber devices B via wireless signals.

Controller is an SDN controller that consists of several main logical pieces 1 South bound protocols for configuring and enabling functionality in network devices 2 A topology manager that maintains the topology 3 Core modules plugins like Traffic engineering database Path computation engine that enable applications 4 Core applications that use the protocols to provide functionality like PCE server and OpenFlow controller 5 User oriented applications that solve end to end use cases and 6 North bound interfaces that enable users user apps to talk to the Controller .

Modules of controller can communicate using open interfaces so that each module can be independently replaced to serve a special purpose application or network orchestration need. Controller is configured such that new modules can be added at various layers like application layer while re using the functionality provided by modules at other layers. Controller can communicate with other controllers at the network layer and or the application layer to carry out the desired user functionality.

Some examples of applications that controller can interface with that solve end to end use cases include Connectivity on demand Firewall in cloud video on demand data replication automatic service creation e.g. for mobile and the like. By having a modular design the techniques of this disclosure can provide incremental value added services on an extensible controller . The plugin model allows for a third party plugins like a different path computation engine to be plugged into controller to provide differentiated services. An example of this would be the Topology service where static network topology file or an active topology feed using Border Gateway Protocol Traffic Engineering BGP TE are allowed. The techniques of this disclosure enable the plugins and controller apps to provide RESTful APIs to application programmers. An example would be an Application Layer Traffic Optimization ALTO engine would use the topology service and provide a RESTFul interface to the ALTO clients. A RESTful web service also called a RESTful web API is a web service implemented using Hypertext Transfer Protocol HTTP and the principles of Representational State Transfer REST .

In one example a video server receives a subscriber s request from a video service application on the client for streaming video. The video server does not know where the subscriber is physically located. Rather than just relying on the video server to make a guess about which data center to send the video traffic from for that subscriber the video service application on the client can communicate with video on demand services module of controller via a northbound interface of video on demand services module . The video service application on the client can specify to video on demand services module the content and the content type high definition or standard definition . In response the video on demand services module can obtain information regarding the closest data center that has the capacity to service the content type and can indicate to the video service application on the client the data center from which it should fetch the video. For example video on demand services module may pass the request to connectivity scheduling module which may instruct ALTO module to query an ALTO server for the closest server or data center hosting the video. The ALTO server in the network is receiving BGP TE feeds from the network so is aware of the network topology. The ALTO server provides the content server location to ALTO module which passes the information up to the connectivity scheduling module .

In addition the video service application on the client can request a bandwidth guaranteed path e.g. path having 6 megabits of bandwidth with certain characteristics. In accordance with the techniques of this disclosure the video service application on the client can communicate with connectivity on demand services module of controller via a northbound interface of connectivity on demand services module to request the path. In some examples connectivity on demand services module may reference reservation repository and or may reference policies applied by policy engine . Connectivity on demand services module provides the client requirements to path computation engine in SDN network layer . In response the path computation engine computes a path in the network based on network topology information found in traffic engineering database TED .

After path computation engine computes the path having the guaranteed bandwidth the path provisioning module of SDN network layer provisions the computed path in the network. As one example path provisioning module or path computation element protocol PCEP Adapter provides information to the routers in the network to give the head end router an indication of the path such as by providing an Explicit Route Object ERO . The router signals the path using Resource Reservation Protocol with Traffic Engineering extensions RSVP TE to signal a Multi Protocol Label Switching MPLS LSP. As another example OpenFlow adaptor may provision the path using OpenFlow. In this manner controller can provide the video service application on the client with a guarantee from connectivity on demand services module that the bandwidth is available on the path through the network chosen by path computation engine and provisioned by path provisioning module . Thus the video service application client was able to not only able to connect to the best data center that could serve the content but also reserve network resources to ensure that user gets a great video watching experience.

In one example assume some kind of attack is happening on the Internet e.g. hackers trying to bring down the Department of Defense website . A network administrator wants to block all traffic coming from a certain set of hosts. Using the northbound API associated with firewall in cloud module the network administrator s client device can communicate with controller via a northbound API to request to block the traffic from the hosts. Firewall in cloud module converts the instructions into firewall specific rules that need to be programmed into routers in the network. For example firewall in cloud module may generate a firewall rule such as if traffic comes from IP address X then drop. Firewall in cloud module may use firewall policy engine and or firewall rules repository in generating the firewall rules.

Firewall in cloud module provides the firewall rules to firewall adapter . Firewall adapter programs the appropriate router s in the network with the generated rules using commit commands on the router. Commitment management module may be involved in this process. The router uses its usual firewall components but the firewall rules are received automatically from the firewall adapter . In this manner the network administrator does not need to individually program one or more routers in the network by individual specific configuration commands. Instead the network administrator can use the northbound interfaces to tell the SDN controller at a high level what outcome is desired in the network. The SDN controller breaks down the request into specific configuration commands and configures individual routers in the network with those commands. If the network uses independent standalone firewall devices that sit at the edges of the network then the firewall adapter will program the firewall devices rather than the routers and thus accomplish the same behavior.

In some aspects SDN controller also can provide a virtualization environment interconnecting two or more data centers for example. For example a network administrator may request a data replication event such as a request to cause data stored at a first data center to be replicated stored at a second data center. The network administrator s client device can interact with a northbound API of controller associated with data replication services module . Data replication services module receives the request and can invoke connectivity scheduling module . Connectivity scheduling module is receiving network topology data at TED such as via BGP TE . Connectivity scheduling module causes path computation engine to compute a path between the data centers. For example path computation engine may use CSPF or other algorithm providers for computing a path.

Path provisioning module provisions the path such as by using PCEP adapter to provide the path ERO to the head end router of the path. For example the head end router may signal an LSP over the path using RSVP TE for sending the data between the data centers enabling the replication. In the case of the data replication the LSP path may not need to be the shortest or best path which can be taken into account by path computation engine . Connectivity scheduling module may use OpenFlow adapter to map the data stream to the LSP. Multiple data streams can be mapped to the same network path. Connectivity scheduling module can provide path resizing and rerouting based on changing needs such as changing requests by the network administrator or changes to network topology as learned via BGP TE ALTO or other protocol on the southbound side of SDN network layer .

SDN controller can address issues facing customers. For example as customers continue to adopt virtualization technologies in their data centers and ramp up on virtual machines workload mobility and dynamic resource sharing this can cause problems in efficient configuration of the network that lead to sub par performance and operational inefficiencies i.e. too much manual overhead. In accordance with the techniques of this disclosure SDN controller can reduce the impact of certain problems with data center resource management particularly across multiple data centers around capacity planning data center utilization and efficient operations. This may allow for decreases in time to market for new applications in services within existing data centers. The SDN controller may provide certain benefits to the customers such as network resource isolation while retaining control dynamic allocation and sharing of resources leading to greater efficiencies and an ability to more easily build services utilizing resources across multiple data centers.

By using published and well defined interfaces between its various modules the SDN controller can enable development and deployment of custom applications and customized network engineering. In one example a customer might write its own path computation module that is highly optimized for that customer s network topology e.g. ring topology or that customer s application. The customer could then replace the standard path computation module in the SDN controller with the customized path computation module while retaining all other functionality.

In some aspects SDN controller provides the ability to selectively steer traffic via an application trigger in a dynamic manner. Based upon an application trigger to sample a particular video stream for quality monitoring SDN controller which in this case includes OpenFlow Adapter will dynamically program the router to steer the selected video stream out an alternative port for video quality monitoring purposes.

Many customers do not have the desired level of visibility and control into the applications running on their network. Not just this visibility but then also the ability to make proactive critical real time decisions on these flows is an important requirement. In the security space SDN controller allows customers to control threats by monitoring traffic for attacks by sending a stream to external analyzer and dynamically isolating offending flows users when a threat is identified.

Another application where SDN controller may be used is lawful intercept whereby individual traffic flows can be identified and then according to a policy dynamically mirrored to an external monitor for further analysis and processing. SDN controller can provide proactive and automated application monitoring and intelligence that will speed time to business decisions and outcomes. SDN controller can also reduce security risks and reduce overhead for application visibility and control by bypassing manual methods of collection and analysis. SDN controller can also reduce overall network expenditure since the SDN controller can steer flows from multiple routers along the same path without requiring a per router lawful intercept port.

In another example aspect SDN controller can be used to dynamically reserve bandwidth for high definition video sessions.

In one example a bandwidth calendaring application BCA executing on connectivity scheduling module accepts requests from client applications for one or more temporarily dedicated paths between specified endpoints. Connectivity scheduling module interfaces with path computation engine in the SDN network layer of controller . TED receives base network topology and overlay network topology information from network devices e.g. via BGP TE module and ALTO module . Path computation engine analyzes the various topologies to reconcile requests from multiple client applications and attempts to identify paths through a layer or combination of layers of the network that can be established at the requested time in view of the specifications requested for the temporarily dedicated paths and the anticipated bandwidth capacity available in the network.

Path computation engine schedules the identified paths through the one or more layers of the network to carry traffic for the requested paths. To then establish a requested path path computation engine instructs path provisioning module to program at the scheduled time path forwarding information into one or more network nodes at any layer of the multi layer multi topology network that participates in forwarding traffic along the identified path. For example path provisioning module may provide the computed path to the head end router of the path which in turn uses the path information to signal an LSP along the specified path using RSVP TE. In another example path provisioning module may program path forwarding information e.g. including MPLS labels directly into each network device along the path. In this way connectivity scheduling module and path computation engine may establish dedicated bandwidth channels in the form of reserved paths through the network as well as steer traffic onto the dedicated bandwidth channels to provide connectivity between distributed client applications for instance.

The techniques may provide one or more advantages. For example connectivity scheduling module may have access by operation of path computation engine and TED to an enhanced view of the current state of the network at multiple different layers which may enable path computation engine to identify paths that are not visible to a label edge router for example having a more limited view. Path computation engine may additionally by virtue of having access to this enhanced view steer traffic to underutilized portions of the network to increase the network capacity utilization. Still further using connectivity scheduling module and path computation engine to identify establish and in some cases preempt temporarily dedicated paths for reconciling multiple possibly conflicting application requests may reduce first in time first in right access to network resources in favor of explicit centralized prioritization of application requests for dedicated paths.

In some examples a northbound API GUI associated with connectivity scheduling module can show MPLS paths and bandwidth reservations. The northbound API GUI can also include SSH terminals into routers to show OpenFlow entries. In some examples connectivity scheduling module may provide a Bandwidth Calendaring Application Hybrid Mode that uses a combination of both Openflow and PCE to program devices in the network. Path provisioning module uses PCEP adapter to program specific bandwidth reservations across backbone via MPLS for the user s video flows. Path provisioning module uses OpenFlow adapter to program a specific forwarding path for the users video flows.

In some examples path computation engine may determine that an end to end path through the overlay network layer is not available and so PCEP adapter may program a tunnel on a path computed in a base network layer and store a new overlay network link for the tunnel to the topology information for the overlay network layer in the multi topology traffic engineering database . Openflow adapter may program the network device state information for the tunnel to overlay switches in the overlay network layer based on the new overlay network link stored by the PCEP adapter as part of the end to end path to service a request. In some examples PCEP adapter may refer to PCEP library . PCEP adapter may in some examples refer to resource dependency .

In other examples connectivity scheduling module may provide a Bandwidth Calendaring Application Openflow Mode in which path provisioning module uses only OpenFlow adapter to program the network for the specific forwarding path for user s video flows.

Further example mechanisms for SDN controlled bandwidth calendaring are found in U.S. application Ser. No. 13 339 983 filed Dec. 29 2011 entitled Multi Topology Resource Scheduling within a Computer Network the entire content of which is incorporated by reference herein.

In some examples SDN controller can also provide mobile network control. Mobile core control plane applications such as subscriber session setup authentication charging and signaling execute on general purpose hardware of the data center intermediated with the mobile core by SDN controller . Subscriber devices issue service requests via radio access networks toward the mobile core which in turn redirects the control plane messages to the SDN controller to encapsulate the control plane messages within a SDN protocol. For each such service request from a subscriber device a northbound interface associated with mobile services module of SDN controller extracts and processes the control plane messages for subscriber session management. The mobile services adapter in the SDN network layer provides to the mobile core responsive to the service request and using the SDN protocol any service responses or other control plane messages that the mobile core forwards to the corresponding subscriber device for subscriber session management and control. In addition the mobile core control plane applications of the data center may determine and communicate to northbound interfaces of mobile services module mobility specific information for processing and forwarding subscriber data traffic for the subscriber devices. Mobile services adapter in turn programs the support nodes of the mobile core data plane with the mobility specific information to configure data plane processing and forwarding for subscriber data traffic associated with a subscriber session for the subscriber device.

For example assume a mobile subscriber wants to communicate from his mobile handset out of California to a friend s mobile device in Minnesota and there is no network path already set up between the two. The mobile handset will interface with northbound APIs of mobile services which in turn cause mobile services adapter to first set up an MPLS transport path between California and Minnesota. For example mobile services may involve PCE to determine an appropriate path. Mobile services adapter can call in the mobile specification application to signal all the mobile protocol parameters so that the mobile phone call can be successfully completed between California and Minnesota. In some cases mobile services adapter can configure each node in the path with the appropriate parameters and in other cases mobile services adapter can provide the parameter information to the requesting mobile device or other device to allow that device to perform the signaling and setup based on the parameters. After those parameters are all set up the user s mobile phone call connects. In this manner controller can facilitate automatic services creation.

Example mechanisms for an SDN controlled mobile network that can be used by controller are found in U.S. application Ser. No. 13 724 975 filed Dec. 21 2012 entitled Software Defined Mobile Core the entire content of which is incorporated by reference herein.

Controller includes a control unit coupled to a network interface to exchange packets with other network devices by inbound link and outbound link . Control unit may include one or more processors not shown in that execute software instructions such as those used to define a software or computer program stored to a computer readable storage medium again not shown in such as non transitory computer readable mediums including a storage device e.g. a disk drive or an optical drive or a memory such as Flash memory or random access memory RAM or any other type of volatile or non volatile memory that stores instructions to cause the one or more processors to perform the techniques described herein. Alternatively or additionally control unit may comprise dedicated hardware such as one or more integrated circuits one or more Application Specific Integrated Circuits ASICs one or more Application Specific Special Processors ASSPs one or more Field Programmable Gate Arrays FPGAs or any combination of one or more of the foregoing examples of dedicated hardware for performing the techniques described herein.

Control unit provides an operating environment for network services applications access authorization provisioning module path computation element and edge authorization provisioning module . In one example these modules may be implemented as one or more processes executing on one or more virtual machines of one or more servers. That is while generally illustrated and described as executing on a single controller aspects of these modules may be delegated to other computing devices.

Network services applications represent one or more processes that provide services to clients of a service provider network that includes controller to manage connectivity in the aggregation domain alternatively referred to as the path computation domain according to techniques of this disclosure. Network services applications may provide for instance include Voice over IP VoIP Video on Demand VOD bulk transport walled open garden IP Mobility Subsystem IMS and other mobility services and Internet services to clients of the service provider network. Networks services applications require services provided by path computation element such as node management session management and policy enforcement. Each of network services applications may include client interface by which one or more client applications request services. Client interface may represent a command line interface CLI or graphical user interface GUI for instance. Client may also or alternatively provide an application programming interface API such as a web service to client applications.

Network services applications issue path requests to path computation element to request paths in a path computation domain controlled by controller . For example a path request includes a required bandwidth or other constraint and two endpoints representing an access node and an edge node that communicate over the path computation domain managed by controller . Path requests may further specify time date during which paths must be operational and CoS parameters for instance bandwidth required per class for certain paths .

Path computation element accepts path requests from network services applications to establish paths between the endpoints over the path computation domain. Paths may be requested for different times and dates and with disparate bandwidth requirements. Path computation element reconciling path requests from network services applications to multiplex requested paths onto the path computation domain based on requested path parameters and anticipated network resource availability.

To intelligently compute and establish paths through the path computation domain path computation element includes topology module to receive topology information describing available resources of the path computation domain including access aggregation and edge nodes interfaces thereof and interconnecting communication links.

Path computation module of path computation element computes requested paths through the path computation domain. In general paths are unidirectional. Upon computing paths path computation module schedules the paths for provisioning by path provisioning module . A computed path includes path information usable by path provisioning module to establish the path in the network. Provisioning a path may require path validation prior to committing the path to provide for packet transport. Path computation module may correspond to path computation engine and path provisioning module may correspond to path provisioning module .

Path computation module includes data structures to store path information for computing and establishing requested paths. These data structures include constraints path requirements operational configuration and path export . Network services applications may invoke northbound API to install query data from these data structures. Constraints represent a data structure that describes external constraints upon path computation. Constraints allow network services applications to e.g. modify link attributes before path computation module computes a set of paths. For examples Radio Frequency RF modules not shown may edit links to indicate that resources are shared between a group and resources must be allocated accordingly.

Network services applications may modify attributes of a link to effect resulting traffic engineering computations. In such instances link attributes may override attributes received from topology indication module and remain in effect for the duration of the node attendant port in the topology. A link edit message to constraints may include a link descriptor specifying a node identifier and port index together with link attributes specifying a bandwidth expected time to transmit shared link group and fate shared group for instance. The link edit message may be sent by the PCE .

Operational configuration represents a data structure that provides configuration information to path computation element to configure the path computation algorithm with respect to for example class of service CoS descriptors and detour behaviors. Operational configuration may receive operational configuration information in accordance with CCP. An operational configuration message specifies CoS value queue depth queue depth priority scheduling discipline over provisioning factors detour type path failure mode and detour path failure mode for instance. A single CoS profile may be used for the entire path computation domain. Example CoS values are described in U.S. application Ser. No. 13 842 453 filed Mar. 15 2013 entitled Aggregation Network with Centralized Control the entire content of which is incorporated by reference herein. The Service Class assigned to a Class of Service may be independent of the node as an attribute of the path computation domain.

Path export represents an interface that stores path descriptors for all paths currently committed or established in the path computation domain. In response to queries received via northbound API path export returns one or more path descriptors. Queries received may request paths between any two edge and access nodes terminating the path s . In some examples path descriptors may be used by network services applications to set up forwarding configuration at the edge and access nodes terminating the path s . A path descriptor may include an Explicit Route Object ERO . A path descriptor or path information may be sent responsive to a query from an interested party. A path export message delivers path information including path type primary or detour bandwidth for each CoS value. In response to receiving the path descriptor the receiving device may use RSVP TE to signal an MPLS LSP from the ingress to the egress of the path.

Path requirements represent an interface that receives path requests for paths to be computed by path computation module and provides these path requests including path requirements to path engine for computation. Path requirements may be received or may be handled by the PCE. In such instances a path requirement message may include a path descriptor having an ingress node identifier and egress node identifier for the nodes terminating the specified path along with request parameters including CoS value and bandwidth. A path requirement message may add to or delete from existing path requirements for the specified path.

Topology module includes topology indication module to handle topology discovery and where needed to maintain control channels between path computation element and nodes of the path computation domain. Topology indication module may include an interface to describe received topologies to path computation module .

Topology indication module may use a topology discovery protocol to describe the path computation domain topology to path computation module . In one example using a cloud control protocol mechanism for topology discovery topology indication module may receive a list of node neighbors with each neighbor including a node identifier local port index and remote port index as well as a list of link attributes each specifying a port index bandwidth expected time to transmit shared link group and fate shared group for instance.

Topology indication module may communicate with a topology server such as a routing protocol route reflector to receive topology information for a network layer of the network. Topology indication module may include a routing protocol process that executes a routing protocol to receive routing protocol advertisements such as Open Shortest Path First OSPF or Intermediate System to Intermediate System IS IS link state advertisements LSAs or Border Gateway Protocol BGP UPDATE messages. Topology indication module may in some instances be a passive listener that neither forwards nor originates routing protocol advertisements. In some instances topology indication module may alternatively or additionally execute a topology discovery mechanism such as an interface for an Application Layer Traffic Optimization ALTO service. Topology indication module may therefore receive a digest of topology information collected by a topology server e.g. an ALTO server rather than executing a routing protocol to receive routing protocol advertisements directly.

In some examples topology indication module receives topology information that includes traffic engineering TE information. Topology indication module may for example execute Intermediate System to Intermediate System with TE extensions IS IS TE or Open Shortest Path First with TE extensions OSPF TE to receive TE information for advertised links. Such TE information includes one or more of the link state administrative attributes and metrics such as bandwidth available for use at various LSP priority levels of links connecting routers of the path computation domain. In some instances indication module executes BGP TE to receive advertised TE information for inter autonomous system and other out of network links. Additional details regarding executing BGP to receive TE info are found in U.S. patent application Ser. No. 13 110 987 filed May 19 2011 and entitled DYNAMICALLY GENERATING APPLICATION LAYER TRAFFIC OPTIMIZATION PROTOCOL MAPS which is incorporated herein by reference in its entirety.

Traffic engineering database TED stores topology information received by topology indication module for a network that constitutes a path computation domain for controller to a computer readable storage medium not shown . TED may include one or more link state databases LSDBs where link and node data is received in routing protocol advertisements received from a topology server and or discovered by link layer entities such as an overlay controller and then provided to topology indication module . In some instances an operator may configure traffic engineering or other topology information within MT TED via a client interface.

Path engine accepts the current topology snapshot of the path computation domain in the form of TED and computes using TED CoS aware traffic engineered paths between nodes as indicated by configured node specific policy constraints and or through dynamic networking with external modules via APIs. Path engine may further compute detours for all primary paths on a per CoS basis according to configured failover and capacity requirements as specified in operational configuration and path requirements respectively .

In general to compute a requested path path engine determines based on TED and all specified constraints whether there exists a path in the layer that satisfies the TE specifications for the requested path for the duration of the requested time. Path engine may use the Djikstra constrained SPF CSPF path computation algorithms for identifying satisfactory paths though the path computation domain. If there are no TE constraints path engine may revert to SPF. If a satisfactory computed path for the requested path exists path engine provides a path descriptor for the computed path to path manager to establish the path using path provisioning module . A path computed by path engine may be referred to as a computed path until such time as path provisioning programs the scheduled path into the network whereupon the scheduled path becomes an active or committed path. A scheduled or active path is a temporarily dedicated bandwidth channel for the scheduled time in which the path is or is to become operational to transport flows.

Path manager establishes computed scheduled paths using path provisioning module which in this instance includes forwarding information base FIB configuration module illustrated as FIB CONFIG. policer configuration module illustrated as POLICER CONFIG. and CoS scheduler configuration module illustrated as COS SCHEDULER CONFIG. .

FIB configuration module programs forwarding information to data planes of aggregation nodes or access nodes of the path computation domain. The FIB of an aggregation node or access node includes the MPLS switching table the detour path for each primary LSP the CoS scheduler per interface and policers at LSP ingress. FIB configuration module may implement for instance a software defined networking SDN protocol such as the OpenFlow protocol or the I2RS protocol to provide and direct the nodes to install forwarding information to their respective data planes. Accordingly the FIB may refer to forwarding tables in the form of for instance one or more OpenFlow flow tables each comprising one or more flow table entries that specify handling of matching packets. FIB configuration module may in addition or alternatively implement other interface types such as a Simple Network Management Protocol SNMP interface path computation element protocol PCEP interface a Device Management Interface DMI a CLI Interface to the Routing System I2RS or any other node configuration interface. FIB configuration module interface establishes communication sessions with aggregation nodes or access nodes to install forwarding information to receive path setup event information such as confirmation that received forwarding information has been successfully installed or that received forwarding information cannot be installed indicating FIB configuration failure . Additional details regarding PCEP may be found in J. Medved et al. U.S. patent application Ser. No. 13 324 861 PATH COMPUTATION ELEMENT COMMUNICATION PROTOCOL PCEP EXTENSIONS FOR STATEFUL LABEL SWITCHED PATH MANAGEMENT filed Dec. 13 2011 and in Path Computation Element PCE Communication Protocol PCEP Network Working Group Request for Comment 5440 March 2009 the entire contents of each of which being incorporated by reference herein. Additional details regarding I2RS are found in Interface to the Routing System Framework Network Working Group Internet draft Jul. 30 2012 which is incorporated by reference as if fully set forth herein.

FIB configuration module may add change i.e. implicit add or delete forwarding table entries in accordance with information received from path computation module . A FIB configuration message from path computation module to FIB configuration module may specify an event type add or delete a node identifier a path identifier one or more forwarding table entries each including an ingress port index ingress label egress port index and egress label and a detour path specifying a path identifier and CoS mode.

Policer configuration module may be invoked by path computation module to request a policer be installed on a particular aggregation node or access node for a particular LSP ingress. As noted above the FIBs for aggregation nodes or access nodes include policers at LSP ingress. Policer configuration module may receive policer configuration requests. A policer configuration request message may specify an event type add change or delete a node identifier an LSP identifier and for each class of service a list of policer information including CoS value maximum bandwidth burst and drop remark. FIB configuration module configures the policers in accordance with the policer configuration requests.

CoS scheduler configuration module may be invoked by path computation module to request configuration of CoS scheduler on the aggregation nodes or access nodes. CoS scheduler configuration module may receive the CoS scheduler configuration information. A scheduling configuration request message may specify an event type change a node identifier a port identity value port index and configuration information specifying bandwidth queue depth and scheduling discipline for instance.

A base network layer of network or base network includes network switches A B collectively network switches connected to hosts B C and arranged in a physical topology. Network switches receive and forward packet data units PDUs for network flows according to forwarding information programmed into the switches by an administrator or external entity e.g. overlay controller or multi topology path computation element and or according to forwarding information learned by the switches whether by operation of one or more protocols e.g. interior gateway protocols IGPs or by recording information learned during PDU forwarding. Each of network switches may represent a router a layer three L3 switch a layer two L2 switch an L2 L3 switch or another network device that switches traffic according to forwarding information. Accordingly PDUs forwarded by network switches A may include for example L3 network packets e.g. Internet Protocol packets and or L2 packets e.g. Ethernet datagrams or Asynchronous Transfer Mode ATM cells . PDUs may be unicast multicast anycast and or broadcast.

An overlay network layer of network includes overlay switches A B collectively overlay switches arranged in a virtual topology over a physical topology defined by network switches . Individual links of the virtual topology of the overlay network or overlay links may be established paths through the base network and or physical links connecting overlay switches . The overlay network may represent a virtual private network VPN an OpenFlow network consisting of one or more OpenFlow switches or an application layer network with selection functionality built in to endpoint devices for example. Accordingly each of overlay switches may represent a router or routing instance e.g. a virtual routing and forwarding VRF instance a Virtual Private Local Area Network LAN Service VPLS instance a dedicated L2 L3 or L2 L3 switch or a virtual or soft switch e.g. an OpenFlow switch implemented by a router or by a dedicated switch for example. Overlay switch A for instance represents a dedicated overlay switch. Overlay switch B is implemented by network switch A and may represent for instance a soft switch. Network may include multiple overlay network layers of different or similar types e.g. multiple VPNs and or OpenFlow networks .

Topology server receives topology information from network switches for the base network of multi topology network . For example topology server may execute one or more IGPs or Exterior Gateway Protocols e.g. the Border Gateway Protocol BGP to listen to routing protocol advertisements sent by network switches . Topology server collects and stores the base network topology information then provides the base network topology information to multi topology path computation element PCE in base topology update messages . Topology information may include traffic engineering information for the network links such as the links administrative attributes and bandwidth at various priority levels available for use by label switched paths LSPs . In some examples network switches may send topology update messages to topology server that specify L2 link information for L2 links connecting the network switches. In some examples topology server is a component of PCE .

Overlay controller receives topology information for the overlay network of multi topology network in topology update messages sent by overlay switches in respective communication sessions . Topology update messages sent by overlay switches may include virtual and physical switch port information PDUs and associated metadata specifying respective ports and or interfaces on which PDUs are received. In some examples overlay controller is a routing protocol listener that executes one or more routing protocols to receive routing protocol advertisements sent by overlay switches . Such routing protocol advertisements may be associated with one or more VRFs for instance. Overlay controller collects and stores the overlay topology information then provides the overlay topology information to PCE in overlay topology update messages . In some examples overlay controller is a component of PCE .

Network switches may be configured to or otherwise directed to establish paths through the base network of multi topology network . Such paths may include for instance IP tunnels such as Generic Route Encapsulation GRE tunnels General Packet Radio Service GPRS Tunneling Protocol GTP tunnels LSPs or a simple route through the base network or a VPN identified by a static route with a route target for instance . Network switches provide path status information for paths established through the base network of multi topology network to PCE in communication sessions . Path status alternatively path state or LSP state information may include descriptors for existing operational paths as well as indications that an established path or path setup operation has failed. For example network switch A may attempt establish an LSP using a reservation protocol such as Resource reSerVation Protocol RSVP but fail due to insufficient network resources along a path specified by an Explicit Route Object ERO . As a result network switch A may provide an indication that the path setup operation failed to PCE in a communication session . PCE receives path status information and adds established paths through the base network of network as links in the overlay network topology.

PCE presents an interface by which clients A N collectively clients may request for a specified time a dedicated path between any combination of hosts . For example client A may request a 100 MB s path from host A to host B from 1 PM to 3 PM on a particular date. As another example client N may request a 50 MB s path from host A to host C from 2 PM to 3 PM on the same date. As a still further example client A may request a mesh or multipath of 50 MB s paths connecting each of hosts to one another from 4 PM to 6 PM on a particular date. The requested mesh is a multipoint to multipoint path consisting of multiple point to point paths. In addition to the bandwidth hosts and time path parameters exemplified above clients may request paths that conform to other quality of service QoS path request parameters such as latency and jitter and may further specify additional associated classifiers to identify a flow between the specified endpoints. Example flow classifiers or parameters are provided below.

PCE uses base network topology information for network received from topology server overlay network topology information for network received from overlay controller and path status information received from network switches to compute and schedule paths between hosts through network that satisfy the parameters for the paths requested by clients . PCE may receive multiple path requests from clients that overlap in time. PCE reconciles these requests by scheduling corresponding paths for the path requests that traverse different parts of network and increase capacity utilization for example or by denying some of the path requests.

At the scheduled time for a scheduled path PCE installs forwarding information to network nodes e.g. overlay switches and network switches to cause the nodes to forward traffic in a manner that satisfies the requested path parameters. In some examples PCE stores all path requests and then attempts to compute and establish paths at respective requested times. In some examples PCE receives path requests and schedules respective satisfactory paths in advance of the requested times. PCE in such examples stores the scheduled paths and uses resources allocated in advance for the scheduled paths as a constraint when attempting to compute and schedule later requested paths. For example where a scheduled path will consume all available bandwidth on a particular link at a particular time PCE may later compute a requested path at an overlapping time such that the later requested path does not include the completely subscribed link.

A requested path may traverse either or both domains of network . That is a requested path may traverse either or both of the base network and overlay network of multi topology network . For example because both host B and host C couple in the base network domain to one of network switches a requested path for traffic from host B to host C may traverse only the base network domain as a simple network route for instance from network switch A to network switch B. Host A however couples in the overlay network domain to overlay switch A. As a result any requested path for traffic between host A and host C for example first traverses the overlay network domain and then traverses the base network domain.

PCE installs forwarding information to overlay switches using overlay controller . Overlay controller presents a programming interface by which PCE may add delete and modify forwarding information in overlay switches . Forwarding information of overlay switches may include a flow table having one or more entries that specify field values for matching PDU properties and a set of forwarding actions to apply to matching PDUs. A set of one or more PDUs that match a particular flow entries represent a flow. Flows may be broadly classified using any parameter of a PDU such as source and destination MAC and IP addresses a Virtual Local Area Network VLAN tag transport layer information a Multiprotocol Label Switching MPLS or Generalized MPLS GMPLS label and an ingress port of a network device receiving the flow. For example a flow may be all PDUs transmitted in a Transmission Control Protocol TCP connection all PDUs sourced by a particular MAC address or IP address all PDUs having the same VLAN tag or all PDUs received at the same switch port.

PCE invokes the programming interface of overlay controller by sending overlay network path setup messages directing overlay controller to establish paths in the overlay network of network and or steer flows from hosts onto established paths. Overlay controller responds to overlay network path setup messages by installing to overlay switches using communication sessions forwarding information that implements the paths and or directs flows received from hosts onto established paths.

PCE installs forwarding information to network switches using communication sessions . Each of network switches may present a programming interface in the form of a management interface configuration interface and or a path computation client PCC . PCE may invoke the programming interface of network switches to configure a tunnel e.g. an LSP install static routes configure a VPLS instance configure an Integrated Routing and Bridging IRB interface and to otherwise configure network switches to forward packet flows in a specified manner. In some instances PCE directs one or more of networks switches to signal a traffic engineered LSP TE LSP through the base network of network to establish a path. In this way PCE may program a scheduled path through network by invoking a programming interface of only the head network device for the path.

At the end of a scheduled time for a requested path PCE may again invoke the programming interfaces of network switches and overlay switches to remove forwarding information implementing the requested paths. In this way PCE frees resources for future scheduled paths.

Because PCE has an enhanced view of the current state of the network at both the overlay network layer and base network PCE may identify paths that are not visible to any one of network switches or overlay switches having a more limited view. PCE may additionally by virtue of having access to this enhanced view steer traffic to underutilized portions of network to increase capacity utilization of network . In addition centralizing the path computation and establishment with PCE may allow network operators to reconcile multiple possibly conflicting application path requests and may reduce first in time first in right access to network resources in favor of explicit centralized prioritization of application requests for dedicated paths.

PCE includes a control unit and a network interface not shown to exchange packets with other network devices. Control unit may include one or more processors not shown in that execute software instructions such as those used to define a software or computer program stored to a computer readable storage medium again not shown in such as non transitory computer readable mediums including a storage device e.g. a disk drive or an optical drive or a memory such as Flash memory or random access memory RAM or any other type of volatile or non volatile memory that stores instructions to cause the one or more processors to perform the techniques described herein. Alternatively or additionally control unit may comprise dedicated hardware such as one or more integrated circuits one or more Application Specific Integrated Circuits ASICs one or more Application Specific Special Processors ASSPs one or more Field Programmable Gate Arrays FPGAs or any combination of one or more of the foregoing examples of dedicated hardware for performing the techniques described herein.

Control unit provides an operating environment for bandwidth calendaring application BCA . In one example BCA is a Java application executing on a virtual machine executed by PCE . However BCA may be implemented using any suitable programming language that produces instructions executable by a suitable platform. Furthermore while illustrated and described executing on a path computation element aspects of BCA may be delegated to other computing devices.

Bandwidth calendaring application accepts requests from client applications to schedule point to point and multipoint to multipoint paths multipaths between different endpoints. Reference herein to a path encompasses multipaths. Paths may be scheduled at different times and dates with BCA reconciling path requests from multiple client applications to schedule requested paths through a network based on requested path parameters and anticipated network resource availability.

Clients request paths through a network using client interface of BCA . In general a path request includes a requested date time a required bandwidth or other constraint and at least two endpoints. Client interface may be a command line interface CLI or graphical user interface GUI for instance. Client may also or alternatively provide an application programming interface API such as a web service. A user uses a client application to invoke client interface to input path request parameters and submit the request to BCA . Client interface receives path requests from clients and pushes the path requests to path request queue a data structure that stores path requests for computation distribution by path manager .

To compute and schedule paths through a network intelligently BCA receives topology information describing available resources at multiple layers of the network. Topology server interface illustrated as topology server IF communicates with a topology server to receive topology information for a base network layer of the network while overlay controller interface communicates with an overlay controller to receive topology information for an overlay network layer of the network. Topology server interface may include a routing protocol daemon that executes a routing protocol to receive routing protocol advertisements such as Open Shortest Path First OSPF or Intermediate System to Intermediate System IS IS link state advertisements LSAs or BGP UPDATE messages. Topology server interface may in some instances be a passive listener that neither forwards nor originates routing protocol advertisements.

In this example topology server interface receives topology information that includes traffic engineering TE information. Topology server interface may for example execute Intermediate System to Intermediate System with TE extensions IS IS TE or Open Shortest Path First with TE extensions OSPF TE to receive TE information for advertised links. Such TE information includes one or more of the link state administrative attributes and metrics such as bandwidth available for use at various LSP priority levels of links connecting routers of the domain. In some instances topology server interface executes Border Gateway Protocol to receive advertised TE information for inter AS and other out of network links. Additional details regarding executing BGP to receive TE info are found in U.S. patent application Ser. No. 13 110 987 filed May 19 2011 and entitled DYNAMICALLY GENERATING APPLICATION LAYER TRAFFIC OPTIMIZATION PROTOCOL MAPS which is incorporated herein by reference in its entirety.

Topology server interface may in some instances receive a digest of topology information collected by a topology server rather than executing a routing protocol to receive routing protocol advertisements directly. Topology server interface stores base network topology information with TE information in multi topology traffic engineering database illustrated as multi topology TED hereinafter MT TED which is stored by a computer readable storage medium of control unit for use in path computation. MT TED is described in further detail below.

Overlay controller interface illustrated as overlay controller IF receives topology information from an overlay controller that describes overlay network links connecting overlay switches. In general overlay network links are not advertised by network switches e.g. routers of the base network for the overlay network and so will not be described by topology information received by topology server interface . An overlay controller augments the base network topology with overlay network topology links by providing overlay network topology information to overlay controller interface which stores the overlay network topology information to MT TED . Overlay controller interface may receive topology information for multiple different overlay networks including VPNs and or OpenFlow networks. Different overlay networks may require different instances of overlay controller interface that communicate with network switches of the overlay network or with a topology server for example to receive overlay network topology information for respective overlay networks.

Multi topology traffic engineering database stores topology information for a base network layer and one or more overlay network layers of a network that constitutes a path computation domain for PCE . MT TED may organize topology information for respective network layers hierarchically with the base network topology information supporting the topology information for one or more overlay networks. Paths in a lower layer topology may appear as links in a higher layer topology. For example tunnels e.g. TE LSPs created in the base network layer can appears as links in an overlay network TE topology. BCA may then correlate overlay network links with paths established in the base network layer to efficiently compute paths that cross multiple overlay topologies. MT TED may include one or more link state databases LSDBs where link and node data is received in routing protocol advertisements received from a topology server and or discovered by link layer entities such as an overlay controller and then provided to BCA via overlay controller interface . In some instances an operator may configure traffic engineering or other topology information within MT TED via operator interface .

Topology server interface may also receive from a topology server or by execution of routing protocols to receive routing protocol advertisements that include reachability information endpoint information that describes endpoints reachable by specified nodes in any of the network topologies. Topology server interface may receive endpoint information for a base layer of the network as well as for one or more services e.g. VPNs provided by the network that may correspond to overlay networks of the network. Endpoint information may associate network address prefixes with a nodes of the multi topology network layers where network address prefixes may be e.g. IPv4 or IPv6. For example topology server interface may receive a BGP UPDATE message advertising a particular subnet as reachable from a particular node of the base network. As another example topology server interface may receive an Application Layer Traffic Optimization map that includes PIDs associating respective nodes of a multi topology network layer with network address prefixes reachable from the nodes. Endpoints that have network addresses that are members of the subnet are therefore reachable from the node and BCA may calculate paths for those endpoints to terminate i.e. begin or end at the node. Topology server interface stores endpoint information received for a layer to a corresponding one of endpoint databases A K illustrated as endpoint DB A K and collectively referred to as endpoint databases where K refers to a number of layers of the multi topology network that constitutes a path computation domain for PCE . Some of endpoint databases may therefore be associated with respective service instances e.g. respective VPNs that constitute overlay network layers of a multi topology network. BCA may therefore use endpoint databases to locate and validate endpoints specified in path requests received from clients.

Each of service path engines A K collectively SPEs compute requested paths through a layer of the multi topology network with which it is associated and for which it is responsible. Control unit may execute multiple SPEs concurrently e.g. as separate processes. Each of SPEs is associated with a corresponding one of generated path databases A K illustrated as generated path DB A K and collectively referred to as generated path databases . Path manager dequeues path requests from path request queue and assigns path requests to SPEs based on the layer of the multi topology network in which the endpoints reside as determined by path manager from endpoint databases . That is endpoints reachable by layers of a multi topology network that is a path computation domain for PCE are stored by at least one of endpoint databases and path manager determines the one or more endpoint databases that include endpoints specified for a dequeued path request.

Paths are unidirectional. If a client requests a bidirectional path path manager triggers two path requests for the requested path one for each direction. In some cases a path may cross multiple layers of the network e.g. at a gateway to the base layer that is implemented by one of the overlay network nodes or at a network node that participates in multiple overlay networks. In such cases multiple SPEs may cooperate to compute segments of the multi layer path that path manager stitches together at the gateway. Upon computing paths SPEs schedule the paths by storing the paths to respective generated path databases . A scheduled path stored in one of generated path databases includes path information used by path manager to establish the path in the network and may include scheduling information used by scheduler to trigger path manager to establish the path. As described in further detail below path scheduling may require locking generated path databases to perform path validation prior to committing the path.

When a servicing path request received from path manager an SPE may initially validate the request by determining from endpoint databases that the endpoints for the requested path whether expressed as logical interfaces or network addresses are known to PCE i.e. exist within the path computation domain of PCE . The SPE may additionally validate flow classifiers to ensure that the flow classifiers specified for a requested path exist. If initial validation fails for either both of these reasons the SPE rejects the requested path and path manager sends a path rejection message detailing the reasons to the requesting client via client interface .

To compute a requested path at a layer of a multi topology network a service path engine for the layer uses MT TED and the corresponding one of generated path databases for the layer to determine whether there exists a path in the layer that satisfies the TE specifications for the requested path for the duration of the requested time. SPEs may use the Djikstra constrained SPF CSPF and or the Bhandari Edge disjoint shortest pair for determining disjointed main and backup paths path computation algorithms for identifying satisfactory paths though the multi topology network. If a satisfactory computed path for the requested path exists the computing service path engine for the layer re validates the computed path and if validation is successful schedules the computed path by adding the computed path to the one of generated path databases for the layer. In addition the computing SPE adds the requested path start complete times to scheduler . A computed path added to one of generated path databases is referred to as a scheduled path until such time as path manager programs the scheduled path into the multi topology network whereupon the scheduled path becomes an active path. A scheduled or active path is a temporarily dedicated bandwidth channel for the scheduled time in which the path is or is to become operational to transport flows.

As noted above generated path databases store path information for scheduled and active paths. Path information may include an ERO that specifies a list of overlay or base network nodes for a TE LSP routes or tunnels to be configured in one or more overlay network or base network nodes forwarding information for overlay network nodes specifying respective sets of forwarding actions to apply to PDUs inbound to the overlay network nodes and or any other information usable by any of topology node interfaces to establish and steer flows onto scheduled paths in a multi topology network.

SPEs compute scheduled paths based upon a current state or snapshot of the multi topology network as represented by MT TED and generated path databases . Because multiple SPEs execute simultaneously in this example to compute and schedule paths through the multi topology network multiple SPEs may attempt to update generated path databases simultaneously which could in some cases result in network resource oversubscription and failure by PCE to satisfy requested paths. An SPE may therefore having computed a path execute a transaction that conforms to the ACID properties atomicity consistency isolation durability or another type of atomic transaction to both re validate and update generated path databases with a scheduled path. That is the SPE may first lock generated path databases to prevent other SPEs from modifying generated path databases . The SPE may then validate the computed path against the locked generated path databases as well as MT TED . If the computed path is valid the SPE updates generated path databases by adding the computed path as a scheduled path. The SPE then unlocks generated path databases . In this way all affected links are updated in the same transaction and subsequent path validations by other SPEs account for the updates. SPEs may use any suitable data structure locking mechanism such as monitors mutexes or semaphores to lock generated path databases .

If the SPE fails to validate a previously computed path the SPE attempts to re compute the path. Upon identifying a satisfactory path against the current snapshot of the multi topology network the SPE again attempts to validate the computed path and update generated path databases .

In some cases SPEs may be unable to identify a path through an overlay network with which to satisfy a path request. This failure may be due to any of a number of factors. For example sufficient network resources with which to satisfy the path request may be unavailable for the scheduled time due for instance to previously scheduled paths that include one or more links of the base network layer for any possible paths between the endpoints of the path request at an overlapping time. In this example path computation fails. In other words one or more paths between the endpoints of the path request exist but the paths are already sufficiently subscribed to prevent the additional reservation of adequate resources for the requested path. As another example SPEs may be unable to identify any paths through an overlay network between the endpoints of the path request because the computation failed due to a missing link in the overlay network. In other words the computed overlay network graph after removing unusable edges unable to satisfy path request constraints includes two disjoint subgraphs of the overlay network. However in this case a suitable path may be generated by creating a tunnel through the base layer between the subgraphs for the overlay network.

Where path computation fails because sufficient network resources do not exist at the requested time the computing SPE may consider policies set by an operator via operator interface that establish priorities among clients of PCE and or among path request parameters including bandwidth hosts time and QoS parameters as well as flow classifiers. A policy of policies may prioritize the requested path for which path computation failed over and against one or more scheduled paths of generated path databases . In such instances the computing SPE may preempt one or more of these scheduled paths by removing again in accordance with policies the paths from generated path databases and scheduler . In addition the computing SPE in such instances enqueues the removed paths as path requests to path request queue . Components of PCE may then again attempt to compute satisfactory paths for the path requests corresponding to paths removed from generated path databases . Where SPEs are unable to identify a satisfactory path for such a path request SPEs direct path manager to send a path rejection message to a requesting client that issued the path request via client interface . In effect PCE revokes a grant of scheduled multi topology network resources made to the requesting client.

Where path computation fails due to a missing link between disjoint subgraphs of an overlay network each providing reachability to respective endpoints for a requested path the computing SPE requests one of tunnel managers A K collectively tunnel managers to establish a tunnel in a lower layer of the multi topology network. For example one of SPEs for an overlay network may request a tunnel in a lower layer overlay network or in the base network layer. Each of tunnel managers is associated with one of the layers of the multi topology network and with one of generated path databases . In other words each of tunnel managers manages tunnels for one of the topologies.

Tunnel managers operate as intermediaries between generated path databases and SPEs . A higher layer SPE of SPEs may request a lower layer one of tunnel managers to establish a tunnel between two nodes of the lower layer to create a link in the higher layer. Because a tunnel traverses two layers of the multi topology network each of the two nodes may straddle the two layers by having an ingress and egress interface coupling the two layers. That is a first one of the two nodes may be an ingress network switch having an ingress interface to the base network layer while a second one of the two nodes may be an egress network switch having an egress interface from the base network layer. The tunnel manager in response may enqueue a path request specifying the two nodes in the lower layer of the multi topology network to path request queue . If a lower layer SPE is able to schedule a path for the path request this path becomes a link in the lower layer generated path database and the lower layer SPE notifies the requesting one of tunnel managers with link tunnel information for the link. The tunnel manager propagates this tunnel information to MT TED which triggers the higher layer SPE that a new link is available in the higher layer topology and prompts the higher layer SPE to reattempt computing a satisfactory path for the original requested path. Tunnel managers may also validate tunnel setup at their respective layer of a multi topology network.

Scheduler instigates path setup by tracking scheduled start times for scheduled paths in generated path databases and triggering path manager to establish the scheduled paths at their respective start times. Path manager establishes each scheduled path using one or more of topology node interfaces including overlay controller interface device management interface and network switch interface . Different instances of PCE may have different combinations of topology node interfaces .

Path manager may invoke the overlay controller interface to sending overlay network path setup messages e.g. overlay network path setup messages of directing an overlay controller to establish paths in an overlay network and or steer flows from hosts onto established paths in accordance with path information for scheduled paths in generated path databases . In this way BCA may program paths according to a permanent virtual circuit PVC or hop by hop model by programming forwarding state in network and or overlay switches to execute the paths being programmed.

Device management interface may represent a Simple Network Management Protocol SNMP interface a Device Management Interface DMI a CLI or any other network device configuration interface. Path manager may invoke device management interface to configure network switches e.g. routers with static routes TE LSPs or other tunnels in accordance with path information for scheduled paths in generated path databases . Network switch interface establishes communication sessions such as communication sessions of with network switches to receive and install path state information and to receive path setup event information. Network switch interface may be a PCE protocol PCEP interface a DMI or SNMP interface for example.

Path manager may invoke device management interface and or network switch interface to configure and direct network switches to establish paths in a base network layer or overlay network layer of a multi topology network. For example path manager may first configure a TE LSP within a network switch at a network edge then direct the network switch to signal a path for the TE LSP using RSVP with traffic engineering extensions RSVP TE or another signaling protocol. In this way BCA may program paths including TE LSPs into the network according to a soft PVC SPVC model. In this model the network presents a programming interface that BCA invokes to dynamically set up the SPVCs. In some examples BCA may use a combination of PVC and SPVC models to program paths into a multi topology network.

Upon receiving confirmation from topology node interfaces that a scheduled path setup is successful path manager transitions a status of the scheduled path in generated path databases to active. At the scheduled end time if any for an active path scheduler notifies path manager to tear down the active path using topology node interfaces . After tearing down the path path manager removes the path from generated paths .

A base network layer of multi topology network includes routers A D collectively routers connected in the illustrated topology by network links. Base network layer routers and interconnecting network links are illustrated in with a thin line weight in comparison to nodes and interconnecting communication links of the overlay network layer of multi topology network . Each of routers may represent an example of any of EPs AXs AGs or ERs of or network switches A B of . Routers execute routing protocols to exchange routes that specify reachability to network subnets that each includes one or more of hosts A E collectively hosts . Each of hosts may represent an example of any of hosts of . For example router D provides reachability to the 3.0.0.0 8 network subnet which includes host B having network address 3.4.5.6 . As another example router B provides reachability to the 1.0.0.0 8 network subnet which includes hosts A C and D. Routers also exchange topology information by which the routers may determine paths through the base network layer to a router that provides reachability for the network subnets. Network subnets include prefixes that conform to a network addressing scheme of the base network layer. The network addressing scheme in the illustrated example is IPv4. In some examples the network addressing scheme is IPv6 or another network addressing scheme.

Each of routers may be geographically distributed over a wide area. The base network layer of multi topology network may include multiple autonomous systems that transport traffic between hosts to migrate data among distributed applications executing on hosts for example.

Path computation clients PCCs A D collectively PCCs of respective routers provide path status information for paths established through the base network of multi topology network to PCE in respective PCE protocol PCEP sessions . Path status information may include descriptors for existing operational paths as well as indications that an established path or path setup operation has failed. For example PCE may direct router A to establish an LSP over a computed path. Router A may attempt to signal the LSP using a reservation protocol such as RSVP TE but fail due to insufficient network resources along a path specified by an Explicit Route Object ERO . As a result router A may provide an indication that the path setup operation failed to PCE in a PCEP session .

PCE may be a stateful PCE that maintains synchronization not only between PCE and multi topology network base network layer topology and resource information as provided by PCCs but also between PCE and the set of computed paths and reserved resources in use in the network as provided by PCCs in the form of LSP state information. PCCs may send path setup failure and path failure event messages using LSP state report messages in extended PCEP sessions to provide LSP state information for LSPs configured in any of routers . Extensions to PCEP that include LSP state report messages are described more fully in J. Medved et al. U.S. patent application Ser. No. 13 324 861 PATH COMPUTATION ELEMENT COMMUNICATION PROTOCOL PCEP EXTENSIONS FOR STATEFUL LABEL SWITCHED PATH MANAGEMENT filed Dec. 13 2011 which is incorporated herein by reference in its entirety.

PCE receives path status information and adds established paths through the base network layer of multi topology network as links in an overlay network topology stored by PCE . The overlay network topology may be stored in an example of MT TED of . Tunnel in this example may be an instance of an established path computed by PCE and signaled by router A to reach router B. Tunnel may be a bi directional tunnel. Tunnel may thereafter be used to exchange L2 traffic between OpenFlow switch A and B. As a result tunnel is a link in the overlay topology network and is represented as such in the overlay network topology stored by PCE .

Extended PCEP sessions also allow PCE to actively update LSP parameters in PCCs that have delegated control to PCE over one or more LSPs headed by corresponding routers . The delegation and control techniques may for example allow PCE to trigger LSP re route by an LSP head end router such as any of routers in order to improve LSP placement. In addition LSP state injection using extended PCEP sessions may further enable to PCE to modify parameters of TE LSPs including bandwidth and state to synchronously coordinate demand placement thereby permitting ordered control of path reservations across network routers.

PCE may also configure new LSPs by configuring any of routers to include new LSP interfaces. For example PCE may use an example of client interface of to configure router A to include an LSP represented by tunnel . PCE may then use a PCEP session with PCC A to direct router A to signal the LSP toward router B. In this way PCE may program tunnels for the overlay network layer of multi topology network between any of routers .

The service provider or other administrator for network deploys Application Layer Traffic Optimization ALTO server to provide an application layer traffic optimization service over network . The application layer traffic optimization service may in some instances conform to the ALTO protocol. In general the ALTO service enables service and or content providers to influence the node selection process by applications to further service provider objectives which may include improving path computation by reducing transmission costs along network layer topology links to the provider load balancing service level discrimination accounting for bandwidth constraints decreasing round trip delay between hosts or between routers and other objectives. The ALTO service and ALTO protocol is described in further detail in J. Seedorf et al. RFC 5693 Application Layer Traffic Optimization ALTO Problem Statement Network Working Group the Internet Engineering Task Force draft October 2009 and R. Alimi et al. ALTO Protocol draft ietf alto protocol 06.txt ALTO Working Group the Internet Engineering Task Force draft October 2010 each of which is incorporated herein by reference in its entirety. Furthermore while generally described with respect to the ALTO service and ALTO servers as described in Seedorf et al. the techniques of this disclosure are applicable to any form of application layer traffic optimization.

ALTO server establishes respective peering sessions with routers A B and D that are edge routers of the base network layer of multi topology network . Each of peering sessions may comprise an Interior Border Gateway Protocol IBGP session or an exterior Border Gateway Protocol BGP session for instance. In this way ALTO Server receives in peering sessions topology information for the base network layer originated or forwarded by routing protocol speakers of multi topology network . The received topology information describes the topology of the routers base network layer of network and reachability of network address prefixes by any of routers . Peering sessions may comprise Transmission Control Protocol TCP sessions between ALTO server and routers A B and D. In some instances ALTO server may establish a single peering session with a route reflector not shown that reflects topology information to ALTO server that is received by the route reflector from routers .

Peering sessions may also or alternatively include interior gateway protocol IGP sessions between ALTO server and routers . ALTO server may operate as a passive IGP listener by peering with routers in peering sessions . That is ALTO server receives routing information from routers in peering sessions but does not originate or forward routing information for ALTO server does not route packets in its capacity as an ALTO server . Peering sessions may represent for example an OSPF or IS IS neighbor relationship or adjacency or may simply represent movement of current routing information from routers to ALTO server . In some instances peering sessions include traffic engineering extensions e.g. OSPF TE or IS IS TE and routers provide traffic engineering information to ALTO server .

ALTO server generates one or more network maps and cost maps for multi topology network using topology information received in peering sessions and provides these maps to ALTO clients such as PCE . A network map contains network location identifiers or PIDs that each represents one or more network devices in a network. In general a PID may represent a single device or device component a collection of devices such as a network subnet or some other grouping. A cost map contains cost entries for pairs of PIDs represented in the network map and an associated value that represents a cost to traverse a network path between the members of the PID pair. The value can be ordinal i.e. ranked or numerical e.g. actual . ALTO server provides the network maps and cost maps to PCE which uses the network maps and cost maps to compute paths through multi topology network .

In this example ALTO server generates at least two views of multi topology network in the form of network maps and corresponding cost maps in accordance with techniques of this disclosure a first view that constitutes an endpoint database for a base network layer e.g. an example of endpoint databases of and a second view for the base network layer that describes an L3 traffic engineering database at link level granularity where link level refers to the level of individual interfaces of routers . The second view in other words provides traffic engineering information for links connecting pairs of interfaces on respective routers . provides an example of the first view generated by ALTO server while provides an example of the second view.

Further details regarding generating network and cost maps for a network are found in Penno et al. U.S. patent application Ser. No. 12 861 645 entitled APPLICATION LAYER TRAFFIC OPTIMIZATION SERVICE SPANNING MULTIPLE NETWORKS filed Aug. 23 2010 the entire contents of which are incorporated herein by reference. Additional details regarding ALTO map updates are found in Raghunath et al. U.S. patent application Ser. No. 12 861 681 entitled APPLICATION LAYER TRAFFIC OPTIMIZATION SERVICE MAP UPDATES filed Aug. 23 2010 the entire contents of which are incorporated herein by reference.

ALTO server may comprise for example a high end server or other service device or a service card or programmable interface card PIC insertable into a network device such as a router or switch. ALTO server may operate as an element of a service plane of a router to provide ALTO services in accordance with the techniques of this disclosure. In some instances ALTO server is incorporated into PCE . ALTO server may represent an example embodiment of topology server of . Additional details regarding providing ALTO services as an element of a service plane of a router are found in Raghunath et al. incorporated above.

Multi topology network also includes overlay network layer of interconnected OpenFlow OF switches A F collectively OpenFlow switches controlled by OpenFlow controller . While the overlay network layer is an L2 network in this example the overlay network layer may be an L3 network in some instances. Each of OpenFlow switches performs packet lookups and forwarding according to one or more flow tables each having one or more flow entries. Each flow entry specifies one or more match fields and a set of instructions to apply to packets the match values of the match fields. A match field may match any of the PDU parameters described above with respect to e.g. source and destination MAC and IP addresses . The set of instructions associated with each flow entry describe PDU forwarding and PDU modifications for PDU flows. For example a set of instructions may direct one of OpenFlow switches to decrement a time to live TTL value for PDUs in matching flows and then output the PDUs to a particular outbound interface of the OpenFlow switch. Additional details regarding OpenFlow are found in OpenFlow Switch Specification version 1.1.0 OpenFlow Consortium February 2011 which is incorporated by reference herein. While not illustrated as such to simply the figure PCE may couple to ALTO server and OpenFlow controller to exchange data and control messages using communication links.

OpenFlow switches D F represent dedicated OpenFlow switches that may each be a standalone device in the form of a router L3 L2 or L2 L3 switch or another network device that switches traffic according to forwarding information. As dedicated OpenFlow switches OpenFlow switches D F do not in this example share a chassis or other hardware resources with a base network layer device e.g. any of routers . Routers A C implement corresponding OpenFlow switches A C to direct traffic on respective subsets of physical or virtual interfaces of the routers. For example router A may implement OpenFlow switch A to control a VPLS instance that switches L2 traffic among a set of interfaces that includes interfaces to OpenFlow switches B i.e. a virtual interface for tunnel E and F. In this way OpenFlow switches A C share hardware resources with corresponding routers A C.

The overlay network layer includes tunnel connecting OpenFlow switches A B. Tunnel is a service link that transports L2 communications between routers A B. Tunnel is illustrated in as a dashed line to reflect that tunnel may not directly couple routers A B to one another but may be transported over one or more physical links and intermediate network devices that form tunnel . Tunnel may be implemented as a pseudowire operating over a TE LSP or GRE tunnel for example. Pseudowire service emulation is described in additional detail in Pseudo Wire Emulation Edge to Edge PWE3 Architecture Request for Comments 3985 Network Working Group Bryant and Pate ed. March 2005 which is incorporated by reference as if fully set forth herein.

Router B includes an integrated routing and bridging IRB interface that is a gateway between the overlay network layer and the base network layer of multi topology network . IRB interface connects the bridge domain that is the L2 overlay network layer of multi topology network to a routed domain that is the base network layer. IRB interface thus includes both a bridging instance that includes L2 learning tables as well as a routing instance mapped to the bridging instance. The bridging instance may include OpenFlow switch B operating over a VPLS or other L2 instance. IRB interface therefore acts as a L3 routing interface for a bridge domain in which OpenFlow switch B participates. In this way IRB interface provides simultaneous support for L2 bridging and L3 routing and can function as a gateway between the layers of multi topology network .

The bridge domain in this example includes subnet 1.0.0.0 8 for which router B advertises itself to other routers as providing reachability to. Elements of the overlay network e.g. hosts A C and D may identify routable L3 traffic by addressing the L3 traffic to a gateway L2 address e.g. a gateway MAC address known to IRB interface . The gateway L2 address may be a MAC address of router B a MAC address of an interface of router B that couples to an overlay network link or any other L2 address that IRB interface may use to classify PDUs arriving on an L2 interface of router B as L3 traffic.

OpenFlow controller establishes OpenFlow protocol sessions with each of OpenFlow switches to configure the flow tables therein and to receive copies of PDUs sent to OpenFlow controller by OpenFlow switches . OpenFlow switches also send OpenFlow controller identifiers for the respective physical and virtual if any ports on which PDUs are received. A port on which a PDU is received is also referred to as an in port. OpenFlow controller analyzes the received PDUs and associated in ports to determine an overlay network layer topology for multi topology network . In this example in other words OpenFlow controller performs L2 topology discovery. For example OpenFlow controller may receive a message in an OpenFlow protocol session from OpenFlow switch F that includes a copy of a PDU received by OpenFlow switch F at port P. The PDU specifies a destination MAC address D. OpenFlow controller may have previously configured OpenFlow switch D to output PDUs having destination MAC address D to port P of OpenFlow switch D. OpenFlow controller may use this information to determine that a L2 link is present in the overlay network layer between OpenFlow switch D and F. OpenFlow controller provides the discovered L2 topology to PCE which stores the L2 topology to a multi topology database which may be an example of MT TED of . OpenFlow controller may represent an example of overlay controller of . In some examples OpenFlow controller is incorporated within PCE .

PCE presents an interface by which clients may request for a specified time a dedicated path between any combination of hosts . PCE uses base network topology information for multi topology network received from ALTO server overlay network topology information for multi topology network received from OpenFlow controller and path status information received from PCCs to compute and schedule paths between hosts through multi topology network that satisfy the parameters for the paths requested by the clients. PCE may receive multiple path requests from clients that overlap in time. PCE reconciles these requests by scheduling corresponding paths for the path requests that traverse different parts of multi topology network and increase capacity utilization for example or by denying some of the path requests.

At the scheduled time for a scheduled path PCE installs forwarding information to multi topology network nodes e.g. OpenFlow switches and routers to cause the nodes to forward traffic in a manner that satisfies the requested path parameters. A requested path may traverse either or both domains of multi topology network . That is a requested path may traverse either or both of the base network layer and overlay network layer of multi topology network . PCE installs forwarding information to OpenFlow switches using OpenFlow controller . OpenFlow controller presents a programming interface by which PCE may configure flow tables of OpenFlow switches using OpenFlow protocol sessions . PCE invokes the programming interface of OpenFlow controller by sending overlay network path setup messages not shown in directing OpenFlow controller to establish paths in the overlay network layer of multi topology network and or steer flows from hosts onto established paths. OpenFlow controller responds to overlay network path setup messages by installing forwarding information to OpenFlow switches that implements the paths and or directs flows received from hosts onto established paths.

PCE installs forwarding information to routers using PCEP sessions with PCCs and in some instances using network management interfaces to router routers . PCE may invoke the network management interfaces of routers to configure a tunnel e.g. an LSP install static routes configure a VPLS instance configure IRB interface and to otherwise configure routers to forward packet flows in a specified manner. PCE also communicates with PCCs to direct routers to signal LSPs through the base network layer of multi topology network to establish paths that may be used by the overlay network to transport L2 traffic along scheduled paths.

In this way the described techniques use network application programming interfaces APIs i.e. PCEP and OpenFlow to obtain topology information for multiple layers of multi topology network and also to program ephemeral forwarding information into the multiple layers. Obtaining topology information for multiple layers allows PCE to have access to a full multi topology and utilization of the network for path computation. As a result the techniques may improve network utilization by steering traffic to underutilized portions of multi topology network . In addition the techniques may avoid programming forwarding information into nodes of multi topology network using configuration methods which may require commits involving significant overhead.

In some examples a method includes storing by one or more databases of a controller that manages a network of one or more network devices network topology information and network state information for the network devices. The method also includes exchanging with at least one of the network devices and by one or more network device protocol interfaces of the controller state information comprising at least one of network topology information and network device state information. The method also comprises receiving by one or more application interfaces of the controller and from applications requests for application specific network configurations. The method also includes computing by one or more core modules of the controller respective network configurations to conform the network topology and network device states to satisfy the requests. The method also includes generating by one or more core applications of the controller network device state information to implement computed network configurations and to use the network device protocol interfaces to program the network device state information to the network devices to program the network configuration in the network.

In these and other examples the application interfaces may include at least one of a video on demand service interface a connectivity on demand service interface a firewall in cloud service interface and a data replication service interface. In these and other examples the core modules may include at least one of a path computation engine a path computation element protocol PCEP server a software defined networking controller and an OpenFlow controller.

In these and other examples the network device protocol interfaces may include at least one of a path computation element protocol PCEP interface a software defined networking protocol interface an OpenFlow protocol interface an application layer traffic optimization protocol interface a Border Gateway Protocol interface a firewall rule installation protocol interface a charging rule installation protocol interface and a policy rule installation protocol interface.

In these and other examples the one or more databases comprise at least one of a traffic engineering database a multi topology traffic engineering database an application layer traffic optimization ALTO map and a routing information base. In these and other examples the network device protocol interfaces application interfaces core modules and core applications provide may open interfaces to enable a modular plugin architecture for adding or replacing any of the network device protocol interfaces application interfaces core modules and core applications. In these and other examples any of the network device protocol interfaces application interfaces core modules and core applications added or replaced may serve a special purpose application or network orchestration function.

The techniques described herein may be implemented in hardware software firmware or any combination thereof. Various features described as modules units or components may be implemented together in an integrated logic device or separately as discrete but interoperable logic devices or other hardware devices. In some cases various features of electronic circuitry may be implemented as one or more integrated circuit devices such as an integrated circuit chip or chipset.

If implemented in hardware this disclosure may be directed to an apparatus such a processor or an integrated circuit device such as an integrated circuit chip or chipset. Alternatively or additionally if implemented in software or firmware the techniques may be realized at least in part by a computer readable data storage medium comprising instructions that when executed cause a processor to perform one or more of the methods described above. For example the computer readable data storage medium may store such instructions for execution by a processor.

A computer readable medium or computer readable storage device may form part of a computer program product which may include packaging materials. A computer readable medium may comprise a computer data storage medium such as random access memory RAM read only memory ROM non volatile random access memory NVRAM electrically erasable programmable read only memory EEPROM Flash memory magnetic or optical data storage media and the like. In some examples an article of manufacture may comprise one or more computer readable storage media.

In some examples the computer readable storage media may comprise non transitory media. The term non transitory may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM or cache .

The code or instructions may be software and or firmware executed by processing circuitry including one or more processors such as one or more digital signal processors DSPs general purpose microprocessors application specific integrated circuits ASICs field programmable gate arrays FPGAs or other equivalent integrated or discrete logic circuitry. Accordingly the term processor as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition in some aspects functionality described in this disclosure may be provided within software modules or hardware modules.

Various embodiments have been described. These and other embodiments are within the scope of the following examples.

