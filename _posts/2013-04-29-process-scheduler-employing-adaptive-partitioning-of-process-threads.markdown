---

title: Process scheduler employing adaptive partitioning of process threads
abstract: A system includes a processor and memory storage units storing software code. The software code comprises code for a scheduling system and for generating a plurality of adaptive partitions that are each associated with one or more process threads and that each have a corresponding processor budget. The code also is executable to, when the system is under a normal load, allocate the processor to one of the threads that is in a ready state and has the highest priority among the process threads that are in a ready state. The code is also executable to, when the system is in overload, allocate the processor to one of the process threads that is in a ready state and has the highest priority among the process threads that are in a ready state and for which the adaptive partition that the process thread is associated with has available guaranteed processor budget.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09424093&OS=09424093&RS=09424093
owner: 2236008 Ontario Inc.
number: 09424093
owner_city: Waterloo, Ontario
owner_country: unknown
publication_date: 20130429
---
This application is a continuation of U.S. patent application Ser. No. 11 317 468 filed Dec. 22 2005 and issued as U.S. Pat. No. 8 434 086 which claims the benefit of priority from U.S. Provisional Application No. 60 662 070 filed Mar. 14 2005 each of which are incorporated herein by reference.

The present invention is directed to a manner in which an operating system schedules the running of threads and the like. More particularly the invention is directed to an operating system having adaptive partition scheduling for process threads.

The kernel of an operating system should be capable of dividing up CPU resources so that each thread that is active in the system obtains an adequate amount of CPU time to properly execute the corresponding process. To this end the kernel may implement a scheduling system that determines how the available CPU time is allocated between multiple threads.

There are at least three types of process scheduling systems a FIFO scheduling system a round robin scheduling system and a sporadic scheduling system. In each system a priority value is assigned to each thread of a process that is executed by the CPU. High priority values are assigned to threads that may be important to the operation of the overall system while threads that may be less important to the operation of the system have lower priority values. Whether the scheduling system gives a thread access to the CPU also depends on the state of the thread. For example a thread may be ready or blocked although other states also may be used . A thread is ready when it is capable of being executed in that all conditions needed for it to run have been met. In contrast a thread is blocked when it tries to initiate an operation that cannot be completed immediately and must wait for the completion of some event before execution of the thread may continue.

In a FIFO scheduling system the currently executing thread continues to use all of the CPU time until it gives up the CPU by blocking it finishes execution or it is preempted by a higher priority thread. Once one of these criterion are met the FIFO scheduling system allocates the CPU to the highest priority process thread that is in a ready state. Generally there is one ready queue for each priority level.

A round robin scheduling system uses an additional parameter a timeslice to allocate CPU time to a thread. A timeslice is an amount of time during which a thread is allowed to access the CPU. In a round robin scheduling system the currently executing thread continues to use all of the CPU time until the occurrence of one of the following events 1 the currently executing process blocks 2 the currently executing process finishes 3 the currently executing process is preempted by a higher priority thread or 4 the currently executing process uses up its timeslice. Once the currently executing process blocks or uses up its timeslice it is put at the back of the ready queue for its priority level.

Sporadic scheduling is somewhat similar to round robin scheduling. In a sporadic scheduling system the currently executing process continues to use all of the CPU time until the occurrence of one of the following events 1 the currently executing process blocks 2 the currently executing process finishes 3 the currently executing process is preempted by a higher priority thread or 4 the currently executing process uses up a capped limit on the execution time assigned to the thread within a given period of time. The capped limit is known as a budget while the given period of time in which this budget may be used is known as the replenishment period. In operation the budget for a thread is replenished upon expiration of the replenishment period. Once the currently executing process blocks it is put at the back of the ready queue for its priority level. However if the currently executing process uses up its budget within the replenishment period its priority level is reduced by a predetermined value and it is placed at the back of the ready queue for this lower priority level. The priority level of the process thread may be returned to its original value in response to a number of different conditions.

In certain operating systems such as those available from QNX Software Systems in Kanata Ontario each thread in the system may run using any of the foregoing scheduling systems. Consequently the scheduling systems are effective on a per thread basis for all threads and processes on a node. Each thread is assigned to a particular scheduling system type through the operation of the process thread itself. This provides the software designer with a significant degree of design flexibility but also involves a need for coordination between software designers implementing code for the same system. This coordination includes the assignment of priorities to the different threads as well as the scheduling system type assigned to each thread.

Some available operating systems apply scheduling on a global basis. One such global scheduling system is known as fair share scheduling. In a fair share scheduling system CPU usage may be equally distributed among system users groups or processes. For example if four users A B C D are concurrently executing one process each the fair share scheduler will logically divide the available CPU cycles such that each user gets 25 of the whole 100 4 25 . If user B starts a second process each user will still receive 25 of the total cycles but both of user B s processes will each receive 12.5 of the total available CPU time. On the other hand if a new user starts a process on the system the scheduler will reapportion the available CPU cycles such that each user gets 20 of the whole 100 5 20 .

Another layer of abstraction allows partitioning of users into groups and application of the fair share system to the groups as well. In this case the available CPU cycles are divided first among the groups then among the users within the groups and then among the processes for that user. For example if there are three groups 1 2 3 containing three two and four users respectively the available CPU cycles may be distributed as follows 100 3 groups 33.3 per group Group 1 33.3 3 users 11.1 per user Group 2 33.3 2 users 16.7 per user Group 3 33.3 4 users 8.3 per user. Other percentage distributions among the groups also may be used.

One manner of logically implementing fair share scheduling strategy is to recursively apply a round robin scheduling strategy at each level of abstraction processes users groups etc. . In round robin scheduling threads of equal importance or priority take turns running. They each run for intervals or timeslices that are the property of each thread or group of threads.

While the foregoing scheduling systems have advantages in different applications they may experience deficiencies when used in certain system applications. For example when per thread scheduling systems are used in real time systems where the latencies of a process thread have been planned solely through the assignment of priority levels very long latencies for low priority threads may occur. Malicious software processes may configure themselves for high priority execution and thereby preempt proper scheduling of lower priority threads. This problem also may occur for example during system development when a high priority thread malfunctions and enters an infinite loop. Global fair share scheduling systems may avoid such problems but lack the responsiveness needed for use in a real time system. Therefore a need exists for a scheduling system that may effectively allow high priority threads to operate on a real time basis while concurrently providing some sort of fair share CPU access to all threads.

A system is set forth that comprises a processor such as a single processor or symmetric multiprocessor and one or more memory storage units. The system also includes software code that is stored in the memory storage units. The software code is executable by the processor and comprises code for generating a plurality of adaptive partitions that are each associated with one or more software threads. Each of the adaptive partitions has a corresponding processor budget. The code also is executable to generate at least one sending thread and at least one receiving thread. The receiving thread responds to communications from the sending thread to execute one or more tasks corresponding to the communications. A scheduling system also forms at least part of the code that is executable by the processor. In operation the scheduling system selectively allocates the processor to each sending and receiving thread based at least in part on the processor budget of the adaptive partition associated with the respective thread. In this type of sending receiving environment the scheduling system bills the processor budget of the adaptive partition associated with the sending thread for processor allocation used by the receiving thread to respond to communications sent by the sending thread.

Other systems methods features and advantages of the invention will be or will become apparent to one with skill in the art upon examination of the following figures and detailed description. It is intended that all such additional systems methods features and advantages be included within this description be within the scope of the invention and be protected by the following claims.

Memory may be used to store among other things the software code that defines the functions that are to be executed by the system . Although memory is shown as a single unit it may be implemented as multiple memory units of the same or different memory type. For example memory may be implemented with multiple flash memory devices. Alternatively or in addition memory may be implemented with one or more flash memory devices and one or more hard disc storage devices. It will be recognized that a substantial number of alternative memory device combinations may be used to implement memory .

Some of the software code that may be stored in memory and executed by CPU is identified in . The identified software code shown here includes an operating system and one or more software applications . In this example a process scheduler and device drivers are included as members of the operating system . The process scheduler and device drivers however also may be implemented as software modules that are separate from the operating system . Software applications may be used to define the high level functions that system is to perform. Device drivers may be used to provide a hardware abstraction layer through which software applications communicate with the hardware components of the system . The hardware components may include the components accessed through I O interface .

Process scheduler comprises the executable software code that is used to allocate the processing time of the CPU CPU time to each thread of the system . The CPU time may be allocated so that each thread obtains an adequate amount of CPU time to properly execute the corresponding process.

As shown the process scheduler creates a number of different adaptive partitions at block . Each adaptive partition constitutes a virtual container for scheduling attributes associated with a set of process threads that have been grouped together by the process scheduler . Threads that work toward a common or related function may be assigned to the same adaptive partition. In object oriented systems the adaptive partitions may be generated as instances of an adaptive partition class that includes corresponding scheduling methods and attributes. Each of the adaptive partitions generated at block is uniquely identified for further access by the process scheduler .

The number of partitions generated by the process scheduler may be initiated through the use of configuration parameters. The configuration data may be provided by user initiated commands or through programs that interface with the process scheduler .

At block the size of the averaging window that will be used by the process scheduler is calculated. As will be set forth in further detail below the averaging window is the time over which the process scheduler attempts to keep adaptive partitions at their guaranteed CPU percentages when the system is overloaded. A typical time is 100 milliseconds though other averaging window sizes may be appropriate. The averaging window size may be specified at boot time and may be respecified any time thereafter as well. Considerations in choosing the size of the averaging window may include 

The size of the averaging window windowsize may be assigned in terms of milliseconds which the scheduling process converts to clock ticks. A clock tick is the interval between clock interrupts the system timer . All of the time budgets used by the scheduling process may be averaged over the same windowsize.

At block the scheduling process assigns a guaranteed CPU time budget to each adaptive partition. The guaranteed budget may be assigned as a percentage of the overall available system budget. The sum of all adaptive partitions CPU percentages in such instances will be 100 . For the purpose of assigning shares of the overall CPU time budget the processors in a symmetric multiprocessing system regardless of the number may be considered in the same manner as a single processor.

The guaranteed budget used for each adaptive partition may be determined in a number of different manners. For example the CPU time used by each partition under several different load conditions may be measured and they used to construct a graph of load versus CPU time consumed by each partition. Measurements also may be made under overload conditions. Using this information a system designer may balance the needs of the various threads contained in each partition under the various conditions and assign the appropriate guaranteed CPU time budgets. The measurements also may be used to dynamically vary the guaranteed CPU time budgets with respect to CPU load conditions. For example the process scheduler may operate in accordance with different operating modes in response to different operating conditions. While operating in a particular mode the process scheduler employs a unique set of adaptive partition parameters. The availability and parameters associated with a particular mode may be specified at boot time. A application programming interface API at run time may be used to switch modes as needed. For example a first set of guaranteed CPU time percentages may be employed during startup while a second set of guaranteed CPU time percentages may be employed during normal operations after system startup has been completed.

A priority is assigned to each thread and each thread is associated with a respective adaptive partition at block . Functionally related software may be assigned to the same adaptive partition. This effects a hybrid process scheduling system in which the priority assigned to a thread as well as the guaranteed CPU time percentages of the adaptive partitions are used in the scheduling decisions executed by the process scheduler .

In assigning the threads to respective adaptive partitions adaptive partition scheduling may be used as a structured way of deciding when the running of a particular function of the system will be inhibited. When used in this manner separate threads may be placed into different adaptive partitions based on whether the threads should be starved of CPU time under a particular set of circumstances. For example suppose the system is designed to operate as a packet router. Some of the processes that may be executed by a packet router include 1 routing packets 2 collecting and logging statistics for packet routing 3 route topology protocols with peer routers and 4 collecting logging and route topology metrics. In such instances the threads of these processes may be assigned to two adaptive partitions one for threads associated with routing and one for threads associated with the topology of the network. When the system is overloaded i.e. there is more outstanding work than the machine may possibly accomplish. Consequently there may be a need to determine which applications are to be run slower. To this end it may be preferable to route packets as opposed to collecting routing metrics if the CPU does not have enough resources to execute threads for both routing and metrics. It also may be reasonable to run the threads associated with network topology protocols even when CPU resources are limited. Under these circumstances it may be preferable to employ three adaptive partitions as opposed to the two adaptive partitions initially considered. The three adaptive partitions along with exemplary guaranteed budgets may include 

In this case adaptive partition scheduling has been used to reorganize various system functions so that certain functions are given priority during high CPU loads while still ensuring that all system functions are given CPU execution time. Threads associated with routing and threads associated with logging routing metrics have been associated with separate adaptive partitions despite the fact that they are functionally related to routing operations. Similarly two functionally unrelated components routing metric logging and topology metric logging have been associated with the same adaptive partition. This manner of assigning the various threads to different partitions enables the process scheduler to reduce CPU time usage by the logging threads under high CPU load conditions and give priority to routing threads and topology protocol threads while still conducting metrics logging.

At block a determination may be made as to which threads of the system will be allowed to run in a critical state. Designating a thread as critical gives it the ability to run in a manner that approximates a real time system even when the guaranteed budget for the adaptive partition budget might otherwise be exceeded. When a critical thread associated with a particular adaptive partition is run despite the lack of guaranteed budget for the adaptive partition the adaptive partition is said to have gone into short term debt. Critical threads may be associated with the various partitions for example at boot time. Critical threads are discussed in further detail below.

Each adaptive partition that is associated with a critical thread is assigned a critical time budget at block . The critical time budget may be specified for example in time units such as milliseconds. In the exemplary system the critical time budget is the amount of CPU time available to all critical threads associated with a given adaptive partition above that partition s guaranteed time budget during an averaging window. By employing critical designations and critical time budgets a critical thread generally has an opportunity to run on the CPU even if its associated adaptive partition has exhausted its guaranteed budget. This will occur as long as the partition still has critical time budget available. Critical threads may provide the ability for real time behavior within these partitions.

Various policies that the process scheduler must follow may be optionally assigned to the system at block . For example a bankruptcy policy may be applied to one or more of the adaptive partitions to determine how the system and or process scheduler handles a bankrupt state of the adaptive partition. Bankruptcy of a critical adaptive partition occurs when the adaptive partition has exhausted both its guaranteed CPU time budget and critical time budget over the duration of the averaging window. As a further example system may execute an overload notification policy that allows a process to register with the process scheduler so that it is notified when a system overload occurs. A system overload occurs for example when all of the ready threads cannot be executed on the CPU over the duration of the averaging window. A process may register to be informed of an overload condition when the system enters and or leaves the overload state. Applications may use this overload notification to gracefully degrade their service for example by skipping less important functions or by reducing the precision of computation. Adaptive partitions may go over budget when some other adaptive partition is sleeping as will be set forth below. This is not by itself necessarily considered to be a system overload and therefore does not require triggering of the overload notification policy.

At block the process scheduler is configured with data corresponding to the foregoing states values and or assignments provided at blocks through . As noted above these states values and or assignments may be provided for use in the system in a number of different manners such as by the system designer by the system user through other software programs etc.

Block represents execution of the threads in accordance with the configuration data of block . The attributes of the various adaptive partitions may be dynamic. To this end the parameters set forth in one or more of the foregoing blocks may be changed in response to system requirements system state changes in system functions etc. as represented by the flowline returning to block .

The threads in system also may vary dynamically over time. For example a thread or group of threads associated with an adaptive partition may spawn or generate other threads during operation. The originating thread may be referred to as a parent thread while a spawned thread may be referred to as a child thread. Process scheduler may be configured so that child threads inherit the adaptive partition of their parent thread automatically. Alternatively or in addition an API may be provided that will allow spawning threads into other adaptive partitions. Such an API may be made available only to code with sufficient privilege. For example a system application launcher may have such privileges.

In operation each adaptive partition and thread may assume different operative states. Adaptive partitions for example may operate in an active state or a sleep state. In the active state the scheduling attributes of the adaptive partition may be used to schedule CPU time for the associated threads. A sleep state occurs when there are no ready threads associated with the adaptive partition. In such instances the process scheduler effectively treats the adaptive partition as non existent.

Threads may assume for example a running state a ready state or a blocked state. A thread is in the running state while it is being executed by the CPU. It is in a ready state when a set of conditions have been met that render the thread fully prepared for execution by the CPU at a time determined by the process scheduler . A thread is in the blocked state while the thread waits for the occurrence of one or more events. While in the blocked state the thread is not ready to consume any CPU resources. Once the events awaited by the thread occur the thread may become unblocked and enter the ready or running state.

The adaptive partitions and corresponding threads can be used to describe the operation of the process scheduler during various load conditions. In this example the process scheduler makes the CPU available to execute ready threads and assigned to each adaptive partition and based on the priority of the ready threads. Under normal load conditions the highest priority thread in the system will run immediately when it becomes ready. Whether a thread is ready may be indicated to the process scheduler in a number of different manners including for example through the occurrence of an interrupt event or the like. In the illustrated example the highest priority ready thread is thread which has a priority of 17. Thread will continue to operate in a running state until it is finished blocked or until the budget for adaptive partition is exhausted. Under heavy load if an adaptive partition exceeds its CPU budget then its highest priority thread does not run until the partition once again has time available in its CPU budget. This is a safeguard on the system that divides insufficient CPU time among the partitions and . In this state the processor runs the highest priority thread in an adaptive partition with CPU time remaining in its guaranteed CPU time budget.

When an adaptive partition enters a sleep state the process scheduler allocates the CPU budget of the sleeping partition to other active adaptive partitions even if the other active adaptive partitions have exceeded their budget. For example if adaptive partition enters a sleep state the process scheduler allocates the budget for adaptive partition to adaptive partition since adaptive partition has the highest priority ready thread . If two or more adaptive partitions have threads with the same highest priority the process scheduler divides the free time in proportion to the other adaptive partitions percentages. This allocation assists in preventing long ready queue delay times in the case where two adaptive partitions have the same priority.

In the example of there are three adaptive partitions and with 70 20 and 10 CPU budget guarantees respectively. Further each adaptive partition and includes a ready thread and having a priority of 14. If adaptive partition enters a sleep state through a blocking of thread the process scheduler allocates all of the available CPU time to adaptive partitions and in a 2 1 ratio the ratio corresponding to their original CPU budget allocations. If adaptive partition is in a sleep state for a short time then the process scheduler may ensure that partition later receives CPU time at its guaranteed CPU time budget by reallocating the CPU resources so that adaptive partitions and pay back the CPU time that each utilized at the expense of partition . If adaptive partition is in a sleep state for a long time then some or all of the time used by adaptive partitions and may become free. Whether an adaptive partition is in a sleep state for a short time or a long time can be determined in various manners. For example an adaptive partition can be said to be in a sleep state for a short time when it is in the sleep state for a duration of time that is less than windowsize budget percentage windowsize milliseconds within one averaging window.

If all adaptive partitions are at their CPU budget limits then the process scheduler may specify running of the highest priority thread in the system . If two adaptive partitions have threads with the same highest priority then the adaptive partition that has used the smallest fraction of its budget may be run. This manner of operation may be used to prevent long ready queue delays that would otherwise occur. In the example shown in the window size may be 100 ms adaptive partition is allotted 70 of the CPU budget and has used 40 ms adaptive partition is allotted 20 of the CPU budget and has used 5 ms and adaptive partition is allotted 10 of the CPU budget and has used 7 ms. All partitions have a ready thread at priority 14. In this situation thread of adaptive partition is run because its relative fraction free is 5 ms 20 ms or 0.25 while the relative fraction free for adaptive partition is 40 ms 70 ms or 0.57 and 7 ms 10 ms or 0.70 for adaptive partition .

If an adaptive partition has exhausted the assigned CPU budget and one or more of its ready threads are designated as critical then the process scheduler may allow the adaptive partition to use its critical CPU budget to run the critical threads. The critical time budget is the amount of CPU time that the process scheduler allocates to an adaptive partition to run all critical threads associated with the adaptive partition. This critical time budget constitutes CPU time that the process scheduler allocates to the adaptive partition the partition s normal budget during an averaging window. Consequently a critical thread may run even if the adaptive partition with which it is associated is out of budget as long as the adaptive partition has not exhausted its critical time budget.

In this example adaptive partition has exhausted its guaranteed CPU time budget. Nonetheless critical thread may be allowed to run because of the importance that has been placed on it through the critical designation. Consequently the process scheduler may allocate CPU time to critical thread provided there is still CPU time in the critical time budget of adaptive partition .

When the process scheduler schedules the running of a critical thread such as thread the process scheduler bills the thread s run time against the available guaranteed CPU time budget of the thread s associated adaptive partition. However the process scheduler counts the CPU time used by the critical thread against the critical time budget of the adaptive partition only if the process scheduler would not have otherwise run the critical thread. To this end CPU time used in running a critical thread is not counted against the critical time budget of the associated adaptive partition when 1 the system is not overloaded or 2 the system is overloaded but one or more adaptive partitions may not be exhausting their guaranteed CPU time budgets.

A critical thread may remain in a critical running state until it enters a blocking state. That is it may leave the running or ready state as is the case with any other thread. As noted above this may occur while the thread is waiting for a message interrupt notification etc. The criticality of a thread or billing to its adaptive partition s critical time budget may be inherited along with the adaptive partition during operations which trigger priority inheritance.

The short term debt is bounded by the critical time budget specified for the partition. Over time the partition may be required to repay short term debt. A critical thread that exceeds the critical time budget of the adaptive partition may be considered to drive the associated adaptive partition into bankruptcy. Bankruptcy may be handled as an application error and the designer may specify the system s response. Exemplary choices for a response include 1 forcing the system to reboot 2 notifying an internal or external system watchdog and or 3 terminating and or notifying other designated processes. The system may add an entry to a log or the like if an adaptive partition exhausts its critical time budget. When the budgets for the adaptive partitions are dynamically altered through for example a mode change an API call to modify CPU percentages or the like the process scheduler may impose a delay before testing an adaptive partition for bankruptcy. This allows the budgets to stabilize before an adaptive partition may be declared bankrupt.

The designation of a thread as critical may occur in a number of different manners. For example the system may automatically mark threads that are initiated by an I O interrupt as critical. The system also may specify a set of additional applications or operating system notification events for example timers which may mark their associated handler threads as critical. An API also may be used to mark selected threads as critical. Still further child threads of critical parent threads may be automatically designated as critical.

The process scheduler also may be adapted for use in client server systems in which messages are passed from one or more sending client threads for receipt and or processing by one or more receiving server threads. shows a number of interrelated processes that may be used to implement process scheduler in such a client server environment. At block a client thread in a first adaptive partition sends a message to a server thread in a second adaptive partition. When a client thread sends a message to a server thread the server thread that receives the message may inherit the priority of the sending thread. This prevents priority inversion since the server thread is doing work on behalf of the sending client. Inheriting of the priority level of the sending thread by the server thread is shown at block . The process scheduler also may assign the same critical status of the client thread to the server thread at block . At block the process scheduler may associate the server thread with the same adaptive partition that is associated with the client thread.

As shown by line of client thread has passed a message that is received or otherwise processed by thread of adaptive partition . Similarly client thread of adaptive partition has passed a message that is received or otherwise processed by thread of adaptive partition as indicated by line . When the message transfers and occur the process scheduler associates each server receiving with the adaptive partition of the thread that sent the message. In this case server thread is temporarily associated with adaptive partition as indicated by line and is assigned the same priority 14 as thread . Likewise server thread is temporarily associated with adaptive partition as indicated by line and is assigned to the same priority 12 as thread . Both threads and may be placed in a ready state while threads and may be placed in a blocked state as the respective messages are processed. Once thread has completed processing the message received from thread thread may return to its original state where it is associated with adaptive partition with a default priority of 7. Also thread may return to its original state where it is associated with adaptive partition with a default priority of 7 once it has completed processing of the message received from thread .

Sometimes a client thread may attempt to communicate with a server thread that is busy processing one or more messages that have been previously received from other client threads. For example if thread of adaptive partition becomes unblocked and attempts to send a message to server thread while server thread is busy processing one or more prior messages received from thread server thread typically will be unable to respond to thread until it has completed processing the prior messages from client thread . In such instances the process scheduler temporarily may raise the priorities of one or more server threads that for example wait on the same connection path as the client thread. The query and action corresponding to these operations are illustrated at blocks and of . As applied to the system of the connection path between threads assigned to adaptive partition and threads assigned to adaptive partition for example may be assigned the same identification name number. As a result the priority of server thread may be raised in an attempt to reduce the latency that may otherwise occur before server thread is available to process the message from client thread . Depending on the nature of the threads in adaptive partition the priority levels of multiple server threads may be raised.

At block of the process scheduler operates to bill the execution time for each thread and in the appropriate manner. The appropriate manner of billing the execution time may vary. One manner includes applying the execution time of the receiving thread against the CPU budget and or critical CPU budget of the adaptive partition associated with the sending thread. In the example of the execution time used by thread in responding to a message sent by thread is counted against the CPU budget and or critical CPU budget of adaptive partition . Similarly the execution time used by thread in responding to a message sent by thread is counted against the CPU budget and or critical CPU budget of adaptive partition .

System components such as filesystems device drivers and the like may be assigned a guaranteed budget of zero. In such instances the CPU time used by the threads associated with the system component is billed to their clients. However sometimes the process scheduler finds out too late which threads a particular system component thread has been ultimately working for. As a result the process scheduler may not be able to bill the running of the threads of the system components in a timely manner and or to the proper adaptive partition. Additionally some system components such as device drivers may have background threads e.g. for audits or maintenance that require budgets that cannot be attributed to a particular client. In those cases the system designer may measure the background operations and unattributable loads associated with the various system components. The resulting measurements may be used to provide non zero budgets to the adaptive partitions associated with the threads of the various system components.

In operation the process scheduler may do more than simply direct the running of the highest priority ready thread associated with an adaptive partition having guaranteed CPU time left in its budget. For example when all adaptive partitions have exhausted their guaranteed CPU time budgets at approximately the same time then the process scheduler may direct the running of the highest priority thread in the system irrespective of the attributes of the associated adaptive partition. Also when adaptive partitions have threads of the equal highest priorities the process scheduler may assign CPU time using the ratio of their guaranteed CPU time percentages. Finally critical threads may be run even if their adaptive partition is out of budget provided the adaptive partition still possesses an amount of its critical time budget.

Process scheduler may employ one or more ordering functions f ap associated with each adaptive partition ap in its scheduling determinations. Ordering functions may be calculated in a variety of different manners. The results obtained by calculating the ordering functions may be compared with one another to determine how the process scheduler will scheduled the threads associated with the various adaptive partitions of the system.

In calculating the ordering functions f ap a number of different variables may be used. A view of the variables are described here in connection with one example. In the following example let is critcal ap be a boolean variable. The value of is critcal ap depends on 1 whether the adaptive partition ap has available critical budget and 2 whether the highest priority ready thread in the adaptive partition ap has been assigned a critical state. Let has budget ap be a boolean variable that indicates whether an adaptive partition ap has consumed less CPU time than its guaranteed CPU time budget during the last averaging window. Let highest prio ap be an integer variable that indicates the highest priority of all ready to run threads in an adaptive partition ap. Let relative fraction used ap be a real number variable that corresponds to the ratio of the number of microseconds of CPU time consumed by the adaptive partition ap during the last averaging window divided by the budget of the adaptive partition ap when expressed for example in microseconds. Finally let b ap be a boolean variable corresponding to the current rate of CPU time consumption of threads in the adaptive partition ap. More particularly b ap may be assigned a logical true value if given the current rate of CPU time consumption by the threads associated with the partition ap the value of has budget ap also would be a logical true value at the time the process scheduler would likely be called upon to again schedule a thread associated with the adaptive partition. Otherwise b ap may be assigned a logical false value. It will be recognized that other variables or less than all of the foregoing variables may be used to calculate and ordering function f ap . Which variables are used may be dependent on the system designer and or end user.

The value of b ap may be calculated in a number of different manners. For example let the variable t indicate the current time in a high resolution counter and a tick be the length of time between regular events during which the process scheduler examines all partitions. The period of the tick should be less than the size of the averaging window windowsize . Let the function cpu time used t t correspond to a real value of the CPU time used by the threads of partition ap between absolute times t and t. Further let budget ap correspond to the time value of the guaranteed budget for the partition ap. The value for b ap then may be calculated using the following equation ap Boolean cpu time used now now windowsize cpu time used now windowsize tick now windowsize 

One manner of calculating an ordering function f ap using the foregoing variables is shown in . The illustrated operations may be executed for each adaptive partition ap used in the system . As shown the process scheduler determines whether the partition has any threads that are ready to run at block . If there are no ready threads associated with the adaptive partition ap the ordering function f ap for the adaptive partition may be assigned the values f 0 0 0 0 at block and calculation of the ordering function for the next adaptive partition may be initiated at block . The process scheduler determines if the adaptive partition ap has critical CPU budget available at block and if so whether the highest priority ready thread in the adaptive partition ap has been assigned a critical state. Based on this determination the process scheduler assigns the appropriate logical state to the is critical ap variable at block . At block the process scheduler evaluates the CPU budget used by the adaptive partition during the last averaging window. At block the process scheduler determines if the adaptive partition has used less CPU time than its guaranteed CPU time budget. Based on this determination the process scheduler assigns the appropriate logical state to the has budget ap variable at block . The relative ratio of CPU budget time used during the last averaging window is calculated at block by taking the value obtained at block and dividing it by the guaranteed CPU budget time for that adaptive partition. This value is assigned to the relative fraction used ap variable at block .

At block the process scheduler calculates one or more values corresponding to the current rate of CPU time consumption by threads associated with the adaptive partition. These values are used to assign the appropriate logical state to the b ap variable at block .

Using all or a subset of the foregoing variables the ordering function f ap for the given adaptive partition ap is calculated at block . In this example the ordering function f ap is calculated using the ordered values x a y z where x is critical ap OR has budget ap a Not x AND b ap y highest prio ap and z 1 relative fraction used ap . In calculating the ordering function f ap the value of x is given more significance than the values of a y or z the value of a is given more significance than the values of y or z and the value of y is given more significance than the value of z.

The process scheduler runs the highest priority thread of the adaptive partition having the largest f ap as shown at block of . However the process scheduler must determine whether the running time for the thread is to be billed to the critical budget of the adaptive partition or solely to the guaranteed budget. To this end the process scheduler may compute another function fcritical ap using one or more of the foregoing variables. In this example fcritical ap is calculated using the ordered values w d y z where w has budget ap d NOT w AND b ap y highest prio ap and z 1 relative fraction used ap . In the calculation of fcritical ap the value of w is given more significance than the values of d y or z the value of d is given more significance than the values of y or z and the value of y is given more significance than the value of z. This calculation is shown at block . A comparison between the fcritical ap and f ap values for the adaptive partitions is executed at block . If the value of fcritical ap is less than the value of f ap then the running time for the thread is billed to the critical budget of the adaptive partition at block as well as to the guaranteed budget at block . If the value of fcritical ap is greater than or equal to the value of f ap then the running time for the thread is solely billed to the guaranteed budget of the adaptive partition at block . Any calculations used by the process scheduler should ensure that the critical budget for the adaptive partition is only used if the highest priority critical thread associated with that adaptive partition would not have been selected to run by process scheduler had the thread or the partition not been critical.

The function c ap may be precalculated during for example system startup and whenever the guaranteed CPU budgets are reallocated between the various adaptive partitions of the system. At block the CPU budget percentage for each adaptive partition are determined for example at start up. At block the system may compute for each adaptive partition a factor q ap . The value of q ap may be calculated for example as the product of the percentage CPU budgets of all the other adaptive partitions. At block a scaling factor is calculated. In this example if the maximum averaging error is max error e.g. 0.005 for a percent then k min list of q ap max error. A constant scaling factor c ap is calculated at step . In this example c ap is calculated as c ap q ap k. The value total CPU time consumed c ap has the same ordering properties as total CPU time consumed windowsize percentage within an error tolerance of max error.

To practically compare the relative fraction used by different adaptive partitions the process scheduler may need to multiply the run time of the threads associated with the adaptive partitions by c ap . However the billed times may be large numbers. If the process scheduler is to be implemented using single multiply instructions in these calculations the billed times may be first scaled choosing a number of most significant bits of the CPU budget time at block . The degree of scaling may be set by the value of max error. However any reasonable choice for max error e.g. to can be satisfied by choosing only the most significant 16 bits of the billed run time. In such instances the system may be calculating total CPU time consumed 32 c ap . At block the relative budget ratio is calculated as c ap adaptive partition execution time where adaptive partition execution time constitutes a selected number of the most significant bits of total CPU time consumed .

An error tolerance of 0.5 to 0.25 is considered sufficient for an implementation. However the application may include the notion that for any specified error tolerance a minimal number of bits is chosen to both represent c ap the scaled value of the CPU time executed by adaptive partition ap during the last averaging windowsize time and the product of c ap and the scaled CPU time. The minimal number of bits is chosen for both representations and executing multiplication functions so that all representations and arithmetic errors are less than or equal to a chosen error tolerance.

Billing of CPU time to each of the adaptive partition in a system may take place in a number of different manners and may occur many times during the operation of the process scheduler . For example billing of an adaptive partition may occur whenever 1 a thread starts running from a blocked state 2 a thread stops running i.e. when it has been preempted by a higher priority thread when it has been blocked or the like and or 3 at other times when an accurate accounting of the CPU time is needed by the process scheduler .

Typically process schedulers use standard timer interrupts or ticks to determine how long a thread has used the CPU. Tick periods are often on the order of one to several milliseconds.

The process scheduler however may include code that effectively microbills the execution time of the various threads of the system. To this end a high resolution hardware and or software counter having a period substantially less than the tick periods may be employed. Each time a thread starts or stops running the process scheduler assigns a timestamp to the associated partition corresponding to the value of the high resolution counter. The timestamp values may be scaled to a useful common time unit. The differences between the timestamps for adjacent start and stop times of a thread are used to microbill the appropriate adaptive partition.

The high resolution counter may be implemented in a number of different manners. For example some CPUs have a built in counter that increments at about the clock frequency at which the CPU is run. In such situations the built in counter may be used in the microbilling process. In another example a high resolution counter may be simulated using software by querying an intermediate state of a programmable count down timer that for example may normally be used to trigger clock interrupts. This may be the same counter used to provide an indication that a tick interval has occurred. In such situations the timestamps should take into consideration both the counter value and the number of ticks that have occurred from a given reference point in time so that the timestamps accurately reflect the start times and stop times of the individual threads.

The foregoing process scheduler also may be used in systems that employ mutexes. Mutexes are used to prevent data inconsistencies due to race conditions. A race condition often occurs when two or more threads need to perform operations on the same memory area but the results of computations depend on the order in which these operations are performed. Mutexes may be used for serializing shared resources. Anytime a global resource is accessed by more than one thread the resource may have a mutex associated with it. One may apply a mutex to protect a segment of memory critical region from other threads. The application gives a mutex to threads in the order that they are requested. However the process scheduler may be adapted to deal with the problems that occur when a low priority thread which may hold the mutex unreasonably delays access to higher priority threads that are waiting for the same mutex.

The thread most likely to run next may be computed by applying pairwise a compare two threads process repeatedly on pairs of threads in a list of waiting threads. The compare two threads process may be executed as follows where A and B are the two threads to be compared A function f ap is constructed which includes the ordered values x a y z . This is the same ordering function f ap constructed above. Then let partition of X mean the partition containing the thread X. Then if f partition of A f partition of B thread A is more likely to run than thread B. The function f X is constructed for each thread to be compared until the thread with the highest f X is determined. The thread with the highest f X may be determined to be the thread most likely to run next and its associated adaptive partition may be billed accordingly for the running time of the thread holding the mutex once the adaptive partition associated with the thread holding the mutex has exhausted its guaranteed CPU budget.

The systems and methods described above may be configured to run in a transaction processing system where it is more important to continue to process some fraction of the offered load rather than to fail completely in the event of an overload of processing capacity of the system. Examples of such applications include Internet routers and telephone switches. The systems and methods also may be configured to run in other real time operating system environments such as automotive and aerospace environments where critical processes may be designated that need to be executed during critical events. An example may be in an automotive environment where an airbag deployment event is a low probability event but must be allocated processor budget should the event be initiated.

The systems and methods also may be configured to operate in an environment where untrusted applications may be in use. In such situations applications such as Java applets may be downloaded to execute in the operating system but the nature of the application may allow the untrusted application to take over the system and create an infinite loop. The operating system designer will not want such a situation and may create appropriate adaptive partitions so the untrusted application may be run in isolation while limiting access to CPU time which other processes will have need of.

While various embodiments of the invention have been described it will be apparent to those of ordinary skill in the art that many more embodiments and implementations are possible within the scope of the invention. Accordingly the invention is not to be restricted except in light of the attached claims and their equivalents.

