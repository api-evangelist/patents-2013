---

title: System, method, and computer program product for low latency scheduling and launch of memory defined tasks
abstract: A system, method, and computer program product for low-latency scheduling and launch of memory defined tasks. The method includes the steps of receiving a task metadata data structure to be stored in a memory associated with a processor, transmitting the task metadata data structure to a scheduling unit of the processor, storing the task metadata data structure in a cache unit included in the scheduling unit, and copying the task metadata data structure from the cache unit to the memory.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09378139&OS=09378139&RS=09378139
owner: NVIDIA Corporation
number: 09378139
owner_city: Santa Clara
owner_country: US
publication_date: 20130508
---
The present invention relates to task management and more particularly to the latency associated with loading tasks from memory.

Programming tasks are typically implemented by generating a data structure in a memory that includes information associated with instructions and data to be processed by those instructions. Programming tasks may be written to a memory by one processor and then launched on the same processor or another processor. When such tasks are scheduled and launched the task typically resides in the memory and must first be loaded into an on chip memory such as a level 1 or level 2 cache. Such memory read requests are high latency operations that may cause the processor to stall while the memory where the data structure is stored processes the read requests and transfers the data to the on chip memory. This latency delays the launch of a task until the task is resident in the on chip memory thereby decreasing the efficiency of the processor in scheduling and launching the tasks. Thus there is a need for addressing this issue and or other issues associated with the prior art.

A system method and computer program product for low latency scheduling and launch of memory defined tasks. The method includes the steps of receiving a task metadata data structure to be stored in a memory associated with a processor transmitting the task metadata data structure to a scheduling unit of the processor storing the task metadata data structure in a cache unit included in the scheduling unit and copying the task metadata data structure from the cache unit to the memory.

A hardware scheduling mechanism for a multi threaded processor is described below. The hardware scheduling mechanism provides a means to implement task scheduling including dependent execution of tasks out of order execution of tasks prioritization of tasks and preemption of tasks. A task is associated with a task metadata data structure that encapsulates the task state necessary for configuring a processing unit to complete some subset of work i.e. a program kernel configured to process data . In one embodiment a central processing unit CPU is coupled to a parallel processing unit PPU and the PPU is configured to execute one or more tasks. The tasks are written to a memory accessible by the PPU by either a device driver executing on the CPU or predecessor tasks executed on the PPU. In order to launch a task on the PPU a method call is sent to the PPU that points to a task metadata data structure in the memory. The PPU then loads the task state defined by the task metadata data structure from the memory and launches the task on a processing unit of the PPU.

Because of the latency associated with loading a task metadata data structure from the memory and because the PPU is typically responsible for receiving the data that defines the task state from the CPU and then storing the data in the memory in one embodiment the hardware scheduling mechanism includes a local cache for storing one or more task metadata data structures recently written to a memory associated with the processor. The task state for the most recently received tasks are therefore resident in on chip memory and available for low latency scheduling and launch.

More illustrative information will now be set forth regarding various optional architectures and features with which the foregoing framework may or may not be implemented per the desires of the user. It should be strongly noted that the following information is set forth for illustrative purposes and should not be construed as limiting in any manner. Any of the following features may be optionally incorporated with or without the exclusion of other features described.

In one embodiment the PPU includes an input output I O unit configured to transmit and receive communications i.e. commands data etc. from a central processing unit CPU not shown over the system bus . The I O unit may implement a Peripheral Component Interconnect Express PCIe interface for communications over a PCIe bus. In alternative embodiments the I O unit may implement other types of well known bus interfaces.

The PPU also includes a host interface unit that decodes the commands and transmits the commands to the task management unit or other units of the PPU e.g. memory interface as the commands may specify. The host interface unit is configured to route communications between and among the various logical units of the PPU .

In one embodiment a program encoded as a command stream is written to a buffer by the CPU. The buffer is a region in memory e.g. memory or system memory that is accessible i.e. read write by both the CPU and the PPU . The CPU writes the command stream to the buffer and then transmits a pointer to the start of the command stream to the PPU . The host interface unit provides the task management unit TMU with pointers to one or more streams. The TMU selects one or more streams and is configured to organize the selected streams as a pool of pending grids. The pool of pending grids may include new grids that have not yet been selected for execution and grids that have been partially executed and have been suspended.

A work distribution unit that is coupled between the TMU and the SMs manages a pool of active grids selecting and dispatching active grids for execution by the SMs . Pending grids are transferred to the active grid pool by the TMU when a pending grid is eligible to execute i.e. has no unresolved data dependencies. An active grid is transferred to the pending pool when execution of the active grid is blocked by a dependency. When execution of a grid is completed the grid is removed from the active grid pool by the work distribution unit . In addition to receiving grids from the host interface unit and the work distribution unit the TMU also receives grids that are dynamically generated by the SMs during execution of a grid. These dynamically generated grids join the other pending grids in the pending grid pool.

In one embodiment the CPU executes a driver kernel that implements an application programming interface API that enables one or more applications executing on the CPU to schedule operations for execution on the PPU . An application may include instructions i.e. API calls that cause the driver kernel to generate one or more grids for execution. In one embodiment the PPU implements a SIMD Single Instruction Multiple Data architecture where each thread block i.e. warp in a grid is concurrently executed on a different data set by different threads in the thread block. The driver kernel defines thread blocks that are comprised of k related threads such that threads in the same thread block may exchange data through shared memory. In one embodiment a thread block comprises 32 related threads and a grid is an array of one or more thread blocks that execute the same stream and the different thread blocks may exchange data through global memory.

In one embodiment the PPU comprises X SMs X . For example the PPU may include 15 distinct SMs . Each SM is multi threaded and configured to execute a plurality of threads e.g. 32 threads from a particular thread block concurrently. Each of the SMs is connected to a level two L2 cache via a crossbar or other type of interconnect network . The L2 cache is connected to one or more memory interfaces . Memory interfaces implement 16 32 64 128 bit data buses or the like for high speed data transfer. In one embodiment the PPU comprises U memory interfaces U where each memory interface U is connected to a corresponding memory device U . For example PPU may be connected to up to 6 memory devices such as graphics double data rate version 5 synchronous dynamic random access memory GDDR5 SDRAM .

In one embodiment the PPU implements a multi level memory hierarchy. The memory is located off chip in SDRAM coupled to the PPU . Data from the memory may be fetched and stored in the L2 cache which is located on chip and is shared between the various SMs . In one embodiment each of the SMs also implements an L1 cache. The L1 cache is private memory that is dedicated to a particular SM . Each of the L1 caches is coupled to the shared L2 cache . Data from the L2 cache may be fetched and stored in each of the L1 caches for processing in the functional units of the SMs .

In one embodiment the PPU comprises a graphics processing unit GPU . The PPU is configured to receive commands that specify shader programs for processing graphics data. Graphics data may be defined as a set of primitives such as points lines triangles quads triangle strips and the like. Typically a primitive includes data that specifies a number of vertices for the primitive e.g. in a model space coordinate system as well as attributes associated with each vertex of the primitive. The PPU can be configured to process the graphics primitives to generate a frame buffer i.e. pixel data for each of the pixels of the display . The driver kernel implements a graphics processing pipeline such as the graphics processing pipeline defined by the OpenGL API.

An application writes model data for a scene i.e. a collection of vertices and attributes to memory. The model data defines each of the objects that may be visible on a display. The application then makes an API call to the driver kernel that requests the model data to be rendered and displayed. The driver kernel reads the model data and writes commands to the buffer to perform one or more operations to process the model data. The commands may encode different shader programs including one or more of a vertex shader hull shader geometry shader pixel shader etc. For example the TMU may configure one or more SMs to execute a vertex shader program that processes a number of vertices defined by the model data. In one embodiment the TMU may configure different SMs to execute different shader programs concurrently. For example a first subset of SMs may be configured to execute a vertex shader program while a second subset of SMs may be configured to execute a pixel shader program. The first subset of SMs processes vertex data to produce processed vertex data and writes the processed vertex data to the L2 cache and or the memory . After the processed vertex data is rasterized i.e. transformed from three dimensional data into two dimensional data in screen space to produce fragment data the second subset of SMs executes a pixel shader to produce processed fragment data which is then blended with other processed fragment data and written to the frame buffer in memory . The vertex shader program and pixel shader program may execute concurrently processing different data from the same scene in a pipelined fashion until all of the model data for the scene has been rendered to the frame buffer. Then the contents of the frame buffer are transmitted to a display controller for display on a display device.

The PPU may be included in a desktop computer a laptop computer a tablet computer a smart phone e.g. a wireless hand held device personal digital assistant PDA a digital camera a hand held electronic device and the like. In one embodiment the PPU is embodied on a single semiconductor substrate. In another embodiment the PPU is included in a system on a chip SoC along with one or more other logic units such as a reduced instruction set computer RISC CPU a memory management unit MMU a digital to analog converter DAC and the like.

In one embodiment the PPU may be included on a graphics card that includes one or more memory devices such as GDDR5 SDRAM. The graphics card may be configured to interface with a PCIe slot on a motherboard of a desktop computer that includes e.g. a northbridge chipset and a southbridge chipset. In yet another embodiment the PPU may be an integrated graphics processing unit iGPU included in the chipset i.e. Northbridge of the motherboard.

As described above the work distribution unit dispatches active grids for execution on one or more SMs of the PPU . The scheduler unit receives the grids from the work distribution unit and manages instruction scheduling for one or more thread blocks of each active grid. The scheduler unit schedules threads for execution in groups of parallel threads where each group is called a warp. In one embodiment each warp includes 32 threads. The scheduler unit may manage a plurality of different thread blocks allocating the thread blocks to warps for execution and then scheduling instructions from the plurality of different warps on the various functional units i.e. cores DPUs SFUs and LSUs during each clock cycle.

In one embodiment each scheduler unit includes one or more instruction dispatch units . Each dispatch unit is configured to transmit instructions to one or more of the functional units. In the embodiment shown in the scheduler unit includes two dispatch units that enable two different instructions from the same warp to be dispatched during each clock cycle. In alternative embodiments each scheduler unit may include a single dispatch unit or additional dispatch units .

Each SM includes a register file that provides a set of registers for the functional units of the SM . In one embodiment the register file is divided between each of the functional units such that each functional unit is allocated a dedicated portion of the register file . In another embodiment the register file is divided between the different warps being executed by the SM . The register file provides temporary storage for operands connected to the data paths of the functional units.

Each SM comprises L processing cores . In one embodiment the SM includes a large number e.g. 192 etc. of distinct processing cores . Each core is a fully pipelined single precision processing unit that includes a floating point arithmetic logic unit and an integer arithmetic logic unit. In one embodiment the floating point arithmetic logic units implement the IEEE 754 2008 standard for floating point arithmetic. Each SM also comprises M DPUs that implement double precision floating point arithmetic N SFUs that perform special functions e.g. copy rectangle pixel blending operations and the like and P LSUs that implement load and store operations between the shared memory L1 cache and the register file . In one embodiment the SM includes 64 DPUs 32 SFUs and 32 LSUs .

Each SM includes an interconnect network that connects each of the functional units to the register file and the shared memory L1 cache . In one embodiment the interconnect network is a crossbar that can be configured to connect any of the functional units to any of the registers in the register file or the memory locations in shared memory L1 cache .

In one embodiment the SM is implemented within a GPU. In such an embodiment the SM comprises J texture units . The texture units are configured to load texture maps i.e. a 2D array of texels from the memory and sample the texture maps to produce sampled texture values for use in shader programs. The texture units implement texture operations such as anti aliasing operations using mip maps i.e. texture maps of varying levels of detail . In one embodiment the SM includes 16 texture units .

The PPU described above may be configured to perform highly parallel computations much faster than conventional CPUs. Parallel computing has advantages in graphics processing data compression biometrics stream processing algorithms and the like.

In one embodiment the program offset field stores a memory offset for the start of program instructions for the task. The grid dimensions field includes grid dimensions for the grid. A grid is an array of thread blocks generated to implement the program specified by the program offset field on different sets of input data e.g. pixel data corresponding to each thread. The grid may be one dimensional two dimensional three dimensional or n dimensional. In one embodiment the grid dimensions field includes an x dimension a y dimension and a z dimension for the size of a three dimensional grid array. The block dimensions field stores the dimension for each of the thread blocks and is equal to the number of threads included in each thread block e.g. 32 . The resources field includes state information related to hardware resources allocated to the task. For example the resources field may include a location and size of a circular queue implemented in a memory that stores thread blocks to be added to the task. The cache control field includes data associated with configuring the cache. For example the cache control field may include data that specifies what portion of the L1 cache shared memory is configured as a cache and what portion is configured as a shared memory. The cache control field may also specify how much memory is allocated to each thread in a thread block. The memory barriers field may include counters that are configured to manage task dependency. Similarly the semaphores field may include pointers to semaphores that should be released when a task is completed.

The TMD defines in the memory the encapsulated state information necessary to execute a particular task on a processing unit of the PPU . In other words the TMD may be generated in the memory and the fields of the TMD may be filled by software either a device driver or application executing on the CPU or a different task executing on the PPU and then a pointer to the TMD is passed to the TMU in the PPU to indicate that the task is ready to be scheduled. In some system implementations the TMD for a task is written into a system memory i.e. a memory associated with the CPU and then copied to a video memory i.e. memory . One mechanism for copying the task to the video memory involves transmitting packets of data from the system memory to the PPU via the system bus . The PPU then uses various hardware engines to store the data in the video memory. Once the PPU is ready to schedule the task the TMD or at least portions of the TMD is read from the video memory into on chip memory structures accessible to the TMU and or the SMs . Because memory access requests for writing and reading the data have a high latency the conventional techniques have a high latency between the time when the task state associated with a task is generated and the time when a task can be launched. A new hardware mechanism for transmitting task metadata to the PPU is described that uses a different hardware mechanism for copying the task metadata to the memory than the hardware mechanism for different types of data copied to the memory .

When the TMU receives an instruction that indicates a task is ready to be scheduled the TMU may pull at least a portion of the TMD associated with the task from the memory . In one embodiment the TMU includes a scheduler cache that buffers the portions of one or more TMDs in an on chip memory accessible by the TMU . The data in the portion of the TMD may be analyzed by the TMU to resolve dependencies between tasks to determine whether the task is ready to be launched. For example the TMD may include approximately 256 bytes of information related to the state encapsulating a task. The scheduler cache may include entries that are sized to hold a portion e.g. 64 bytes of the TMD that is used to resolve dependencies for the task delaying retrieval of the full TMD from memory until the task is ready to be launched. Once the input dependencies for a task have been resolved the task may then be sent to the SL unit to be scheduled and launched. The SL unit is configured to retrieve the remaining fields of the TMD for the task and transmit the task state i.e. the data included in the TMD to the WDU such that the task can be executed on one or more of the SMs .

Conventionally a CPU e.g. a device driver could make the TMD accessible to the PPU in multiple ways. In one technique the driver could write the TMD to a system memory that is also accessible to the PPU . Then the driver could transmit a command to one or more copy engines on the PPU that copy the data in the system memory to the video memory i.e. memory . Once the data was copied the driver could pass a command with a pointer to the TMD in memory to the TMU that indicated that the task was ready to be scheduled. Another technique for making the TMD accessible to the PPU is to stream the data to the PPU through a pushbuffer i.e. a circular queue in memory that was accessible to both the CPU and the PPU where the CPU could write instructions and data into the pushbuffer and the PPU could read the instructions and data from the pushbuffer thereby allowing asynchronous operation of the CPU and the PPU . The driver would first write an instruction to the pushbuffer indicating that the data immediately following the instruction in the pushbuffer is associated with an inline memory request. The instruction may include any necessary state for the PPU to process the inline memory request such as a length of the data a destination address in the memory to store the data and so forth. Then the driver would stream the data for the TMD into the pushbuffer. The PPU via the I O unit and the host interface unit would receive and decode the inline memory request instruction from the pushbuffer and configure the crossbar to transmit the data for the TMD to the memory at an address specified by the instruction. If the TMD was generated by a task executing on the PPU then the task could simply write the data for the TMD to the memory via existing mechanisms in the SM .

It will be appreciated that these conventional techniques for storing a TMD in the memory create a high latency between when the task state for a task is generated and stored in the memory and when the task state is available in an on chip memory and ready to be transmitted to the WDU for launch. First the TMD must be stored in the memory and the PPU via the host interface unit must wait to receive an acknowledgement from the memory that the write was successful. Then the TMU can read a first portion of the TMD from the memory storing the first portion of the TMD in the scheduler cache unit . Finally once all input dependencies for the task have been resolved the remaining fields of the TMD can be fetched from the memory and the task can be launched by the SL unit . Receiving the TMD at the PPU storing the TMD in the memory retrieving a first portion of the TMD from the memory and retrieving the second portion of the TMD from the memory create a very inefficient and high latency operation that delays the launching of tasks that are otherwise available for immediate launch. Therefore a different mechanism is proposed for storing TMDs received by the PPU such that the latency between when a task is ready to be launched and when the task is launched is reduced.

In one embodiment a new instruction that is similar to the inline memory request described above is capable of being executed by the PPU . A new inline task memory request is defined that indicates to the PPU that the data being copied to the memory via the pushbuffer should be transmitted to the TMU to be copied out to the memory rather than being handled by the host interface unit . The host interface unit receives the inline task memory request as well as the data defining a task state i.e. TMD and transmits the inline task memory request and the data to the TMU . The TMU writes the data associated with the inline task memory request to a launch cache unit . The launch cache unit provides temporary on chip memory storage for one or more full TMDs . The scheduler cache unit and the SL unit are configured to check the launch cache unit to determine whether a copy of the TMD is already stored in the launch cache unit before requesting the data for a task from the memory . If the TMD is stored on chip in the launch cache unit then the launch cache unit indicates a cache hit and the data is available for launch with low latency. However if the TMD is not stored in the launch cache unit or has been evicted from the launch cache unit then the launch cache unit indicates a cache miss and the data is retrieved from the memory . Thus when a TMD is received by the PPU the TMD is stored in an available slot of the launch cache unit before being copied out to memory . As long as the TMD has not been evicted from the launch cache unit when the TMU receives an instruction that indicates the task is ready to be scheduled then the data for the TMD is already resident in on chip memory accessible to the TMU and the task can be launched with a lower latency than associated with the prior art techniques.

In one embodiment the launch cache unit is a write through cache such that the data written to the launch cache unit is also transmitted to the memory as the data is written into the launch cache unit . After the TMD has been written to the launch cache unit an instruction may be transmitted to the TMU that includes a pointer to the TMD . The TMU is configured to receive this instruction and read a first portion of the TMD from the memory . The scheduler cache unit transmits a memory read request to the launch cache unit to retrieve the first portion of the TMD from the memory . Because the task was recently copied into the launch cache unit the memory read request results in a cache hit and the first portion of the TMD is copied from the launch cache unit to the scheduler cache unit . This operation is performed with low latency because the full TMD is resident in on chip memory. The TMU may then resolve any input dependencies for the task. Once the input dependencies for the task have been resolved the SL unit retrieves task state information for the task from the scheduler cache unit and or the launch cache unit such that the task state can be transmitted to the WDU so that the task can be launched. It will be appreciated that other information may also be examined when determining which task should be launched in addition to the input dependency information. For example a task priority may be examined to determine the highest priority task that is eligible for launch or system resources may be examined to determine that there are available processors i.e. SMs or memory to launch the task.

When the SL unit receives the task state the SL unit may transmit a memory read request to the launch cache unit to read additional fields of the TMD that are not resident in the scheduler cache . Again because the TMD was recently copied into the launch cache unit the memory read request results in a cache hit and the remaining portion of the TMD is copied from the launch cache unit to the SL unit . This operation is also performed with low latency because the full TMD is resident in on chip memory. This multi level cache hierarchy implemented in the TMU as well as the new method for storing TMDs in the memory via the TMU allow low latency scheduling and launch of memory defined tasks transmitted to the PPU relative to prior art techniques.

In another embodiment the launch cache unit is a write back cache. It will be appreciated that the launch cache unit stores a local copy of the TMD for use by the SL unit . However the task state information stored in the TMD may need to be accessible to the SMs once the task is launched. Because the SMs do not have access to the launch cache unit the TMU implements a mechanism that initiates a write back of the TMD stored in the launch cache unit when a task is launched. That way the TMD is not immediately copied to memory when received by the TMU but is only copied to memory when the task is launched on the SM . In such cases care must be taken that the task state in a TMD has been successfully written to the memory e.g. by waiting to transmit the task state data to the WDU until an acknowledgement has been received that the data is stored in the memory before a task executed on the SMs attempts to access the task state for the TMD in the memory .

In one embodiment when the launch cache unit is full the TMD is not stored in the launch cache unit . Instead the TMD is simply copied into the memory . In such cases when the scheduler cache unit attempts to load the first portion of the TMD into the on chip memory the launch cache unit will indicate a cache miss and retrieve the data for the first portion of the TMD from the memory . In other words the launch cache unit may not implement an eviction policy when the cache is full. In another embodiment the launch cache unit may implement an eviction policy to free up an entry in the launch cache unit when a new TMD is received by the TMU . For example the launch cache unit may track via a counter or other mechanism an order for which TMDs are received by the TMU . When the launch cache unit determines that the launch cache unit is full the launch cache unit may evict the TMD associated with the longest pending task from the launch cache unit . Other eviction policies may be implemented such as eviction based on random selection of an entry eviction based on a task priority value and so forth. In one embodiment care is taken that no TMDs associated with tasks that are ready to be launched should be evicted from the launch cache unit . For example when the dependencies have been resolved for a task but the SL unit is waiting for the WDU to be ready to accept a new task. In another embodiment care is taken that no TMDs associated with tasks that have been recently launched and then preempted should be evicted from the launch cache unit . Such tasks are likely to be ready to be executed in the near future and therefore the TMDs associated with such tasks should not be evicted from the launch cache unit to make room for a newly arriving TMD .

In one embodiment the TMD includes read only fields and hardware writeable fields. The read only fields are written by software when the TMD is generated and cannot be updated by the TMU in the launch cache unit . Thus these fields cannot be dirty in the launch cache unit . However the hardware writeable fields can be updated periodically by the TMU after being read from the launch cache unit and stored in the scheduler cache unit or in other registers internal to the TMU . When the TMU overwrites this data the launch cache unit may mark the data as dirty and schedule a write back to the memory to keep coherence with the copy of the TMD in the memory . Again when a task is launched by transmitting the task state to the WDU the TMU is configured to ensure that any dirty data in the launch cache unit for the task is updated in the memory to be accessible to the SMs . In another embodiment the scheduler cache unit may be configured to read and write the hardware writeable fields directly from to memory bypassing the launch cache unit . Thus only the read only fields are read into the launch cache unit and read from the launch cache unit by the scheduler cache unit . In such embodiments the SL unit is configured to read the read only fields from the launch cache unit and the hardware writeable fields from the scheduler cache unit .

In some embodiments the TMDs are generated by tasks executing on the SMs in the PPU . In such embodiments instead of storing the TMD in the memory directly the SMs may generate messages that are transmitted to the TMU via a reflected interface implemented by a memory management unit. These messages may contain the equivalent instruction for the inline task memory request as well as the data that defines the TMD for a task. By transmitting these messages to the TMU the task being executed by the SMs may use the TMU to store the task in the memory rather than storing the TMD in the memory directly and then requiring the TMU to read the task from the memory .

In another embodiment the PPU may enable context switching between different time slice groups TSGs . A TSG includes all of the tasks that are communicating via a single virtual channel within the PPU . The TSG in effect allows different tasks to be switched into and out of the state of the PPU to enable different sets of tasks to be executed substantially concurrently. Because the inline task memory request relies on state within the TMU in order to process continually incrementing addresses in memory for storing the TMDs this state must be stored and re loaded whenever context switching is performed. In addition the PPU should be configured to resume the same inline task memory request when a particular TSG context is restored that was in progress when the TSG context was switched out.

The system also includes input devices a graphics processor and a display i.e. a conventional CRT cathode ray tube LCD liquid crystal display LED light emitting diode plasma display or the like. User input may be received from the input devices e.g. keyboard mouse touchpad microphone and the like. In one embodiment the graphics processor may include a plurality of shader modules a rasterization module etc. Each of the foregoing modules may even be situated on a single semiconductor platform to form a graphics processing unit GPU .

In the present description a single semiconductor platform may refer to a sole unitary semiconductor based integrated circuit or chip. It should be noted that the term single semiconductor platform may also refer to multi chip modules with increased connectivity which simulate on chip operation and make substantial improvements over utilizing a conventional central processing unit CPU and bus implementation. Of course the various modules may also be situated separately or in various combinations of semiconductor platforms per the desires of the user.

The system may also include a secondary storage . The secondary storage includes for example a hard disk drive and or a removable storage drive representing a floppy disk drive a magnetic tape drive a compact disk drive digital versatile disk DVD drive recording device universal serial bus USB flash memory. The removable storage drive reads from and or writes to a removable storage unit in a well known manner.

Computer programs or computer control logic algorithms may be stored in the main memory and or the secondary storage . Such computer programs when executed enable the system to perform various functions. The memory the storage and or any other storage are possible examples of computer readable media.

In one embodiment the architecture and or functionality of the various previous figures may be implemented in the context of the central processor the graphics processor an integrated circuit not shown that is capable of at least a portion of the capabilities of both the central processor and the graphics processor a chipset i.e. a group of integrated circuits designed to work and sold as a unit for performing related functions etc. and or any other integrated circuit for that matter. For example the central processor may execute a driver kernel that generates TMDs for execution by the graphics processor and the graphics processor may include the TMU configured to receive a copy of the TMDs and store a copy of the TMDs in a cache unit in the TMU .

Still yet the architecture and or functionality of the various previous figures may be implemented in the context of a general computer system a circuit board system a game console system dedicated for entertainment purposes an application specific system and or any other desired system. For example the system may take the form of a desktop computer laptop computer server workstation game consoles embedded system and or any other type of logic. Still yet the system may take the form of various other devices including but not limited to a personal digital assistant PDA device a mobile phone device a television etc.

Further while not shown the system may be coupled to a network e.g. a telecommunications network local area network LAN wireless network wide area network WAN such as the Internet peer to peer network cable network or the like for communication purposes.

While various embodiments have been described above it should be understood that they have been presented by way of example only and not limitation. Thus the breadth and scope of a preferred embodiment should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

