---

title: Application program interface of a parallel-processing computer system that supports multiple programming languages
abstract: A runtime system implemented in accordance with the present invention provides an application platform for parallel-processing computer systems. Such a runtime system enables users to leverage the computational power of parallel-processing computer systems to accelerate/optimize numeric and array-intensive computations in their application programs. This enables greatly increased performance of high-performance computing (HPC) applications.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08745603&OS=08745603&RS=08745603
owner: Google Inc.
number: 08745603
owner_city: Mountain View
owner_country: US
publication_date: 20130510
---
This application is a continuation of U.S. patent application Ser. No. 11 714 619 filed Mar. 5 2007 entitled An application program interface of a parallel processing computer system that supports multiple programming languages which claims priority to U.S. Provisional Patent Application 60 815 532 filed Jun. 20 2006 entitled Systems and Methods for High Performance Computing Using Stream Processors and U.S. provisional Patent Application 60 903 188 filed Feb. 23 2007 entitled Programming Tools for a High Performance Computing System all of which are hereby incorporated by reference in their entireties.

This application relates to U.S. patent application Ser. No. 11 714 592 now U.S. Pat. No. 8 136 104 entitled Systems and methods for determining compute kernels for an application in a parallel processing computer system filed on Mar. 5 2007 which is hereby incorporated by reference in its entirety.

This application relates to U.S. patent application Ser. No. 11 714 591 now U.S. Pat. No. 8 381 202 entitled A runtime system for executing an application in a parallel processing computer system filed on Mar. 5 2007 which is hereby incorporated by reference in its entirety.

This application relates to U.S. patent application Ser. No. 11 714 654 now U.S. Pat. No. 8 108 844 entitled Systems and methods for dynamically choosing a processing element for a compute kernel filed on Mar. 5 2007 which is hereby incorporated by reference in its entirety.

This application relates to U.S. patent application Ser. No. 11 714 630 now U.S. Pat. No. 8 261 270 entitled Systems and methods for generating reference results using a parallel processing computer system filed on Mar. 5 2007 which is hereby incorporated by reference in its entirety.

This application relates to U.S. patent application Ser. No. 11 714 629 now U.S. Pat. No. 8 024 708 entitled Systems and methods for debugging an application running on a parallel processing computer system filed on Mar. 5 2007 which is hereby incorporated by reference in its entirety.

This application relates to U.S. patent application Ser. No. 11 714 480 now U.S. Pat. No. 8 146 066 entitled Systems and methods for caching compute kernels for an application running on a parallel processing computer system filed on Mar. 5 2007 which is hereby incorporated by reference in its entirety.

This application relates to U.S. patent application Ser. No. 11 714 582 now U.S. Pat. No. 8 136 102 entitled Systems and methods for compiling an application for a parallel processing computer system filed on Mar. 5 2007 which is hereby incorporated by reference in its entirety.

This application relates to U.S. patent application Ser. No. 11 714 583 now U.S. Pat. No. 7 814 486 entitled A multi thread runtime system filed on Mar. 5 2007 which is hereby incorporated by reference in its entirety.

This application relates to U.S. patent application Ser. No. 11 716 508 now U.S. Pat. No. 8 375 368 entitled Systems and Methods for Profiling an Application Running on a Parallel Processing Computer System filed on Mar. 9 2007 which is hereby incorporated by reference in its entirety.

The present invention relates generally to the field of parallel computing and in particular to systems and methods for high performance computing on a parallel processing computer system including multiple processing elements that may or may not have the same processor architecture.

Today a parallel processing computer system including one or more processors and or coprocessors provides a tremendous amount of computing capacity. But there is lack of an efficient stable robust and user friendly software development and execution platform for such computer system. Therefore there is a need for a software development and execution platform that provides an easy to use program interface and rich library resources supports program debugging and profiling and enables the execution of the same program on any types of parallel processing computer system.

In some embodiments a method for providing an application programming interface in a parallel processing computer system includes receiving from an application one or more operation requests directed to the parallel processing computer system wherein the application is written in one or more programming languages and generating a programming language independent processor independent intermediate representation for at least one of the one or more operation requests wherein the intermediate representation includes one or more objects corresponding at least one of the one or more operation requests.

In some embodiments the method for providing an application programming interface can further include identifying from a plurality of language specific application program interface modules an application program interface module and generating the intermediate representation using the identified application program interface module.

Some embodiments can also include dynamically selecting at least one of one or more processing elements associated with the parallel processing computer system for the intermediate representation and dynamically preparing one or more compute kernels for the intermediate representation wherein the one or more compute kernels are configured to execute on the selected one or more processing elements.

Reference will now be made in detail to embodiments examples of which are illustrated in the accompanying drawings. In the following detailed description numerous specific details are set forth in order to provide a thorough understanding of the subject matter presented herein. But it will be apparent to one of ordinary skill in the art that the subject matter may be practiced without these specific details. In other instances well known methods procedures components and circuits have not been described in detail so as not to unnecessarily obscure aspects of the embodiments. For example it will be apparent to one of ordinary skill in the art that the subject matter presented herein can be implemented on any type of parallel processing computer system containing any types of processing elements including processors and or coprocessors although embodiments described below are related to particular types such as graphics processing units GPUs and multi core CPUs.

The term parallel processing computer system herein refers to a computing system that is capable of performing multiple operations simultaneously. A parallel processing computer system may contain one or more processing elements including but not limited to processors and coprocessors which may be deployed on a single computer or a plurality of computers linked wired and or wireless by a network in a cluster or grid or other types of configuration. In some embodiments a processing element includes one or more cores which may share at least a portion of the same instruction set or use completely different instruction sets. The cores within a processing element may share at least a portion of a common memory space and or have their own memory spaces. Mechanisms used to implement parallel execution of the operations include but are not limited to multiple instruction multiple data MIMD execution of multiple instruction sequences single instruction multiple data SIMD execution of a single instruction sequence vector processing pipelining hardware multi threading very long instruction word VLIW or explicitly parallel instruction computing EPIC instructions superscalar execution and a combination of at least two of the aforementioned mechanisms. The parallel processing computer system as a whole may use a single system image multiple system images or have no underlying operating system.

In some embodiments a processing element or a core within a processing element may or may not run an operating system or a virtual machine monitor e.g. hypervisor that manages one or more operating systems running on a computer system at the same time. Examples of such processors and coprocessors include graphics processing units GPUs by nVidia and ATI single core and multiple core x86 and Itanium processors by Intel single and multiple core x86 and x86 64 processors by AMD single core and multiple core PowerPC processors by IBM the Cell processor by STI the Niagara processor by Sun Microsystems and the Threadstorm processor or X1E multi streaming processor by Cray Inc.

In some embodiments a processing element may be a thread running on a physical processor or virtual machine such as application level threads kernel threads or hardware threads. In some other embodiments a processing element may be a virtual machine running inside a hypervisor. In other embodiments a processing element may be a functional unit within a physical processor or a virtual machine.

It will be apparent to one skilled in the art that the embodiments described in detail below in connection with are only for illustrative purpose. For example depicts that the different components of the runtime system run on the same main CPU . In other embodiments the different components may run on multiple physical processing elements of the same parallel processing computer system or multiple computers that are connected via a network or a combination thereof. Communication mechanisms known in the art can be employed to ensure that information be exchanged among the different processing elements and computers accurately and efficiently.

At run time an application invokes the runtime system by calling into one of its Language Specific Interfaces LSI to perform operations predefined in the application . In some embodiments the LSI includes multiple modules each module providing an Application Program Interface API to the runtime system for applications written in a specific programming language. The runtime system is configured to spawn zero or more threads initialize one or more processing elements including processors coprocessors processor coprocessor cores functional units etc. as necessary and execute compute kernels associated with the predefined operations on the processing elements accessible to the runtime system . In some embodiments a compute kernel is an executable program or subroutine that runs on one or more processing elements that are part of a parallel processing computer system to perform all or parts of one or more operation requests. In some embodiments the runtime system includes dynamically linked libraries static libraries or a combination of both types of libraries. In some embodiments the runtime system includes one or more standalone programs that run independently of user applications with which the other components of the runtime system e.g. its libraries communicate.

From the runtime system s user s perspective the aforementioned approach has multiple advantages. First it allows easy porting of existing applications from one programming language to another language. Second it allows easy integration of the runtime system s libraries with other standard libraries such as Math Kernel Library MKL and Message Passing Interface MPI . For example data are easily passed between the runtime system s libraries and other standard libraries called from the application through the computer hardware s main system memory. This is important because many applications rely at least in part on these standard libraries. Third this approach allows the source code of the application to be compiled using existing compilers. Fourth this approach is highly compatible with existing software development tools. Standard tools such as debuggers profilers code coverage tools etc. can be easily integrated with the runtime system . Finally an application that runs on an older version of the runtime system can be executed on a new version of the runtime system without being re compiled. This feature enables the application to benefit from any features developed in the new generations of parallel processing computer systems.

The runtime system supports programming development using multiple programming languages. In some embodiments the application can be written in different programming languages including C C Fortran 90 MatLab and the like. For each programming language the corresponding LSI module in the LSI binds the application with the rest of the runtime system . Because of the LSI the Front End FE of the runtime system can be invoked by applications written in different programming languages without the need to have multiple front ends one for each programming language.

In some embodiments the LSI module for a programming language includes functions procedures operators if the language supports operator overloading macros constants objects types and definitions of other language constructs which represent common mathematical and I O operations operations supported by the runtime system values of special significance e.g. PI objects manipulated by the runtime system and so on. The application uses the functions procedures operators etc. defined by the LSI to request that the runtime system perform the operations specified in the application. The LSI captures and marshals each of these requests including its kind its arguments and any other information used for error handling debugging or profiling purposes e.g. an API call s address or its return address and passes the request to the Front End FE of the runtime system .

In some embodiments the LSI uses a functional programming model in which the result e.g. an array is returned by each operation. In some other embodiments the LSI implements an imperative programming model in which each operation overwrites one of its operands which is similar to many existing mathematical libraries. In other embodiments the LSI adopts a programming model that is a combination of the two models.

The LSI may operate on its own data types such as array objects or on native data types or a combination of the two. If the LSI operates on its own data types native data types in the application may be explicitly converted to the LSI s own data types e.g. using a function that takes an operand using a native data type and returns an object of the corresponding LSI data type . In some embodiments the programming language associated with the application may arrange for the conversion to be performed without it having been explicitly requested by the application.

In some embodiments the use of the LSI s own data types has several advantages over native data types. First it facilitates the use of data representations in the FE that are different from the native representations of data in the programming language. Second the use of opaque data types that require conversion back to native data types to be operated on outside the runtime system facilitates progressive evaluation in which the processing of requests to the FE may be deferred and batched for more efficient processing. Finally the use of opaque data types facilitates the use of memory by the runtime system that is not directly accessible to the application such as the memory on a GPU board.

In some embodiments to simplify memory management the LSI has facilities to mark the scope in which an object can be accessed. For example the C LSI module does this using object constructors and object destructors which define the scopes of respective individual objects by notifying the FE of each object s creation and destruction. The C LSI module uses explicit function calls such as begin scope and end scope to define the scopes of objects created between the two function calls. Some embodiments may support explicit object creation and destruction calls rather than or in addition to these scoping constructs.

In some embodiments the FE is responsible for invoking the supervisor . The supervisor is configured to start up some other components of the runtime system at the beginning of executing the application . In response some components invoke their own modules to create their own threads. At the conclusion of the execution the supervisor is responsible for shutting down these components and killing threads associated with these components.

In some embodiments the FE processes some requests from the LSI immediately but defers processing some other requests for progressive evaluation in batches. For each deferred request the FE is responsible for generating one or more intermediate representation IR nodes each IR node representing an operation to be performed by the runtime system . Generally the term intermediate representation refers to a data structure that is constructed from input data to a program and from which part or all of the output data of the program is constructed in turn. In this application an IR node has information identifying its type arguments and optionally information used for error handling debugging or profiling purposes such as the address of an API call in the application corresponding to a request to the LSI or its return address. These IR nodes are used by other components of the runtime system e.g. the program generator to generate compute kernels to be executed on the parallel processing computer system. In some embodiments the IR nodes include information used for generating optimized compute kernels for the different types of processing elements of the parallel processing computer system.

In some embodiments the compilation scheduler C Scheduler takes the IR nodes generated by the FE buffers them in a data structure called a work queue and according to a predefined schedule requests that the ProgGen generate one or more compiled programs corresponding to the IR nodes. A compiled program is also referred to as a compute kernel that includes executable binary code for one of the processing elements of the parallel processing computer system. Therefore a compiled program sequence corresponds to a sequence of compute kernels which may correspond to one or more processing elements. For convenience the two terms compiled program and compute kernel are used interchangeably in this application.

The ProgGen returns to the C Scheduler a sequence of compiled programs corresponding to the IR nodes. In some embodiments the ProgGen generates multiple copies of compiled programs one for each type of processing element of the parallel processing computer system for executing the compiled programs. The C Scheduler then submits the sequences of compiled programs to the execution scheduler E Scheduler to arrange for execution of the compiled programs on the respective processing element s .

To improve its performance the runtime system includes two caches the trace cache and the macro cache . In some embodiments the operation of the trace cache is transparent to the application . A developer does not have to make any special arrangement to invoke the trace cache . The C Scheduler automatically caches program sequences generated by the ProgGen in the trace cache . Therefore the trace cache can reduce the runtime system s compilation overhead by comparing the incoming IR nodes from the FE with the previously generated program sequences that are stored in the trace cache and reusing the program sequences whenever possible.

In some embodiments upon receiving a set of IR nodes from the FE the C Scheduler checks the trace cache for the previously compiled program sequences that correspond to the same set of IR nodes or a subset thereof and re uses the pre compiled program sequences.

In some embodiments the C Scheduler automatically starts searching the trace cache for compiled program sequences that match a set of IR nodes that the C Scheduler has accumulated whenever a predefined condition is met e.g. the set of IR nodes correspond to at least a predefined number of application function API calls . When such condition is met the C Scheduler does not or cannot verify whether processing the set of IR nodes can yield the best performance that the application can achieve. For example the C Scheduler may start searching the trace cache even if the last IR node in the set corresponds to an API call in the middle of a for loop structure in the application .

The macro cache offers an alternative solution to this problem by allowing an application developer to explicitly define a code macro in the application . Note that the term code macro and code reuse section are used interchangeably throughout this application. One rationale for using the macro cache is that processors usually perform better when handling a sequence of computationally intensive operations. Therefore it is advantageous for the application developer to group multiple e.g. dozens or even hundreds of computationally intensive operations in the application together using keywords provided or accepted by the runtime system . These keywords provide clear instructions to the runtime system in particular the C Scheduler as to which set of IR nodes should be processed together to achieve better performance.

In some embodiments the number of operations packaged within a macro is heuristic based. For example the number of operations that the ProgGen is likely to combine into a single compiled program also known as a compute kernel can be used to determine the size of the macro. The macro cache enables the developer to specify a set of API calls to be compiled into one or more program sequences once by the ProgGen which are stored in the macro cache and repeatedly invoked subsequently as a macro group. This feature substantially reduces the API call overhead because only one API call is required per macro group invocation rather than one API call per operation.

In some embodiments when a code reuse section of API calls is first invoked the C Scheduler and the ProgGen treat the code reuse section like other regular API calls. The only difference is that the compiled program sequences corresponding to the code reuse section are stored in the macro cache not the trace cache . Upon receiving subsequent invocations of the same code reuse section by the application the C Scheduler retrieves the compiled program sequences from the macro cache and executes them accordingly.

In some embodiments the IR nodes generated by the FE are organized into a directed acyclic graph DAG . The ProgGen transforms the DAG into one or more sequences of compiled programs i.e. compute kernels to be executed on a parallel processing computer system. In some embodiments each compiled program sequence is itself a DAG.

In some embodiments the operation requests from an application correspond to one of two types of operations i intrinsic or ii primitive. An intrinsic operation includes one or more programs or functions. In some embodiments these programs and functions are hand coded for a specific type of processor such as GPU to unleash its parallel processing capacity. Examples of intrinsic operations include matrix multiplication matrix solvers FFT convolutions and the like. As shown in the intrinsic operations are stored in the intrinsic library . In some embodiments these intrinsic operations are highly optimized and may or may not be fully pre compiled. At run time for an IR node associated with an intrinsic operation the ProgGen invokes a processor specific hand crafted routine from the intrinsic library and constructs one or more program sequences to implement the operation specified by the IR node.

In some embodiments primitive operations correspond to element wise data array operations such as arithmetic operations and trigonometric functions. For IR nodes associated with primitive operations the ProgGen dynamically generates processor specific program sequences that perform the operations prescribed by the IR nodes. For example to generate a program sequence for a GPU the ProgGen performs a lookup in the primitive library for a primitive operation s source code combines the source code with source code associated with other primitive operations and invokes the GPU compiler and the GPU assembler to generate executable binary code for the GPU. Similarly to generate a program sequence for a multi core CPU the ProgGen retrieves from the primitive library source code corresponding to one or more primitive operations and invokes the CPU compiler and the CPU assembler to generate executable binary code on the multi core CPU. In some embodiments the source code for either GPU or CPU is preprocessed before compilation. In some embodiments dynamic linking and or loading of the binary code for either GPU or CPU into memory follows the assembly operation. One skilled in the art would understand that this compilation process may be implemented by fewer individual steps than discussed here such as by directly generating lower level program representations by combining steps or by restricting functionality.

In some embodiments library routines corresponding to the primitive operations are also hand coded and highly optimized. In some embodiments they are highly accurate. In some embodiments the primitive operations are stored in the primitive library as source code rather than as assembly code or binary code to facilitate dynamic insertion into programs by the ProgGen .

The ProgGen generates compiled program sequences for processing elements of the parallel processing computer system in response to API calls in the application . The program sequences are then executed on processing elements of the parallel processing computer system e.g. a GPU or multi core CPUs . In some embodiments the performance of the runtime system depends on the computational intensity of a compute kernel which is measured by the number of operations executed between two consecutive memory accesses also known as the program sequence s operation memory access ratio .

Generally different types of IR nodes have different operation memory access ratios. For example a map operation could perform as little as one operation per element which cannot saturate a typical processor s capacity if executed naively in isolation. Reduce operations share the same problem because they also have low operation memory access ratios. In some embodiments a compute kernel prepared by the ProgGen for a specific type of processing elements e.g. GPUs or multi core CPUs corresponds to one or more IR nodes. A compute kernel with a high operation memory access ratio is therefore more efficient when being executed on a particular processing element and vice versa.

Conventionally it is an application developer s responsibility for putting the API calls in an application into different groups that may correspond to different compute kernels being executed by the runtime system. To achieve best performance the developer has to understand how the runtime system prepares compute kernels for different types of processing elements and group the API calls differently for different processing elements. This approach is also referred to as kernel style programming . One of the significant disadvantages with this approach is that the compute kernels prepared by the developer are less optimal for a specific type of processing elements and therefore result in poor performance of the runtime system.

The ProgGen solves this problem through a process called automatic compute kernel analysis . In some embodiments the ProgGen automatically determines which set of IR nodes should correspond to a compute kernel targeting a specific type of processing elements based on the IR nodes associated operation memory access ratios. For example the ProgGen first estimates the ratios of the IR nodes currently in the work queue and attempts to merge operations into the compute kernel until the operation memory access ratio of the compute kernel exceeds a predefined threshold. One of the exemplary methods is the loop fusion with scalar replacement which is the process of merging back to back array operations into a reduced number of separate array operations so as to decrease memory accesses and thereby increase the operation memory access ratio. As a result the application developer is responsible for specifying what operations should be performed by the runtime system in the application. The application prepared in such manner is portable across different types of processing element architectures with good performance.

In some embodiments where the ProgGen invokes a GPU assembler the output programs of the ProgGen are in the GPU binary instruction set architecture ISA format which requires no further compilation assembly or linking before being loaded and executed on a GPU processor. Consequently the GPU programs can be deployed on the GPU s significantly faster. This approach also allows the intrinsic operations and the ProgGen generated programs to implement GPU specific optimizations at the machine language level for significant performance gains. Note that different types of GPUs e.g. ATI and nVidia GPUs have different ISAs which can be supported by a simple embodiment.

The execution scheduler E Scheduler has two main functions. First it dispatches compiled programs to respective executors at run time. In some embodiments the E Scheduler is responsible for synchronizing the computational tasks at different executors . It ensures that data buffers are in the correct place i.e. available before an operation that uses them is allowed to proceed. Second the E Scheduler schedules and in some cases performs data array movement. In some embodiments the E Scheduler is responsible for transferring data arrays between the application and the runtime system . For example it performs scatter type and gather type I O operations to meet the application s data layout requirements.

The executors are responsible for executing the compiled programs they receive from the E Scheduler . Depending on a processing element s architecture the runtime system may include multiple types of executors one for each type of processor of the parallel processing computer system. A GPU executor manages the execution of compiled compute kernels on a particular GPU that the GPU executor is associated with It keeps the GPU s input buffer as full as possible by submitting many operations and overlapping the execution of certain compute and I O operations. A multi GPU computer hardware platform may include more than one GPU executor . Each GPU executor manages I O operations between the CPU and the GPU performs GPU memory allocation de allocation and swaps memory in the low memory situations.

A CPU executor plays a role analogous to that of a GPU executor . In some embodiments the CPU executor is configured to manage operations that can be performed more efficiently on single core or multi core CPUs. For example a CPU may be a preferred choice for handling double precision calculations. In some embodiments if the GPUs become at least temporarily or permanently less available for some reasons the runtime system automatically invokes the CPU executor to handle the workload normally handled by the GPUs. In some other embodiments the runtime system may transfer some workload from the CPU side to the GPU side for similar reasons. In some embodiments CPU executors may be responsible for transferring data arrays between the application and the runtime system . In some embodiments there can be multiple CPU executors in a multi CPU or a multi core computer. The multiple CPU executors perform independent tasks to exploit the computer s parallel data processing capacity. In some embodiments a CPU executor may have one or more slave threads to which it distributes work.

In some embodiments the runtime system cannot access a processing element directly. Instead it has to access the processing element through a software interface e.g. a software library or a driver interface provided by the vendor of the processing element. For example shows that the GPU executors call the GPU driver library to access a GPU s resources or execute compute kernels on the GPU. In some embodiments the GPU driver library enables the runtime system to control the GPU s memory and thereby facilitates alignment of data arrays according to their designated usage for improved performance and enables memory aware scheduling algorithms. Through the GPU driver library the runtime system can execute binary codes that directly target a specific GPU s instruction set architecture ISA . This configuration makes it possible for the intrinsic library to include intrinsic operations that are optimized based on a particular GPU s ISA to maximize the runtime system s performance on the GPU. In some embodiments a software library like the GPU driver library can also reduce the runtime system s overhead and the interface latency between the executors and the respective processing elements particularly coprocessors managed as devices such as GPUs.

In some embodiments the GPU driver library possesses the following performance enhancing features or a subset thereof 

In some other embodiments the runtime system accesses a GPU through a standard software interface such as OpenGL or Direct3D. These software packages support the portability of the runtime system across different architectures of processing elements.

In some embodiments API calls in the application specify the semantics of operations performed by the runtime system . The compute kernels to be executed on the processing element s of the parallel processing computer system may have a different sequence from the sequence of operations specified by the API calls so long as the execution order preserves the specified semantics. In some embodiments for certain operations one API call is split into multiple compute kernels. In some other embodiments for certain sequences of operations multiple API calls are merged into one compute kernel. In yet some other embodiments for certain sequences of operations multiple API calls are converted into multiple compute kernels. The E Scheduler is responsible for coordinating the execution of compute kernels on different processors to ensure that the result of the application is reliable and accurate i.e. independent from the specific computer hardware platforms.

In some embodiments there is no developer specified application level error handler in the application. In this case an execution error may result in the immediate termination of the application. In some embodiments an error message is returned to the application developer indicating the location e.g. file name and line number of an operation that triggers the error. In some embodiments an explanation of what might have caused the error is also provided for the developer s reference.

In some other embodiments a developer can insert error handlers into the application to specify how execution errors should be handled. An application level error handler has the flexibility to ignore certain errors resolving certain errors by aborting the computations that caused the errors or invoking a previously defined error handler when certain error occurs. Through its arguments an application level error handler function is provided with application level arguments such as error type file name line number and program counter.

In some embodiments different components of the runtime system run in multiple threads to improve the system s performance. As shown in the C Scheduler runs in the application thread and the E Scheduler is associated with the E Scheduler thread . The two threads are separated from each other by the thread boundary . This thread boundary enables the E Scheduler to operate continuously including dispatching work to the multiple executors and coordinating their operations even if the application does not make more API calls to the runtime system for an extended period of time. Different thread assignments of components of the runtime system are also possible. For example the Program Generator can also run in its own thread.

In some embodiments different components of the runtime system run in different processes. The lifetimes of these processes may be independent of the lifetime of the application using the runtime system . In some embodiments one or more of these processes may service one or more independent applications at one moment.

Similarly there are one or more GPU and or CPU executor threads associated with the respective GPU executors and CPU executors . In some embodiments each executor runs in one or more threads of its own. In some other embodiments multiple executors share the same thread. In some embodiments as a highly asynchronous device coprocessor may temporarily block its associated executor module from access when its input buffer backs up. With a thread boundary separating the executor s thread from the E Scheduler thread the communication between the executor and the E Scheduler is not affected by the input buffer backup at the coprocessor. This multi thread configuration can effectively increase the runtime system s total throughput.

Note that the location and existence of the thread boundaries shown in are illustrative. In some embodiments the runtime system operates without any thread boundaries. Likewise in some other embodiments an application may run in multiple threads each calling into the runtime system independently.

In some embodiments the runtime system includes optional thread boundaries between the LSI FE and the C Scheduler between the C Scheduler and the ProgGen between the C Scheduler and the E Scheduler and between the E Scheduler and each executor . As described elsewhere in the application in other embodiments the different components of the runtime system run on separate processing elements or separate computers connected via a network. At least a subset of the components run asynchronously. In this case communication mechanisms known in the art are employed to ensure that information exchanged between the components is processed in an appropriate manner.

Communication across these thread boundaries is accomplished via invoking function calls that are defined in a function call interface. When the thread boundaries are present the interface arguments to one of these function calls is packed into a message and the message is put into a message queue that exists for each thread. When the thread boundaries are present a thread except the application s thread waits on its message queue for messages. Upon receipt of a new message the thread determines the message type unpacks the interface arguments and performs operations defined in the message.

In some embodiments a message queue is a data structure that includes a list of reference counted messages a mutual exclusive lock also known as mutex and or a condition variable that prevent simultaneous access to the message queue structure. The mutex and a condition variable are configured to allow a thread associated with the message queue to be activated when a message arrives. Messages can be placed in the message queue by any thread in the runtime system and then are removed by the receiving thread in a predefined order e.g. sequentially . In some embodiments a message queue may be optimized to batch up messages and thereby reduce the contention for the mutex and the condition variable. In some embodiments the message queue may accept messages having different priorities and deliver messages of higher priorities first. In some embodiments a message sending thread defers submission of any new message until a previously submitted message has been processed. This configuration is useful to implement a synchronous function call interface if a component of the runtime system blocks waiting on results of some operations.

Throughout this application it will be appreciated by one skilled in the art that function calls and message queues are interchangeable where communication is implied between components that have an optional thread boundary.

Within a multi thread runtime system there is a possibility that a first thread may produce work or allocate resources at a rate faster than a second thread that consumes the work or de allocate the resources. When this situation occurs a flow control mechanism is often employed to cause the first thread to temporarily stop or slow down until the second thread catches up the pace of the first thread. In some embodiments this mechanism is not only useful for improving the overall performance of the runtime system but also helpful to prevent an application from mal functioning.

In some embodiments a flow control mechanism is implemented at the thread boundary between the C Scheduler and the E Scheduler . The C Scheduler is responsible for allocating buffers in the main CPU memory for use by the E Scheduler and the executors . Sometimes the C Scheduler may fail to allocate one or more buffers because of temporary depletion of the main CPU memory. If so the C Scheduler may send messages to the E Scheduler repeatedly requesting that the E Scheduler report back to the C Scheduler after it finishes a task that may cause release of some the main CPU memory . In some embodiments the C Scheduler may return an error to the application if the E Scheduler fails to process a sufficient amount of task after a predefined time period.

There are other reasons for controlling the work flow between the C Scheduler and the E Scheduler even if there is sufficient main CPU memory for allocating buffers related to a compiled program sequence. For example another flow control mechanism is to have the C Scheduler and the E Scheduler share a set of atomic counters so as to prevent the outstanding work at the E Scheduler from exceeding a predefined threshold and therefore improve the E Scheduler s efficiency. Advantageously the flow control mechanism also provides the C Scheduler more opportunities to optimize the work to be submitted to the E Scheduler . In some embodiments these atomic counters are functions of the number of outstanding program sequences the number of outstanding operations the time required to perform these operations and other parameters that indicate system resources used or to be used by the outstanding program sequences.

In some embodiments before submitting a new program sequence to the E Scheduler the C Scheduler checks whether the addition of the new program sequence to the outstanding ones would exceed a predefined threshold for one or more of the atomic counters. If so the C Scheduler suspends the submission until the E Scheduler reports back that a sufficient amount of work has been processed such that the atomic counters are below the predefined threshold. The C Scheduler then atomically increments the atomic counters by the numbers determined by the new work it submits to the E Scheduler . The E Scheduler decrements the atomic counters in response to a notification from the executors with regard to the amount of work that has been completed and or when the E Scheduler releases resources it has used.

In some embodiments a flow control mechanism is implemented at the thread boundary between the E Scheduler and each executor in the runtime system . For example the E Scheduler and the executor may share a set of counters that indicates resource consumption and the workload at the executor . In some embodiments the E Scheduler uses these counters in its decision as to which executor to send work to.

In some embodiments the runtime system does not allocate memory space for a data array generated by operations in the application until the C Scheduler is ready to send compiled program sequences associated with these operations to the E Scheduler for execution.

In some embodiments as shown in the C Scheduler allocates space in the main system memory associated with the main CPU to host the data array before submitting any program sequences to the E Scheduler . If the CPU is chosen to execute the program sequences the main system memory is used directly to store the result from executing the program sequences. But if a GPU is chosen to execute the program sequences the corresponding GPU memory is typically used to store the result from executing the program sequences.

In some embodiments the main system memory can be used as a temporary backup storage by transferring data from the GPU memory to the main system memory in the event that the GPU memory does not have sufficient space. In some embodiments the GPU executor uses the main system memory to store data generated by the GPU so that the data can be accessed by the rest of the application .

In some embodiments if the memory consumed by operations currently being performed by the runtime system exceeds a predefined threshold level the runtime system may refrain from any further memory allocation. In this case the C Scheduler waits for the runtime system to complete at least some currently running operations by sending a wait request to the E Scheduler . The C Scheduler keeps waiting until the wait request is acknowledged by the E Scheduler e.g. when the E Scheduler completes at least some of the currently running operations or is ready to handle more work.

In some embodiments the C Scheduler may not be able to allocate the main system memory to store the result from executing a program sequence even after the E Scheduler has completed all the other outstanding operations. For example this situation may occur if the program sequence requires more memory space than is available. If so the C Scheduler may choose to re invoke the ProgGen to reduce the size of the program sequence or the amount of memory requested by the program sequence. Alternatively the C Scheduler can mark the result from executing the program sequence as invalid . If the invalid result is ever passed to the application through the FE or the LSI they may generate an error message to the application.

In some embodiments the runtime system and the application exchange data by sharing a portion of the memory space. The application may add a flag to a write operation to request that the runtime system takes over a data buffer owned by the application . As a result the runtime system can read and write the data buffer directly without having to copy data from the data buffer into its own memory space. Alternatively the application may include another flag in the write operation that allows the runtime system to share the data buffer with the application for a predefined time period or indefinitely. In this case the application and the runtime system have read only access to the data buffer until either party releases its read only access right. After that the other party may have write access to the data buffer. In some embodiments different write API calls are used rather than specifying the different behaviors with a flag parameter.

In some embodiments the application can make read API calls to the runtime system to retrieve result data from the runtime system . A special instance of the read API call allows the application to access the result data stored in an internal buffer in the runtime system s memory space without copying the data back to the application s data buffer. In this case neither the application nor the runtime system can modify the internal buffer as long as both parties have references to the internal buffer. But the runtime system can regain its full control over the internal buffer after the application releases its references to the internal buffer. In some embodiments the application may get read only access to the internal buffer by including a special flag in the read call.

In the preceding description the LSI FE C Scheduler ProgGen E Scheduler executors and their subcomponents are all members of the runtime system . In particular the LSI FE C Scheduler and ProgGen work in concert to perform functions such as progressive evaluation and dynamic program generation and compilation.

The following is a detailed description of some of the key components of the runtime system using exemplary data structures algorithms and code segments.

The LSI is responsible for interpreting API calls in the application to perform a variety of operations including but not limited to the following 

In some embodiments for each API call the LSI generates one or more objects each object including a handle pointing to a port returned by the FE . A port is an object generated by the FE that represents data or operations to be performed by the runtime system . The ports and their subsidiary objects and fields comprise the IR nodes described above. As shown in besides a handle an LSI object may include several other fields such as data type and operation type of the corresponding port as well as allocation type indicating how the LSI object is allocated. In some embodiments some of the fields in the LSI object can be used by a debugging tool to detect a misuse of the LSI object and determine the LSI object s scope.

A handle in an LSI object is used for separating the data processed by the runtime system from the data directly visible to the application . In some embodiments multiple LSI objects may contain the same handle and therefore correspond to the same data or operation object in the runtime system . A handle may have a counter field tracking the number of LSI objects that contain this handle.

There are several advantages from having the handle inside the LSI object . First the handle provides compatibility between different programming languages. In some embodiments the objects e.g. ports within the runtime system are implemented as C objects. The handle provides an easy and secure access to the objects for applications written in other programming languages. Because the handle insulates any object pointers used internally by the runtime system from being exposed to the application it is less likely for the runtime system to be adversely affected by an accidental or intentional error or mal function in the application .

Second the use of the handle in the LSI object makes it easier for the runtime system to support binary compatibility between releases. To maintain binary compatibility the new release of the runtime system is designed such that there is no change to the signatures of any API function calls that are supported by old releases of the runtime system . The existence of the handle in the LSI object ensures that the objects used internally by any release of the runtime system are opaque to the application . The application cannot directly manipulate the contents of the internal objects. In some embodiments the runtime system also reserves additional storage for the internal objects that can accommodate potential future needs.

In some embodiments the LSI translates the application s operation requests including object creation and object destruction requests as well as other API function calls into data structures understood by the FE and other components of the runtime system. In some embodiments this translation is accomplished by mapping a large number of API calls in the application into a small number of internal function calls that include as function arguments a specification of the operations to perform. In some embodiments the LSI also associates with an operation request other information such as the program counter of the application code that initiates the request. This program counter can be used to provide error handling information. In some embodiments the runtime system uses the program counter to implement its caches or profiling functionality etc.

In some embodiments the LSI includes multiple LSI modules corresponding to different programming languages such as C C Fortran 90 and MatLab. Each LSI module provides an interface between the application and the runtime system . In some embodiments a single LSI module works with multiple programming languages such as C and C . In some embodiments multiple LSI modules work in concert to support an application written in several programming languages.

An LSI module enables an application to access the data and operations of the runtime system using types and functions appropriate for the programming language s that the LSI module supports. In some embodiments the C LSI module provides access to the runtime system s operations such as mathematical functions via C functions via argument type overloading and operator overloading. In some embodiments the C LSI module provides access to the runtime system s data via pointers to objects allocated by the C LSI module and the runtime system s operations via functions that operate on the pointers. In some embodiments compatibility between the C and C LSI modules may be obtained by providing a mechanism that converts between the C pointers and the C objects.

In some embodiments the LSI is responsible for providing error related information to the application when there is a misuse of the runtime system by the application or the runtime system suffers some other type of errors. In some embodiments a default error handler may be used to shut down the application in case of errors. In some other embodiments the application may include one or more error handlers that provide customized error handling behavior. In some embodiments the LSI maintains these error handlers on a stack. The most recently added error handler is invoked first to resolve an error. If this error handler fails to resolve the error the LSI either proceeds to the older error handlers or terminates the application .

In some embodiments the error related information includes a program counter a line number and a filename of the application code that causes the error. For example the LSI captures the program counter of every API call in the application . The runtime system then converts the program counter to a file name and a line number of the application code using operating system specific tools and the application s debugging information. The error related information makes it easy to debug the application code.

In some embodiments the runtime system primarily operates on data arrays which are represented by the LSI as array objects. A particular LSI module may represent multiple data types e.g. 32 bit floating point values 64 bit floating point values or 32 bit integers as either different types of array objects or different types of pointers to array objects or a single type of array objects whose type is checked at run time to determine compatibility of operations. In some embodiments a hybrid approach is used where different data types are represented by the LSI as different objects and or pointer types and a check for compatibility is also performed at run time. For example the distinction between single and double precision floating point values may be distinguished by the objects static types. But whether the values are real or complex may be checked at run time.

In addition to operating on data arrays the LSI may provide access to other functionalities supported by the runtime system . For example the LSI can provide the application access to one or more random number generators RNGs supported by the runtime system including creating destroying RNG objects setting and reading seed data associated the RNG objects and generating random numbers using the RNG objects.

In some embodiments the LSI enables the application to share the main CPU memory with the runtime system at a low cost. In some embodiments the LSI provides functions for performing typical memory allocation operations such as allocating de allocating memory and resizing memory blocks. In some embodiments the runtime system provides interfaces that allow the application to write data into a memory space allocated by the memory allocation system without copying the memory space into or out of the runtime system . The runtime system can access the data in this memory space directly when it is told to do so by the application .

In some embodiments the runtime system does not allocate memory space for data arrays immediately when the LSI allocates new array objects. Rather the memory space for data arrays is allocated when it is needed to store an operation result. Likewise an LSI call to perform an operation may not cause the runtime system to perform that operation immediately. Rather the runtime system accumulates the operation request using a work queue mechanism described below. Using the work queue the runtime system can form dense compute kernels to exploit the capacity of the high performance processors managed by the runtime system . By queuing operation requests the runtime system can use techniques such as progressive evaluation to optimize and execute operation requests more efficiently than processing each operation request individually and or synchronously when the application submits the request.

In some embodiments an LSI module uses language specific facilities such as constructors and destructors to allocate and de allocate LSI objects that represent data and operations processed by the runtime system which simplifies the runtime system s memory management. For example the C LSI module creates an LSI object by calling the object s constructor destroys an LSI object by calling its destructor and duplicates an LSI object by making other function calls. With these function calls into the runtime system the runtime system only performs operations and generates results that are actually required by the application .

In some embodiments the LSI provides automatic or semi automatic object management such as allocation de allocation and duplication using a stack of scopes . A scope acts like a container for a set of allocated LSI objects. When a new scope is inserted into the scope stack the new LSI objects associated with the scope are allocated in a predefined order. When a scope is removed from the scope stack the objects associated with that scope are de allocated in the same order or the reverse order thereof.

To facilitate normal program semantics in which data is typically returned from inner level functions to outer level functions of a program LSI objects can be moved from the current scope to the next older scope on the scope stack. When the current scope is de allocated the LSI objects are preserved. This scope stack mechanism can substantially reduce the number of function calls to object management routines e.g. object de allocation .

In some embodiments the runtime system can predict the application s future behavior from one or more hint instructions the application passes through the LSI . For example the application can use the hint instruction hint read to notify the runtime system that the application intends to read a certain data array out of the runtime system in the near future or that the runtime system should start executing operation requests buffered in the work queue immediately. A more detailed description of the hint instructions is provided below in connection with the work queue in .

In some embodiments the LSI enables the application or a debugging utility to access the debugging functionality of the runtime system . The LSI modules may provide interfaces to enable or disable features like reference result generation and to retrieve a reference result array object corresponding to a particular array object. A more detailed description of the reference result generation is provided below in connection with . Likewise the LSI modules may provide interfaces to enable the debugging utility to examine intermediate operation results generated by the runtime system .

In some embodiments an LSI module is packaged as a dynamically loaded library also known as a shared library or a portion thereof. Therefore a new version of the LSI module can replace an existing version without recompiling the application that invokes the LSI module. In some embodiments the LSI module may be statically linked to the application . One or more LSI modules may be combined together in a single dynamically loaded library or static library. Thus an advantage of the LSI is that it allows applications written in different languages to issue operation requests to the same runtime system which executes those requests on underlying processing elements transparently to the application.

The FE of the runtime system transforms operation requests made by the application through the LSI into internal data structures used by other components of the runtime system . In some embodiments the FE is responsible for mapping handles used by the LSI into ports used by the runtime system to represent data and associated operations. The FE uses a function to map a handle to a pointer to a port used internally by the runtime system. The same or a different function is used to map the port pointer back to the handle. In some embodiments the mapping between the handle and the port pointer is achieved mathematically e.g. by XOR ing a port pointer maintained by the runtime system with a constant value to produce a corresponding handle and vice versa. In some other embodiments as shown in the FE keeps a table of active handles and their associated ports. A mapping between a handle and its associated port or ports is achieved through a table lookup. In either configuration the FE can perform additional tasks e.g. checking a handle s validity.

A port may correspond to i an operation requested by the application that has not been performed ii data that results from an operation requested by the application iii data written into the runtime system by the application or iv a data buffer used internally by the runtime system .

In some embodiments as shown in an operation request from the LSI to the FE includes a code segment and one or more input handles associated with the operation. Upon receipt of the request the FE generates one or more ports and associated data structures to represent the output of the operation. The FE then passes the ports to the C Scheduler .

In some embodiments the FE attempts to optimize operations and their associated inputs before passing them on to the C Scheduler . For example the FE may eliminate an operation through constant folding if all operands of the operation are constants. Some operations with at least one constant operand can be simplified through strength reduction. For example pow x 2 can be converted to x x. The benefit of performing these optimizations at the FE is to ensure that the constant values used to perform the optimizations are represented in the trace cache key . In some embodiments the constant values are part of the trace cache key to ensure that the compute kernel s generated for the optimized operations are not only valid for those particular values but also more efficient. In some other embodiments values in the trace cache key that change frequently may cause significant program generation and compilation overhead. Therefore scalar values passed to the LSI by the application are not included in the trace cache key because the runtime system does not know whether they are literal constants runtime constants or variables in the application .

In some embodiments the FE manages input and output operations between the application and the runtime system . For a write operation request from the application the FE creates a new port to represent the data to be transferred from the application s memory space to the runtime system s memory space. The FE then notifies the C Scheduler which in turn notifies the E Scheduler causing the data to be written into the runtime system from a data buffer held by the application .

For a read operation request from the application the FE notifies the C Scheduler to prepare the data to be transferred back from the runtime system s memory space to the application s memory space by performing one or more operations including i looking up the trace cache for pre compiled program sequences associated with the read operation request ii optionally invoking the ProgGen to generate the necessary program sequences and iii issuing the program sequences to be executed by the E Scheduler . The C Scheduler then waits on behalf of the FE until the E Scheduler returns the data back to a data buffer owned by the application .

In some embodiments as shown in the FE maintains a shared buffer table for managing buffers shared by the runtime system and the application to track the usage of the shared buffers by the read and write operations described above. When a buffer is shared with the application the FE inserts a new entry into the shared buffer table. As long as the entry remains in the table the runtime system will not modify the data in the corresponding shared buffer. In some embodiments the application is also prevented from modifying the same buffer using e.g. the operating system s virtual memory protection mechanisms.

If the runtime system finishes using a shared buffer before the application the FE moves a corresponding entry from the shared buffer table to an application buffer table for tracking memory regions used only by the application . When the application finishes using the buffer the FE releases the corresponding table entry from the application buffer table accordingly. If the application finishes using a shared buffer before the runtime system the FE removes the buffer s corresponding table entry from the shared buffer table . But the buffer remains accessible by the runtime system until the runtime system finishes using the buffer.

In some embodiments the FE maintains a single buffer table to track memory usage by the application and the runtime system . Each entry in the buffer table includes a buffer name that uniquely identifies a memory buffer a buffer location in the memory space and a buffer ownership field indicating that the buffer is owned by the application or the runtime system or both.

In some embodiments the FE provides an interface between the LSI and the rest of the runtime system . A debugger request from the application passes through this interface to access the rest of the runtime system . The FE is configured to channel such request to the C Scheduler to support features like reference result generation and program debugging.

Next the code segment is executed on a parallel processing computer system that has deployed the runtime system . At run time the code segment is dynamically linked to a library of the runtime system that includes the C LSI module which makes a function call to invoke the FE . As shown in the function call includes arguments identifying the array operation type MAP the array element operation type OP ADD and the two input arrays. In some embodiments the runtime system provides a function constantf32 that automatically converts a constant value 1 into a PS Arrayf32 constant data array. The constant data array has the same dimensions as the data array x and its elements all share the same value 1 . The FE in response generates new ports xp yp and p and their associated data structures corresponding to the API call in the source code notifies the C Scheduler of the creation of the new ports and returns handles to the new ports back to the C LSI module. In some embodiments the new ports are stored in a data structure called the work queue .

When the second statement in the code segment is executed on the computer system the runtime system may or may not have the result data i.e. the updated array object x available immediately for the application to access. In some embodiments the runtime system more specifically the C Scheduler and the ProgGen waits until a predefined number of ports have been accumulated in the work queue or another predefined condition is met e.g. the application makes a read API call for the array object x . Once one of the predefined conditions is met the runtime system then performs the application specified operations and returns the result data to the application .

For simplicity the example code segment includes only primitive operations such as add multiply and divide . For example the operation is an overloaded operator that performs an element wise addition between two array objects A and B. The result array object C has the same dimensionality as A and B. A release operation like Release C indicates that the application no longer holds a reference of the array object C. In some embodiments the release operation in the source code is optional because the runtime system may dynamically determine the scope of a data array object and cause its release if the data array object is no longer referenced by the application . Because the data type PS Arrayf32 is exclusively handled by the runtime system executing the code segment causes a series of API calls to one specific module of the LSI which generates and manages a set of LSI objects . As shown in an LSI object includes a handle pointing to a port created by the FE . In some embodiments the LSI also records the position in a sequence of operations at which the application releases its last handle to a particular array object. This information provides an indication to the runtime system about when to start processing ports accumulated in the work queue.

In some embodiments the ports generated by the FE are organized into a DAG. As shown in a DAG may include at least three types of entities 

The DAG is passed from the FE to the C Scheduler . The C Scheduler identifies the ports in the DAG and generates an entry in the work queue for each identified port. As shown in the C Scheduler enters five new entries into the work queue each entry corresponding to an instruction in the application . When a predefined condition is met the C Scheduler invokes the ProgGen to query the work queue and generate compiled program sequences for selected entries in the work queue.

As shown in the work queue is a repository for operation requests from the FE that have yet to be compiled. It maintains information about the operation requests for generating program sequences. In some embodiments the work queue stores the following attributes associated with a stream of operation requests 

In sum the FE maps user friendly inputs to a parse tree data flow output. In some embodiments the FE and the other components of the runtime system are kept in separate dynamically linked libraries DLLs . A DLL can be updated or replaced without recompiling applications that use it. This allows bug patches and backwards compatible upgrades to be distributed without interrupting an application developer s workflow.

After accumulating a sufficient number of entries in the work queue the trace cache lookup and update module is invoked to check the trace cache for previously compiled program sequences that match the current entries in the work queue . If no match is found the module then submits compile requests to the ProgGen and asks the ProgGen to generate new compiled program sequences for the work queue entries. If there is a match in the trace cache the request instantiation and dispatch for the execution module submits the compiled program sequences as part of its execution requests to the E Scheduler for execution.

In some embodiments the request instantiation and dispatch for execution module allocates space in the main system memory which is also referred to as backing store for a new buffer required by a program sequence to be dispatched. The backing store can be used for different purposes. In some embodiments programs that run on single core or multi core CPUs can write result data into the backing store. In some embodiments data requested by the application via a read API call can be copied into the backing store from the GPU buffer that contains the original data. In some embodiments the backing store can be used as an extension to the GPU buffer if the GPU Executor needs space than what is available in the GPU memory.

If an attempt to allocate the backing store fails the request instantiation and dispatch for execution module waits for the GPU Executor to complete some operations and tries again. In some embodiments after a predefined number of failed attempts the C Scheduler requests and receives from the ProgGen a program sequence that requires less memory and repeats the aforementioned procedure with the new program sequence. If ultimately no progress is made the C Scheduler then marks the output ports of the program sequence as invalid . When there is a subsequent operation that attempts to access data associated with the invalid ports the FE causes an error to be delivered to the application via the LSI .

In some embodiments the C Scheduler prevents memory exhaustion and other resource overloads caused by the runtime system by controlling the workflow submitted to the E Scheduler . For example before submitting a compiled program sequence to the E Scheduler the C Scheduler checks whether the E Scheduler currently has the capacity to accept the program sequence. If there is not enough capacity the C Scheduler withholds sending the program sequence and waits until the E Scheduler has the required capacity. After allocating resources for the program sequence the E Scheduler reduces its current capacity which is a parameter shared between the C Scheduler and the E Scheduler . In some embodiments the E Scheduler s capacity is a function of its buffer size the number of program sequences to be processed the number of operations to be performed and the time expected for executing the operations.

In some embodiments the C Scheduler prepares a trace cache key for the current content of the work queue by adding a new entry into a trace cache key accumulator data structure . This trace cache key is used for searching the trace cache for a previously compiled program sequence that match the current content of the work queue. As shown in the trace cache key accumulator contains multiple variable sized key entries. The multiple key entries are concatenated together with the first key entry representing the currently oldest entry in the work queue. The key accumulation is an operation of O N where N is the number of work queue entries being put into the accumulator. For each key entry a small constant amount of work is performed. In some embodiments a key entry in the trace cache key accumulator includes 

Next the C Scheduler checks whether a predefined condition to compile program sequences for the current work queue entries is met . There are one or more heuristic based conditions that may trigger the C Scheduler to start processing the work sequence entries. In some embodiments the C Scheduler may start the process when the application drops its last reference to a port or when receiving a write read access request associated with a port. In some embodiments the C Scheduler may start the process after there are at least a predetermined number of entries in the trace cache key accumulator. In some embodiments entries for particular operations or particular sequences of operations may trigger the C Scheduler to process the entries in the work queue . By deferring the process the runtime system can effectively prioritize its resources for more compute intensive compute kernels to be generated and thereby increase the system s throughput.

After deciding to process the current work queue entries yes the C Scheduler performs a trace cache lookup using the information in the trace cache key accumulator to identify and replay the previously compiled program sequences corresponding to the work queue entries . If there is a matching entry in the trace cache yes the C Scheduler then re instantiates the identified program sequences in the trace cache updates the trace cache and sends the instantiated program sequences to the E Scheduler for execution . A more detailed description of the trace cache lookup is provided below in connection with .

If there is no matching entry in the trace cache no the C Scheduler invokes the ProgGen to generate one or more compiled program sequences for the work queue entries . In some embodiments the C Scheduler and the ProgGen share an access to the work queue. In some embodiments the C Scheduler and the ProgGen may pass the work queue or references to it back and forth. In some embodiments the C Scheduler may construct more than one work queue and pass the relevant one s or references to them to the ProgGen . The ProgGen determines the exact number of work queue entries to be processed and generates compiled program sequences accordingly. A more detailed description of the program generation is provided below in connection with . The C Scheduler then inserts the newly compiled program sequences into the trace cache using the information in the trace cache key accumulator and submits the instantiated program sequences to the E Scheduler for execution . A more detailed description of the trace cache insertion is provided below in connection with .

In either case yes or no the C Scheduler performs an update to the work queue and the trace cache key accumulator . In some embodiments the C Scheduler may not be able to process all the entries currently in the work queue through a single trace cache lookup or invocation of the ProgGen because of e.g. the limited capacity of the E Scheduler and or the executors . If so the C Scheduler only eliminates from the work queue and the trace cache key accumulator the entries that have been processed and moves its pointer to a new beginning entry in the work queue. In some embodiments the ProgGen may distinguish between the number of entries of the work queue for which it generated a compiled program sequence and the number of entries in the trace cache key accumulator that are used for the key of the program sequence. This configuration enables the ProgGen to use information from release entries in the work queue that follow entries for operations it does not wish to compile as part of the program sequence.

Next the C Scheduler determines whether the work queue has a sufficient number of entries that may require a new round of processing . In some embodiments the C Scheduler checks whether the remaining work queue entries can trigger a compilation using the same criteria mentioned above . For example if the initial compilation was triggered by a data access request but the work queue entry corresponding to the data access request has not been processed yet the C Scheduler then repeats the aforementioned process until the data access request is met. But if the remaining work queue entries do not satisfy any of the criteria the C Scheduler may return to wait for the new incoming operation requests from the Front End .

Note that the order of operations shown in is only for illustrative purposes. One skilled in the art may find other possible orders that can achieve the same or similar result. For example the C Scheduler may choose to submit the compiled program sequences and then perform an update insertion to the trace cache .

In sum the C Scheduler is responsible for aggregating operation requests to be processed by the ProgGen and caching the compiled program sequences generated by the ProgGen in the trace cache . In some embodiments the C Scheduler also supports the use of a macro cache which bypasses some of the aforementioned procedures such as request aggregation and trace cache lookup. In some embodiments the C Scheduler is at least in part responsible for implementing some features relating to internal result comparison. A more detailed description of the internal result comparison is provided below in connection with .

In some embodiments the invocations from the FE to the C Scheduler are synchronous. In these embodiments there is no asynchronous callback into the C Scheduler . The C Scheduler is further responsible for synchronizing the application with the E Scheduler and through it with the executors . For instance the C Scheduler may block the application to await completion of data requested by a read API call. Alternatively the C Scheduler may throttle the application to reduce the rate of incoming new operation requests in order to avoid significant backup at the executors or exhaustion of main system memory.

As will be explained below the code reuse section between the pair of keywords PS BEGIN MACRO and PS END MACRO in the second set of operations may trigger the C Scheduler to start processing the existing entries in the work queue. Therefore the C Scheduler notifies the ProgGen to start processing the existing work queue entries. This process corresponds to the operations of .

After processing all existing entries in the work queue the C Scheduler starts processing the second set of operations . If this is the first time that the code reuse section is being processed the C Scheduler cannot find a matching entry in the macro cache . Therefore the C Scheduler processes the code reuse section just like its processing of another regular code segment.

In some embodiments the C Scheduler does not use the trace cache key accumulator to generate a key for the macro. Rather the FE or the LSI or both generates a unique macro cache key for the code reuse section. This macro cache key includes an identifier of the code reuse section as well as input output I O and control parameters associated with the code reuse section. By including the I O and control parameters in the macro cache key different executions of the code reuse section can involve only a specific code segment that is consistent with the I O and control parameters. For example the code reuse section in includes an IF ELSE condition block. Depending on the specific value of the control parameter x one execution of the code reuse section may involve only the IF part of the block and another execution may involve the ELSE part of the block. In some embodiments these two executions are associated with two different entries in the macro cache .

At the end of the first execution of the code reuse section the C Scheduler inserts the macro cache key and compiled program sequences into the macro cache . The macro cache key is used for searching the macro cache for a matching entry when the C Scheduler intercepts a subsequent entry to the code reuse section. As shown in the code reuse section is within a for loop the C Scheduler can easily identify and re instantiate the macro cache entry corresponding to the same code reuse section after the first iteration of the for loop.

After processing the second set of operations the C Scheduler starts accumulating new entries in the work queue and the trace cache key accumulator for the third set of operations . The intrinsic operation FFT may correspond to one or multiple compiled program sequences in the trace cache . Finally a data access operation M.read triggers the C Scheduler to stop accumulating more work queue entries and start checking the trace cache or invoking the ProgGen if no match is in the trace cache .

In sum the runtime system is driven by a dynamic stream of operation requests from the application which are then compiled into program sequences executable on processing elements of a parallel processing computer system such as multi core CPUs or GPUs. In some embodiments the runtime system processes the request stream dynamically or just in time . The runtime system does not need to know any information about the application in advance. Because program sequence generation by the ProgGen is a relatively expensive operation the C Scheduler tries to re use the previously compiled program sequence in the trace cache and or the macro cache as much as possible to achieve good performance.

As noted above the trace cache is used for caching and re playing a previously compiled program sequence for a set of operation requests issued by the application . In some embodiments the trace cache implements some or all of the following features to achieve a reliable and efficient result 

Different trace cache keys associated with different trace cache entries within the same bucket may have different numbers of trace cache key entries. The maximum entry length of the bucket indicates the maximum number of key entries of an individual trace cache key within the bucket . In some embodiments this parameter is used by the C Scheduler to first compare the trace cache key in the accumulator against the longest trace cache entry in the bucket if possible. For example if the trace cache key accumulator has four key entries the bucket has first and second trace cache entries whose trace cache keys have four and five key entries respectively and the bucket s maximum entry length is therefore five the C Scheduler does not perform trace cache lookup until a new key entry is appended to the trace cache key accumulator. When there are five key entries in the accumulator the C Scheduler first checks if the five key entries match the trace cache key associated with the second trace cache entry. It does not check the trace cache key associated with the first trace cache entry only if the second trace cache entry is a mismatch.

In some embodiments the trace cache entries within the bucket are arranged such that the most recently inserted trace cache entry is located at the head of the entry list and the least recently inserted trace cache entry is located at the tail of the entry list . After selecting the bucket the C Scheduler starts with the most recent entry and iterates through each one in the entry list until identifying a match or reaching the tail of the entry list . The matching trace cache entry is returned to the C Scheduler as the result of trace cache lookup.

As noted above the trace cache may have a limit on the total number of entries which is a user configurable variable to avoid waste of memory space. In some embodiments the trace cache maintains a list of global least recently used LRU trace cache entries corresponding to different buckets. In some other embodiments each bucket has its own list of LRU entries. These per bucket LRU lists are used to limit the number of entries per bucket to avoid excessive trace cache lookup cost. In some embodiments empty buckets are immediately reclaimed by the runtime system for other purposes. In some other embodiments the trace cache keeps information to track the empty buckets that are not immediately reclaimed by the runtime system .

If a matching bucket is found yes the C Scheduler selects one entry in the bucket and compares its trace cache key against the key entries in the trace cache key accumulator . In some embodiments the C Scheduler starts the key data comparison with the largest trace cache entry in the bucket and if it fails moves to the next largest entry in the list until either a matching trace cache key entry is found yes or the last entry in the bucket has been examined yes .

If a matching trace cache entry is found yes the C Scheduler generates a new executable object for the matching trace cache entry . The executable object is then sent to the E Scheduler for execution. In some embodiments to generate the executable object the C Scheduler identifies a list of inputs and outputs of a program sequence that is part of the trace cache entry. Each of the inputs and outputs corresponds to a port in the work queue that is used to create the program sequence.

Trace cache lookup is an O N operation where N is the number of work queue entries that are being checked for a match. Each trace cache lookup performs a hash lookup to find a bucket that may contain a matching trace cache key and then at most a predefined number of key comparisons. Trace cache lookup also includes a predefined number of constant time operations such as LRU and statistics updates etc.

Insertion of an entry into the trace cache is O . Several constant time operations are performed e.g. reclaiming an expired key and bucket creating a new one . But none of them depends on the size of either the trace cache or the key being inserted.

The operation of the trace cache assumes that i two sets of work queue entries are deemed identical if they satisfy a set of predefined conditions and ii therefore program sequences generated for one set of work queue entries can be reused for the other set. In some embodiments the set of predefined conditions includes one or more of the following 

As noted above the runtime system employs the macro cache to further improve performance especially when handling short arrays. Like the trace cache the macro cache reduces the cost of translating operation requests into program sequences to be executed on a processing element. But unlike the trace cache the macro cache is visible to an application developer. The developer can specify a particular code segment which is also known as a code reuse section in the application to be handled by the macro cache . Therefore the developer has to understand what type of operations in the application can be better handled by the macro cache and choose to use this feature carefully.

When a set of operation requests corresponding to a code reuse section reach the C Scheduler for the first time the C Scheduler invokes the ProgGen to compile the operation requests into program sequences and the program sequences are recorded into the macro cache . Subsequently upon receipt of the same set of operation requests the C Scheduler retrieves the previously compiled program sequences from the macro cache and replays them with a single call to the E Scheduler .

The total number of entries in the macro cache is limited by a user configurable parameter to avoid waste of the runtime system s memory space. In some embodiments the macro cache maintains a global list that tracks the least recently used entries within the macro cache . Each bucket has a per bucket LRU list. When the macro cache reaches within a predefined range of its limit on the total number of entries it starts eliminating some entries in the different LRU lists to leave more room for newly generated macro cache entries.

Upon receipt of a set of operation requests corresponding to a code reuse section the C Scheduler first identifies a macro cache key associated with the code reuse section and then performs a macro cache lookup using the macro cache key to identify a bucket corresponding to the macro cache key.

If this is the first time that the C Scheduler processes the code reuse section no matching bucket can be found in the macro cache no . The C Scheduler then starts a macro recording process including accumulating the work queue entries corresponding to the code reuse section and invoking the ProgGen to compile the accumulated work queue entries when reaching the end of the code reuse section . The ProgGen returns one or more compiled program sequences corresponding to the code reuse section. The C Scheduler is responsible for inserting the compiled program sequences into the macro cache .

To insert a macro cache entry into the macro cache the C Scheduler first checks if the macro cache has reached its capacity limit. If so the C Scheduler eliminates one or more entries from the global list of least recently used entries from the macro cache . Next the C Scheduler checks if the macro cache includes a bucket matching the macro cache key associated with the code reuse section . If no matching bucket is found no the macro cache generates a new bucket and inserts into the new bucket a new macro cache entry . As shown in the new macro cache entry includes the I O and control key parameters associated with the code reuse section and the newly compiled program sequences .

If an existing bucket is identified as the matching bucket yes the C Scheduler then generates a new entry in the bucket to store the newly compiled program sequences. In some embodiments the C Scheduler eliminates one or more entries from the bucket s per bucket list of LRU entries to leave room for the newly created macro cache entry.

If this is not the first time that the C Scheduler processes the code reuse section the C Scheduler should find a bucket matching the macro cache key yes . The C Scheduler then starts a macro replaying process including identifying a macro cache entry in the bucket that has matching inputs identical in terms of size base type kind and if constants constant value and control data updating the global list of LRU entries and per bucket list of LRU entries to identify the matching entry as the most recently used one and instantiating the previously compiled program sequences associated with the matching macro cache entry .

At the end of the macro cache lookup and update the C Scheduler sends an execution request including the newly compiled program sequences from the ProgGen or the previously compiled program sequences from the macro cache to the E Scheduler .

In some embodiments the runtime system does not allow an input variable or control parameter to be modified within the code reuse section to ensure that each occurrence of a code reuse section uses the same matching entry in the macro cache . For example if there is a statement A A 3 before the IF ELSE block in the code reuse section the C Scheduler no longer processes the program segment as a valid code reuse section. Instead the C Scheduler raises a warning signal to the application and processes the program segment like any other regular code segments in the application .

The LSI translates the code reuse section into a code segment . The code segment has its own set of parameters corresponding to the inputs outputs and controls of the code reuse section . For example the element Ins 0 corresponds to the input variable A and the element Ins 1 corresponds to the input variable B . A function call record or replay in the code segment initiates the macro cache lookup which has been described above in connection with . The signature of the function call includes the macro cache key  key as well as the I O and control variables. The C Scheduler uses this information to determine whether there is a matching entry in the macro cache . If no matching entry is found e.g. this is the first occurrence of the code reuse section the C Scheduler processes the code reuse section by accumulating entries in the work queue . The trace cache key accumulator is not invoked during this process because the compiled program sequences are stored in the macro cache not in the trace cache .

Depending on the specific value of the control parameter x the C Scheduler processes either the IF branch or the ELSE branch. A function call finish recording follows the work queue entries accumulation. In some embodiments this function call triggers the ProgGen to generate compiled program sequences for the IF or ELSE branch of the code reuse section. The C Scheduler then inserts the compiled program sequences into the macro cache . Subsequently when the same code reuse section is re executed the runtime system repeats the aforementioned procedures i.e. checking if there is a matching entry in the macro cache for a given value of the control parameter x re instantiating the previously compiled program sequences if there is a matching entry or generating a new entry in the macro cache for another control parameter.

In some embodiments the macro cache implements at least some of the features below to achieve a satisfactory result 

In some embodiments the C Scheduler starts macro caching with a function call MacroBegin . If a code reuse section is currently being recorded the C Scheduler checks the function call is part of a recursion while allowing the existing code reuse section to be recorded. If no code reuse section is currently being recorded the C Scheduler submits the current work queue entries to the ProgGen for compiled program sequences generation and then starts macro recording of the code reuse section.

In some embodiments the trace cache is disabled while a code reuse section is being recorded. For example there is no accumulation of trace cache key entries in the trace cache key accumulator during macro recording and the generated program sequences are not deposited in the trace cache . In some other embodiments the trace cache is enabled during macro recording of a code reuse section such that repeated sequences of operation requests can be recognized and replayed using the trace cache .

In some embodiments read write operations are deemed illegal during the macro recording of a code reuse section. Recursion that starts a code reuse section with the same key inputs and controls is also illegal during the macro recording. The occurrence of any illegal operations during the macro recording causes an error to be reported back the application as described previously.

In some embodiments the C Scheduler performs error checking to verify that only ports that are declared as inputs to a code reuse section and ports that computed from the input ports are used by the operations within a code reuse section. In some embodiments the inputs to a code reuse section cannot be overwritten during the execution of the code reuse section and any outputs from the code reuse section that are not properly declared in the macro specification may cause an error at run time.

In some embodiments constant folding and other specialization utilizing the values of constant inputs to a code reuse section are not allowed while the code reuse section is being recorded.

At the end of macro recording the C Scheduler makes another function call MacroEnd . This function call causes all operations registered in the work queue to be compiled and a macro cache entry to be generated in the macro cache that can be used to replay the code reuse section.

As noted above when a macro cache entry is used to replay a previously recorded code reuse section one or more executable objects are created to perform the previously compiled program sequences associated with the code reuse section. The C Scheduler associates the ports in the executable objects with the new inputs and outputs of the code reuse section being replayed. The C Scheduler also creates temporary ports for temporary values used by the program sequences and attaches the temporary ports to the executable objects. The new executable objects are then submitted to the E Scheduler for execution.

As noted above random number generators can be used inside a code reuse section for generating random numbers. In some embodiments the macro cache supports updating a random number generator s seed data by recording all uses of the random number generator inside the code reuse section so that each successive invocation of the random number generator produces different values.

In some embodiments if a code reuse section uses a random number generator that produces seed data to be used as an input to the code reuse section the macro cache records references to the random number generator s seed data ports used by the random number generator. Likewise function calls to the random number generator inside the code reuse section are recorded. Before a macro cache entry corresponding to the code reuse section is replayed the random number generator is invoked to produce new seed data and update its internal seed state. The new seed data is used as inputs to the executable objects that are executed as part of replaying the macro cache entry.

In some embodiments if a code reuse section uses a random number generator that produces new seed data as a by product of program sequences that are executed when replaying a macro cache entry corresponding to the code reuse section the macro cache records references to the random number generator s seed data ports produced by these program sequences during the initial macro recording. Likewise function calls to the random number generator inside the code reuse section are recorded. After the macro cache entry s associated executable objects are created the random number generator is invoked to update its internal seed state.

Compared with the trace cache the macro cache may dramatically improve the runtime system s performance of executing operations on small arrays. In some embodiments there are fixed and variable costs associated with performing operations using the runtime system . Under normal transparent use of the API the fixed cost includes but is not limited to the cost of calling functions in the LSI for each operation constructing the corresponding IR nodes and verifying correct API usage in the FE entering the IR nodes into the work queue in the C Scheduler trace cache key accumulation and trace cache lookup . The fixed cost is independent from the size of the data arrays being processed but is proportional to the number of calls to the LSI . By contrast the variable cost is a function of the size of the data arrays being processed and the cost for processing a long data array is usually higher than the cost for processing a short data array. In some embodiments the variable cost is dominated by the cost of executing the compute kernels in the program sequence and movement of data. Therefore the fixed cost often dominates the cost for processing a short data array using the trace cache . Because the macro cache circumvents the calls to the LSI contained within a code reuse section in the case of a macro cache hit it can significantly reduce the fixed cost during the replay of a macro cache entry and therefore achieve a better performance when processing short arrays.

In some embodiments an LSI module provides language specific syntactic markers used to identify code reuse sections within an application. At run time the functions provided by the LSI module performs error checking and argument validation before invoking the FE to require that the code reuse section be replayed or recorded by the runtime system .

In some embodiments a static preprocessor or compiler may automatically insert macros around code reuse sections if the tool deems it is desired to do so.

In some embodiments the LSI the FE and the C Scheduler may detect and report an error when the application performs one or more operations prohibited from being within a code reuse section.

In some embodiments the LSI modules for different languages may be interoperable. For example the macro cache implementation for a particular LSI module may support passing objects of all interoperating LSI modules into a particular code reuse section. The C LSI supports this feature by using overloaded functions to coerce C object pointers and C objects into a common representation used for building the code reuse section s input and output description arrays.

As noted above the C Scheduler invokes the ProgGen to generate compiled program sequences when necessary such as on a cache miss in the trace cache or macro cache .

In some embodiments the ProgGen is responsible for determining whether there are a sufficient number of operations in the work queue to generate an efficient program sequence and if so which operations from the work queue should be performed by the program sequence. In some embodiments the ProgGen is also responsible for generating the executable program sequence and sets of input and output ports for operations in the work queue that correspond to the inputs and outputs operated on by the program sequence. In some embodiments the ProgGen is also responsible for determining which work queue entries the C Scheduler should remove from the work queue after the ProgGen returns the program sequence and which work queue entries the C Scheduler should include in the trace cache key if the program sequence is to be inserted into the trace cache .

During the construction of a program sequence the ProgGen is configured to determine which operations to include in the program sequence to choose the most efficient kinds of execution targets for these operations and to select and or generate optimized compute kernels for the operations. As previously mentioned in some embodiments the ProgGen may generate compute kernels or whole program sequences for multiple kinds of processing elements for the same set of operations so that the kinds of processing elements may be selected dynamically by the C Scheduler and or E Scheduler .

In some embodiments the compute kernel or a program within the program sequence is the smallest executable unit of computation managed by the E Scheduler and Executors . In some embodiments for some kinds of processing elements the compute kernel is executed in parallel SPMD style single program multiple data on the selected processing element s of the parallel processing system. In some embodiments for some kinds of processing elements the compute kernel may perform vector operations which compute multiple result elements in parallel. In some embodiments for some kinds of processing elements both forms of parallel execution may be employed by the same compute kernel. In some embodiments for certain kinds of processing elements other forms of parallel execution may be used to compute multiple result elements in parallel. In some embodiments for some kinds of processing elements the compute kernel may contain loops to iterate over many data elements or groups of data elements.

Note that conceptually and on some processing elements even in practice each result element computed by a compute kernel is computed simultaneously in parallel. In some embodiments for some processing elements synchronization of all processing elements executing the compute kernel may not be feasible or even possible within the kernel but only between kernel executions. Without synchronization the correct ordering of stores of result elements by one processing element and loads of the elements by another processing element cannot be guaranteed in a parallel processing system.

In order to generate efficient compute kernels from a sequence of operations the ProgGen must determine which operations may be executed together as part of the same compute kernels. The transformations used to form compute kernels are similar to loop fusion and scalar replacement. To fuse operations into the same compute kernel the ProgGen generates code into the body of the compute kernel that implements the fused operations element wise one element or one vector of elements at a time such that the result s of a fused operation may be passed directly to a consuming operation fused into the same kernel using the most efficient means possible on the target architecture e.g. using registers without first storing the results to an array and then subsequently loading them back again. In fact the stores of results may be omitted entirely in the case that all consumers of a result are fused into the same kernel as the operation that generates the result. It is also possible to eliminate redundant calculations within a compute kernel by common sub expression elimination. On the other hand in some embodiments some operations may be computed by more than one compute kernel where the ProgGen decides that recomputing the results of the operations would be more efficient than storing them and reloading them.

In some embodiments attention is given to formation of large compute kernels containing many operations. Executing fewer larger kernels usually incurs less overhead than executing more smaller kernels since on many target architectures there is synchronization and other overhead associated with launching compute kernels. Another important factor is the reduction in memory accesses by scalar replacement. On modern processors the ratio of the amount of memory bandwidth to computational throughput is typically very low. Thus the more computation that can be done per memory access the more efficiently the compute kernel is likely to be. Automatic kernel synthesis by the runtime system both saves the application developer a great deal of effort and allows the system to optimize performance for new processing elements without changes to the application.

In some embodiments some primitive operations in the primitive libraries for the chosen type of processing element can be fusible into a compute kernel. In some embodiments there may be multiple primitive libraries one for each type of processing element. In some embodiments primitive operations are represented as source code of a high level language such as C or High Level Shading Language HLSL . In some embodiments primitive operations are represented as assembly code. In some embodiments primitive operations are represented using a lower level intermediate representation which is capable of representing implementation details of the primitive operations. In some embodiments implementations of primitive operations are not stored directly in the primitive libraries but are generated on demand. The type of representation of the primitive operations determines what type of processing is required in order to produce the final executable binary for the compute kernel. Depending on the representation it might require compilation code generation register allocation and scheduling assembly and or linking.

In addition to being implemented as primitive operations for the chosen type of processing element in order for two operations to be fused into the same compute kernel the ProgGen needs to select the same type of processing element for both operations. Moreover where one operation produces the input of another the second operation may only consume the results of the first computed at the same array positions meaning that the operations may be composed element wise. In order to achieve this the ProgGen in some embodiments determines at kernel generation time which result elements of the producer operation are consumed by which elements of the consumer operation such as in the case of simple element wise operations like ADD and MULTIPLY. Where this determination cannot be made such as with a gather of elements from run time computed locations the operations cannot be fused.

In some embodiments the ProgGen may not be capable of performing the index transformations necessary for fusion of certain operations. For example to fuse a transpose operation with the operations producing the array to be transposed either the producing operations i.e. the entire sub graph of operations feeding the transpose up to loads from memory or the consuming operations i.e. the entire sub graph of operations consuming the result of the transpose down to stores to memory must be transposed. In some embodiments the results of operations fused into the same compute kernel may be required to be the same size shape and type. In some embodiments the sizes shapes and types of the results only need to match where they need to be written to memory. In some embodiments computations of certain elements may be duplicated and or guarded by conditionals including predicates as needed in order to facilitate fusion with operations computing different numbers of elements. For example operations computing a scalar result could be fused with an operation that multiplies that scalar by a vector thereby producing a vector. For some types of processing elements the computations generating the scalar result could be duplicated for each processing element.

In some embodiments for some types of processing elements portions of operations computing recurrences such as reductions and parallel prefix scans may be fused with producers and or consumers since such operations have statically determinable correspondences between input elements and output elements.

In some embodiments independent computations with the same algorithmic structure may be fused. For example different independent reductions of the same sized arrays may be fused so that they are computed simultaneously. This is most advantageous when both operations share some or all of the same input arrays and same data access patterns since the number of accesses to memory may then be reduced but may also yield benefits on some target architectures where the overhead of launching kernels is high by reducing the number of kernels.

In some embodiments the ProgGen may choose to place independent operations into separate compute kernels so that those kernels may be executed in a task parallel manner.

In some embodiments some target architecture s may have resource limitations that cannot be efficiently mitigated by software thus imposing restrictions on the compute kernels that can be generated. For example there may be limits on the numbers of instructions that can comprise a compute kernel the number of hardware registers of different kinds that can be used by the instructions the number of input and or output arrays that can be accessed by the kernel the number of arguments to the kernel and or the amount of memory that can be accessed by the kernel. In such embodiments the ProgGen estimates the amount of resources the kernel will use as it constructs the kernel and ensures the limits required for correct operation are not exceeded by the completed executable compute kernel.

In some embodiments the ProgGen may put operations into separate compute kernels for other processor specific reasons such as to impose a global barrier across all hardware threads or to change the number of active threads.

In some embodiments not all operations may be fusible for every type of processing element. There is no requirement that the same operations be fusible for every type of processing element supported by the runtime system . In some embodiments such non fusible operations may be implemented differently from the primitive operations. These operations called intrinsic operations need not be constrained by all of the same restrictions as primitive operations. Intrinsic operations may be implemented using any number of hand coded compute kernels which are stored in the intrinsic library . In some embodiments for some kinds of processing elements multiple kernels may be required in order to implement certain operations. This is why the ProgGen can return program sequences rather than just individual compute kernels.

In some embodiments there may be multiple intrinsic libraries one for each type of processing element. Each intrinsic may have an associated custom hand coded routine to be invoked by the ProgGen that selects the compute kernels to use for a particular intrinsic operation specifies how to pass data between the kernels decides what temporary memory is needed to hold intermediate results determines the launch specifications for the kernels and so on. In some embodiments these hand coded kernels may be written in device specific high level languages such as HLSL. In some embodiments they may be written in assembly language. In some embodiments both kinds of compute kernels may be supported for intrinsic operations. In some embodiments compute kernels for intrinsic operations are stored in executable binary form in which case they just need to be loaded on demand by the runtime system .

In some embodiments for some target architectures the implementations of certain operations may be comprised of multiple parts where some parts are fusible and some parts are non fusible and or different fusible parts have different fusion criteria. For example the first step of a multi step reduction algorithm may in some cases be fused with the producers of the input to the operation the last step may be fusible with consumers of the result of the reduction and intermediate steps as well as the first and last of the algorithm could be fused with other similar operations.

For a specific set of work queue entries the ProgGen is responsible for choosing a processing element to execute the programs corresponding to the work queue entries. If the ProgGen determines that one or more CPUs are a preferred choice for the set of work queue entries it generates either CPU based source code or interpreter operations . In some embodiments a set of CPU based tools such as preprocessor compiler assembler and linker are invoked to convert the CPU based source code into the CPU binary code . Depending on the specific source code and the CPU architecture some of the CPU based tools such as the preprocessor and linker are optional in the conversion of the source into the binary code.

If the ProgGen chooses one or more GPUs to handle the set of work queue entries the ProgGen then generates the GPU source code for the work queue entries . The GPU compiler and GPU assembler are invoked to generate the GPU assembly code and the GPU binary code respectively. Finally the ProgGen combines the CPU binary code the interpreter operations and the GPU binary code into a compiled program sequence which is sent to the E Scheduler for execution .

In some embodiments the primitive and intrinsic operations for which the ProgGen can generate compiled program sequences include without limitation map element wise operations such as add multiply sine cosine etc. reduction in one or multiple dimensions generators random number generation index identity spread replicate an element row or column to create a higher dimensional array transpose block copy periodic copy gather matrix multiply convolution correlation LU decomposition LU solver LU condition LU unpack FFT and sparse matrix vector multiplication.

The ProgGen first makes a quick decision about whether to generate a program sequence from the current contents of the work queue or not. In some embodiments this decision may be based simply on the number of entries in the work queue.

In some embodiments the ProgGen checks whether the leading entry in the work queue is a fusible operation or not . If the entry is not fusible there is little benefit from including the entry in a large program sequence. Accordingly the ProgGen generates a compiled program sequence including this entry and other associated entries. For this reason in some embodiments the entry of a non fusible operation into an empty work queue may cause the C Scheduler to invoke the ProgGen to start processing the current work queue entries.

In some embodiments the program sequence generated by the ProgGen is a list of executable programs aka kernels and associated information needed to allocate memory for output and temporary buffers bind actual buffers and constants to formal program input and output parameters dispatch the programs to executors enforce dependences between programs execute the SPMD single program multiple data programs in parallel on the desired target architecture s and accumulate execution data for the profiler corresponding to the original API calls. In some embodiments it may be convenient and more efficient to store this information in more than one form and or in target specific ready to use forms. For example executable instructions and read only data would typically be loaded into memory accessible to the processing element s .

In some embodiments each program additionally contains execution data and or other information for the profiler or handles to such. For example a program could contain a list of the return addresses of calls into the API corresponding to the operations that comprise the program which with ordinary debugging information could be used to identify the lines of the application executed to form the program. The program sequence additionally contains descriptions of any temporary buffers e.g. size or data type and dimensions that need to be allocated for storage of intermediate results within or between programs of the sequence. Temporary buffers are used only within the span of a single program sequence. In some embodiments the program sequence may contain descriptions of input constants and buffers and or output buffers as well. These input constants and buffers and output buffers correspond to ports associated with computations in the work queue.

The ProgGen returns enough information to the C Scheduler so that it may infer this correspondence. In some embodiments the ProgGen may return explicit mappings from the formal parameter identifiers to input and output ports. In some embodiments the ProgGen may return these mappings in terms of work queue entries e.g. the 2nd input to the operation in the 5th work queue entry . In some embodiments the mapping of work queue entries and or macro inputs and outputs may be stored as part of the program sequence . In some embodiments these mappings may be stored in their respective cache entries.

In some embodiments the ProgGen also decides the formats and or layouts of the output and temporary buffers the specifications of which are attached to the descriptions of the output and temporary buffers. The format and or layout may include for example the amount of column and or row padding or the data tiling pattern. In some embodiments these buffer descriptions may be used by the runtime system to determine the amount of memory required for the buffers and to specify how the array s elements must be indexed by software and or by hardware. For instance the column length summed with the amount of padding at the end of each column is necessary in order to compute linear indices of two dimensional arrays with column major layout.

As an example many GPUs provide hardware support for indexing into multi dimensional arrays with a variety of linear and tiled data layouts. Due to the effects of these data layouts on memory addresses of elements accessed in particular sequences and their interaction with the GPUs memory controllers and caches certain layouts yield significantly better performance than other layouts for certain access patterns. It may in some cases be desirable to reinterpret the layouts of buffers for example writing them using one layout and reading them using another thereby allowing different programs to access the same data with different indexing schemes. In some embodiments this can be done by specifying buffer layouts on a per program basis in the program sequence for cases where buffers should be interpreted differently from their natural layouts. The GPU Executor can then use this information to modify the GPU s hardware buffer layout settings as necessary.

The launch specification includes whatever other information or arguments in addition to the inputs and outputs are needed to launch the SPMD data parallel program on the processing element s . For example it may include the number of processors cores and or threads to use to execute the program. Alternatively it may specify the number of work items to be executed in parallel rather than the amount of hardware resources to apply. In some embodiments it may specify other required resources such as amount of on chip memory needed. In some embodiments the launch specification could also include arguments derived from parameters of the operations to be performed such as sizes and offsets of sub arrays to be copied.

In some embodiments the E Scheduler selects among executors of the same kind for executing the program sequence. The metrics employed by the E Scheduler in choosing a processing element may include for example the ProgGen s estimate of the cost for executing a specific instance of program sequence on the processing element s the location of the input data to the instance of the program sequence the desired location of the output data produced by the instance of the program sequence and the amount of outstand workload at the processing element. In some embodiments these cost estimates may be recorded in the program sequence . In other embodiments they may be returned by the ProgGen separately.

In some embodiments the ProgGen generates multiple compiled programs or even whole program sequences for the same sequence of operations each targeting a different processor type. The C Scheduler and or the E Scheduler choose to execute one of the multiple compiled program sequences on a particular type of processing element based on several factors including but not limited to processor load processor failure faults processor memory load and contents and system configuration. The E Scheduler then submits the chosen instance to the corresponding executor s for execution. This configuration can dramatically enhance the runtime system s performance reliability and fault recoverability.

If the ProgGen decides to process the work queue entries because a predefined condition is met yes the ProgGen performs a backward analysis over the work queue entries which identifies consumer entries that accept as input the results from producer entries propagates information about the consumer entries to the producer entries determines the locations of release entries in the work queue corresponding to results of earlier compute entries and identifies the dead operations i.e. operations whose results are released without being consumed by other operations . In some embodiments the identification of dead operations may be omitted under the assumption that the user would not request unnecessary operations to be performed. This analysis which is similar to live variables analysis and def use analysis is essential to the kernel formation process.

In some embodiments the ProgGen may choose to only process a subset of the current work queue entries based on the certain heuristics. The remaining work queue entries are left for the next compilation request. In some embodiments if the leading work queue entry is fusible the ProgGen then selects a specific number of work queue entries to process during this invocation . The ProgGen selects the set of work queue entries based on deterministic properties of the entries e.g. the numbers and types of operations data array sizes patterns of call addresses or return addresses of calls into the runtime system and hints from the application s developer. In some embodiments the ProgGen considers the loop boundaries in the application the predicted memory usage by the program sequence generated from the selected work queue entries and the estimated number of instructions in the program sequence.

In some embodiments the ProgGen identifies the loop boundaries in the application through analysis of API call return addresses repeated addresses may indicate that the operations are contained within a loop. The predicted memory usage by the operations is based on the sizes of arrays read by work queue operations that are not computed within the current work queue sizes of arrays computed that are not released and sizes of arrays computed by non fusible operations all of which always require buffers to be allocated to store their data. Additional heuristics are necessary to approximate memory consumed by released arrays identified during the backward pass computed by fusible operations since once they are fused not all of those results need to be stored to memory. In some embodiments the sizes of such arrays could be multiplied by estimates of the likelihood that they would be instantiated in memory. The number of instructions is estimated by querying the primitive libraries for the instruction count of each operation performed by the program sequence.

The ProgGen pays careful attention to the loop boundaries inferred from the sequence of API calls from the application because the loop boundaries provide hints about the structure of the computation and where good program boundaries might be they also provide hints about what operation sequences the application will generate in the future. The former is important to the formation of efficient compute kernels while the latter is important for choosing where trace cache keys will begin. Loop boundaries are good candidate locations for program boundaries because applications will typically have few ports live at such points. Loop boundaries are good locations for trace cache keys to begin because future iterations will start at the same PC which is a requirement for reuse via the Trace Cache . The ProgGen takes memory usage into consideration in order to ensure that all output and temporary buffers of any program sequence may be allocated memory up front prior to execution of any programs in the sequence. The instruction count is merely a proxy for the length of the program sequence. It is undesirable for a program sequence to be excessively long. It would be best for it to contain just one or a small number of programs. Therefore the ProgGen limits itself to the number of instructions needed to fill a few programs.

In some embodiments the ProgGen does not restrict itself in advance as to which work queue entries it can process during its scheduling pass in which case it must decide how much of the work queue to consume on the fly during the scheduling pass .

In some embodiments the ProgGen distinguishes the number of work queue entries to be compiled and number of work queue entries in the trace cache key accumulator. By doing so the ProgGen can use information about port releases further ahead in the work queue without generating programs for all of the intervening work queue entries. In some embodiments the ProgGen may use information about port releases from the entire work queue in the case that the program sequence is to be part of an entry in the macro cache since the ProgGen knows with certainty that unlike when the trace cache is used all of the program sequences generated from the current work queue will be replayed in the same sequence as they are generated.

In some embodiments after the work queue selection pass the ProgGen performs a forward pass over the selected work queue entries . During this forward pass the ProgGen identifies inputs to and outputs from the program sequence and collects roots for the subsequent scheduling pass. Inputs to the program sequence correspond to ports referenced by operations among the work queue entries chosen for processing that do not have compute entries within the current work queue . Outputs to the program sequence correspond to ports with compute entries among the set of work queue entries to be processed but without release entries among the entries to be included in the trace cache key or in some embodiments anywhere in the work queue in the case of a program sequence to be inserted into the macro cache . The inputs must have been computed by previous program sequences and the outputs may be consumed by subsequent program sequences or may be read by the application. The scheduling roots are operations with no input operands computed within the current set of work queue entries being processed all of the operations inputs if any are inputs to the program sequence .

In some embodiments the ProgGen determines the number of work queue entries to process and their associated inputs and outputs during the scheduling pass on the fly.

In some embodiments the ProgGen chooses the type of processing element for the operations to be processed during this forward pass . In some embodiments the decision may be made earlier such as during the backward pass or later during the scheduling pass any time before each operation is assigned to a particular compute kernel. The ProgGen is free to make the choice based on whatever heuristics it wishes unless there is a separate restraint on the selection of a particular processor or executor type from the C Scheduler . For example the C Scheduler may require use of the CPU Interpreter for the computation of reference results. In some embodiments the ProgGen may assign operations to the most efficient type of processing element available based on data type. For example single precision operations may be assigned to the GPU and double precision operations may be assigned to the CPU. In some embodiments the decision may be based on more sophisticated criteria such as estimates of the efficiency of individual operations on the available targets or predicted memory residency of an operation s inputs.

In some embodiments the ProgGen counts the number of unique operations within the selected set of work queue entries producing values consumed by each operation during this pass. For example if an operation has two inputs but only one is computed by another operation in one of the selected work queue entries then the first operation has a count of one. In other embodiments this count may be computed in any other pass of the ProgGen provided that it is computed prior to the scheduling of any of an operation s producers. This count is used by the scheduling pass to keep track of how many producers of an operation have not yet been scheduled.

The scheduling pass packs operations from the work queue into compute kernels translates the compute kernels into executable binaries and builds the program sequence . Additional details of the scheduling pass are now described with reference to which is a flow chart of the scheduling pass. To maximize the number of operations available for fusion the ProgGen generates programs for all ready non fusible operations before processing any fusible operations. Next the ProgGen builds a program for at least some of the remaining fusible operations . In some embodiments the ProgGen schedules as many operations as possible into the program until none is left for fusion or the size of the program reaches a limit set by a processing element. At that point the ProgGen compiles the program and assembles the program into binary code for a processing element . If the ProgGen succeeds in compilation and assembling yes the compiled program is instantiated and appended to the current program sequence . Otherwise no the ProgGen reduces the number of operations in the program and rolls back its scheduler state . The ProgGen attempts to compile and assemble the downsized program until success. The ProgGen repeats the process by alternating scheduling and code generation for fusible and non fusible operations until all operations are scheduled yes .

In some embodiments for some target architectures certain operations require both fusible and non fusible portions. In this case the ProgGen may coerce the standard alternation between scheduling fusible and non fusible operations to schedule the fusible and non fusible sub operations in a desired order.

The selection of operations to fuse together into a compute kernel can be implemented by any of many different algorithms. In some embodiments the algorithm for fusing primitive operations into compute kernels might be very similar to algorithms used to perform loop fusion. In some embodiments a graph partitioning algorithm could be used to find the minimum cost way of dividing operations between compute kernels. However these schemes have several drawbacks. First they are fairly complex. Second they are relatively costly which is disadvantageous for a dynamic compiler. Third in some embodiments they could potentially require recompiling and or assembling several compute kernels in the event of compilation and or assembly failure for one kernel.

In some embodiments a simpler algorithm similar to instruction scheduling is used to assign operations to compute kernels one kernel at a time. In such a scheme the ProgGen essentially traverses the operation dependence graph in order to find operations eligible for fusion. Operations are eligible to be fused into the compute kernel currently being formed once all of their inputs are guaranteed to have been previously computed by this compute kernel or a previously generated one either in the program sequence currently being created or in a previous one and when other fusibility criteria are met as discussed above.

In some embodiments the ProgGen starts the scheduling pass with operations having no inputs computed within the current work queue which are also known as the scheduling roots which were identified and collect during the forward pass . As operations are scheduled dependences of the operations consuming their results called consumers are satisfied. A non root operation is ready to be scheduled once all of the operations computing its inputs called producers have been scheduled. This scheduling process accomplishes a pass over the live ports in the work queue entries in a topological order which may or may not be the same order in which the ports are inserted into the work queue.

The way this procedure works is as follows. The ProgGen maintains two lists of operations the non fusible ready list and the fusible ready list. Each scheduling root is inserted into one of the lists according to its fusibility. Multi stage operations are enlisted according to the fusibility of their first stages. Then the ProgGen begins its alternation between non fusible and fusible operations. It selects non fusible operations from the non fusible ready queue one at a time and generates and or selects the corresponding compute kernels for each one invoking corresponding hand written kernel selection routines as needed. As each operation is processed the producer count of each consumer of the processed operation is decremented. Once the producer count of any consumer reaches zero that consumer may be appended to one of the two ready lists according to its fusibility. The ProgGen continues this process until the non fusible ready list is empty. In some embodiments the ProgGen may attempt to optimize the order of processing of the non fusible ready list according to a heuristic cost function. In some embodiments the order in which non fusible operations are processed may be irrelevant.

For a non fusible intrinsic operation the code generator invoked by the ProgGen identifies which compute kernels should be loaded and determines their launch specifications. It also adds any temporary buffers required to hold intermediate results to the program sequence . In some embodiments the code generator refers to the selected kernels by name or by number. In some embodiments the intrinsic compute kernels are in an intrinsic libraries . In some embodiments the intrinsic compute kernels are pre compiled. In some embodiments the pre compiled kernel binaries in the intrinsic libraries are compressed and or encrypted. They need to be decompressed and or decrypted when loaded from a computer s hard drive to its memory by the runtime system .

A similar procedure is then followed for the fusible ready list except that the ProgGen attempts to form just a single compute kernel. When an operation is inserted into the fusible ready list or sometime prior to that the ProgGen must create a description of its fusibility criteria. In some embodiments this information may include the type dimensionality and sizes of the result of the operation. In some embodiments it may include the launch specification required by the operation s implementation such as the number of processors threads loop iterations or other units of work required. In some embodiments it may include an encoding of the structure of the algorithm used by the operation. For example in some embodiments element wise and reduction operations might be fusible with like operations but not with each other so each would require a different algorithmic key during fusibility tests. In some embodiments the fusibility criteria would include a specification of whether the operation or for multi stage operations the sub operation is fusible with producers and or consumers. Such a criterion is simplistic but takes advantage of the fact that most primitive operations have element wise access patterns and is sufficient in systems not capable of transforming producer and or consumer operations in order to align element computations.

The ProgGen searches the fusible ready list for the best candidate for starting a compute kernel. In some embodiments the heuristic for selecting the best candidate may consider a number of factors. In some embodiments the number of operations that may be fused with the ready operation may be considered. In some embodiments whether an operation s consumers may be fused with it may factor into the decision. In some embodiments the number of dependent operations may be used to drive the choice. In some embodiments the chosen type of processing element may factor into the decision. In some embodiments the selection may be based on these and or other factors. In some embodiments the fusible ready list may be sorted according to these priorities or may be implemented as a priority queue where the highest priority operation is selected. Once the first operation called the seed is selected subsequent ready operations that are compatible must be tested for fusibility with this operation and selected.

In some embodiments the ProgGen uses heuristics to determine the next operation to schedule. In some embodiments these heuristics may be similar to those guiding the choice of the seed operation. In some embodiments other considerations may be more important. For example operations that have no inputs such as index are actually scheduled as late as possible so that they can be fused with its consumer operations. In some embodiments operations are fusible with some of their producer operations but not others. In this case the ProgGen may prefer to first schedule the non fusible producer operations. In some embodiments the ProgGen fuses operations different iterations of a loop in the application to avoid producing programs that span multiple partial iterations. This configuration can increase the hit rates of the trace cache and the program cache reduce the number of data arrays written to memory and help a user to understand the transformations.

This scheduling process continues until the fusible ready list is emptied none of the operations remaining in the fusible ready list may be fused with those operations comprising the compute kernel under construction none of the operations in the fusible ready list is deemed beneficial to add to the kernel or certain kernel limitations are reached. In some embodiments these kernel limitations may be driven by hardware resource limitations as described above such as the number of instructions. Since the kernel has not yet been compiled and or assembled in some embodiments the ProgGen must estimate the resources used by operations comprising the kernel under construction. For example the ProgGen may use estimates from the primitive libraries of the number of instructions required for each operation to determine whether the hardware instruction limit has been reached or not.

In some embodiments the ProgGen schedules multiple operations into multiple compute kernels simultaneously. In this case new compute kernels with are created on demand in the program sequence when no suitable compute kernel exists. If possible operations are fused into existing compute kernels.

Once all of the operations to comprise a kernel have been selected the ProgGen generates code for the compute kernel. In some embodiments the ProgGen iterates over the scheduled operations in a topological data flow order i.e. the order in which the operations are scheduled such that all producer operations are processed before their corresponding consumer operations and invokes an operation specific target specific code generator for each operation to generate the source code fragments necessary to implement the operation. The fragments may include code for the body of the compute kernel variable declarations function declarations and so on. Additionally the ProgGen inserts loads for reading from arrays that are computed by prior programs or program sequences and stores for writing into arrays that are not released or that are consumed by operations that have not been scheduled. For each result of an operation whose corresponding port has been released that needs to be written to memory a temporary buffer of the appropriate size is added to the program sequence . Any input buffer read is also added to the input list for the kernel and any output buffer written is added to the output list for the kernel. After generating the source code fragments for all operations loads and stores the ProgGen invokes a routine targeting at a specific processor to combine the fragments and finalize the source code for the compute kernel.

In some embodiments the ProgGen has different code generators for different processing elements. The code generator for a particular processing element may be implemented using routines that emit necessary syntactic elements such as parameter and variable declarations. By accumulating source code strings for various program regions independently e.g. variable declaration blocks and loop bodies the source code for the processing element may be generated directly.

In some embodiments the source code for fusible primitive operations is in primitive libraries . The primitive libraries contains the source code for individual routines instruction estimates for the routines and a list of subroutines from the primitive libraries that must also be inserted. When a routine is requested for insertion the primitive libraries checks whether that routine has already been inserted into the current compute kernel and if not checks whether any subroutines need to be inserted. If so the subroutines are inserted followed by the source code for the requested routine. This process is performed recursively for the inserted subroutines.

In some embodiments the instruction estimates are determined offline as the primitive operations are parsed for insertion into the primitive libraries . For each primitive operation a dummy program is constructed that calls the primitive operation. The dummy program is compiled for a processing element and instructions in the program are counted. Overhead instructions are subtracted from the program and the estimated number of instructions is recorded along with the source code in the primitive libraries .

In some embodiments the source code for the primitive operations in the primitive library may be encrypted or otherwise obfuscated such as by XOR ing a particular bit pattern with the source code character strings.

In some embodiments a program cache is associated with the ProgGen . The program cache is used for storing the binary compute kernels or programs generated by of the GPU and or CPU assemblers. If the C Scheduler fails to find a matching program sequence in the trace cache or macro cache it may invoke the ProgGen to search the program cache for programs identical to previously generated programs. The data path shown in is an exemplary invocation of the program cache . One skilled in the art will appreciate that this approach works for different processor architectures including multi core CPU and GPU.

In some embodiments the program cache lookup is accelerated using a program s source code as a first stage lookup key.

In some embodiments the ProgGen invokes the compiler and assembler to perform code generation and compilation for a processing element architecture after the source code is finalized as shown by .

In some embodiments if an initial attempt by the ProgGen to compile or assemble a program sequence for a processing element fails because of e.g. hardware limitations the ProgGen may re generate the program sequence with a set of tighter constraints on code fusion and or program size. In some embodiments the constraints become monotonically tighter during successive invocations to increase the chance of a successful compilation. In some embodiments only the failing program needs to be regenerated and recompiled and or reassembled and the previously compiled and assembled programs in the program sequence may be retained. In some embodiments the operations comprising the failed kernel are split into two kernels using a process different from the original scheduling process such as graph partitioning.

The ProgGen is responsible for determining how a compute kernel within a program sequence should be launched on its processing element s such as the number of processors of the parallel processing system and the number of cores and or threads per processor used for executing the program. In some embodiments the ProgGen is also responsible for determining the workload among different processors threads loop iterations etc. In some embodiments the ProgGen determines the distribution of work and or data into parcels but not the mapping of those parcels to specific hardware resources such as processors or cores which is left to the E Scheduler and or Executors .

In some embodiments the launch specification can be created for individual operations and is generated for each operation prior to fusion. In some embodiments the launch specification is used as part of the criteria to determine whether two operations are fusible. In some embodiments the launch specification is created after a compute kernel is formed so that characteristics of the whole compute kernel such as the total number of instructions may be considered when computing the launch specification.

Referring to the ProgGen accumulates the generated compute kernels as part of a program sequence to be returned to the C Scheduler as they are completed. In some embodiments input and output buffers of the program sequence are determined prior to the scheduling pass . In some embodiments they are determined during the scheduling pass . Temporary buffers are added to the program sequence during the scheduling pass because that is when compute kernels are selected and or generated.

In some embodiments the ProgGen returns a partially constructed program sequence to the C Scheduler and a callback routine or object. The C Scheduler then inserts the partially constructed program sequence into either the trace cache or the macro cache as it normally does and forwards the partially constructed program sequence to the E Scheduler which may in turn forward compute kernels from the program sequence or even the entire program sequence to the appropriate Executors . Afterward the C Scheduler invokes the callback to invoke the ProgGen to complete the program sequence. In some embodiments as the ProgGen completes the compute kernels in the sequence it signals their completion directly to the appropriate Executors . The ProgGen also returns to the C Scheduler the control over the completed program sequence. This configuration allows the generation of a program sequence in parallel with its execution while allowing the ProgGen to operate in the same thread as the C Scheduler . In some other embodiments the ProgGen operates in a different thread from the C Scheduler .

As noted above shows an example pseudo code sequence input to the LSI . shows the graph of the corresponding IR nodes output by the FE . shows the operations inserted into the work queue .

Assume that the C Scheduler invokes the ProgGen after the insertion of Release F entry into the work queue . Initially the ProgGen analyzes the work queue to determine whether it should generate a program sequence for the current work queue entries. In this particular case the first operation in the work queue is fusible. There are only a small number of operations in the work queue and the operations in the work queue have low computational intensity. Therefore the ProgGen may choose not to generate a program sequence and return its decision to the C Scheduler .

Suppose that the next operation request from the application is a read of G. In this situation the C Scheduler invokes the ProgGen again and in some embodiments compels the ProgGen to generate a program sequence for the current contents of the work queue . Thus the ProgGen iterates backwards over the work queue beginning with the last entry . It records the Release F entry at the work queue position 4 in a hash table indexed by port and then creates an entry in the hash table for the port corresponding to G which is computed by operations in the current work queue and is therefore alive. The ProgGen then records the Release C entry in the hash table. Next the ProgGen processes the compute entry for F. Note that F is alive because it is computed by the current work queue and consumed by the operation corresponding to G. Finally the ProgGen processes the compute entry for C which is alive because it is computed by the current work queue and consumed by the operation that computes F.

Following the backward pass the ProgGen analyzes the number of operations in the work queue the memory consumption of the operations and the number of instructions required by the operations . Because arrays C and F are released the ProgGen assumes that they do not require memory allocation. Arrays A B D and E are identified as inputs because they are not determined by the current work queue entries. G is identified as an output because it is not released within the work queue . The total memory space required by the operations is dependent upon the size of arrays A B D E and G. If the ProgGen determines that the amount of required memory space by the program sequence is not excessive it may decide to consume the entire work queue .

In some embodiments the ProgGen iterates forward over the work queue entries . While doing so the ProgGen collects the input ports A B E and D to the program sequence in the order in which they are referenced by the operations in the work queue . In some embodiments the ProgGen also collects the output ports G. To facilitate reuse of the program sequence with different buffers of the size the ProgGen records the inputs and outputs of the program sequence as lists of ports to return to the C Scheduler and as abstract specifications of buffers in the program sequence .

In this case the operation corresponding to C is identified as the scheduling root because it is the only operation in the work queue that does not depend on the result of another operation in the work queue . In some embodiments the ProgGen also chooses processing elements for the program sequence during this forward pass. For illustrative purpose it is assumed that all the operations in the program sequence are to be performed on single precision floating point data and the program sequence is therefore assigned to a GPU.

In some embodiments the ProgGen is also responsible for determining the number of unscheduled producers for each operation during the forward pass. In this example C has none F has one unscheduled producer C and G has one unscheduled producer F .

During the scheduling pass the ProgGen adds the scheduling root C to the fusible ready list and creates its launch specification for the targeted processor or coprocessor i.e. the GPU . In some embodiments the launch specification on the GPU depends on the dimensions of the output array and the GPU computes in terms of four elements at a time. Assuming that the array C has size 100 200 the launch specification therefore specifies a 25 200 pixel rectangle.

In this example because there are neither non fusible operations nor other fusible ready operations C is removed from the fusible ready list and used as the seed for a compute kernel. The producer count of F that consumes C is then decremented to zero. Next F is appended to the fusible ready list removed from the fusible ready list and scheduled in the current compute kernel. The same process then applies to G and other consumer arrays until no schedulable operations remain.

In some embodiments the ProgGen then invokes an operation specific target processor specific code generator for each scheduled operation in the scheduled order i.e. C then F and then G. For illustrative purpose all the operations herein are primitive operations. Therefore for each operation the code generator creates a temporary variable generates a statement including the temporary variable to compute the result and appends the statement to the kernel body. In some embodiments the primitive operations are retrieved from the primitive library the definitions for the primitive operations are accumulated and calls to the primitive operations are generated and appended to the kernel body. After generating the code for all of the operations the ProgGen then adds the main function body and input declarations to the kernel body.

As noted above the FE the C Scheduler and the ProgGen generate a dependency relationship between different program sequences and different programs within the same program sequence. The E Scheduler is responsible for dynamically managing the dependency relationship as it arranges for execution of the program sequences through different executors .

In some embodiments the E Scheduler is responsible for scheduling the execution of program sequences and returning the execution results to the application . For a sequence of programs the E Scheduler is configured to choose one or more executors dispatch programs to the executors move data between the GPU memory spaces and the main CPU s system memory and synchronize the executors to ensure that the programs are executed in a predefined order. In some embodiments the E Scheduler is also partially responsible for implementing some internal result comparison features which may be used in the context of reference result generation and or program debugging. In some embodiments as shown in the E Scheduler operates in its own thread to enable job scheduling even if the other threads of the runtime system are busy.

In some embodiments upon receipt of a request the E Scheduler places the request s outputs in a data structure called the pending operation table each output having a corresponding entry in the pending operation table. If the E Scheduler cannot start an operation immediately e.g. if its input is not ready that operation is suspended and associated with an entry in the pending operation table. The entry corresponds to an output of another operation and that output is the missing input to the suspended operation. When the other operation completes the entry corresponding to its output is then removed from the pending operation table. The suspended operation that has been associated with the output is then retried. A more detailed description of the pending operation table is provided below in connection with .

In some embodiments a request is either entirely processed in the E Scheduler or partially processed by the E Scheduler and then dispatched to one of the executors . An executor receiving a request either completes the request immediately or merely returns a completion signal via an asynchronous callback. In some embodiments there is no distinction between the two possible outcomes from the E Scheduler s perspective. In either case the E Scheduler can safely assume that the executor is ready to accept new assignments. If a request causes the E Scheduler to suspend while waiting for a predefined condition e.g. for a buffer to become available the E Scheduler also inserts the request into the pending operation table e.g. associating the request with the buffer that the E Scheduler is waiting and re processes the request when the predefined condition is satisfied.

In some embodiments as shown in the E Scheduler includes one or more process functions for processing request messages stored in a message queue associated with the E Scheduler . For each request message the process functions prepare its associated input and output buffers and ultimately invoke one or more launch functions associated with the E Scheduler . Exemplary process and launch functions are shown in .

In some embodiments the process functions are responsible for inserting data movement requests into predefined locations in the message queue to ensure that input buffers required by different operations are at the required locations before any operation is launched. The process functions are also responsible for allocating input and output data buffer handles for operations as well as initiating data movement operations if necessary.

In some embodiments each output buffer has an entry in the pending operation table if its handle is not yet marked as done and or may be referenced by subsequent operations. If all inputs and outputs are allocated appropriately including temporary buffers if necessary the requests are sent to the launch functions of the E Scheduler .

In some embodiments the launch functions are responsible for processing requests that may have been deferred. The launch functions can be invoked either through original processing of a request message or by replaying a deferred operation that has been recorded in the pending operation table. In either case the requests output operands if any should be allocated prior to invocation of the launch functions. In some embodiments this is achieved in a process function that first invokes a launch function for a particular operation.

In some embodiments the launch functions first check whether their inputs are ready. If the inputs are not ready the E Scheduler places a deferred operation entry in the pending operation table. In some embodiments the entry is indexed by the first input buffer that is ready. Operations are not deferred more than once per input. When all inputs are ready the actual operation is launched. When launching an operation the E Scheduler sends a request to one of the executors . If the executor performs an operation asynchronously the E Scheduler is configured to receive an operation completion callback from the executor and mark the corresponding output buffer handles as being ready.

In some embodiments the pending operation table is indexed by the handle of a buffer that needs to be finished before the deferred operation can proceed. When an operation completion callback corresponding to an operation is received the E Scheduler marks the output buffer handles produced by the operation as being completed . Thus any deferred operations depending on any of those handles are re tried.

The two pseudo codes in summarize the E Scheduler s use of the pending operation table as described above. In particular the pseudo code illustrates a simplified version of a process function for adding new entries to the pending operation table as it processes a program sequence issues such as allocating data buffer handles and initiating data movement between executors are ignored here for simplicity . The pseudo code illustrates a simplified version of another process function for eliminating entries from the pending operation table as it processes an operation completion callback returned by an executor . At the core of two pseudo codes are the launch functions which determine whether all the inputs are ready and the operation can be issued to a respective executor or whether the operation needs to be re inserted into the pending operation table.

For illustrative purpose assume that the E Scheduler updates the pending operation table by associating the program C with the first entry T0 that is associated with the program A. Further assume that an executor responsible for executing the program A subsequently invokes a callback to the E Scheduler indicating that the program A has been executed and therefore the temporary output T0 is available. In this case according to the pseudo code the E Scheduler removes the first entry T0 from the pending operation table as shown by the x drawn through that entry and replays operation s that depends on the temporary output T0. But the E Scheduler may find that it cannot submit the program C to any executor because the E Scheduler has not yet received a callback indicating that the program B has been executed from the same or another executor responsible for executing the program B. The E Scheduler then updates the pending operation table by associating the program C with the entry T1 .

After receiving the second callback associated with the program B the E Scheduler removes the T1 entry from the pending operation table . At this time because the two inputs T0 T1 to the program C are both ready the E Scheduler can submit the program C to an executor for execution. Note that the aforementioned example is only for illustrative purpose. In some embodiments there is no fixed order in which independent programs within a program sequence are executed at different executors. It is therefore completely possible for the program B to be executed before the program A under a different situation. But one skilled in the art will understand that the underlying principle remains the same.

An executor is a software module that manages execution of programs or compute kernels on one or more similar functional units within a processing element of a parallel processing computer system. In some embodiments the functional units managed by a single executor may share the same memory space associated with the host processing element. The executor may be responsible for managing the memory space if it is distinct from the parallel processing computer system s main system memory e.g. the CPU memory shown in . In some embodiments the executor is responsible for initiating data transfers between the parallel processing computer system s main system memory and the host processing element s memory space. In some embodiments the executors are chosen and initialized by the supervisor .

In some embodiments the E Scheduler submits to the executors different types of data array operation messages including but not limited to Move Data In Compute Data Compute Sequential and Move Data Out . An executor notifies the E Scheduler of its receipt and completion of the data array operation messages via callback messages .

The message Move Data In requests that an executor move data from a buffer in the main system memory into the memory space of a processor associated with the executor. The message Compute Data requests that the executor execute a single program or intrinsic operation on the processor. The message Compute Sequential requests that the executor perform a set of operations on the processor in a specific order. The message Move Data Out requests that the executor move data from the processor s local memory space to the main system memory space.

In some embodiments some executors e.g. the CPU executors may share the main system memory with the application . Messages such as Move Data In and Move Data Out are therefore optional. The CPU executors can perform operations associated with the Compute Data and Compute Sequential messages directly in the main system memory space .

By contrast some other executors e.g. the GPU executors manage GPUs each having its own local memory space . In this case the E Scheduler may send to the GPU executors messages such as Move Data In and Move Data Out to cause data movement between one GPU s local memory space and the main system memory space or another GPU s local memory space.

In some embodiments upon receipt of an operation message from the E Scheduler the GPU executors allocate space in their local memory space for objects to be processed. In some embodiments a GPU executor initiates a hardware direct memory access DMA to the data identified by the E Scheduler and launches computational tasks on a GPU. The GPU executor is also responsible for tracking the completion of a designated task on the corresponding GPU.

In some embodiments as shown in the callback messages from the executors to the E Scheduler fall into one of two categories i early acknowledgement callback messages that allow the E Scheduler to dispatch operations that are dependent upon the operation being acknowledged by the callback messages ii and final completion messages that allow the E Scheduler to de allocate resources held by the operations that have been completed. For example the Move Data In and Compute Data messages enable the GPU executors to operate in parallel with the E Scheduler using early acknowledgement callback messages. In some embodiments early acknowledgement and final completion messages are encapsulated into a single message. For example the Move Data Out and Compute Sequential messages are more efficiently handled with a single callback message.

As noted above the E Scheduler is responsible for dynamically managing the dependency relationship between different program sequences and programs within the same sequence. Therefore an executor can execute programs it receives from the E Scheduler in an arbitrary order. In some embodiments the executor chooses an execution order that may generate the highest yield for a given computer system.

In some embodiments the main system memory space is used as a backup for a GPU s local memory space or for data transfer between two GPUs local memory spaces . In some embodiments if the memory allocation requests for a particular GPU s local memory space exceed its physical limit the corresponding GPU executor can temporarily move some data from the GPU s local memory space to the main system memory space .

In some embodiments as shown in the runtime system includes a special type of executor called interpreter which can be one of the CPU executors that has access to the main system memory space but does not allocate its own buffers in the main system memory space .

In some embodiments the interpreter operates in two basic modes i sequential mode in which the interpreter processes one operation at a time on one CPU and ii parallel mode in which the ProgGen performs just in time JIT compilation of multiple operations and the interpreter then arranges for execution of the resulting compute kernels across multiple CPUs . In some embodiments the sequential mode is used for generating reference results. The parallel mode is used for achieving a high performance by either distributing the workload to multiple pre allocated and or dynamically allocated threads or calling into existing multi threaded math libraries such as MKL.

In some embodiments there is an interface between the GPU executor and the GPU e.g. the GPU driver libraries in . The interface enables the GPU executor to access and or control the GPU s local memory space . Exemplary low level GPU interfaces include GPU driver libraries provided respective GPU vendor such as ATI s CTM and nVIDIA s CUDA.

In some embodiments the interface provides a mechanism for the GPU executor to enumerate the graphics cards on a computer system to discover the resources associated with each graphics card and to connect to one or more of them. In some embodiments the GPU executor connects to one or more GPUs after discovery via the Supervisor .

In some embodiments the GPU executor is responsible for allocating and de allocating objects such as command buffers program constants programs and buffers in several classes of memory. Exemplary classes of memory include Cached System Memory Uncached System Memory and GPU local memory . Cached System Memory and Uncached System Memory are accessible by both the GPUs and CPUs but differ in whether the CPUs are able to cache elements in them.

In some embodiments the two types of system memory are parts of the ordinary main memory of a main processing element e.g. CPU . The GPU memory is directly accessible by a GPU . Elements in this memory are either copied in by the GPU via one of the two types of system memory or directly generated by the GPU . In some embodiments the GPU memory is faster for GPU calculations than the system memory. The relative offsets of objects within the GPU and system memories affect the amount of time required for the GPUs or CPUs to perform an operation on those objects. In some embodiments these classes of memories may consist of multiple non contiguous extents. Other memory types with other properties may also be managed by the executor .

In some embodiments a GPU interface such as ATI s CTM provides the GPU Executor with a model of interacting with the GPU by building and sending command buffers of operations that execute in sequence. Each operation in the command buffer changes the GPU state and launches a compute kernel in parallel on one or more processing elements managed by the GPU executor in accordance with the launch specification associated with the compute kernel. In some embodiments the operation copies an object from a location within one memory extent to another.

In some embodiments the GPU executor packs operations into the command buffer until it is full and then sends the command buffer via the low level interface to the GPU . The GPU executor may order the commands to minimize the total number of commands in the command buffer the time required for state changes in the command buffer and the amount of time required for transferring data back at the CPU.

In some embodiments for each command buffer built and submitted to the GPU the GPU executor maintains a Completion List of the commands in the command buffer and the operations to be performed upon the completion of a command. After the GPU completes the operations in the command buffer as defined within the GPU interface the GPU executor prepares and sends the callback messages to the E Scheduler .

In some embodiments one operation executed by the GPU requires additional operations to be sent to the GPU . For example a copy of data may be performed in multiple pieces if either the sending or receiving memory extent is smaller than the total amount of data to be copied. In this case the additional operations are packed into the current command buffer along with the remainder of work to complete this operation.

In some other embodiments the command buffer in connection with a GPU interface e.g. nVIDIA s CUDA may be hidden behind the interface. The GPU executor may indirectly process the command buffer.

In some embodiments the GPU executor allocates buffers in different memory banks of the GPU s local memory space such that the GPU executor can access the multiple buffers simultaneously. In some embodiments these allocated buffers are spaced apart from each other as far as possible so as to provide maximal independence when not all memory banks are used.

In some embodiments the GPU executor specifies in the launch specification of a compute kernel targeting the GPU that all buffers and objects referred to by the kernel be present in a memory space accessible by the GPU . If a buffer is not present the GPU executor may synthesize a Move Data In message and submit the message to the GPU in advance.

In some embodiments the E Scheduler may send a sequence of Compute messages to the executors that is not appropriate for execution with a single layout of buffers in the GPU Memory Cached System Memory and Uncached System Memory. Accordingly the executors may reorder the Compute messages and insert intermediate swap messages into the messages. In some embodiments the intermediate swap messages cause a Move Data Out operation of a currently unused buffer to the system memory managed by the C Scheduler free the memory managed by the executors and perform a Move Data In operation of the buffer. Note that there are many well known algorithms for ordering the Compute messages and selecting which buffer to perform a Move Data Out operation.

In some embodiments one or more programming tools can be used for developing an application that takes full advantage of the runtime system . The profiler is one of such tools.

A profiler is responsible for measuring an application s performance as it is being executed on the runtime system such as the frequency and duration of functional API calls into the runtime system and their corresponding locations in the source code of the application . In some embodiments the profiler may use a wide variety of techniques to collect data including hardware interrupts code instrumentation operating system hooks and performance counters.

In some embodiments the performance related information includes raw data relating to various application execution characteristics from running the application in the runtime system including 

In some embodiments the profiler separates data collection from data visualization. This configuration allows the performance data to be collected for various application execution runs at one point in time and analyzed at another point in time. This configuration also allows data associated with different application execution runs to be compared to each other and to be examined for performance anomalies between different application execution runs.

In some embodiments the collector runs in parallel to the runtime system . It is configured to add little overhead to the runtime system so as to reveal the system s actual performance. For example the collector collects data for individual dynamically generated compute kernels that are executed on an individual processor of the parallel processing computer system. This configuration allows a deeper understanding of how the original sequential source code is partitioned and distributed on the different processors of the parallel processing computer system.

The collected performance data is stored in e.g. a database . In some embodiments the database is implemented as a repository of profiling output files arranged in a hierarchical file structure in a platform independent format. This configuration allows the data collection to be performed on one platform and the data analysis and visualization to be performed on another platform. It also allows the comparison of data from different platforms and from different system configurations.

In some embodiments analyzing the performance data at the processor level is not the most convenient approach for tuning the source code of an application because the runtime system dynamically partitions and parallelizes the application into multiple parallel compute kernels of binary codes that are executed on potentially different processors. Therefore the analyzer may map the performance data gathered for these individual compute kernels back to the original source code of the application . This configuration allows the tuning of the application without the necessity to understand how its source code is partitioned and distributed in the runtime system .

In some embodiments the analyzer is responsible for transforming the performance information amassed by the collector into one or more performance reports. In some embodiments the performance reports include 

A user can look at the performance reports using the viewer to identify performance issues associated with the application . In some embodiments the viewer is a simple text editor if the performance reports are written in plain text. In some other embodiments the viewer presents a graphical representation of the application s performance using the performance reports.

In some embodiments as noted above the ProgGen dynamically generates a compiled program or compute kernel that may correspond to one or multiple API calls. The C Scheduler submits the compute kernel to the E Scheduler . The E Scheduler then chooses specific executors for executing the compute kernel. Based on the execution results the collector generates a compute kernel CK record for the compute kernel.

On the other hand a compute kernel corresponds to at least one program counter associated with an API call in the source code. Therefore it is possible to attribute the CK execution counters back to the execution counters in each PC record . To do so the CK record includes an array of data structures each data structure including at least one pointer to a corresponding PC record and a weight for estimating the relative cost of performing the API call corresponding to the PC. In some embodiments the sum of the total weights associated with different data structures in the CK record is 1.0. In some embodiments the ProgGen is responsible for estimating a weight based on the particular operation and the array size associated with a compute kernel. In some other embodiments the weight of a compute kernel is dynamically determined based upon the kernel s actual performance values.

In some embodiments one execution of the application with the profiler enabled generates one set of CK records and PC records in the database . After a repeatedly execution of the application the database accumulates a predefined amount of information which is sufficient for the analyzer to characterize the application s performance.

In some embodiments the analyzer is executed offline which attributes the program sequences execution counters back to the API calls in the source code of the application using the data structures shown in . For example the Total GPU Time of a CK record is divided among the member PCs based on their respective numbers of instructions. In some embodiments one PC may be associated with multiple compute kernels or even multiple program sequences. The ultimate GPU Time of a particular PC at the application level is the sum of the GPU time of the same PC among all the CK records that include this particular PC.

In some embodiments the analyzer generates a performance report that includes one or more PC level execution counters some of which are actual values and some of which are estimates derived from the corresponding CK records. Because the performance report includes the file name and line number of the source code for each PC an application developer can use the performance report to fine tune the application to achieve a better performance.

Program debugger is another tool used for monitoring the execution of the application using the runtime system .

In some embodiments the debugger is used for generating a reference result on a reference processor e.g. CPU . It is assumed that the reference processor can generate an accurate result for a given operation. The reference result is compared with an optimized result generated on a second processing element e.g. GPU for the same operation. Discrepancies between the two results can be used to identify errors in the runtime system and improve its accuracy on the processing element.

In some embodiments the runtime system processes the reference ports without using the trace cache or macro cache . This configuration not only separates the processing element s impact from the trace macro cache s impact but also ensures that the reference result can be used to debug the trace cache and the macro cache .

In some embodiments the ProgGen dynamically generates program sequences for the reference ports and applies no optimization to the generated program sequences. In some embodiments the runtime system allows a user to invoke the reference result generation debug option either at the beginning of executing an application or in the middle of executing the application.

As noted above the ProgGen is responsible for fusing multiple operations together for performance reason. is a block diagram illustrating such an example in which the ProgGen merges two separate but consecutive work queue entries ADD and MUL into one operation. It is assumed that the debugger has been turned on and a breakpoint is set between the ADD entry and the MUL entry. In some embodiments the ProgGen may divide the two entries into two separate program sequences. The two program sequences are executed separately by the E Scheduler and the respective executors . But because of the split of the two entries into separate program sequences the temporary result a user can access at the breakpoint is not necessarily the same one if the debugger is not turned on and the two entries are fused into one. This debugging mode is referred to as intrusive mode .

In some embodiments instead of splitting the two work queue entries into separate program sequences the runtime system generates a cloned work queue including the ADD entry but not the MUL entry. The ProgGen generates compiled program sequences corresponding to the cloned work queue which are then submitted to the E Scheduler for execution. In some embodiments the program sequences corresponding to the cloned work queue are not saved in either the trace cache or the macro cache . The result associated with the cloned work queue is then presented to the end user through the debugger. Meanwhile the original work queue is processed in a normal manner by the runtime system .

In other words the ProgGen generates two sets of program sequences . These two sets of program sequences are both sent to the E Scheduler for execution. Because the ProgGen is not forced to break the entries in the work queue the result corresponding to the work queue is the actual one produced by the application . The debugging result which corresponds to the cloned work queue is a reliable and accurate estimate of the actual result that can be obtained without affecting the actual result. Therefore the cloned work queue is like a snapshot of the application right before the breakpoint without causing any impact on the regular execution of the application . This debugging mode is referred to as non intrusive mode .

The choice between the intrusive mode and the non intrusive mode of debugging is a compromise between efficiency and accuracy. The intrusive debugging mode is more efficient because it does not require the generation and processing of the cloned work queue. One the other hand the non intrusive debugging mode guarantees that the final result of the application is not affected by the debugging session and is therefore more accurate.

In some embodiments the debugger is used for result comparison. The result comparison has two modes result checking after computation and result checking on copy out. In some embodiments result comparison is enabled at the system initiation time through configuration file settings and disabled by default to avoid overhead. In some other embodiments result comparison is enabled and disabled dynamically at run time.

During the result checking after computation mode the results generated by the application are checked asynchronously against separately generated reference results to identify erroneous operations in the application . A subsequent computation or copy out operation may continue after a result is generated whether or not it has been checked. This configuration allows the runtime system to perform more operations without affecting the results generated by the runtime system . In some embodiments it is possible to identify a particular operation that causes a specific erroneous result by examining the results from every operation during this mode.

During the copy out mode the application s results are checked synchronously before they are copied out of the runtime system . This configuration can prevent the application from accessing invalid data. Because the checking does not happen after every intermediate step it is less expensive but offers little information as to the cause of an error associated with a particular operation.

In some embodiments the allowed error for an individual optimized result allowed error is determined using the formula allowed error ABS reference Scale . The optimized result is deemed correct if ABS optimized reference 

In some embodiments an application developer may choose specific values for the allowed error and Scale variables. For example the fact that both variables are set to zero indicates that the optimized result is acceptable if it matches the reference result exactly.

It will be apparent to one skilled in the art that the runtime system shown in is one of many implementations of a general runtime system running on a parallel processing computer system. is a block diagram of such a general runtime system . The runtime system includes an application program interface which in some embodiments at least partially corresponds to the LSI and the FE shown in .

At run time the application program interface receives one or more operation requests such as API calls from the application . The application program interface is responsible for preparing an intermediate representation for the operation requests. In some embodiments the intermediate representation is language independent and cross platform. The directed acyclic graph shown in is an exemplary one.

The dynamic program generator dynamically prepares one or more compute kernels for the intermediate representation and returns the compute kernels to the program execution scheduler . In some embodiments the dynamic program generator behaves like a just in time compiler and the compute kernels are configured to be executed on one or more specific types of processing elements. In some embodiments the compute kernels are derived from both pre compiled intrinsic operations and dynamically compiled primitive operations. The program execution scheduler dynamically dispatches the compute kernels to one or more program executors for execution on one or more processing elements of a parallel processing computer system.

At compilation time the parser of the static compiler identifies one or more operation requests in the application source code and prepares an intermediate representation for the operation requests. The back end of the static compiler performs loop transformations over the intermediate representation and prepares an executable application that includes one or more compute kernels. In some embodiments the compute kernels are configured to execute on one or more specific types of processing elements of a parallel processing computer system.

At run time the runtime system receives the pre compiled compute kernels from the executable application through its API . The program execution scheduler dynamically dispatches the compute kernels to one or more program executors for execution. Because the compute kernels are prepared at compile time for the specific types of processing elements they can only be executed on the same types of processing elements.

At compile time the parser identifies one or more operation requests in an application source code and prepares an intermediate representation for the operation requests. In some embodiments the static compiler includes a loop analyzer that performs loop analysis over the intermediate representation at compile time. But no processor specific compute kernels are generated at compile time.

At run time the runtime system receives the intermediate representation in the executable application with runtime API calls through the runtime system API and performs loop transformations for the intermediate representation. The dynamic program generator dynamically prepares one or more compute kernels for one or more specific types of processing elements. Finally the program execution scheduler and the program executors work in concert to execute the compute kernels on selected types of processing elements of a parallel processing computer system.

In some embodiments it is possible to extend the existing static compiler loop vectorization technology to generate more direct calls to the runtime system s API when parallel loops are discovered. In some embodiments the static compiler collapses many primitive operations into some pre compiled intrinsic operations through code optimization techniques. In some embodiments static compilation is performed and all primitive operations that are not coalesced by the static compilation become pre compiled intrinsic operations.

Although the aforementioned embodiments illustrate one application invoking the runtime system it will be apparent to one skilled in the art that the runtime system can be implemented on one computer and the runtime system communicates with one or more multiple applications threads and processors simultaneously over shared memory pipes and or other communication mechanisms known in the art. In some embodiments the runtime system is implemented in a client server network environment as a service provider. An application runs on one or more client computers. The application submits operation requests to the runtime system running on one or more server computers. The runtime system in response provides services back to the application at the client computers. Therefore different applications may submit their respective operation requests to the runtime system at the same time and receive results generated by the runtime system through the network connecting the client computers to the server computers.

A runtime system implemented in accordance with some embodiments of the present invention provides an application development platform for a parallel processing computer system including multiple processing elements. The runtime system enables application developers to accelerate and optimize numeric and array intensive operations in their application programs by leveraging the computational power of various types of processing elements as defined in this application including stream processors and multi core processors. This enhancement greatly increases the performance of applications running on the parallel processing computer system. In some embodiments the numeric and array intensive operations on the parallel processing computer system can be as much as 120 times faster than today s fastest CPU implementations. Note that although some of the description herein refers to one or more specific types of processing elements such as stream processor and multi core CPU one skilled in the art will appreciate that the present invention can be implemented on other types of processing elements not explicitly enumerated.

The runtime system can be used for developing debugging profiling and executing application programs that invoke the runtime system as well as off the shelf packages incorporating the runtime system through the system s API e.g. the LSI in . In some embodiments the runtime system is deployed in a computing cluster of 20 100 multi CPU x86 servers running Unix Linux or Windows operating systems and or having a multi user environment. For example the runtime system can fit seamlessly into the management and monitoring systems that cluster administrators use today. At least some of the x86 servers may be configured with GPUs or other types of processing elements. The runtime system provides optimization and acceleration of compute intensive operations on CPUs and or GPUs that are required by applications running on the cluster. In some other embodiments the runtime system is deployed in single user single system or in multiple user single system configurations.

Users of the runtime system can use their existing tools for development such as existing editors debuggers profilers and integrated development environments IDEs to create application programs for the runtime system. An advantage of the runtime system is that users do not need to learn new techniques that are foreign to their development language. For example with the runtime system a FORTRAN 90 programmer does not need to learn memory management on a particular types of processing elements including stream processors and coprocessors. In other words users of the runtime system are able to write application programs in manner to which they are accustomed and their programs and code changes can endure for years across multiple generations of hardware.

In some embodiments the runtime system includes libraries of routines such as the primitive library and the intrinsic library in . These library routines can be source code or binary code associated with a specific processing element. These library routines are designed to be accurate and efficient when executed on a specific processor such that the applications invoking the library routines can produce more accurate results than what would typically be obtained using software packages offered by an operating system. The library routines are available for different classes of functions some of which are described below.

In some embodiments the library routines are configured to handle a wide range of array based mathematical reduction trigonometric and Boolean functionalities. In some embodiments the mathematical functionality includes but is not limited to operations such as 

In some embodiments the trigonometric functionality includes but is not limited to operations such as 

In some embodiments the primitive library routines support execution of matrix operations such as matrix multiplication transpose and identity in parallel on various types of processing elements. For example through the LSI the runtime system can provide functions at different levels of BLAS Basic Linear Algebra Subroutines including level 1 vector vector level 2 vector matrix and level 3 matrix matrix operations for vectors and 2D matrices on real numbers. These functions are implemented as the primitive operations in the primitive library and therefore can execute in parallel on multiple processing elements. In some embodiments the primitive library routines support execution of inverse hyperbolic functions in parallel on various types of processing elements.

In some embodiments the intrinsic library routines are individually hand tuned pre compiled and highly optimized binary code to perform predefined sophisticated operations on a specific type of processing elements as defined above e.g. stream processors or multi core CPUs in parallel. Exemplary sophisticated operations include LU decomposition LU solve LU condition and LU unpack 1D 2D real and complex Fast Fourier Transform and convolution. An application can access these operations through the runtime system s API e.g. the LSI in . In some embodiments the matrix operations described above are implemented as intrinsic library routines that can solve multiple independent matrixes of data in parallel on one or more processing elements.

In some embodiments the runtime system provides multiple random number generators RNG including a fast short period RNG and a slower long period RNG. These RNGs can be used separately or in combination. These random number generators execute in parallel on one or more processing elements.

In one embodiment the runtime system API provides inverse hyperbolic functions. As with the basic math and trigonometric functions described above in some embodiments these are provided as highly accurate primitives that are optimized for specific target stream and multi core processors.

The runtime system provides a debugger with reproducible execution with compatibility for break watchpoints stepped execution and data inspection. The runtime system allows users to run their programs in reproducible execution mode which will make their programs run in a synchronous mode. In this mode the user will be able to set break watch points within code sections and inspect data within data types.

A unique feature of the runtime system debugger is that it enables a user to set breakpoints and to review the state of program data in such a way that the actual result of the operation if it were performed in the context of normal program execution is not affected by data examination. This could be a significant factor in the runtime system as certain mathematical vector operations are performed asynchronously in an optimized fashion in certain types of processing elements such as stream processors for reasons of efficiency and may provide different results if they were instead performed individually which would be the case if a user set a breakpoint intermediate between two sequentially executed instructions. An example of two such instructions is a vector multiply add sequence known as MUL ADD. Thus the runtime system debugger in one embodiment enables a user to set a breakpoint anywhere in the application program in code sections including such as after the MUL in a MUL ADD sequence to inspect data types without modifying the actual result of that operation in normal system operation even when that data would be an intermediate result in a sequential stream processor operation.

It is also a unique feature of the runtime system debugger that helps debugging problems within compute kernels e.g. by finding gross differences when results are generated in different ways and set breakpoints in relation to line numbers of the application program. This provides previously unavailable visibility into the status of code running in a processing element which enables users to debug runtime system code segments that are specified in high level application source code. Without this feature such code segments would be very difficult to debug given the opacity to a conventional application of operations being executed in a processing element such as traditional 3D graphics operations .

4.8 Debugger Functionality Reference Results and Result Comparison Reference Results vs. Results from the Stream or Multi Core Processor

The runtime system accelerates software operations by generating and or executing optimized parallel programs on stream and multi core processors. This has the potential of generating different results as compared to performing the operations on a single CPU core. This is because different stream and multi core processors support different arithmetic precision and because parallel algorithms perform operations in different orders than would be performed by sequential CPU programs.

One of the concerns of users of the runtime system is likely to be whether a given implementation of the runtime system based one or more types of processing elements provides accurate results compared to a traditional CPU implementation. The runtime system provides a mechanism to compare variables including results between programs run using a reference implementation on the CPU vs. an optimized implementation on a stream or multi core processor. This involves the runtime system generating equivalent program code sequences for the reference implementation and the stream or multi core processor executing the equivalent programs sequences and then generating side by side results. In a Result Comparison mode the runtime system determines when the side by side results are within a defined tolerances and if not performs an action that can be selected from aborting the program throwing an error and continuing program execution or initiating a dialog with the user. In another mode referred to as Reference Results mode the runtime system can display for the user the reference result and the side by side optimized result from execution on the stream or multi core processor to enable users to analyze the accuracy and precision of program results generated by the stream or multi core processor. Other embodiments of the runtime system can include identifying which specific calculations are causing loss of precision or accuracy.

The runtime system provides performance profiling including bottleneck identification. The runtime system profiler provides sufficient information to users to enable them to understand the drivers of performance. The information may include for example measured performance data such as the time to perform an operation or the number of operations performed estimated performance data such as time estimates derived from data sizes and information about the code compiled for the specific stream or multi core processor such as where more efficient special purpose mechanisms could be used to accelerate the operations performed.

The runtime system profiler enables a user to view performance data associated with one or more processing elements for executing operations corresponding to particular lines of code in the application source program. This is important because in some embodiments execution on the stream or multi core processor may be asynchronous with respect to the application s calls to the runtime s API which would frustrate attempts to use standard timing and profiling tools to measure performance. Furthermore the program code executed in a stream or multi core processor under control of the runtime system may bear little similarity to the particular API call at a particular place file and line in an application that resulted in the generation and execution of that stream or multi core processor program code.

On the other hand understanding how the runtime system fused sequences of primitive operations into programs for the stream or multi core processor can be important for tuning performance of the application. Therefore the runtime system profiler enables a user to view performance data corresponding to programs and program sequences and also to view performance data corresponding to individual calls into the runtime system s API which comprise the programs executed on the stream or multi core processor.

In one embodiment the runtime system profiler writes collected performance data to a file which it can analyze offline after program execution is complete. In another embodiment users can review performance data with a graphical user interface that provides a number of different views into execution of programs and program sequences handed off to the runtime processor. These different views include summary compute I O and runtime.

4.10 Double Precision Functions in a Stream Processor API or Cross Platform API for Stream and Multi Core Processors.

In some embodiments the runtime system API which enables cross platform execution on different types of processing elements supports double precision implementations of all its supported operations even though some processing elements do not natively support fast double precision. In one embodiment the double precision operations can be performed on the stream or multi core processor using its native support for double precision. In one embodiment the double precision operations can be performed on the CPU.

In another embodiment the runtime system can support a mode in which computations on double variables e.g. floating point numbers with greater than 24 bits of precision can be performed on the some processing elements by combining multiple single precision floating point data elements to achieve the level of double precision during arithmetic implementations.

This technique could be applied to computations done on values of any precision or word bit length. For example these techniques would be applicable in the situation where CPUs provide quadruple precision whereas a particular stream or multi core processor provided only single and or double precision.

In some embodiments assuming that an old version of the runtime system is available for a particular target platform i.e. a combination of processing elements and operating system application programs executable on the old version of the runtime system can be run on another platform that is loaded with a new version of the runtime system. This cross platform binary compatibility is due to the fact that the runtime system contains implementations of the operations for multiple stream and multi core processors and uses handles to separate the application from the runtime system. Given an application the runtime system automatically maps its old handles to ports supported by the new version of the runtime system and selects and executes the appropriate implementations transparently.

In some embodiments the runtime system provides applications with interfaces to operations implemented in multiple languages such as C C and FORTRAN. Additional languages can be supported as necessary. For ease of use API s provided by the embodiments can conform to the style and usage of the individual languages but this is optional. There is also no requirement for API s to be consistent across different languages. In some embodiments the interfaces associated with different languages can interoperate with each other.

The runtime system can interface with the available GPUs through a graphics API such as OpenGL or Direct3D or through a low level proprietary driver provided by GPU manufacturers. It can also interface with multi core processors through low level drivers such as OpenMP and MPI. Other embodiments can make use of different APIs and drivers appropriate to the system resources.

An embodiment of the runtime system can be configured to interoperate with standard cluster management tools along the following lines 

There are several unique concepts that are included in different embodiments of the runtime system. These concepts include system level concepts that relate to functions and methods provided by interoperation between several blocks and features methods and data structures associated with individual system blocks. The purpose of this summary is to outline some of the concepts functions of both types.

The runtime system enables compute intensive operations typically matrix array operations in an application program to be executed on one or more processing elements such as stream and or multi core processors and or GPUs instead of on a single CPU core. The runtime system offers several unique features that enable application programmers to take advantage of these capabilities with relative ease. For example to execute a particular operation on the stream and or multi core processors the runtime system requires a user to do no more than 1 to ensure that an appropriate version of the runtime system is available on the target computer system i.e. with appropriate execution modules for the available processors and 2 to specify in the application which operations are to be executed under the control of the runtime system using the runtime system s API.

When booted on a computer system the runtime system conducts a system inventory to determine what hardware e.g. CPU and GPU and software graphics drivers and GPU interfaces or APIs such as OpenGL Direct 3D or low level proprietary GPU interfaces provided by GPU vendors are installed on the target computer system. Using this information the runtime system is able to select appropriate execution modules executors which manage execution of the application programs on the hardware including the GPU and multi core CPU. This information also enables the runtime system to compile the specified application programs for the target hardware GPU or multi core CPU or both and to select the appropriate versions of pre compiled programs and library routines to execute on the target hardware.

The runtime system is designed to take advantage of proprietary low level GPU interfaces which enable the runtime system to utilize the GPU memory and to exploit target specific hardware and software features not exposed through traditional graphics APIs such as OpenGL and Direct3D to improve the runtime system s performance. For example such low level target specific interfaces are less or even not subject to the limits on the numbers of instructions and registers used by GPU programs. In some embodiments some application programs can be written in a GPU s native assembly or machine binary code rather than or in addition to programs written in high level languages and platform independent pseudo assembly such as Direct3D Shader Assembly. In some embodiments special more efficient data layouts and data transfer mechanisms may be exploited. In some embodiments non graphics oriented capabilities can be used such as writing to arbitrary memory offsets from GPU pixel shaders rather than to just the current pixel and accessing hardware performance counters. In some embodiments a GPU program may be compiled or assembled into a binary image that may be executed without further translation which can reduce run time overhead simplify testing and enable greater control over execution on the GPU than if the driver further processed the assembled program.

Progressive Evaluation is the technique at the heart of the runtime system. An application generates a dynamic sequence of calls to API functions and operators that the runtime system cannot predict in advance. Under Progressive Evaluation the runtime system treats calls to API functions and operators merely as requests to perform computation and defers processing them. The requests are batched to improve application performance on the processing element.

After sufficient deferred requests have been received the runtime system e.g. its ProgGen module may apply generate and compile one or more compute kernels which are executable programs or subroutines that run on the processing element s which may be one or more stream and or multi core processors to perform the computations requested by a sequence of API calls. Formation of efficient compute kernels for the processing element is critical to performance. During this process the runtime system determines which operations can execute in parallel optimizes the sequence of operations generates source code for the processing element compiles and assembles the kernels.

In some embodiments some of these code translation steps may be skipped by directly generating lower level program representations. For example assembly code or even executable binary instructions could be generated directly rather than compiling and or assembling higher level program representations.

In some embodiments some intrinsic operations may use pre formed hand written kernels rather than dynamically generated kernels.

In some embodiments when processing requests for operations to be performed by the CPU the runtime system may generate code for one or more CPUs each with one or more cores. This is accomplished using basic multiprocessing features provided by the host system or cluster. In some embodiments the runtime system may treat some or all such requests as intrinsic operations and use pre formed kernels. In some embodiments the pre formed kernels may be used only in certain contexts such as to process small arrays or to use as reference results.

Note that throughout the present application the terms program or kernel for a stream or multi core processor refer to a computational unit that may be invoked by the runtime system. Depending on the specific type of the stream or multi core processor the computational unit may be a standalone program or a subroutine.

The runtime system s API enables programmers to issue requests to the runtime system. In some embodiments the API commands include specific calls to invoke basic input and output functions to perform a variety of operations on data arrays. The API commands allow users to define Macros groups of executions that are executed repeatedly also known as Code Reuse Sections . The operations include at least a subset of the following operations 

The runtime system s API includes calls that enable programmers to execute primitive operations for mathematical trigonometric and other functions optimized for specific types of stream and or multi core processors. In some embodiments these primitive operations are more accurate and provide higher performance than those provided by standard libraries from the hardware vendor or development environment. In some embodiments these primitive operations are stored internally as source code to facilitate dynamic formation of compute kernels which are then dynamically compiled for the specific type of particular stream or multi core processors.

The runtime system s API enables an application program to execute pre written optimized routines called intrinsic operations. An intrinsic operation implements a predefined set of matrix array operations for execution on one or more specific stream and or multi core processors. In some embodiments the intrinsic operations include one or more programs for the stream or multi core processors. They may include control code executed on the CPU which may select one or more kernels to invoke arrange for any temporary buffers required to pass intermediate results between kernels determine how the kernels should execute in parallel e.g. by choosing the number of threads to launch and determine kernel arguments. Because they are written by hand rather than being generated from sequences of calls to primitive operations automatically intrinsic operations may take advantage of features not supported by the kernel formation machinery in the ProgGen module. In some embodiments intrinsic operations are written using high performance low level interfaces to a specific type of processing elements. In some other embodiments intrinsic operations are written in either a high level language or in assembly language. In some embodiments intrinsic programs may be pre compiled to reduce the runtime system s overhead and increase the system s robustness. In some embodiments pre compiled intrinsic operations for the CPU may be linked into the runtime system and called as ordinary procedures by the CPU executor.

The runtime system s API also provides functions implemented using its own primitive operations and intrinsic operations.

7.7 Dynamic Program Generation and Compilation for Multiple Different Stream and Multi Core Processors.

As mentioned above the runtime system e.g. its ProgGen module generates and compiles compute kernels for specific types of processing elements including stream processors and or multi core CPUs to perform sequences of primitive operations invoked by an application through the runtime system s API. In some embodiments the runtime system e.g. its ProgGen module generates kernels for multiple different types of processing elements where multiple different types of processors are present in the same system.

As an initial step of processing code segments in an application program the runtime system generates a uniform intermediate representation IR for each function such as the math reduction trigonometric Boolean and intrinsic functions described above directed to the runtime system for execution. The intermediate representation includes information about each function s inputs and outputs one or more operations performed by the function and the operations execution order. In some embodiments the intermediate representation is represented as a directed acyclic graph DAG . In some embodiments the same IR can be used to generate object codes for CPU GPU or any other processor supporting the runtime system. This enables the runtime system to support features such as executing the same set of instructions on a CPU and a GPU separately for code comparison or reference results determination and dynamic selection of processor type on which to execute the instructions.

The runtime system s API enables software applications to run compute intensive operations transparently on stream and or multi core processors. During normal operation the runtime system automatically and transparently compiles a set of API calls into a program sequence for the stream and or multi core processors that it then executes on available processors. The user is not required to have any prior knowledge about the specific stream or multi core processors in order to take advantage of the hardware s high performance capacity. In some embodiments the runtime system manages two caches to store compiled programs for reuse. This configuration enables the runtime system to search these caches for possibly reusable programs or program sequences before compiling new ones.

Compiled program sequences and signatures of the sequences of calls used to generate them are stored in the Trace Cache TC . Program sequences generated for previously executed sequences of calls to the runtime system s API are reused if the C Scheduler determines that they are identical to new sequences of API calls in an application program submitted to the runtime system for execution. This takes advantage of programs optimized for the stream and or multi core processors while avoiding the time required to repeatedly generate and compile the programs which takes a good deal of time.

The ProgGen module additionally stores individually compiled programs and their source code in the Program Cache. In the case that there is a previously generated and compiled program in the Program Cache that is identical to a newly generated program to be compiled the previously compiled program may be re used. In some embodiments this saving could be substantial depending on the particular compiler used for a particular target multi core processor.

In addition to the transparent execution mode the runtime system enables a user to define a sequence of API calls that may be executed repeatedly via a macro feature which is also known as a Code Reuse Section. In this mode of operation the user indicates one or more groups of operations and or macros that are to be recorded for subsequent use. After performing these operations and generating the program sequences necessary to implement them on the multi core processors the compiled program sequences are stored in the Macro Cache MC along with an invocation signature that must be matched in order to reuse the program sequence. The MC based code reuse is more efficient than the TC based code reuse because while the Macro Cache may reuse program sequences based on just a signature of the inputs to the macro the TC based code reuse requires more work from the C Scheduler to determine whether there is a match for a new sequence of operations as well as the full sequence of API calls themselves. In contrast the API calls to perform the operations do not need to be repeated when a macro is being replayed. In some embodiments the Program Cache may be used in conjunction with the Macro Cache. In some embodiments the Trace Cache may be used in conjunction with the Macro Cache. This configuration may be useful if a single macro includes a long sequence of repeated operations.

Although some of various drawings illustrate a number of logical stages in a particular order stages which are not order dependent may be reordered and other stages may be combined or broken out. While some reordering or other groupings are specifically mentioned others will be obvious to those of ordinary skill in the art and so do not present an exhaustive list of alternatives. Moreover it should be recognized that the stages could be implemented in hardware firmware software or any combination thereof.

The foregoing description for purpose of explanation has been described with reference to specific embodiments. But the illustrative discussions above are not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. For example although some of the embodiments described above are based on a language specific application program interface one skilled in the art will appreciate that many features associated with the language specific application program interface can be realized using a programming language designed to support the runtime system.

