---

title: Method and system for incremental cache lookup and insertion
abstract: Methods, systems, and articles of manufacture for caching are disclosed. These include incrementally reading a cache by, receiving a request from a client to retrieve an object from a cache; comparing a size of the requested object to a threshold retrieval size; transmitting a response including an incremental lookup indicator to the client when the size is greater than the threshold retrieval size; receiving one or more follow-on requests including the incremental lookup indicator from the client; responsive to each of the follow-on requests, retrieving a portion of the requested object from the cache; and transmitting the retrieved portion to the client. Corresponding incremental insertion of data items into the cache are also disclosed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09317470&OS=09317470&RS=09317470
owner: Google Inc.
number: 09317470
owner_city: Mountain View
owner_country: US
publication_date: 20130111
---
The present application claims the benefit of U.S. Provisional Patent Application Ser. No. 61 586 677 entitled Method and System for Incremental Cache Lookup and Insertion filed on Jan. 13 2012 which is hereby incorporated by reference in its entirety for all purposes.

In many computing environments caches are used to transparently store data items in order to serve that content faster in response to future requests. Many types of data items for example video images documents query results objects or other types of data structures may be cached. Caching can reduce the latency involved in serving content to requesters. Caching can also reduce network traffic by enabling a server to serve content that is frequently requested from a local storage rather than by making frequent retrievals from a remote location over a wide area network.

In web environments caching is frequently used by web browsers as well as by servers. The servers that may use caching include origin servers as well as proxy servers. A web browser executing on a client device may locally cache data so that the number of Hyper Text Transfer Protocol HTTP requests to remote web servers may be reduced. A proxy server may maintain a cache so that it may reduce repeated requests to origin servers for data items that are repeatedly requested by clients. For example when a proxy server encounters a request for an object for the first time it requests that object from an origin server over a wide area network and stores a copy of it in a local cache. Subsequent requests for that object may be serviced by retrieving that object from the local cache rather than from the remotely located origin server. Upon subsequent requests the proxy server would first seek to retrieve the object from the local cache and would request the origin server for the object only if the sought object is not cached. However in many environments proxy servers such as HTTP proxy servers concurrently service numerous requests for various types of data items. Concurrently servicing numerous requests for large data items can lead to scalability and performance concerns with respect to caching.

Methods systems and articles of manufacture for using a cache are disclosed. These include looking up a cache by receiving a request from a client to read a data item from a cache comparing a size of the requested data item to a threshold retrieval size transmitting a response including an incremental lookup indicator to the client if the size is greater than the threshold retrieval size receiving one or more follow on requests including the incremental lookup indicator from the client responsive to each of the follow on requests retrieving a portion of the requested data item from the cache and transmitting the retrieved portions to the client.

These also include writing to a cache by comparing a size of a data item to be cached to a threshold insert size if the size is greater than the threshold insert size transmitting a first request to write the data item to a cache where the first request includes a first portion of the data item and an indication that more portions of the data item are to follow receiving one or more responses from the cache where each of the responses includes an incremental insert indicator and responsive to each of the received one or more responses transmitting a follow on request to the cache including a next portion of the data item and the corresponding incremental insert indicator.

Further features and advantages as well as the structure and operation of various embodiments are described in detail below with reference to the accompanying drawings. It is noted that the invention is not limited to the specific embodiments described herein. Such embodiments are presented herein for illustrative purposes only. Additional embodiments will be apparent to persons skilled in the relevant art s based on the teachings contained herein.

While illustrative embodiments for particular applications are described in the present disclosure it should be understood that embodiments are not limited thereto. Other embodiments are possible and modifications can be made to the embodiments within the spirit and scope of the teachings herein and additional fields in which the embodiments would be of significant utility. Further when a particular feature structure or characteristic is described in connection with an embodiment it is submitted that it is within the knowledge of one skilled in the relevant art to effect such feature structure or characteristic in connection with other embodiments whether or not explicitly described.

Web servers such as HTTP proxy servers in particular concurrently service requests from numerous users. The concurrent servicing of numerous requests imposes a high level of demand on the local memory of web servers and associated caches. When a conventional proxy server attempts to service a large number of concurrent requests for large objects it may make numerous calls to a cache to access the objects. Simultaneously retrieving numerous large objects may however cause memory and local network congestion at the conventional proxy server. Denial of service attacks such as by automated web clients making numerous concurrent requests in order to overwhelm the web infrastructure may cause severe memory congestion due to repeated and highly frequent accesses to cache to obtain entire large objects even in response to web clients that momentarily disconnect without waiting for the response after issuing the request for an object.

The present disclosure is generally directed to methods and systems for caching. Embodiments disclosed herein can be used for example to improve the scalability of caches in web environments such as caches used by HTTP proxy servers or other web servers. According to an embodiment an HTTP proxy server retrieves cached objects in portions rather than as entire objects. By retrieving cached objects in portions the proxy server may limit the amount of consumed local memory even when reading an object of a large or unknown size. The proxy server may coordinate its retrieval of portions of the large object from the cache with the writing of the retrieved portions to requesting web clients. Another embodiment enables the proxy server to insert large objects in portions into the cache rather than as entire objects. By reading and or writing objects in portions from to their caches the proxy servers may limit consumption of their local resources and alleviate many issues associated with concurrently servicing large numbers of requests. According to some embodiments the proxy server can coordinate the incremental cache accesses for content with the corresponding communication with web clients using that content.

Proxy server may be communicatively coupled to origin server and web client through wide area network WAN and to cache through local area network LAN . According to another embodiment proxy server may connect to web client over a LAN. WAN can include any network or combination of networks that can carry data communication. These networks can include for example one or more LANs or a network such as the Internet. LAN and WAN networks can include any combination of wired e.g. Ethernet or wireless e.g. Wi Fi 3G or 4G network components.

Proxy server cache origin server and web client can be implemented on any computing device. Proxy server may be capable of handling a high volume of HTTP requests for service from web clients. Web client may include for example a mobile computing device e.g. a mobile phone a smart phone a personal digital assistant PDA a navigation device a tablet or other mobile computing devices a personal computer a workstation a set top box or other computing device on which a web browser or other web application can execute. Cache interacts with proxy server to cache data items that are transferred between the proxy server and web clients. Cache can be implemented in one or more caching servers that are physically separated from the servers for which caching is provided. According to another embodiment cache may be implemented as part of the server for which caching is provided. Origin server proxy server and cache may include but are not limited to a central processing unit an application specific integrated circuit a computer workstation a distributed computing system a computer cluster an embedded system a stand alone electronic device a networked device a rack server a set top box or other type of computer system having at least one processor and memory. A computing process performed by a clustered computing environment or server farm may be carried out across multiple processors located at the same or different locations. Hardware can include but is not limited to a processor memory network interface and a user interface display.

At stage a request for an object may be received. For example a request for an object specified by a web client using a URL may be received by a proxy server. The proxy server may include a cache client that interacts with a cache associated with the proxy server.

At stage the cache client requests the object from the cache. Stage may include determining whether the requested object is a cacheable object and a request to the cache may be made only if the requested object is determined to be a cacheable object. According to some embodiments some types of data items and or objects may be configured as cacheable. In other embodiments any data item or object may be cacheable. The cache may store any type of data including objects. Exemplary cached objects include HTTP pages images video documents and other data items. The cache may be local to the cache client e.g. cache client and cache located in the same computing device or the cache and the cache client may be interconnected via a local area network. The request may be formed at the cache client and may include a cache key to identify the requested object. According to an embodiment the cache key is determined based upon the URL of the requested object. In some embodiments the cache key may be based on other information associated with the respective requested objects. The message exchange between the cache client and the cache can be in accordance with a configured application programming interface API between the two entities. For example the API may be implemented by cache client in proxy server and cache interface of cache .

At stage the cache client receives a response from the cache. According to an embodiment the response includes a portion of the requested object and an incremental lookup indicator. The incremental lookup indicator is generated by the cache to indicate to the cache client whether or not additional portions of the requested object are available for retrieval in the cache. The incremental lookup indicator may include a content identifier content id that identifies the object being accessed. According to an embodiment the content id identifies the instance of the object being accessed. In addition the incremental lookup indicator may include a lookup identifier lookup id for example indicating the beginning of the portion of that object to be returned next from the cache.

At stage the received response is stored in the local memory of the proxy server. Specifically the portion of the requested object returned in the received response and the returned incremental lookup indicator are stored in the proxy server.

At stage the proxy server begins transmitting the already retrieved object data to the requestor. According to an embodiment the proxy server may transmit the retrieved object data to the requesting web client using the HTTP protocol. For example HTTP streaming may be used to transmit incrementally retrieved cached objects from the server to the requesting user.

At stage it is determined whether the current received portion is the last portion of the requested object. If the last portion has been received method terminates. If the last portion has not been received processing in method proceeds to step .

At stage the cache client forms a follow on request. The follow on request includes the incremental lookup indicator or parts thereof received from the cache. For example the follow on request can include in addition to the cache key corresponding to the object the content id and the lookup id last received from the cache. By including the incremental lookup indicator in the follow on request the cache client can indicate to the cache the instance of the object from which data is being read. The incremental lookup indicator may also indicate to the cache the portion of the object to be next retrieved.

At stage a read request for an object is received at the cache. The request may be received from a web server such as but not limited to a proxy server. The read request may include a cache key corresponding to the object to be retrieved.

At stage the cache searches for the requested object using the received cache key. The search may include searching a hash index using the received cache key. If the search does not find a requested object then a cache miss has occurred and the cache returns an error to the cache client.

If the requested object is found at stage then at stage it is determined whether the size of the requested object is greater than a threshold retrieval size. The threshold retrieval size may be a configured parameter representing a maximum size of an object to be returned as a single object. The threshold retrieval size may be specified in bits bytes or any other measure suited for representing the size of objects in computer systems. The threshold retrieval size may be preconfigured or in some embodiments may be dynamically determined based upon system conditions. The threshold retrieval size may also be configured differently based upon the type of object or other factors associated with the request.

If it is determined that the size of the requested object is less than or equal to the threshold retrieval size then at stage the cache returns the requested object in its entirety to the cache client. Having returned the entire object to the client the request servicing is complete and method terminates.

On the other hand if the size of the requested object is greater than the threshold retrieval size then the processing of method proceeds to stage . At stage a content id is determined for the requested object. The content id as described above may be generated by the cache to identify the particular instance of the object being returned to the cache client. In the event that a concurrent write to the same object occurs in the cache then the content id may be used to distinguish between the instances e.g. the instance from which the object was returned to the cache client and the instance created for the concurrent write of the object.

At stage the portion of the requested object to be returned to the cache client in response to the current request is determined. The portion to be returned may be based on a preconfigured size. For example a portion to be returned may have a size equal to the threshold retrieval size. In another embodiment the size of the portion may be determined dynamically based upon system factors such as but not limited to current traffic between cache and cache client and a priority level of the request. When the current traffic between the cache and the cache client is lower than a configured threshold the size of the portion may be set to a larger value and when the traffic is above the threshold the size of the portion may be set to a smaller value. If a priority level is indicated in the request the cache may configure larger sizes of portions for the highest priority requests and smaller sizes for the lower priority requests. The size of the portion to be returned may be configured so that the cache client can begin transmitting the response to the requesting web client with minimal latency and so that the cache client can continuously transmit e.g. stream the content of the object returned by the cache in portions to the requesting web client.

At stage a lookup id is determined for the portion to be returned. The lookup id in combination with the content id may be used to keep track of the next portions to be returned of a requested object. The lookup id may be a value e.g. in bytes of the total size of the portion s already returned. In another embodiment the lookup id may be a pointer to the location in cache memory of the portion to be returned next. According to an embodiment the lookup id may be generated so that the combination of the associated content id and lookup id is unique for requests over some time interval. For example a portion of the lookup id may represent an identifier for a HTTP session between the proxy server cache client and web client.

At stage the selected portion of the object is returned to the cache client from the cache. Along with the selected portion the content id and the lookup id are returned to the cache client. The content id as noted before identifies the particular instance of the requested object from which data is being returned. The lookup id as noted before is used to keep track of the next data portion of the requested object to be returned. In some other embodiments the cache may not return a lookup id to the cache client and may return the content id and an indication of whether more portions are to be transmitted.

The cache may maintain internal tables or other data structures to keep track of the next portions to be returned for objects that are being currently read by one or more cache clients on behalf of one or more web clients. Moreover internal data structures are maintained by the cache for providing an index to cached content e.g. cached objects . illustrates an exemplary internal data structures maintained by the cache for facilitating access to cached objects.

At stage the cache can update its internal data structures as necessary to reflect the status of the servicing of the current request by the cache client. For example a lookup id may be associated with the content id corresponding to the requested object and the lookup id may be updated to represent the next portion of the requested object to be returned. A reference count associated with the content id may be incremented if necessary. The reference count indicates that the content id associated with that reference count has an ongoing read operation. If a reference count is greater than 0 then the associated instance of the object represented by the content id may not be removed from cache.

At stage it is determined whether any more portions of the requested object are to be returned to the cache client. This determination may be made based upon the total size of the requested object and the total size of the portions that have already been returned.

If no more portions are to be returned then the retrieval of the requested object has been completed and method terminates. If it is determined that more portions are to be returned to the cache client then at stage the cache can wait for a follow on request to retrieve the next portion.

At stage a follow on request is received for the next portion of the requested object. According to an embodiment the follow on request includes the content id identifying the instance of the requested object being read and the lookup id which indicates the next portion to be read. According to another embodiment the follow on request includes the content id and relies upon the cache to use its internal tables to return the correct portions of the object.

At stage based upon the received follow on request the cache finds the next portion to be returned of the requested object. The cache can use one or more of the cache key the content id and the lookup id included with the follow on request to find the next portion to be returned. Stages can be repeated until all portions of the requested object have been returned from the cache to the cache client.

At stage an object or a portion of an object is received at the proxy server. The received object or the portion of the object may be streamed from a source such as an origin server to the proxy server to be served to a web client that requested the object. The object may be streamed for example using HTTP streaming from an origin server to a proxy server.

At stage the web proxy may determine whether the object should be cached. The determination as to which objects are cached and which are not can be configurable. The determination may be based upon one or more factors such as but not limited to the type of object identity of the requesting web client size of object network performance metrics and cache performance metrics. According to another embodiment the determination of whether to cache the object may be made dynamically based upon system conditions for example indicated by performance metrics for the network s and or caches.

At stage it is determined whether the size of the object is greater than a threshold insertion size. The threshold insertion size represents a minimum size of an object that is to be inserted in portions into the cache by the web proxy. The threshold insertion size may be a configured parameter or in some embodiments may be determined dynamically. According to an embodiment the threshold insertion size may be set at a value determined so that only a specified amount of memory in the proxy server is consumed by the object at any time. For example the proxy server can write any one or more portions of the object that causes the memory consumed by the object to exceed the specified limit.

If at stage it is determined that the size of the object is not greater than the threshold insertion size then method proceeds to stage . At stage the object is sent to the cache as a single object. After sending the object to the cache at stage method terminates.

If at stage it is determined that the size of the object is greater than the threshold insertion size then at stage a portion of the object to be transmitted to the cache is selected. The selected portion may be from the beginning of the object up to a predetermined size. The size of the portions may be configured or may be dynamically determined.

At stage an insert request is formed and transmitted to the cache. The insert request may include a header portion indicating that the message is an insert request the selected portion of the object and a flag indicating that more portions of the object are to be sent from the proxy server to the cache. The formed insert request is then transmitted to the cache.

At stage the proxy server waits for a response from the cache. According to an embodiment the waiting can be performed by a thread or process spawned in the proxy server while the rest of the proxy server continues to serve incoming client requests.

At stage a response to the insert request is received from the cache. The received insert response may include an incremental insert identifier containing a content id and an insert identifier insert id . The content id as noted above identifies the particular instance of the object in the cache. A separate copy i.e. an instance of the object may be created in the cache for the object being written. The insert id is a value generated by the cache and identifies the next portion of the object to be written.

At stage the proxy server determined whether there are more portions to be sent to the cache. According to an embodiment this determination may be based upon the status of the HTTP streaming of the object from the origin server or other source to the proxy server.

If at stage it is determined that no more portions of the current object are to be sent to the cache then the entire object has been transmitted to the cache and method can terminate.

If at stage it is determined that more portions of the current object are yet to be sent to the cache processing of method proceeds to stage . At stage the next portion of the current object to be sent to the cache is selected. The portion to be sent may be selected in a manner similar to the selection of the portion at stage .

At stage a follow on insert request is formed by the proxy server. The follow on insert request may include the selected portion of the object and information to identify the instance of the associated object in the cache. According to an embodiment the content id and the insert id received earlier from the cache are included in the follow on insert request. Moreover the follow on insert request can also include a flag to communicate to the cache whether more portions are to be expected.

At stage based on a determination whether more portions of the current object are to be transmitted to the cache in addition to the currently formed follow on insert request a flag in the follow on insert request is marked. The flag may have binary semantics to indicate based on its value either that more portions are to follow or that the current portion is the last portion of the object.

Stages may be repeated for each portion of the object until the entire object is transmitted from the proxy server to the cache.

At stage a request to insert an object is received at the cache. The insert request may be originated from the proxy server. The insert request can include a portion of an object to be inserted into the cache and a flag indicating that more portions of the object will follow. The insert request can also include information about the object such as but not limited to its URL. According to another embodiment the proxy server may calculate a key based upon information regarding the object such as its URL and include the key in the insert request.

At stage the received portion of the object is written to cache memory. When stage is reached for the first insert request regarding an update or insert of a particular object the memory location to be written to may be determined in accordance with any suitable technique for determining a location for writing a new object to the cache.

At stage it is determined whether more portions of the object are to be received to be cached. The insert request may include a flag or other field indicating whether more portions of the object are yet to be received by the cache.

If at stage it is determined that more portions are to be received then at stage an insert identifier insert id is generated for the object being incrementally inserted. The insert id indicates the portion of the object to be updated next. According to an embodiment the insert id may include a memory address where the next received portion of the object is to be written to. The insert id may also identify the location of the object in cache memory.

At stage a response to the insert request is generated at the cache. The response includes the insert id generated in stages . The response may also include other parameters associated with the object being updated such as its cache key or its content id if one is generated before the entire object is inserted. The generated response is sent to the cache client.

At stage a follow on request to incrementally insert the next portion of the object is received at the cache. The one or more follow on requests to incrementally insert an object includes the insert id returned by the cache after the previous portion was written to cache memory.

At stage the cache memory location where the received portion is to be written is determined. According to an embodiment the insert id received with the follow on request indicates the address to be written to. According to another embodiment the insert id is used to access a data structure maintained by the cache indicating the next location to be written to for each of the object instances that are currently being written to.

After determining the location to be written to the portion is written to the determined location at stage . Stages to are repeated until the respective portions corresponding to the entire object are received at the cache.

When at stage it is determined that the all portions have been received at the cache method proceeds to stage . At stage a content id is generated for the newly written object. According to another embodiment however the content id for the newly written object may have been generated earlier before the completion of receiving all portions. For example the content id may have been generated when the first request to insert a particular object is received before receiving any follow on insert requests for that object. As noted above the content id uniquely identifies an instance of an object in cache memory. The content id may include the address in the cache where the beginning of the object is stored or may be otherwise associated with the location whether the object is stored.

The internal data structures of the cache are updated in stage . Updating of internal data structures may include incorporating the newly inserted object into the cache so that a cache lookup with the corresponding cache key would access the newly inserted object.

When a newly inserted object is ready to be incorporated into the cache so that it is reachable through a cache lookup using a corresponding cache key there may or may not be other cached instances of the same object in the cache. For example if the object was previously inserted then an instance of that object may already be reachable through a corresponding cache key. When another instance of the object is present in the cache then before incorporating the newly written instance of the object into the cache the earlier instance may be removed from the cache. However a cached object may only be removed from the cache when there are no ongoing accesses to that object. As described above a reference count associated with each cached object may indicate the number of ongoing accesses to that object.

After the internal tables are updated to incorporate the newly inserted object into the cache the insert operation has completed and method terminates.

In order to enable concurrent lookup and insert of the same object the incremental cache may at times have more than one instance of a particular object. For example if the canonical instance of a particular object is being currently read by one or more users or clients a concurrent write to the same object would generate a second instance of that object in the cache. For example object B may be created in response to an insert operation and may be a second instance of the object corresponding to canonical object instance . During the incremental insert operation an insert id from an index of insert ids may maintain a mapping to object B . Object B may have its own content id that uniquely identifies it.

A content id is associated with each cached object and uniquely identifies the object. According to an embodiment each instance of an object has its own unique content id. As illustrated in a content id is associated with cached object and content id is associated with newly inserted object B . Content ids and uniquely identify their respective associated object instances in the cache.

Each canonical object instance can also be associated with a reference count that keeps track of the number of read operations that are currently being performed on that object. For example as illustrated in cached object may be associated with reference count .

Some of the canonical object instances may also be mapped from an index of lookup identifiers. For example as illustrated in lookup id and lookup id of an index of lookup ids may map to object A . In the example lookup id and lookup id correspond respectively to two ongoing concurrent lookup operations upon object A

One of ordinary skill in the art may appreciate that embodiments of the disclosed subject matter can be practiced with various computer system configurations including multi core multiprocessor systems minicomputers mainframe computers computers linked or clustered with distributed functions as well as pervasive or miniature computers that may be embedded into virtually any device.

For instance a computing device having at least one processor device and a memory may be used to implement the above described embodiments. A processor device may be a single processor a plurality of processors or combinations thereof. Processor devices may have one or more processor cores. 

Various embodiments are described in terms of this example computer system . After reading this description it will become apparent to a person skilled in the relevant art how to implement the invention using other computer systems and or computer architectures. Although operations may be described as a sequential process some of the operations may in fact be performed in parallel concurrently and or in a distributed environment and with program code stored locally or remotely for access by single or multi processor machines. In addition in some embodiments the order of operations may be rearranged without departing from the spirit of the disclosed subject matter.

As will be appreciated by persons skilled in the relevant art processor device may be a single processor in a multi core multiprocessor system such system operating alone or in a cluster of computing devices operating in a cluster or server farm. Processor device is connected to a communication infrastructure for example a bus message queue network or multi core message passing scheme. Computer system may also include display interface and display unit .

Computer system also includes a main memory for example random access memory RAM and may also include a secondary memory . Secondary memory may include for example a hard disk drive and removable storage drive . Removable storage drive may include a floppy disk drive a magnetic tape drive an optical disk drive a flash memory drive or the like. The removable storage drive reads from and or writes to a removable storage unit in a well known manner. Removable storage unit may include a floppy disk magnetic tape optical disk flash memory drive etc. which is read by and written to by removable storage drive . As will be appreciated by persons skilled in the relevant art removable storage unit includes a computer readable storage medium having stored thereon computer software and or data.

In alternative implementations secondary memory may include other similar means for allowing computer programs or other instructions to be loaded into computer system . Such means may include for example a removable storage unit and an interface . Examples of such means may include a program cartridge and cartridge interface such as that found in video game devices a removable memory chip such as an EPROM or PROM and associated socket and other removable storage units and interfaces which allow software and data to be transferred from the removable storage unit to computer system .

Computer system may also include a communications interface . Communications interface allows software and data to be transferred between computer system and external devices. Communications interface may include a modem a network interface such as an Ethernet card a communications port a PCMCIA slot and card or the like. Software and data transferred via communications interface may be in the form of signals which may be electronic electromagnetic optical or other signals capable of being received by communications interface . These signals may be provided to communications interface via a communications path . Communications path carries signals and may be implemented using wire or cable fiber optics a phone line a cellular phone link an RF link or other communications channels.

In this document the terms computer storage medium and computer readable storage medium are used to generally refer to media such as removable storage unit removable storage unit and a hard disk installed in hard disk drive . Computer storage medium and computer readable storage medium may also refer to memories such as main memory and secondary memory which may be memory semiconductors e.g. DRAMs etc. .

Computer programs also called computer control logic are stored in main memory and or secondary memory . Computer programs may also be received via communications interface . Such computer programs when executed enable computer system to implement the embodiments described herein. In particular the computer programs when executed enable processor device to implement the processes of the embodiments such as the stages in the methods illustrated by flowcharts of and data structures illustrated in discussed above. Accordingly such computer programs represent controllers of computer system . Where an embodiment is implemented using software the software may be stored in a computer storage medium and loaded into computer system using removable storage drive interface and hard disk drive or communications interface .

Embodiments of the invention also may be directed to computer program products including software stored on any computer readable storage medium. Such software when executed in one or more data processing device s causes a data processing device s to operate as described herein. Examples of computer readable storage mediums include but are not limited to primary storage devices e.g. any type of random access memory and secondary storage devices e.g. hard drives floppy disks CD ROMS ZIP disks tapes magnetic storage devices and optical storage devices MEMS nanotechnological storage device etc. .

The present invention has been described above with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof. The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries can be defined so long as the specified functions and relationships thereof are appropriately performed.

The foregoing description of the specific embodiments will so fully reveal the general nature of the invention that others can by applying knowledge within the skill of the art readily modify and or adapt for various applications such specific embodiments without undue experimentation without departing from the general concept of the present invention. Therefore such adaptations and modifications are intended to be within the meaning and range of equivalents of the disclosed embodiments based on the teaching and guidance presented herein. It is to be understood that the phraseology or terminology herein is for the purpose of description and not of limitation such that the terminology or phraseology of the present specification is to be interpreted by the skilled artisan in light of the teachings and guidance.

The breadth and scope of the present invention should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

