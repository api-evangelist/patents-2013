---

title: Dynamic detection and reduction of unaligned I/O operations
abstract: Detection and reduction of unaligned input/output (“I/O”) requests is implemented by a storage server determining an alignment value for data stored by the server within a storage system on behalf of a first client, writing the alignment value to a portion of the volume that stores the data for the first client, but not to a portion of the volume that stores data for a second client, and changing a location of data within the portion of the volume that stores the data for the first client, but not a location of data in the portion of the volume that stores data for the second client, to an alignment corresponding to the alignment value. The alignment value is applied to I/O requests directed to the portion of the volume that stores the data blocks for the first client after the location of the data blocks has been changed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08880842&OS=08880842&RS=08880842
owner: NetApp, Inc.
number: 08880842
owner_city: Sunnyvale
owner_country: US
publication_date: 20130607
---
This application is a continuation of application Ser. No. 12 950 392 filed Nov. 19 2010 which is hereby incorporated by reference.

At least one embodiment of the present invention pertains to data storage systems and more particularly to dynamically detecting and reducing unaligned write requests in a storage container that may have differing misalignments for different portions by computing updating and applying an alignment value to a portion of indirect block access requests.

A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever. The following notice applies to the software and data as described below and in the drawings hereto Copyright 2010 NetApp Inc. All Rights Reserved.

Various forms of network based storage systems exist today. These forms include network attached storage NAS storage area networks SAN s and others. Network based storage systems are commonly used for a variety of purposes such as providing multiple users with access to shared data backing up critical data e.g. by data mirroring etc.

A network based storage system typically includes at least one storage server which is a processing system configured to store and retrieve data on behalf of one or more client processing systems clients . The files may be stored in a storage system that includes one or more arrays of mass storage devices such as magnetic or optical disks or tapes by using a data storage scheme such as Redundant Array of Inexpensive Disks RAID . In a SAN context a storage server provides clients with block level access to stored data rather than file level access. Some storage servers are capable of providing clients with both file level access and block level access such as certain storage servers made by NetApp Inc. NetApp of Sunnyvale Calif.

Client devices may implement a hypervisor software layer. A hypervisor software layer also referred to as a virtual machine monitor allows the client processing system to run multiple virtual machines. A virtual machine is a software implementation of a machine i.e. a computer that executes instructions like a physical machine e.g. different operating systems different instances of the same operating system or other software implementations that appear as different machines within a single computer . Additionally data managed by a storage server on behalf of multiple client devices multiple client virtual machines within one or more client machines or multiple portions of a single client virtual machine may be stored within a single storage container or volume e.g. a LUN Logical Unit Number large NFS Network File System file or another equivalent logical division of storage .

A storage system often uses a fixed block size for all internal operations. For example WAFL Write Anywhere File Layout uses 4 KB 4096 bytes blocks for all operations as do client side file systems such as NTFS New Technology File System and ext4fs fourth extended filesystem . Since file systems usually start individual files on block boundaries application writers take advantage of a file system s block size and alignment to increase the performance of their input output I O operations for example always performing I O operations that are a multiple of 4 KB and always aligning these operations to the beginning of a file. Other file systems or applications however may use block boundaries of a different size e.g. 512 bytes .

When the block boundaries of a client side file system and a server side file system do not match I O operations may result in a partial write e.g. writing to only a portion of a storage system block. Partial writes can result from the starting block of a client side file becoming unaligned from the server side container blocks. Accordingly data stored on behalf of a client that could fit within a single server side container file block may end up overlapping more than one server side container file block. When the overlapping data is accessed via an I O operation it requires reading two server side container file blocks rather than just one. A write request to the unaligned client side file system block that spans two server side container file blocks e.g. overwriting the overlapping data includes preserving the contents of the server side container file blocks that are not being overwritten. As a result the two server side container file blocks are read e.g. into one or more buffers the client side block is written into corresponding portions of the buffered server side container file blocks preserving the contents that is not being overwritten and the updated server side container file blocks are written back to the storage system. In contrast the identical client side write operation would include a single write operation for blocks that are aligned.

One solution to the problem of unaligned I O operations is to obtain cooperation from the client side file system to ensure that there is alignment in all cases. There are two problems however with the cooperative approach 1 cooperation is usually performed within the context of the granularity of a single storage system volume which is not helpful if more than one virtual machine file system is stored within a single volume because each virtual machine may have a different misalignment and 2 if misalignment is discovered after the volume has been filled with data all of the data has to be moved from a misaligned position into an aligned position which can be a very computationally expensive operation e.g. in terms of I O operations for both the storage server and the client.

Detection and reduction of unaligned input output I O requests is implemented by a storage server determining an alignment value for data stored by the server within a storage system on behalf of a first client writing the alignment value to a portion of the volume that stores the data for the first client but not to a portion of the volume that stores data for a second client and changing a location of data within the portion of the volume that stores the data for the first client but not a location of data in the portion of the volume that stores data for the second client to an alignment corresponding to the alignment value. The alignment value is applied to I O requests directed to the portion of the volume that stores the data blocks for the first client after the location of the data blocks has been changed.

In the following detailed description of embodiments of the invention reference is made to the accompanying drawings in which like references indicate similar elements and in which is shown by way of illustration specific embodiments in which the invention may be practiced. These embodiments are described in sufficient detail to enable those skilled in the art to practice the invention and it is to be understood that other embodiments may be utilized and that logical mechanical electrical functional and other changes may be made without departing from the scope of the present invention. The following detailed description is therefore not to be taken in a limiting sense and the scope of the present invention is defined only by the appended claims. References in this specification to an embodiment one embodiment or the like mean that the particular feature structure or characteristic being described is included in at least one embodiment of the present invention. However occurrences of such phrases in this specification do not necessarily all refer to the same embodiment.

As set forth in further detail below embodiments of dynamic detection and reduction of unaligned I O operations include determining an alignment value for data stored by the server within a storage system on behalf of a first client writing the alignment value to a portion of the volume that stores the data for the first client but not to a portion of the volume that stores data for a second client and changing a location of data within the portion of the volume that stores the data for the first client but not a location of data in the portion of the volume that stores data for the second client to an alignment corresponding to the alignment value. For one embodiment the alignment value is applied to I O requests directed to the portion of the volume that stores the data blocks for the first client after the location of the data blocks has been changed.

Instead of moving all of the data blocks for an entire volume and cooperating with a client device to align the client side blocks with the moved storage system blocks for one embodiment a portion of data blocks within the volume can be moved and an alignment value can be applied to I O requests directed to that portion. Applying an alignment value to I O requests for only a portion of a volume e.g. an indirect block allows for adjustments to be made for differing misalignments for multiple virtual machines within a single volume. Additionally misalignments discovered or introduced after an initial storage system set up can be handled on an as needed basis with a lower computation cost because the application of an alignment value removes the need for client cooperation and only a portion of the volume is realigned.

A client may also refer one or more virtual machines. A hypervisor also referred to as a virtual machine monitor is a software layer that allows a processing system to run multiple virtual machines e.g. different operating systems different instances of the same operating system or other software implementations that appear as different machines within a single computer . The hypervisor software layer resides between the virtual machines and the hardware and or primary operating system of a machine and allows for the sharing of the underlying physical machine resources between different virtual machines. Accordingly references to multiple clients may refer to multiple virtual machines within the same client device a virtual machine in one client and a separate client device virtual machines from multiple client devices or a combination thereof.

The storage server in which an embodiment of dynamic detection and reduction of unaligned I O operations can be implemented is described in further detail below with reference to .

For one embodiment storage server is referred to as a network storage subsystem. A network storage subsystem provides networked storage services for a specific application or purpose. Examples of such applications include database applications web applications Enterprise Resource Planning ERP applications etc. e.g. implemented in a client . Examples of such purposes include file archiving backup mirroring etc. provided for example on archive backup or secondary storage server connected to a primary storage server . A network storage subsystem can also be implemented with a collection of networked resources provided across multiple storage servers and or storage units.

For example a first storage server functions as a primary provider of data storage services to client devices . Data storage requests from clients are serviced storage system organized as one or more storage objects. A secondary storage server takes a standby role in a protection relationship with the primary storage server replicating storage objects from the primary storage server to storage objects organized on disks of the secondary storage server. In operation the secondary storage server does not service requests from clients until data in the primary storage object becomes inaccessible such as in a disaster with the primary storage server such event considered a failure at the primary storage server . Upon a failure at the primary storage server requests from clients intended for the primary storage object are serviced using replicated data i.e. the secondary storage object at the secondary storage server .

It will be appreciated that in other embodiments the network storage system may include more than two storage servers . In these cases protection relationships may be operative between various storage servers such that one or more primary storage objects from one storage server may be replicated to another storage server other than the secondary storage server described above. Secondary storage objects may further implement protection relationships with other storage objects such that the secondary storage objects are replicated e.g. to tertiary storage objects to protect against failures with secondary storage objects. Accordingly the description of a single tier protection relationship between primary and secondary storage objects of storage servers should be taken as illustrative only.

The storage server is further coupled to a storage system . The storage system includes a set of mass storage devices. The mass storage devices in the storage system may be for example conventional magnetic disks solid state disks SSD magneto optical MO storage or any other type of non volatile storage devices suitable for storing large quantities of data. The storage server manages the storage system for example by receiving and responding to various I O requests from the client s directed to data stored in or to be stored in the storage system . Although illustrated as separate components for one embodiment the storage server and storage system may be a part of housed within a single device.

As used herein a data block refers to a fixed size block e.g. 4 KB that stores client side data. An indirect block refers to a block that stores locations of data blocks. For example indirect blocks in WAFL store locations for 255 510 512 OR 1024 data blocks. As a result a data block stored by the storage server on behalf of a client is referenced via an address maintained on the client side and mapped directly or indirectly to an address on the storage side.

For one embodiment storage system is managed as a RAID array. RAID is a data storage scheme that divides and replicates data among multiple hard disk drives e.g. in stripes of data. Data striping is the technique of segmenting logically sequential data such as a single file so that segments can be assigned to multiple physical devices hard drives. Redundant parity data is stored to allow problems to be detected and possibly fixed. For example if one were to configure a hardware based RAID 5 volume using three 250 GB hard drives two drives for data and one for parity the operating system would be presented with a single 500 GB volume and the exemplary single file may be stored across the two data drives.

It will be appreciated that certain embodiments of the present invention may include solid state memories e.g. flash storage devices constituting storage system . For example storage system may be operative with non volatile solid state NAND flash devices which are block oriented devices having good random read performance i.e. random read operations to flash devices are substantially faster than random write operations to flash devices. Data stored on a flash device is accessed e.g. via I O requests in units of blocks which in the present embodiment are 4 KB in size although other block sizes e.g. 2 KB 8 KB etc. may also be used. For one embodiment 4 KB as used herein refers to 4 096 bytes. For an alternate embodiment 4 KB refers to 4 000 bytes.

For one embodiment the storage server is operative as multiple functional components that cooperate to provide the network based storage system. To that end the storage server is organized as one or more processors a memory a network adapter a storage adapter and a random access memory RAM coupled to a bus system. The bus system shown in is an abstraction that represents any one or more separate physical buses and or point to point connections connected by appropriate bridges adapters and or controllers.

The processor s are the central processing units CPUs of the storage server and thus control its overall operation. The processor s accomplish this by executing software stored in memory . For one embodiment individual adapters e.g. network adapter and storage adapter each include a processor and memory for carrying out respective module operations.

The memory includes the main memory of the storage server . The memory stores among other things the storage server s operating system which according to one embodiment includes instructions executed by processor s to implement an embodiment of detection and reduction of unaligned I O operations as described herein.

The network adapter includes functionality that enables the storage server to connect to clients via network and may include protocol components such as a Media Access Control MAC layer Common Internet File System CIFS Network File System NFS Internet Protocol IP layer Transport Control Protocol TCP layer User Datagram Protocol UDP layer and other protocols known in the art for facilitating such connectivity. In contrast the storage adapter may connect to one or more storage devices within storage system e.g. via cluster switching fabric and may be operative to service client I O requests within the storage system . For one embodiment the storage adapter includes storage access components such as a storage abstraction layer supporting multi protocol data access e.g. Common Internet File System protocol the Network File System protocol and the Hypertext Transfer Protocol a storage layer implementing storage protocols e.g. RAID protocol and a driver layer implementing storage device protocols e.g. Small Computer Systems Interface protocol for carrying out operations in support of storage access operations. Illustratively a storage abstraction layer e.g. file system of the storage adapter divides the physical storage of storage system into storage objects e.g. pages files etc. . Requests received by the storage server e.g. via network adapter may thus include storage object identifiers e.g. a byte offset or logical block address LBA to indicate a storage object on which to carry out the request. For one embodiment the storage adapter performs dynamic detection and reduction of unaligned I O operations as described further below. Alternatively dynamic detection and reduction of unaligned I O operations is performed by the network adapter processor s or combination of one or more of the processor s network adapter and storage adapter .

The storage server also includes a RAM . For one embodiment a cache is implemented within the RAM . Alternatively the cache may be implemented one or more other volatile and or non volatile memories.

It will be readily apparent that input output devices such as a keyboard a pointing device and a display may be coupled to the storage server . These features have not been illustrated for the sake of clarity.

Multi protocol engine includes a media access layer of network drivers e.g. gigabit Ethernet drivers that interface with network protocol layers such as the IP layer and its supporting transport mechanisms the TCP layer and the User Datagram Protocol UDP layer . A file system protocol layer provides multi protocol file access and to that end includes support for the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A Virtual Interface VI layer implements the VI architecture to provide direct access transport DAT capabilities such as Remote Direct Memory Access RDMA as required by the DAFS protocol . An Internet Small Computer System Interface iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a Fibre Channel FC driver layer receives and transmits block access requests and responses to and from the storage server. In certain cases a Fibre Channel over Ethernet FCoE layer not shown may also be operative in multi protocol engine to receive and transmit requests and responses to and from the storage server . The FC and iSCSI drivers provide respective FC and iSCSI specific access control to the blocks and thus manage exports of Logical Unit Numbers LUN s to either iSCSI or FC protocol or alternatively to both iSCSI and FC protocol when accessing blocks on the storage server .

The storage operating system also includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on storage devices implementing secure storage e.g. storage system . Information may include data received from a client in addition to data accessed by the storage operating system in support of storage server operations such as program application data or other system data. Preferably client data may be organized as one or more logical storage objects e.g. volumes that comprise a collection of storage devices cooperating to define an overall logical arrangement. In one embodiment the logical arrangement may involve logical volume block number vbn spaces wherein each volume is associated with a unique vbn.

File system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustratively embodied as e.g. a SCSI target module . SCSI target module is generally disposed between drivers and file system to provide a translation layer between the block LUN space and the file system space where LUN s are represented as blocks. File system illustratively implements the WAFL file system having an on disk format representation that is block based using e.g. 4 KB blocks and using a data structure such as index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . File system uses files to store metadata describing the layout of its file system including an inode file which directly or indirectly references points to the underlying data blocks of a file. For one embodiment the file system performs dynamic detection and reduction of unaligned I O operations as described further below. Alternatively dynamic detection and reduction of unaligned writes is implemented its own layer or performed by another layer in cooperation with the file system .

Operationally a request from a client is forwarded as a packet over the network and onto the storage server where it is received at a network adapter e.g. network adapter . A network driver such as layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to file system . There file system generates operations to load retrieve the requested data from the disks if it is not resident in core i.e. in memory . If the information is not in memory file system accesses the inode file to retrieve a logical vbn and passes a message structure including the logical vbn to the RAID system . There the logical vbn is mapped to a disk identifier and device block number e.g. disk dbn and sent to an appropriate driver e.g. SCSI of disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the storage server . Upon completion of the request the storage server returns a reply to the client over the network .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the storage server may alternatively be implemented in hardware. That is in an alternate embodiment a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by the storage server in response to a request issued by a client . It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

When implemented in a cluster data access components of the storage operating system may be embodied as D module for accessing data stored on disk. In contrast multi protocol engine may be embodied as N module to perform protocol termination with respect to a client issuing incoming access over the network as well as to redirect the access requests to any other N module in the cluster. A cluster services system may further implement an M host to provide cluster services for generating information sharing operations to present a distributed file system image for the cluster. For instance media access layer may send and receive information packets between the various cluster services systems of the nodes to synchronize the replicated databases in each of the nodes.

In addition a cluster fabric CF interface module CF interface modules A B may facilitate intra cluster communication between N module and D module using a CF protocol . For instance D module may expose a CF application programming interface API to which N module or another D module not shown issues calls. To that end CF interface module can be organized as a CF encoder decoder using local procedure calls LPCs and remote procedure calls RPCs to communicate a file system command to between D modules residing on the same node and remote nodes respectively.

Illustratively the storage operating system issues a read or write command to a storage device controller e.g. device controller through disk driver system for accessing a physical storage object e.g. disk using the disk identifier mapped from the logical vbn by RAID system .

When the client side file is not aligned with the storage system blocks a request to read or write a block will suffer a performance set back as compared to an aligned system. For example when blocks are not aligned the client side block that starts at 4096 spans two blocks within the storage system volume the storage system blocks that start at 4096 and 8192. Given that the unaligned client side block only partially overlaps each of two storage system blocks the storage server cannot simply overwrite a single block of data. A write request directed to the client side block that starts at 4096 results in reading two blocks and preserving the contents of the storage system blocks that are not being overwritten while writing the new data to the portions that are being overwritten. The write request for a single client side block results in two reads and two writes which is computationally more expensive than the single write that results in the corresponding aligned system.

Adjustments for misalignment have typically been performed within the context of the granularity of a single storage system volume. Once a volume has been filled with data moving all of the data from a misaligned position into an aligned position can be a very computationally expensive operation for the storage server and the client . Additionally an attempt to move an entire volume of data blocks by a single value e.g. a number of bytes is not helpful if more than one client is stored within a single volume because each client may have a different misalignment.

Instead of moving all of the data blocks for an entire volume and cooperating with the client to align the client side blocks with the moved storage system blocks for one embodiment a portion of data blocks within the volume can be moved and an alignment value can be applied to I O requests directed to that portion. Applying an alignment value to I O requests for only a portion of a volume e.g. an indirect block allows for adjustments to be made for differing misalignments for multiple virtual machines within a single volume. Additionally misalignments discovered or introduced after an initial storage system set up can be handled on an as needed basis with a lower computation cost because the application of an alignment value removes the need for client cooperation and only a portion of the volume is realigned.

For one embodiment alignments of data blocks are performed at a granularity of an indirect block and an alignment value is written to each indirect block. When the location of a data block is looked up in an indirect block this alignment value will be applied to the data block address before lookup. For one embodiment the alignment value is refers to a multiple of 512 bytes e.g. an alignment value of zero would be the same as the normal case without the alignment value an alignment value of one refers to 512 bytes an alignment value of two refers to 1024 bytes etc. Alternatively the alignment value refers directly to a number of bytes or a different multiple of bytes.

Similar to the I O operation in references a pair of 4 KB buffers. The first buffer starts at volume byte offset 1049088 and contains 3584 bytes. The second buffer starts at volume byte offset 1052672 and contains 512 bytes. The alignment of the data blocks in the indirect block and adjustment of the alignment value from zero to seven is performed without requiring cooperation from the client . Accordingly instead of using the volume byte offset 1049088 as done above the storage server adds the alignment value to the offset 1049088 3584 1052672. For one embodiment the alignment value is added to the access byte offset provided by the client. Alternatively the alignment value is subtracted from the client access byte. For yet another alternative embodiment another or an additional mathematical operation is performed to the alignment value or client access byte to determine the volume byte offset.

In the example illustrated in the storage server performs the read starting at byte offset 1052672. Byte offset 1052672 is 1044480 4096 4096 so the read references FBN 257. The storage server copies the first 3584 bytes of this data block into the first buffer and the last 512 bytes into the second buffer. As a result of aligning the data blocks for the indirect block and accessing them by way of the alignment value the storage server placed 4096 bytes of user data into the buffers but only read 4096 bytes from disk.

The alignment value is used when mapping a FBN in an indirect block to a physical or disk data block number and does not have to be known to other parts of the file system. In particular indirect block lookups are done the same way with or without a non zero alignment value. For example the alignment value can be applied after the data block is determined from the indirect block. As a result embodiments are easily retro fit onto existing file systems.

The alignment value for each indirect block is not known when the file system is created and even when it is known the alignment value can change over time e.g. if a client side file system is deleted and recreated with a different misalignment . To determine a new or updated alignment value this storage server tracks I O operations associated with each indirect block. For every I O operation a count for an alignment value is incremented in a histogram table or other data structure based on a byte distance between an offset value from the client and the offset for the start of a data block the same data block or the next data block depending on the direction of the shift .

For each 4096 byte access to the indirect block the storage server increments the alignment value of one of the table entries. For example if the 4096 byte access is aligned on a 4096 byte boundary we increment a table entry that corresponds to an alignment value of zero. If the access is offset by 512 bytes from the data block boundary the storage server increments a table entry that corresponds to an alignment value of one. For all I O requests that are not the same length as the data block e.g. 1 KB accesses in the middle of a 4 KB data block the storage server increments a table entry to indicate that the indirect block will not benefit from a change in the alignment value.

The storage server determines whether a threshold has been reached for the table at block . For example after some number of I O requests the storage server examines the table entries to determine a new value for the alignment. Alternatively the storage server examines the table entries when a particular entry reaches a maximum value after an expiration of period of time or a combination thereof. For example the indirect block illustrated in initially uses the alignment value of 0 and subsequently in uses the alignment value of 7. As an example the table illustrated in illustrates the tracking of I O requests directed to the indirect block while the alignment value was set at 0. Once a threshold has been reached e.g. an alignment value has reached a threshold of 16 the storage server uses the alignment value in this example the alignment value of 7 with the highest value as the new alignment value.

For one embodiment the storage server uses the table entry with the highest value as the new alignment value. For one embodiment if the entry indicates that the indirect block will not benefit from a change in the alignment value the storage server does not change the alignment value. For example if more than one entry share or are above a particular value or if there is not threshold difference between the two highest values the storage server does not change the alignment value.

If the table threshold has been reached the storage server changes the alignment of the indirect block by reading all of the storage system data blocks in the indirect block at block changing the alignment value of the indirect block at block and writing all of the data blocks back to disk using the new alignment at block . In this way as the optimal alignment value changes over time the storage server updates the alignment value in each indirect block individually as needed.

If the table threshold has not been reached or after the storage server has changed the alignment of the indirect block the storage server looks up the FBN with the alignment value applied and performs the I O request at block .

When a file system is new it is not desirable to start with an alignment value of zero and then have to change the alignment value later for each indirect block. Neighboring indirect blocks are often a part of the same virtual machine and therefore it would be beneficial for a new indirect block to start with the alignment value of a neighboring indirect block. For one embodiment the storage server can often avoid this subsequent change of the alignment value of a new indirect block from zero to another value by initially using the alignment value of a nearby indirect block whenever beginning to use a new indirect block.

For one embodiment when consecutive indirect blocks are of the same alignment an extra pointer in one indirect block can point to the same disk block as the first or last depending on direction the blocks are shifted pointer in the neighboring indirect block. This avoids partial I O operations when dealing with consecutive indirect blocks that have the same alignment values.

For one embodiment the first and last data block in the indirect block are handled differently from the remainder of data blocks because with an alignment value applied the first and last data blocks may be referenced by the previous or subsequent indirect block. For one embodiment an indirect block has references to all of the data blocks it is expected to reference which includes modifying the legacy indirect block. First an additional pointer is added to the indirect block to reference data that comes before a newly aligned first data block. Second when the last data block in an indirect block is referenced the data in that block may be referenced by the next indirect block. Depending on the file system this may mean that reading or writing the last block in an indirect block may require reading or writing two indirect blocks and two data blocks similar to the partial write case outlined above. This case however happens on indirect block boundaries which only occurs a small percentage of the time because indirect blocks reference hundreds of data blocks. This border block scenario is described further with respect to .

For one embodiment the data blocks in each indirect block are preceded by at least a portion of a block that is preferably not used or accessed by the client to provide spacing between indirect blocks. This spacing region provides room for misalignment and the shifting of blocks due to performing an alignment. For one embodiment should a client access this spacing region the I O request will be directed to data split between the last data block of the one indirect block and the first data block of the next indirect block. For example in reading client block results in reading 2 KB from the second half of FBN 254 and 2 KB from the first half of FBN 255. The impact of an overlapping data block on the border of an indirect block and this spacing region after alignment will be discussed further with respect to .

As described above for one embodiment the data blocks in each indirect block are preceded by at least a portion of a block that is preferably not used or accessed by the client. Additionally the shifting of data blocks resulting from the alignment of the first VM s indirect block creates an additional at least a portion of a block that is preferably not used by the client illustrated as shaded portions adjacent to FBN 1 and FBN 509 . For one embodiment while nearly all of the data blocks of the first VM s indirect block are now aligned the last block remains split between the last data block of the first VM s indirect block and the second VM s block because of these unused shaded block portions. In this scenario no improvement has been gained for I O accesses that are directed to the last data block of the newly aligned indirect block when I O access to the last data block are permitted . For an alternative embodiment when the shifting of data blocks after alignment creates an amount of space that is equal to the amount of space consumed by the last data block of the indirect block overlapping into the subsequent indirect block I O requests after the alignment are directed to the last data block of the indirect block including the newly created space and no longer require accessing the subsequent indirect block.

Thus embodiments of detecting and reducing unaligned I O requests are implemented in a computer system as described herein. In practice the method may constitute one or more programs made up of computer executable instructions. The computer executable instructions may be written in a computer programming language e.g. software or may be embodied in firmware logic or in hardware circuitry. The computer executable instructions to implement a persistent cache may be stored on a machine readable storage medium. A computer readable storage medium or a non transitory computer readable storage medium as the terms are used herein include any mechanism that provides i.e. stores and or transmits information in a form accessible by a machine e.g. a computer network device personal digital assistant PDA manufacturing tool any device with a set of one or more processors etc. . A non transitory computer readable storage medium as the term is used herein does not include a signal carrier wave etc. The term RAM as used herein is intended to encompass all volatile storage media such as dynamic random access memory DRAM and static RAM SRAM . Computer executable instructions can be stored on non volatile storage devices such as magnetic hard disk an optical disk and are typically written by a direct memory access process into RAM memory during execution of software by a processor. One of skill in the art will immediately recognize that the terms machine readable storage medium and computer readable storage medium include any type of volatile or non volatile storage device that is accessible by a processor. For example a machine readable storage medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. .

Although the present invention has been described with reference to specific exemplary embodiments it will be recognized that the invention is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense.

Therefore it is manifestly intended that this invention be limited only by the following claims and equivalents thereof.

