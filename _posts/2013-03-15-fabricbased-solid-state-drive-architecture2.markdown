---

title: Fabric-based solid state drive architecture
abstract: Embodiments of apparatus, methods and systems of solid state drive are disclosed. One embodiment of a solid state drive includes a non-blocking fabric, wherein the non-blocking fabric comprises a plurality of ports, wherein a subset of the plurality of ports are each connected to a flash controller that is connected to at least one array of flash memory. Further, this embodiment includes a flash scheduler for scheduling data traffic through the non-blocking fabric, wherein the data traffic comprises a plurality of data packets, wherein the flash scheduler extracts flash fabric header information from each of the data packets and schedules the data traffic through the non-blocking fabric based on the extracted flash fabric header information. The scheduled data traffic provides transfer of data packets through the non-blocking fabric from at least one array of flash memory to at least one other array of flash memory.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09606863&OS=09606863&RS=09606863
owner: SMART High Reliability SOlutions, LLC
number: 09606863
owner_city: Newark
owner_country: US
publication_date: 20130315
---
This patent application is a continuation in part of U.S. patent application Ser. No. 13 280 206 by Ajoy Aswadhati filed on Oct. 24 2011 and entitled Solid State Drive Architecture which claims priority to provisional patent application 61 455 712 filed on Oct. 25 2010 and herein incorporated by reference.

The present disclosure relates generally to data storage systems and more particularly to a solid state based storage system and methods of use and operation.

Solid State Drives SSDs typically present themselves to an OS operating system such as Windows Linux Unix as a block storage device. Once the operating system recognizes the drive it communicates to the SSD through a driver provided by for example the SSD vendor or utilizing well known protocols such as Advanced Host Control Interface AHCI . The host OS formats the drive into a contiguous Logical Block Address LBA space depending on the advertised capacity of the drive. LBA data transfer size is usually either 512 bytes or multiples of 512 bytes such as 1 Kbytes 2 Kbytes or 4 Kbytes. Once the formatting is done the host OS creates a file system and stores data on the drive and retrieves it using the Logical Block Addressing mechanism. It primarily uses the following basic commands LBA Write. A block of data equal to the LBA transfer size is sent along with the write command to be stored into non volatile memory. LBA Read. A block of data equal to the LBA transfer size is read from non volatile memory and transferred to the host. LBA Erase Also known as TRIM command in Serial ATA protocol . This is a command only transfer from the host that tells the SSD that the block of data is no longer needed by the host.

A typical NAND flash architecture is shown in . illustrates a single die and a NAND flash package can incorporate multiple such dies. A NAND die is essentially organized as multiple blocks and each block contains multiple pages. Pages can be written sequentially within a block and accessed randomly for reads. Pages cannot be overwritten without erasing an entire block containing those pages. Pages can be written only if a block has been erased earlier. To build a large capacity drive a plurality of NAND devices is connected to a flash controller. The plurality of devices can be individually accessed by the flash controller.

Embodiments presented herein illustrate a flash based Solid State Drive SSD SSD controller and SSD controller hardware and software components. At least some embodiments of the hardware component and software component provide high throughput scalability reliability and availability in an SSD form factor. Some components of the overall SSD controller architecture can be implemented either in hardware e.g. in an application specific integrated circuit ASIC or field programmable gate array FPGA technology or implemented in software. This permits a wide range of implementation choices based on desired cost and performance tradeoff.

It is understood that as used herein the term flash refers to without exception any type of non volatile memory. Additionally as used herein the term eCPU is synonymous with the term eHost .

Host communication with a typical SSD is shown in the Software Hardware Stack. To map the LBA s presented by the host to store retrieve erase data in the flash array a flash translation layer FTL is provided by the flash controller. In addition to mapping host LBAs into Flash block and page addresses at least some embodiments of the SSD provides the following capabilities in addition to normal house keeping.

The following terms are used throughout the description to refer to storage addressing organizational concepts for one exemplary embodiment although other embodiments can employ other storage architectures.

HOST The CPU central processing unit subsystem to which SSD controller appears as a slave device. The solid state drive SSD is connected to the host CPU by attaching to any host interfaces

eCPU embedded CPU or eHost At least some embodiments of the SSD include an embedded processor eCPU inside the SSD controller. Embodiments include a single processor core with multi threading capability or multiple cores and combinations of these.

LUN logical unit number The LUN is the minimum unit that can independently execute commands and report status. There are one or more LUNs per target flash controller. A LUN typically maps to a single die in a package. Most high density flash devices include multiple dies per device package.

LBA Logical Block Address The LBA is an addressing mechanism typically used by hosts to address a block of data to write or read from storage medium. For example hard disk drives or solid state drives use an LBA size of 512 bytes. In PCI e based storage and xHCI based systems there is flexibility to increase the LBA size.

LBN Logical Block Number The eCPU or eHost treats the entire collection of flash arrays segmented as an array of blocks. The block size in bytes is same as the NAND flash block size. Each array will have specified number LBN of addressable blocks based on device configuration.

An Embedded Processor eCPU acts as the central unit to receive and process commands from the host CPU. The eCPU can include a plurality of such CPUs to support multiple threading or a single CPU that can support multithreaded programming.

A Host Interface is a generic interface that is agnostic to various industry standard interfaces such as PCIe USB SATA SAS etc. The Host Interface isolates the internal functionality of the SSD controller from the specifics of the host channel enabling the same fundamental core architecture to be targeted to any industry standard bus with relatively minor modifications.

A Flash Global DMA Controller with non blocking fabric referred to as non blocking fabric in moves data between the eCPU the Host DMA Interface a DRAM DMA Memory Controller and a plurality of Flash Controllers . A non blocking switch fabric within the Flash Global DMA Controller within Non Blocking Fabric permits concurrent data movement between modules listed . This feature can provide significant advantages in a given embodiment as compared to prior art serial processed solid state disk designs. To further reduce latency the non blocking switch fabric can utilize a cut through design where data can be transported across the fabric even before the entire packet is ready at its ingress interface.

A Flash Global DMA Controller Scheduler referred to as a flash scheduler in handles all requests for data movement between the Flash Controllers the DRAM Controller connected to volatile DRAM eCPU and the Host DMA Interface . The scheduler considers the priorities of various requests and grants access to the requesting entities based on the data movement within the fabric and the priority of the requests.

A Flash Translation Layer runs as a process on eCPU . The Flash Translation Layer maps LBAs to appropriate Flash controller array LUN Block Pages on the flash devices. The detailed signaling and mapping of the translated commands is done by the flash controller. For example Address and Data need to be multiplexed on the same bus for accessing a flash device. This is done by the flash controller and beyond the scope of the Flash Translation Layer software. This partitioning also makes the design scalable and portable with minimal to practically no impact on the software. For targeting future nonvolatile memory devices the bulk of the architecture can be preserved by changing only the NAND specific part of the Flash Controller.

All the blocks in the SSD controller have internal registers that are accessible by the eCPU . A Register Access block provides a mechanism for the eCPU to read and write to these registers. As shown in eCPU uses the Register Access block for any access to registers in the controller. Each block has a specified address range that allows it to recognize register accesses to that block.

Each Flash Controller talks directly to the flash devices in a Flash Array with an ability to address every LUN individually in the array. In one embodiment the Flash Controllers can access an SLC array or an MLC array. The number of flash controllers provided in the SSD depends on the capacity and performance needed.

A typical data flow to from host is as follows. When a host issues an LBA read the command is sent to the eCPU . The eCPU first checks whether the requested data is in cache e.g. saved in DRAM . When the requested data is in cache then a DMA transfer of this data is initiated by the eCPU to the host interface buffer. When the requested data is in flash then the eCPU sends a command to the correct flash controller to fetch the data to cache. The eCPU DRAM controller host interface and other flash controllers can continue to perform other tasks while the flash controller retrieves the data. Once data is moved to cache cache not shown by the flash controllers the eCPU gets a response to its earlier command indicating that the data is available. The eCPU then initiates a DMA transfer from cache to the host interface buffer. When a host issues an LBA write the host interface sends the command to the eCPU and sends the data to a write cache memory. When the flash translation layer decides to move the data from the write cache the eHost sends a command to the appropriate flash controller to retrieve the data from the write cache and move the data to a flash location specified in the command.

The flash scheduler reads the flash fabric headers that are presented to the non blocking fabric D to D N . The flash scheduler extracts the relevant information that is critical to the data movement between the various ports of the non blocking fabric that is expected to pass through the non blocking fabric . Examples of the data used in decision making are the Destination id Priority and external events such as Power Down. Based on the required usage model for the SSD various scheduling algorithms can be implemented in arriving at a final decision of which input ports can be connected to which output ports of the Non blocking fabric . This connectivity decision is sent via the Fabric Interface D .

The data traffic includes a plurality of data packets. The flash scheduler extracts flash fabric header information from each of the data packets and schedules the data traffic through the non blocking fabric in C in based on the extracted flash fabric header information. As previously described the scheduled data traffic provides transfer of data packets through the non blocking fabric from at least one array of flash memory to at least one other array of flash memory .

At least some embodiments of the flash scheduler extract priority information of each data packet from the flash fabric header information of the data packet and wherein the flash scheduler schedules high priority data packets before low priority packets enabling data packet movement through the non blocking fabric . The priority scheduling is enabled through the use of plurality of queues in cache not shown . At least some embodiments of the enabled data packet movement include at least one of reading and writing data packets from or to arrays of flash memory. Further at least some embodiments of the enabled data packet movement includes at least one of reading and writing data packets from or to arrays of flash memory and at least one of receiving and providing data packets to at least one of an embedded central processing unit eCPU and volatile memory.

For at least some embodiments the priority information of each of the data packet is influenced by events external to the flash scheduler. One example of an external event includes a power failure. For this example writes are elevated in priority which can include preempting higher priority reads.

For at least some embodiments if a scheduled data packet fails then the flash scheduler provides an indication of the failure to an eCPU connected to a port of the non blocking fabric. This indication of failure beneficially allows the eCPU to eliminate polling. Eliminating polling in the controller frees up the eCPU to perform other cycles such as monitoring the health of the SSD reducing latency to incoming requests from the Host system. Examples of failure of a scheduled packet can be a flash page write not completing successfully corrupted data read from a flash page and other such errors that occur when interfacing to flash devices.

As will be described embodiments of the flash fabric header information of the data packets include commands associated with the data packets. For example one embodiment of the flash fabric header information includes a write cancel operation bit that directs the flash scheduler to cancel a write of the data packet to an array of flash memory through the use of a bypass queue. Another embodiment of the flash fabric header information includes a write immediate operation bit that directs the flash scheduler to increase the priority of the data packet. Another embodiment of the flash fabric header information includes a multiple queues operation bit that directs the flash scheduler to put the data packet in multiple queues. Another embodiment of the flash fabric header information includes a no retry operation bit that directs the flash scheduler to avoid retries of operations of the data packet read from flash devices. Another embodiment of the flash fabric header information includes stacked header information wherein the flash schedule performs multiple operations based on the stacked header information autonomous from an embedded central processing unit eCPU connected to the first port of the non blocking fabric.

Additionally embodiments of the SSD include a data scrambler and a data pattern compression engine wherein the data pattern compression engine parses data packets recognizing patterns and compresses the data packets based on the recognized patterns. As will be described other embodiments of the SSD include a circular buffer for tracking alerts and errors of operation of the flash scheduler .

Embodiments of operation the SSD of provide a method of controlling data packets of arrays of flash memory. The method includes extracting by a flash scheduler flash fabric header information from each data packet of a data traffic wherein the data traffic comprises of a plurality of data packets. Further the method includes scheduling by the flash scheduler the data traffic through a non blocking fabric of a solid state drive based on the extracted flash fabric header information wherein the non blocking fabric comprises a plurality of ports and wherein a subset of the plurality of ports are each connected to a flash controller that is connected to at least one array of flash memory and wherein at least one of the plurality of the ports of the non blocking fabric comprises electrical connections to at least an embedded central processing unit eCPU . The scheduled data traffic provides transfer of data packets through the non blocking fabric from at least one array of flash memory to at least one other array of flash memory based on the flash header information.

A flash controller Finite State Machine FSM reads the command headers in the write queue and controls an ECC Error Correction Coding and scrambler and a flash interface controller to move write data from the write FIFO to the correct LUN move read data from the correct LUN to the read FIFO perform integrity checks and read retries attach headers to results placed in the read FIFO and request movement of data from the read FIFO to the proper fabric destination. The read FIFO can include multiple FIFO constructs with different priorities for each FIFO different FIFOs for each command response type and or different FIFOs for each response from LUN.

The ECC and Scrambler block performs error correction and detection with the scrambler improving the randomness of data written to flash. This block is preferably instantiated as part of each Flash Controller as shown in . Alternately ECC and or scrambling can be a centralized resource shared between multiple Flash Controllers based on cost and performance required by the SSD.

As previously described the Non blocking Fabric switches data and command packets internally between the attached SSD controller blocks. Each packet contains a Flash Fabric Header FFH that describes its contents destination tasks performed or to be performed status etc. When the host interacts with the SSD controller for data exchange the fundamental operations are an LBA Write data LBA Read request and LBA delete. The eCPU on the SSD controller examines these requests and creates a Flash Fabric Header FFH e.g. with the following structure in one embodiment. In other embodiments field definition and size can be extended to add more distributed functionality and scalability. The bit locations and fields are for one exemplary implementation and could change during implementation for optimal use. The header definition contained in Table 1 does however show various features of an exemplary embodiment of a header and their manner of implementation through header use.

One feature that can be implemented efficiently with the header mechanism is mirroring otherwise known as RAID RAID is an acronym for Redundant Array of Independent Disks . Use of the header mechanism facilitates replication of data without eCPU intervention. This is useful for RAID implementations where a physical copy of the data is stored in a different area e.g. stored on flash devices served by two different flash controllers. The Next Header bit can indicate that there is an additional Flash Fabric Header attached contiguous to the first header. This second header can specify the second destination for this data. The switch fabric in conjunction with the scheduler can duplicate the data to be mirrored along with appropriate header changes so that data is simultaneously and independently stored into different destinations. Without this mechanism the CPU needs to make physical copies which is compute intensive and requires potentially twice the compute time to support RAID.

A Flash Fabric Header is created by the eHost for every request originated by the Host. This header is then prepended optionally to the payload for the case of LBA Writes . This process creates the Flash Fabric Packet. Once a packet is created and ready for transmission to its destination the eHost sends the Flash Fabric Header to the Global Scheduler. The Global Scheduler receives all such requests from multiple sources and schedules traffic to be switched through the non blocking fabric asynchronously based on fabric availability the priority encoded in the packet and other criteria. There is flexibility in accommodating various efficient scheduling algorithms. Further to provide optimal flexibility and configurability the Global Scheduler can be implemented in software either as an independent thread or assigned to a dedicated CPU core. In some embodiments the global scheduler is implemented in hardware.

In the architecture described below only the data that needs to be processed by the eCPU will be sent to the eCPU and the payload will be kept in buffers until the necessary action is taken. This saves CPU cycles in not having to move and copy payload data that is not useful in decision making.

Upon reception of this packet by the flash controller the flash controller will attempt to complete the write to the specified Channel LUN Plane Block Page location and place the packet write status in the status field of a response Flash Fabric Header. Note that the flash controller will only update the relevant fields of the Flash Fabric Header in this case status and direction . The flash controller then returns the response packet through the fabric to the eHost for appropriate action. The eHost examines the response Flash Fabric Header. If the write terminated unsuccessfully the eHost will attempt to retry the write to a different location. Upon successful write the eHost also updates the metadata which holds LBA to Flash translation information.

If the write fails the failed status is sent to the eHOST. The eHOST maintains the write data in a cache until it sees a successful write response. If the eHOST receives a failed write status it updates the Flash Fabric Header with a new write destination and schedules it for transmission to the new destination.

This packet is sent to the fabric for routing to the appropriate flash controller. On reception of this packet the flash controller will fulfill the Read request and send back the following packet to the fabric 

This data can be either sent to DRAM for buffering in anticipation of wear leveling and garbage collection or sent to the host interface. On completion the status indicated in the header is sent to the eHost and the data is saved in cache on chip or DRAM . The eHost can initiate a DMA of this data from cache to Host if it was requested earlier.

The flash controller will send back the success or failure of the Block Erase in the following format updating the status field.

The flash controller processes every packet it receives via the fabric and follows the commands embedded in the Flash Fabric header. In all cases it will send back the original header with status updated to the fabric and with optional payload for reads to the eHost to indicate command completion. The eHost takes further action based on the status of command completion.

The use of Flash Fabric headers permits a highly efficient and scalable distributed processing. The eCPU can avoid issuing single commands and wasting precious cycles to wait for their completion either by polling or waiting for an interrupt from a flash controller. This architecture permits all the completed commands from Flash controller DRAM controllers to send back the status to the eHost autonomously without eCPU intervention. This in turn permits queuing up of all the completed commands to the eHost so it can decide to process these completed commands based on its current load and threshold of these completed command queues.

In some embodiments the eCPU has separate queues to handle these response packets based on priority. All queues can be classified based on type as well such as Read response queues Write response queues and Block Erase response queues. Read response queues can for instance have higher priority than other queues. The priority fields in the Flash Fabric header can be utilized for this purpose and for scheduling data through the fabric. For instance in the event of a power failure the metadata in DRAM can be scheduled to be flushed into non volatile storage ahead of the data that can be recovered such as Read data in cache. The Global Scheduler can steer traffic to these queues without any eCPU intervention after examining the priority and the type of response packet.

The eCPU does not need to poll for a successful completion of any command and waste cycles. If the Flash Controller terminates a transaction either successfully or unsuccessfully it sends the response packet to the eCPU so it can take the appropriate action. For instance if a write to the specified destination Flash controller Device Block Page was not successful the response packet will come back with an error in the Flash Fabric header as well as the write LBA data. This will permit the eCPU to schedule the write to a different destination by merely changing the destination field in the header. It does not need to allocate the write data from the prior write command as it is still part of the Flash Fabric packet. In another embodiment the controller sends only the header part of the response packet back to the eCPU indicating a failure condition. In the latter case data is maintained in a buffer whereas a new header is generated and the packet with this new header is re transmitted. In the foregoing embodiment advantageously causes higher fabric bandwidth utilization.

WRITE CANCEL If an operating system decides to write to the same LBA that was sent earlier to the SSD it is implied that the write data associated with the prior LBA write needs to be discarded. Using the Flash Fabric header bit WRITE CANCEL it is possible to cancel a prior LBA write to a destination flash controller substantially simultaneously with setting a higher priority that ends in a bypass queue of the flash controller. This advantageously eliminates a write operation to the flash. It is conceivable that it can greatly improve the endurance of the SSD as un necessary writes to flash are prevented in a just in time fashion.

WRITE IMMEDIATE In the event of an impending power outage or other critical events where the OS needs to flush its data to Flash this bit in the Flash Fabric header can be set and sent to the flash controller along with priority bits set to highest level. The write immediate operation is queued such that write operations are given higher priority than pending read operations in the channel controller.

MULTIPLE QUEUES The use of the priority field can enable commands to and from the flash controller to be queued in different queues. This enables higher priority commands to get ahead and lower priority commands to be scheduled later. For example a mirror packet for RAID purposes can have a lower priority than the original copy or other critical traffic.

NO RETRY It is important to read data from flash that has not been used for a long time to verify the integrity of the data in flash. The eCPU can have a background task to read all the valid pages in the flash array by setting this bit. When the flash controller sees this bit in the Flash Fabric header it will not attempt a retry of the specified commands such as an LBA read but the flash controller will inform eCPU of the success and failure of the command through a response packet. This will enable the software on the eCPU to derive failure patterns and construct the health of the flash array for further action.

AUTONOMOUS DATA MOVEMENT The eCPU or any block in the SSD controller can stack two Flash Fabric headers using the Next Header bit. This can enable autonomous data movement without CPU intervention. For example if a page of data needs to be moved from a specific block to another block due to garbage collection or any other request the eHost can create a stacked Flash Fabric header. The first header has the instructions for reading from a specified page block and the second stacked header has the second destination page block. This is interpreted by the Flash controller to move data accordingly using the fabric and global scheduler and if the move is within the range of the original Flash controller it will do it internally. At the end of the transaction the receiving flash controller can indicate the completion status via the response header to the eCPU for further action. This will greatly eliminate CPU cycles in moving data from one page block to another page block.

MULTIPLE PROFILES Since storage applications offer different workloads the SSD controller can offer different profiles that will load appropriate firmware register settings based on an Application Programming Interface API or graphical user interface. For example it might be desirable for some workloads to throttle LBA writes to improve endurance at the expense of performance. This can be accomplished by setting appropriate registers in the controller and throttle the LBA writes based on the requested settings. Variations of this can be used for other mechanisms such as prioritizing reads over writes providing peak write performance at the cost of endurance etc.

WRITE AMPLIFICATION METER Due to the limitations of flash endurance ability to write a block for a specified number of times after which it reaches end of life it is important to keep track of actual amount of data that is written to flash. A simple write amplification formula is specified as follows 

There are other variations of the above mentioned formula. The SSD controller provides logic to implement this and other formula variations to be used for warranty tracking. Due to the varying work loads of different applications the endurance or lifetime of current day SSDs are specified in terms of amount of data written to the drive. By keeping track of this statistic and the write amplification the SSD manufacturer has a chance to prove or disprove a customer claim if the drive failed within the warranty window. This is also useful for failure analysis.

DATA SCRAMBLER The SSD controller will scramble the data written to the drive and the ECC is computed on top of the scrambled data. Scrambling is valuable when data patterns are regular which in turn can cause read disturb errors from flash arrays.

REGULAR PATTERN DATA COMPRESSION The SSD controller can optionally include a front end engine which parses incoming data patterns to do on the fly compression. It identifies whether an LBA data block that is to be written can be expressed as a repeatable pattern such as all 1 all 0 s and other such examples. For LBAs with such patterns the SSD does not copy the entire LBA data into a corresponding flash block page. Instead it coalesces multiples of these patterns with their corresponding LBA numbers into a specific block page along with the appropriate metadata to extract them.

EVENT CORRELATION OF SSD HEALTH STATUS The SSD controller can include a circular buffer that is used for tracking various alert error events that occur during operation of the controller. Examples of these alerts are too many read errors or block erase errors in a specific LUN or device. The SSD controller does a correlation of these events and coalesces these multiple events into one event with a count associated with that event. Using the count in conjunction with the events maximizes the use of valuable memory space. Without this mechanism a single recurring bad event can fill up the entire buffer allocated for keeping these status events. The recurring bad event if every occurrence is saved separately could prevent visibility of other potentially more critical errors.

AUTOMATIC FLASH DIMM RECOGNITION AND CONFIGURATION shows an embodiment of an SSD where the controller configures the Flash Controller automatically after querying the type and manufacturer of the non volatile memory. Currently there is no single unifying standard for non volatile memory array shown in from different manufacturers. A non volatile memory array can be constructed similarly to memory DIMMs that are pluggable into a computer subsystem. The SSD controller can query the DIMM configuration using an interface such as I2C. Based on this information it can auto configure the Flash Controller to communicate with the non volatile memory array in the DIMM without manual intervention. The interface to the connector can be any physical form factor DIMM is used merely as an example.

Designing a unified non volatile memory controller for supporting multiple manufacturers is more expensive as opposed to a device that supports only a specific manufacturer and device family. An attractive compromise can be further refined if the SSD controller is implemented in a field programmable gate array FPGA FPGA. An FPGA bit file that matches the profile of the attached Flash DIMM can be downloaded into the FPGA. This has the added benefit of supporting multiple manufacturers device families while having a controller that is hardware programmed specific to the flash array potentially reducing the size of the FPGA device and leading to lower cost.

The eHost is responsible for mapping LBAs to appropriate Flash controller array LUN Block Pages of a flash device. In one embodiment software is responsible for this translation. The detailed signaling and mapping of the translated commands is later done by the flash controller. This partitioning also makes the design scalable and portable with minimal to practically no impact on the software. For targeting future flash devices a designer simply needs to change the stages in the flash controller that interface to the raw NAND flash device.

Every transaction that arrives on the host interface is passed to a Host Interface thread running on the eCPU. Some examples of transactions that arrive include Write an LBA Read a specified LBA Erase a specified LBA and query status of the flash controller capabilities of the flash controller etc.

Each of the volatile memory port host to N ECC engines to N RAID engines to N are a port coupled to the fabric . Advantageously use of the fabric shown in the configuration in allows decoupling of the data and control and asynchronously notifies the CPU of the transfer of data upon completion as discussed above in reference to . This offloads the CPU allowing it to perform other tasks during data transfer by the fabric . Further the bottleneck associated with using a single bus to transfer control and data in prior art techniques is eliminated. The fabric is analogous to the fabric C except that the fabric may be blocking or non blocking.

It is understood that nearly each of the types of devices coupled to the fabric shown in includes multiple ports. For example the ECC engine ports comprise of ECC engine or ECC engine to ECC engine N or ECC engine N with N being an integer value. The channel controllers comprise of channel controller to channel controller N. The host ports include host port to host port N and the RAID engines include RAID engine to N.

The fabric may be a blocking type of fabric or a non blocking type of fabric. In operation data also referred to herein as payload is received from the host or eCPU by the fabric through the host ports N or host ports to N. The received data is then either transferred to the ECC engines to N or transferred to the RAID engines to N or the channel controllers depending on whether or not error correction and coding is required. If transferred to the ECC engines to N the data is corrected coded and then transferred to the RAID engines to N or the channel controllers . The channel controllers are each coupled to a controller. If header non data is received by the fabric from the host it is transferred to the channel controllers or the RAID engines to N. These transfers are performed asynchronously with the host being alerted upon completion of the transfer. Further header is transferred through the fabric independently of data. Further details of an exemplary port is shown in .

