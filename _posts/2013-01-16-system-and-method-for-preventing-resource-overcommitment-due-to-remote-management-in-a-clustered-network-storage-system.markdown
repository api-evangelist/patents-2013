---

title: System and method for preventing resource over-commitment due to remote management in a clustered network storage system
abstract: A clustered network storage system includes a management module and a resource allocation mechanism configured to allow remote management of node resources from the management module while preventing resource over-commitment due to the remote management. Preventing resource over-commitment includes conditionally granting access to the resources of a node according to a credit based system. Preventing resource over-commitment further includes scheduling jobs that access the resources of the node according to a prioritization scheme.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09146780&OS=09146780&RS=09146780
owner: NETAPP, INC.
number: 09146780
owner_city: Sunnyvale
owner_country: US
publication_date: 20130116
---
This is a continuation is U.S. patent application Ser. No. 12 771 959 filed on Apr. 30 2010 which is incorporated herein by reference.

At least one embodiment of the present invention pertains to network storage systems and more particularly to performing remote management of resources of a node of a clustered network storage system while avoiding resource over commitment due to remote resource allocation without the knowledge of resource availability.

A storage controller is a physical processing device that is used to store and retrieve data on behalf of one or more hosts. A network storage controller can be configured e.g. by hardwiring software firmware or any combination thereof to operate as a storage server that serves one or more clients on a network to store and manage data in a set of mass storage devices such as magnetic or optical storage based disks tapes or flash memory. Some storage servers are designed to service file level requests from hosts as is commonly the case with file servers used in a network attached storage NAS environment. Other storage servers are designed to service block level requests from hosts as with storage servers used in a storage area network SAN environment. Still other storage servers are capable of servicing both file level requests and block level requests as is the case with certain storage servers made by NetApp Inc. of Sunnyvale Calif. employing the Data ONTAP storage operating system.

As storage systems become larger in order to accommodate the need for more capacity several problems arise. Particularly the efficient use of storage space becomes more difficult. An example of inefficient use of storage space is duplication of data. Deduplication operations eliminate the redundant data objects and instead reference just the original object. These operations provide a benefit in storage space efficiency. The result can be reduced cost in storage acquisition or longer intervals between storage capacity upgrades and more efficient management of data stored on the volumes. Deduplication and other storage efficiency functions can be performed by a storage controller.

A network storage system can have a simple architecture for example an individual storage server can provide one or more clients on a network with access to data stored in a mass storage subsystem. Recently however with storage capacity demands increasing rapidly in almost every business sector there has been a trend towards the use of clustered network storage systems to improve scalability.

In a clustered storage system two or more storage server nodes are connected in a distributed architecture. The nodes are generally implemented by two or more storage controllers. Each storage server node is in fact a storage server although it is implemented with a distributed architecture. For example a storage server node can be designed to include a network module N module to provide network connectivity and a separate data module e.g. D module to provide data storage access functionality where the N module and D module communicate with each other over some type of physical interconnect. Two or more such storage server nodes are typically connected to form a storage cluster where each of the N modules in the cluster can communicate with each of the D modules in the cluster.

A clustered architecture allows convenient scaling through the addition of more N modules and D modules all capable of communicating with each other. Further a storage cluster may present a single system image of stored data to clients and administrators such that the actual location of data can be made transparent to clients and administrators. An example of a storage controller that is designed for use in a clustered system such as this is a NetApp FAS series Filer employing NetApp s Data ONTAP GX storage operating system.

Efficient use of storage space can also be a concern in a clustered storage system. The problem can even be magnified due to the distributed architecture of a clustered storage system. Because of the distributed architecture the storage a client accesses may not all be controlled by the same D module. Further a single D module may control storage accessed by multiple clients and managed by administrators in multiple locations. These multiple administrators may simultaneously request storage efficiency operations to be performed by the D module. Storage efficiency operations are operations performed by the D module to improve the way storage space is used. Deduplication and compression operations are examples of storage efficiency operations. However each D module has a limited amount of processing resources to perform the requested storage efficiency operations. If too many storage efficiency operation requests are made simultaneously the processing resources on the D module become overloaded and system performance for all users will be degraded.

The problem then becomes how to provide remote administrators the ability to schedule the limited storage efficiency resources while avoiding resource over commitment caused by too many concurrent requests for the processors on a given D module.

The techniques introduced here enable remote allocation of node resources in a clustered processing system while avoiding resource over commitment. As a result the storage efficiency resources on any node of a clustered network storage system can be managed from any node in the system without causing degraded system performance due to resource over commitment. Further the techniques described herein enable priority and credit based resource scheduling to improve quality of service for clients. These techniques collectively allow convenient storage efficiency management in a clustered network storage system while preventing the undesired result of resource over commitment.

As described in greater detail below in accordance with certain embodiments the preventing of resource over commitment can be implemented by conditionally granting access to resources of a storage server node hereinafter simply node in a clustered network storage system according to credits granted to the storage node and prioritizing the allocations based on available credits of a storage node.

According to one embodiment a system for preventing resource over commitment on a node of a clustered network storage system includes a resource allocation mechanism configured to allow remote management of node resources. The resource allocation mechanism can include a job submission module a job scheduler and a job buffer e.g. a queue as henceforth assumed herein to facilitate description . The job submission module which can submit a job request for processing only when a credit is available can be implemented on the management module. The job scheduler can be implemented on a data module and schedules jobs according to a prioritization scheme in response to receiving a job request from the job submission module. The job queue can also be implemented on the data module and can be configured to queue jobs until the resources become available to process the job.

According to one embodiment a clustered network storage system can include a plurality of storage nodes and a plurality of virtual servers. Each of the virtual servers includes a management module configured to access resources on the plurality of nodes. Resource over commitment could arise if a plurality of virtual servers attempt to access resources of a single node at the same time. The clustered network storage system therefore can include a resource allocation mechanism configured to allow the plurality of virtual servers to access node resources while preventing such resource over commitment.

Other aspects of the techniques summarized above will be apparent from the accompanying figures and from the detailed description which follows.

References in this specification to an embodiment one embodiment or the like mean that the particular feature structure or characteristic being described is included in at least one embodiment of the present invention. Occurrences of such phrases in this specification do not necessarily all refer to the same embodiment.

The storage server can be for example one of the FAS series of storage server products available from NetApp Inc. The client systems are connected to the storage server via the computer network which can be a packet switched network for example a local area network LAN or wide area network WAN . Further the storage server can be connected to the disks via a switching fabric not shown which can be a fiber distributed data interface FDDI network for example. It is noted that within the network data storage environment any other suitable number of storage servers and or mass storage devices and or any other suitable network technologies may be employed.

The storage server can make some or all of the storage space on the disk s available to the client systems in a conventional manner. For example each of the disks can be implemented as an individual disk multiple disks e.g. a RAID group or any other suitable mass storage device s . The storage server can communicate with the client systems according to well known protocols such as the Network File System NFS protocol or the Common Internet File System CIFS protocol to make data stored on the disks available to users and or application programs. The storage server can present or export data stored on the disks as volumes and or qtrees to each of the client systems . Various functions and configuration settings of the storage server can be controlled by a user e.g. a storage administrator from a management station coupled to the network .

The storage server can include a storage efficiency framework to manage the storage on the disks . One problem in particular is duplicate data. An average UNIX or Windows disk volume contains thousands or even millions of duplicate data objects. As data is created distributed backed up and archived duplicate data objects are stored unabated across all storage tiers. The end result is inefficient utilization of data storage resources. Deduplication operations eliminate the redundant data objects and instead reference just the original object. This storage efficiency framework can include a storage efficiency engine not shown to perform for example deduplication and compression operations to more efficiently use storage space on the disks.

Each node essentially provides similar functionality to that of a storage server in . However unlike the storage servers in the nodes are not operated and managed as distinct independent entities. Rather they are operated and managed collectively as a single entity i.e. a cluster . The cluster presents to users and administrators a single system image of all data stored by the cluster regardless of where any particular data resides within the cluster.

Each of the nodes is configured to include several modules including an N module a D module and an M host each of which can be implemented by using a separate software module and an instance of a replicated database RDB . RDB can be implemented as a number of individual databases each of which has an instance located in each of the nodes . The N modules include functionality that enables their respective nodes to connect to one or more of the client systems over the network while the D modules provide access to the data stored on their respective disks . The M hosts provide management functions for the clustered storage server system including user interface functionality to enable an administrator to manage and control the cluster. Accordingly each of the server nodes in the clustered storage server arrangement provides the functionality of a storage server.

The RDB is a database that is replicated throughout the cluster i.e. each node includes an instance of the RDB . The various instances of the RDB are updated regularly to bring them into synchronization with each other. The RDB provides cluster wide storage of various information used by all of the nodes and includes a volume location database VLDB . The VLDB indicates the location within the cluster of each volume of data in the cluster i.e. the owning D module for each volume and is used by the N modules to identify the appropriate D module for any given volume to which access is requested. Each volume in the system is represented by a data set identifier DSID and a master data set identifier MSID each of which is stored in two places on disk in the volume itself and in the VLDB. The DSID is a system internal identifier of a volume. The MSID is an external identifier for a volume used in file handles e.g. NFS and the like. The VLDB stores the identity and mapping MSIDs to DSIDs of all volumes in the system.

The nodes are interconnected by a cluster switching fabric which can be embodied as a Gigabit Ethernet switch for example. The N modules and D modules cooperate to provide a highly scalable distributed storage system architecture of a clustered computing environment implementing exemplary embodiments of the present invention. Note that while there is shown an equal number of N modules and D modules in there may be differing numbers of N modules and or D modules in accordance with various embodiments of the technique described here. For example there need not be a one to one correspondence between the N modules and D modules. As such the description of a node comprising one N module and one D module should be understood to be illustrative only.

Various functions and configuration settings of the cluster can be controlled by a user e.g. a storage administrator from a management station coupled to the network . A plurality of virtual interfaces VIFs allow the disks associated with the nodes to be presented to the client systems as a single shared storage pool. depicts only the VIFs at the interfaces to the N modules for clarity of illustration.

The storage controller can be embodied as a single or multi processor storage system executing a storage operating system that preferably implements a high level module called a storage manager to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks on the disks. Illustratively one processor can execute the functions of the N module on the node while another processor executes the functions of the D module .

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data associated with the present invention. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system portions of which are typically resident in memory and executed by the processing elements functionally organizes the storage controller by among other things invoking storage operations in support of the storage service provided by the node . It will be apparent to those skilled in the art that other processing and memory implementations including various computer readable storage media may be used for storing and executing program instructions pertaining to the technique introduced here.

The network adapter includes a plurality of ports to couple the storage controller to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus can include the mechanical electrical and signaling circuitry needed to connect the storage controller to the network . Illustratively the network can be embodied as an Ethernet network or a Fibre Channel FC network. Each client can communicate with the node over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system to access information requested by the clients . The information may be stored on any type of attached array of writable storage media such as magnetic disk or tape optical disk e.g. CD ROM or DVD flash memory solid state drive SSD electronic random access memory RAM micro electro mechanical and or any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is stored on disks . The storage adapter includes a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance Fibre Channel FC link topology.

Storage of information on disks can be implemented as one or more storage volumes that include a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number VBN space on the volume s . The disks can be organized as a RAID group. One or more RAID groups together form an aggregate. An aggregate can contain one or more volumes file systems.

The storage operating system facilitates clients access to data stored on the disks . In certain embodiments the storage operating system implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by disks . In certain embodiments a storage manager logically organizes the information as a hierarchical structure of named directories and files on the disks . Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the storage manager to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers LUNs .

The network protocol stack in the N module includes a network access layer which includes one or more network drivers that implement one or more lower level protocols to enable the processing system to communicate over the network such as Ethernet Internet Protocol IP Transport Control Protocol Internet Protocol TCP IP Fibre Channel Protocol FCP and or User Datagram Protocol Internet Protocol UDP IP . The network protocol stack also includes a multi protocol layer which implements various higher level network protocols such as Network File System NFS Common Internet File System CIFS Hypertext Transfer Protocol HTTP Internet small computer system interface iSCSI etc. Further the network protocol stack includes a cluster fabric CF interface module which implements intra cluster communication with D modules and with other N modules.

In addition the storage operating system includes a set of data access layers organized to provide data paths for accessing information stored on the disks of the node these layers in combination with underlying processing hardware forms the D module . To that end the data access layers include a storage manager module that manages any number of volumes a RAID system module and a storage driver system module .

The storage manager primarily manages a file system or multiple file systems and serves client initiated read and write requests. The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with a RAID redundancy protocol such as RAID 4 RAID 5 or RAID DP while the disk driver system implements a disk access protocol such as SCSI protocol or FCP.

The data access layers also include a CF interface module to implement intra cluster communication link with N modules and or other D modules. The nodes in the cluster can cooperate through their respective CF interface modules and to provide a single file system image across all D modules in the cluster . Stated another way the various D modules can implement multiple distinct file systems within a single global namespace. A namespace in this context is a set of names that map to named objects e.g. files directories logical units . Global means that each name is independent of the physical location of the corresponding object. A global namespace therefore applies at least across an entire cluster and potentially can be extended to apply to multiple clusters. Thus any N module that receives a client request can access any data container within the single file system image located on any D module of the cluster and the location of that data container can remain transparent to the client and user.

The CF interface modules implement the CF protocol to communicate file system commands among the modules of cluster over the cluster switching fabric . Such communication can be effected by a D module exposing a CF application programming interface API to which an N module or another D module issues calls. To that end a CF interface module can be organized as a CF encoder decoder. The CF encoder of e.g. CF interface on N module can encapsulate a CF message as i a local procedure call LPC when communicating a file system command to a D module residing on the same node or ii a remote procedure call RPC when communicating the command to a D module residing on a remote node of the cluster. In either case the CF decoder of CF interface on D module de encapsulates the CF message and processes the file system command.

In operation of a node a request from a client is forwarded as a packet over the network and onto the node where it is received at the network adapter . A network driver of layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the storage manager . At that point the storage manager generates operations to load retrieve the requested data from disk if it is not resident in memory . If the information is not in memory the storage manager indexes into a metadata file to access an appropriate entry and retrieve a logical VBN. The storage manager then passes a message structure including the logical VBN to the RAID system the logical VBN is mapped to a disk identifier and disk block number DBN and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the DBN from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client over the network .

The data request response path through the storage operating system as described above can be implemented in general purpose programmable hardware executing the storage operating system as software or firmware. Alternatively it can be implemented at least partially in specially designed hardware. That is in an alternate embodiment of the invention some or all of the storage operating system is implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC for example.

The N module and D module can be implemented as processing hardware configured by separately scheduled processes of storage operating system . In an alternate embodiment the modules may be implemented as processing hardware configured by code within a single operating system process. Communication between an N module and a D module is thus illustratively effected through the use of message passing between the modules although in the case of remote communication between an N module and D module of different nodes such message passing occurs over the cluster switching fabric . A known message passing mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF API.

The operating system also includes M host which provides management functions for the cluster including user interface functionality to enable an administrator to manage and control the cluster e.g. through management station . Because each node in the cluster has an M host the cluster can be managed via the M host in any node in the cluster . The functionality of an M host includes generating a user interface such as a graphical user interface GUI and or a command line interface CLI for a storage network administrator. The functionality of an M host can also include facilitating the provisioning of storage creating and destroying volumes installation of new software controlling access privileges scheduling and configuring data backups scheduling data mirroring function and other functions. The M host communicates with its local D module or with any remote D module by using a set of APIs. The M host includes network interfaces not shown to communicate with D modules and to communicate with one or more external computers or input output terminals used by network administrators.

As described above with reference to a non clustered network storage system each D module of the clustered network storage system of includes a storage efficiency engine . Each storage efficiency engine can perform for example deduplication and or compression operations to more efficiently use storage space on the storage devices not shown managed by D module . The goal of deduplication is to ensure that there is no more than one instance of a particular data segment stored on a volume thereby reducing the used storage space. This task is significantly more difficult in a clustered network storage system as data contained on a volume may be spread over multiple storage devices controlled by multiple D modules. A D module also has limited processing resources for storage efficiency operations so there is a limit on the number of storage efficiency operations that can be carried out simultaneously. These limited resources and the potentially large number of requests that can be made of the D module in a clustered network storage system give rise to resource over commitment. As used herein the term resource over commitment refers to a processor memory or other component in a node being tasked with more operations than it has the ability to carry out at an acceptable or desired minimum level of performance or efficiency.

The resource allocation mechanism according to the example of further includes a job submission module in at least one M host and a scheduling module and a job queue in each D module . The job submission module provides a mechanism for controlling when jobs e.g. storage efficiency jobs can be submitted to a D module for processing. In one embodiment the job submission module is managed according to a credit based scheme. Initially credits may be allocated to the job submission module based on a variety of factors for example the projected usage of node resources. When each D Module registers on the network it registers with the M host the availability of storage efficiency job slots. The M host converts these job slots to the credits allocated to the D Module . The M host maintains a record of how many credits a particular D module has available. The job submission module can send a job request to the storage efficiency engine in any node if the corresponding D module has a credit available. If no credit is available for a particular storage efficiency engine the job submission module can wait for a credit to become available before submitting a job request to that engine.

When a storage efficiency operation is requested either by a network administrator or some predetermined event and a credit is available the job submission module sends a job request through the cluster switching fabric to the D module where the operation is to be performed. The scheduling module which is implemented on that D module receives the job request and schedules a job according to a prioritization scheme.

A job is scheduled in response to receiving a job request at the scheduling module . A job entry is placed in the job queue for the requested job according to the prioritization scheme. If the storage efficiency engine has processing resources available to process the job the job is immediately processed. If the storage efficiency engine does not have the processing resources available to immediately process the job i.e. the maximum number of jobs is already being processed the job entry waits in the job queue until capacity becomes available. The fact that a job entry has been placed in the job queue will be transparent to the client. A success status indication will be returned to the client when the job entry is placed in the job queue .

Prioritization of jobs in the job queue happens for example when two or more clients who own volumes on a given D module request a storage efficiency job at a time when the storage efficiency engine is running at full capacity. In that event the job scheduling module will place the higher priority job at the top of the queue for it to be processed first. Priority can be determined by the cluster administrator. Several factors can be considered when determining priority of clients for scheduling purposes. For example the amount of storage space a requesting client has on the D module the quality of service promised to the client and the type of job request. Other factors affecting priority of clients can also be considered.

The scheduling module can issue a special request for a job to be processed. Through a special request the requested job can be sent directly to the storage efficiency engine and bypass the job queue . This may happen for example when a high priority client requests a job to be performed or when a high priority job type is requested. The special request only changes the way jobs are performed when the storage efficiency engine is running at full capacity. If the storage efficiency engine is not running at full capacity all jobs are sent directly to the storage efficiency engine for processing and no jobs are placed in the queue. However when the special request is made and the storage efficiency engine is running at full capacity it is up to the scheduling module to determine which currently running job is pre empted to make room for the special request. If there is no job that can be pre empted the job request will fail and an error be returned to the requestor.

For example assume that two clients client A and client B both own volumes with storage on a given D module . Assume further that client A has a higher priority than client B based on some predetermined factor e.g. client A has more storage capacity than client B. Each client is managed from a separate instance of M host e.g. client A is managed from M host instance A and client B is managed from M host instance B . Client B submits a job request through the job submission module of M host B but the storage efficiency engine is running at full capacity so a job entry is placed into the job queue by the scheduling module on the target D module . Client A then requests a job for processing through the job submission module of M host A before the job requested by client B is processed. The scheduling module on the target D module places a job entry for client A in the job queue so that it will be processed prior to the job for client B. If two clients have the same priority a job entry for each client will be placed in the queue in the order in which they are received.

Further if the job requested by client A was a special request the scheduling module could determine if a job currently running on the storage efficiency engine could be pre empted. If a job could be pre empted then the requested job could begin processing immediately. If not an error would be returned to the requestor of the special request.

The system of includes a plurality of client systems a plurality of virtual servers and implemented in a clustered network storage system and a computer network connecting the client systems and the clustered network storage system. As shown in each virtual server includes an N module an M host and a plurality of D modules which communicate through the cluster switching fabric . Each virtual server shown in this figure is associated with a separate M host but owns volumes on the plurality of common D modules . The virtual server configuration shown in can give rise to resource over commitment as described above with respect to because the level of isolation between the virtual servers that is broken when multiple virtual servers request storage efficiency jobs to be processed by a single D module in the cluster. The resource allocation mechanism e.g. the job submission module the scheduling module the job queue and the storage efficiency engine in the example of operates in substantially the same manner as described with respect to .

Initially at the job submission module of the node receives a request to perform a storage efficiency operation. The request can come from a network administrator or can be triggered by some predetermined event. At the job submission module checks to determine if a job credit is available. If a job credit is not available No an error message is returned and the process ends. If a job credit is available Yes the job submission module sends a job request to the D module where the job is to be performed at . The job request can be sent through the cluster switching fabric .

At the D module receives the job request from the job submission module . Once the job is submitted at the M host debits a credit from the D Module record maintained by the M host hence reducing the capability of the D Module to accept more jobs. For each D Module a resource object is created in the M host for each function that consumes resources on the D Module. This object can be stored in a replicated database that is available to all instances of the M host in the cluster. This object will keep track of the D Blade resource in distributable credits. When a cluster administrator or other local administrator or process requests a job which requires D Module resources the M host reduces the number of credits in the resource object available for distribution.

Once a resource has been allocated and a credit is removed from the resource object the job requested is guaranteed. If the resource object has no more credits available all other requests are best effort meaning requests that are not guaranteed will be processed if there are resources available and will have to yield to guaranteed requests. The resource object can be controlled by role based access control RBAC and cluster wide allocation policies. Since the administrator is dealing with credits perhaps spanning multiple D Modules the administrator is spared the details of D Module resource limitations.

At the scheduling module schedules a job based on the prioritization scheme in response to receiving the job request. At the job queue queues the job for processing i.e. places a job entry for the requested job in the job queue . If the storage efficiency engine is not processing at full capacity when a job is placed in the job queue the job is immediately processed. However if the storage efficiency engine is processing the maximum number of jobs at the time the job is placed in the job queue the job waits for processing and a success indication is sent to the user.

At the storage efficiency engine processes the job and removes or invalidates the corresponding job entry in the job queue . The storage efficiency engine processes jobs stored in the job queue when the storage efficiency engine has processing capacity available. At the M host re allocates the job credit to D module record maintained by the M host upon completion of the job. Once a job is completed by the D Module it signals the M host that a job is complete. Consequently the M host adds a credit to the D module record maintained at the M host and the process ends.

The techniques introduced above can be implemented by programmable circuitry programmed or configured by software and or firmware or they can be implemented by entirely by special purpose hardwired circuitry or in a combination of such forms. Such special purpose circuitry if any can be in the form of for example one or more application specific integrated circuits ASICs programmable logic devices PLDs field programmable gate arrays FPGAs etc.

Software or firmware for implementing the techniques introduced here may be stored on a machine readable storage medium and may be executed by one or more general purpose or special purpose programmable microprocessors. A machine readable medium as the term is used herein includes any mechanism that can store information in a form accessible by a machine a machine may be for example a computer network device cellular phone personal digital assistant PDA manufacturing tool any device with one or more processors etc. . For example a machine accessible medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. etc.

The term logic as used herein can include for example special purpose hardwired circuitry software and or firmware in conjunction with programmable circuitry or a combination thereof.

Although the present invention has been described with reference to specific exemplary embodiments it will be recognized that the invention is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense.

