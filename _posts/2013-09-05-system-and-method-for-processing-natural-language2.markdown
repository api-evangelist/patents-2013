---

title: System and method for processing natural language
abstract: A method for processing natural language includes generating a first layer of a multi-layer knowledge network including a plurality of word nodes arranged to represent a word or an entity name, generating a second layer of the multi-layer knowledge network with a natural language dataset, the second layer including one or more instance nodes arranged to represent a word or an entity of the natural language dataset, each of the instance nodes being linked by one or more semantic or syntactic relations to form one or more sub-graphs, and, referencing the first layer of the multi-layer knowledge network with the second layer of the multi-layer knowledge network by establishing a reference between each of the word nodes and each of the instance nodes when the word or the entity name represented by each word node is associated with the word or the entity represented by the instance node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09639523&OS=09639523&RS=09639523
owner: 
number: 09639523
owner_city: 
owner_country: 
publication_date: 20130905
---
This invention relates to a system and method for processing natural language and particularly although not exclusively to a system and method for processing natural language to generate a knowledge network.

Since the dawn of computing early computer scientists have discussed the concept of using a computer to converse with a human being with natural language. Various attempts have been made including the famous implementation of Eliza which was able to talk to a human person by responding to natural language inputs in the forms of English sentences from a human user.

Despite these attempts there have not been any significant developments in computer science which has provided a computer with the ability to process natural language from a human being so as to form any meaningful result. One challenge faced by computer scientists is that there is a lack of computation ability to properly process human natural language by computers so as to enable a computer to understand the input and in so doing being able to provide a meaningful feedback to a user based on knowledge acquired through natural language inputs.

In accordance with a first aspect of the present invention there is provided a method for processing natural language comprising the steps of 

In an embodiment the method further comprises the step of generating a third layer of the multi layer knowledge network comprising one or more graph based probabilistic rules.

In an embodiment the third layer of the multi layer knowledge network is connected to the first layer.

In an embodiment the plurality of word nodes includes one or more semantic relations between each of the plurality of word nodes.

In an embodiment the natural language dataset is a supervised training dataset arranged to include a plurality of sentence portions having one of more words and one or more co reference tags arranged to reference one or more words of each sentence portion with one or more associated words within the sentence portion or in another sentence portion.

In an embodiment the second layer of the multi layer knowledge network is generated by a step of parsing the supervised training dataset into the one or more dependency graphs such that each of the one or more dependency graphs represents a portion of the training dataset and each of the plurality of instance nodes of each of the one or more dependency graphs represents each word of the portion of the supervised training dataset.

In an embodiment the step of generating the second layer of the multi layer knowledge network further includes the step of referencing one or more dependency graphs together by establishing a co reference relationship between a plurality of instance nodes based on one or more co reference tags associated with associated words of the supervised training dataset.

In an embodiment each of the one or more dependency graphs further includes one or more instance syntactic or semantic relations arranged to link two or more instance nodes in each graph which have a syntactic or semantic relationship.

In an embodiment each of the one or more dependency graphs are appended to the second layer of the multi layer knowledge network to form the sub graphs of the second layer.

In an embodiment a lexical database is processed to generate the first layer of the multi layer knowledge network by referencing one or more synsets for each word in the lexical database with each word within the lexical database to form a plurality of inter referenced word nodes each representative of a word and one or more synsets associated with the word.

In an embodiment the plurality of inter reference word nodes are inter referenced by one or more semantic relationships between the plurality of word nodes.

In an embodiment the plurality of inter reference word nodes are further inter referenced by one or more possible semantic relationships between the plurality of word nodes.

In an embodiment the one or more semantic relationships or the one or more possible semantic relationships are inherited from semantic relations between the synsets of the lexical database.

In an embodiment wherein each of the one or more graph based probabilistic rules include one or more primary set nodes and one or more primary semantic or syntactic relations between the primary set nodes.

In an embodiment the one or more primary set nodes and the one or more primary semantic or syntactic relations define the condition pattern of an embodiment of the first aspect.

In an embodiment the one or more primary set nodes and the one or more primary semantic or syntactic relations between the primary set nodes are referenced to the one or more sub graphs.

In an embodiment the one or more primary set nodes are linked by one or more secondary syntactic or semantic relationships.

In an embodiment the one or more graph based probabilistic rules further includes one or more secondary set nodes.

In an embodiment the one or more secondary set nodes include one or more tertiary semantic or syntactic relations arranged to link the one or more secondary set nodes to the one or more primary set nodes.

In an embodiment the secondary semantic or syntactic relations includes a probability value arranged to represent the possibility of the semantic or syntactic relationship between a pair of primary set nodes.

In an embodiment the tertiary semantic or syntactic relations includes a probability value arranged to represent the possibility of the semantic or syntactic relationship between the secondary set node and the primary set node.

In an embodiment the result pattern in accordance with one embodiment of the first aspect includes the one or more secondary set nodes the one or more secondary semantic or syntactic relations and the one or more tertiary semantic or syntactic relations.

In an embodiment each of the primary set nodes is arranged to reference two or more instance nodes in the second layer.

In an embodiment the two or more instance nodes in the second layer each belong to separate sub graphs within the second layer.

In an embodiment the separate sub graphs are matched to the sub graph composed by the primary set nodes and the primary syntactic or semantic relations.

In an embodiment each of the one or more graph based probabilistic rules further include one or more secondary set nodes.

In an embodiment the method further comprises the step of processing the second layer of the multi layer knowledge network to generate the one or more graph based probabilistic rules.

In an embodiment the step of processing the second layer of the multi layer knowledge network to generate the one or more graph based probabilistic rules further comprises the steps of 

In an embodiment the step of establishing the one or more graph based probabilistic rules includes establishing one or more primary set nodes in accordance with one embodiment of the first aspect by inheriting the one of more instance nodes from the first sub graph to reference each of the matching instance node of each of the one or more matching sub graphs to form the graph based probabilistic rule.

In an embodiment further comprises the step of establishing the one or more primary semantic or syntactic relations of one embodiment of the first aspect between two or more primary set nodes by inheriting the one or more matching semantic or syntactic relationships from each of the matching instance node of each of the one or more matching sub graphs referenced by the one or more primary set nodes.

In an embodiment further comprises the step of establishing one or more secondary set nodes of one embodiment of the first aspect to reference one or more connected instance nodes connected to the one or more matching sub graphs.

In an embodiment further comprises the step of establishing one or more secondary semantic or syntactic relationships of one embodiment of the first aspect between the two or more primary set nodes by inheriting the one or more semantic or syntactic relations between the matching instance nodes in the matching sub graphs referenced by the one or more primary set nodes.

In an embodiment further comprises the step of establish one or more tertiary semantic or syntactic relations of one embodiment of the first aspect between the one or more primary set nodes with one or more secondary set nodes by inheriting the one or more semantic or syntactic relationships from each of the matching instance node of each of the one or more matching sub graphs referenced by the one or more primary set nodes with the one or more connected instance nodes connected to the matching sub graphs referenced by the one or more secondary set nodes.

In an embodiment the one or more tertiary semantic or syntactic relations or the one or more secondary tertiary semantic or syntactic relations further includes a probability value arranged to represent the possibility of the one or more tertiary semantic or syntactic relations or the one or more secondary tertiary semantic or syntactic relations when an instance sub graph is matched to the condition pattern.

In an embodiment further comprises the step of referencing each of the one or more probabilistic rules to the one or more word nodes of the first layer.

In an embodiment further comprises the step of establishing one or more secondary set nodes to reference each connected instance node of each of the connected one or more matching sub graphs.

In an embodiment the step of extending the second layer with the one or more probabilistic rules includes the steps of 

In an embodiment the one or more probabilistic rules include one or more sub graphs having primary set nodes and primary semantic or syntactic relations which are compared with the starting sub graph.

In an embodiment the step of inheriting the one or more secondary syntactic or semantic relationships one or more secondary set nodes or one or more tertiary syntactic or semantic relationships to the starting sub graph includes referencing the one or more connected instance nodes referenced by the one or more secondary set node to the instance node of the starting sub graph.

In an embodiment the step of comparing the first sub graph with one or more other sub graphs of the second layer to find one or more matching sub graphs includes 

where upon the words being identical or having a possible hypernym hyponym relation the one or more matching sub graphs are deemed to match.

In an embodiment the step of comparing the first sub graph with one or more other sub graphs of the second layer to find one or more matching sub graphs further includes 

where upon the instance semantic or syntactic relation of the instance nodes of the first sub graph are identical with the instance semantic or syntactic relation of the instance nodes of the other sub graph the one or more matching sub graphs are deemed to match.

In accordance with a second aspect of the present invention there is provided a multi layer knowledge network generated in accordance with the method for processing natural language.

In accordance with a third aspect of the present invention there is provided a method for processing a natural language input including the steps of 

In an embodiment the system may discover localizing temporary graph based probabilistic rules in accordance with an embodiment of the first aspect wherein comparing the first sub graph with one or more other sub graphs of the second layer to find one or more matching sub graphs the matching sub graphs are limited in the nodes above an activation threshold or from particular input data source.

In an embodiment the step of comparing a condition pattern of each of the one or more graph based probabilistic rules with the co referenced dependency graph includes 

In an embodiment the one or more conflict results is reported if an instance semantic or syntactic relation between two instance nodes in the matching sub graph is not matched to a secondary semantic or syntactic relation in the result pattern when comparing each instance semantic or syntactic relations and instance nodes directly connected to each matching sub graphs to the result pattern of the matched graph based probabilistic rule.

In an embodiment if an instance semantic or syntactic relation is matched to a secondary semantic or syntactic relation but the probability of the secondary semantic or syntactic relation is lower than a predetermined threshold then the instance semantic or syntactic relation is deemed not to be matching with the secondary semantic or syntactic relation.

In an embodiment the one or more conflict results is reported if an instance semantic or syntactic relation connected to the matching sub graph and the connected instance node are not matched to a tertiary semantic or syntactic relation and the connected secondary set node in the result pattern when comparing each instance semantic or syntactic relations and instance nodes directly connected to each matching sub graphs to the result pattern of the matched graph based probabilistic rule.

In an embodiment if an instance semantic or syntactic relation is matched to a tertiary semantic or syntactic relation but the probability of the tertiary semantic or syntactic relation is lower than a predetermined threshold then the instance semantic or syntactic relation is deemed not to be matching with the tertiary semantic or syntactic relation.

In an embodiment when upon if the co referenced dependency graph does not match with the condition pattern of one or more graph based probabilistic rules the graph based probabilistic rules are ignored.

In accordance with a fourth aspect of the present invention there is provided a method for active learning comprising the steps of 

In an embodiment the step of determining the correct result or the conflict result includes processing the plurality of natural language inputs.

In an embodiment further comprises the step of appending the co referenced dependency graphs to the second layer of the multi layer knowledge network if the co referenced dependency graphs determined to have a correct result.

In an embodiment the co referenced dependency graphs determined to have a conflict result with a score being lower than a threshold are reported to a supervisor for additional supervising.

In an embodiment the co referenced dependency graph determined to have a conflict result with the score being higher than a threshold the co referenced dependency graph is ignored

In accordance with a fifth aspect of the present invention there is provided a method of active learning wherein the score is calculated based on the number of conflicts found in the co referenced dependency graphs in accordance.

In accordance with a sixth aspect of the present invention there is provided a method for processing natural language query comprising the steps of 

In an embodiment further comprises the step of calculating the score based on the matched words relations and the probabilities of the relations for each particular matching sub graph.

In accordance with a seventh aspect of the present invention there is provided a method for generating a natural language knowledge network comprising the steps of 

In an embodiment the one or more probabilistic rules are arranged to associate at least one of the plurality of data structure with one or more other data structures.

In an embodiment the first instance of the natural knowledge network is expanded by establishing an associating reference between each of the plurality of data structure based on the one or more probabilistic rules.

In an embodiment the associating references are established when upon the probabilistic rule meets a predetermined condition.

In an embodiment one or more probabilistic rules each includes a probabilistic variable arranged to represent an accuracy of the associating reference.

In an embodiment each of the one or more graph based probabilistic rule further includes one or more secondary set nodes one or more secondary semantic or syntactic relations between primary set nodes or between a primary set nodes and a secondary set nodes wherein each of the one or more secondary semantic or syntactic relations have a probability value arranged to represent the accuracy of the one or more secondary semantic or syntactic relations.

In an embodiment further comprises the step of referencing the first layer of the multi layer knowledge network with the third layer of the multi layer knowledge network by establishing a reference between each of the word nodes and each of the set nodes when the word represented by each word node is associated with the word represented by each set node.

In accordance with an eighth aspect of the present invention there is provided a method for processing natural language input comprising the steps of 

In an embodiment the method further comprises the step of finding the conflicts N within each potential co referenced dependency graph and determining the score for the potential co referenced dependency graph as N.

In accordance with a ninth aspect of the present invention there is provided a system for processing natural language comprising 

In an embodiment the system further comprises a third knowledge network module arranged to generate a third layer of the multi layer knowledge network comprising one or more graph based probabilistic rules.

In an embodiment the third layer of the multi layer knowledge network is connected to the first layer.

In an embodiment the plurality of word nodes includes one or more semantic relations between each of the plurality of word nodes.

In an embodiment the second layer is extended with the one or more probabilistic rules by a processor arranged to

In accordance with a tenth aspect of the present invention there is provided a system for processing a natural language input including 

In accordance with a eleventh aspect of the present invention there is provided a system for active learning comprising 

In accordance with a twelfth aspect of the present invention there is provided a system for processing natural language query comprising 

Referring to an embodiment of a computing system computer server apparatus processor or any other device arranged to implement or operate a system for processing natural language which in one example embodiment comprises 

In this example embodiment of the computing system the interface and processor are implemented by a computer having an appropriate user interface. The computer may be implemented by any computing architecture including stand alone PC client server architecture dumb terminal mainframe architecture or any other appropriate architecture. The computing device is appropriately programmed to implement the invention.

In this embodiment the computing device may include a storage module such as a database either locally remotely or both so as to store data used to provide a system for processing natural language. This data may include without limitation a multi layered knowledge network arranged to store knowledge processed from natural language inputs. The knowledge network may exist in the form of a data structure of multiple layers of data nodes although as a person skilled in the art would appreciate the term layers when used with reference to a knowledge network may not be limited to a define structure of various layers but is an abstract term use to describe the presentation of information in an organised form. Thus the data relating to the knowledge network may exist as multiple reference structures or in another structure suitable. For example a word list or a word index can be considered as the word layer of the multiple layer knowledge network.

Referring to there is a shown a schematic diagram of a computer system which in this embodiment comprises a server . The server comprises suitable components necessary to receive store and execute appropriate computer instructions. The components may include a processing unit read only memory ROM random access memory RAM and input output devices such as disk drives input devices such as an Ethernet port a USB port etc. Display such as a liquid crystal display a light emitting display or any other suitable display and communications links . The server includes instructions that may be included in ROM RAM or disk drives and may be executed by the processing unit . There may be provided a plurality of communication links which may variously connect to one or more computing devices such as a server personal computers terminals wireless or handheld computing devices. At least one of a plurality of communications link may be connected to an external computing network through a telephone line or other type of communications link.

The service may include storage devices such as a disk drive which may encompass solid state drives hard disk drives optical drives or magnetic tape drives. The server may use a single disk drive or multiple disk drives. The server may also have a suitable operating system which resides on the disk drive or in the ROM of the server .

The system has a database residing on a disk or other storage device which is arranged to store data for the operation or implementation of the system for processing natural language. The database may also be in communication with an interface which is implemented by computer software residing on the server .

With reference to there is illustrated a data flow and block diagram of one embodiment of the present invention. In this embodiment the system for processing natural language comprises 

In another embodiment the system for processing natural language further comprises a probabilistic rule module arranged to generate a third layer of the multi layer knowledge network comprising one or more graph based probabilistic rules. In some example embodiments a name entity recognition system such as but without limitations a Stanford Named Entity Recognizer may be employed to recognize the entity names.

Referring to the system includes a processor which is arranged to operate a word layer generator module an instance layer module and a probabilistic rule module . The processor may be a CPU or processing device arranged to execute computer or electronic instructions so as to perform the functions of each of the three modules and . In some examples the modules and may be implemented as individual program modules each arranged to operate a particular function which will be described in detail below.

For each of the three modules and the processor may be arranged to read write or otherwise access a knowledge network which is arranged to store knowledge derived from natural language inputs or existing knowledge stored within so as to generate a knowledge database acquired from input data or natural language received from a user or other external source. This knowledge which is stored within the knowledge network thus in one example be able to provide a computer system or process with information which can be subsequently used to process a natural language input from a user either in the form of understanding a natural language input and responding with knowledge within the network or to assess the correctness or conflict with a new natural language input. The knowledge network can be used in many different ways but in one example it can be used to make sense of any subsequent natural language input.

In this example the knowledge network is a multiple layer knowledge network which includes a word layer an instance layer and a set layer each of which is arranged to store particular data relating to natural languages. Each of these layers is further described below.

In this example flow diagram as shown in the primary process of the system is to build the multiple layer knowledge network via a supervised learning method as below. Other methods are also possible to initiate the building process of the knowledge network .

In this first example a lexical database having a records of words and their natural language relationships is used as a data source by a word layer generator module to generate the word layer of the multiple layer knowledge network . In this worked example. The lexical database is WordNet although other lexical databases are also possible. Each word of WordNet is combined with the synsets of the word to generate a word node of the word layer. The semantic relations between the synsets are also inherited from the lexical database so as to be the possible semantic relations between the word nodes. A further description as to the methods which are used to generate the word layer is further provided below with reference to .

After the word layer has been generated the instance layer of the multiple layer knowledge network is built from a natural language training set by the instance layer module . In this example the instance layer module operates on a supervised learning method by using the natural language input of the training set to learn knowledge to build the instance layer.

Preferably the training set contains a set of natural language texts. Each text is tagged to represent one or more co reference relations between words. The term co reference may in this embodiment has an ordinary meaning in the field of linguistics and is representative of a situation when multiple expressions in a sentence or document refer to the same thing or in linguistic jargon they have the same reference. 

This training set may then be entered as an input to the system upon which each natural language text in the training set is parsed by a dependency parser. Examples of a dependency parser may include without limitations the Stanford dependency parser. The parser is then arranged to parse the natural language text into one or more dependency graphs.

In this example each dependency graph is arranged to represent a sentence in the text from the training set . Co reference relations are then added between the words node with the same co reference tag in the dependency graphs. As a result of this process the dependency graphs are thus connected to one or more co referenced dependency graph.

A co referenced dependency graph may in some examples include one or more nodes and syntactic or semantic relations between the nodes. Each node represents a word or an entity. The semantic relations may be co reference part of or other semantic relations. The syntactic dependency relations may be nsubj dobj or other syntactic dependency relations.

The co referenced dependency graphs are then added to the instance layer of the multiple layer knowledge network . This process may be completed by first having each word node in the co referenced dependency graph being converted to an instance node and referenced to the word node in the word layer which represents the same word. Embodiments of the instance layer module are further described below with reference to .

In one example the supervised learning method may be the primary method to build the initial knowledge of a knowledge network which may also be known as a multi layer knowledge network or the Natural Language Semantic Knowledge Network NLSKN . As it may need a dependency parser to parse the text in order to maximize the precision of dependency parsing the co reference tagged natural language texts of the supervised training set may be written in simple English sentences. This way like human children that need enlightenment books the supervised training set is considered as the enlightenment book for NLSKN. The goal of the supervised learning process is to build a high precision knowledge base for NLSKN.

In one optional embodiment after the module completes the above operations the set layer is generated with the information within the instance layer by the probabilistic rule module . In this optional embodiment the set layer is composed by a set of graph based probabilistic rules.

Preferably each graph based probabilistic rule contains a condition graph pattern and a probabilistic result graph pattern. A condition pattern is an induction of a set of sub graphs in the instance layer and can also be considered as the index of a class of instance contexts. The probabilistic result pattern can be considered as an imagination in a context when it satisfies the condition pattern. Accordingly the probabilistic result pattern may be considered as an imperfect reference between a set of words and one or more particular word nodes. As this imperfect reference may be correct but may also be considered incorrect the reference between the set of words and the word node can be considered imagination in a natural language context.

Each node in the set layer may also be considered as the index of a class of instance nodes. The classifications of the instance nodes are based on the difference of contexts. The probabilistic rule module is further described below with reference to .

Referring to there is illustrated another embodiment of the system for processing natural language. In this embodiment the semantic engine is based on a multiple layer knowledge network which contains a word layer an instance layer and a set layer and is similar or identical to the knowledge network .

In this example the word layer is built from the data of a lexical database such as WordNet. Each word of WordNet is combined with the synsets of the word to generate a word node of the word layer. The semantic relations between the synsets are inherited to be the possible semantic relations between the word nodes . The instance layer of the multiple layer knowledge network is then built from a natural language training set based on a supervised learning method. The training set contains a set of natural language texts with each text being tagged to represent the co reference relations between words . Each natural language text is then parsed by a dependency parser for example Stanford dependency parser into one or more dependency graphs. Each dependency graph represents a sentence in the text and co reference relations are added between the words node with the same co reference tag in the dependency graphs. The dependency graphs are then connected to one or more co referenced dependency graph by this way.

The co referenced dependency graphs are added to the instance layer of the multiple layer knowledge network . Each word node in the co referenced dependency graph is converted to an instance node and referenced to the word node in the word layer which represents the same word.

Once the instance layer is generated the set layer is then generated based on the data of the instance layer. The set layer is composed by a set of graph based probabilistic rules with each graph based probabilistic rule contains a condition graph pattern and a probabilistic result graph pattern. A condition pattern is an induction of a set of sub graphs in the instance layer. A condition pattern can be also considered as the index of a class of instance contexts. The probabilistic result pattern can be considered as the imagination in a context when it satisfies the condition pattern. Each node in the set layer can be considered as the index of a class of instance nodes. The classifications of the instance nodes are based on the difference of contexts .

Based on the graph based probabilistic rules of the set layer the instance layer can be extended by reasoning or imagination . If an instance sub graph in the instance layer can be matched to the condition pattern of a probabilistic rule the sub graph can be extended by inheriting the nodes relations with probabilities of the result pattern of the probabilistic rule. Inheriting the result relations with probability 100 is considered as reasoning and thus can be appended to the instance layer to grow the instance layer with this new knowledge. However if the probability of a relation is lower than 100 the relation inheriting is considered as imagination as it cannot be considered to be fully correct and thus may or may not be added to the instance layer. In some examples a threshold value can be used to assess when the probabilistic rule is considered to be worthy of inclusion into the instance layer.

The process of generating or update the graph based probabilistic rules is a continuous and repetitive process so as to extend the instance layer. By extending the instance layer the knowledge stored within the knowledge network can be increased thus allowing natural language inputs or interactions with users or other sources to increase the knowledge of the network .

In some other embodiments the graph based probabilistic rules may also be pre defined whilst some of the pre defined rules may introduce some particular semantic relations as the part of result patterns when some sub graphs satisfy the condition pattern. Co referenced dependency graphs may also be extended by these rules then include the pre defined semantic relations.

In this embodiment with the initial knowledge in the multiple layer knowledge network an active learning method is used to acquire more knowledge in comparable low cost. Natural language texts are inputted as the knowledge source. Once inputted an input text is parsed by a dependency parser to generate one or more dependency graphs. Preferably the text is automatically tagged by a co reference resolution system For example Stanford Deterministic Co reference Resolution System such as by as an example whereby there is an entry of three sentences

Each sentence 1 2 and 3 is tagged with reference tags which show the co reference between related words or terms. Thus the sentences would now be represented as Tom 1 is a boy. Rose 2 is his 1 sister. She 2 is a student. 

The dependency graphs are connected by the co reference relations based on the co reference tags to form one or more co referenced dependency graphs.

Based on the graph based probabilistic rules in the set layer of the multiple layer knowledge network the system attempts to examine the co referenced dependency graphs to find semantic conflicts. If there is no semantic conflict in the one or more co referenced dependency graphs of the input text the system believes the text can be understood and is reasonable based on the existing knowledge. Under these conditions the one or more co referenced dependency graphs are appended to the instance layer of the multiple layer knowledge network . If the conflicts are more than a threshold all the one or more dependency graphs are ignored because the system can not understand the text.

In one example as illustrated with reference to if there are conflicts but less than the threshold the system may report to a supervisor to ask for additional supervising . The supervisor may provide co reference tags or choose the correct co referenced dependency graph within several potential co referenced dependency graphs generated by the system.

In some examples the additional supervising is advantageous in that the process of supervising is to provide a learning environment in which the system could build knowledge into the knowledge network by processing natural language inputs assessing it for correctness by way of assessing the input for conflict and provided the conflict of the input is zero or below a predetermined threshold the input may then be used to extend the knowledge network as a form of precise knowledge. As the precision of the learnt knowledge may depend on the precision of the automated dependency parsing and the co reference resolution of the system the process of supervising may therefore be advantageous in that it can assist in ensuring there is a higher level of precision of the co reference tags and the dependency graphs when the system has not enough confidence for its automated process.

Initially when the multiple layer knowledge network is small most of the input texts may not be understood. As a result the active learning can only learn some very simple text. The process of semantic examination can be operated by attempt to guarantee all the knowledge in the multiple layer knowledge network is consistent to each others further details regarding the process of semantic examination are described below with reference to .

In some embodiments the rules in the knowledge network or the NLSKN are based on a limited training set. When new knowledge is learnt some rules in NLSKN may be found conflict to each others. In these situations the system may then report to the supervisor for additional supervising and from this point allow the supervisor to choose a correct rule between a pair of conflict rules.

As the probabilistic rules are approximately correct it may be possible to improve the resource usage of a system for processing natural language by limiting the reasoning generated from the probabilistic rules. This is effectively a pruning method whereby probabilistic rules are pruned based on their probability of correctness. With more and more knowledge added to the multiple layer knowledge network the probabilistic rules are updated and new conflicts may be found. The system will ask for additional supervising when internal conflicts are found.

The knowledge in the multiple layer knowledge network may be learnt from natural language input or generated by the internal reasoning or imagination process. Once the knowledge is stored within the network the knowledge can be employed for a natural language question and answer system . In these example embodiments a natural language question is parsed into dependency graphs by the natural language question and answer system and then connected so as to form a co referenced dependency graph.

Once the co referenced dependency graph is formed it is extended based on the probabilistic rules in a similar manner in which the instance layer is extended with the probabilistic rules . The advantageous of this example is that because a question may be expressed in natural language in a number of different ways by extending the co reference dependency graph with the probabilistic rule we can form an extended co reference dependency graph which may include various manners in which the question is asked and thus allowing for a greater chance that relevant knowledge within the knowledge network can be extracted to answer the question. From a human way of thinking this is much like the process whereby a question is interpreted by a respondent with common sense or knowledge of the respondent so as to expand their capabilities in responding properly. As an example assuming a natural language question of Who is Tom s father is inputted into the system and also assuming that the system is connected to a knowledge network which include the knowledge Tom is Joe s son. in which case after processing the extended co referenced dependency graph of the question Who is Tom s father will include a reference or connection via the probabilistic rule to a question Whose son is Tom in which case an answer of Joe can be generated.

The extension to the dependency graph to form the extended dependency graph may take place if the co reference dependency graph is found to be matching with the condition pattern within a set layer of the multi layer knowledge network where there may be one or more probabilistic rules.

In these examples the extended co referenced dependency graph is used as a graph pattern by a graph matching method described below with reference to in order to find matching sub graphs in the instance layer of the multiple layer knowledge network . It follows that in some examples an evaluation is made to the extended matching sub graphs to find the best matching sub graph as there could be more than one matching sub graph. Based on the best matching sub graph the sub graph is processed so as to form an answer in natural language. Further details regarding the query answer system are described below with reference to .

With reference to there is illustrated an embodiment of a knowledge network . In this embodiment there is illustrated a simplified structure of one embodiment of a knowledge network which may also be referred to as a Natural Language Semantic Knowledge Network NLSKN .

In this embodiment the knowledge network has three layers a word layer set layer and instance layer . In each layer there may be nodes which represent the data structure of the specific layer. For example in the word layer there are word nodes in the set layer there are set nodes S and in the instance layer there are instance nodes I . These nodes are linked or referred with semantic relations which allow the system to query the information of each layer.

There are possible semantic relations between words. As one example each word may be a subset hyponym of other words. For example dog may be a subset hyponym of mammal. Besides dog may also be a subset hyponym of pet. The possible superset hyperonym subset hyponym relations between words are not restrictive.

In one embodiment a word may be referenced to one or multiple instance individuals or sets. A set is referenced to a word. A set may have one or multiple instance individuals.

There maybe semantic relations or syntactic dependency relations between sets. The semantic relations may be co reference part of or other semantic relations. The syntactic dependency relations may be nsubj dobj or other syntactic dependency relations.

These semantic syntactic dependency relations may also have probabilities between 0 and 1 which reflect the correctness or logic of the semantic syntactic dependency relations. The probabilities of confirmed relations are 1.

In this embodiment the knowledge network or the NLSKN is unique in that the knowledge of NLSKN has an instance layer which is constructed with examples stored within the instance layer in the form of instance nodes I . For example there are three sentences

The sentence 2 and sentence 3 are not reasonable in common sense. In a word based concept level semantic knowledge network a car can run and a road can be run on. The conflict to the common sense in the sentence 2 and 3 can not be recognized by the knowledge of concept level semantic networks. In a phrase based concept level semantic knowledge network stone car and car without wheel should be both separate concepts if the common sense is included. And the combination phrases like stone car and car without wheel are unlimited.

In one example with using the NLSKN these three sentences are parsed into three dependency graphs to compare with the existing sub graphs in the instance layer. Several examples of stone car are found but no example of stone car can run. Similarly no car can run without wheels in the examples. So NLSKN can find the conflict from a common sense point of view.

The set layer of NLSKN is composed by a group of graph based probabilistic rules. Each probabilistic rule is composed by a condition pattern and connected result pattern. A condition pattern is composed by one or more set nodes and semantic or syntactic relations between the nodes. A result pattern is composed by some semantic or syntactic relations with probabilities and connected set nodes. Each set node represents a set of instance nodes within the special context represented by the condition pattern and the result pattern. These set nodes can be considered as the classification of the instance nodes. Different from the manual defined classification such as synsets of WordNet most of the set nodes S are discovered automatically from the instance sub graphs.

With reference to there is shown the process to construct word layer of NLSKN in accordance with one embodiment. In this example embodiment the NLSKN is constructed by use of a lexical database such as WordNet although other forms of lexical database or lexical data in natural language form may also be used.

In this example WordNet includes two layers as the word layer and the synset layer. A word has one or multiple synsets. The semantic relations are between synsets. In this embodiment the process includes a combination of each word with its synsets and inherits the semantic relations of its synsets . For example in I W has two synsets as S and S. W is then combined with S and S in II . The combined word node W in III has two semantic relations with W and W. The words and semantic relations between words can be extended.

After this process is complete and with reference to the instance layer is constructed with a natural language training set by a supervised learning method. As a supervised method co reference tags are added to natural language sentences to be learnt. Then the sentences are parsed into dependency graphs . For example Stanford Dependency Parser can be used to parse the sentences. A dependency graph is composed by word nodes and semantic syntactic dependency relations. Co reference relations are added between the co referenced word nodes to connect the dependency graphs . Finally the word nodes of the dependency graphs are converted into instance nodes and the reference relations are added between the instance nodes and the corresponding word nodes of the NLSKN.

For example there are two co reference tagged sentences Tom 1 came to the gymnasium 2 . He 1 swam in the pool of the gymnasium 2 . At first the sentences are parsed into two dependency graphs

Finally convert the words as instance nodes and connect them to the corresponding word nodes in the NLSKN.

As shown in these embodiments the instance layer can be extended based on the graph based probabilistic rules in the set layer. This extending process can be considered as being similar to the process of reasoning or imagination of a human. The extended part of the instance layer is inherited from the result patterns of matched probabilistic rules. When the probability of a relation in the extended part is 100 it is similar to reasoning. If the probability is smaller than 100 it is similar to imagination.

As natural language is not completed sometimes a word or a phrase only shows a clue of a complicated meaning. An idiom may represent a story or a semantic pattern with a complicated structure among different elements in the context. To understand these hidden meaning of natural language reasoning and imagination are necessary.

Accordingly the process of extending the instance layer based on the graph based probabilistic rules can be considered as an attempt to discover the hidden meaning in natural language.

With reference to there is illustrated an embodiment of a method for graph matching which is a method which is used to find a matching graph in the knowledge network or NLSKN based on a given graph pattern. This particular step of graph matching may be used by the system for processing natural language so as to find a matching sub graph pattern within the knowledge network for the purposes of assessing the correctness of a natural language input which may have been parsed into a co reference dependency graph or some other graph structure or alternatively the graph matching method can be used to locate matching sub graph patterns within the network itself so as to identify correlations and relations between two previously separate graphs but may indeed be related. A simple example illustrated the usage of the graph matching method is suppose there is two graph patterns within the knowledge network with one graph pattern being Tom has a car whilst a second is He has a sedan then these two graph patterns can be effectively matched to each other thus on further processing such as by extending the set layer or instance layer of the knowledge network a new relationship between car and sedan can be linked and thus allow the knowledge network to learn more information with existing graph patterns.

In one example the graph matching method operates by firstly using a given graph pattern G and a key node i G the process is to find matching graphs or matching sub graphs G G . . . Gn in knowledge network or NLSKN G 

Given a graph pattern G and a key node i G to find matching graph sub graphs G G . . . Gn in NLSKN G the graph matching algorithm operates by using the given graph pattern G which includes an instance layer or a set layer with the similar structure of NLSKN and referenced to the word layer of G . G can be a part of G . In this example the key node i is one of the nodes of the instance set layer of G. Preferably the key node is referenced to a verb although other words or terms are possible.

Preferably the graph matching algorithm can be used within the set layer the instance layer or between the set layer and instance layer of the NLSKN or between a given graph pattern and the NLSKN.

In one example the process of the graph matching method is used between a pair of relation r i i and r i i in the multiple layer knowledge network where 

If four words w W w W w Wand w Wcan be found that wis the same or hypernymy of wand wis the same or hypernymy of w the relation ris considered as a matching relation of relation r.

However when prep in I I is compared to prep in I I . The referenced word node sea of I is different and not a hypernymy of the referenced word pool of I. So prep in I I is not a matching instance relation of prep in I I .

The matching instance nodes nsubj I I and prep in I I compose to a matching graph. nsubj I I is another matching sub graph.

Graph matching method forms the bases of the following methods for NLSKN. The rule conclusion method the reasoning and imagination the consistency examination method and the question answer system are all arranged in some embodiments to use an embodiment of or are based on the graph matching method. The given graph pattern G includes a word layer and an instance set layer with the same structure of NLSKN. G can be a separate graph pattern or a part of G . The key node N is one of the nodes of the instance set layer of G. The graph matching algorithm can be used within or between the set layer instance layer of the NLSKN or between a given separate pattern and the NLSKN.

With reference to there is illustrated a flow diagram illustrating the steps which are undertaken to build the set layer of the NLSKN by summing up the graph based probabilistic rules from the knowledge at the instance layer.

By using the supervised learning method NLSKN is constructed with two layers including the word layer and the instance layer. After that the set layer can be built by an induction method from the data of the instance layer based on the graph matching algorithm.

Given a graph pattern G the system attempt to discover possible graph based probabilistic rules. The graph pattern can be a sub graph of input content and can be part of the NLSKN with high activations.

At first find matching graph G G . . . G n for the given graph pattern G in the NLSKN G . t 0 is a given threshold. If n t select all the nodes in the instance layer of G and connected relations as G copy G to the set layer . For each instance node im G and each semantic syntactic relation r mkl of the matching instance node i mk of im to find matching edges of r mkl in the matching graphs G G . . . G n. If p matching edges are found copy r mkl and the node on the other end i mkl as r mkl and i mkl to the set level and connect to the corresponding set node i m. Then set the probability of r mkl as p n. r mkl and i mkl are considered as the possible dependency relation and the possible set node. Similarly find all the possible dependency relations and possible sets to connect to G . All these possible dependency relations and possible sets are considered as the result pattern G . Correspondingly G is the condition pattern. The probabilistic rule Gr G UG .

In the graph pattern Gman swim I I is copied and merge to the NLSKN. The instance nodes I and I are copied to the set layer as S and S. prep in I I and prep in I I are also copied to the set layer as prep in S S and prep in S S . Because in this example as S is connected to the word sea and S is connected to a different word pool each of them has only one sample in the surrounding areas of the two matching graphs G and G . According the probability of the possible dependency relation prep in I I and prep in I I are both 0.5. The S nodes and relations between the S nodes as shown in include a graph based probabilistic rule for the NLSKN. The sub graph man swim S S is the condition pattern and sub graphs sea S S and pool S S are the probabilistic results pattern.

Besides the graph based probabilistic rules the superset subset relation between nodes at the word layer can be discovered from the instance layer. For a pair of words W and W when the system finds the sub graph like I in NLSKN a possible superset subset relation is added between from W to W as in II .

With reference to in this example embodiment based on the probabilistic reasoning rules the system can generate new knowledge from existing knowledge. In NLSKN G given a sub graph G G if G is a matching graph of a condition pattern G of a graph based probabilistic rule R G R result pattern G R. G can be copied to instance layer to connect to the corresponding nodes of G. The copy of G is G. G is the generated new knowledge . For each dependency relations r G if P r 1 and r is directly connected to a node of G or r is indirectly connected to a node of G through some dependency relations with the probabilities 1 r and the node directly connect to r is considered as the result of reasoning. All the other dependency relations and connected nodes in G which are not the result of reasoning are considered as the result of guess or imagination.

The system keeps generating new knowledge by this method based on the existing knowledge in NLSKN when there is no input or query.

Given a graph pattern G and a key node i G to find matching graph sub graphs G G . . . Gn in NLSKN G Graph matching algorithm is a base of the following methods for NLSKN. The given graph pattern G includes an instance layer or a set layer with the similar structure of NLSKN and referenced to the word layer of G . G can be a part of G . The key node i is one of the nodes of the instance set layer of G. Normally the key node is referenced to a verb. The graph matching algorithm can be used within the set layer the instance layer or between the set layer and instance layer of the NLSKN or between a given graph pattern and the NLSKN.

TYPE nsubj I I TYPE nsubj I I DIRECTION nsubj I I DIRECTION nsubj I I . And the referenced word node he of I is a hypernymy of the referenced word node man of I. The instance relation nsubj I I is a matching instance relation of nsubj I I . Similarly prep in I I is a matching instance relation of prep in I I nsubj I I is a matching instance relation of nsubj I I .

However when prep in I I is compared to prep in I I . The referenced word node sea of I is different and not a hypernymy of the referenced word pool of I. So prep in I I is not a matching instance relation of prep in I I .

The matching instance nodes nsubj I I and prep in I I compose to a matching graph. nsubj I I is another matching sub graph.

In some examples the NLSKN can be very big. According in some examples a pruning method is necessary for graph matching. To facilitate this process each set node or instance node in the NLSKN has an activation. Each time when a sub graph at the instance layer of NLSKN is matched or a rule is satisfied the activations of matching nodes are activated to a high level and spread to the linked nodes. The activations of all the nodes are decreased after a particular period such as per second or per minute. When the graph matching algorithm attempts to find matching graphs sub graphs for a given graph pattern G the graph matching algorithm only finds a necessary number of matching graphs sub graphs with high activations.

With reference to there is illustrated the method in which the system can conduct an assessment of the reasonability or consistency of a natural language input. In these examples any input knowledge or the knowledge represented by a sub graph of the NLSKN can be examined to assess if the knowledge is reasonable and consistent with the other knowledge in NLSKN.

The steps in which this can be conducted may include given a graph pattern G which represents the knowledge to be assessed or examined 

This reasonability consistency examination method may not be completed because of the limitation of the time and memory space consuming. It is possible to ignore some conflicts. In the NLSKN each rule has a matching number which shows how many matching graphs can be found at the instance layer for the condition pattern of the rule. To reduce the computing time the examination only considers the rules built manually or the rules with high matching numbers.

In the examination process some equivalent or approximate methods may be used for example given a graph pattern G and an additional relation R and the connected instance node i that r is connected to G. G i r G . The system finds matching graphs in NLSKN G for G. The number of matching graphs is M G . Similarly find matching graphs in NLSKN G for G . The number of matching graphs is M G . A conflict for the relation r is reported if M G M G 0 and M G t t is the threshold. When the graph matching is limited by pruning based on activations this method is an approximate method of the examination method based on probabilistic rules at the set layer.

As is the case in Natural Language the context of the Natural Language input may be important in ascertaining whether an input is correct or make sense. Accordingly in some special contexts the rules may be different from common sense. For example in a fantasy fiction context a man may fly. However in common sense a man can not fly. When the system attempts to understand the fantasy fiction some localizing rules may be needed. Normally the localizing rules are not stored in NLSKN permanently and thus in the examination process the system may discover localizing temporary graph based probabilistic rules in the knowledge limited by activations or particular input data source.

Semantic reasonability consistency examination can be used to improve the performance of dependency parsing and co reference resolution. shows the process to find the most reasonable co referenced dependency graph within multiple potential co referenced dependency graphs for an input text.

At first the input text is parsed by a dependency parser into a set of potential results Gp G . . . G . . . Gn. Each potential result Gi contains one or more dependency graphs .

Then a set of potential co reference results Rp R R . . . Rm are generated by a co reference resolution system. Each potential co reference result Rj A contains all the co referenced word pairs in the input text.

For each Gi and Rj the dependency graphs in Gi are connected by the co reference relations based on the co reference word pair in Rj. n m potential co referenced dependency graphs B DG DG DG . . . DGnm based on the combination of Gp and Rp are generated .

Each potential co referenced dependency graph DGij in DG is examined by the consistency examination algorithm to generate a conflict number .

The potential co referenced dependency graph DGij with the minimal conflict number is considered as the best understanding for the input text .

As the precision of dependency parser is not 100 . There are likely to be mistakes in the results of dependency parser. Similarly there are always mistakes in the results of co reference resolution. Most of existing dependency parsers base on syntactic analysis semantic feature is not considered. Similarly most of co reference resolution systems do not consider the semantic feature too. The automatically generated co referenced dependency graphs always have mistakes. The consistency examination can help to find the semantic conflicts in the co referenced dependency graphs based on the knowledge in NLSKN. Some dependency parsers and co reference resolution systems can provide multiple potential results or intermediate results. Eliminating the co referenced dependency graphs with more conflicts in the potential results may help to improve the quality of dependency parsing or co reference resolution if the conflicts finding is in high precision.

The conflict finding depends on the knowledge in NLSKN. When the system attempts to examine a co referenced dependency graph many relations in the co referenced dependency graph are considered as conflicts if the knowledge in NLSKN is not enough. Because these relations can not be matched by any existing condition pattern of the probabilistic rules. This phenomenon can be used to test if the system can understand an input natural language text. Based on the examination we propose an active learning method below.

As supervised learning can be expensive when the NLSKN stores a set of general knowledge the active learning can be practiced. The system attempts to learn knowledge from natural language materials.

Referring to the active learning process includes a dependency parser module and a co reference resolution module to parse the input text into potential co referenced dependency graphs a consistency examination module to find the semantic conflict in the potential co referenced dependency graphs based on the knowledge in NLSKN a module to show the potential co referenced dependency graphs to the supervisor an interface to getting the manual co reference tags from the supervisor and a process to append a co reference dependency graph to NLSKN .

The active learning method attempts to choose the training materials automatically to reduce the cost of supervising. It is based on the consistency examination method and the existing knowledge in NLSKN.

The better quantity and quality of existing knowledge leads to better learning. If a co referenced dependency graph is examined without any conflict found it shows that based on existing knowledge in NLSKN the co referenced dependency graph is reasonable. This co referenced dependency graph can be learnt directly. Based on the number of the conflicts a score is generated. If the score is same or higher than a threshold due to the enormous number of the founded conflicts it shows that the co referenced dependency graph can not be understood based on the existing knowledge in NLSKN. The co referenced dependency graph will be ignored or reserved for future learning. If the score is lower than the threshold due to a few number of the founded conflicts the system will ask the supervisor for additional supervising. The co referenced dependency graph is considered understandable but there are some problems.

Only when the system asks for an additional supervising the supervisor is needed to work. It reduces the working load of the supervisor. For the reasonable co referenced dependency graphs without conflicts the system can learn automatically.

Because the active learning method can sort the input natural language materials as understandable questioning and not understandable the supervisor may re arrange the natural language materials in the training set based on the feedback of the active learning method to help the system achieve better learning.

The examination process attempt to guarantee all the knowledge in NLSKN is consistent to each others. Because the probabilistic rules are approximately correct and the reasoning is limited by the pruning the consistency is approximate. With more and more knowledge being added to NLSKN the probabilistic rules are updated and new conflicts may be found. The system will ask for additional supervising when internal conflicts are found.

Based on the existing knowledge which may be stored in the knowledge network the NLSKN system may be arranged to answer natural language question inputted by a user or other natural language sources.

Referring to the question answer module includes a dependency parser and a co reference resolution system to generate a co referenced dependency graph from the input natural language question a reasoning module to extend the co referenced dependency graph of the question a graph matching module to find matching graphs sub graphs in NLSKN a reasoning imagination module to extend the found matching graphs sub graphs an evaluation module to calculate a score for each extended matching graph sub graph and choose the best extended matching graph sub graph and an expression module to express the best extended matching graph in natural language .

The question is processed into a co referenced dependency graph by a dependency parser and co reference resolution system. The co referenced dependency graph is considered a graph pattern to find matching sub graphs in NLSKN. Before the graph matching step the graph pattern is extended by reasoning based on the graph based probabilistic rules. In natural language a single meaning can be expressed in different ways. The extended graph pattern may include different semantic or syntactic structures to represent the same meaning of the question.

Then the system find matching sub graphs in the instance layer of NLSKN for the extended graph pattern. All these matching sub graphs are related to the question. All these matching sub graphs are extended by reasoning or imagination based on the graph based probabilistic rules. The extended matching sub graphs may include different expressions or additional knowledge which is not discovered before.

After that the system rematches the extended graph pattern and the extended matching sub graph to get updated matching sub graphs. Most of these matching sub graphs may be approximate results. Evaluate each of them to generate a score. Express the best updated matching sub graph in natural language as the answer. The expression of the matching sub graph may be composed by the original input natural language sentences to build parts of the matching sub graph or some automatically generated natural language sentences based on some predefined graph to text patterns.

These question answer method has chance to find the answers in different expression or to discover the answer in hidden knowledge.

Although not required the embodiments described with reference to the Figures can be implemented as an application programming interface API or as a series of libraries for use by a developer or can be included within another software application such as a terminal or personal computer operating system or a portable computing device operating system. Generally as program modules include routines programs objects components and data files assisting in the performance of particular functions the skilled person will understand that the functionality of the software application may be distributed across a number of routines objects or components to achieve the same functionality desired herein.

It will also be appreciated that where the methods and systems of the present invention are either wholly implemented by computing system or partly implemented by computing systems then any appropriate computing system architecture may be utilised. This will include stand alone computers network computers and dedicated hardware devices. Where the terms computing system and computing device are used these terms are intended to cover any appropriate arrangement of computer hardware capable of implementing the function described.

It will be appreciated by persons skilled in the art that numerous variations and or modifications may be made to the invention as shown in the specific embodiments without departing from the spirit or scope of the invention as broadly described. The present embodiments are therefore to be considered in all respects as illustrative and not restrictive.

Any reference to prior art contained herein is not to be taken as an admission that the information is common general knowledge unless otherwise indicated.

