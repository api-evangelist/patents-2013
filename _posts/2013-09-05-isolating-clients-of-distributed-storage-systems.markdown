---

title: Isolating clients of distributed storage systems
abstract: A distributed storage system that includes memory hosts. Each memory host includes non-transitory memory and a network interface controller in communication with the memory and servicing remote direct memory access requests from clients. The memory receives a data transfer rate from each client in communication with the memory host through remote direct memory access. Each memory host also includes a data processor in communication with the memory and the network interface controller. The data processor executes a host process that reads each received client data transfer rate, determines a throttle data transfer rate for each client, and writes each throttle data transfer rate to non-transitory memory accessible by the clients through remote direct memory access.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09313274&OS=09313274&RS=09313274
owner: Google Inc.
number: 09313274
owner_city: Mountain View
owner_country: US
publication_date: 20130905
---
A distributed system generally includes many loosely coupled computers each of which typically include a computing resource e.g. processor s and storage resources e.g. memory flash memory and or disks . A distributed storage system overlays a storage abstraction e.g. key value store or file system on the storage resources of a distributed system. In the distributed storage system a server process running on one computer can export that computer s storage resources to client processes running on other computers. Remote procedure calls RPC may transfer data from server processes to client processes.

A remote procedure call is a two sided software operation initiated by client software executing on a first machine and serviced by server software executing on a second machine. Servicing storage system requests e.g. read data in software may require an available processor which may place a significant limitation on a distributed storage system. In the case of a distributed storage system this means a client process cannot access a remote computer s storage resources unless the remote computer has an available processor to service the client s request. Moreover the demand for processor resources and storage resources in a distributed system often do not match. In particular computing resources i.e. processors may have heavy and or unpredictable usage patterns while storage resources may have light and very predictable usage patterns.

Isolating performance of users connecting to the same server is typically done by limiting or rejecting user operations at the server. If there are insufficient resources to process a request the server can reject it. For example server side resource management involves tracking the amount of resources consumed by each user on a server. As requests enter the server the server uses its global knowledge of incoming requests to decide whether or not to service a new request.

When client requests to a server are single sided operations e.g. remote direct memory access RDMA such as those in a single sided distributed storage system there is no server side processing of the request. Eliminating the server side processing not only prevents the server from rejecting requests when it becomes overloaded it prevents the server process from even detecting it is overloaded. Consequently resources shared among users clients accessing the same server such as bandwidth cannot be managed in a traditional manner. For example server side resource management does not work for RDMA requests because the server process does not service the request. The request is serviced directly by special purpose hardware. Although each client can strictly limit the rate at which requests are made to the server in order to reduce resource usage at the server the client lacks global knowledge of how much resources other clients may be using. Performance isolation among clients ensures misbehaving clients do not unnecessarily reduce the performance of well behaved clients and allow different quality of service classes to be established among clients.

One aspect of the disclosure provides a distributed storage system that includes memory hosts. Each memory host includes non transitory memory and a network interface controller in communication with the memory and servicing remote direct memory access requests from clients. The memory receives a data transfer rate from each client in communication with the memory host through remote direct memory access. Each memory host also includes a data processor in communication with the memory and the network interface controller. The data processor executes a host process that reads each received client data transfer rate determines a throttle data transfer rate for each client and writes each throttle data transfer rate to non transitory memory accessible by the clients through remote direct memory access.

Implementations of the disclosure may include one or more of the following features. In some implementations after establishing a communication connection with a client the data processor instantiates a first memory region in the non transitory memory for receiving the data transfer rate for that client and a second memory region in the non transitory memory for writing the throttle rate for that client. The host process may periodically read the first memory region for each client before determining the throttle rate for each client. In some examples the host process registers a set of remote direct memory accessible regions of the memory with the network interface controller. The host process establishes a remote direct memory access capable connection with the client in response to receiving a connection request from the client. The host process may unilaterally break the connection with a client when the client fails to adhere to its corresponding throttle data transfer rate over a period of time.

In some implementations the memory receives the client data transfer rate of a client after every transfer of a threshold amount of data between the client and the memory host. The host process may determine the throttle data transfer rate of each client after receipt of a client data transfer rate from any one client.

The host process may receive an isolation configuration providing a bandwidth capacity for the memory host and a list of bandwidth reservations for clients. Each bandwidth reservation reserves a threshold data transfer rate for a client. The host process determines the throttle data transfer rates of clients based on the isolation configuration. The bandwidth capacity of a memory host may include reserved bandwidth for servicing memory access requests associated with bandwidth reservations and flex bandwidth for servicing memory access requests unassociated with any bandwidth reservations. When determining the throttle data transfer rate of a client for any associated bandwidth reservations of the client the host process may assign reserved bandwidth equal to an equally divided share of those bandwidth reservations across the memory hosts and assign an equally divided share of flex bandwidth with respect to all clients in communication with the memory host. Moreover when determining the throttle data transfer rate of a client the host process may redistribute unused bandwidth associated with one or more bandwidth reservations of a client to other clients.

In some implementations the host process associates an isolation class with a client. The isolation class has one or more associated memory access requests. The host process determines an assigned bandwidth for each client based on the bandwidth capacity of the memory host an allotted bandwidth for each isolation class of each client based on the assigned bandwidths for each client a bandwidth for each memory access request associated with each isolation class based on the bandwidth of the corresponding isolation class and the throttle transfer rate for each client based on at least one of the assigned bandwidth of the client the allotted bandwidth for each isolation class or the bandwidth for each memory access request.

The distributed storage system may include a curator in communication with the memory hosts. The curator allocates memory in the memory hosts for data chunks of a file. In response to a memory access request from a client in communication with the memory hosts and the curator the curator returns a file descriptor to the client that maps data chunks of a file on the memory hosts for remote direct memory access of the data chunks on the memory hosts. The file descriptor includes a client key for each data chunk of the file. Each client key allows access to the corresponding data chunk on its memory host. The curator denies access to file descriptors to clients failing to adhere to their corresponding throttle data transfer rates over a period of time.

Another aspect of the disclosure provides a method of isolation in a distributed storage system. The method includes receiving into non transitory memory a data transfer rate from each client in communication with the memory through remote direct memory access and reading into a data processor in communication with non transitory memory each received client data transfer rate. The method also includes determining a throttle data transfer rate for each client and writing from the data processor each throttle data transfer rate to non transitory memory accessible by the clients through remote direct memory access.

In some implementations the method includes after establishing a communication connection with a client instantiating a first memory region in the non transitory memory for receiving the data transfer rate for that client and a second memory region in the non transitory memory for writing the throttle rate for that client. The method may also include periodically reading the first memory region for each client before determining the throttle rate for each client. The method may include registering a set of remote direct memory accessible regions of the memory with a network interface controller and establishing a remote direct memory access capable connection with the client in response to receiving a connection request from the client. If a client fails to adhere to its corresponding throttle data transfer rate over a period of time the method may include unilaterally breaking the connection with the client.

The method may include receiving the client data transfer rate of a client in the memory after every transfer of a threshold amount of data between the client and the memory. Moreover the method may include determining the throttle data transfer rate of each client after receipt of a client data transfer rate from any one client.

In some implementations the method includes receiving an isolation configuration providing a bandwidth capacity for the memory host and a list of bandwidth reservations for clients and determining the throttle data transfer rates of clients based on the isolation configuration. Each bandwidth reservation reserves a threshold data transfer rate for a client. The bandwidth capacity of a memory host may include reserved bandwidth for servicing memory access requests associated with bandwidth reservations and flex bandwidth for servicing memory access requests unassociated with any bandwidth reservations.

The step of determining the throttle data transfer rate of a client may include for any associated bandwidth reservations of the client assigning reserved bandwidth equal to an equally divided share of those bandwidth reservations across memory hosts of the distributed storage system and assigning an equally divided share of flex bandwidth with respect to all clients in communication with the memory host. The step may also include redistributing unused bandwidth associated with one or more bandwidth reservations of a client to other clients.

In some implementations the method includes associating an isolation class that has one or more associated memory access requests with a client and determining an assigned bandwidth for each client based on the bandwidth capacity of the memory host an allotted bandwidth for each isolation class of each client based on the assigned bandwidths for each client a bandwidth for each memory access request associated with each isolation class based on the bandwidth of the corresponding isolation class and the throttle transfer rate for each client based on at least one of the assigned bandwidth of the client the allotted bandwidth for each isolation class or the bandwidth for each memory access request. The method may include receiving a key with a client memory access request to receive access to data in the memory.

The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects features and advantages will be apparent from the description and drawings and from the claims.

Referring to in some implementations a distributed storage system includes loosely coupled memory hosts e.g. computers or servers each having a computing resource e.g. one or more processors or central processing units CPUs in communication with storage resources e.g. memory flash memory dynamic random access memory DRAM phase change memory PCM and or disks that may be used for caching data. A storage abstraction e.g. key value store or file system overlain on the storage resources allows scalable use of the storage resources by one or more clients . The clients may communicate with the memory hosts through a network e.g. via RPC .

The single sided distributed storage system may eliminate the need for any server jobs for responding to remote procedure calls RPC from clients to store or retrieve data on their corresponding memory hosts and may rely on specialized hardware to process remote requests instead. Single sided refers to the method by which most of the request processing on the memory hosts may be done in hardware rather than by software executed on CPUs of the memory hosts . Rather than having a processor of a memory host e.g. a server execute a server process that exports access of the corresponding storage resource e.g. non transitory memory to client processes executing on the clients the clients may directly access the storage resource through a network interface controller NIC of the memory host . In other words a client process executing on a client may directly interface with one or more storage resources without requiring execution of a routine of any server processes executing on the computing resources . This offers a single sided distributed storage architecture that offers relatively high throughput and low latency since clients can access the storage resources without interfacing with the computing resources of the memory hosts . This has the effect of decoupling the requirements for storage and CPU cycles that typical two sided distributed storage systems carry. The single sided distributed storage system can utilize remote storage resources regardless of whether there are spare CPU cycles on that memory host furthermore since single sided operations do not contend for server CPU resources a single sided system can serve cache requests with very predictable low latency even when memory hosts are running at high CPU utilization. Thus the single sided distributed storage system allows higher utilization of both cluster storage and CPU resources than traditional two sided systems while delivering predictable low latency.

In some implementations the distributed storage system includes a storage logic portion a data control portion and a data storage portion . The storage logic portion may include a transaction application programming interface API e.g. a single sided transactional system client library that is responsible for accessing the underlying data via single sided operations. The data control portion may manage allocation and access to storage resources with tasks such as allocating storage resources registering storage resources with the corresponding network interface controller setting up connections between the client s and the memory hosts handling errors in case of machine failures etc. The data storage portion may include the loosely coupled memory hosts 

In some implementations the distributed storage system stores data in dynamic random access memory DRAM and serves the data from the remote hosts via remote direct memory access RDMA capable network interface controllers . A network interface controller also known as a network interface card network adapter or LAN adapter may be a computer hardware component that connects a computing resource to the network . The network interface controller implements communication circuitry using a specific physical layer OSI layer 1 and data link layer layer 2 standard such as Ethernet Wi Fi or Token Ring. This provides a base for a full network protocol stack allowing communication among small groups of computers on the same LAN and large scale network communications through routable protocols such as Internet Protocol IP . Both the memory hosts and the client may each have a network interface controller for network communications. A host process executing on the computing processor of the memory host registers a set of remote direct memory accessible regions of the memory with the network interface controller . The host process may register the remote direct memory accessible regions of the memory with a permission of read only or read write. The network interface controller of the memory host creates a client key for each registered memory region 

The single sided operations performed by the network interface controllers may be limited to simple reads writes and compare and swap operations none of which may be sophisticated enough to act as a drop in replacement for the software logic implemented by a traditional cache server job to carry out cache requests and manage cache policies. The transaction API translates commands such a look up or insert data commands into sequences of primitive network interface controller operations. The transaction API interfaces with the data control and data storage portions of the distributed storage system .

The distributed storage system may include a co located software process to register memory for remote access with the network interface controllers and set up connections with client processes . Once the connections are set up client processes can access the registered memory via engines in hardware of the network interface controllers without any involvement from software on the local CPUs of the corresponding memory hosts .

Referring to in some implementations the distributed storage system includes multiple cells each cell including memory hosts and a curator in communication with the memory hosts . The curator e.g. process may execute on a computing processor e.g. server connected to the network and manages the data storage e.g. manages a file system stored on the memory hosts controls data placements and or initiates data recovery. Moreover the curator may track an existence and storage location of data on the memory hosts . Redundant curators are possible. In some implementations the curator s track the striping of data across multiple memory hosts and the existence and or location of multiple copies of a given stripe for redundancy and or performance. In computer data storage data striping is the technique of segmenting logically sequential data such as a file in a way that accesses of sequential segments are made to different physical storage devices e.g. cells and or memory hosts . Striping is useful when a processing device requests access to data more quickly than a storage device can provide access. By performing segment accesses on multiple devices multiple segments can be accessed concurrently. This provides more data access throughput which avoids causing the processor to idly wait for data accesses.

In some implementations the transaction API interfaces between a client e.g. with the client process and the curator . In some examples the client communicates with the curator through one or more remote procedure calls RPC . In response to a client request the transaction API may find the storage location of certain data on memory host s and obtain a key that allows access to the data . The transaction API communicates directly with the appropriate memory hosts via the network interface controllers to read or write the data e.g. using remote direct memory access . In the case that a memory host is non operational or the data was moved to a different memory host the client request fails prompting the client to re query the curator .

Referring to in some implementations the curator stores and manages file system metadata . The metadata includes a file map that maps files to file descriptors . The curator may examine and modify the representation of its persistent metadata . The curator may use three different access patterns for the metadata read only file transactions and stripe transactions. Read only access allows the curator to examine a state of the metadata with minimal contention. A read only request returns the most recent state of a file but with no synchronization with concurrent updates. The read only access may be used to respond to lookup requests from clients e.g. for internal operations such as file scanning .

Referring also to in some implementations the memory hosts store file data . The curator may divide each file and its data into stripes and replicate the stripes for storage in multiple storage locations. A stripe replica is also referred to as a chunk or data chunk . Mutable files may have additional metadata stored on the memory host s such as lock words and version numbers. The lock words and version numbers may be used to implement a distributed transaction commit protocol.

File descriptors stored by the curator contain metadata such as the file map that maps the stripes to data chunks i.e. stripe replicas stored on the memory hosts . To open a file a client sends a request to the curator which returns a file descriptor . The client uses the file descriptor to translate file chunk offsets to remote memory locations . After the client loads the file descriptor the client may access the data of a file via RDMA or another data retrieval method.

Referring to RDMA is a connection based process to process communication mechanism so RDMA connections typically do not support authentication or encryption by themselves. As a result the distributed storage system may treat the RDMA connections as secure resources. In order for a client process to access the memory of a host process through RDMA the network interface controller of the memory host executes a connection handshake with a network interface controller of the client process to establish the RDMA capable connection between the host process and the client process . The RDMA connection handshake may implement a higher level secure protocol that evaluates the identities of the host and client processes as known at the time of creation of the trusted RDMA connection . After an RDMA capable connection is established the client process or the host process can unilaterally break the connection . If either the client process or the host process dies the client and or the memory host via operating systems can tear down the corresponding RDMA connection s .

Access to file data e.g. data chunks stored in remote memory locations may be controlled by access control lists . Each access control list may have a unique name a list of data chunks and a list of clients that have permission to read and write the data chunks associated with that access control list . In some examples the access control list provides an access permission level for each associated client or each associated data chunk . The memory hosts may receive the access control lists through a secure communication channel and can be enforced by the memory hosts using protection domains . Each RDMA accessible memory region registered with the network interface controller of each memory host is associated with a protection domain . In some implementations when the curator allocates memory for the data chunks it associates the allocated memory regions of the data chunks with one or more protection domains . A memory host may have many protection domains associated with various regions of its memory . Each protection domain may also have one or more associated connections .

When a client instantiates a memory access request for a file stored on one or more of the memory hosts the client requests a file descriptor from the curator to identify which memory host s store the data chunks of the file . In addition to mapping data chunks of the file to memory regions of memory hosts the file descriptor may also include a client key for accessing those data chunks . The client then searches a connection cache for any open RMDA capable connections to the identified memory hosts . If each memory host fails to have an open connection with the client that is in the same protection domain as the requested data chunk s the client sends a connection request to any memory hosts not having the necessary open connection s .

In response to receiving a connection request from a client process of a client to access a data chunk e.g. to access a memory region storing the data chunk the host process may establish a remote direct memory access capable connection with the client process when both the client and the requested data chunk are associated with the same access control list received by the memory host . The client process may include the access control list in the connection request . The host process may associate the established open connection with a protection domain and the client process may store the open connection in the connection cache . The connection is capable of accessing via RDMA only the memory regions associated with its protection domain . The network interface controller of the memory host may tear down the connection upon receiving an RDMA request having an address for unregistered memory .

In the example shown in first and second clients send memory access requests to a memory host over respective first and second RDMA connections . The memory host has first and second protection domains associated with its memory . The first protection domain is associated with first and second memory regions e.g. storing corresponding first and second data chunks and the first RDMA connection while the second protection domain is associated with a third memory region e.g. storing a corresponding third data chunks and only the second RDMA connection

The first client sends first and second memory access requests over the first RMDA connection to the memory host . The first memory access request is for accessing the second memory region for the second data chunk and the second memory access request is for accessing the third memory region for the third data chunk . The first memory access request succeeds because the second memory region belongs to the same protection domain as the first connection . The second memory access request fails because the third memory region belongs to a different protection domain the second protection domain rather than the protection domain of the second memory access request i.e. the first protection domain .

The second client sends third and fourth memory access requests over the second RDMA connection to the memory host . The third memory access request is for accessing the first memory region for the first data chunk and the fourth memory access request is for accessing the third memory region for the third data chunk . In this case both memory access requests succeed because the RDMA connection of the second client belongs to the protection domains of both the first memory region and the third memory region

When client requests to a memory host e.g. server are single sided operations e.g. remote direct memory access RDMA there is no server side processing of the request. Eliminating the server side processing not only prevents the memory host from rejecting requests when it becomes overloaded it prevents the server process from even detecting that the memory host is overloaded. Consequently bandwidth for computing resources and or storage resources shared among clients cannot be managed in a traditional manner. For example server side resource management does not work for RDMA requests because the host process does not service the request . The request is serviced directly by special purpose hardware the network interface controller . Although each client can strictly limit the rate at which requests are made to the memory host in order to reduce resource usage at the memory host the client lacks global knowledge of how much resources other clients may be using. Performance isolation among clients ensures misbehaving clients do not unnecessarily reduce the performance of well behaved clients and allow different quality of service classes to be established among clients .

Referring again to in some implementations each client tracks the amount of data transferred between it and each memory host and writes a transferred data amount also referred to as bytes transferred to an RDMA accessible memory region on the memory host . In other words each client keeps a running sum of the total number of bytes read written to the memory host and periodically writes this sum to the memory host . Each client has its own memory region on each memory host . The memory host creates and initializes the memory region at connection setup time and sends a location of the memory region to the client upon the initial establishment of the connection . The client writes the memory region after a threshold change in the amount of data transferred e.g. 128 kilobytes. This policy causes clients that are using more bandwidth and are more likely to require throttling to update their bytes transferred more frequently and low bandwidth clients to communicate less frequently with the memory hosts . The threshold for pushing an update of bytes transferred could vary widely based on the actual implementation network etc. Requiring clients to push their bytes transferred to the memory host simplifies server isolation logic and in turn greatly reduces CPU usage.

Periodically the memory host scans the memory regions containing clients bytes transferred e.g. every 100 milliseconds computes bandwidth usage and computes client bandwidth shares also referred to as throttle rates for the memory host . In some implementations the memory host periodically reads the bytes transferred amount e.g. a sum compares it to the last sum it read and computes a data rate for the client from the difference. The scan rate may be implementation dependent. Shorter times between scans result in finer grain control of the clients but the tradeoff is higher server CPU utilization. The memory host writes each client s computed throttle rate to another local memory region . The client reads this throttle rate from the memory host e.g. via RDMA when the client writes the bytes transferred to the memory host . The client limits its data rate to the throttle rate most recently read from the memory host e.g. via RDMA.

The client is responsible for reading its current throttle rate from the memory host and self enforcing that throttle rate . The client is also responsible for tracking and periodically writing its own bytes transferred to the memory host . This gives the memory host the required global knowledge of data rates for each connected client without the host process tracking each RDMA request the hardware processes. With this information the memory host can partition bandwidth for each client and compute the appropriate throttle rates . The bandwidth of a memory host may include a reserved portion reserved bandwidth and a flex portion flex bandwidth . In some implementations flex bandwidth is any unused reserved bandwidth .

By default every client may receive an even share of bandwidth from a memory host . The memory host may be work conserving by distributing any unused bandwidth among clients that can use it. In some implementations bandwidth is not distributed equally. First the memory host assigns reserved bandwidth to each client and any unused reserved bandwidth is placed in a flex pool as flex bandwidth . The memory host may divide the flex pool or flex bandwidth equally among clients with the exception that a client that doesn t need the flex bandwidth won t take it from the pool but instead leaves it to be equally divided among clients that can make use of the extra bandwidth . If the amount of bandwidth available to a client is insufficient or the client requires bandwidth guarantees e.g. since even share bandwidth can vary over time the client may request assignment of reserved bandwidth to an isolation class .

Isolation classes allow requests running as the same client to receive differentiated service. A client can have multiple associated isolation classes . A memory host may define an isolation class using an identifier such as a client name plus an arbitrary string. A client may execute an application having one or more client requests . Each client may have one or more associated isolation classes and each isolation class can contain one or more client requests . A client flag may determine which isolation class a request should use. Alternatively the isolation class may be specified by the client on a per request basis so a single client can use multiple isolation classes . Requests running as different clients may not share the same isolation class because an isolation class is a child of the client . An alternative implementation could have isolation classes spanning multiple clients . An isolation class may be allocated flex bandwidth plus reserved bandwidth .

In some implementations client isolation class and client requests form a hierarchical relationship. Each client may have one or more associated isolation classes and each isolation class may have one or more associated client requests . The memory host may first divide its bandwidth among clients . Then for each client the memory host divides the assigned bandwidth for the respective client among its associated isolation classes . Then for each isolation class the memory host divides the corresponding assigned bandwidth among associated client requests .

Each cell has a rated capacity in terms of bytes per second. In principle the rated capacity of a cell is the amount of data that clients can read from and write to the cell per second. In practice the rated capacity of a cell may be divided evenly over the memory hosts in the cell and enforced on a per memory host basis. For example a cell with 1000 memory hosts and a rated capacity of 1 TB s may need to offer at least 1 GB s of load on each memory host in the cell in order to serve 1 TB s of data . The rated bandwidth capacity of a memory host can be less than the network interface controller bandwidth of the memory host but not greater than the network interface controller bandwidth.

Memory hosts access and compute shares of bandwidth according to a cell isolation configuration e.g. stored as a file . The cell isolation configuration includes a rated bandwidth capacity of each memory host in megabytes per second and a list of bandwidth reservations . Each bandwidth reservation includes a client name isolation class and a bandwidth specified in megabytes per second. In some examples the isolation configuration does not provision flex bandwidth which may be any unused reserved bandwidth .

In some implementations the isolation policy as set forth by a cell isolation configuration only applies to memory hosts that are in danger of exceeding their rated bandwidth capacity . Once engaged the isolation policy aims to distribute the bandwidth of the memory host fairly among the clients actively accessing that memory host . The isolation policy may attempt to distribute bandwidth evenly to active clients up to the offered load of the client . For example a memory host with a 1 GB s rated bandwidth capacity and four active clients that have offered loads of 0.1 0.2 0.4 and 0.8 GB s then a fair bandwidth distribution may be 0.1 0.2 0.35 and 0.35 GB s respectively.

In some examples a client may access an overloaded memory host from multiple processes within a data center. In this case the isolation policy distributes the fair share of bandwidth of the client evenly among the isolation classes of the client and the tasks that are actively accessing the memory host . In other words each client is assigned bandwidth of the memory host then each isolation class associated with that client splits the assigned bandwidth and then each client request within an isolation class splits the isolation class bandwidth .

Clients that need more than their fair share of bandwidth of a cell can reserve bandwidth . Bandwidth reservations are in terms of bytes per second for the entire cell . A bandwidth reservation is distributed evenly over all of the memory hosts in the cell . For example if a cell has 1000 memory hosts and a client reserves 500 GB s of cell bandwidth then the client is guaranteed to receive at least 0.5 GB s of bandwidth from each memory host in the cell . If a client does not use its reserved bandwidth the memory host may distribute the reserved bandwidth of that client to other clients that can use the bandwidth .

Bandwidth reservations can affect the fair share bandwidth of other clients . Using the earlier example in which a memory host with a 1 GB s rated bandwidth capacity and four active clients that have offered loads of 0.1 0.2 0.4 and 0.8 GB s if the client with the 0.8 GB s offered load reserves 0.2 GB s of the bandwidth of the memory host then the pool of available flex bandwidth of the memory host is only 0.8 GB s. Taking into account this bandwidth reservation the isolation policy may distribute 0.1 0.2 0.25 and 0.45 0.2 reserved 0.25 flex GB s of bandwidth to the clients respectively.

When a memory host detects it is above its rated bandwidth capacity the memory host throttles clients that are using more than their share of the memory host bandwidth . Each client may use a leaky bucket scheme to throttle its accesses to a specific memory host . The memory host controls the fill rate of the client s leaky bucket by periodically re computing bandwidth shares and updating the client s leaky bucket fill rate. In some examples each client data channel has a leaky bucket with a max capacity of 128 KB but other capacities are possible as well and may be implementation dependent. The capacity of the leaky bucket determines the maximum burst rate a client can achieve. This allows their instantaneous data rate to temporarily exceed their throttle rate . Before initiating an RDMA operation the client requests tokens from the appropriate leaky bucket. The number of requested tokens is equal to the payload size of the RDMA operation. If there are enough tokens available the operation proceeds if not the data channel indicates a temporary error has occurred and the operation should be retried later. The client may have logic in place for handling other temporary data channel errors. The fill rate of the leaky bucket is set to the current throttle rate assigned by the memory host .

Memory hosts may also validate that clients are respecting throttling requests and blacklist misbehaving clients that are not respecting throttling requests. Blacklisting may be accomplished by tearing down all the RDMA connections between the memory host and blacklisted client .

The memory host assigns clients their reserved bandwidth and their fair share of the flex bandwidth of the memory host as an assigned bandwidth . If the offered load of a client is less than the assigned bandwidth the unused portion of the reserved bandwidth is distributed to other clients . Therefore the assigned bandwidth i.e. a client s share of a memory host s bandwidth changes dynamically based on the bandwidth usage of other clients . The assigned bandwidth share may be valid for roughly 100 ms and the memory host may recompute the assigned bandwidth share of a client in another 100 ms.

In some implementations an algorithm for computing the assigned bandwidth share of a client for a memory host is 

In some implementations for isolation classes and client processes the memory host computes the assigned bandwidth for each client as constrained by the rated bandwidth of the memory host a bandwidth for each isolation class as constrained by the assigned bandwidth allotted to the client and the bandwidth for each client process as constrained by the bandwidth allotted to the isolation class of which it is a member. In some examples individual client requests may or may not have reserved bandwidth .

After computing the assigned bandwidth for each client request the memory hosts adjusts a current throttle rate for each client request to approach the assigned bandwidth share . Since application burstiness can keep the application from ever reaching its target bandwidth share the memory host may adjust the throttle rate to account for this burstiness and to more efficiently use the memory host bandwidth .

After computing the client bandwidth shares the memory host may either perform an additive increase of the client throttle rate if the measured data rate of the client is less than its assigned bandwidth or cut the client throttle rate down to the assigned bandwidth if the measured data rate of the client request is greater than its target bandwidth share .

Memory hosts may communicate throttle rates to clients by writing the throttle rate to a local RDMA accessible memory region . Clients RDMA read their throttle rate from the memory region e.g. when a client writes its bytes transferred to the memory host i.e. after every 128 KB of data transferred . Again this causes clients that are using more bandwidth and are more likely to require throttling to update their data rate more frequently. The client may also RDMA read the throttle rate when it cannot read or write due to throttling. This read may be rate limited to once every 100 ms.

The curator s may incorporate memory host throttling information into its their load balancing policies for example to minimize throttling a client due to placement of too many chunks of that client on a single memory host . The curator may receive a status from each memory host that includes throttling information e.g. whether the memory host is over its rated bandwidth and which clients are being throttled. If a client is being throttled on many memory hosts across a cell the cell may alert the client that it s using too much bandwidth . If a client is being throttled on a single memory host or a small number of memory hosts the curator s may migrate the chunks of that client on the overloaded memory host s to other memory hosts in the cell . If the condition persists the throttling may be caused by hot chunks . The throttling information may be monitored to detect when the cell is overloaded and whether more memory hosts and bandwidth need to be added to the cell .

Referring again to in some implementations the curator can create copy resize and delete files . Other operations are possible as well. To service a copy request from a client the curator creates a new file descriptor having a state initially set to COPY PENDING. The curator may set initialize one or more of the following fields size owner group permissions and or backing file. The curator populates a stripes array of the file descriptor with empty stripes and then commits the file descriptor to its file map . Committing this information to the file map allows the curator to restart a resize operation if the curator crashes or a tablet containing the file system metadata migrates to another curator . Once the curator commits the file descriptor to the file map the curator responds to the client copy request by informing the client that the copy operation has been initiated. The curator initiates memory host pull chunk operations which instruct memory hosts to allocate a new chunk and to read chunks of the backing file into the memory of the memory hosts . When a pull chunk operation returns successfully the curator adds the new chunk to the appropriate stripe in the file descriptor . The curator commits the stripe with the new chunk to the file map .

In the case of a crash or a migration incrementally updating the file descriptors allows a new curator to restart a copy operation from the location the prior curator stopped. This also allows clients to check the status of a copy operation by retrieving the file descriptor e.g. via a lookup method and inspecting the number of stripes in the file descriptor populated with chunks . Once all chunks have been copied to the memory hosts the curator transitions the state of the file descriptor to READ and commits it to the file map .

The curator may maintain status information for all memory hosts that are part of the cell . The status information may include capacity free space load on the memory host latency of the memory host from a client s point of view and a current state. The curator may obtain this information by querying the memory hosts in the cell directly and or by querying a client to gather latency statistics from a client s point of view. In some examples the curator uses the memory host status information to make rebalancing draining recovery decisions and allocation decisions.

The curator s may allocate chunks in order to handle client requests for more storage space in a file and for rebalancing and recovery. The curator may maintain a load map of memory host load and liveliness. In some implementations the curator allocates a chunk by generating a list of candidate memory hosts and sends an allocate chunk request to each of the candidate memory hosts . If the memory host is overloaded or has no available space the memory host can deny the request. In this case the curator selects a different memory host . Each curator may continuously scan its designated portion of the file namespace examining all the metadata every minute or so. The curator may use the file scan to check the integrity of the metadata determine work that needs to be performed and or to generate statistics. The file scan may operate concurrently with other operations of the curator . The scan itself may not modify the metadata but schedules work to be done by other components of the system and computes statistics.

The file descriptor may provide the state of a file . A file can be in one of the following states READ READ WRITE DELETED or CREATE COPY RESIZE PENDING. In the READ state clients can read the file but not write to the file . Read only files are read only for the entire life time of the file i.e. read only files are never written to directly. Instead read only files can be copied into the file system from another file system. A backing file may be used to restore data when a memory host crashes consequently the backing file persists for the entire life time of the file . In the READ WRITE state clients with the appropriate permissions can read and write a mutable file s contents. Mutable files support concurrent fine grain random writes. Random and sequential write performance may be comparable. Writes are strongly consistent that is if any client can observe the effect of a write then all clients can observe the effect of a write. Writes can also be batched into transactions. For example a client can issue a batch of asynchronous writes followed by a sync operation. Strong consistency and transactional semantics ensure that if any client can observe any write in a transaction then all clients can observe all writes in a transaction. In the DELETED state the file has been deleted. The chunks belonging to the file are stored in a deleted chunks field and wait for garbage collection. The CREATE COPY RESIZE PENDING state denotes a file has a create copy or resize operation pending on the file.

An encoding specified by a file encoding protocol buffer of the file descriptor may be used for all the stripes within a file . In some examples the file encoding contains the following fields data chunks which provides a number of data chunks per stripe stripe length which provides a number of bytes per stripe and sub stripe length which provides a number of bytes per sub stripe. The sub stripe length may be only valid for READ WRITE files. The data for a file may be described by an array of stripe protocol buffers in the file descriptor . Each stripe represents a fixed region of the file s data identified by an index within the array. The contents of a stripe may include an array of chunk protocol buffers each describing a chunk within the stripe including a chunk handle an identity of the memory host holding the chunk and a current state of the chunk . For RDMA purposes the chunk protocol buffers may also store a virtual address of the chunk in the memory host and a client key e.g. a 32 bit key. The client key is unique to a chunk on a memory host and is used to RDMA read that chunk .

Stripes can be further divided into sub stripes with associated sub stripe metadata . Each sub stripe may include an array of sub chunks each having corresponding associated sub chunk metadata .

Referring to the transaction API may facilitate transactions having atomicity consistency isolation durability to a degree such that the transaction may be serializable with respect to other transactions. ACID atomicity consistency isolation durability is a set of properties that guarantee that database transactions are processed reliably. In some implementations the transaction API includes a reader class and a transaction class . A client may instantiate a reader inheriting the reader class to execute a read or batches of reads on the memory hosts in a cell . Moreover the client may instantiate a transaction inheriting the transaction class to execute one or more reads and or writes. The reads and writes in a transaction may be to different files in a cell but in some implementations all reads and writes in a transaction must be to files in the same cell . Executed reads may be snapshot consistent meaning that all reads in a transaction can see a snapshot of the file at a logical instant in time. Writes can be buffered until the client tries to commit the transaction

Referring to in response to receiving a write memory access request for a file a transaction may acting as a writer write or modify data of the file e.g. of chunks and or sub chunks . After the write operation the transaction may compute a checksum of the modified data and associate the checksum with the modified data e.g. with the chunks and or sub chunks . In some examples the transaction stores the checksum in the sub chunk metadata for the modified sub chunk . The transaction may execute a hash function such as a cryptographic hash function to compute the checksum . Moreover the hash function may be configured for randomization. Each checksum may be a word having at least 64 bits. A network interface controller servicing the remote direct memory access requests on a corresponding memory host may determine the checksum of any data accessed on its memory host .

When a client adds a file read request to the reader e.g. via a transaction the reader translates the read request into a RDMA read network operation and stores a state of the network operation in memory allocated for the reader . Reads that cross chunk boundaries get translated into multiple RDMA operations.

In some implementations to translate a file read request into a RDMA read network operation the reader computes a target stripe number from a file offset of the read request . The reader may use the stripe number to index into a chunk handle cache. The chunk handle cache returns a network channel to access the corresponding chunk and a virtual address and r key of the chunk . The reader stores the network channel and r key directly in an operation state of the RDMA read. The reader uses the virtual address of the chunk and the file offset to compute the virtual address within the chunk to read. The reader computes the offset into a memory block supplied by the client e.g. a receiving memory block for each RDMA read operation . The reader may then initialize an operation status.

While buffering new reads the reader may calculate and store a running sum of the amount of metadata that will be retrieved to complete the read. This allows metadata buffer space to be allocated in one contiguous block during execution minimizing allocation overhead.

In response to receiving a memory access request from the client the transaction may retrieve a file descriptor from the curator that maps requested data chunks of a file on memory hosts for remote direct memory access of those data chunks on the memory hosts . The file descriptor may include a client key for each data chunk of the file . Moreover each client key allows access to the corresponding data chunk on its memory host .

Referring to in some implementations the reader executes a read operation in two phases. In the first phase the reader reads the data and associated metadata of a file . In the second phase the reader validates that the data read in the first phase satisfies data consistency constraints of the reader . In the first phase the reader identifies one or more memory locations corresponding to the data and transmits its RDMA read operations. While iterating through and transmitting RDMA reads the reader initializes and transmits RDMA reads to read sub chunk metadata and to read data needed to compute checksums of the sub chunks such as of the first and last sub chunks in an unaligned file access. After the data and metadata are received the reader may check lock words in the sub chunk metadata to ensure that the sub chunks were not locked while the data was being read. If a sub chunk was locked the reader rereads the sub chunk and its corresponding metadata . Once the reader finds reads all of the sub chunk locks in an unlocked state the reader computes the sub chunk checksums and compares the computed checksums with the checksums read from the sub chunk metadata .

In other words for detecting read write conflicts the reader in response to receiving a read memory access request for data of a file stored in the memory hosts of a cell may compute a first checksum of the data compare the first checksum with a second checksum associated with the data e.g. stored in the metadata of the corresponding sub chunk and allow a read operation on the data when the first and second checksums match. The reader may execute a hash function such as a cryptographic hash function to compute the checksums . The reader may read the data and metadata associated with the data after receiving the read write request and before processing the read write request . Moreover the reader may determine whether the data was locked while reading the data for example by evaluating a lock word and or a version number stored in the metadata . The reader rereads the data and associated metadata when the data was locked while previously reading the data .

While checksums are commonly used to guard against hardware error or even software error using it to guard against what is actually normal operation poses certain additional requirements. Since a conflict may not be a rare event the chance of getting a coincidentally matching checksum can be minimized by having checksum size large enough to provide a relatively small probability of a coincidental match. In some examples a 64 bit checksum is sufficient since checking a random bad checksum every nanosecond may produce a false positive less than once every five centuries which is much less frequent than the rates of other types of system failures. Additionally a hash function for computing the checksum may produce different numbers for all common modifications of the data . For example simply adding up all the data would not suffice since a change that simply re ordered some of the data would not change the checksum . However a cryptographic hash functions which by design does not allow simple modifications of the data to produce any predictable checksum may be sufficient.

A sub chunk checksum may fail a compare for one of three reasons 1 the data read was corrupted by a concurrent write 2 the data was corrupted while in transit to the client or 3 the data stored in the memory host is corrupt. Cases 1 and 2 are transient errors. Transient errors are resolved by retrying the sub chunk read. Case 3 is a permanent error that may require the client to notify the curator of a corrupt sub stripe

To differentiate between a transient error and a permanent error the client may re read the sub chunk data and the sub chunk metadata . The reader then checks a sub chunk lock word and re computes and compares the sub chunk checksum . If the checksum error still exists and a sub chunk version number has changed since the sub chunk was initially read then the checksum compare failure was likely caused by a concurrent write so the reader retries the sub chunk read. If the version number has not changed since the sub chunk was initially read then the error is permanent and the reader notifies the curator and the curator tries to reconstruct the data of the chunk . If the curator is unable to reconstruct the chunk data the curator replaces the old chunk with a new uninitialized chunk .

Unlike locking the checksum compare method for detecting read write conflicts does not actually care if a conflicting write existed as long as the data is consistent. For example if the data is being overwritten with identical data or if a write is preparing to start but has not actually begun or has just finished the locking method will cause the read to fail unnecessarily while the checksum compare will allow the read to succeed. Since the time between locking and unlocking may be much greater than the duration of an actual write this can be a significant improvement.

The reader does not know which version of the data it has read and it may not matter. If it is advantageous to have the read transaction obtain a version number this may be done without an additional round trip latency penalty if the version number itself is covered by the checksum . Although computing checksums may incur a nontrivial penalty in processor time both for the reader and the writer a checksum may be necessary anyway to guard against hardware errors depending on the implementation.

Sub chunk locks may become stuck due to a client trying to execute a transaction but crashing during a commit protocol of the transaction . A reader can detect a stuck lock by re reading the sub chunk lock word and version number . If a sub chunk lock word and version number do not change during some time out period then the sub chunk lock is likely stuck. When the reader detects a stuck lock it notifies the curator of the stuck lock and the curator recovers the sub stripe and resets the stuck lock.

Referring also to in some implementations after the reader validates each sub chunk lock word and or checksum the reader may proceed to the second phase of executing the read operation i.e. the validation phase . To validate the values the reader rereads sub chunk metadata and rechecks if the sub chunk lock words are unlocked and the sub chunk version numbers have not changed since the version numbers were initially read during the first phase of the read operation. In other words the reader may read an initial version number and an initial lock value associated with each data chunk of a read set of the transaction . After reading the data the reader reads a final version number and a final lock value associated with each data chunk of the read set and determines the read data as valid when the initial version number matches the final version number and the initial lock value matches the final lock value

If the reader is associated with a transaction the reader may reread the metadata associated with all sub chunks read by the transaction . If a single sub chunk version number mis compares the reader returns an error. If all sub chunk version numbers are the same the reader discards the prefix and suffix of the reader memory block in order to trim extraneous data read to compute the checksum of the first and last sub chunks in the read. The reader may set a status to OK and return to the client .

If the reader encounters an error on a network channel while reading data or metadata of a chunk the reader may select a different chunk from the chunk handle cache and notifies the curator of a bad memory host. If no other good chunks exist from which the reader can read the reader may wait to receive a response to the error notification it sent to the curator . The response from the curator may contain an updated file descriptor that contains a new good chunk to read from.

In some implementations the transaction class uses validation sets to track which sub stripes have been read by the transaction . Each read of a transaction adds the version numbers of all sub stripes read to a validation set of the transaction . The transaction may validate the validation set in two cases 1 as part of the commit protocol and 2 the validation phase of reads of a transaction . A transaction may fail to commit if the commit protocol finds that any sub stripe version number differs from the number recorded in the validation set . Validation of the full validation set before data is returned to the client allows early detection e.g. before the commit phase of a doomed transaction . This validation also prevents the client from getting an inconsistent view of file data .

A transaction may provide a synchronous serializable read operation e.g. using a reader . In some examples a reader is instantiated and associated with the transaction . Read results of the reader return the latest committed data . As such uncommitted writes of the same transaction are not seen by a read of that transaction

A transaction may buffer data for a later transaction commit. The transaction class translates a buffer write request into one or more prepare write network operations. One network operation is needed for each stripe touched by the write operation. Processing a buffer write request may involve preparing sub stripe lock network operations. One lock operation is needed for each sub stripe touched by the requested write. These operations are buffered for transmission during the transaction commit. The transaction may translate buffer write requests into network operations and execute identify or coalesce writes that affect the same region of a file . The transaction may apply write operations in the same order by the memory hosts for all chunks to ensure that all replicas are consistent.

The transaction may provide a commit operation that results in all reads and writes in the transaction being schedulable as a single atomic serializable operation. In some implementations the transaction commit protocol proceeds through a lock phase a validate phase a write phase and an unlock phase. During the lock phase the sub stripe lock network operations which were created in response to buffer write requests are sent. Each sub stripe lock operation executes an atomic compare and swap operation on the lock word in all replicas . If the contents of the lock word match the specified compare data e.g. a client identifier the lock word is written with the specified swap data and the previous contents of the word are returned. If the client succeeds in writing its unique client ID into the metadata lock word it has successfully taken the lock. If the transaction fails to take the lock for any sub stripe in the write set the commit fails and is aborted. The commit protocol proceeds to the validate phase once all sub stripe locks are held.

During the validate phase the transaction may read the version number out of the metadata for all sub stripes referenced in the validation set and comparing the version numbers to the version numbers recorded in the validation set. If a version number does not match the sub stripe was written by another transaction after it was read by this transaction so the transaction fails. In this case the reader releases the locks it holds and returns a transaction conflict error to the client . Once all version numbers in the validation set have been validated the client writes the buffered write data of the transaction to each replica and updates the metadata associated with each sub stripe written by the transaction during the write phase. Updating metadata of a sub stripe may include computing and writing a new check word and incrementing the version number of the sub stripe . Once all data and metadata has been updated the transaction releases the locks that it holds during the unlock phase.

For data chunks of a read set of the transaction a method executing a transaction may include reading data of the data chunks of the read set through remote direct memory access and determining a validity of the read data by evaluating a lock and a version of each data chunk of the read set . For data chunks of a write set of the transaction the method may include setting locks on the data chunks of the write set writing data to the locked data chunks through remote direct memory access releasing the locks of the locked data chunks and incrementing a version number of each released data chunk .

File transaction access may provide exclusive read write access to the state of a file descriptor . Updates to the file state may be applied at the end of a transaction and are atomic. File transaction access can be used for operations such as creating finalizing and deleting a file . These operations may require the curator to communicate with other components such as memory hosts and thus a file transaction access may last for several seconds or more. While active the file transaction access blocks any other operations that need to modify the state of the file descriptor . Read access may not be blocked.

To reduce contention stripe transaction access may provide relatively finer grain synchronization for operations that only need to modify the state of a single stripe with the file descriptor . This mode can be used for stripe operations such as opening closing rebalancing and recovering. There can be many concurrent stripe transactions for different stripes within a file but stripe transactions and file transactions are mutually exclusive. Within a stripe transaction the curator may examine the state of a stripe and various fields of the file descriptor that remain immutable for the duration of the transaction such as the file encoding and instance identifier. The stripe transaction access does not provide access to fields that can change underfoot such as the state of other stripes . Operations may hold only one active transaction at a time to avoid deadlock. Moreover transactions may only atomically commit on a single file .

In some implementations the method includes after establishing a communication connection with a client instantiating a first memory region in the non transitory memory for receiving the data transfer rate for that client and a second memory region in the non transitory memory for writing the throttle rate for that client . The method may also include periodically reading the first memory region for each client before determining the throttle rate for each client . The method may include registering a set of remote direct memory accessible regions of the memory with a network interface controller and establishing a remote direct memory access capable connection with the client in response to receiving a connection request from the client . If a client fails to adhere to its corresponding throttle data transfer rate over a period of time the method may include unilaterally breaking the connection with the client .

The method may include receiving the client data transfer rate of a client in the memory after every transfer of a threshold amount of data between the client and the memory . Moreover the method may include determining the throttle data transfer rate of each client after receipt of a client data transfer rate from any one client .

In some implementations the method includes receiving an isolation configuration providing a bandwidth capacity for the memory host and a list of bandwidth reservations for clients and determining the throttle data transfer rates of clients based on the isolation configuration . Each bandwidth reservation reserves a threshold data transfer rate for a client . The bandwidth capacity of the memory host may include reserved bandwidth for servicing memory access requests associated with bandwidth reservations and flex bandwidth for servicing memory access requests unassociated with any bandwidth reservations 

The step of determining the throttle data transfer rate of a client may include for any associated bandwidth reservations of the client assigning reserved bandwidth equal to an equally divided share of those bandwidth reservations across the memory hosts and assigning an equally divided share of flex bandwidth with respect to all clients in communication with the memory host . The step may also include determining the throttle data transfer rate of a client includes redistributing unused bandwidth associated with one or more bandwidth reservations of a client to other clients .

In some implementations the method includes associating an isolation class that has one or more associated memory access requests with a client and determining an assigned bandwidth for each client based on the bandwidth capacity of the memory host an allotted bandwidth for each isolation class of each client based on the assigned bandwidths for each client a bandwidth for each memory access request associated with each isolation class based on the bandwidth of the corresponding isolation class and the throttle transfer rate for each client based on at least one of the assigned bandwidth of the client the allotted bandwidth for each isolation class or the bandwidth for each memory access request .

Various implementations of the systems and techniques described here can be realized in digital electronic circuitry integrated circuitry specially designed ASICs application specific integrated circuits computer hardware firmware software and or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and or interpretable on a programmable system including at least one programmable processor which may be special or general purpose coupled to receive data and instructions from and to transmit data and instructions to a storage system at least one input device and at least one output device.

These computer programs also known as programs software software applications or code include machine instructions for a programmable processor and can be implemented in a high level procedural and or object oriented programming language and or in assembly machine language. As used herein the terms machine readable medium and computer readable medium refer to any computer program product apparatus and or device e.g. magnetic discs optical disks memory Programmable Logic Devices PLDs used to provide machine instructions and or data to a programmable processor including a machine readable medium that receives machine instructions as a machine readable signal. The term machine readable signal refers to any signal used to provide machine instructions and or data to a programmable processor.

Implementations of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry or in computer software firmware or hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. Moreover subject matter described in this specification can be implemented as one or more computer program products i.e. one or more modules of computer program instructions encoded on a computer readable medium for execution by or to control the operation of data processing apparatus. The computer readable medium can be a machine readable storage device a machine readable storage substrate a memory device a composition of matter effecting a machine readable propagated signal or a combination of one or more of them. The terms data processing apparatus computing device and computing processor encompass all apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can include in addition to hardware code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them. A propagated signal is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.

A computer program also known as an application program software software application script or code can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.

The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Moreover a computer can be embedded in another device e.g. a mobile telephone a personal digital assistant PDA a mobile audio player a Global Positioning System GPS receiver to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user one or more aspects of the disclosure can be implemented on a computer having a display device e.g. a CRT cathode ray tube LCD liquid crystal display monitor or touch screen for displaying information to the user and optionally a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input. In addition a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user for example by sending web pages to a web browser on a user s client device in response to requests received from the web browser.

One or more aspects of the disclosure can be implemented in a computing system that includes a backend component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a frontend component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification or any combination of one or more such backend middleware or frontend components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN an inter network e.g. the Internet and peer to peer networks e.g. ad hoc peer to peer networks .

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other. In some implementations a server transmits data e.g. an HTML page to a client device e.g. for purposes of displaying data to and receiving user input from a user interacting with the client device . Data generated at the client device e.g. a result of the user interaction can be received from the client device at the server.

While this specification contains many specifics these should not be construed as limitations on the scope of the disclosure or of what may be claimed but rather as descriptions of features specific to particular implementations of the disclosure. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable sub combination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a sub combination or variation of a sub combination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multi tasking and parallel processing may be advantageous. Moreover the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly other implementations are within the scope of the following claims. For example the actions recited in the claims can be performed in a different order and still achieve desirable results.

