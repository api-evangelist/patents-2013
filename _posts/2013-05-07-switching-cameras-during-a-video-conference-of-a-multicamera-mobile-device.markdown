---

title: Switching cameras during a video conference of a multi-camera mobile device
abstract: Some embodiments provide a method for conducting a video conference between a first mobile device and a second device. The first mobile device includes first and second cameras. The method selects the first camera for capturing images. The method transmits images captured by the first camera to the second device. The method receives selections of the second camera for capturing images during the video conference. The method terminates the transmission of images captured by the first camera and transmits images captured by the second camera of the first mobile device to the second device during the video conference.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09055185&OS=09055185&RS=09055185
owner: APPLE INC.
number: 09055185
owner_city: Cupertino
owner_country: US
publication_date: 20130507
---
This application is a continuation application of U.S. patent application Ser. No. 12 794 775 filed Jun. 6 2010 now published as U.S. Publication 2011 0249078. U.S. patent application Ser. No. 12 794 775 claims the benefit of U.S. Provisional Application 61 321 871 entitled Dual Camera Mobile Device with Video Conferencing Capabilities filed Apr. 7 2010. U.S. patent application Ser. No. 12 794 775 now published as U.S. Publication 2011 0249078 is incorporated herein by reference.

Many of today s portable devices such as smartphones provide video capture functionality. A user of the portable device can capture both still images and video through a camera on the phone. However to transmit captured video to another party the user must generally either send the video directly to the other party or upload the video to another location e.g. an Internet video hosting site after the video is done being captured. Unfortunately this does not allow the other party to view the live video stream as it is captured by the portable device.

In addition standard portable devices are only equipped with one camera and processing information from this one camera is difficult enough. An ideal device would have multiple cameras and could send out live video that is a composition of video from at least two cameras. This is an especially difficult problem in light of the limited resources available for portable devices both in terms of the device processing multiple captured video streams and a network to which the device is connected handling the transmission of the live video streams.

Some embodiments of the invention provide a mobile device with two cameras that can take pictures and videos. The mobile device of some embodiments has a display screen for displaying the captured picture images and video images. It also includes a storage for storing the captured images for later transmission to another device. The device further has a network interface that allows the device to transmit the captured images to one or more devices during a real time communication session between the users of the devices. The device also includes an encoder that it can use to encode the captured images for local storage or for transmission to another device. The mobile device further includes a decoder that allows the device to decode images captured by another device during a real time communication session or to decode images stored locally.

One example of a real time communication session that involves the transmission of the captured video images is a video conference. In some embodiments the mobile device can only transmit one camera s captured video images at any given time during a video conference. In other embodiments however the mobile device can transmit captured video images from both of its cameras simultaneously during a video conference or other real time communication session.

During a video conference with another device the mobile device of some embodiments can transmit other types of content along with the video captured by one or both of its cameras. One example of such other content includes low or high resolution picture images that are captured by one of the device s cameras while the device s other camera is capturing a video that is used in the video conference. Other examples of such other content include 1 files and other content stored on the device 2 the screen display of the device i.e. the content that is displayed on the device s screen 3 content received from another device during a video conference or other real time communication session etc.

The mobile devices of some embodiments employ novel in conference adjustment techniques for making adjustments during a video conference. For instance while transmitting only one camera s captured video during a video conference the mobile device of some embodiments can dynamically switch to transmitting a video captured by its other camera. In such situations the mobile device of some embodiments notifies any other device participating in the video conference of this switch so that this other device can provide a smooth transition on its end between the videos captured by the two cameras.

In some embodiments the request to switch cameras not only can originate on the local device that switches between its cameras during the video conference but also can originate from the other remote device that is receiving the video captured by the local device. Moreover allowing one device to direct another device to switch cameras is just one example of a remote control capability of the devices of some embodiments. Examples of other operations that can be directed to a device remotely in some embodiments include exposure adjustment operations e.g. auto exposure focus adjustment operations e.g. auto focus etc. Another example of a novel in conference adjustment that can be specified locally or remotely is the identification of a region of interest ROI in a captured video and the use of this ROI identification to modify the behavior of the capturing camera to modify the image processing operation of the device with the capturing camera or to modify the encoding operation of the device with the capturing camera.

Yet another example of a novel in conference adjustment of some embodiments involves real time modifications of composite video displays that are generated by the devices. Specifically in some embodiments the mobile devices generate composite displays that simultaneously display multiple videos captured by multiple cameras of one or more devices. In some cases the composite displays place the videos in adjacent display areas e.g. in adjacent windows . In other cases the composite display is a picture in picture PIP display that includes at least two display areas that show two different videos where one of the display areas is a background main display area and the other is a foreground inset display area that overlaps the background main display area.

The real time modifications of the composite video displays in some embodiments involve moving one or more of the display areas within a composite display in response to a user s selection and movement of the display areas. Some embodiments also rotate the composite display during a video conference when the screen of the device that provides this composite display rotates. Also the mobile device of some embodiments allows the user of the device to swap the videos in a PIP display i.e. to make the video in the foreground inset display appear in the background main display while making the video in the background main display appear in the foreground inset display .

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed.

In the following description numerous details are set forth for purpose of explanation. However one of ordinary skill in the art will realize that the invention may be practiced without the use of these specific details. In other instances well known structures and devices are shown in block diagram form in order not to obscure the description of the invention with unnecessary detail.

Some embodiments of the invention provide a mobile device with two cameras that can take pictures and videos. Examples of mobile devices include mobile phones smartphones personal digital assistants PDAs laptops tablet personal computers or any other type of mobile computing device. As used in this document pictures refer to still picture images that are taken by the camera one at a time in a single picture mode or several at a time in a fast action mode. Video on the other hand refers to a sequence of video images that are captured by a camera at a particular rate which is often referred to as a frame rate. Typical frame rates for capturing video are 25 frames per second fps 30 fps and 60 fps. The cameras of the mobile device of some embodiments can capture video images i.e. video frames at these and other frame rates.

The mobile device of some embodiments 1 can display the captured picture images and video images 2 can store the captured images for later transmission to another device 3 can transmit the captured images to one or more devices during a real time communication session between the users of the devices and 4 can encode the captured images for local storage or for transmission to another device.

One example of a real time communication session that involves the transmission of the captured video images is a video conference. In some embodiments the mobile device can only transmit one camera s captured video images at any given time during a video conference. In other embodiments however the mobile device can transmit captured video images from both of its cameras simultaneously during a video conference or other real time communication session.

The mobile devices of some embodiments generate composite displays that include simultaneous display of multiple videos captured by multiple cameras of one or more devices. In some cases the composite displays place the videos in adjacent display areas e.g. in adjacent windows . illustrates one such example of a composite display that includes two adjacent display areas and that simultaneously display two videos captured by two cameras of one device or captured by two cameras of two different devices that are in a video conference.

In other cases the composite display is a PIP display that includes at least two display areas that show two different videos where one of the display areas is a background main display area and the other is a foreground inset display area that overlaps the background main display area. illustrates one such example of a composite PIP display . This composite PIP display includes a background main display area and a foreground inset display area that overlaps the background main display area. The two display areas and simultaneously display two videos captured by two cameras of one device or captured by two cameras of two different devices that are in a video conference. While the example composite PIP displays illustrated and discussed in this document are similar to the composite PIP display which shows the entire foreground inset display area within the background main display area other composite PIP displays that have the foreground inset display area overlapping but not entirely inside the background main display area are possible.

In addition to transmitting video content during a video conference with another device the mobile device of some embodiments can transmit other types of content along with the conference s video content. One example of such other content includes low or high resolution picture images that are captured by one of the device s cameras while the device s other camera is capturing a video that is used in the video conference. Other examples of such other content include 1 files and other content stored on the device 2 the screen display of the device i.e. the content that is displayed on the device s screen 3 content received from another device during a video conference or other real time communication session etc.

The mobile devices of some embodiments employ novel in conference adjustment techniques for making adjustments during a video conference. For instance while transmitting only one camera s captured video during a video conference the mobile device of some embodiments can dynamically switch to transmitting the video captured by its other camera. In such situations the mobile device of some embodiments notifies any other device participating in the video conference of this switch so that this other device can provide a smooth transition on its end between the videos captured by the two cameras.

In some embodiments the request to switch cameras not only can originate on the local device that switches between its cameras during the video conference but also can originate from the other remote device that is receiving the video captured by the local device. Moreover allowing one device to direct another device to switch cameras is just one example of a remote control capability of the devices of some embodiments. Examples of other operations that can be directed to a device remotely in some embodiments include exposure adjustment operations e.g. auto exposure focus adjustment operations e.g. auto focus etc. Another example of a novel in conference adjustment that can be specified locally or remotely is the identification of a region of interest ROI in a captured video and the use of this ROI identification to modify the behavior of the capturing camera to modify the image processing operation of the device with the capturing camera or to modify the encoding operation of the device with the capturing camera.

Yet another example of a novel in conference adjustment of some embodiments involves real time modifications of composite video displays that are generated by the devices. Specifically in some embodiments the real time modifications of the composite video displays involve moving one or more of the display areas within a composite display in response to a user s selection and movement of the display areas. Some embodiments also rotate the composite display during a video conference when the screen of the device that provides this composite display rotates. Also the mobile device of some embodiments allow the user of the device to flip the order of videos in a PIP display i.e. to make the video in the foreground inset display appear in the background main display while making the video in the background main display appear in the foreground inset display .

Several more detailed embodiments are described below. Section I provides a description of the video processing architecture of some embodiments. Section II then describes the captured image processing unit of some embodiments. In some embodiments this unit is the component of the device that is responsible for processing raw images captured by the cameras of the device.

Next Section III describes the video conferencing architecture of some embodiments. This section also describes the video conference module of some embodiments as well as several manners for setting up a single camera video conference. Section IV then describes in conference adjustment and control operations of some embodiments. Section V next describes the hardware architecture of the dual camera device of some embodiments. Lastly U.S. patent application Ser. No. 12 794 766 now issued as U.S. Pat. No. 8 744 420 entitled Establishing a Video Conference During a Phone Call describes several additional embodiments relating to some of the features described above such as some ofthe in conference adjustments etc. This United States patent application entitled Establishing a Video Conference During a Phone Call is incorporated herein by reference.

In some embodiments the media exchange module allows programs on the device that are consumers and producers of media content to exchange media content and instructions regarding the processing of the media content. In the video processing and encoding module the media exchange module of some embodiments routes instructions and media content between the video processing module and the CIPU driver and between the video processing module and the encoder driver . To facilitate the routing of such instructions and media content the media exchange module of some embodiments provides a set of application programming interfaces APIs for the consumers and producers of media content to use. In some of such embodiments the media exchange module is a set of one or more frameworks that is part of an operating system running on the dual camera mobile device. One example of such a media exchange module is the Core Media framework provided by Apple Inc.

The video processing module performs image processing on the images and or the videos captured by the cameras of the device. Examples of such operations include exposure adjustment operations focus adjustment operations perspective correction dynamic range adjustment image resizing image compositing etc. In some embodiments some image processing operations can also be performed by the media exchange module . For instance as shown in the media exchange module of some embodiments performs a temporal noise reduction TNR operation e.g. by TNR that reduces noise in video images captured by the cameras of the device. Further examples of such image processing operations of the video processing module and the media exchange module will be provided below.

Through the media exchange module the video processing module interfaces with the CIPU driver and the encoder driver as mentioned above. The CIPU driver serves as a communication interface between a captured image processing unit CIPU and the media exchange module . As further described below the CIPU is the component of the dual camera device that is responsible for processing images captured during image capture or video capture operations of the device s cameras. From the video processing module through the media exchange module the CIPU driver receives requests for images and or videos from one or both of the device s cameras. The CIPU driver relays such requests to the CIPU and in response receives the requested images and or videos from the CIPU which the CIPU driver then sends to the video processing module through the media exchange module . Through the CIPU driver and the media exchange module the video processing module of some embodiments also sends instructions to the CIPU in order to modify some of its operations e.g. to modify a camera s frame rate exposure adjustment operation focus adjustment operation etc. .

The encoder driver serves as a communication interface between the media exchange module and an encoder hardware e.g. an encoder chip an encoding component on a system on chip etc. . In some embodiments the encoder driver receives images and requests to encode the images from the video processing module through the media exchange module . The encoder driver sends the images to be encoded to the encoder which then performs picture encoding or video encoding on the images. When the encoder driver receives encoded images from the encoder the encoder driver sends the encoded images back to the video processing module through the media exchange module .

In some embodiments the video processing module can perform different operations on the encoded images that it receives from the encoder. Examples of such operations include storing the encoded images in a storage of the device transmitting the encoded images in a video conference through a network interface of the device etc.

In some embodiments some or all of the modules of the video processing and encoding module are implemented as part of an operating system. For example some embodiments implement all four components and of this module as part of the operating system of the device. Other embodiments implement the media exchange module the CIPU driver and the encoder driver as part of the operating system of the device while having the video processing module as an application that runs on the operating system. Still other implementations of the module are possible.

The operation of the video processing and encoding module during a video capture session will now be described. To start a video capture session the video processing module initializes several components that are needed for the video capture session. In some embodiments these components include 1 the CIPU 2 a scaling and compositing module not shown of the video processing module 3 an image processing module not shown of the video processing module and 4 the encoder . Also the video processing module of some embodiments initializes a network manager not shown when it is participating in a video conference.

Through the media exchange module and the CIPU driver the video processing module sends its initialization request to the CIPU in order to have one or both of the cameras of the device start video capturing. In some embodiments this request specifies a particular frame rate exposure level and scaling size for each camera that needs to capture a video. In response to this request the CIPU starts to return video images from the requested cameras at the specified rate s exposure level s and scaling size s . These video images are returned to the video processing module through the CIPU driver and the media exchange module which as mentioned above performs TNR operations on the video images before supplying them to the video processing module . At the video processing module the video images are stored in a buffer not shown for additional image processing.

The image processing module of the video processing module retrieves the video images stored in the buffer for additional video processing. The scaling and compositing module then retrieves the processed video images in order to scale them if necessary for real time display on the display screen of the device. In some embodiments this module creates composite images from the images captured by two cameras of the device or from images captured by the camera s of the device along with the camera s of another device during a video conference in order to provide a real time display of the captured video images on the device or to create a composite video image for encoding.

The processed and or composited video images are supplied to the encoder through the encoder driver and the media exchange module . The encoder then encodes the video images. The encoded images are then returned to the video processing module again through the encoder driver and the media exchange module for storage on the device or for transmission during a video conference. When the device is participating in a video conference the network manager that was initialized by the video processing module then retrieves these encoded images packetizes them and transmits them to one or more other devices through a network interface not shown of the device.

The images captured by cameras of the dual camera mobile device of some embodiments are raw unprocessed images. These images require conversion to a particular color space before the images can be used for other operations such as transmitting the images to another device e.g. during a video conference storing the images or displaying the images. In addition the images captured by the cameras may need to be processed to correct errors and or distortions and to adjust the images color size etc. Accordingly some embodiments perform several processing operations on the images before storing transmitting and displaying such images. Part of the processing of such images is performed by the CIPU .

One example of such a CIPU is illustrated in . Specifically this figure conceptually illustrates a captured image processing unit CIPU of some embodiments. This CIPU includes a single processing pipeline that either processes images from only one of the device s cameras at a time or processes images from both of the device s cameras simultaneously in a time division multiplex fashion i.e. in a time interleaved manner . The CIPU s processing pipeline can be configured differently to address differing characteristics and or operational settings of the different cameras. Examples of different camera characteristics in some embodiments include different resolutions noise sensors lens types fixed or zoom lens etc. Also examples of different operational settings under which the device can operate the cameras in some embodiments include image resolution size frame rate zoom level exposure level etc.

As shown in the CIPU includes a sensor module a line frame buffer a bad pixel correction BPC module a lens shading LS module a demosaicing module a white balance WB module a gamma module a color space conversion CSC module a hue saturation and contrast HSC module a scaler module a filter module a statistics engine two sets of registers and a controller module . In some embodiments all of the modules of the CIPU are implemented in hardware e.g. an ASIC FPGA a SOC with a microcontroller etc. while in other embodiments some or all of the modules of the CIPU are implemented in software.

As shown in the sensor module communicatively couples to two pixel arrays and and two sets of sensors and of two cameras of the device. In some embodiments this communicative coupling is facilitated through each camera sensor s mobile industry processor interface MIPI .

Through this communicative coupling the sensor module can forward instructions to the cameras to control various aspects of each camera s operations such as its power level zoom level focus exposure level etc. In some embodiments each camera has four operational power modes. In the first operational power mode the camera is powered off. For the second operational power mode the camera is powered on but it is not yet configured. In the third operational power mode the camera is powered on the camera s sensor is configured and the camera sensor s pixels are collecting photons and converting the collected photons to digital values. However the camera sensor is not yet sending images to the sensor module . Finally in the fourth operational power mode the camera is in the same operational power mode as the third power mode except the camera is now sending images to the sensor module .

During the operation of the device the cameras may switch from one operational power mode to another any number of times. When switching operational power modes some embodiments require the cameras to switch operational power modes in the order described above. Therefore in those embodiments a camera in the first operational power mode can only switch to the second operational power mode. When the camera is in the second operational power mode it can switch to the first operational power mode or to the third operational power mode. Similarly the camera can switch from the third operational power mode to the second operational power mode or the fourth operation power mode. When the camera is in the fourth operational power mode it can only switch back to the third operational power mode.

Moreover switching from one operational power mode to the next or the previous operational power mode takes a particular amount of time. Thus switching between two or three operational power modes is slower than switching between one operational power mode. The different operational power modes also consume different amounts of power. For instance the fourth operational power mode consumes the most amount of power the third operational power mode consumes more power than the first and second and the second operational power mode consumes more than the first. In some embodiments the first operational power mode does not consume any power.

When a camera is not in the fourth operational power mode capturing images the camera may be left in one of the other operational power modes. Determining the operational mode in which to leave the unused camera depends on how much power the camera is allowed to consume and how fast the camera may need to respond to a request to start capturing images. For example a camera configured to operate in the third operational power mode e.g. standby mode consumes more power than a camera configured to be in the first operational power mode i.e. powered off . However when the camera is instructed to capture images the camera operating in the third operational power mode can switch to the fourth operational power mode faster than the camera operating in the first operational power mode. As such the cameras can be configured to operate in the different operational power modes when not capturing images based on different requirements e.g. response time to a request to capture images power consumption .

Through its communicative coupling with each camera the sensor module can direct one or both sets of camera sensors to start capturing images when the video processing module requests one or both cameras to start capturing images and the sensor module receives this request through the controller module as further described below. Bayer filters are superimposed over each of the camera sensors and thus each camera sensor outputs Bayer pattern images which are stored in the pixel array associated with each camera sensor. A Bayer pattern image is an image where each pixel only stores one color value red blue or green.

Through its coupling with the pixel arrays and the sensor module retrieves raw Bayer pattern images stored in the camera pixel arrays and . By controlling the rate at which the sensor module retrieves images from a camera s pixel array the sensor module can control the frame rate of the video images that are being captured by a particular camera. By controlling the rate of its image retrieval the sensor module can also interleave the fetching of images captured by the different cameras in order to interleave the CIPU processing pipeline s image processing of the captured images from the different cameras. The sensor module s control of its image retrieval is further described below and in the above incorporated U.S. patent application Ser. No. 12 794 766 now issued as U.S. Pat. No. 8 744 420 entitled Establishing Video Conference During a Phone Call .

The sensor module stores image lines i.e. rows of pixels of an image in the line frame buffer which the sensor module retrieves from the pixel arrays and . Each image line in the line frame buffer is processed through the CIPU processing pipeline . As shown in the CIPU processing pipeline is formed by the BPC module the LS module the demosaicing module the WB module the gamma module the CSC module the HSC module the scaler module and the filter module . In some embodiments the CIPU processing pipeline processes images from the line frame buffer on a line by line i.e. row by row basis while in other embodiments the CIPU processing pipeline processes entire images from the line frame buffer on a frame by frame basis.

In the exemplary pipeline illustrated in the BPC module is the module that retrieves the images from the line frame buffer . This module performs a bad pixel removal operation that attempts to correct bad pixels in the retrieved images that might have resulted from one or more of the camera sensors being defective e.g. the defective photo sensors do not sense light at all sense light incorrectly etc. . In some embodiments the BPC module detects bad pixels by comparing a particular pixel in an image with one or more neighboring pixels in the image. If the difference between the value of the particular pixel and the values of the neighboring pixels is greater than a threshold amount the particular pixel s value is replaced by the average of several neighboring pixel s values that are of the same color i.e. red green and blue as the particular pixel.

The operation of the BPC module is in part controlled by the values stored for this module in the two sets of registers of the CIPU . Specifically to process the images captured by the two different cameras of the device some embodiments configure the CIPU processing pipeline differently for each camera as mentioned above. The CIPU processing pipeline is configured for the two different cameras by storing two different sets of values in the two different sets of registers Ra and Rb of the CIPU . Each set of registers includes one register Ra or Rb for each of the modules within the CIPU processing pipeline . Each register in each register set stores a set of values that defines one processing pipeline module s operation. Accordingly as shown in the register set is for indicating the mode of operation of each processing pipeline module for one camera camera A of the dual camera mobile device while the register set is for indicating the mode of operation of each module for the other camera camera B of the dual camera mobile device.

One example of configuring the CIPU processing pipeline differently for each camera is to configure the modules of the CIPU processing pipeline to process different sized images. For instance if the camera sensor is 640 480 pixels and the camera sensor is 2048 1536 pixels the set of registers is configured to store values that instruct the modules of the CIPU processing pipeline to process 640 480 pixel images and the set of registers is configured to store values that instruct the modules of the CIPU processing pipeline to process 2048 1536 pixel images.

In some embodiments different processing pipeline configurations i.e. register values are stored in different profile settings. In some of such embodiments a user of the mobile device is allowed to select one of the profile settings e.g. through a user interface displayed on the mobile device to set the operation of a camera s . For example the user may select a profile setting for configuring a camera to capture high resolution video a profile setting for configuring the same camera to capture low resolution video or a profile setting for configuring both cameras to capture high resolution still images. Different configurations are possible which can be stored in many different profile settings. In other of such embodiments instead of allowing the user to select a profile setting a profile setting is automatically selected based on which application or activity the user selects. For instance if the user selects a video conferencing application a profile that configures both cameras to capture video is automatically selected if the user selects a photo application a profile that configures one of the cameras to capture still images is automatically selected etc.

After the BPC module the LS module receives the bad pixel corrected images. The LS module performs a lens shading correction operation to correct for image defects that are caused by camera lenses that produce light falloff effects i.e. light is reduced towards the edges of the camera sensor . Such effects cause images to be unevenly illuminated e.g. darker at corners and or edges . To correct these image defects the LS module of some embodiments estimates a mathematical model of a lens illumination fall off. The estimated model is then used to compensate the lens fall off of the image to evenly illuminate unevenly illuminated portions of the image. For example if a corner of the image is half the brightness of the center of the image the LS module of some embodiments multiplies the corner pixels value by two in order to produce an even image.

The demosaicing module performs a demosaicing operation to generate full color images from images of sampled colors. As noted above the camera sensors output Bayer pattern images which are incomplete because each pixel of a Bayer pattern image stores only one color value. The demosaicing module reconstructs a red green blue RGB image from a Bayer pattern image by interpolating the color values for each set of colors in the Bayer pattern image.

The WB module performs a white balance operation on the RGB images received from the demosaicing module so that the colors of the content of the images are similar to the colors of such content perceived by the human eye in real life. The WB module adjusts the white balance by adjusting colors of the images to render neutral colors e.g. gray white etc. correctly. For example an image of a piece of white paper under an incandescent light may appear yellow whereas the human eye perceives the piece of paper as white. To account for the difference between the color of the images that the sensor captures and what the human eye perceives the WB module adjusts the color values of the image so that the captured image properly reflects the colors perceived by the human eye.

The statistics engine collects image data at various stages of the CIPU processing pipeline . For example shows that the statistics engine collects image data after the LS module the demosaicing module and the WB module . Different embodiments collect data from any number of different stages of the CIPU processing pipeline . The statistics engine processes the collected data and based on the processed data adjusts the operations of the camera sensors and through the controller module and the sensor module . Examples of such operations include exposure and focus. Although shows the statistics engine controlling the camera sensors and through the controller module other embodiments of the statistics engine control the camera sensors through just the sensor module .

The processed data can also be used to adjust the operations of various modules of the CIPU . For instance the statistics engine of some embodiments adjusts the operations of the WB module based on data collected after the WB module . In some of such embodiments the statistics engine provides an automatic white balance AWB function by using the processed data to adjust the white balancing operation of the WB module . Other embodiments can use processed data collected from any number of stages of the CIPU processing pipeline to adjust the operations of any number of modules within the CIPU processing pipeline . Further the statistics engine can also receive instructions from the controller module to adjust the operations of one or more modules of the CIPU processing pipeline .

After receiving the images from the WB module the gamma module performs a gamma correction operation on the image to code and decode luminance or tristimulus values of the camera system. The gamma module of some embodiments corrects gamma by converting a 10 12 bit linear signal into an 8 bit non linear encoding in order to correct the gamma of the image. Some embodiments correct gamma by using a lookup table.

The CSC module converts the image received from the gamma module from one color space to another color space. Specifically the CSC module converts the image from an RGB color space to a luminance and chrominance YUV color space. However other embodiments of the CSC module can convert images from and to any number of color spaces.

The HSC module may adjust the hue saturation contrast or any combination thereof of the images received from the CSC module . The HSC module may adjust these properties to reduce the noise or enhance the images for example. For instance the saturation of images captured by a low noise camera sensor can be increased to make the images appear more vivid. In contrast the saturation of images captured by a high noise camera sensor can be decreased to reduce the color noise of such images.

After the HSC module the scaler module may resize images to adjust the pixel resolution of the image or to adjust the data size of the image. The scaler module may also reduce the size of the image in order to fit a smaller display for example. The scaler module can scale the image a number of different ways. For example the scaler module can scale images up i.e. enlarge and down i.e. shrink . The scaler module can also scale images proportionally or scale images anamorphically.

The filter module applies one or more filter operations to images received from the scaler module to change one or more attributes of some or all pixels of an image. Examples of filters include a low pass filter a high pass filter a band pass filter a bilateral filter a Gaussian filter among other examples. As such the filter module can apply any number of different filters to the images.

The controller module of some embodiments is a microcontroller that controls the operation of the CIPU . In some embodiments the controller module controls 1 the operation of the camera sensors e.g. exposure level through the sensor module 2 the operation of the CIPU processing pipeline 3 the timing of the CIPU processing pipeline e.g. when to switch camera sensors when to switch registers etc. and 4 a flash strobe not shown which is part of the dual camera mobile device of some embodiments.

Some embodiments of the controller module process instructions received from the statistics engine and the CIPU driver . In some embodiments the instructions received from the CIPU driver are instructions from the dual camera mobile device i.e. received from the local device while in other embodiments the instructions received from the CIPU driver are instructions from another device e.g. remote control during a video conference . Based on the processed instructions the controller module can adjust the operation of the CIPU by programming the values of the registers . Moreover the controller module can dynamically reprogram the values of the registers during the operation of the CIPU .

As shown in the CIPU includes a number of modules in the CIPU processing pipeline . However one of ordinary skill will realize that the CIPU can be implemented with just a few of the illustrated modules or with additional and different modules. In addition the processing performed by the different modules can be applied to images in sequences different from the sequence illustrated in .

An example operation of the CIPU will now be described by reference to . For purposes of explanation the set of registers Ra is used for processing images captured by camera sensor of the dual camera mobile device and the set of registers Rb is used for processing images captured by camera sensor of the dual camera mobile device. The controller module receives instructions from the CIPU driver to produce images captured by one of the cameras of the dual camera mobile device.

The controller module then initializes various modules of the CIPU processing pipeline to process images captured by one of the cameras of the dual camera mobile device. In some embodiments this includes the controller module checking that the correct set of registers of the registers are used. For example if the CIPU driver instructs the controller module to produce images captured by the camera sensor the controller module checks that the set of registers Ra is the set of registers from which the modules of the CIPU read. If not the controller module switches between the sets of registers so that the set of registers Ra is the set that is read by the modules of the CIPU .

For each module in the CIPU processing pipeline the mode of operation is indicated by the values stored in the set of registers Ra. As previously mentioned the values in the set of registers can be dynamically reprogrammed during the operation of the CIPU . Thus the processing of one image can differ from the processing of the next image. While the discussion of this example operation of the CIPU describes each module in the CIPU reading values stored in registers to indicate the mode of operation of the modules in some software implemented embodiments parameters are instead passed to the various modules of the CIPU .

In some embodiments the controller module initializes the sensor module by instructing the sensor module to delay a particular amount of time after retrieving an image from the pixel array . In other words the controller module instructs the sensor module to retrieve the images from the pixel array at a particular rate.

Next the controller module instructs the camera sensor through the sensor module to capture images. In some embodiments the controller module also provides exposure and other camera operation parameters to the camera sensor . In other embodiments the camera sensor uses default values for the camera sensor operation parameters. Based on the parameters the camera sensor captures a raw image which is stored in the pixel array . The sensor module retrieves the raw image from the pixel array and sends the image to the line frame buffer for storage before the CIPU processing pipeline processing the image.

Under certain circumstances images may be dropped by the line frame buffer . When the camera sensors and or are capturing images at a high rate the sensor module may receive and store images in the line frame buffer faster than the BPC module can retrieve the images from the line frame buffer e.g. capturing high frame rate video and the line frame buffer will become full. When this happens the line frame buffer of some embodiments drops images i.e. frames based on a first in first out basis. That is when the line frame buffer drops an image the line frame buffer drops the image that was received before all the other images in the line frame buffer .

The processing of the image by the CIPU processing pipeline starts by the BPC module retrieving the image from the line frame buffer to correct any bad pixels in the image. The BPC module then sends the image to the LS module to correct for any uneven illumination in the image. After the illumination of the image is corrected the LS module sends the image to the demosaicing module where it processes the raw image to generate an RGB image from the raw image. Next the WB module receives the RGB image from the demosaicing module and adjusts the white balance of the RGB image.

As noted above the statistics engine may have collected some data at various points of the CIPU processing pipeline . For example the statistics engine collects data after the LS module the demosaicing module and the WB module as illustrated in . Based on the collected data the statistics engine may adjust the operation of the camera sensor the operation of one or more modules in the CIPU processing pipeline or both in order to adjust the capturing of subsequent images from the camera sensor . For instance based on the collected data the statistics engine may determine that the exposure level of the current image is too low and thus instruct the camera sensor through the sensor module to increase the exposure level for subsequently captured images. Thus the statistics engine of some embodiments operates as a feedback loop for some processing operations.

After the WB module adjusts the white balance of the image it sends the image to the gamma module for gamma correction e.g. adjusting the gamma curve of the image . The CSC module receives the gamma corrected image from the gamma module and performs color space conversion. In this example the CSC module converts the RGB image to a YUV image. In other words the CSC module converts an image that is represented in an RGB color space to an image that is represented in a YUV color space. The HSC module receives the YUV image from the CSC module and adjusts the hue saturation and contrast attributes of various pixels in the image. After the HSC module the scaler module resizes the image e.g. enlarging or shrinking the image . The filter module applies one or more filters on the image after receiving the image from the scaler module . Finally the filter module sends the processed image to the CIPU driver .

In this example of the operation of the CIPU described above each module in the CIPU processing pipeline processed the image in some manner. However other images processed by the CIPU may not require processing by all the modules of the CIPU processing pipeline . For example an image may not require white balance adjustment gamma correction scaling or filtering. As such the CIPU can process images any number of ways based on a variety of received input such as instructions from the CIPU driver or data collected by the statistic engine for example.

Different embodiments control the rate at which images are processed i.e. frame rate differently. One manner of controlling the frame rate is through manipulation of vertical blanking intervals VBI . For some embodiments that retrieve image lines for processing images on a line by line basis a VBI is the time difference between retrieving the last line of an image of a video captured by a camera of the dual camera mobile device from a pixel array and retrieving the first line of the next image of the video from the pixel array. In other embodiments a VBI is the time difference between retrieving one image of a video captured by a camera of the dual camera mobile device from a pixel array and retrieving the next image of the video the pixel array.

One example where VBI can be used is between the sensor module and the pixel arrays and . For example some embodiments of the sensor module retrieve images from the pixel arrays and on a line by line basis and other embodiments of the sensor module retrieve images from the pixel arrays and on an image by image basis. Thus the frame rate can be controlled by adjusting the VBI of the sensor module increasing the VBI reduces the frame rate and decreasing the VBI increases the frame rate.

In the example frame rate the VBI of the sensor module for the pixel array is set to three time units e.g. by the controller module . That is the sensor module retrieves an image from the pixel array every third time instance along the timeline . As shown in the example frame rate the sensor module retrieves an image at the time instances and . Thus the example frame rate has a frame rate of one image per three time units.

The example frame rate is similar to the example frame rate except the VBI is set to two time units. Thus the sensor module retrieves an image from the pixel array every second time instance along the timeline . The example frame rate shows the sensor module retrieving an image from the pixel array at the time instances and . Since the VBI of the example frame rate is less than the VBI of the example frame rate the frame rate of the example frame rate is higher than the frame rate of the example frame rate .

The example frame rate is also similar to the example frame rate except the VBI of the sensor module for the pixel array is set to one time unit. Therefore the sensor module is instructed to retrieve an image from the pixel array every time instance along the timeline . As illustrated the sensor module retrieves an image from the pixel array at the time instances . The VBI of the example frame rate is less than the VBIs of the example frame rates and . Therefore the frame rate of the example frame rate is higher than the example frame rates and .

As described above by reference to the media exchange module allows consumers and producers of media content in the device to exchange media content and instructions regarding the processing of the media content the CIPU driver serves as a communication interface with the captured image processing unit CIPU and the encoder driver serves as a communication interface with the encoder hardware e.g. an encoder chip an encoding component on a system on chip etc. .

The video conference module of some embodiments handles various video conferencing functions such as image processing video conference management and networking. As shown the video conference module interacts with the media exchange module the video conference client and the network interface . In some embodiments the video conference module receives instructions from and sends instructions to the video conference client . The video conference module of some embodiments also sends data to and receives data from networks e.g. a local area network LAN a wireless local area network WLAN a wide area network WAN a network of networks a code division multiple access CDMA network a GSM network etc. through the network interface .

The video conference module includes an image processing layer a management layer and a network layer . In some embodiments the image processing layer performs image processing operations on images for video conferencing. For example the image processing layer of some embodiments performs exposure adjustment image resizing perspective correction and dynamic range adjustment as described in further detail below. The image processing layer of some embodiments sends requests through the media exchange module for images from the CIPU .

The management layer of some embodiments controls the operation of the video conference module . For instance in some embodiments the management layer initializes a camera cameras of the dual camera mobile device processes images and audio to transmit to a remote device and processes images and audio received from the remote device. In some embodiments the management layer generates composite e.g. PIP displays for the device. Moreover the management layer may change the operation of the video conference module based on networking reports received from the network layer .

In some embodiments the network layer performs some or all of the networking functionalities for video conferencing. For instance the network layer of some embodiments establishes a network connection not shown between the dual camera mobile device and a remote device of a video conference transmits images to the remote device and receives images from the remote device among other functionalities as described below and in the above incorporated U.S. patent application Ser. No. 12 794 766 now issued as U.S. Pat. No. 8 744 420 entitled Establishing Video Conference During a Phone Call . In addition the network layer receives networking data such as packet loss one way latency and roundtrip delay time among other types of data processes such data and reports the data to the management layer .

The video conference client of some embodiments is an application that may use the video conferencing functions of the video conference module such as a video conferencing application a voice over IP VOIP application e.g. Skype or an instant messaging application. In some embodiments the video conference client is a stand alone application while in other embodiments the video conference client is integrated into another application.

In some embodiments the network interface is a communication interface that allows the video conference module and the video conference client to send data and receive data over a network e.g. a cellular network a local area network a wireless network a network of networks the Internet etc. through the network interface . For instance if the video conference module wants to send data e.g. images captured by cameras of the dual camera mobile device to another device on the Internet the video conference module sends the images to the other device through the network interface .

The video conference server of some embodiments routes messages among video conference clients. While some embodiments implement the video conference server on one computing device other embodiments implement the video conference server on multiple computing devices. In some embodiments the video conference server is a publicly accessible server that can handle and route messages for numerous conferences at once. Each of the video conference clients and of some embodiments communicates with the video conference server over a network e.g. a cellular network a local area network a wireless network a network of networks the Internet etc. through a network interface such as the network interface described above.

The video conference request messaging sequence of some embodiments starts when the video conference client receives at operation 1 a request from a user of the device to start a video conference with the device . The video conference client of some embodiments receives the request to start the video conference when the user of the device selects a user interface UI item of a user interface displayed on the device . Examples of such user interfaces are illustrated in and which are described below.

After the video conference client receives the request the video conference client sends at operation 2 a video conference request which indicates the device as the recipient based on input from the user to the video conference server . The video conference server forwards at operation 3 the video conference request to the video conference client of the device . In some embodiments the video conference server forwards the video conference request to the video conference client using push technology. That is the video conference server initiates the transmission of the video conference request to the video conference client upon receipt from the video conference client rather than waiting for the client to send a request for any messages.

When the video conference client of some embodiments receives the video conference request a user interface is displayed on the device to indicate to the user of the device that the user of the device sent a request to start a video conference and to prompt the user of the device to accept or reject the video conference request. An example of such a user interface is illustrated in which is described below. In some embodiments when the video conference client receives at operation 4 a request to accept the video conference request from the user of the device the video conference client sends at operation 5 a video conference acceptance to the video conference server . The video conference client of some embodiments receives the request to accept the video conference request when the user of the device selects a user interface item of a user interface as illustrated in for example.

After the video conference server receives the video conference acceptance from the video conference client the video conference server forwards at operation 6 the video conference acceptance to the video conference client . Some embodiments of the video conference server forward the video conference acceptance to the video conference client using the push technology described above.

Upon receiving the video conference acceptance some embodiments establish at operation 7 a video conference between the device and the device . Different embodiments establish the video conference differently. For example the video conference establishment of some embodiments includes negotiating a connection between the device and the device determining a bit rate at which to encode video and exchanging video between the device and the device .

In the above example the user of the device accepts the video conference request. In some embodiments the device can be configured e.g. through the preference settings of the device to automatically accept incoming video conference requests without displaying a UI. Moreover the user of the device can also reject at operation 4 the video conference request e.g. by selecting a user interface item of a user interface displayed on the device . Instead of sending a video conference acceptance the video conference client sends a video conference rejection to the video conference server which forwards the video conference rejection to the video conference client . The video conference is then never established.

In some embodiments a video conference is initiated based on an ongoing phone call. That is while the user of a mobile device is engaged in a phone call with a second user the user can turn the phone call into a video conference with the permission of the other party. For some embodiments of the invention illustrates the start of such a video conference by a dual camera handheld mobile device . This figure illustrates the start of the video conference in terms of five operational stages and of a user interface UI of the device .

As shown in the UI includes a name field a selection menu and a selectable UI item . The name field displays the name of the person on the other end of the phone call with whom a user would like to request a video conference. In this example the selectable UI item which can be implemented as a selectable button provides a selectable End Call option for the user to end the phone call. The selection menu displays a menu of selectable UI items such as a Speakerphone item a Mute item a Keypad item a Phonebook item a Hold item a Video Conference item etc. Different embodiments display the selection menu differently. For the embodiments illustrated by the selection menu includes several equally sized icons each of which represents a different operation. Other embodiments provide a scrollable menu or give priority to particular items e.g. by making the items larger .

The operation of the UI will now be described by reference to the state of this UI during the five stages and that are illustrated in . In the first stage a phone call has been established between the handheld mobile device user and Nancy Jones. The second stage displays the UI after the user selects the selectable Video Conference option e.g. through a single finger tap by finger to activate a video conference tool. In this example the Video Conference option which can be implemented as a selectable icon allows the user to start a video conference during the phone call. In the second stage the Video Conference option is highlighted to indicate that the video conference tool has been activated. Different embodiments may indicate such a selection in different ways e.g. by highlighting the border or the text of the item .

The third stage displays the UI after the device has started the video conference process with the selection of the Video Conference option . The third stage is a transitional hold stage while the device waits for the video conference to be established e.g. while the device waits for the device on the other end of the call to accept or reject the video conference . In the third stage the user of the device can still talk to the user of the other device i.e. Nancy Jones while the video conference connection is being established. In addition some embodiments allow the user of the device to cancel the video conference request in the third stage by selecting a selectable UI item displayed on the UI not shown for canceling the video conference request. During this hold stage different embodiments use different displays in the UI to indicate the wait state.

As shown in in some embodiments the wait state of the third stage is illustrated in terms of a full screen display of a video being captured by the device along with a Preview notation at the bottom of this video. Specifically in the third stage illustrates the start of the video conference process by displaying in a display area of the UI a full screen presentation of the video being captured by the device s camera. In some embodiments the front camera is the default camera selected by the device at the start of a video conference. Often this front camera points to the user of the device at the start of the video conference. Accordingly in the example illustrated in the third stage illustrates the device as presenting a full screen video of the user of the device . The wait state of the device is further highlighted by the Preview designation below the video appearing in the display area during the third stage .

The transitional third hold stage can be represented differently in some embodiments. For instance some embodiments allow the user of the device to select the back camera as the camera for starting the video conference. To allow for this selection some embodiments allow the user to specify e.g. through a menu preference setting the back camera as the default camera for the start of a video conference and or allow the user to select the back camera from a menu that displays the back and front cameras after the user selects the Video Conference option . In either of these situations the UI e.g. display area displays a video captured by the back camera during the third hold stage .

Also other embodiments might indicate the activation of the video conference tool by displaying the smaller version of the video captured by the device by displaying a still image that is stored on the device by providing a message to highlight the wait state of the device e.g. by showing Conference Being Established by not displaying the Preview designation etc. Also in the third stage the UI of some embodiments provides an End button not shown to allow the user to cancel entering the video conference and revert back to the phone call if he decides not to enter the video conference at this stage e.g. while the user is waiting for the remote user to respond to his request .

The fourth stage illustrates the UI in a transitional state after the remote user has accepted the video conference request and a video conference connection has been established. In this transitional state the display area that displays the video of the local user that is being captured by the front camera in this example gradually decreases in size i.e. gradually shrinks as indicated by the arrows . The display area i.e. the local user s video shrinks so that the UI can display a display area e.g. a display window that contains the video from a camera of the remote device behind the display area . In other words the shrinking of the local user s video creates a PIP display that has a foreground inset display of the local user s video and a background main display of the remote user. In this example the background main display presents a video of a lady whose video is being captured by the remote device s front camera e.g. Nancy Jones the user of the remote device or a lady whose video is being captured by the remote device s back camera e.g. a lady whose video is being captured by Nancy Jones . One of ordinary skill will realize that the transitional fourth stage shown in is simply one exemplary approach used by some embodiments and that other embodiments might animate the transitional fourth stage differently.

The fourth stage also illustrates a selectable UI item in a lower display area . The selectable UI item which can be implemented as a selectable button provides a selectable End Conference option below the PIP display . The user may select this End Conference option to end the video conference e.g. through a single finger tap . Different embodiments may allow the user to end the conference in different ways such as by toggling a switch on the mobile device by giving voice commands etc. Moreover different embodiments may allow the End Conference option to fade away during the video conference thereby allowing the PIP display to take up the entire display area . The End Conference option may then reappear at a single finger tap at the bottom of the display area giving the user access to the End Conference option . In some embodiments the layout of the display area is same as the display area described in further detail below.

The fifth stage illustrates the UI after the animation of the fourth transitional state has ended. Specifically the fifth stage illustrates a PIP display that is presented by the UI during the video conference. As mentioned above this PIP display includes two video displays a larger background display from the remote camera and a smaller foreground inset display from the local camera.

This PIP display is only one manner of presenting a composite view of the videos being captured by the remote and local devices. In addition to this composite view the devices of some embodiments provide other composite views. For example instead of having a larger background display of the remote user the larger background display can be of the local user and the smaller foreground inset display of the remote user. As further described below some embodiments allow a user to switch during a video conference between the local cameras and or remote cameras as the cameras for the inset and main views in the PIP display .

Also some embodiments allow the local and remote videos to appear in the UI in two side by side display areas e.g. left and right display windows or top and bottom display windows or two diagonally aligned display areas. The manner of the PIP display or a default display mode may be specified by the user in some embodiments through the preference settings of the device or through controls that the user can select during a video conference as further described below and in the above incorporated U.S. patent application Ser. No. 12 794 766 now issued as U.S. Pat. No. 8 744 420 entitled Establishing Video Conference During a Phone Call .

When the user of the device of invites the remote user to a video conference the remote user may accept or reject the invitation. illustrates a UI of the remote user s device at six different stages and that show the sequence of operations for presenting and accepting a video conference invitation at the remote user s device. The description of the UI below refers to the user of the device i.e. the device that receives the video conference request as the invite recipient and the user of the device i.e. the device that sends the video conference request as the invite requestor. Also in this example it is assumed that the invite recipient s device is a dual camera device like that of the invite requestor. However in other examples one or both of these devices are single camera devices.

The first stage illustrates the UI when the invite recipient receives an invitation to a video conference from the invite requestor John Smith. As shown in the UI in this stage includes a name field a message field and two selectable UI items and . The name field displays the name of a person who is requesting a video conference. In some embodiments the name field displays a phone number of the person who is requesting a video conference instead of the name of the person. The message field displays an invite from the invite requestor to the invite recipient. In this example the Video Conference Invitation in the field indicates that the invite requestor is requesting a video conference with the invite recipient. The selectable UI items and which can be implemented as selectable buttons provide selectable Deny Request and Accept Request options and for the invite recipient to use to reject or accept the invitation. Different embodiments may display these options differently and or display other options.

Upon seeing the Video Conference Invitation notation displayed in the message field the invite recipient may deny or accept the request by selecting the Deny Request option or Accept Request option in the UI respectively. The second stage illustrates that in the example shown in the user selects the Accept Request option . In this example this selection is made by the user s finger tapping on the Accept Request option and this selection is indicated through the highlighting of this option . Other techniques are provided in some embodiments to select the Accept or Deny Request options and e.g. double tapping etc. to indicate the selection e.g. highlighting the border or text of the UI item .

The third stage displays the UI after the invite recipient has agreed to join the video conference. In this stage the UI enters into a preview mode that shows a full screen presentation of the video from the remote device s front camera in a display area . The front camera in this case is pointed to the user of the remote device i.e. Nancy Jones in this example . Accordingly her image is shown in this preview mode. This preview mode allows the invite recipient to make sure that her video is displayed properly and that she is happy with her appearance before the video conference begins e.g. before actual transmission of the video begins . In some embodiments a notation such as a Preview notation may be displayed below the display area to indicate that the invite recipient is in the preview mode.

Some embodiments allow the invite recipient to select the back camera as the default camera for the start of the video conference or to select the front or back camera at the beginning of the video conference as further described in the above incorporated U.S. patent application Ser. No. 12 794 766 now issued as U.S. Pat. No. 8 744 420 entitled Establishing Video Conference During a Phone Call . Also other embodiments display the preview display of the invite recipient differently e.g. in a smaller image placed in the corner of the display area . Yet other embodiments do not include this preview mode but rather start the video conference immediately after the invite recipient accepts the request.

In the third stage the UI shows two selectable UI items and one of which overlaps the display area while the other is below this display area . The selectable UI item is an Accept button that the user may select to start video conferencing. The selectable UI item is an End button that the invite recipient can select if she decides not to join the video conference at this stage.

The fourth stage displays the UI after the invite recipient selects the Accept button . In this example the Accept button is highlighted to indicate that the invite recipient is ready to start the video conference. Such a selection may be indicated in different ways in other embodiments.

The fifth stage illustrates the UI in a transitional state after the invite recipient has accepted the video conference request. In this transitional stage the display area that displays the video of the invite recipient that is being captured by the front camera in this example gradually decreases in size i.e. gradually shrinks as indicated by the arrows . The invite recipient s video shrinks so that the UI can display a display area e.g. a display window that contains the video from a camera of the invite requestor behind the display area . In other words the shrinking of the invite recipient s video creates a PIP display that has a foreground inset display area of the invite recipient s video and a background main display of the invite requestor.

In this example the background main display presents a video of a man whose video is being captured by the local device s front camera i.e. John Smith the user of the local device . In another example this video could have been that of a man whose video is being captured by the local device s back camera e.g. a man whose video is being captured by John Smith . Different embodiments may animate this transitional fifth stage differently.

The UI at the fifth stage also displays a display area e.g. a tool bar or a menu bar that includes selectable UI item e.g. mute button for muting the audio of the other user during the video conference selectable UI item e.g. end conference button for ending the video conference and selectable UI item e.g. switch camera button for switching cameras which is described in further detail below. As such the invite recipient may select any of the selectable UI items e.g. through a single finger tap to perform the desired operation during the video conference. Different embodiments may allow the invite recipient to perform any of the operations in different ways e.g. by toggling a switch on the mobile device by giving voice commands etc.

Although shows an example layout for the display area some embodiments provide different layouts of the display area such as the layout of display area of which includes just a selectable End Conference UI item for ending the video conference. Other layouts of display area can include any number of different selectable UI items for performing different functions. Moreover the fifth stage shows the display area displayed at the bottom of the UI . Different embodiments of the display area can be displayed at different locations within the UI and or defined as different shapes.

The sixth stage illustrates the UI after the animation of the fifth transitional stage has ended. Specifically the sixth stage illustrates a PIP display that is presented by the UI during the video conference. As mentioned above this PIP display includes two video displays a larger background display from the local camera and a smaller foreground inset display from the remote camera. This PIP display is only one manner of presenting a composite view of the videos being captured by the remote and local devices. In addition to this composite view the devices of some embodiments provide other composite views. For example instead of having a larger background display of the invite recipient the larger background display can be of the invite requestor and the smaller foreground inset display of the invite recipient. As further described in the above incorporated U.S. patent application Ser. No. 12 794 766 now issued as U.S. Pat. No. 8 744 420 entitled Establishing Video Conference During a Phone Call some embodiments allow a user to control the inset and main views in a PIP display to switchably display the local and remote cameras. Also some embodiments allow the local and remote videos to appear in the UI in two side by side display areas e.g. left and right display windows or top and bottom display windows or two diagonally aligned display areas. The manner of PIP display or a default display mode may be specified by the user in some embodiments through the preference settings of the device or through controls that the user can select during a video conference as further described in the above incorporated U.S. patent application Ser. No. 12 794 766 now issued as U.S. Pat. No. 8 744 420 entitled Establishing Video Conference During a Phone Call .

Although shows the sequence of operations for presenting and accepting a video conference invitation in terms of six different operational stages some embodiments may implement the operation in less stages. For instance some of such embodiments may omit presenting the third and fourth stages and and go from the second stage to the fifth stage after the user selects the Accept Request option . Other embodiments that implement that operation i.e. presenting and accepting a video conference invitation in less stages may omit the first and second stages and and present the user with the third stage when the invite recipient receives an invitation to a video conference from the invite requestor.

In addition to activating the video conference tool through a selectable option during a phone call some embodiments allow a user of a dual camera device to initiate a video conference directly without having to make a phone call first. illustrates another such alternative method to initiate a video conference. This figure illustrates the UI at seven different stages and that show an alternative sequence of operations for starting a video conference.

In the first stage a user is looking through a contacts list on this mobile device for the person with whom he wants to engage in a video conference similar to how he would find a contact to call. In the second stage the user selects the person with whom he would like to have a video conference e.g. through a single finger tap on the person s name . This selection triggers the UI to display the contact s information and various user selectable options. In this example Jason s name is highlighted to indicate that this is the person with whom the user would like to have a video conference. Different embodiments may indicate such a selection in different ways. While the second stage allows the user of the device to select a person with whom the user would like to have a video conference through a contact list some embodiments allow the user to select the person through a Recents call history that lists a particular number or name of a person with whom the user of the device recently had a video conference or a phone call.

In the third stage the UI displays the selected person s information and various selectable UI items and after the person s name has been selected. In this example one of the various selectable UI items which can be implemented as a selectable icon or button provides a video conference tool. The Video Conference option allows the user to invite the person identified by the contact to a video conference. Different embodiments display the information and selectable UI items and differently e.g. in a different arrangement .

The fourth stage shows the user selecting the Video Conference option e.g. through a single finger tap . In this example the Video Conference option is highlighted to indicate that the video conference tool has been activated. Such selections may be indicated differently in different embodiments e.g. by highlighting the text or border of the selected icon .

The fifth sixth and seventh stages and are similar to the third fourth and fifth stages and illustrated in and may be understood by reference to the discussion of those stages. In brief the fifth stage illustrates a transitional holding stage that waits for the remote user to respond to the invitation to a video conference. The sixth stage illustrates that after the remote user has accepted the video conference request the display area that displays the video of the local user gradually decreases in size so the UI can show a display area that contains the video from a camera of the remote user behind the display area . In the seventh stage the PIP display is presented by the UI during the video conference. In some embodiments the layout of display area in the sixth stage and the seventh stage is like the layout of the display area of described above.

In order to provide a seamless transition e.g. handoff of audio data from the telephone call to the video conference some embodiments do not terminate the telephone call before establishing the video conference. For instance some embodiments establish a peer to peer video conference connection e.g. after completing the message sequence illustrated in before terminating the phone call and starting to transmit audio video data through the peer to peer communication session. Alternatively other embodiments establish a peer to peer video conference connection e.g. after completing the message sequence illustrated in and start transmitting audio video data through the peer to peer communication session before terminating the phone call and starting to present the received audio video data.

A peer to peer video conference connection of some embodiments allows the mobile devices in the video conference to directly communicate with each other instead of communicating through a central server for example . Some embodiments of a peer to peer video conference allow the mobile devices in the video conferences to share resources with each other. For instance through a control communication channel of a video conference one mobile device can remotely control operations of another mobile device in the video conference by sending instructions from the one mobile device to the other mobile device to direct the other mobile device to process images differently i.e. share its image processing resource such as an exposure adjustment operation a focus adjustment operation and or a switch camera operation described in further detail below.

As mentioned above conceptually illustrates a software architecture for a video conferencing and processing module of a dual camera mobile device of some embodiments. As shown the video conferencing and processing module includes a client application a video conference module a media exchange module a buffer a captured image processing unit CIPU driver an encoder driver and a decoder driver . In some embodiments the buffer is a frame buffer that stores images of a video for display on a display of the dual camera mobile device.

In some embodiments the client application is the same as the video conference client of . As mentioned above the client application may be integrated into another application or implemented as a stand alone application. The client application may be an application that uses the video conferencing functions of the video conference module such as a video conferencing application a voice over IP VOIP application e.g. Skype or an instant messaging application.

The client application of some embodiments sends instructions to the video conference module such as instructions to start a conference and end a conference receives instructions from the video conference module routes instructions from a user of the dual camera mobile device to the video conference module and generates user interfaces that are displayed on the dual camera mobile device and allow a user to interact with the application.

As shown in the video conference module includes a video conference manager an image processing manager a networking manager and buffers and . In some embodiments the video conference module is the same as the video conference module illustrated in and thus performs some or all of the same functions described above for the video conference module .

In some embodiments the video conference manager is responsible for initializing some or all of the other modules of the video conference module e.g. the image processing manager and the networking manager when a video conference is starting controlling the operation of the video conference module during the video conference and ceasing the operation of some or all of the other modules of the video conference module when the video conference is ending.

The video conference manager of some embodiments also processes images received from one or more devices in the video conference and images captured by one of both cameras of the dual camera mobile device for display on the dual camera mobile device. For instance the video conference manager of some embodiments retrieves decoded images that were received from another device participating in the video conference from the buffer and retrieves images processed by CIPU i.e. images captured by the dual camera mobile device from the buffer . In some embodiments the video conference manager also scales and composites the images before displaying the images on the dual camera mobile device. That is the video conference manager generates the PIP or other composite views to display on the mobile device in some embodiments. Some embodiments scale the images retrieved from the buffers and while other embodiments just scale images retrieved from one of the buffers and .

Although illustrates the video conference manager as part of the video conference module some embodiments of the video conference manager are implemented as a component separate from the video conference module . As such a single video conference manager can be used to manage and control several video conference modules . For instance some embodiments will run a separate video conference module on the local device to interact with each party in a multi party conference and each of these video conference modules on the local device are managed and controlled by the one video conference manager.

The image processing manager of some embodiments processes images captured by the cameras of the dual camera mobile device before the images are encoded by the encoder . For example some embodiments of the image processing manager perform one or more of exposure adjustment focus adjustment perspective correction dynamic range adjustment and image resizing on images processed by the CIPU . In some embodiments the image processing manager controls the frame rate of encoded images that are transmitted to the other device in the video conference.

Some embodiments of the networking manager manage one or more connections between the dual camera mobile device and the other device participating in the video conference. For example the networking manager of some embodiments establishes the connections between the dual camera mobile device and the other device of the video conference at the start of the video conference and tears down these connections at the end of the video conference.

During the video conference the networking manager transmits images encoded by the encoder to the other device of the video conference and routes images received from the other device of the video conference to decoder for decoding. In some embodiments the networking manager rather than the image processing manager controls the frame rate of the images that are transmitted to the other device of the video conference. For example some such embodiments of the networking manager control the frame rate by dropping i.e. not transmitting some of the encoded frames that are supposed to be transmitted to the other device of the video conference.

As shown the media exchange module of some embodiments includes a camera source module a video compressor module and a video decompressor module . The media exchange module is the same as the media exchange module shown in with more detail provided. The camera source module routes messages and media content between the video conference module and the CIPU through the CIPU driver the video compressor module routes message and media content between the video conference module and the encoder through the encoder driver and the video decompressor module routes messages and media content between the video conference module and the decoder through the decoder driver . Some embodiments implement the TNR module included in the media exchange module not shown in as part of the camera source module while other embodiments implement the TNR module as part of the video compressor module .

In some embodiments the CIPU driver and the encoder driver are the same as the CIPU driver and the encoder driver illustrated in . The decoder driver of some embodiments acts as a communication interface between the video decompressor module and decoder . In such embodiments the decoder decodes images received from the other device of the video conference through the networking manager and routed through the video decompressor module . After the images are decoded they are sent back to the video conference module through the decoder driver and the video decompressor module .

In addition to performing video processing during a video conference the video conferencing and processing module for the dual camera mobile device of some embodiments also performs audio processing operations during the video conference. illustrates such a software architecture. As shown the video conferencing and processing module includes the video conference module which includes the video conference manager the image processing manager and the networking manager the media exchange module and the client application . Other components and modules of the video conferencing and processing module shown in are omitted in to simplify the description. The video conferencing and processing module also includes frame buffers and audio processing manager and audio driver . In some embodiments the audio processing manager is implemented as a separate software module while in other embodiments the audio processing manager is implemented as part of the media exchange module .

The audio processing manager processes audio data captured by the dual camera mobile device for transmission to the other device in the video conference. For example the audio processing manager receives audio data through the audio driver which is captured by microphone and encodes the audio data before storing the encoded audio data in the buffer for transmission to the other device. The audio processing manager also processes audio data captured by and received from the other device in the video conference. For instance the audio processing manager retrieves audio data from the buffer and decodes the audio data which is then output through the audio driver to the speaker .

In some embodiments the video conference module along with the audio processing manager and its associated buffers are part of a larger conference module. When a multi participant audio conference is conducted between several devices without exchange of video content this video conferencing and processing module only uses the networking manager and the audio processing manager to facilitate the exchange of audio over an Internet Protocol IP layer.

The operation of the video conference manager of some embodiments will now be described by reference to . conceptually illustrates a process performed by a video conference manager of some embodiments such as video conference manager illustrated in . This can be equivalent to being performed by the management layer of . In some embodiments the video conference manager performs process when a user of the dual camera mobile device accepts e.g. through a user interface displayed on the dual camera mobile device a video conference request or when a user of another device accepts a request sent by the user of the dual camera mobile device.

The process begins by receiving at instructions to start a video conference. In some embodiments the instructions are received from the client application or are received from a user through a user interface displayed on the dual camera mobile device and forwarded to the video conference manager by the client application . For example in some embodiments when a user of the dual camera mobile device accepts a video conference request the instructions are received through the user interface and forwarded by the client application. On the other hand when a user of the other device accepts a request sent from the local device some embodiments receive the instructions from the client application without user interface interaction although there may have been previous user interface interaction to send out the initial request .

Next the process initializes at a first module that interacts with the video conference manager . The modules of some embodiments that interact with the video conference manager include the CIPU the image processing manager the audio processing manager and the networking manager .

In some embodiments initializing the CIPU includes instructing the CIPU to start processing images captured by one or both cameras of the dual camera mobile device. Some embodiments initialize the image processing manager by instructing the image processing manager to start retrieving images from the buffer and processing and encoding the retrieved images. To initialize the audio processing manager some embodiments instruct the audio processing manager to begin encoding audio data captured by the microphone and decoding audio data stored in the buffer which was received from the other device in order to output to the speaker . The initializing of the networking manager of some embodiments includes instructing the networking manager to establish a network connection with the other device in the video conference.

The process then determines at whether there are any modules left to initialize. When there are modules left to initialize the process returns to operation to initialize another of the modules. When all of the required modules have been initialized the process generates at composite images for displaying on the dual camera mobile device i.e. local display . These composite images may include those illustrated in in the above incorporated U.S. patent application Ser. No. 12 794 766 now issued as U.S. Pat. No. 8 744 420 entitled Establishing Video Conference During a Phone Call and can include various combinations of images from the cameras of the local dual camera mobile device and images from cameras of the other device participating in the video conference.

Next the process determines at whether a change has been made to the video conference. Some embodiments receive changes to the video conference through user interactions with a user interface displayed on the dual camera mobile device while other embodiments receive changes to the video conference from the other device through the networking manager i.e. remote control . The changes to video conference settings may also be received from the client application or other modules in the video conference module in some embodiments. The video conference settings may also change due to changes in the network conditions.

When a change has been made the process determines at whether the change to the video conference is a change to a network setting. In some embodiments the changes are either network setting changes or image capture setting changes. When the change to the video conference is a change to a network setting the process modifies at the network setting and then proceeds to operation . Network setting changes of some embodiments include changing the bit rate at which images are encoded or the frame rate at which the images are transmitted to the other device.

When the change to the video conference is not a change to a network setting the process determines that the change is a change to an image capture setting and then proceeds to operation . The process then performs at the change to the image capture setting. In some embodiments change to the image capture settings may include switching cameras i.e. switching which camera on the dual camera mobile device will capture video focus adjustment exposure adjustment displaying or not displaying images from one or both cameras of the dual camera mobile device and zooming in or out of images displayed on the dual camera mobile device among other setting changes.

At operation the process determines whether to end the video conference. When the process determines to not end the video conference the process returns to operation . When the process determines that the video conference will end the process ends. Some embodiments of the process determine to end the video conference when the process receives instructions from the client application to end the video conference i.e. due to instructions received through the user interface of the local dual camera mobile device or received from the other device participating in the video conference .

In some embodiments the video conference manager performs various operations when the video conference ends that are not shown in process . Some embodiments instruct the CIPU to stop producing images the networking manager to tear down the network connection with the other device in the video conference and the image processing manager to stop processing and encoding images.

In addition to temporal noise reduction and image processing operations performed by the CIPU and or CIPU driver some embodiments perform a variety of image processing operations at the image processing layer of the video conference module . These image processing operations may include exposure adjustment focus adjustment perspective correction adjustment of dynamic range and image resizing among others.

The process will now be described by reference to . The process starts by retrieving at an image from the buffer . In some embodiments the retrieved image is an image of a video i.e. an image in a sequence of images . This video may have been captured by a camera of a device on which the process is performed.

Next the process performs at exposure adjustment on the retrieved image. Some embodiments perform exposure adjustments through a user interface that is displayed on the dual camera mobile device. illustrates an example exposure adjustment operation of such embodiments.

This figure illustrates the exposure adjustment operation by reference to three stages and of a UI of a device . The first stage illustrates the UI which includes a display area and a display area . As shown the display area displays an image of a sun and a man with a dark face and body. The dark face and body indicates that the man is not properly exposed. The image could be a video image captured by a camera of the device . As shown the display area includes a selectable UI item for ending the video conference. In some embodiments the layout of the display area is the same as the layout of the display area of described above.

The second stage illustrates a user of the device initiating an exposure adjustment operation by selecting an area of the display area . In this example a selection is made by placing a finger anywhere within the display area . In some embodiments a user selects exposure adjustment from a menu of possible image setting adjustments.

The third stage shows an image of the man after the exposure adjustment operation is completed. As shown the image is similar to the image but the man in the image is properly exposed. In some embodiments the properly exposed image is an image that is captured after the improperly exposed image. The exposure adjustment operation initiated in the second stage adjusts the exposure of subsequent images captured by the camera of the device .

Returning to the process next performs at focus adjustment on the image. Some embodiments perform focus adjustment through a user interface that is displayed on the dual camera mobile device. conceptually illustrates an example of such focus adjustment operations.

The second stage illustrates a user of the device initiating a focus adjustment operation by selecting an area of the display area . In this example a selection is made by placing a finger anywhere within the display area . In some embodiments a user selects focus adjustment from a menu of possible image setting adjustments.

The third stage shows an image of the man after the focus adjustment operation is completed. As shown the image is the same as the image but the man in the image appears sharper. This indicates that the lens of the camera is properly focused on the man. In some embodiments the properly focused image is an image that is captured after the improperly focused image. The focus adjustment operation initiated in the second stage adjusts the focus of subsequent images captured by the camera of the device .

Back to the process performs at image resizing on the image. Some embodiments perform image resizing on the image to reduce the number of bits used to encode the image i.e. lower the bit rate . In some embodiments the process performs image resizing as described by reference to in the above incorporated U.S. patent application Ser. No. 12 794 766 now issued as U.S. Pat. No. 8 744 420 entitled Establishing Video Conference During a Phone Call .

The process next performs at perspective correction on the image. In some embodiments the process performs perspective correction as described in in the above incorporated U.S. patent application Ser. No. 12 794 766 now issued as U.S. Pat. No. 8 744 420 entitled Establishing Video Conference During a Phone Call . Such perspective correction involves using data taken by one or more accelerometer and or gyroscope sensors that identifies orientation and movement of the dual camera mobile device. This data is then used to modify the image to correct for the perspective being off.

After perspective correction is performed on the image the process adjusts at the dynamic range of the image. In some embodiments the dynamic range of an image is the range of possible values that each pixel in the image can have. For example an image with a dynamic range of 0 255 can be adjusted to a range of 0 128 or any other range of values. Adjusting the dynamic range of an image can reduce the amount of bits that will be used to encode the image i.e. lower the bit rate and thereby smooth out the image.

Adjusting the dynamic range of an image can also be used for various other purposes. One purpose is to reduce image noise e.g. the image was captured by a noisy camera sensor . To reduce noise the dynamic range of the image can be adjusted so that the black levels are redefined to include lighter blacks i.e. crush blacks . In this manner the noise of the image is reduced. Another purpose of dynamic range adjustment is to adjust one or more colors or range of colors in order to enhance the image. For instance some embodiments may assume that the image captured by the front camera is an image of a person s face. Accordingly the dynamic range of the image can be adjusted to increase the red and pinks colors to make the person s cheeks appear rosy rosier. The dynamic range adjustment operation can be used for other purposes as well.

Finally the process determines at one or more rate controller parameters that are used to encode the image. Such rate controller parameters may include a quantization parameter and a frame type e.g. predictive bi directional intra coded in some embodiments. The process then ends.

While the various operations of process are illustrated as being performed in a specific order one of ordinary skill will recognize that many of these operations exposure adjustment focus adjustment perspective correction etc. can be performed in any order and are not dependent on one another. That is the process of some embodiments could perform focus adjustment before exposure adjustment or similar modifications to the process illustrated in .

As shown in the networking manager includes a session negotiating manager a transmitter module a universal transmission buffer a universal transmission buffer manager a virtual transport protocol VTP manager a receiver module and a media transport manager .

The session negotiating manager includes a protocol manager . The protocol manager ensures that the transmitter module uses a correct communication protocol to transmit data to a remote device during the video conference and enforces rules of the communication protocol that is used. Some embodiments of the protocol manager support a number of communication protocols such as a real time transport protocol RTP a transmission control protocol TCP a user datagram protocol UDP and a hypertext transfer protocol HTTP among others.

The session negotiating manager is responsible for establishing connections between the dual camera mobile device and one or more remote devices participating in the video conference as well as tearing down these connections after the conference. In some embodiments the session negotiating manager is also responsible for establishing multimedia communication sessions e.g. to transmit and receive video and or audio streams between the dual camera mobile device and the remote devices in the video conference e.g. using a session initiation protocol SIP .

The session negotiating manager also receives feedback data from the media transport manager and based on the feedback data determines the operation of the universal transmission buffer e.g. whether to transmit or drop packets frames through the universal transmission buffer manager . This feedback in some embodiments may include one way latency and a bandwidth estimation bit rate. In other embodiments the feedback includes packet loss information and roundtrip delay time e.g. determined based on packets sent to the remote device in the video conference and the receipt of acknowledgements from that device . Based on the information from the media transport manager the session negotiating manager can determine whether too many packets are being sent and instruct the universal transmission buffer manager to have the universal transmission buffer transmit fewer packets i.e. to adjust the bit rate .

The transmitter module retrieves encoded images e.g. as a bitstream from a video buffer e.g. the buffer of and packetizes the images for transmission to a remote device in the video conference through the universal transmission buffer and the virtual transport protocol manager . The manner in which the encoded images are created and sent to the transmitter module can be based on instructions or data received from the media transport manager and or the session negotiating manager . In some embodiments packetizing the images involves breaking the received bitstream into a group of packets each having a particular size i.e. a size specified by the session negotiating manager according to a particular protocol and adding any required headers e.g. address headers protocol specification headers etc. .

The universal transmission buffer manager controls the operation of the universal transmission buffer based on data and or instructions received from the session negotiating manager . For example the universal transmission buffer manager may be instructed to direct the universal transmission buffer to transmit data stop transmitting data drop data etc. As described above in some embodiments when a remote device participating in the conference appears to be dropping packets this will be recognized based on acknowledgements received from the remote device. To reduce the packet dropping the universal transmission buffer manager may be instructed to transmit packets at a slower rate to the remote device.

The universal transmission buffer stores data received from the transmitter module and transmits the data to the remote device through the VTP manager . As noted above the universal transmission buffer may drop data e.g. images of the video based on instructions received from the universal transmission buffer manager .

In some embodiments RTP is used to communicate data packets e.g. audio packets and video packets over UDP during a video conference. Other embodiments use RTP to communicate data packets over TCP during the video conference. Other transport layer protocols can be used as well in different embodiments.

Some embodiments define a particular communication channel between two mobile devices by a pair of port numbers i.e. source port number and destination port number . For instance one communication channel between the mobile devices can be defined by one pair of port numbers e.g. source port and destination port and another different communication channel between the mobile devices can be defined by another different pair of port numbers e.g. source port and destination port . Some embodiments also use a pair of internet protocol IP addresses in defining communication channels. Different communication channels are used to transmit different types of data packets in some embodiments. For example video data packets audio data packets and control signaling data packets can be transmitted in separate communication channels. As such a video communication channel transports video data packets and an audio communication channel transports audio data packets.

In some embodiments a control communication channel is for messaging between the local mobile device and a remote device during a video conference. Examples of such messaging include sending and receiving requests notifications and acknowledgements to such requests and notifications. Another example of messaging includes sending remote control instruction messages from one device to another. For instance the remote control operations described in the above incorporated U.S. patent application Ser. No. 12 794 766 now issued as U.S. Pat. No. 8 744 420 entitled Establishing Video Conference During a Phone Call e.g. instructing a device to only send images from one particular camera or to only capture images with a particular camera can be performed by sending instructions from a local device to a remote device through the control communication channel for the local device to remotely control operations of the remote device. Different embodiments implement the control communication using different protocols like a real time transport control protocol RTCP an RTP extension SIP etc. For instance some embodiments use RTP extension to relay one set of control messages between two mobile devices in a video conference and use SIP packets to relay another set of control messages between the mobile devices during the video conference.

The VTP manager of some embodiments allows different types of data packets that are specified to be transmitted through different communication channels e.g. using different pairs of port numbers to be transmitted through a single communication channel e.g. using the same pair of port numbers . One technique for doing this involves identifying the data packet types identifying the communication channel through which data packets are specified to be transmitted by extracting the specified pair of port numbers of the data packets and specifying the data packets to be transmitted through the single communication channel by modifying the pair of port numbers of the data packets to be the pair of port numbers of the single communication channel i.e. all the data packets are transmitted through the same pair of port numbers .

To keep track of the original pair of port numbers for each type of data packet some embodiments store a mapping of the original pair of port numbers for the data packet type. Some of these embodiments than use the packet type field of the protocol to differentiate the different packets that are being multiplexed into one communication channel. For instance some embodiments that have the VTP manager multiplex audio video and control packets into one RTP stream use the RTP packet type field to differentiate between the audio video and control packets that are transmitted in the one RTP channel to the other device in the video conference. In some of these embodiments the VTP manger also routes control messaging in SIP packets to the other device.

Some embodiments identify examine the data packet signatures i.e. packet header formats to distinguish between different packets that are communicated using different protocols e.g. to differentiate between packets transported using RTP and packets transported using SIP . In such embodiments after the data packets of the different protocols are determined the fields of the data packets that use the same protocol e.g. audio data and video data using RTP are examined as described above to identify the different data types. In this manner the VTP manager transmits different data packets which are intended to be transmitted through different communication channels through a single communication channel.

Although one way of combining different types of data through a single communication channel is described above other embodiments utilize other techniques to multiplex different packet types into one communication stream. For example one technique of some embodiments involves keeping track of the original pair of port numbers of the data packets and storing the original pair of port numbers in the data packet itself to be later extracted. Still other ways exist for combining different types of data between two video conference participants into one port pair channel.

When the VTP manager receives data packets from the remote device through a virtualized communication channel the VTP manager examines the signatures of the data packets to identify the different packets that are sent using the different protocols. Such signatures can be used to differentiate SIP packets from RTP packets. The VTP manager of some embodiments also uses the packet type field of some or all of the packets to demultiplex the various different types of packets e.g. audio video and control packets that were multiplexed into a single virtualized channel. After identifying these different types of packets the VTP manager associates each different type of packet with its corresponding port pair numbers based on a mapping of port pair numbers and packet types that it keeps. The VTP manager then modifies the pair of port numbers of the data packets with the identified pair of port numbers and forwards the data packets to be depacketized. In other embodiments that use different techniques for multiplexing different packet types into the single channel the VTP manager uses different techniques for parsing out the packets.

By using such techniques for multiplexing and de multiplexing the different packets the VTP manager creates a single virtualized communication channel e.g. a single pair of port numbers transmits the video data audio data and control signaling data through the single virtualized communication channel and receives audio video and control packets from the remote device through the single virtualized communication channel. Thus from the perspective of the network data is transmitted through this single virtualized communication channel while from the perspective of the session negotiating manager and the protocol manager the video data audio data and control signaling data are transmitted through different communication channels.

Similar to the images that are transmitted to the remote device in the video conference images transmitted from the remote device in the video conference are received in packet format. The receiver module receives the packets and depacketizes them in order to reconstruct the images before storing the images in a video buffer e.g. the buffer of to be decoded. In some embodiments depacketizing the images involves removing any headers and reconstructing a bitstream that only has image data and potentially size data from the packets.

The media transport manager processes feedback data e.g. one way latency bandwidth estimation bit rate packet loss data roundtrip delay time data etc. received from the network to dynamically and adaptively adjust the rate of data transmission i.e. bit rate . The media transport manager also controls error resilience based on the processed feedback data in some other embodiments and may also send the feedback data to the video conference manager in order to adjust other operations of the video conference module such as scaling resizing and encoding. In addition to having the universal transmission buffer drop packets when a remote device in the conference is not able to process all of the packets the video conference module and encoder can use a lower bit rate for encoding the images so that fewer packets will be sent for each image.

In some embodiments the media transport manager may also monitor other variables of the device such as power consumption and thermal levels that may affect how the operational power modes of the cameras are configured as discussed above. This data may also be used as additional inputs into the feedback data e.g. if the device is getting too hot the media transport manager may try to have the processing slowed down .

Several example operations of the networking manager will now be described by reference to . The transmission of images captured by a camera of the dual camera mobile device to a remote device in the video conference will be described first followed by the description of receiving images from the remote device. The transmitter module retrieves encoded images from the buffer which are to be transmitted to the remote device in the video conference.

The protocol manager determines the appropriate protocol to use e.g. RTP to transmit audio and video and the session negotiating manager informs the transmitter module of such protocol. Next the transmitter module packetizes the images and sends the packetized images to the universal transmission buffer . The universal transmission buffer manager receives instructions from the session negotiating manager to direct the universal transmission buffer to transmit or drop the images. The VTP manager receives the packets from the universal transmission buffer and processes the packets in order to transmit the packets through a single communication channel to the remote device.

When receiving images from the remote device the VTP manager receives packetized images from the remote device through the virtualized single communication channel and processes the packets in order to direct the images to the receiver module through a communication channel that is assigned to receive the images e.g. a video communication channel .

The receiver module depacketizes the packets to reconstruct the images and sends the images to the buffer for decoding by the decoder . The receiver module also forwards control signaling messages to the media transport manager e.g. acknowledgements of received packets from the remote device in the video conference .

Several example operations of the networking manager were described above. These are only illustrative examples as various other embodiments will perform these or different operations using different modules or with functionalities spread differently between the modules. Furthermore additional operations such as dynamic bit rate adjustment may be performed by the modules of networking manager or other modules.

Some embodiments rotate the PIP display that is presented during a video conference when a user of the mobile device used for the video conference rotates the device during the conference. illustrates the rotation of a UI of a device when the device is rotated from a vertical position to a horizontal position. The device is held vertically when the long side of the screen is vertical whereas the device is held horizontally when the long side of the screen is horizontal. In the example illustrated in the UI rotates from a portrait view that is optimized for a vertical holding of the device to a landscape view that is optimized for horizontal holding of the device . This rotation functionality allows the user to view the UI displayed in an upright position when the mobile device is held either vertically or horizontally.

The second stage illustrates the UI after the user begins to tilt the device sideways. In this example the user has started to tilt the device from being held vertically to being held horizontally as indicated by the arrow . The appearance of the UI has not changed. In other situations the user may want to tilt the device from being held horizontally to being held vertically instead and in these situations the UI switches from a horizontally optimized view to a vertically optimized view.

The third stage illustrates the UI in a state after the device has been tilted from being held vertically to being held horizontally. In this state the appearance of the UI still has not changed. In some embodiments the rotation operation is triggered after the device is tilted past a threshold amount and is kept past this point for a duration of time. In the example illustrated in it is assumed that the threshold amount and the speed of the rotation do not cause the UI to rotate until a short time interval after the device has been placed in the horizontal position. Different embodiments have different threshold amounts and waiting periods for triggering the rotation operation. For example some embodiments may have such a low threshold to triggering the rotation operation as to make the UI appear as if it were always displayed in an upright position notwithstanding the orientation of the device . In other embodiments the user of the device may specify when the rotation operation may be triggered e.g. through a menu preference setting . Also some embodiments may not delay the rotation after the device is tilted past the threshold amount. Moreover different embodiments may allow the rotation operation to be triggered in different ways such as by toggling a switch on the mobile device by giving voice commands upon selection through a menu etc.

The fourth stage illustrates the UI after the rotation operation has started. Some embodiments animate the rotation display areas to provide feedback to the user regarding the rotation operation. illustrates an example of one such animation. Specifically it shows in its fourth stage the start of the rotation of the display areas and together. The display areas and rotate around an axis going through the center of the UI i.e. the z axis . The display areas and are rotated the same amount but in the opposite direction of the rotation of the device e.g. through the tilting of the device . In this example since the device has rotated ninety degrees in a clockwise direction by going from being held vertically to being held horizontally the rotation operation would cause the display areas and to rotate ninety degrees in a counter clockwise direction. As the display areas and rotate the display areas and shrink proportionally to fit the UI so that the display areas and may still appear entirely on the UI . Some embodiments may provide a message to indicate the state of this device e.g. by displaying the word Rotating .

The fifth stage illustrates the UI after the display areas and have rotated ninety degrees counter clockwise from portrait view to landscape view. In this stage the display areas and have been rotated but have not yet expanded across the full width of the UI . The arrows indicate that at the end of the fifth stage the display areas and will start to laterally expand to fit the full width of the UI . Different embodiments may not include this stage since the expansion could be performed simultaneously with the rotation in the fourth stage .

The sixth stage illustrates the UI after the display areas and have been expanded to occupy the full display of the UI . As mentioned above other embodiments may implement this rotation differently. For some embodiments simply rotating the screen of a device past a threshold amount may trigger the rotation operation notwithstanding the orientation of the device .

Also other embodiments might provide a different animation for indicating the rotation operation. The rotation operation performed in involves the display areas and rotating about the center of the UI . Alternatively the display areas may be individually rotated about the center axis of their individual display areas. One such approach is shown in . shows an alternative method to animating the rotation of the display areas and of PIP display of a UI . The PIP display illustrated in is the same PIP display illustrated in .

The fourth stage illustrates the alternative method to animating the rotation. In this stage the rotation operation has started. Specifically the fourth stage shows the start of the rotation of the display areas and . The display areas and each rotate around axes and respectively going through the center of each of the display areas i.e. the z axis . The display areas and are rotated the same amount but in the opposite direction of the rotation of the device e.g. through the tilting of the device . Similar to that illustrated in the fourth stage of above since the device has rotated ninety degrees in a clockwise direction by going from being held vertically to being held horizontally the rotation operation would cause the display areas and to rotate ninety degrees in a counter clockwise direction. As the display areas and rotate the display areas and shrink proportionally to fit the UI so that the display areas and may still appear entirely on the UI .

The fifth stage illustrates the UI after each of the display areas and have rotated ninety degrees counter clockwise from portrait view to landscape view. In this stage the display areas and have been rotated but have not yet expanded across the full width of the UI . Moreover the display area has not moved into its final position. The final position of the inset display area in the PIP display is determined by the position of the inset display area in the PIP display as shown in the first stage e.g. the inset display area in the lower left corner of the PIP display . In this stage the inset display area is still in the upper left corner of the UI .

The arrows indicate that at the end of the fifth stage the display areas and will start to laterally expand until the main display area fits the full width of the UI for a device that is held horizontally. Moreover the arrow indicates that the inset display area will slide to the lower left corner of the PIP display .

Different embodiments may implement this differently. In some embodiments the moving of the inset display area may occur simultaneously as the expansion of the main display area or sequentially. Moreover some embodiments may resize the inset display areas before during or after the expansion of the main display area to create the new PIP display . In this example the display area disappears while the display areas and are rotating. However the display area may remain on the UI during the rotation and rotate along with the display areas and in some embodiments.

The sixth stage illustrates the UI after the inset display area has reached its new location and the display areas and have been properly expanded to fit the full width of the UI . In this example the inset display area is now in the lower left corner of the PIP display overlapping the main display area . The PIP display now has the same display arrangement as the PIP display from the first stage . The appearance of the display area below the PIP display in the sixth stage indicates that the rotation operation is completed. As noted above simply rotating the screen of a device past a threshold amount may trigger the rotation operation notwithstanding the orientation of the device .

In the examples described above by reference to the orientation of the display area also changes i.e. from portrait to landscape . That is after the display area is rotated in the third stage the orientation of the display area changes from portrait to landscape by horizontally expanding the PIP display so that it fills the entire UI . In some embodiments when the device is rotated video captured by the remote device rotates but the orientation of the display area that displays the video captured by the remote device remains unchanged. One such example is illustrated in . This figure is similar to except that video displayed in the display area rotates but the display area remains displayed in portrait orientation.

Some embodiments provide a rotation operation in which the orientation of the display area that displays video captured by the local device changes instead of remaining in the same orientation as shown in to reflect the orientation of the local device after the rotation operation is performed on the local device. illustrates an example of such a rotation operation of a UI by reference to six different stages and . In this figure the first stage shows the inset display area which displays video captured by a camera of the device in a portrait orientation. The second and third stages and are similar to the second and third stages and of as they show the tilting of the device at various stages of the rotation operation. At this point the camera of the device is capturing images in a landscape orientation. To indicate this transition some embodiments provide an animation as shown in fourth and fifth stages and while other embodiments do not provide any animation at all.

In the fourth stage the image displayed in the inset display area is rotated but not the inset display area itself since the tilting of the device in the second and third stages and has rotated the inset display area to a landscape orientation. In the fifth stage the rotated image in the inset display area is horizontally expanded to fill the inset display area and the inset display area starts to move towards the lower left area of the PIP display to position the inset display area in the same relative position as the inset display area in the PIP display of the first stage .

In some embodiments the orientation of the display area that displays the video captured by the remote device also changes to reflect the orientation of the remote device after a rotation operation is performed on the remote device. illustrates four different stages of a UI of the device in which 1 the orientation of the display area that displays the video captured by the local device display area in this example changes to reflect the orientation of the local device after a rotation operation is performed on the local device and 2 the orientation of the display area that displays video captured by the remote device display area in this example changes to reflect the orientation of the remote device after a rotation operation is performed on the remote device.

In the first stage the UI is the same as the UI in . Specifically the first stage shows the display areas and in a portrait orientation because the device is shown in a portrait orientation and the remote device is in a portrait orientation not shown . From the first stage to the second stage a rotation operation is performed on the local device by rotating the device ninety degrees from an upright position to a sideways position. The second stage shows the UI after the rotation operation of the device is completed. In this stage the videos displayed in the display areas and have rotated to an upright position. However only the display area of the locally captured video has rotated from a portrait orientation to a landscape orientation since the rotation operation is only performed on the local device i.e. the device . The display area remains in the portrait orientation.

From the second stage to the third stage a rotation operation is performed on the remote device by rotating the remote device from an upright position to a sideways position not shown . The third stage shows the UI after the rotation operation of the remote device is completed. In this stage the video displayed in the display area and the display area of the remotely captured video have rotated from a portrait orientation to a landscape orientation since the rotation operation is only performed on the remote device. Thus this stage of the UI displays the display areas and of the locally and remotely captured videos both in landscape orientation.

From the third stage to the fourth stage a rotation operation is performed on the local device by rotating the device ninety degrees from a sideways position to an upright position. The fourth stage shows the UI after the completion of this rotation operation. In this fourth stage the videos displayed in the display areas and have rotated to an upright position. However only the display area of the locally captured video has rotated from a landscape orientation to a portrait orientation since the rotation operation is only performed on the local device i.e. the device . The display area remains in the landscape orientation.

From the fourth stage to the first stage a rotation operation is performed on the remote device by rotating the remote device ninety degrees from a sideways position to an upright position not shown . In this case the first stage shows the display area after the completion of this rotation operation. Therefore the UI of this stage shows the display areas and in a portrait orientation. Although illustrates a sequence of different rotation operations other embodiments can perform any number of rotation operations in any number of different sequences.

Even though and illustrate different manners in which the animation of a rotation can be performed one of ordinary skill will realize that other embodiments may display the animation of the rotation in other different ways. In addition the animation of the rotation operation can cause changes to the image processing operations of the local mobile device such as causing the video conference manager to re composite the display area s at different angles in the UI and scale the images displayed in the display area s .

Some embodiments allow a user to identify a region of interest ROI in a displayed video during a video conference in order to modify the image processing e.g. the image processing manager in the encoding e.g. the encoder in the behavior of the mobile devices and their cameras during the video conference or a combination thereof. Different embodiments provide different techniques for identifying such a region of interest in a video. illustrates a user interface of some embodiments for identifying a region of interest in a video in order to improve the image quality of the video.

In a UI of a mobile device presents a PIP display during a video conference with a remote user of another mobile device. The PIP display in includes two video displays a background main display and a foreground inset display . In this example the background main display presents a video of a tree and a person with a hat which are assumed to be a tree and a person whose video is being captured by the remote device s front camera or a tree and a person whose video is being captured by the remote device s back camera. The foreground inset display presents a video of a man which in this example is assumed to be a man whose video is being captured by the local device s front camera or a person whose video is being captured by the local device s back camera. Below the PIP display is a display area that includes a selectable UI item labeled End Conference e.g. a button that allows the user to end the video conference by selecting the item.

This PIP display is only one manner of presenting a composite view of the videos being captured by the remote and local devices. Some embodiments may provide other composite views. For instance instead of having a larger background display for the video from the remote device the larger background display can be of the video from the local device and the smaller foreground inset display can be of the video from the remote device. Also some embodiments allow the local and remote videos to appear in the UI in two side by side display areas e.g. left and right display windows or top and bottom display windows or two diagonally aligned display areas. In other embodiments the PIP display may also contain a larger background display and two smaller foreground inset displays. The manner of the PIP display or a default display mode may be specified by the user in some embodiments.

In the second stage the operation of identifying a region of interest is initiated. In this example the operation is initiated by selecting an area in the video presented in the background display that the user wants to identify as the region of interest e.g. by tapping a finger on the device s screen at a location about the displayed person s face in the background display .

As shown in the third stage the user s selection of the area causes the UI to draw an enclosure e.g. a dotted square surrounding the area of the user s selection. The fourth stage displays the UI after the identification of the region of interest has been completed. As a result of this process the quality of the video within the region of interest has been substantially improved from that in the first stage . The removal of the enclosure indicates that the ROI selection operation is now completed. In some embodiments the ROI identification process also causes the same changes to the same video displayed on the remote device as it does to the local device . In this example for instance the picture quality within the region of interest of the same video displayed on the remote device is also substantially improved.

In some embodiments the user may enlarge or shrink the enclosure in the third stage e.g. by holding the finger down on the display and moving the finger toward the upper right corner of the screen to enlarge the enclosure or moving the finger toward the lower left corner of the screen to shrink the enclosure . Some embodiments also allow the user to relocate the enclosure in the third stage e.g. by holding the finger down on the display and moving the finger horizontally or vertically on the display . In some other embodiments the selection of the area may not cause the UI to draw the enclosure at all in the third stage .

Other embodiments provide different techniques for allowing a user to identify a region of interest in a video. illustrates one such other technique. In the user identifies a region of interest by drawing a shape that bounds the region. The shape in this example is a rectangle but it can be other shapes e.g. any other polygon a circle an ellipse etc. . Some embodiments provide this alternative technique of in a device UI that also provides the technique illustrated in . Other embodiments however do not provide both these techniques in the same UI.

In the second stage the operation of identifying a region of interest is initiated. In this example the operation is initiated by selecting for a duration of time a first position for defining the region of interest in a video presented in the background display area e.g. by holding a finger down on the device s screen at a location about the displayed person s face in the background display for a duration of time . In the third stage the UI indicates that the first position has been selected in terms of a dot next to the selected first position on the background display area .

The fourth stage illustrates the UI after the user has selected a second position for defining the region of interest. In this example the user selects this second position by dragging the finger across the device s screen from the first location after the dot appears and stopping at a location between the displayed hat and the displayed tree in the background display area as indicated by an arrow . As shown in the fourth stage this dragging caused the UI to draw a rectangular border for the region of interest area that has the first and second positions and at its opposing vertices.

The fifth stage illustrates the UI after identification of the region of interest has been completed. In this example the user completes identification of the region of interest by stopping the dragging of the finger and removing the finger from the device s display screen once the desired region of interest area has been identified. The fifth stage illustrates that as a result of the drawing process the quality of the video within the region of interest has been substantially improved from that in the first stage . In some embodiments the drawing process also causes the same changes to the display on the remote device as it does to the local device . In this example for instance the picture quality within the region of interest of the same video displayed on the remote device will be substantially improved.

The description of above illustrates different manners of identifying a region of interest in a video in order to improve the picture quality of the identified region. In some embodiments improving the picture quality of the identified region of interest causes changes to the encoding operations of the dual camera mobile device such as allocating more bits to the identified region when encoding the video.

Some embodiments allow the user to identify a region of interest in a video to make different changes to the mobile devices or their cameras. For instance illustrates an example of identifying a region of interest in a video to expand or shrink the region of interest area on the display. In this approach the user identifies a region of interest in a video by selecting an area on the display as the center of the region of interest and then expanding or shrinking the region of interest area.

In a UI of a mobile device presents a PIP display during a video conference with a remote user of another mobile device. The PIP display in is substantially similar to the PIP display of but the foreground inset display of is located in the lower left corner of the PIP display .

In the second stage the operation of identifying a region of interest is initiated. In this example the operation is initiated by selecting an area in the video presented in the background display that the user wants to identify as the region of interest e.g. by holding two fingers and down on the background display area where the tree is displayed . At this stage the user can make the region of interest area expand and take a larger portion of the background display area by dragging his fingers and farther away from each other. The user can also make the region of interest shrink to take a smaller portion of the background display area by dragging his fingers and closer together.

The third stage illustrates the UI after the user has started to make the region of interest expand to take up a larger portion of the background display area by moving his fingers and farther away from each other i.e. the finger moves toward the upper left corner of the background display area and the finger moves toward the lower right corner of the display as indicated by arrows . In some embodiments the finger movement also causes the same changes to the display of the remote device as it does to the local device. In this example for instance the region of interest of the same video will expand and take up a larger portion of the background display area of the remote device. In some embodiments the expansion of the region of interest in the local display and or remote display causes one or both of the mobile devices or their cameras to modify one or more of their other operations as further described below.

The fourth stage displays the UI after the identification of the region of interest has been completed. In this example the user completes the identification of the region of interest by stopping the dragging of his fingers and and removing the fingers and from the device s display screen once the region of interest has reached the desired proportion in the background display area . As a result of this process the region of interest has taken up a majority of the background display . The identification of the region of interest operation is now completed.

Some of the examples above illustrate how a user may identify a region of interest in a video for improving the image quality within the selected region of interest in the video e.g. by increasing the bit rate for encoding the region of interest portion of the video . In some embodiments identifying a region of interest in the video causes changes to the image processing operations of the mobile device such as exposure scaling focus etc. For example identifying a region of interest in the video can cause the video conferencing manager to scale and composite the images of the video differently e.g. identifying a region of interest to which to zoom .

In other embodiments identifying a region of interest in the video causes changes to the operation of the mobile device s camera s e.g. frame rate zoom exposure scaling focus etc. . In yet other embodiments identifying a region of interest in the video causes changes to the encoding operations of the mobile device like allocating more bits to the identified region scaling etc. In addition while the example ROI identification operations described above may cause only one of the above described modifications to the mobile device or its cameras in some other embodiments the ROI identification operation may cause more than one of the modifications to the operation of the mobile device or its cameras. In addition in some embodiments the layout of the display area in is the same as the layout of the display area of described above.

Some embodiments provide procedures to switch cameras i.e. change the camera by which images are captured during a video conference. Different embodiments provide different procedures for performing the switch camera operation. Some embodiments provide procedures performed by a dual camera mobile device for switching cameras of the device i.e. local switch while other embodiments provide procedures for the dual camera mobile device to instruct another dual camera mobile device in the video conference to switch cameras of the other device i.e. remote switch . Yet other embodiments provide procedures for both. Section IV.B.1 will describe a process for performing a local switch camera operation on a dual camera mobile device. Section IV.B.2 will describe a process for performing a remote switch camera operation on the dual camera mobile device.

The process begins by starting at a video conference between the local dual camera mobile device and the remote mobile device. Next the process sends at a video image from the currently selected camera e.g. the camera 1 of the local dual camera mobile device to the remote mobile device for display on the remote mobile device. At the process also generates and displays a composite display based on this video image and the video image that it receives from the remote mobile device.

The process then determines at whether a request to end the video conference is received. As described above a video conference can end in some embodiments at the request of a user of the local dual camera mobile device e.g. through a user interface of the local dual camera mobile device or a user of the remote mobile device e.g. through a user interface of the remote mobile device . When the process receives a request to end the video conference the process ends.

When the process does not receive a request to end the video conference the process then determines at whether the user of the local dual camera mobile device has directed the device to switch cameras for the video conference. The process returns to operation when the process determines at that it has not been directed to switch cameras. However when the process determines at that it has been so directed the process transitions to .

At the process sends a notification to the remote mobile device to indicate that the local dual camera mobile device is switching cameras. In some embodiments the process sends the notification through the video conference control channel that is multiplexed with the audio and video channels by the VTP Manager as described above.

After sending its notification the process performs at a switch camera operation. In some embodiments performing at the switch camera operation includes instructing the CIPU to stop capturing video images with the camera 1 and to start capturing video images with the camera 2. These instructions can simply direct the CIPU to switch capturing images from the pixel array associated with the camera 2 and to start processing these images. Alternatively in some embodiments the instructions to the CIPU are accompanied by a set of initialization parameters that direct the CIPU 1 to operate the camera 2 based on a particular set of settings 2 to capture video generated by the camera 2 at a particular frame rate and or 3 to process video images from the camera 2 based on a particular set of settings e.g. resolution etc. .

In some embodiments the switch camera instruction at also includes instructions for switching the unused camera to the fourth operational power mode as described above. In this example the switch camera instructions include instructions for the camera 2 to switch to its fourth operational power mode. In addition the switch camera instructions also include instructions for the camera 1 to switch from its fourth operational power mode to another operational power mode such as the first operational power mode to conserve power or to the third operational power mode so it can quickly switch to the fourth operational power mode and start capturing images when requested to do so. The switch camera operation also involves compositing images captured by the camera 2 of the local dual camera mobile device instead of images captured by the camera 1 with images received from the remote mobile device for display on the local dual camera mobile device.

After directing the switch camera at the process performs at a switch camera animation on the local dual camera mobile device to display a transition between the display of images from the camera 1 and the display of images from the camera 2. Following the switch camera animation on the local dual camera mobile device the process loops back through operations until an end video conference request or a new switch camera request is received.

The first stage is the same as the fifth stage of the UI of which shows the UI after a video conference is set up. At this stage the UI displays a PIP display that includes two video displays a larger background display from the remote camera and a smaller foreground inset display from the local camera. In this example the background main display area presents a video of a lady which in this example is assumed to be a lady whose video is being captured by the remote device while the foreground inset display area presents a video of a man which in this example is assumed to be a man whose video is being captured by the local device s front camera.

The second stage then shows the initiation of the switch camera operation through the selection of the PIP display area of the UI . As shown a selection is made by placing the user s finger on the PIP display . The third stage shows the UI that includes a selectable UI item e.g. switch camera button for requesting a switch between the cameras of the local device during the video conference. The fourth stage illustrates the UI after the user of the local device selects e.g. through a single finger tap the selectable UI item and after this selection is indicated through the highlighting of the selectable UI item . By selecting this selectable UI item the user is directing the device to switch from the front camera of the device to the back camera of the device during the video conference. In other examples where the back camera of the device is capturing video the user s selection of the selectable UI item directs the device to switch from the back camera of the device to the front camera of the device . After the fourth stage the video conference manager sends instructions to the CIPU and the remote device to start the switch camera operation.

The last four stages and of the UI illustrate an example of a switch camera animation on the local device. This animation is intended to provide an impression that the video captured from the front and the back cameras of the local device are being concurrently displayed on two opposing sides of a viewing pane that can have only one of its sides viewed by the user at any given time. When a switch camera is requested in the middle of a video conference this viewing pane is made to appear to rotate around the vertical axis such that the presentation of one camera s video on one side of the viewing pane that was previously showing one camera s video to the user rotates away from the user until it is replaced by the other side of the viewing pane which shows the video of the other camera. This animation and appearance of the perceived viewing pane s rotation is achieved by 1 gradually shrinking and applying perspective correction operations on the video image from one camera in the display area for that camera followed by 2 a gradual expansion and reduction in perspective correction operation to the video image from the other camera in the display area.

Accordingly the fifth stage illustrates the start of the rotation of the viewing pane about the vertical axis . To give an appearance of the rotation of the viewing pane the UI has reduced the size of the front camera s video image in the video display area and has applied perspective operations to make it appear that the right side of the video image is farther from the user than the left side of the video image.

The sixth stage illustrates that the viewing pane has rotated by 90 degrees such that the user can only view the edge of this pane as represented by the thin line displayed in the middle of the display area . The seventh stage illustrates that the viewing pane has continued to rotate such that the backside of the viewing pane is now gradually appearing to the user in order to show the video captured from the user s back camera. Again this representation of the rotation animation is achieved in some embodiments by reducing the size of the back camera s video image in the video display area and applying perspective operations to make it appear that the left side of the video image is farther from the user than the right side of the video image.

The eighth stage illustrates the completion of the animation that shows the switch camera operation. Specifically this stage displays in the display area the video image of a car that is being captured by the back camera of the device .

The example described above by reference to invokes a switch camera operation through a switch camera user interface. Other embodiments invoke a switch camera operation differently. For example some embodiments invoke the switch camera operation by having a switch camera selectable UI item permanently displayed on a UI during a video conference such the UI of . In a switch camera button is shown in a display area along with a mute button and an end conference button . The layout of the display area is the same layout of the display area described above by reference to .

In some embodiments when the remote mobile device receives images from a different camera of the local dual camera mobile device i.e. the local dual camera mobile device switched cameras the remote mobile device also performs a switch camera animation to display a transition between the display of image from one camera of the local dual camera mobile device and the display of images from the other camera of the local dual camera mobile device. illustrates an example of one of such switch camera animation in terms of five operational stages and of a UI . This figure shows an example switch camera animation on the remote mobile device . The operational stages are the same as the example animation of except the animation is performed on images displayed in the display area which is where images from the local dual camera mobile device are displayed on the remote mobile device . As such the image of the man displayed in the display area is animated to appear to rotate 180 degrees on a vertical axis located in the middle of the display area to show the transition between the display of the image of the man in the display area and the display of the image of a car . The implementation of the switch camera animation of some embodiments is the same as the implementation of the animation described above.

The above example illustrates a switch camera animation on a remote device with a particular user interface layout. Other embodiments might perform this switch camera animation on a remote device with a different user interface layout. For instance illustrates one such example of a remote device that has a different user interface layout . In particular UI of has a mute button an end conference button and a switch camera button included in a display area which is permanently displayed on one side of the composite display during a video conference. The layout of the three buttons is described above by reference to . Other than the different user interface layout the five stages and of are identical to the five stages and of .

The process of will be described by reference to and . illustrates a UI of a local device through which a user requests that a remote device switch between its two cameras during a video conference. This figure illustrates eight different operational stages and of this UI . illustrates a UI of a remote device that receives the switch camera request from the local device . illustrates six different operational stages and of the UI .

As shown in the process begins by starting at a video conference between the local and remote devices. The process then at receives images from one camera of each device e.g. from the front camera of each device and generates a composite view for the video conference based on these images. At the process also sends a video image from the local device to the remote device.

Next the process determines at whether a request to end the video conference has been received. As described above a video conference can end in some embodiments at the request of a user of the local or remote device. When the process receives a request to end the video conference the process ends.

When the process does not receive a request to end the video conference the process then determines at whether the user of the device on which the process is executing i.e. the user of the local device has directed the device to request that the remote device switch between its cameras for the video conference. The process returns to operation when the process determines at that it has not been directed to initiate a remote switch camera. When the process determines at that it has been so directed the process transitions to which will be described further below.

The first four stages and of the UI of illustrate an example of receiving a user s request to switch cameras of the remote device. The first and second stages and are the same as the first and second stages and of . The third stage is the same as the third stage except the third stage includes a selectable UI item for a request to the remote device to switch cameras in addition to the selectable UI item for requesting the local device to switch cameras. The fourth stage illustrates the user of the local device selecting the UI item e.g. through a single finger tap of the selectable UI item for requesting the remote device to switch cameras. The selection is indicated by the highlighting of the selectable UI item . shows one example of performing this operation but other embodiments may differently perform the operation for requesting the remote device to switch cameras.

The example described above by reference to invokes a remote switch camera operation through a remote switch camera user interface. Other embodiments invoke a remote switch camera operation differently. For instance some embodiments invoke the switch camera operation by having a switch camera selectable UI item permanently displayed on a UI during a video conference such as the UI of . In a remote switch camera button is shown in a display area along with a mute button an end conference button and a local switch camera button .

Some embodiments provide a similar layout as the one illustrated in except the remote switch camera selectable UI item is displayed in PIP display instead of the display area . illustrates such a layout . In particular the figure shows the PIP display with the remote switch camera selectable UI item and the display area with only a mute button a local switch camera button and an end conference button .

As mentioned above the process transitions to when the user requests a remote switch camera. At the process sends the request to switch cameras to the remote device. In some embodiments this request is sent through the video conference control channel that is multiplexed with the audio and video channels by the VTP Manager as described above.

After the request to switch cameras is received the process determines at whether the remote device has responded to the request to switch cameras. In some embodiments the remote device automatically sends an accept response i.e. sends an acknowledgement to the local device through the video conference control channel. In other embodiments however the user of the remote device has to accept this request through the user interface of the remote device.

The first two stages and of the UI of illustrate an example of the remote user accepting a request to switch cameras of the remote device . The first stage shows 1 a display area for displaying text that notifies the remote user of the request 2 a selectable UI item e.g. allow button for accepting the request to switch cameras of the remote device and 3 a selectable UI item e.g. reject button for rejecting the request to switch cameras of the remote device. The second stage then illustrates the UI after the user of the remote device has selected e.g. through a single finger tap the UI item for accepting the request to switch cameras as indicated by the highlighting of the selectable UI item .

When the process determines at that it has not yet received a response from the remote device the process determines at whether a request to end the video conference has been received. If so the process returns to operation to continue to receive images from the camera of the other device. Otherwise the process receives at images from the currently used cameras of the remote and local devices generates a composite view for the video conference based on these images transmit the local device s video image to the remote device and then transitions back to .

When the process determines at that it has received a response from the remote device it determines at whether the remote device accepted the request to switch cameras. If not the process ends. Otherwise the process receives at images from the other camera of the remote device and then performs at a switch camera animation on the local device to display a transition between the video of the previously utilized remote camera and the video of the currently utilized remote camera i.e. the received images at operation . After the process transitions back to which was described above.

The last four operational stages and that are illustrated for the UI in illustrate one example of such a remote switch camera animation on the local device . The example animation is similar to the example animation illustrated in the stages and of except shows in the display area an animation that replaces the video of a woman that is captured by the front camera of the remote device with the video of a tree that is captured by the back camera of the remote device. The last four stages of and illustrate the same animation as the one in except the display area of contains different selectable UI items than the display area in .

In some embodiments when the remote device switches cameras the UI of the remote device also performs a switch camera animation to display a transition between the two cameras. The last four operational stages and that are illustrated for the UI in illustrate an example of a switch camera animation that is displayed on the remote device when the remote device switches between cameras. This animation is similar to the animation illustrated in the stages and of except that the animation in the display area replaces the video of a woman that is captured by the front camera of the remote device with the video of a tree that is captured by the back camera of the remote device .

As noted above and show various examples of switch camera animations performed on a user interface. In some embodiments the switch camera animation causes changes to the image processing operations of the respective dual camera mobile device such as scaling compositing and perspective distortion which can be performed by the video conference manager and the image processing manager for example.

During a video conference between a dual camera mobile device and another mobile device different embodiments provide different techniques for adjusting the exposure of images captured by cameras of either mobile device. Some embodiments provide techniques for a user of the dual camera mobile device to adjust the exposure of images captured by a camera of the other device while other embodiments provide techniques for the user to adjust the exposure of images captured by a camera of the dual camera mobile device. Several example techniques will be described in detail below.

As shown in the process begins by starting at a video conference between the local and remote devices. The process then receives at a video from the remote device for display on the display screen of the local device. Next the process determines at whether a request to end the video conference has been received. As described above some embodiments can receive a request to end the video conference from a user of the local or remote device. When the process receives a request to end the video conference the process ends.

However when the process does not receive a request to end the video conference the process then determines at whether a request for adjusting the exposure of the remote device s camera has been received. When the process determines that a request for adjusting the exposure of the remote device s camera has not been received the process returns back to operation to receive additional video captured from the remote device. and illustrate three different examples of providing a way for a user to make such a request. In and the first stages and all show PIP displays and of the local devices and that display two videos one captured by a camera of the local device and the other captured by a camera of the remote device. In first stages and the man in the background display and is dark indicating that the man is not properly exposed.

The second stage of illustrates one way for the user of the local device to request the remote device to perform an exposure adjustment by selecting the remote device s video e.g. through a single tap on the background display . In this way the UI automatically associates the user s selection of a region of interest defined by a box with the user s desire to direct the remote device to perform an exposure adjustment on the region of interest and thus directs the video conference manager of the local device to contact the remote device to perform an exposure adjustment operation. The defined region of interest is used by the remote device in the calculation of the exposure adjustment.

Like the second stage of the second stage of shows the local user s selection of the remote device s video except this selection directs the UI to display a selectable UI item as shown in the third stage . The fourth stage illustrates the user of the local device selecting the selectable UI item to direct the remote device to perform an exposure adjustment operation as described above.

The second stage of is similar to the second stage of but instead of the user s selection of the remote device s video directing the UI to display a single selectable UI item the user s selection directs the UI to display a menu of selectable UI items and as shown in the third stage . The selectable UI items include an Auto Focus item an Auto Exposure item a Switch Camera item and a Cancel item . In some embodiments the Switch Camera selectable UI item is used to request a local switch camera operation while in other embodiments the Switch Camera selectable UI item is used to request a remote switch camera operation. The fourth stage illustrates the user selecting the Auto Exposure item to direct the remote device to perform an exposure adjustment operation as described above.

When the process determines at that the local user directed the local device to request an exposure adjustment operation the process sends at a command to the remote device through the video conference control channel to adjust the exposure of the video captured by the camera that is currently capturing and transmitting video to the local device. After operation the process transitions back to operation which is described above.

In some embodiments the user of the remote device is required to provide permission before the remote device performs an exposure adjustment operation while in other embodiments the remote device performs the exposure adjustment operation automatically upon receiving the request from the local device. Moreover in some embodiments some of the video conference functionalities are implemented by the video conference manager . In some of these embodiments the video conference manager performs the exposure adjustment operation by instructing the CIPU to adjust the exposure setting of the sensor of the remote device camera being used.

The last stages and of and show the remote device s video lighter which indicates that the man is properly exposed. Although and provide examples of receiving an exposure adjustment request to correct the exposure of a remote device some embodiments provide ways for user of the local device to request that the local device adjust the exposure of a camera of the local device. Such a request can be made similar to the ways illustrated in and for requesting a remote device to adjust its camera s exposure.

In some embodiments the process is performed by the image processing layer shown in while in other embodiments the process is performed by the statistics engine shown in . Some embodiments perform the process on images captured by cameras of local or remote devices in a video conference while other embodiments perform the process as part of the process e.g. operation illustrated in . Some embodiments perform an exposure adjustment operation to expose images captured by the cameras of the dual camera mobile device that are not too light and not too dark. In other words the process is performed to capture images in a manner that maximizes the amount of detail as possible.

The process begins by receiving at an image captured by a camera of the dual camera mobile device. In some embodiments when the received image is a first image captured by a camera of a device in a video conference the process is not performed on the first image i.e. there was no image before the first image from which to determine an exposure value . The process then reads at pixel values of a defined region in the received image. Different embodiments define regions differently. Some of such embodiments define differently shaped regions such as a square a rectangle a triangle a circle etc. while other of such embodiments define regions in different locations in the image such as center upper center lower center etc.

Next the process calculates at an average of the pixel values in the defined region of the image. The process determines at whether the calculated average of the pixel values is equal to a particular defined value. Different embodiments define different particular values. For example some embodiments define the particular value as the median pixel value of the image s dynamic range. In some embodiments a range of values is defined instead of a single value. In such embodiments the process determines at whether the calculated average of the pixel values is within the define range of values.

When the calculated average of the pixel values is not equal to the particular defined value the process adjusts at the exposure value based on the calculated average. When the calculated average of the pixel values is equal to the particular defined value the process ends. In some embodiments an exposure value represents an amount of time that a camera sensor is exposed to light. In some embodiments the adjusted exposure value is used to expose the next image to be captured by the camera that captured the received image. After the exposure value is adjusted based on the calculated average the process ends.

In some embodiments the process is repeatedly performed until the calculated average of pixel values is equal to the particular defined value or falls within the defined range of values . Some embodiments constantly perform the process during a video conference while other embodiments perform the process at defined intervals e.g. 5 seconds 10 seconds 30 seconds etc. during the video conference. Furthermore during the video conference the process of some embodiments dynamically re defines the particular pixel value before performing the process .

The first example illustrates an operation with no exposure adjustment. As such the image appears the same as the image . Since no exposure adjustment was performed the person in the image remains dark like the person in the image .

In the second example an exposure adjustment operation is performed on the image . In some embodiments the exposure adjustment operation is performed by the process using the defined region . Based on the exposure adjustment operation the exposure level of the camera is adjusted and the camera captures the image using the adjusted exposure level. As shown in the person in the image is not as dark as the in the image . However the person s face and body in the image is still not clear.

The third example shows an exposure adjustment operation performed on the image . Similar to the second example the exposure adjustment operation of the example of some embodiments is performed by the process using the defined region . Based on the exposure adjustment operation the exposure level of the camera is adjusted and the camera captures the image using the adjusted exposure level. As seen in the person in the image is perfectly exposed since the person s face and body is visible.

In some embodiments the selection of the defined region may be made by the user of the dual camera mobile device. The device itself may also automatically adjust its defined region for the exposure adjustment operation through the feedback loop for exposure adjustment mentioned above in the CIPU . The statistics engine in may collect data to determine whether the exposure level is appropriate for the images captured and adjust the camera sensors e.g. though a direct connection to the sensor module accordingly.

As shown in the process begins by starting at a video conference between the local and remote devices. The process then receives at a video from the remote device for display on the display screen of the local device. Next at the process determines whether a request to end the video conference has been received. As described above a video conference can end in some embodiments at the request of a user of the local or remote device. When the process receives a request to end the video conference the process ends.

Otherwise the process determines at whether it has received a request for adjusting the focus of the remote camera of the remote device. When the process determines that it has not received a request for adjusting the focus of the remote camera of the remote device the process returns to operation to receive additional video from the remote device. and illustrate three different ways that different embodiments provide to a user to make such a request. In and the first stages and all show a PIP display and of the local device and that displays two videos one captured by the local device and the other captured by the remote device. The display areas and in show an end conference button. However in the layout of the display area is the same as the layout of the display area of described above. Moreover the switch camera button shown in the display area can be selected to invoke a local switch camera operation in some embodiments or a remote switch camera operation in other embodiments. As shown in the first stages and the video of the remote device that is displayed in the background display and is blurry.

The second stage of illustrates an approach whereby the user of the local device requests a focus adjustment from the remote device by simply selecting the remote device s video e.g. through a single tap on the remote device s video . Under this approach the UI automatically associates the user s selection of a region of interest defined by a box with the user s desire to direct the remote device to perform an operation such as focus on the region of interest and therefore directs the video conference manager of the local device to contact the remote device to perform an adjustment operation such as an focus adjustment operation . The defined region of interest is used by the remote device in the calculation of the focus adjustment.

The second stage of similarly shows the local user s selection of the remote video e.g. through the user s tapping of the remote device s video . However unlike the example illustrated in this selection in directs the UI to display a menu of selectable UI items and which can be implemented as selectable buttons as shown in the third stage . These selectable UI items include an Auto Focus item an Auto Exposure item a Switch Camera item and a Cancel item . In some embodiments the Switch Camera selectable UI item is used to request a local switch camera operation while in other embodiments the Switch Camera selectable UI item is used to request a remote switch camera operation. The fourth stage then illustrates the local user selecting the auto focus item .

The second stage of again similarly shows the local user s selection of the remote video e.g. through the user s tapping of the remote device s video . However unlike the example illustrated in this selection in directs the UI to request a focus adjustment operation i.e. in second stage . After the focus adjustment operation is completed the UI displays a menu of selectable UI items and i.e. in third stage which can be implemented as selectable buttons. These selectable UI items include an Auto Exposure item and a Cancel item .

When the process determines at that the local user directed the local device to request a focus adjustment operation the process sends at a command to the remote device through the video conference control channel to adjust the focus of the camera whose video the remote device is currently capturing and transmitting. After the process transitions back to which was described above.

In some embodiments the user of the remote device has to provide permission before the remote device performs this operation while in other embodiments the remote device performs this operation automatically upon receiving the request for the local device. Also in some embodiments the focus adjustment operation adjusts the focus settings of the remote device s camera that is being used during the video conference. In some of such embodiments some of the video conference functionalities are implemented by the video conference module as discussed above. In these embodiments the video conference manager instructs the CIPU to adjust the sensor of the remote device camera being used.

The last stages and of and show the remote device s video properly focused. Although and provide examples of receiving a focus adjustment request to correct the focus of a remote device some embodiments allow the local device s user to request that the local device adjust the focus of a camera of the local device. Such a request can be made similar to the approaches shown in and to requesting a remote device to adjust its camera s focus.

As discussed above in the defined region of interest was used by the remote mobile device in the computation for exposure adjustment and focus adjustment of the videos respectively. However in some other embodiments the user s selection of a region of interest may be used to direct the remote device to perform one or more operations. For example in some embodiments both exposure adjustment and focus adjustment may be performed based on the defined region of interest thereby directing the remote device to perform both operations.

During a video conference some embodiments may wish to adjust or maintain the rate at which images of a video captured by a camera of the dual camera mobile device are transmitted i.e. frame rate to the other device in the video conference. For example assuming a fixed bandwidth some of such embodiments reduce the frame rate of the video to increase the picture quality of the images of the video while other of such embodiments increase the frame rate of the video to smooth out the video i.e. reduce jitter .

Different embodiments provide different techniques for controlling the frame rate of images of a video during the video conference. One example previously described above adjusts the VBI of the sensor module for a camera in order to control the rate at which images captured by the camera are processed. As another example some embodiments of the management layer of the video conference module shown in control the frame rate by dropping images. Similarly some embodiments of the image processing layer control the frame rate by dropping images. Some embodiments provide yet other techniques for controlling frame rates such as dropping frames in the universal transmission buffer .

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives RAM chips hard drives EPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

Some embodiments are implemented as software processes that include one or more application programming interfaces APIs in an environment with calling program code interacting with other program code being called through the one or more interfaces. Various function calls messages or other types of invocations which further may include various kinds of parameters can be transferred via the APIs between the calling program and the code being called. In addition an API may provide the calling program code the ability to use data types or classes defined in the API and implemented in the called program code.

At least certain embodiments include an environment with a calling software component interacting with a called software component through an API. A method for operating through an API in this environment includes transferring one or more function calls messages other types of invocations or parameters via the API.

One or more Application Programming Interfaces APIs may be used in some embodiments. For example some embodiments of the media exchange module or provide a set of APIs to other software components for accessing various video processing and encoding functionalities described in .

An API is an interface implemented by a program code component or hardware component hereinafter API implementing component that allows a different program code component or hardware component hereinafter API calling component to access and use one or more functions methods procedures data structures classes and or other services provided by the API implementing component. An API can define one or more parameters that are passed between the API calling component and the API implementing component.

An API allows a developer of an API calling component which may be a third party developer to leverage specified features provided by an API implementing component. There may be one API calling component or there may be more than one such component. An API can be a source code interface that a computer system or program library provides in order to support requests for services from an application. An operating system OS can have multiple APIs to allow applications running on the OS to call one or more of those APIs and a service such as a program library can have multiple APIs to allow an application that uses the service to call one or more of those APIs. An API can be specified in terms of a programming language that can be interpreted or compiled when an application is built.

In some embodiments the API implementing component may provide more than one API each providing a different view of or with different aspects that access different aspects of the functionality implemented by the API implementing component. For example one API of an API implementing component can provide a first set of functions and can be exposed to third party developers and another API of the API implementing component can be hidden not exposed and provide a subset of the first set of functions and also provide another set of functions such as testing or debugging functions which are not in the first set of functions. In other embodiments the API implementing component may itself call one or more other components via an underlying API and thus be both an API calling component and an API implementing component.

An API defines the language and parameters that API calling components use when accessing and using specified features of the API implementing component. For example an API calling component accesses the specified features of the API implementing component through one or more API calls or invocations embodied for example by function or method calls exposed by the API and passes data and control information using parameters via the API calls or invocations. The API implementing component may return a value through the API in response to an API call from an API calling component. While the API defines the syntax and result of an API call e.g. how to invoke the API call and what the API call does the API may not reveal how the API call accomplishes the function specified by the API call. Various API calls are transferred via the one or more application programming interfaces between the calling API calling component and an API implementing component. Transferring the API calls may include issuing initiating invoking calling receiving returning or responding to the function calls or messages in other words transferring can describe actions by either of the API calling component or the API implementing component. The function calls or other invocations of the API may send or receive one or more parameters through a parameter list or other structure. A parameter can be a constant key data structure object object class variable data type pointer array list or a pointer to a function or method or another way to reference a data or other item to be passed via the API.

Furthermore data types or classes may be provided by the API and implemented by the API implementing component. Thus the API calling component may declare variables use pointers to use or instantiate constant values of such types or classes by using definitions provided in the API.

Generally an API can be used to access a service or data provided by the API implementing component or to initiate performance of an operation or computation provided by the API implementing component. By way of example the API implementing component and the API calling component may each be any one of an operating system a library a device driver an API an application program or other module it should be understood that the API implementing component and the API calling component may be the same or different type of module from each other . API implementing components may in some cases be embodied at least in part in firmware microcode or other hardware logic. In some embodiments an API may allow a client program to use the services provided by a Software Development Kit SDK library. In other embodiments an application or other client program may use an API provided by an Application Framework. In these embodiments the application or client program may incorporate calls to functions or methods provided by the SDK and provided by the API or use data types or objects defined in the SDK and provided by the API. An Application Framework may in these embodiments provide a main event loop for a program that responds to various events defined by the Framework. The API allows the application to specify the events and the responses to the events using the Application Framework. In some implementations an API call can report to an application the capabilities or state of a hardware device including those related to aspects such as input capabilities and state output capabilities and state processing capability power state storage capacity and state communications capability etc. and the API may be implemented in part by firmware microcode or other low level logic that executes in part on the hardware component.

The API calling component may be a local component i.e. on the same data processing system as the API implementing component or a remote component i.e. on a different data processing system from the API implementing component that communicates with the API implementing component through the API over a network. It should be understood that an API implementing component may also act as an API calling component i.e. it may make API calls to an API exposed by a different API implementing component and an API calling component may also act as an API implementing component by implementing an API that is exposed to a different API calling component.

The API may allow multiple API calling components written in different programming languages to communicate with the API implementing component thus the API may include features for translating calls and returns between the API implementing component and the API calling component however the API may be implemented in terms of a specific programming language. An API calling component can in one embodiment call APIs from different providers such as a set of APIs from an OS provider and another set of APIs from a plug in provider and another set of APIs from another provider e.g. the provider of a software library or creator of the another set of APIs.

It will be appreciated that the API implementing component may include additional functions methods classes data structures and or other features that are not specified through the API and are not available to the API calling component . It should be understood that the API calling component may be on the same system as the API implementing component or may be located remotely and accesses the API implementing component using the API over a network. While illustrates a single API calling component interacting with the API it should be understood that other API calling components which may be written in different languages or the same language than the API calling component may use the API .

The API implementing component the API and the API calling component may be stored in a machine readable medium which includes any mechanism for storing information in a form readable by a machine e.g. a computer or other data processing system . For example a machine readable medium includes magnetic disks optical disks random access memory read only memory flash memory devices etc.

The peripherals interface can be coupled to various sensors and subsystems including a camera subsystem a wireless communication subsystem s audio subsystem I O subsystem etc. The peripherals interface enables communication between processors and peripherals. Peripherals such as an orientation sensor or an acceleration sensor can be coupled to the peripherals interface to facilitate the orientation and acceleration functions.

The camera subsystem can be coupled to one or more optical sensors e.g. a charged coupled device CCD optical sensor a complementary metal oxide semiconductor CMOS optical sensor. The camera subsystem coupled with the sensors may facilitate camera functions such as image and or video data capturing. Wireless communication subsystems may serve to facilitate communication functions. Wireless communication subsystems may include radio frequency receivers and transmitters and optical receivers and transmitters. They may be implemented to operate over one or more communication networks such as a GSM network a Wi Fi network Bluetooth network etc. The audio subsystems is coupled to a speaker and a microphone to facilitate voice enabled functions such as voice recognition digital recording etc.

I O subsystem involves the transfer between input output peripheral devices such as a display a touch screen etc. and the data bus of the CPU through the Peripherals Interface. I O subsystem can include a touch screen controller and other input controllers to facilitate these functions. Touch screen controller can be coupled to the touch screen and detect contact and movement on the screen using any of multiple touch sensitivity technologies. Other input controllers can be coupled to other input control devices such as one or more buttons.

Memory interface can be coupled to memory which can include high speed random access memory and or non volatile memory such as flash memory. Memory can store an operating system OS . The OS can include instructions for handling basic system services and for performing hardware dependent tasks.

Memory can also include communication instructions to facilitate communicating with one or more additional devices graphical user interface instructions to facilitate graphic user interface processing image video processing instructions to facilitate image video related processing and functions phone instructions to facilitate phone related processes and functions media exchange and processing instructions to facilitate media communication and processing related processes and functions camera instructions to facilitate camera related processes and functions and video conferencing instructions to facilitate video conferencing processes and functions. The above identified instructions need not be implemented as separate software programs or modules. Various functions of mobile computing device can be implemented in hardware and or in software including in one or more signal processing and or application specific integrated circuits.

The above described embodiments may include touch I O device that can receive touch input for interacting with computing system as shown in via wired or wireless communication channel . Touch I O device may be used to provide user input to computing system in lieu of or in combination with other input devices such as a keyboard mouse etc. One or more touch I O devices may be used for providing user input to computing system . Touch I O device may be an integral part of computing system e.g. touch screen on a laptop or may be separate from computing system .

Touch I O device may include a touch sensitive panel which is wholly or partially transparent semitransparent non transparent opaque or any combination thereof. Touch I O device may be embodied as a touch screen touch pad a touch screen functioning as a touch pad e.g. a touch screen replacing the touchpad of a laptop a touch screen or touchpad combined or incorporated with any other input device e.g. a touch screen or touchpad disposed on a keyboard or any multi dimensional object having a touch sensitive surface for receiving touch input.

In one example touch I O device embodied as a touch screen may include a transparent and or semitransparent touch sensitive panel partially or wholly positioned over at least a portion of a display. According to this embodiment touch I O device functions to display graphical data transmitted from computing system and or another source and also functions to receive user input. In other embodiments touch I O device may be embodied as an integrated touch screen where touch sensitive components devices are integral with display components devices. In still other embodiments a touch screen may be used as a supplemental or additional display screen for displaying supplemental or the same graphical data as a primary display and receiving touch input.

Touch I O device may be configured to detect the location of one or more touches or near touches on device based on capacitive resistive optical acoustic inductive mechanical chemical measurements or any phenomena that can be measured with respect to the occurrences of the one or more touches or near touches in proximity to device . Software hardware firmware or any combination thereof may be used to process the measurements of the detected touches to identify and track one or more gestures. A gesture may correspond to stationary or non stationary single or multiple touches or near touches on touch I O device . A gesture may be performed by moving one or more fingers or other objects in a particular manner on touch I O device such as tapping pressing rocking scrubbing twisting changing orientation pressing with varying pressure and the like at essentially the same time contiguously or consecutively. A gesture may be characterized by but is not limited to a pinching sliding swiping rotating flexing dragging or tapping motion between or with any other finger or fingers. A single gesture may be performed with one or more hands by one or more users or any combination thereof.

Computing system may drive a display with graphical data to display a graphical user interface GUI . The GUI may be configured to receive touch input via touch I O device . Embodied as a touch screen touch I O device may display the GUI. Alternatively the GUI may be displayed on a display separate from touch I O device . The GUI may include graphical elements displayed at particular locations within the interface. Graphical elements may include but are not limited to a variety of displayed virtual input devices including virtual scroll wheels a virtual keyboard virtual knobs virtual buttons any virtual UI and the like. A user may perform gestures at one or more particular locations on touch I O device which may be associated with the graphical elements of the GUI. In other embodiments the user may perform gestures at one or more locations that are independent of the locations of graphical elements of the GUI. Gestures performed on touch I O device may directly or indirectly manipulate control modify move actuate initiate or generally affect graphical elements such as cursors icons media files lists text all or portions of images or the like within the GUI. For instance in the case of a touch screen a user may directly interact with a graphical element by performing a gesture over the graphical element on the touch screen. Alternatively a touch pad generally provides indirect interaction. Gestures may also affect non displayed GUI elements e.g. causing user interfaces to appear or may affect other actions within computing system e.g. affect a state or mode of a GUI application or operating system . Gestures may or may not be performed on touch I O device in conjunction with a displayed cursor. For instance in the case in which gestures are performed on a touchpad a cursor or pointer may be displayed on a display screen or touch screen and the cursor may be controlled via touch input on the touchpad to interact with graphical objects on the display screen. In other embodiments in which gestures are performed directly on a touch screen a user may interact directly with objects on the touch screen with or without a cursor or pointer being displayed on the touch screen.

Feedback may be provided to the user via communication channel in response to or based on the touch or near touches on touch I O device . Feedback may be transmitted optically mechanically electrically olfactory acoustically or the like or any combination thereof and in a variable or non variable manner.

These functions described above can be implemented in digital electronic circuitry in computer software firmware or hardware. The techniques can be implemented using one or more computer program products. Programmable processors and computers can be included in or packaged as mobile devices. The processes and logic flows may be performed by one or more programmable processors and by one or more programmable logic circuitry. General and special purpose computing devices and storage devices can be interconnected through communication networks.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself.

As used in this specification and any claims of this application the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification and any claims of this application the terms computer readable medium and computer readable media are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

Each cellular base station covers a service region . As shown the mobile devices in each service region are wirelessly connected to the serving cellular base station of the service region through a Uu interface. The Uu interface uses a protocol stack that has two planes a control plane and a user plane. The user plane supports circuit switched packet switched and broadcast data streams. The control plane carries the network s signaling messages.

Each cellular base station is connected to an RNC through an Iub interface. Each RNC is connected to the core network by Iu cs and an Iu ps interfaces. The Iu cs interface is used for circuit switched services e.g. voice while the Iu ps interface is used for packet switched services e.g. data . The Iur interface is used for connecting two RNCs together.

Accordingly the communication system supports both circuit switched services and packet switched services. For example circuit switched services allow a telephone call to be conducted by transmitting the telephone call data e.g. voice through circuit switched equipment of the communication system . Packet switched services allow a video conference to be conducted by using a transport protocol layer such as UDP or TCP over an internet layer protocol like IP to transmit video conference data through packet switched equipment of the communication system . In some embodiments the telephone call to video conference transition e.g. handoff previously described in the Video Conference Setup section uses the circuit switched and packet switched services supported by a communication system like the communication system . That is in such embodiments the telephone call is conducted through the circuit switched equipment of the communication system and the video conference it conducted through the packet switched equipment of the communication system .

Although the example communication system in illustrates a third generation 3G technology UTRAN wireless mobile communication system it should be noted that second generation 2G communication systems other 3 G communication systems such as 3GPP2 Evolution Data Optimized or Evolution Data only EV DO and 3rd generation partnership project 2 3GPP2 Code Division Multiple Access 1X CDMA 1X fourth generation 4G communication systems wireless local area network WLAN and Worldwide Interoperability for Microwave Access WiMAX communication systems can be used for connecting some of the participants of a conference in some embodiments. Examples of 2G systems include Global System for Mobile communications GSM General Packet Radio Service GPRS and Enhanced Data Rates for GSM Evolution EDGE . A 2 G communication system architecture is similar to the architecture shown in except the 2 G communication system architecture uses base transceiver stations BTSs instead of Node Bs and base station controllers BSC instead of RNC . In a 2 G communication system an A interface between the BSC and the core network is used for circuit switched services and a Gb interface between the BSC and the core network is used for packet switched services.

In some embodiments the communication system is operated by a service carrier who initially provisions a mobile device to allow the mobile device to use the communication system . Some embodiments provision a mobile device by configuring and registering a subscriber identity module SIM card in the mobile device . In other embodiments the mobile device is instead configured and registered using the mobile device s memory. Moreover additional services can be provisioned after a customer purchases the mobile device such as data services like GPRS multimedia messaging service MMS and instant messaging. Once provisioned the mobile device is activated and is thereby allowed to use the communication system by the service carrier.

The communication system is a private communication network in some embodiments. In such embodiments the mobile devices can communicate e.g. conduct voice calls exchange data among each other e.g. mobile devices that are provisioned for the communication system . In other embodiments the communication system is a public communication network. Thus the mobile devices can communicate with other devices outside of the communication system in addition to the mobile devices provisioned for the communication system . Some of the other devices outside of the communication system include phones computers and other devices that connect to the communication system through other networks such as a public switched telephone network or another wireless communication network.

The Long Term Evolution LTE specification is used to define 4 G communication systems. conceptually illustrates an example of a 4 G communication system that is used for connecting some participants of a video conference in some embodiments. As shown the communication system includes several mobile devices several Evolved Node Bs eNBs a Mobility Management Entity MME a Serving Gateway S GW a Packet Data Network PDN Gateway and a Home Subscriber Server HSS . In some embodiments the communication system includes one or more MMEs one or more S GWs one or more PDN Gateways and one or more HS Ss .

The eNBs provide an air interface for the mobile devices . As shown each eNB covers a service region . The mobile devices in each service region are wirelessly connected to the eNB of the service region through a LTE Uu interface. also shows the eNBs connected to each other through an X2 interface. In addition the eNBs are connected to the MME through an S1 MME interface and to the S GW through an S1 U interface. The eNBs are collectively referred to as an Evolved UTRAN E TRAN .

The eNBs provide functions such as radio resource management e.g. radio bearer control connection mobility control etc. routing of user plane data towards the S GW signal measurement and measurement reporting MME selection at the time of mobile device attachment etc. The MME functions include idle mode mobile device tracking and paging activation and deactivation of radio bearers selection of the S GW at the time of mobile device attachment Non Access Stratum NAS signaling termination user authentication by interacting with the HSS etc.

The S GW functions includes 1 routing and forwarding user data packets and 2 managing and storing mobile device contexts such as parameters of the IP bearer service and network internal routing information. The PDN Gateway functions include providing connectivity from the mobile devices to external packet data networks not shown by being the point of exit and entry of traffic for the mobile devices. A mobile station may have simultaneous connectivity with more than one PDN Gateway for accessing multiple packet data networks. The PDN Gateway also acts as the anchor for mobility between 3GPP and non 3GPP technologies such as WiMAX and 3GPP2 e.g. CDMA 1X and EV DO .

As shown MME is connected to S GW through an S11 interface and to the HSS through an S6a interface. The S GW and the PDN Gateway are connected through an S8 interface. The MME S GW and PDN Gateway are collectively referred to as an Evolved Packet Core EPC . The EPC is the main component of a System Architecture Evolution SAE architecture which is the core network architecture of 3GPP LTE wireless communication standard. The EPC is a pure packet system. For example the EPC does not have a voice media gateway. Services like voice and SMS are packet switched routed and are provided by application functions that make use of the EPC service. So using the telephone call to video conference transition previously described above as an example both the telephone call and the video conference are conducted through packet switched equipment of the communication system in some embodiments. In some such embodiments the packet switched channel used for the telephone call is continued to be used for the audio data of the video conference after the telephone call terminates. However in other such embodiments a different packet switched channel is created e.g. when the video conference is established and audio data is transmitted through the newly created packet switched channel instead of the packet switched channel of the telephone call when the telephone call terminates.

Moreover the amount of bandwidth provided by these different technologies ranges from 44 kilobits per second kbps for GPRS to over 10 megabits per second Mbps for LTE. Download rates of 100 Mbps and upload rates of 50 Mbps are predicted in the future for LTE.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. In addition a number of the figures conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. Furthermore the process could be implemented using several sub processes or as part of a larger macro process.

Also many embodiments were described above by reference to a video conference between two dual camera mobile devices. However one of ordinary skill in the art will realize that many of these embodiments are used in cases involving a video conference between a dual camera mobile device and another device such as a single camera mobile device a computer a phone with video conference capability etc. Moreover many of the embodiments described above can be used in single camera mobile devices and other computing devices with video conference capabilities. Thus one of ordinary skill in the art would understand that the invention is not limited by the foregoing illustrative details but rather is to be defined by the appended claims.

