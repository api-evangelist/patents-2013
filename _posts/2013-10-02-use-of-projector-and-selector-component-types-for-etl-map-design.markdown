---

title: Use of projector and selector component types for ETL map design
abstract: A data integration system is disclosed that incorporates one or more techniques for simplifying the design and maintenance of a mapping. As components are added or removed to an existing design, the data integration system removes the need to specify all input and output attributes. In one aspect, components types are implemented that allow assignment expressions to reference all or part of upstream components. Therefore, attributes of certain types of components can be propagated to downstream components or otherwise inherited from upstream components with minimal effort on the part of a map designer. During code generation the attributes required to be projected by any component can be derived based on the needs of the downstream components.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09507838&OS=09507838&RS=09507838
owner: Oracle International Corporation
number: 09507838
owner_city: Redwood Shores
owner_country: US
publication_date: 20131002
---
This Application claims the benefit of and priority to U.S. Provisional Application No. 61 824 544 filed May 17 2013 and entitled USE OF PROJECTOR AND SELECTOR COMPONENT TYPES FOR ETL MAP DESIGN the entire disclosure of which is incorporated herein by reference for all purposes.

In today s increasingly fast paced business environment organizations need to use more specialized software applications. Additionally organizations need to ensure the coexistence of these applications on heterogeneous hardware platforms and systems and guarantee the ability to share data between applications and systems.

Accordingly what is desired is to solve problems relating to developing data integration scenarios some of which may be discussed herein. Additionally what is desired is to reduce drawbacks relating to developing data integration scenarios some of which may be discussed herein.

The following portion of this disclosure presents a simplified summary of one or more innovations embodiments and or examples found within this disclosure for at least the purpose of providing a basic understanding of the subject matter. This summary does not attempt to provide an extensive overview of any particular embodiment or example. Additionally this summary is not intended to identify key critical elements of an embodiment or example or to delineate the scope of the subject matter of this disclosure. Accordingly one purpose of this summary may be to present some innovations embodiments and or examples found within this disclosure in a simplified form as a prelude to a more detailed description presented later.

In various embodiments a data integration system enables users to create a logical design which is platform and technology independent. The user can create a logical design that defines at a high level how a user wants data to flow between sources and targets. The tool can analyze the logical design in view of the user s infrastructure and create a physical design. The logical design can include a plurality of components corresponding to each source and target in the design as well as operations such as joins or filters and access points. Each component when transferred to the physical design generates code to perform operations on the data. Depending on the underlying technology e.g. SQL Server Oracle Hadoop etc. and the language used SQL pig etc. the code generated by each component may be different.

In one aspect a user of data integration system is not required to specify all data attributes at each component in the logical design from start to end. The data integration system provides a plurality of component types such as projector and selector types that avoid the need to fully declare the information that flows through the logical design. The data integration system is able to decide what attributes are needed at operations represented by predetermined component types. This simplifies both the design and maintenance.

In one embodiment a method for facilitating generation of a data mapping includes receiving information specifying one or more components of a logical design wherein at least one of the one or more components is of a first type. A set of data attributes visible to downstream components in the logical design is determined of the at least one of the one or more components that is of the first type based on upstream components in the logical design. Information indicative is then generated of the set of attributes visible to the downstream components in the logical design of the at least one of the one or more components that is of the first type. In one aspect determining the set of data attributes visible to the downstream components in the logical design of the at least one of the one or more components that is of the first type may include deriving one or more attributes visible from an upstream component and exposing the one or more attributes to a downstream component. In another aspect receiving the information specifying the one or more components of the logical design may include receiving information indicative of an operation that changes shape of the information flowing through the logical design.

In some embodiments receiving the information specifying the one or more components of the logical design may include receiving information indicative of an operation that controls the flow of information flowing through the logical design but does not change shape of the information flowing through the logical design. In further embodiments receiving the information specifying the one or more components of the logical design may include receiving information indicative of a source component having one or more attributes of data stored in a source datastore. Receiving the information specifying the one or more components of the logical design may include receiving information indicative of a target component having one or more attributes of data to be stored in a target datastore.

Generating the information indicative of the set of attributes visible to the downstream components in the logical design of the at least one of the one or more components that is of the first type may include exporting a list of attributes to a downstream component. In one embodiment a change may be received in the logical design through the introduction or removal of a component or an attribute into the logical design. A determination of whether the change in the logical design affects the at least one of the one or more components that is of the first type. Based on a determination that the change in the logical design affects the at least one of the one or more components that is of the first type an updated set of data attributes visible to downstream components may be determined.

In further embodiments a change is received in the logical design through the introduction of a component or an attribute into the logical design. A determination may be made whether the change in the logical design affects the at least one of the one or more components that is of the first type. Based on a determination that the change in the logical design affects the at least one of the one or more components that is of the first type the set of data attributes visible to downstream components may be preserved. In another aspect based on a determination that the change in the logical design affects the at least one of the one or more components that is of the first type the set of data attributes visible to downstream components may be automatically renamed.

In one embodiment a non transitory computer readable medium stores computer executable code for facilitating generation of a data mapping. The non transitory computer readable medium may include code for receiving information specifying one or more components of a logical design wherein at least one of the one or more components is of a first type code for determining a set of data attributes visible to downstream components in the logical design of the at least one of the one or more components that is of the first type based on upstream components in the logical design and code for generating information indicative of the set of attributes visible to the downstream components in the logical design of the at least one of the one or more components that is of the first type.

In another embodiment a system for facilitating generation of a data mapping may include a processor and a memory in communication with the processor and configured to store a set of instructions which when executed by the processor configure the processor to receive information specifying one or more components of a logical design wherein at least one of the one or more components is of a first type determine a set of data attributes visible to downstream components in the logical design of the at least one of the one or more components that is of the first type based on upstream components in the logical design and generate information indicative of the set of attributes visible to the downstream components in the logical design of the at least one of the one or more components that is of the first type.

A further understanding of the nature of and equivalents to the subject matter of this disclosure as well as any inherent or express advantages and improvements provided should be realized in addition to the above section by reference to the remaining portions of this disclosure any accompanying drawings and the claims.

In various embodiments a data integration system enables users to create a logical design which is platform and technology independent. The user can create a logical design that defines at a high level how a user wants data to flow between sources and targets. The tool can analyze the logical design in view of the user s infrastructure and create a physical design. The logical design can include a plurality of components corresponding to each source and target in the design as well as operations such as joins or filters and access points. Each component when transferred to the physical design generates code to perform operations on the data. Depending on the underlying technology e.g. SQL Server Oracle Hadoop etc. and the language used SQL pig etc. the code generated by each component may be different.

In one aspect a user of data integration system is not required to specify all data attributes at each component in the logical design from start to end. The data integration system provides a plurality of component types such as projector and selector types that avoid the need to fully declare the information that flows through the logical design. The data integration system is able to decide what attributes are needed at operations represented by predetermined component types. This simplifies both the design and maintenance.

In one embodiment system includes one or more user computers e.g. computers A B and C . User computers can be general purpose personal computers including merely by way of example personal computers and or laptop computers running any appropriate flavor of Microsoft Corp. s Windows and or Apple Corp. s Macintosh operating systems and or workstation computers running any of a variety of commercially available UNIX or UNIX like operating systems. These user computers can also have any of a variety of applications including one or more applications configured to perform methods of the invention as well as one or more office applications database client and or server applications and web browser applications.

Alternatively user computers can be any other electronic device such as a thin client computer Internet enabled mobile telephone and or personal digital assistant capable of communicating via a network e.g. communications network described below and or displaying and navigating web pages or other types of electronic documents. Although the exemplary system is shown with three user computers any number of user computers or devices can be supported.

Certain embodiments of the invention operate in a networked environment which can include communications network . Communications network can be any type of network familiar to those skilled in the art that can support data communications using any of a variety of commercially available protocols including without limitation TCP IP SNA IPX AppleTalk and the like. Merely by way of example communications network can be a local area network LAN including without limitation an Ethernet network a Token Ring network and or the like a wide area network a virtual network including without limitation a virtual private network VPN the Internet an intranet an extranet a public switched telephone network PSTN an infra red network a wireless network including without limitation a network operating under any of the IEEE 802.11 suite of protocols the Bluetooth protocol known in the art and or any other wireless protocol and or any combination of these and or other networks.

Embodiments of the invention can include one or more server computers e.g. computers A and B . Each of server computers may be configured with an operating system including without limitation any of those discussed above as well as any commercially available server operating systems. Each of server computers may also be running one or more applications which can be configured to provide services to one or more clients e.g. user computers and or other servers e.g. server computers .

Merely by way of example one of server computers may be a web server which can be used merely by way of example to process requests for web pages or other electronic documents from user computers . The web server can also run a variety of server applications including HTTP servers FTP servers CGI servers database servers Java servers and the like. In some embodiments of the invention the web server may be configured to serve web pages that can be operated within a web browser on one or more of the user computers to perform methods of the invention.

Server computers in some embodiments might include one or more file and or application servers which can include one or more applications accessible by a client running on one or more of user computers and or other server computers . Merely by way of example one or more of server computers can be one or more general purpose computers capable of executing programs or scripts in response to user computers and or other server computers including without limitation web applications which might in some cases be configured to perform methods of the invention .

Merely by way of example a web application can be implemented as one or more scripts or programs written in any programming language such as Java C or C and or any scripting language such as Perl Python or TCL as well as combinations of any programming scripting languages. The application server s can also include database servers including without limitation those commercially available from Oracle Microsoft IBM and the like which can process requests from database clients running on one of user computers and or another of server computers .

In some embodiments an application server can create web pages dynamically for displaying the information in accordance with embodiments of the invention. Data provided by an application server may be formatted as web pages comprising HTML XML Javascript AJAX etc. for example and or may be forwarded to one of user computers via a web server as described above for example . Similarly a web server might receive web page requests and or input data from one of user computers and or forward the web page requests and or input data to an application server.

In accordance with further embodiments one or more of server computers can function as a file server and or can include one or more of the files necessary to implement methods of the invention incorporated by an application running on one of user computers and or another of server computers . Alternatively as those skilled in the art will appreciate a file server can include all necessary files allowing such an application to be invoked remotely by one or more of user computers and or server computers . It should be noted that the functions described with respect to various servers herein e.g. application server database server web server file server etc. can be performed by a single server and or a plurality of specialized servers depending on implementation specific needs and parameters.

In certain embodiments system can include one or more databases e.g. databases A and B . The location of the database s is discretionary merely by way of example database A might reside on a storage medium local to and or resident in server computer A and or one or more of user computers . Alternatively database B can be remote from any or all of user computers and server computers so long as it can be in communication e.g. via communications network with one or more of these. In a particular set of embodiments databases can reside in a storage area network SAN familiar to those skilled in the art. Likewise any necessary files for performing the functions attributed to user computers and server computers can be stored locally on the respective computer and or remotely as appropriate . In one set of embodiments one or more of databases can be a relational database that is adapted to store update and retrieve data in response to SQL formatted commands. Databases might be controlled and or maintained by a database server as described above for example.

In this embodiment data integration system includes information sources information integration and information destinations . In general information flows from information sources to information integration whereby the information may be consumed made available or otherwise used by information destinations . Data flows may be unidirectional or bidirectional. In some embodiments one or more data flows may be present in data integration system .

Information sources are representative of one or more hardware and or software elements configured to source data. Information sources may provide direct or indirect access to the data. In this embodiment information sources include one or more applications and one or more repositories .

Applications are representative of traditional applications such as desktop hosted web based or cloud based applications. Applications may be configured to receive process and maintain data for one or more predetermined purposes. Some examples of applications include customer relationship management CRM applications financial services applications government and risk compliance applications human capital management HCM procurement applications supply chain management applications project or portfolio management applications or the like. Applications may include functionality configured for manipulating and exporting application data in a variety of human readable and machine readable formats as is known in the art. Applications may further access and store data in repositories .

Repositories are representative of hardware and or software elements configured to provide access to data. Repositories may provide logical and or physical partitioning of data. Repositories may further provide for reporting and data analysis. Some examples of repositories include databases data warehouses cloud storage or the like. A repository may include a central repository created by integrating data from one or more applications . Data stored in repositories may be uploaded from an operational system. The data may pass through additional operations before being made available in a source.

Information integration is representative of one or more hardware and or software elements configured to provide data integration services. Direct or indirect data integration services can be provided in information integration . In this embodiment information integration includes data migration data warehousing master data management data synchronization federation and real time messaging . It will be understood that information integration can include one or more modules services or other additional elements than those shown in here that provide data integration functionality.

Data migration is representative of one or more hardware and or software elements configured to provide data migration. In general data migration provides one or more processes for transferring data between storage types formats or systems. Data migration usually provides for manual or programmatic options to achieve a migration. In a data migration procedure data on or provided by one system is mapped to another system providing a design for data extraction and data loading. A data migration may involve one or more phases such a design phase where one or more designs are created that relate data formats of a first system to formats and requirements of a second system a data extraction phase where data is read from the first system a data cleansing phase and a data loading phase where data is written to the second system. In some embodiments a data migration may include a data verification phases to determine whether data is accurately processed in any of the above phases.

Data warehousing is representative of one or more hardware and or software elements configured to provide databases used for reporting and data analysis. A data warehouse is typically viewed as a central repository of data which is created by integrating data from one or more disparate sources. Data warehousing may include the current storage of data as well as storage of historical data. Data warehousing may include typical extract transform load ETL based data warehouse whereby staging data integration and access layers house key functions. In one example a staging layer or staging database stores raw data extracted from each of one or more disparate source data systems. An integration layer integrates disparate data sets by transforming the data from the staging layer often storing this transformed data in an operational data store ODS database. The integrated data is then moved to yet another database often called the data warehouse database. The data can be arranged into hierarchical groups often called dimensions and into facts and aggregate facts. An access layer may be provided to help users or other systems retrieve data. Data warehouses can be subdivided into data marts whereby each data mart stores subsets of data from a warehouse. In some embodiments data warehousing may include business intelligence tools tools to extract transform and load data into the repository and tools to manage and retrieve metadata.

Master data management is representative of one or more hardware and or software elements configured to manage a master copy of data. Master data management may include a set of processes governance policies standards and tools that consistently define and manage master data. Master data management may include functionality for removing duplicates standardizing data and incorporating rules to eliminate incorrect data from entering a system in order to create an authoritative source of master data. Master data management may provide processes for collecting aggregating matching consolidating quality assuring persisting and distributing data throughout an organization to ensure consistency and control in the ongoing maintenance and application use of information.

Data synchronization is representative of one or more hardware and or software elements configured to synchronize data. Data synchronization may provide for establishing consistency among data from a source to a target and vice versa. Data synchronization may further provide for the continuous harmonization of the data over time.

Federation is representative of one or more hardware and or software elements configured to consolidate a view of data from constituent sources. Federation may transparently map multiple autonomous database systems into a single federated database. The constituent databases maybe interconnected via a computer network and may be geographically decentralized. Federation provides an alternative to merging several disparate databases. A federated database or virtual database for example may provide a composite of all constituent databases. Federation may not provide actual data integration in the constituent disparate databases but only in the view.

Federation may include functionality that provides a uniform user interface enabling users and clients to store and retrieve data in multiple noncontiguous databases with a single query even if the constituent databases are heterogeneous. Federation may include functionality to decompose a query into subqueries for submission to relevant constituent data sources and composite the result sets of the subqueries. Federation can include one or more wrappers to the subqueries to translate them into appropriate query languages. In some embodiments federation is a collection of autonomous components that make their data available to other members of the federation through the publication of an export schema and access operations.

Real time messaging is representative of one or more hardware and or software elements configured to provide messaging services subject to a real time constraint e.g. operational deadlines from event to system response . Real time messaging may include functionality that guarantees an action or response within strict time constraints. In one example real time messaging may be tasked with taking some orders and customer data from one database combining it with some employee data held in a file and then loading the integrated data into a Microsoft SQL Server 2000 database. Because orders need to be analyzed as they arrive real time messaging may pass the orders through to a target database in as close to real time as possible and extract only the new and changed data to keep the workload as small as possible.

Information destinations are representative of one or more hardware and or software elements configured to store or consume data. In this embodiment information destinations may provide direct or indirect access to the data. In this embodiment information destinations include one or more applications and one or more repositories .

Applications are representative of traditional applications such as desktop hosted web based or cloud based applications. Applications may be configured to receive process and maintain data for one or more predetermined purposes. Some examples of applications include customer relationship management CRM applications financial services applications government and risk compliance applications human capital management HCM procurement applications supply chain management applications project or portfolio management applications or the like. Applications may include functionality configured for manipulating and importing application data in a variety of human readable and machine readable formats as is known in the art. Applications may further access and store data in repositories .

Repositories are representative of hardware and or software elements configured to provide access to data. Repositories may provide logical and or physical partitioning of data. Repositories may further provide for reporting and data analysis. Some examples of repositories include databases data warehouses cloud storage or the like. A repository may include a central repository created by integrating data from one or more applications . Data stored in repositories may be uploaded or imported through information integration . The data may pass through additional operations before being made available at a destination.

In various embodiments data integration system provides a new declarative design approach to defining data transformation and integration processes resulting in faster and simpler development and maintenance. Data integration system thus separates declarative rules from the implementation details. Data integration system further provides a unique E LT architecture Extract Load Transform for the execution of data transformation and validation processes. This architecture in embodiments eliminates the need for a standalone ETL server and proprietary engine. In some embodiments data integration system instead leverages the inherent power of RDBMS engines.

In some embodiments data integration system integrates in one or more middleware software packages such as the ORACLE FUSION MIDDLEWARE platform and becomes a component of the middleware stack. As depicted in data integration system may provide run time components as Java EE applications.

In this example one component of data integration system is repositories . Repositories are representative of hardware and or software elements configured to store configuration information about an IT infrastructure metadata of all applications projects scenarios and execution logs. In some aspects multiple instances of repositories can coexist in an IT infrastructure for example Development QA User Acceptance and Production. Repositories are configured to allow several separated environments that exchange metadata and scenarios for example Development Test Maintenance and Production environments . Repositories further are configured to act as a version control system where objects are archived and assigned a version number.

In this example repositories is composed of at least one master repository and one or more work repositories . Objects developed or configured for use within data integration system may be stored in one of these repository types. In general master repository stores the following information security information including users profiles and rights topology information including technologies server definitions schemas contexts languages and so forth and versioned and archived objects. The one or more work repositories may contain actual developed objects.

Several work repositories may coexist in data integration system for example to have separate environments or to match a particular versioning life cycle . The one or more work repositories store information for models including schema definition data stores structures and metadata fields and columns definitions data quality constraints cross references data lineage and so forth. The one or more work repositories may further store projects including business rules packages procedures folders knowledge modules variables and so forth and scenario execution including scenarios scheduling information and logs. In some aspects the one or more work repositories may contain only execution information typically for production purposes and be designated as an execution repository.

In various embodiments repositories store one or more ETL projects. An ETL project defines or otherwise specifies one or more data models that model data attributes of data in a source or target. An ETL project further provides for data quality control as well as defining mappings to move and transform data. Data integrity control ensures the overall consistency of the data. Application data is not always valid for the constraints and declarative rules imposed by a particular source or target. For example orders may be found with no customer or order lines with no product and so forth. Data integration system provides a working environment to detect these constraint violations and to store them for recycling or reporting purposes.

In some embodiments of data integration system there are two different types of controls Static Control and Flow Control. Static Control implies the existence of rules that are used to verify the integrity of application data. Some of these rules referred to as constraints may already be implemented in data servers using primary keys reference constraints etc. Data integration system allows for the definition and checking of additional constraints without declaring them directly in a source. Flow Control relates to targets of transformation and integration processes that implement their own declarative rules. Flow Control verifies an application s incoming data according to these constraints before loading the data into a target. Flow control procedures are general referred to as mappings.

An ETL project can be automated into a package that can be deployed for execution in a runtime environment. Accordingly the automation of data integration flows is achieved by sequencing the execution of the different steps mappings procedures and so forth in a package and by producing a production scenario containing ready to use code for each of these steps. A package is typically made up of a sequence of steps organized into an execution diagram. Packages are the main objects used to generate scenarios for production. They represent the data integration workflow and can perform jobs such as for example start a reverse engineering process on a datastore or a model send an email to an administrator download a file and unzip it define the order in which mappings must be executed and define loops to iterate over execution commands with changing parameters.

A scenario is designed to put a source component mapping package procedure variable into production. A scenario results from the generation of code SQL shell and so forth for this component. Once generated the code of the source component is frozen and the scenario is stored inside repositories such as one or more of work repositories . A scenario can be exported and then imported into different production environments.

In various embodiments data integration system is organized around repositories in a modular fashion accessed by Java graphical modules and scheduling agents. Graphical modules can be used to design and build one or more integration processes stored in repositories . Administrators Developers and Operators may use a development studio to access repositories . Agents can be used to schedule and coordinate a set of integration tasks associated with an integration process stored in repositories . For example at runtime an agent deployed on a desktop web services or otherwise in communication with a source coordinates the execution of one or more integration processes. The agent may retrieve code stored in master repository connect to various source and target systems and orchestrate an overall data integration process or scenario.

In this embodiment data integration system includes desktop that may include one or more of the above discussed graphical modules and or agents. Desktop is representative of one or more desktop or workstation computing devices such as personal computers laptops netbooks tablets and the like. Desktop includes a Java virtual machine JVM and Oracle Data Integrator ODI Studio . Java virtual machine JVM is a virtual machine that can execute Java bytecode. JVM is most often implemented to run on an existing operating system but can also be implemented to run directly on hardware. JVM provides a run time environment in which Java bytecode can be executed enabling features such as runtime web service WS and agent . JVM may include a Java Class Library a set of standard class libraries in Java bytecode that implement the Java application programming interface API and other elements that form a Java Runtime Environment JRE .

Agent is configured to schedule and coordinate a set of integration tasks associated with one or more integration processes stored in repositories . For example at runtime an agent coordinates the execution of integration processes. The agent may retrieve code stored in master repository connect to various source and target systems and orchestrate an overall data integration process or scenario.

Referring again to ODI Studio includes hardware and or software elements configured to design data integration projects. In this example ODI Studio includes four graphical modules or navigators that are used to create and manage data integration projects namely designer module operator module topology module and security module . Designer module is a module configured to define data stores tables files Web services and so on data mappings and packages sets of integration steps including mappings . In various embodiments designer module defines declarative rules for data transformation and data integrity. Accordingly project development takes place in designer module . Additionally in designer module is where database and application metadata are imported and defined. Designer module in one embodiment uses metadata and rules to generate data integration scenarios or load plans for production. In general designer module is used to design data integrity checks and to build transformations such as for example automatic reverse engineering of existing applications or databases graphical development and maintenance of transformation and integration mappings visualization of data flows in the mappings automatic documentation generation and customization of generated code.

Operator module is a module configured to view and manage production integration jobs. Operator module thus manages and monitors data integration processes in production and may show execution logs with error counts the number of rows processed execution statistics the actual code that is executed and so on. At design time developers can also use operator module for debugging purposes in connection with designer module .

Topology module is a module configured to create and manage connections to datasources and agents. Topology module defines the physical and logical architecture of the infrastructure. Infrastructure or projects administrators may register servers database schemas and catalogs and agents in a master repository through topology module . Security module is a module configured to manage users and their repository privileges.

In general a user or process interacts with designer module to create a data integration project having one or more data integration processes for sources and targets . Each data integration process includes at least one data integration task. In some embodiments a data integration tasks is defined by a set of business rules indicative of what bit of data is to be transformed and combined with other bits as well as technical specifics of how the data is actually extracted loaded and so on. In preferred embodiments a data integration tasks is specified using a declarative approach to build data mappings. A mapping is an object that populates one datastore called the target which data coming from one or more other datastores known as sources. In general columns in the source datastore are linked to the columns in the target datastore through mapping. A mapping can be added into a package as a package step. As discussed above a package defines a data integration job. A package is created under a project and is made up of an organized sequence of steps each of which can be a mapping or a procedure. A package can have one entry point and multiple exit points.

In some embodiments when creating a new mapping a developer or technical business user interacts with designer to first define which data is integrated and which business rules should be used. For example the developer may specify what tables are to be joined filters to be applied and SQL expressions to be used to transform data. The particular dialect of SQL that is used is determined by the database platform on which the code is to be executed. Then in a separate step technical staff can interact with designer to choose the most efficient way to extract combine and then integrate this data. For example the technical staff may use database specific tools and design techniques such as incremental loads bulk loading utilities slowly changing dimensions and changed data capture.

In this embodiment mappings can be created for sources and targets . Sources and targets may include one or more legacy applications one or more files XML documents one or more applications one or more data warehouses DW business intelligence BI tools and applications and enterprise process management EPM tools and applications and one or more JVMs including runtime web service and agent .

Orders application is representative of an application for tracking customer orders. An Orders Application data model is created to represent data stored in Orders application as well as any data integrity controls or conditions. For example the Orders Application data model may be based on a Hyper Structured Query Language Database HSQLDB mapping and include five datastores SRC CITY SRC CUSTOMER SRC ORDERS SRC ORDER LINES SRC PRODUCT and SRC REGION.

Parameter file is representative of a flat file e.g. ASCII issued from a production system containing a list of sales representatives and the segmentation of ages into age ranges. In this example a Parameter data model is created to represent the data in the flat file. For example the Parameter data model may be based on a file interface and include two datastores SRC SALES PERSON and SRC AGE GROUP.

Sales administration application is representative of an application for tracking sales. The sales administration application may be a data warehouse populated with transformations of data from orders application and parameter file . A Sales Administration data model is created to represent data stored in sales administration application as well as any data integrity controls or conditions or transformations. For example the Sales Administration data model may be based on a Hyper Structured Query Language Database HSQLDB mapping and include six datastores TRG CITY TRG COUNTRY TRG CUSTOMER TRG PRODUCT TRG PROD FAMILY TRG REGION and TRG SALE.

In accordance with an embodiment the present invention leverages the user s existing infrastructure by enabling the user to customize a data integration process according to the user s particular needs. For example when a data integration plan is designed it can be divided into discrete portions which are executable by a single system referred to as execution units. Once a data integration plan has been divided into a plurality of execution units the user can be presented with a physical plan based on the user s infrastructure and system resources. This plan can be further customized by the user to change which user systems execute which execution units. For example a user may be presented with a plan in which a join operation is executed on a first database and the user may customize the plan by moving the join operation to a second database.

As shown in this results in an extract load transform E LT architecture that does not rely on a stand alone transform server which characterized prior ETL systems. Instead as described above data transforms can be performed on the user s existing infrastructure. The E LT architecture provides users with greater flexibility while reducing costs associated with acquiring and maintaining proprietary transform servers.

Referring again to agents can be used to schedule and coordinate a set of integration tasks associated with an integration process. For example at runtime an agent coordinates the execution of integration processes. The agent may retrieve code stored in master repository connect to the various source and target systems and orchestrates an overall data integration process or scenario. In various embodiments there are two types of agents. In one example a standalone agent is installed on desktop such as agent . In another example an application server agent can be deployed on application server such as a Java EE Agent deployed on an Oracle WebLogic Server and can benefit from the application server layer features such as clustering for High Availability requirements. In yet another example an agent can be deployed on sources and targets such as agent .

In this embodiment data integration system includes application server that may include one or more of the above discussed agents. Application server is representative of one or more application servers web servers or hosted applications. In this example application server includes FMW console servlet container web services container and data sources connection pool .

FMW console is representative of one or more hardware and or software elements configured to manage aspects of application server such as information related to servlet container web services container and data sources connection pool . For example FMW console may be a browser based graphical user interface used to manage an Oracle WebLogic Server domain. FMW console may include functionality to configure start and stop WebLogic Server instances configure WebLogic Server clusters configure WebLogic Server services such as database connectivity JDBC and messaging JMS configure security parameters including creating and managing users groups and roles configure and deploy Java EE applications monitor server and application performance view server and domain log files view application deployment descriptors and edit selected run time application deployment descriptor elements. In some embodiments FMW console includes ODI plug in providing FMW console with access to data integration processes in production and may show execution logs with error counts the number of rows processed execution statistics the actual code that is executed and so forth.

Servlet container is representative of one or more hardware and or software elements configured to extend the capabilities of application server . Servlets are most often used to process or store data that was submitted from an HTML form provide dynamic content such as the results of a database query and manage state information that does not exist in the stateless HTTP protocol such as filling the articles into the shopping cart of the appropriate customer. A servlet is typically a Java class in Java EE that conforms to the Java Servlet API a protocol by which a Java class may respond to requests. To deploy and run a servlet servlet container is used as a component of a web server that interacts with servlets. Accordingly servlet container may extend functionality provided by public web service and data services of web services container as well as access to data pools provided by data sources connection pool . Servlet container is also responsible for managing the lifecycle of servlets mapping a URL to a particular servlet and ensuring that the URL requester has the correct access rights.

In this example servlet container includes Java EE application associated with ODI SDK ODI console and runtime web service associated with Java EE agent . ODI SDK provides a software development kit SDK for data integration and ETL design. ODI SDK enables automation of work that is common and very repetitive allowing a user to script repetitive tasks.

ODI console is a Java Enterprise Edition Java EE application that provides Web access to repositories . ODI console is configured to allow users to browse Design Time objects including projects models and execution logs. ODI console may allow users to view flow maps trace the source of all data and even drill down to the field level to understand the transformations used to build the data. In addition end users can launch and monitor scenario s execution through ODI console . In one aspect ODI console provides administrators with the ability to view and edit Topology objects such as Data Servers Physical and Logical Schemas as well as to manage repositories .

As discussed above a scenario is designed to put a source component mapping package procedure variable into production. A scenario results from the generation of code SQL shell and so forth for this component. A scenario can be exported and then imported into different production environments.

In various embodiments a user may initiate a session with designer module of ODI Studio and connect to repositories . The user may interact with one or more user interface features to create a new data integration project or select from existing data integration projects stored in for example master repository . In general designer module is used to manage metadata to design data integrity checks and to build transformations. In various embodiments the main objects handled through designer module are models and projects. Data models contain all of the metadata in a data source or target e.g. tables columns constraints descriptions cross references etc. . Projects contain all of the loading and transformation rules for a source or target e.g. mappings procedures variables etc. 

In step one or more data models are created. In step one or more projects are created. is a screenshot of user interface for creating a data integration scenario in accordance with an embodiment of the present invention. In this example navigation panel displays information and includes functionality for interacting with projects. Navigation panel displays information and includes functionality for interacting with data models. As discussed above the user may not only create the data model but also develop any data integrity checks for the data in the data models. Additionally the user may specify mappings procedures variables for projects that provide data integrity and transforms for the data in a flow that loads data from a source into a target. In step one or more data integration scenarios are generated. ends in step .

In step target datastore information is received. For example a user may interact with one or more user interface features of designer module to provide target datastore information. In one embodiment the user may drag and drop target datastore information comprising one or more data models from navigation panel onto a mapping or flow panel that visually represents aspects of a selected data model and any associated transforms or data integrity checks.

In step source datastore information is received. For example a user may interact with one or more user interface features of designer module to provide source datastore information. In one embodiment the user may drag and drop source datastore information comprising one or more data models from navigation panel onto the same mapping or flow panel of the target datastore information that visually represents aspects of a selected data model and any associated transforms or data integrity checks.

In various embodiments the source datastore information and the target data store information may be composed of one or more data models and optionally operations. Some examples of operations can include one or more data set operations e.g. unions joins intersections etc. data transformations data filter operations constraints descriptions cross references integrity checks or the like. In further embodiments some of these operations may be preconfigured and visually represented in designer module . In other embodiments custom operations may be provided allowing the user to specify logic mappings and the like that implement an operation.

In step mapping information is received. For example a user may interact with one or more user interface features of designer module to map the source datastore information to the target datastore information. In one embodiment the user may visually connect attributes of data elements in the source datastore information with attributes of data elements in the target datastore information. This may be done by matching column names of tables in the source datastore information and the target datastore information. In further embodiments one or more automatic mapping techniques may be used to provide mapping information.

Referring again to in step data loading strategies are received. A data loading strategy includes information on how the actual data from the source datastore information is to be loaded during an extract phase. Data loading strategies can be defined in a flow tab of designer . In some embodiments a data loading strategy can be automatically computed for a flow depending on a configuration of the mapping.

For example one or more knowledge modules may be proposed for the flow. A knowledge module KM is a component that implements reusable transformation and ELT extract load and transform strategies across different technologies. In one aspect knowledge modules KMs are code templates. Each KM can be dedicated to an individual task in an overall data integration process. The code in KMs appears in nearly the form that it will be executed with substitution methods enabling it to be used generically by many different integration jobs. The code that is generated and executed is derived from the declarative rules and metadata defined in the designer module . One example of this is extracting data through change data capture from Oracle Database 10g and loading the transformed data into a partitioned fact table in Oracle Database 11g or creating timestamp based extracts from a Microsoft SQL Server database and loading this data into a Teradata enterprise data warehouse.

The power of KMs lies in their reusability and flexibility for example a loading strategy can be developed for one fact table and then the loading strategy can be applied to all other fact tables. In one aspect all mappings that use a given KM inherit any changes made to the KM. In some embodiments five different types of KMs are provided each of them covering one phase in a transformation process from source to target such as an integration knowledge module IKM a loading knowledge module LKM and a check knowledge module CKM.

Referring to a user may define a way to retrieve the data from SRC AGE GROUP SRC SALES PERSON files and from the SRC CUSTOMER table in environment . To define a loading strategies a user may select a source set that corresponds to the loading of the SRC AGE GROUP file and select a LKM File to SQL to implement the flow from a file to SQL. In one aspect a LKM is in charge of loading source data from a remote server to a staging area.

In step data integration strategies are received. After defining the loading phase the user defines a strategy to adopt for the integration of the loaded data into a target. To define the integration strategies the user may select a target object and select a IKM SQL Incremental Update. An IKM is in charge of writing the final transformed data to a target. When an IKM is started it assumes that all loading phases for remote servers have already carried out their tasks such as having all remote source data sets loaded by LKMs into a staging area or the source datastores are on the same data server as the staging area.

In step data control strategies are received. In general an CKM is in charge of checking that records of a data set are consistent with defined constraints. An CKM may be used to maintain data integrity and participates in overall data quality initiative. A CKM can be used in 2 ways. First to check the consistency of existing data. This can be done on any datastore or within mappings. In this case the data checked is the data currently in the datastore. In a second case data in the target datastore is checked after it is loaded. In this case the CKM simulates the constraints of the target datastore on the resulting flow prior to writing to the target.

As discussed above automation of data integration flows can be achieved in data integration system by sequencing the execution of the different steps mappings procedures and so forth in a package and by producing a production scenario containing the ready to use code for each of these steps. A package is made up of a sequence of steps organized into an execution diagram. Packages are the main objects used to generate scenarios for production. A scenario is designed to put a source component mapping package procedure variable into production. A scenario results from the generation of code SQL shell and so forth for this component. A scenario can be exported and then imported into different production environments.

In step step information is received. Package step information includes information identifying a step elements properties components and the like. In one example a user may interact with one or more user interface features of designer module to create identify or otherwise specify one or more steps for a package. In one embodiment one or more components are selected and placed on a diagram. These components appear as steps in the package.

In step step sequence information is received. Package step sequence information includes information identifying an ordering for a step dependencies and the like. Once steps are created the steps are ordered or reorder into a data processing chain. In one example a user may interact with one or more user interface features of designer module to provide sequencing or ordering for one or more steps of a package. A data processing chain may include a unique step defined as a first step. Generally each step has one or more termination states such as success or failure. A step in some states such as failure or success can be followed by another step or by the end of the package. In one aspect in case of some states such as failure sequence information may define a number of retries. In another aspect a package may have but several possible termination steps.

As discussed above the automation of data integration flows can be achieved by sequencing the execution of different steps mappings procedures and so forth in a package. The package can then be produced for a production scenario containing the ready to use code for each of the package s steps. In various embodiments the package is deployed to run automatically in a production environment.

In step an integration scenario is retrieved. In one embodiment a package is retrieved from repositories . In step the integration scenario is deployed to one or more agents. In step the integration scenario is executed by the one or more agents. In one aspect the integration scenario can be executed in several ways such as from ODI Studio from a command line or from a web service. Scenario execution can be viewed and monitored for example via operator module and the like as discussed above. ends in step .

In most data integration systems a mapping requires an explicit definition of all input and output attributes that form part of a map. In typical flow based ETL tools connectors are made at the attribute level. This results in a very concise mapping model. However this also generates a huge number of objects and makes constructing and maintaining maps cumbersome due to the number of attribute level connectors.

In various embodiments data integration system incorporates one or more techniques for easing the design and maintenance of a mapping. Components can simply be added to an existing design without the need to specify all input and output attributes and allowing component level connectors to be rerouted. In one aspect components are implemented that allow assignment expressions to reference all or part of upstream components. Accordingly attributes of certain types of components can be propagated to downstream components or otherwise inherited from upstream components with minimal effort on the part of a mapping designer.

In one aspect data integration system minimizes the need to manage attribute level connectivity. For example data integration system may classify components of a mapping such as Joiner Set Table Filter etc. . Some examples of categories are projectors and selectors. In general projectors are components that influence the shape of the data that flows through a map. Selectors are components that control the flow of the data but don t fundamentally change the shape of the flow. In various embodiments selector type components are configured to be transparent in that attributes of upstream components are visible. Projector type components are configured to be opaque that attributes of upstream components are not visible only those made available in the shape of the data flow.

Accordingly requiring each attribute to be connected to a downstream component s attributes in order to be able to see the data from upstream in various embodiments data integration system enables users to directly reference any upstream attribute up to and including the closest projector s attributes. Therefore data integration system greatly eases the design and maintenance of a mapping. Data integration system further makes adding in components to an existing design simple typically just needing component level connectors to be rerouted.

In traditional data integration systems mapping requires an explicit definition of all input and output attributes that form part of component representing the JOIN. In contrast in various embodiments a mapping developer can define how columns of data target TGT EMPDEPT are populated directly from attributes of data source SRC EMP represented by component and attributes of data source SRC DEPT represented by component that flow through component and are thus visible to component . For example the assignment of the target column TGT EMPDEPT.NAME can reference SRC EMP.ENAME without a need to reference to an attribute of component .

In step a component type for a component is determined. As discussed above some types of components influence the shape of the data that flows through a mapping while other types of components control the flow of the data but do not fundamentally change the shape of the flow. In step attributes for downstream components are determined based on the component type. For example if a component controls the flow of the data but does not fundamentally change the shape of the flow data integration system may derive a set of attributes based on attributes of upstream components. In step the derived components are exposed to downstream components. ends in step . Accordingly an assignment of target column TGT EMPDEPT.NAME can reference SRC EMP.ENAME transparently without a need to reference to an attribute of component .

Data integration system further makes adding in components to an existing design simple typically just needing component level connectors to be rerouted. For example if a filter component were added into a design changing component level connectors would not require changes to attribute assignments of certain downstream components.

In step a component definition is received. For example a component definition may include rules operations procedures variables sequences and the like. In step a component type is received. For example if a component changes the shape of data in a flow the component may be classified in one manner. If the component controls the flow of the data but does not fundamentally change the shape of the flow the component may be classified in another manner. In step the component is generated and may be used by data integration system . ends in step .

Accordingly data integration system enables users to create a logical design which is platform and technology independent. The user can create a logical design that defines at a high level how a user wants data to flow between sources and targets. The tool can analyze the logical design in view of the user s infrastructure and create a physical design. The logical design can include a plurality of components corresponding to each source and target in the design as well as operations such as joins or filters and access points. Each component when transferred to the physical design generates code to perform operations on the data. Depending on the underlying technology e.g. SQL Server Oracle Hadoop etc. and the language used SQL pig etc. the code generated by each component may be different.

Thus a user of data integration system is not required to specify all data attributes at each component in the logical design from start to end. Data integration system provides a plurality of component types such as projector and selector types that avoid the need to fully declare the information that flows through the logical design. Data integration system is able to decide what attributes are needed at operations represented by predetermined component types. This simplifies both the design and maintenance.

Bus subsystem provides a mechanism for letting the various components and subsystems of computer system communicate with each other as intended. Although bus subsystem is shown schematically as a single bus alternative embodiments of the bus subsystem may utilize multiple busses.

Storage subsystem may be configured to store the basic programming and data constructs that provide the functionality of the present invention. Software code modules or instructions that provides the functionality of the present invention may be stored in storage subsystem . These software modules or instructions may be executed by processor s . Storage subsystem may also provide a repository for storing data used in accordance with the present invention. Storage subsystem may comprise memory subsystem and file disk storage subsystem .

Memory subsystem may include a number of memories including a main random access memory RAM for storage of instructions and data during program execution and a read only memory ROM in which fixed instructions are stored. File storage subsystem provides persistent non volatile storage for program and data files and may include a hard disk drive a floppy disk drive along with associated removable media a Compact Disk Read Only Memory CD ROM drive a DVD an optical drive removable media cartridges and other like storage media.

Input devices may include a keyboard pointing devices such as a mouse trackball touchpad or graphics tablet a scanner a barcode scanner a touchscreen incorporated into the display audio input devices such as voice recognition systems microphones and other types of input devices. In general use of the term input device is intended to include all possible types of devices and mechanisms for inputting information to computer system .

Output devices may include a display subsystem a printer a fax machine or non visual displays such as audio output devices etc. The display subsystem may be a cathode ray tube CRT a flat panel device such as a liquid crystal display LCD or a projection device. In general use of the term output device is intended to include all possible types of devices and mechanisms for outputting information from computer system .

Network interface subsystem provides an interface to other computer systems devices and networks such as communications network . Network interface subsystem serves as an interface for receiving data from and transmitting data to other systems from computer system . Some examples of communications network are private networks public networks leased lines the Internet Ethernet networks token ring networks fiber optic networks and the like.

Computer system can be of various types including a personal computer a portable computer a workstation a network computer a mainframe a kiosk or any other data processing system. Due to the ever changing nature of computers and networks the description of computer system depicted in is intended only as a specific example for purposes of illustrating the preferred embodiment of the computer system. Many other configurations having more or fewer components than the system depicted in are possible.

Although specific embodiments of the invention have been described various modifications alterations alternative constructions and equivalents are also encompassed within the scope of the invention. The described invention is not restricted to operation within certain specific data processing environments but is free to operate within a plurality of data processing environments. Additionally although the present invention has been described using a particular series of transactions and steps it should be apparent to those skilled in the art that the scope of the present invention is not limited to the described series of transactions and steps.

Further while the present invention has been described using a particular combination of hardware and software it should be recognized that other combinations of hardware and software are also within the scope of the present invention. The present invention may be implemented only in hardware or only in software or using combinations thereof.

The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. It will however be evident that additions subtractions deletions and other modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims.

Various embodiments of any of one or more inventions whose teachings may be presented within this disclosure can be implemented in the form of logic in software firmware hardware or a combination thereof. The logic may be stored in or on a machine accessible memory a machine readable article a tangible computer readable medium a computer readable storage medium or other computer machine readable media as a set of instructions adapted to direct a central processing unit CPU or processor of a logic machine to perform a set of steps that may be disclosed in various embodiments of an invention presented within this disclosure. The logic may form part of a software program or computer program product as code modules become operational with a processor of a computer system or an information processing device when executed to perform a method or process in various embodiments of an invention presented within this disclosure. Based on this disclosure and the teachings provided herein a person of ordinary skill in the art will appreciate other ways variations modifications alternatives and or methods for implementing in software firmware hardware or combinations thereof any of the disclosed operations or functionalities of various embodiments of one or more of the presented inventions.

The disclosed examples implementations and various embodiments of any one of those inventions whose teachings may be presented within this disclosure are merely illustrative to convey with reasonable clarity to those skilled in the art the teachings of this disclosure. As these implementations and embodiments may be described with reference to exemplary illustrations or specific figures various modifications or adaptations of the methods and or specific structures described can become apparent to those skilled in the art. All such modifications adaptations or variations that rely upon this disclosure and these teachings found herein and through which the teachings have advanced the art are to be considered within the scope of the one or more inventions whose teachings may be presented within this disclosure. Hence the present descriptions and drawings should not be considered in a limiting sense as it is understood that an invention presented within a disclosure is in no way limited to those embodiments specifically illustrated.

Accordingly the above description and any accompanying drawings illustrations and figures are intended to be illustrative but not restrictive. The scope of any invention presented within this disclosure should therefore be determined not with simple reference to the above description and those embodiments shown in the figures but instead should be determined with reference to the pending claims along with their full scope or equivalents.

