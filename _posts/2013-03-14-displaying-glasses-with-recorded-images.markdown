---

title: Displaying glasses with recorded images
abstract: Processing a set of images is disclosed, including: receiving a set of images; and searching for a representation of a user's face associated with the set of images and a plurality of sets of extrinsic information corresponding to respective ones of at least a subset of the set of images. Rendering a glasses frame is disclosed, including: receiving a selection associated with the glasses frame; rendering the glasses frame using at least a representation of a user's face and a set of extrinsic information corresponding to an image in a recorded set of images; and overlaying the rendered glasses frame on the image.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08708494&OS=08708494&RS=08708494
owner: Ditto Technologies, Inc.
number: 08708494
owner_city: San Francisco
owner_country: US
publication_date: 20130314
---
This application is a continuation in part of co pending U.S. patent application Ser. No. 13 361 835 entitled FITTING GLASSES FRAMES TO A USER filed Jan. 30 2012 which is incorporated herein by reference for all purposes.

A person seeking to buy glasses usually has to go in person to an optometrist or an eyewear store and try on several glasses frames to see if they fit them. Typically this requires a few hours of browsing through several rows of glasses frames and trying on many pairs of glasses frames most of the time without prior knowledge of whether a particular glasses frame fits or not. Although glasses frames are designed to fit most people not all heads are the same size and therefore not all glasses will fit a person. Additionally glasses frames not only have the functional purpose of correcting the wearer s vision but also an aesthetic purpose which adds other factors to the selection process. What is needed is a way to fit glasses frames to people more efficiently.

The invention can be implemented in numerous ways including as a process an apparatus a system a composition of matter a computer program product embodied on a computer readable storage medium and or a processor such as a processor configured to execute instructions stored on and or provided by a memory coupled to the processor. In this specification these implementations or any other form that the invention may take may be referred to as techniques. In general the order of the steps of disclosed processes may be altered within the scope of the invention. Unless stated otherwise a component such as a processor or a memory described as being configured to perform a task may be implemented as a general component that is temporarily configured to perform the task at a given time or a specific component that is manufactured to perform the task. As used herein the term processor refers to one or more devices circuits and or processing cores configured to process data such as computer program instructions.

A detailed description of one or more embodiments of the invention is provided below along with accompanying figures that illustrate the principles of the invention. The invention is described in connection with such embodiments but the invention is not limited to any embodiment. The scope of the invention is limited only by the claims and the invention encompasses numerous alternatives modifications and equivalents. Numerous specific details are set forth in the following description in order to provide a thorough understanding of the invention. These details are provided for the purpose of example and the invention may be practiced according to the claims without some or all of these specific details. For the purpose of clarity technical material that is known in the technical fields related to the invention has not been described in detail so that the invention is not unnecessarily obscured.

The 3D model generator determines the user s head measurements and a three dimensional 3D model from the video images received of the user s head at different angles. The 3D model may include one or more of the following images video frames of the user s head reference points on the user s head user head measurements and a set of rotation translation matrices. In some embodiments the 3D model contains only reference points associated with the user s head. In some embodiments an initial 3D model is first obtained from a subset of the received video frames images. Then the initial 3D model can be adjusted into an adjusted 3D model using an iterative algorithm incorporating additional information from the received video frames images.

In some embodiments the images are captured at the client computer and sent over the internet to a server to process the images and create a 3D model of the user s head. In some embodiments the client computer can create the 3D model of the user s head and send the 3D model over the internet to select glasses that fit the user. Other embodiments with different tasks being executed by different processors or different locations are also included in the scope of this invention. In some embodiments a scaling reference is captured with the user s head when obtaining video and or images at step of . For example the user is instructed to hold a scaling reference to the user s head and an image is captured of the user with the scaling reference in order to properly scale the user s head measurements and fit glasses frames to the user. In some embodiments the scaling reference is a measurement in standard units of something in the video frame image for example a pupillary distance is entered by the user. In some embodiments the camera is calibrated using the video images captured of the user with or without the scaling reference.

Comparison engine compares the user head measurements from the 3D model to a database of glasses frame information . A penalty function for each of the measurements that factors in fit is used to compare the glasses frame and the user s head measurements. A detailed description of the penalty function is disclosed later in the specification. One or more glasses frames are selected based on a score computed from the penalty function and set thresholds of the score that comprise different levels of fit.

In some embodiments the outputting of glasses frame information at step of comprises displaying information associated with one or more glasses frames determined to match a user. In some embodiments after the user s head measurements are determined the selected glasses that fit the user are displayed to the user on display . In some embodiments the glasses frame information includes a fit score. In some embodiments the one or more glasses frames determined to match a user are displayed in a list or ordered list associated with the fit score and or one or more other attributes associated the user and or the glasses frames in the list. In some embodiments the at least one of the selected glasses is rendered on an image associated with the user. In some embodiments at least one of the selected glasses is rendered on a 3D interactive model of the user. This allows the user to visualize how the user looks with a selected pair of glasses. In some embodiments this allows the user to more easily decide to purchase a pair of glasses without having to visit a physical store.

The measurements of the glasses frames comprise a portion of the glasses frame information stored in a database. In some embodiments glasses frames are measured and the measurements are stored in a database. In some embodiments a 3D model of each glasses frame is stored in the database. In some embodiments the glasses frames are scanned with a 3D imager and are stored in the database. In some embodiments other glasses frame information is stored in the database including one or more of the following glasses frame measurements identifier name picture manufacturer model number description category type glasses frame material brand part number and price.

Additionally the short video or the image of the user with a scaling reference object is used at least in part to calibrate the camera. By detecting points on the scaling reference object with a known measurement and image characteristics a matrix representing camera intrinsic parameters is determined and used to calibrate the camera. The camera intrinsic parameters are found by correlating points on the scaling reference object between different video frames images of the user with the scaling reference object and calculating the matrix that represents the camera intrinsic parameters using a camera calibration algorithm. In some embodiments instead of using calibration to a scaling reference object auto calibration or self calibration e.g. to recorded images which does not require a scaling reference is used.

Reference points are obtained from images video frames of the different orientations. In some embodiments a best image video frame for each of the orientations is selected to be used in determining reference points and user head measurements that can be at least in part used to generate the initial 3D model as shown in . For example the best image video frame out of the front orientation is the image video frame in the front facing orientation with the maximum distance between the endpoints of the eyes. For example the image video frame with the largest distance between the right eye and the right ear is determined as the best right profile. Other embodiments for the selection of the best image video frames include machine learning or various heuristics for certain models that best fit a certain orientation.

In some embodiments the initial 3D model comprises an average of reference points in 3D space from a set of users. Using a set of users instead of just the user s face a generic face is used as the initial 3D model. Instead of generating the initial 3D model from the reference points of the user s head the generic 3D model is used as a starting point and then adjusted. In some embodiments the generic 3D model does not include the user s reference points but is from previous users of the system and or a predetermined generic model. In some embodiments the reference points from the user s head are averaged with other users to at least in part generate an initial 3D model.

In some embodiments the penalty functions have a priority. For example the bridge length penalty may have a higher weight than the temple distance penalty or the lens distance penalty. In some embodiments the bridge distance penalty is greater than the temple distance penalty which is greater than the lens distance penalty.

In another example the frame width penalty may be greater than the bridge distance penalty which may be greater than the temple distance penalty which may be greater than the lens distance penalty. In some embodiments the scale of the y axis of the penalty function determines the weight of the penalty. In some embodiments the penalty functions can be normalized and then multiplied with a constant representing their relative weights. Other combinations of penalty function shapes and scaling can also be used to relatively weight the penalty functions.

For each glasses frame compared to the user s head measurements a score is then calculated using the penalty functions using a linear combination of weighted penalties. For example when evaluating one glasses frame with a 15 mm bridge length if the user s bridge distance is 27 mm then from the penalty function the penalty is high for example 8.5 because the bridge length is much less than the user s bridge distance. The bridge length penalty is then multiplied with its relative weight which is 1 for the bridge length. In this example the temple distance has a relative weight constant that is 0.75 and the lens diameter has a relative weight of 0.5. Likewise the temple distance and lens diameter of the glasses frame is evaluated to the user head measurements using a penalty function. Then each of the weighted penalties are added together to create a score. In some embodiments other distances are factored into the score e.g. lens height compared to eyebrows and cheekbone positions. In some embodiments other factors like feedback mechanisms and user reviews are also factored into the score of the glasses frame to the user. In some embodiments other preferences of the user that are designated by the user e.g. glasses frame material preferred or glasses frame weight are factored into the score and make a qualitative fit score for the user.

The scores for the glasses frames are then sorted or ranked in ascending or descending order. For example the penalties are sorted in ascending order and the glasses frame with the least penalty corresponds to the best fitting. In some embodiments a threshold is set of a score that indicates that the glasses frame does not fit the user. In some embodiments other thresholds are also set for example a threshold for glasses frames that ideally fit the user or a threshold for glasses that have a good fit. The thresholds can be set as a numerical constant or as a relative threshold for example taking the top 25 of results of glasses that represent a good fit. The scores can also be scaled to be more user intuitive like a scale of 1 to 10 to indicate a fit score. Using the scores and thresholds a set of glasses frames are selected into a results list. In some embodiments the results list comprises all glasses in the database that fit the user i.e. all glasses above the does not fit threshold . In some embodiments only the glasses frames that ideally fit the user are selected for the results list. In some embodiments all of the glasses frames are selected and are associated with a fit score for the user.

Embodiments of displaying glasses with recorded images are described. In various embodiments a recorded set of images of various orientations of a user s face is received. In various embodiments a user s face includes the user s head as well. For example the set of images may be recorded by a user turning his or her head in various directions at a camera. An optimal representation of the user s face associated with the set of images in three dimensional 3D space is searched for among a distribution of possible representations. Also an optimal set of extrinsic information e.g. matrices of rotation and translation in 3D space associated with each image of the set of images among is searched for a distribution of possible sets extrinsic information for that image such that the selected set of extrinsic information can be used to transform orient translate the representation of the user s face to best match the orientation of the user s face in that particular image. In some embodiments at least a portion of the determined representation of the user s face for the set of images and the sets of extrinsic information corresponding to respective ones of the set of images are used to model the user s face in determining renderings of a glasses frame to be overlaid on the recorded set of images in a playback of the set of images to the user.

In various embodiments a selected glasses frame is received. The selected glasses is rendered using a representation of a user s face and a set of extrinsic information associated with the image in a recorded set of images for which the glasses is to be rendered. The combination of information associated with the selected glasses frame and the representation of a user s face is transformed oriented translated using the set of extrinsic information to yield the rendering of a user s face at an orientation associated with the image corresponding to the set of extrinsic information. In some embodiments occlusion culling of the portions of the glasses frame that are hidden in the orientation associated with the image associated with the set of extrinsic information is also performed during the rendering. The rendered glasses frame is overlaid on the image corresponding to the set of extrinsic information to result in the appearance of the glasses frame at the same orientation as the user s face in that image and at an appropriate location in the image. By repeating the above process for rendering the glasses frame for various images of the recorded set of images at least a portion of the set of images with their respective overlaid glasses frame renderings may be played back at a display for a user. In some embodiments the playback of the images with overlaid glasses frame renderings may be controlled by user interaction e.g. with an interface of the device on which the set of images is played back .

Client device is configured to record or receive a set of recorded images corresponding to a user s head at various orientations. Examples of client device may include a laptop computer a desktop computer a tablet device a mobile device a smart phone and or any computing device. For example the set of recorded images may comprise a video or a series of snapshots. In some embodiments client device includes or is connected to a camera device. The camera device and or a processor of client device that is running an application can capture a set of images of the user s head as user turns his or her head in different directions e.g. as instructed through a user interface of the application . In various embodiments the set of images is sent to server for server to process. In some embodiments client device includes a user interface through which the user may interact and view a playback associated with the images.

In various embodiments server is configured to receive a set of images sent from a client device such as client device . Server searches for an optimal representation e.g. a mathematical 3D model of the user s e.g. user face associated with a set of images and also optimal sets of extrinsic information corresponding to respective images of the set e.g. a set of extrinsic information is specifically determined for each image of the set .

In various embodiments server is configured to receive a selection associated with a glasses frame made by a user e.g. user at an interface associated with client device . In some embodiments the selected glasses frame is selected from a list of glasses frames that are presented with respective fit scores that are determined by server . In various embodiments server is configured to render the selected glasses frame for at least a subset of a recorded set of images associated with different orientations of a user s face. Server is configured to determine the appropriate orientation of the glasses frame to match the orientation of the user s face in an image based on a set of extrinsic information determined for that image. Server is configured to render the glasses frame with appropriate occluded portions if any for the image based on the set of extrinsic information and a model associated with the user s face used for occlusion. In some embodiments server is configured to create a two dimensional 2D rendering of the glasses frame for the image and to overlay the rendering of the glasses frame over the image. In some embodiments server is configured to send the at least subset of the set of images and the corresponding rendered glasses frames to client device . The client device is configured to present the at least a subset of the set of images and corresponding rendered glasses frames at a user interface such that a user such as user can see a playback of the video that he or she had created with the selected glasses frame overlaid in each of at least a subset of the set of images to experience a virtual try on of the selected glasses frame via the created video series of snapshots. In some embodiments the renderings of the glasses frame are presented to be overlaid the recorded set of images instead of a model of the user s face that is derived from the set of images. In some embodiments client device is configured to receive user input e.g. a finger movement tracked on a touch screen or a mouse movement and to update the playback of the video with the overlaid renderings of the glasses frame in response to the received user input. For example a user may be able to control the playback of the video such that the user can indicate which orientation of his or her face with the rendered glasses frame he or she wishes to view and when.

In some embodiments processing of a recorded set of images and or rendering of a selected glasses frame for one or more images in the set of images described above may also be performed at least in part locally at client device . For example server can send computer code to client device that client device can use to perform at least a portion of the processing and or rendering locally.

Images database is configured to store sets of images. In some embodiments each set of images is associated with a video or series of snapshots of various orientations of a user s face. In some embodiments each set of images is stored with data associated with the whole set or individual images of the set.

Model generator is configured to determine a mathematical 3D model for a user s face associated with each set of images. For example the mathematical 3D model of the user s face i.e. the mathematical model of the user s face in 3D space may be set at the origin. In some embodiments the mathematical 3D model determined for a user s face is referred to as a M matrix. In some embodiments the M matrix may be determined based on a set of reference points associated with features on the user s face from the associated set of images. Examples of reference points include endpoints of the user s eye bridge of the user s nose and tip of the user s nose. In some embodiments model generator is configured to store the M matrix determined for a set of images with the set at images database .

Extrinsic information generator is configured to determine a set of extrinsic information for each of at least a subset of a set of images. For example the set of images may be stored at images database . In various embodiments a set of extrinsic information corresponding to an image of a set of images describes one of more of the orientation and translation of the 3D model determined for the set of images needed to result in correct appearance of the user s face in that particular image. In some embodiments the set of extrinsic information determined for an image of a set of images associated with a user s face is referred to an R t pair where R is a rotation matrix and t is a translation vector corresponding to that image. As such the R t pair corresponding to an image of a set of images can transform the M matrix corresponding to that set of images R M t into the appropriate orientation and translation of the user s face that is shown in the image associated with that R t pair. In some embodiments extrinsic information generator is configured to store the R t pair determined for each of at least a subset of a set of images with the set at images database .

Intrinsic information generator is configured to generate a set of intrinsic information for a camera associated with recording a set of images. For example the camera was used to record a set of images stored at images database . In various embodiments a set of intrinsic information corresponding to a camera describes a set of parameters associated with the camera. For example a parameter associated with a camera comprises a focal length. In some embodiments the set of intrinsic information associated with a camera are found by correlating points on a scaling reference object between different images of the user with the scaling reference object in the images and calculating the set of intrinsic information that represents the camera s intrinsic parameters using a camera calibration technique. In some embodiments the set of intrinsic information associated with a camera is found by using a technique of auto calibration which does not require a scaling reference. In some embodiments the set of intrinsic information associated with a camera is referred to as an I matrix. In some embodiments the I matrix projects a version of a 3D model of a user s face transformed by an R t pair corresponding to a particular image onto the 2D surface of the focal plane of the camera. In other words I R M t results in the projection of the 3D model the M matrix in the orientation and translation transformed by the R t pair corresponding to an image onto a 2D surface. The projection onto the 2D surface is the view of the user s face as seen from the camera. In some embodiments intrinsic information generator is configured to store an I matrix determined for the camera associated with a set of images with the set at images database .

Glasses frame information database is configured to store information associated with various glasses frames. For example information associated with a glasses frame may include measurements of various areas of the frame e.g. bridge length lens diameter temple distance renderings of the glasses frame corresponding to various R t pairs a mathematical representation of a 3D model of the glasses frame that can be used to render glasses image for various R t parameters a price an identifier a model number a description a category a type a glasses frame material a brand and a part number.

Rendering engine is configured to render a glasses frame to be overlaid on an image. For example the selected glasses frame may be a glasses frame for which information is stored at glasses frame information database . For example the image over which the glasses frame is to be overlaid may be stored as part of a set of images stored at images database . In some embodiments rendering engine is configured to render a glasses frame e.g. selected by a user for each of at least a subset of a set of images. In some embodiments rendering engine is configured to transform the glasses frame by the R t pair corresponding to an image. In some embodiments rendering engine is also configured to perform occlusion on the transformed glasses frame using an occlusion body comprising a 3D model of a user s face. The occluded glasses frame at the orientation and translation associated with the R t pair excludes certain portions hidden from view by the occlusion body at that orientation translation. For example the occlusion body may comprise a generic face 3D model or the M matrix associated with the set of images associated with the image. The rendered glasses frame for an image should show the glasses frame at the orientation and translation corresponding to the image and can be overlaid that image in a playback of the set of images to the user at a client device.

At a recorded set of images is received. In various embodiments the set of images corresponds to a recorded video or a series of snapshots of a user s face turned in different orientations. As such each image of the set is associated with an orientation of the user s face in that image.

At a representation of a user s face associated with the set of images and a plurality of sets of extrinsic information corresponding to respective ones of at least a subset of the set of images are searched for. In some embodiments the representation of the user s face is a model in 3D space and is referred to as a M matrix. In some embodiments the M matrix is determined based at least in part on measured reference points of one or more features associated with the user s face. In some embodiments the extrinsic set of information associated with an image of the set of images is referred to as an R t pair. An R t pair is determined for each of at least a subset of the set of images so each image corresponds to a respective R t pair that is associated with the orientation of the user s face in that image.

In some embodiments an optimal M matrix is determined for the set of images and an optimal R t pair is determined for each of at least a subset of the set of images. In a first example a parameter search is used to perform iterative computations until the optimal M and set of R t pairs are found. For example a distribution of M matrices e.g. that have been predetermined based on known face samples or generated on the fly corresponding to the set of images and a distribution of R t pairs corresponding to each image of the set of images are determined and a combination of matrix M and R t pairs that best describes at least a subset of the set of images is selected. In another example a bundle adjust technique is used and the bundle adjust technique may treat the M and the set of R t pairs as unknowns in an optimization problem and iteratively test out various combinations of M matrices and R t pairs until a M and a set of R t pairs are found that best match the set of images. For example the optimal M matrix and an optimal R t pair corresponding to an image result in the minimum reprojection error of any other combination of a M matrix and an R t pair and therefore the combination of this M matrix and this R t pair best matches the image corresponding to the R t pair. While one M matrix is determined for the set of images a set of R t pairs each corresponding to respective ones of at least a subset of the set of images is determined.

In some embodiments a set of intrinsic information associated with the camera associated with the set of images is also determined via iterative computations. In some embodiments the set of intrinsic information associated with the camera is predetermined.

At a selected glasses frame is received. In some embodiments the glasses frame is selected by a user from among a list of glasses frames presented to the user. For example the glasses frames on the list are selected by the server to comprise relatively good fits to the facial features of the user. The user may select a glasses frame that he or she would like to virtually try on.

At the glasses frame is rendered using at least a representation of a user s face and a set of extrinsic information corresponding to an image in a recorded set of images. In some embodiments a 3D model of the selected glasses frame is obtained. In some embodiments the 3D model of the selected glasses frame is referred to as a G matrix. In some embodiments a representation of a user s face comprises a 3D model of a user s face. For example the representation of the user s face may comprise a M matrix derived for the set of images using a process such as process of . In another example the representation of the user s face may comprise a model of a generic face referred to as the O matrix that is not specific to the user s face in the set of images. In some embodiments the set of extrinsic information associated with the image for which the glasses frame is to be rendered can be represented by the R t pair determined for that image. In rendering the glasses frame to match the user s face in the appropriate orientation and translation of the image the model the G matrix associated with the glasses frame is transformed by the R t pair corresponding to the image such that R G t the transformed glasses frame model reflects the orientation of the glasses frame in which it would appear on the user s face in that image. In some embodiments the model associated with the glasses frame the G matrix is placed onto the representation of a user s face and the combination of the representation of the user s face either the O matrix or the M matrix for example and the model associated with the glasses frame the G matrix are transformed by the R t pair corresponding to the image. Then occlusion culling is performed on the transformed glasses frame during the rendering of the glasses frame for that image such that the portions of the glasses frame that are hidden occluded by the user s face and or other head related features e.g. the user s hair are not included in the rendering of the glasses frame. Because the representation of the user s face either the O matrix or the M matrix for example is used here to determine occlusions for the glasses the model is sometimes referred to as an occlusion body. Furthermore the transformed and or occluded glasses frame may be projected onto the 2D surface of the camera associated with the set of images using a matrix I associated with the camera associated with the set of images.

At the rendered glasses frame is overlaid on the image. In some embodiments the rendering of the glasses frame for the image comprises a 2D image. In various embodiments the rendered glasses frame is overlaid on the original image during a playback to the user of at least a subset of the set of images. If and were repeated to determine rendered glasses frame for each of at least a subset of the set of images then the at least subset can be played back to the user with the corresponding rendered glasses frames e.g. at the corresponding orientations to create an experience for the user of virtually trying on the selected glasses frame by simply watching the playback. The playback of the originally recorded video or series of snapshots with the rendered glasses will show the user the same video that was recorded with the addition of an overlay of a rendered glasses frame corresponding to each image of the set of images. As a result in the playback of the video the renderings of the glasses frame appear to track the user s face in corresponding images. The user may even interact control the playback using a input e.g. mouse movement and or finger movement on a touch screen to adjust the playback of the video to view different angles of the user s face with the rendered glasses frame as the user would move his or her head around during a physical try on of the selected glasses frame in a store.

Although the foregoing embodiments have been described in some detail for purposes of clarity of understanding the invention is not limited to the details provided. There are many alternative ways of implementing the invention. The disclosed embodiments are illustrative and not restrictive.

