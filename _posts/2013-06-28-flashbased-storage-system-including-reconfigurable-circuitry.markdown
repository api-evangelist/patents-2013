---

title: Flash-based storage system including reconfigurable circuitry
abstract: Apparatus and method for accelerating processing operations of flash based storage systems are disclosed herein. In some embodiments, an IC component disposed between I/O circuitry and flash storage devices is configured to optimize fulfillment of data read and write requests originating from a network or device external to the flash based storage system using cache memory before involving the flash storage devices.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09286225&OS=09286225&RS=09286225
owner: Saratoga Speed, Inc.
number: 09286225
owner_city: San Jose
owner_country: US
publication_date: 20130628
---
This application is a continuation in part of U.S. patent application Ser. No. 13 844 663 filed Mar. 15 2013 the content of which is incorporated herein by reference in its entirety.

The present disclosure relates generally to networked storage systems and more particularly in some embodiments to subsystems for facilitating data storage and access in flash based networked storage systems.

The large amounts of information generated daily challenge data handling facilities as never before. In the context of today s information generation data is being generated at rates perhaps thousands or tens of thousands of times greater than was the data generation rate in the 1990s. Historically large volumes of data sparked explosive growth in data communications. Responses to growing amounts of data generation centered on improving the movement of data based in increased transmission data rates to enhance throughput in communication channels. For instance transmission pipelines grew from a few tens of megabits per second Mb s transmission rates to several tens of gigabits per second Gb s rates during the 1990s.

In the same period typical storage devices such as hard disk drives HDDs when amassed in sufficient numbers might accommodate large volumes of data but the rates at which data may be stored and retrieved have not scaled at the same rate as the volume of data stored on the devices has increased. Data access rates for HDDs are at similar orders of magnitude today as they were in the 1990s.

Fundamental storage subsystems have not integrated technology to enable scaling of effective data storage at the same rate that data generation is growing. Hence the challenge to systems handling large volumes of data is not likely to be alleviated by the combination of contemporary HDD technology with high speed data transmission channels. In order to handle and manage big data information processing facilities will be pressured to utilize larger volumes of storage with higher performance rates for capturing and accessing data.

Described in detail herein is an apparatus method and article of manufacture for accelerating operations of a flash based storage system using an integrated circuit hardware component interposed between the I O circuits and flash memory of the flash based storage system. In some embodiments at least a portion of the integrated circuit hardware component comprises reconfigurable circuitry. As an example the integrated circuit hardware component comprises a FPGA. The integrated circuit hardware component includes one or more of a reconfigurable application engine a cache management subsystem a behavior control a RAID compute engine and a protocol offload engine.

The reconfigurable application engine comprises reconfigurable circuitry hardware configured to run an application algorithm or other processor intensive operations offloaded by a server CPU included in the flash based storage system. The offloaded processor intensive operation may pertain to operation of the flash based storage system itself or may be requested by an external network connected to the flash based storage system. The cache management subsystem includes a cache control cache modules and a tags and matching logic module the cache modules and tags and matching logic module coupled to and external to the integrated circuit hardware component . The cache control optimizes use of the cache modules to initially source and store data requested by I O requests originating from the external network rather than relying on the flash memory as the initial first source. The behavior control is configured to perform higher level monitoring of certain combination of behaviors attributes characteristics or events occurring in the flash based storage system. The RAID compute engine is configured to facilitate redundant grouping of the flash memory to improve chances of data recovery in failure scenarios. The protocol offload engine is configured to process read and write requests offloaded from the server CPU included in the flash based storage system. The cache modules and tags and matching logic module are used by the protocol offload engine to supply or add data identified in the offloaded read and write requests.

Various modifications to the example embodiments will be readily apparent to those skilled in the art and the generic principles defined herein may be applied to other embodiments and applications without departing from the scope of the invention. Moreover in the following description numerous details are set forth for the purpose of explanation. However one of ordinary skill in the art will realize that the invention may be practiced without the use of these specific details. In other instances well known structures and processes are not shown in block diagram form in order not to obscure the description of the invention with unnecessary detail. Thus the present disclosure is not intended to be limited to the embodiments shown but is to be accorded the widest scope consistent with the principles and features disclosed herein.

Much of the drawback of current storage subsystems has to do with limitations created by bottlenecks in data transport bandwidth inherent in the storage devices themselves. For example disk drives have significant limitations in latency seek times and data transfer rates which for the purposes of this application these quantities will be cumulatively referred to as the access time. The total amount of storage that may be provided by HDD may approximate the expectations of storage capacities required by big data users but the ability to provide high performance data throughput at those storage capacities is limited by the use of HDD technology and the inherent data access limitations.

Although HDD devices can store large quantities of data with high reliability typical HDD devices lack access speeds high enough to directly service the storage requirements of client devices having high data rate requirements. When high performance network data transmission technologies are coupled to a large capacity storage subsystems based on HDD memory there is a performance mismatch between the data rates sustainable by the network attached devices and the HDDs that caching alone does not remedy to fulfill the performance levels required by big data.

Redundant paths may be created by coupling crossover paths from point to point links between two fabric instances such that some or all of the first fabric instance may be coupled to some or all of a second fabric instance . In this way components at endpoints and or root complexes associated with the first fabric instance may conduct transactions with components associated with endpoints and or further root complexes associated with the second fabric instance even though certain point to point links endpoints and root complexes may become unavailable due to failure or transaction congestion.

In continuing accord with example embodiments the root complex can denote the root of an interconnected input output I O hierarchy that connects a central processing unit CPU and a local memory to I O components coupled by the interconnection fabric. A root complex may support transactions to or from one or more ports where each port defines an interface to a portion of the hierarchical interconnect fabric. Accordingly one or more root complexes RC may have redundant connections to a first set of endpoints which are coupled to each other through a first crossover path . The first set of endpoints can be coupled to a second set of endpoints and a third set of endpoints . The third set of endpoints is coupled to one another by a second crossover path as well as to one or more cache modules containing cache memory. The third set of endpoints can also be coupled to a fourth set of endpoints and a fifth set of endpoints .

According to certain example embodiments an interconnection scheme is based on the hierarchical interconnection fabric can interconnect components or devices such as a cache module or for example with further devices not shown as endpoints of each connection. An endpoint can be a type of device that may be a requester or completer of a transaction within the interconnection scheme. According to the placement of point to point links and the interconnection of certain types of endpoints a hierarchy of component connections may be established at the endpoints.

For example the second set of endpoints may be I O circuits that can be connected to external networks and the third set of endpoints may be reconfigurable data transfer circuits. The first set of endpoints may be connection points establishing connections for transactions between I O circuits associated with the second set of endpoints and the reconfigurable data transfer circuits of the third set of endpoints . By maintaining certain types of components at certain levels of endpoint groupings a hierarchy of endpoints may be established. By way of further example the fourth set of endpoints and the fifth set of endpoints may be storage devices that when coupled to the third set of endpoints form a storage tier at a further level of an endpoint hierarchy.

The first crossover path provides a redundant path capability between endpoints in the second set of endpoints and the fourth set of endpoints . For instance a first endpoint in the second set of endpoints may connect through a first redundant path to a completer endpoint in the fourth set of endpoints . Alternately the first endpoint in the second set of endpoints may alternatively connect to a second redundant path to the same completer endpoint in the fourth set of endpoints as was the case for the first redundant path . By utilizing the first crossover path to couple the first and second endpoints in the first set of endpoints the second redundant path couples between the second endpoint of the first set of endpoints and the second endpoint of the third set of endpoints to complete a connection to the completer endpoint in the fourth set of endpoints .

The second redundant path may be utilized in the case for example that the first endpoint in the third set of endpoints becomes unavailable due to either a component failure or perhaps from congestion due to high volumes of data stemming from a previous transaction. By a simple application of symmetry the first redundant path and the second redundant path may be considered rotated or reflected about a vertical axis down the center of the figure not shown such that the second endpoint in the second set of endpoints and a first completer endpoint in the fifth set of endpoints would be coupled to one another by either the mirrored first redundant path or the mirrored second redundant path not shown . Application of this symmetric case of redundant paths would provide a coupling of the second endpoint in the second set of endpoints to the completer endpoint in the fifth set of endpoints in the event that the second endpoint in the third set of endpoints were to become unavailable.

A similar situation for providing redundant paths may be visualized by applying the second crossover path providing alternative connections between the first endpoint in the second set of endpoints to a completer endpoint in the fifth set of endpoints . The second crossover path would alternatively provide a path for coupling the first endpoint in the fifth set of endpoints through either of the endpoints in the third set of endpoints . By application of alternate path connections similar to that described above for the first redundant path and the second redundant path a similar pair of redundant paths would support the coupling the first endpoint in the second set of endpoints to the first endpoint in the fifth set of endpoints .

These redundant paths also allow for data to be routed to or from any root complex to any endpoint or cache memory module even if there is a path or endpoint that for example becomes unavailable due to a component failure or congestion. For instance if the first endpoint in the first set of endpoints becomes unavailable the first root complex may take advantage of an alternate point to point link to the second endpoint in the first set of endpoints and gain access to either the fourth set of endpoints or the fifth set of endpoints through use of either of the endpoints in the third set of endpoints and the second crossover path in a fashion similar to that described above in regard to the first redundant path and the second redundant path . In addition if a first path from an endpoint to a further endpoint or from a root complex to any endpoint is operating at a full capacity a redundant path may be incorporated in order to boost performance by balancing the load between interconnection fabrics.

Various groupings of endpoints have been described as forming a hierarchy within the interconnection fabric. Enumeration is a process initiated by a master processor or server to set up a description of a tree hierarchy corresponding to the interconnection fabric. The enumeration process may start from the tree root root complex and progress down to the leaf nodes of the interconnection fabric. According to some example embodiments a serial communication and connection specification or a serial bus standard such as the peripheral component interconnect express PCIe a configuration process may be implemented as boot software is executed to discover endpoints and assign identifiers to discovered endpoints using a depth first sequence. Once discovered these endpoints can be configured along with further PCIe components such as fabric switches and field programmable gate arrays FPGAs not shown and associated cache modules so that packets can be routed per the enumerated identifiers as well as the address map described below . All levels of nodes or endpoints within the tree are named according to the level at which they reside. The enumerated hierarchy descriptors are stored in tables. Only the root complexes RC have a map of the tree hierarchy. The identities of data packets may be translated from the TCP IP space to this address naming space of the interconnection fabric e.g. PCIe address configuration space .

A further component of redundancy in accordance with some embodiments is the use of a mechanism known as a nontransparent bridge NTB . The NTB allows packets of data to traverse from the tree of one interconnection fabric to another tree in a parallel interconnection fabric. NTB includes registers that map the address space of one tree to the address space of another tree and translates relevant fields in packets as they traverse from one tree to another. Since each CPU die typically has its own root complex and there can be multiple CPU dies in a subsystem NTB can be used to allow multiple CPU dies to connect to devices throughout the subsystem. The first root complex and the second root complex are coupled to inputs of the NTB not shown . An output of the NTB is coupled to the first endpoint of the first set of endpoints .

According to example embodiments the NTB may be programmed to gate the second input coupled to the second root complex in nominal operation. In the event of a CPU failure in the first root complex in association with the first fabric instance software running on a CPU in the second root complex can re configure the NTB connection to become transparent thus making the previous nontransparent connection for the second root complex to become viable for managing the routing of data packets through the first fabric instance . Any components in the first fabric instance that were coupled to the first root complex become part of the interconnection fabric managed by the second root complex after reconfiguring the NTB .

According to some example embodiments the switch may comprise a bridge at an input port and have one or more outputs each appearing as a further bridge connected to the bridge at the input. By incorporating this array of bridges switches can create multiple endpoints out of one endpoint and thus allow one endpoint to be shared with multiple devices.

An interconnection fabric can be configured with a peer to peer P2P connectivity scheme according to example embodiments. Given the point to point nature of PCIe switches are used to allow a single device to communicate with multiple devices. Switches route packets either by address or by destination identifier described below . The switches within the switch fabric establish data communication paths between endpoints connected at the boundaries of the switch fabric in a manner similar to the connectivity description of endpoints in . This point to point communication between endpoints may be accomplished with a serial communication and connection specification or the PCIe standard as the management layer. PCIe is a high speed serial computer expansion bus standard that implements improvements over predecessor connection standards such as the peripheral component interconnect PCI standard or the accelerated graphics port AGP standard.

I O circuits can couple the switches respectively to external networks or devices not shown . The switches may be coupled to reconfigurable data transfer components as well as to one another. The coupling between the switches may be provided by a first crossover path similar to that described for the first set of endpoints . Each of the switches couples to both reconfigurable data transfer components . The first crossover path and the cross coupling of the switches to each of the two reconfigurable data transfer components can establish the cross coupling and redundant paths as well as the ensuing capabilities as described above in relation to first set of endpoints and the third set of endpoints .

The reconfigurable data transfer components each couple to a respective dynamic random access memory DRAM as well as to one another through a second crossover path . The reconfigurable data transfer components are also each cross coupled to a first switch array and a second switch array . The cross coupling of the reconfigurable data transfer components to one another and to the switch arrays provides the redundant paths capabilities described above in . The first switch array and the second switch array are coupled to a first flash array and a second flash array respectively.

One or more of the external networks may provide a transfer request to the I O circuits which in turn can be propagated to the root complexes as requests . The requests are in turn propagated by the root complexes to the switch fabric as respective data transfer requests. According to an example embodiment a data transfer request made by the root complex can establish a point to point connection across the P2P switch fabric by establishing a path through particular switch elements in the fabric. The root complex may be involved in initiating a path through the switch fabric and thereafter data transfers from endpoint to endpoint may be conducted without direct intervention by the CPU in each data packet of the transfer. For example a data sourcing endpoint such as the first I O circuit may be coupled to a data receiving endpoint such as the first flash array by a transfer request initiated with the first root complex . The transfer request initiated by the first root complex may establish a memory mapped input output MMIO connection between the two endpoints and thereafter large amounts of data may be transferred between the endpoints without further involvement by the first root complex

Within the network interface tier in certain embodiments I O circuits collectively referred to as I O circuits provide high speed connections from external networks or devices not shown to an interconnect layer or switch fabrics . I O circuits are coupled to switch fabric and I O circuits are coupled to switch fabric . The I O circuits are also coupled to external networks not shown such as InfiniBand Fibre Channel serial attached SCSI SAS and or Ethernet for example. The I O circuits connect these external networks to switch fabrics . The I O circuits can provide protocol conversion and still produce high speed data communication between the external networks and the switch fabrics

The I O circuits may be coupled by the switch fabrics to the cache tier and or the storage tier . The storage tier may be composed of flash circuits or modules arranged on flash boards collectively referred to as flash boards . The switch fabric may be implemented with an array of switches including I O circuit switch coupled to the I O circuits and flash switches coupled to the flash board . The switch fabric may be implemented with an array of switches including I O circuit switch coupled to the I O circuits and flash switches coupled to the flash board . The cache tier can be implemented with reconfigurable circuitry. In some embodiments the reconfigurable circuitry comprises field programmable gate array FPGA collectively referred to as FPGA which is interposed in the respective switch fabrics to couple respective cache modules to various endpoints under the command and direction of the FPGA

A server layer may be for example server boards collectively referred to as server boards . The server layer which is coupled to the switch fabrics includes CPUs or servers within respective server boards . The server boards also referred to as root complexes act as a control and management agent for respective portions of the switch fabrics

In some embodiments each server in the server layer may be coupled to a respective switch fabric . Each server and each respective switch fabric may be cross coupled to one another. A crossover path comprises a redundant connection pathway between the server board and the switch fabric . A crossover path comprises a redundant connection pathway between the server board and the switch fabric . The I O circuit switches within respective switch fabrics can also be cross coupled to each other via a crossover path . A crossover path couples the FPGA to each other. Redundant paths as described above with reference to can provide access to a parallel or mirrored paths according to cross coupling between the switch fabrics

In redundant switch fabrics such as the switch fabric and the switch fabric there may be a server element or server within the root complex associated with each portion of the redundant switch fabric. According to certain embodiments of redundant switch fabrics two servers may be coupled to the switch fabric and reside on a server board and be directly coupled to one another by direct connections implemented on the server board. The direct connections between servers are primarily for communication and management considerations between the servers and additionally may operate as a cross coupled transmission path for data throughput. Various elements within the switch fabric associated with respective servers may be connected to a corresponding element in the complementary parallel switch fabric and thus provide redundancy by establishing paths in another switch fabric when a portion of a path is not available in the first switch fabric. The crossover paths e.g. crossover paths and or may provide cross coupling as in similar elements described for to establish redundant pathways within the system .

The storage tier can be composed of storage modules e.g. memory elements and implemented with flash memory or any persistent solid state memory technology that provides data access performance suitable for direct or speed buffered connection to data from external network environments. According to some embodiments flash modules containing flash memory technology can be organized in the storage tier . Yet the flash module connects to the switch fabrics with a single connection and operates like a single ended source and sink receiver of data. Flash module access times may also exceed the access performance of HDD technology by several decimal orders of magnitude.

The cache tier can be positioned between the network interface tier and the storage tier . The cache tier can be connected through the switch fabrics to both the I O circuits in the network interface tier and the flash modules in the storage tier . The cache tier can be considered as an intermediate storage layer to temporarily store data that ultimately transitions from the I O circuits to flash modules or vice versa. In certain embodiments the cache tier includes individual cache modules each having DRAM for cache storage and flash memory for backup in case of power loss for example.

Requests originating from I O circuits may be primarily serviced by switch fabric server board and cache modules . Nevertheless the redundant pathways built into the system such as but not limited to crossover paths and or permit other component s to take over in case of primary component failure over capacity or other inability to serve a given request. For example if I O circuit switch is unable to handle a request from I O circuit or then I O circuit switch can take over via the crossover path . Requests originating from I O circuits may be primarily serviced by switch fabric server board and cache modules . Similarly the redundant pathways built into the system such as but not limited to crossover paths and or permit other component s to take over in case of primary component failure over capacity or other inability to serve a given request.

In alternative embodiments one or more of the crossover paths may be omitted from the storage system depending on extent of redundancy desired for the system.

Each of the reconfigurable application engine cache interface cache control flash PCI interface behavior control inter FPGA interface RAID compute engine network PCI interface protocol offload engine and ring s or crossbar s comprises circuitry and logic in some instances specifically configured to perform functionalities and operations as described in detail below. The circuitry may comprise for example a plurality of logic components also referred to as logic blocks interconnects memory elements e.g. flip flops or memory blocks CPUs and other circuit elements. All or just a portion of the FPGA may be reconfigurable circuitry hardware. For example the reconfigurable application engine cache control and or behavior control may be reconfigurable circuitry hardware while the other components of the FPGA comprise non reconfigurable circuitry hardware. The FPGA may comprise an application specific integrated circuit ASIC or an integrated circuit IC component in general.

Although not shown the FPGA may include additional memory CPU and or processors to facilitate functionalities and operations of any of the reconfigurable application engine cache interface cache control flash PCI interface behavior control inter FPGA interface RAID compute engine network PCI interface protocol offload engine and or ring s or crossbar s .

Additional details pertaining to the reconfigurable application engine of the FPGA are described in conjunction with according to some embodiments. illustrates an example block diagram showing functionalities operations implemented in modules according to some embodiments. The modules comprise one or more software components programs applications apps or other units of code base or instructions configured to be executed by one or more processors included in the server board . The modules include an acceleration module a FPGA reconfiguration module and a send receive module . Although modules are shown as distinct modules in it should be understood that modules may be implemented as fewer or more modules than illustrated. It should also be understood that any of modules may communicate with one or more components included in the system to obtain data or other information in connection with the reconfigurable application engine performing processing functions that would otherwise be performed on the server board .

The reconfigurable application engine comprises reconfigurable circuitry hardware that can be re configured under control of the server board . Certain processing functions or operations that may be performed via software implementation at the server board may be offloaded to the reconfigurable application engine . The reconfigurable application engine performs the offloaded processing functions operations using hardware at a significantly faster speed than can be performed by software. Thus the reconfigurable application engine accelerates certain functions operations that would otherwise be performed on the server board .

At a block the acceleration module determines whether a particular processing function or operation to be performed on the server board is a function operation that is suitable for offloading to the reconfigurable application engine . Whether the particular processing function operation to be performed comprises a suitable acceleration task for the reconfigurable application engine may depend on whether the reconfigurable application engine is already configured to perform the particular processing function operation or it is a processing function operation that is sufficiently processing or time intensive enough to warrant offloading to the reconfigurable application engine . Examples of suitable acceleration tasks include but are not limited to data compression data decompression de duplication snapshots thin provisioning encryption decryption a particular algorithm or subroutine or application or a processing or time intensive function operation pertaining to data access or storage within the system . Other examples of suitable acceleration tasks include but are not limited to performance of operations that are requested over a network pertaining to big data applications other than storage. Image processing for example are processor intensive and if there are a large number of images that require similar image processing treatment then the network may request the reconfigurable application engine to be configured as an additional resource to perform a particular image processing task.

If the acceleration module determines that the particular processing function operation is not a suitable acceleration task no branch of block then the acceleration module continues to wait for a suitable acceleration task block . Otherwise the particular processing function operation comprises a suitable acceleration task for the FPGA yes branch of block and the FPGA reconfiguration module checks whether the reconfigurable application engine is configured for the acceleration task block .

If the reconfigurable application engine requires reconfiguration to handle the acceleration task no branch of block then the FPGA reconfiguration module performs tasks to initiate reconfiguration of at least the reconfigurable application engine . The FPGA reconfiguration module communicates with a configuration data source control logic confirms that the reconfigurable application engine is ready for the reconfiguration data and other initialization steps. At a block the FPGA reconfiguration module and or the send receive module loads reconfiguration data and instructions to the FPGA in order to reconfigure the reconfigurable application engine circuitry to perform the particular acceleration task. In some embodiments a hardware description language HDL can be used to specify the reconfiguration. Block may be performed at system initialization e.g. power up or reset of the system and or on the fly during normal system operation.

The reconfiguration data and instructions are received at a block at the FPGA via I O circuit switch . Next at a block at least the reconfigurable application engine is reconfigured in accordance with the reconfiguration data and instructions. Block may be referred to as partial reconfiguration in which a portion of the reconfigurable circuitry hardware is changed while the other part remains running operating. In one embodiment a given server board reconfigures both of the FPGAs . In another embodiment a given server board reconfigures a given FPGA . Each of the reconfigurable application engine may be reconfigured to same or different from each other. For instance the reconfigurable application engine of the first FPGA is configured to perform a particular data encryption algorithm pertaining to data storage or access and the reconfigurable application engine of the second FPGA is configured to run a financial simulation application requested by a network in communication with the system .

If the reconfigurable application engine does not require reconfiguration to perform the acceleration task yes branch of block then the send receive module sends input initial data and request to perform the acceleration task to the FPGA via the I O circuit switch block . In response at a block the FPGA receives the input initial data and request. At the FPGA the requisite information is received by the network PCI interface then ring s or crossbar s and then to the reconfigurable application engine . The reconfigurable application engine performs the acceleration task using the requisite information at a block . The result of or output data at completion of the acceleration task is sent from the reconfigurable application engine via ring s or crossbar s network PCI interface and I O circuit switch to the server board block .

In response at a block the output data from the FPGA is received by the send receive module in the server board . Lastly the server board uses the received output data at a block . Usage may include using the output data within a larger application.

Embodiments of the storage system are configured to handle data requests from the I O circuits at a faster rate than can be performed using HDDs flash based memory or software. The FPGAs cache modules and tags and matching logic modules facilitate faster handling of data communications using hardware circuitry. In some embodiments the cache control included in the FPGA controls data to and from the cache modules via the cache interface and ring s or crossbar s also included in the FPGA . The cache control included in the FPGA also communicates with the tags and matching logic module to control the cache modules . Similarly the cache control included in the FPGA controls data to and from the cache modules via the cache interface and ring s or crossbar s also included in the FPGA . The cache control included in the FPGA also communicates with the tags and matching logic module to control the cache modules . The FPGAs in particular cache interface and cache control cache modules and tags and matching logic modules are collectively referred to as the cache management subsystem. Rather than having the server boards control the data requests from the I O circuits or rely on the flash modules to store and provide the requested data the cache management subsystem acts as the efficient middleman.

From the perspective of the I O circuits it appears that all the data it needs to access and or all the data it needs to store in the system are provided from the cache modules and or . Thus the cache modules appear to be an infinite cache. This is the case even though the flash modules are the primary storage elements for the system and the capacity of the cache modules is insufficient to hold all the data stored in the system .

In some embodiments the total storage capacity of the flash boards is significantly larger than the total storage capacity of the cache modules . The total storage capacity of the flash boards may be a multiple of the total storage capacity of the cache modules . For instance the ratio of the total storage capacity of the flash boards to the total storage capacity of the cache modules may be 1000 1. As another example the total storage capacity of the flash boards may be on the order of terabytes while the total storage capacity of the cache modules may be on the order of gigabytes. In some embodiments the total storage capacity of the cache modules may be limited by the physical dimensions of the chassis in which the cache modules are housed.

The cache management subsystem is implemented within the system to take advantage of features such as the access times for the cache modules which are DRAMs being approximately a thousand times or so faster than for the flash boards . And unlike flash which is limited to a certain number of writes before it degrades DRAMs do not degrade. In some embodiments the cache control may comprise reconfigurable circuitry hardware. In other embodiments the cache control may comprise non reconfigurable circuitry hardware.

The tags and matching logic modules comprise hardware circuitry configured to hold at least a cache table or similar data structure information that correlates memory locations of the flash boards to memory locations of the cache modules respectively. The tags and matching logic modules comprise a type of hardware circuitry e.g. TCAM capable of very rapid searching or lookup of data stored within it.

Each row of the cache lines indicates whether data is stored in that cache line data area what portion of that cache line data area is empty or occupied information about the stored data and other information relating to the stored data and or use of the cache line data area. Data stored in a given cache line is associated with a unique cache tag also referred to as a tag that serves as an identifier or name for the data and or the particular cache line. The cache tags are provided in the cache tags . Each cache tag comprises one or more pieces of information including but not limited to the flash memory address corresponding to the data associated with the given cache tag. For example the flash memory address may be included in an I O request originating from the I O circuit . As another example if the particular data was obtained from the flash modules and then stored on the cache modules the particular flash memory location s from which the data was taken is reflected in the corresponding cache tag. Additional details pertaining to cache tags are described below in the discussion of the protocol offload engine . The cache tags are searched or looked up to determine whether data associated with a given tag resides in the cache modules .

Lastly the data stored in each of a given cache line also has associated with it one or more tracking metrics such as but not limited to age e.g. when the data was written to the cache modules measured in number of CPU cycles number of read requests for that data number of write requests for that data user specified information e.g. data will be rarely accessed data is to be stored in archival area of flash boards often used data etc. system known information e.g. reconfigurable application engine knows that the output data is generates will be rarely accessed data and other data use information that can be tracked for statistical and or cache management purposes. Tracking metrics may also be referred to as data counters.

A backup copy of the cache table is maintained at all times within the system . For instance if the tags and matching logic modules comprise TCAMs then in the case of power failure the cache table will be lost. To ensure against such a scenario a duplicate copy of the cache table can be maintained within the FGPA

In the cache control in each of the FPGAs performs cache capacity management operations for its cache modules respectively to ensure that the cache modules always have sufficient capacity to handle I O requests. If cache capacity is not adequately monitored and optimized for example there may not be enough space to fully handle a new write request. Cache management for cache modules and are performed independently of each other.

At a block the cache control determines whether it is time to perform cache capacity maintenance. Maintenance may be a continuous background operation a periodic background operation or on a need basis type of operation. Maintenance frequency can be a system setting user setting or dynamic setting based on current operating conditions of the system . If maintenance is initiated yes branch of block then the cache control determines whether the current data storage capacity of the cache modules or depending on which set of cache modules is associated with the given cache control is at or above a pre set maximum capacity level block . The pre set maximum capacity level is a certain value that is pre set by the system or user and represents the portion of the total data storage capacity of the cache modules that can be occupied while having a safe amount of available space in case for example an unexpectedly large write request is received. Examples of pre set maximum capacity level include but are not limited to 70 80 or some other value. In some embodiments the pre set maximum capacity level may be adjusted over time as more system usage information becomes available. Instead of expressing the pre set maximum capacity level as a percentage of the total data storage capacity for example it is understood that it can be expressed as a minimum available or reserved free space.

If the current cache capacity is below the pre set maximum capacity level no branch of block then the flow diagram returns to block . Otherwise the current cache capacity is too close to the pre set maximum capacity level and some of the stored data needs to be moved to the flash modules and evicted or erased from the cache modules or depending on which set of cache modules is associated with the given cache control yes branch of block .

Next at a block the cache control determines what data to displace from the cache modules or depending on which set of cache modules is associated with the given cache control according to a set of cache eviction rules. In some embodiments the cache eviction rules may comprise an algorithm implemented in software. The cache eviction rules may be set by the system or a user. The cache eviction rules may comprise one or more rules and if it comprises more than one rule rules may have a priority order relative to each other a certain rule may override another rule two rules in combination may override a third rule or the like. Example cache eviction rules comprise without limitation 

The cache control checks the cache table included in its corresponding tags and matching logic module and in particular compares the information provided in the tracking metrics field of the cache table for all cache lines containing data against each other according to the cache eviction rules. In one embodiment the cache eviction rule may comprise evicting data stored in the cache line s that is the least written. In another embodiment the cache eviction rule may comprise evicting data stored in the cache line s that is the least written except for data that is pinned to stay within the cache based on a user specified directive.

Once the cache line s to empty are identified the cache control sends data stored in those cache line s to the flash modules for storage block . Such data is erased emptied or evicted from those particular cache line s at a block . The flow diagram then returns to block . Thus the cache capacity of cache modules or depending on which set of cache modules is associated with the given cache control is maintained at or below the pre set maximum capacity level. It is understood that blocks and may occur simultaneously of each other.

Next at a block the tags and matching logic module or corresponding to the particular cache control handling the read request performs a look up of its cache table to determine whether the requested data exists in the cache modules or . The cache tags are searched to see which one if any contains the same flash memory address location as the particular memory address location provided in the data request. In one embodiment all of the cache tags in the cache table may be searched fully associative . In another embodiment a subset of the cache tags may be searched set associative . In an alternative embodiment a particular one of the cache tags may be searched direct mapped . The tags and matching logic module is configured to perform the look up function several orders of magnitude faster than may be possible if the cache table resides in the FPGA for example. This may be the case even if there are a large number of rows e.g. cache lines in the cache table such as thousands of rows.

If a matching cache tag is found yes branch of block the cache control accesses the data corresponding to the matching cache tag from the cache module and sends the retrieved data to the originating I O circuit block . The retrieved data is the requested data in the read request. The tracking metrics for at least that data is updated in the block . For example the counter for the number of reads of that data may be incremented by one. If the retrieved data was previously written to the cache module in a previous write request and such data was not evicted from the cache module due to cache management operations see then such data is present in the cache module for later access such as the present read request. Then there is no need to retrieve the data from the flash modules . Data retrieval from a DRAM cache is significantly faster than from flash based memory upwards of a thousand times faster using cache than flash.

If no matching cache tag is found no branch of block the requested data is not present in the cache modules and is retrieved from the flash modules . At a block the cache control initiates retrieval of the requested data from the appropriate flash modules . Next at a block a system setting or user specified setting is checked to see whether the requested data retrieved from the flash modules should be copied to the cache modules . If the system is set not to copy to cache modules no branch of block then the flow diagram proceeds to block . Otherwise the retrieved data is copied to the cache modules yes branch of block and block .

The retrieved data is also sent by the cache control to the I O circuit that made the read request block . The cache table is correspondingly updated at a block . Because data is written to particular cache line s of the cache modules that did not exist before the cache tags and cache lines fields for those cache line s are populated accordingly. The associated tracking metrics are also populated at least for example the age field.

Although blocks and are shown prior to block in it is contemplated that block and blocks may be performed simultaneously to each other or in reverse order from that shown in .

At a block the cache control determines whether the data associated with the write request is exceptional. While the default rule is to store all data associated with write requests to the cache modules and then from the cache modules copy to the flash modules at some later point in time one or more exceptions to the default rule may be implemented. One or more exception criteria may be a system setting or user specified setting. For example the exception may comprise there being no exception to the default rule. As another example data exceeding a certain size e.g. data that if written to the cache modules may exceed the cache capacity or likely to exceed the pre set maximum capacity level may warrant storing directly in the flash modules without first storing in the cache modules . As still another example the write request or the data associated with the write request itself may specify that the data will be rarely accessed e.g. is archival data or has a certain characteristic that warrants being stored directly in the flash modules without first being stored in the cache modules .

If the data associated with the write request is determined to be exceptional yes branch of block then the cache control sends such data to be written to the flash modules block . Otherwise the data associated with the write request is not exceptional no branch of block and operations are performed to write to the cache modules . At a block the tags and matching logic module checks the cache table for a cache tag containing the same flash memory address location as provided in the write request. If a matching cache tag is found yes branch of block this means that an older version of the data associated with the write request or some data in general is currently stored in the cache line s now intended for the data associated with the write request. The cache control facilitates overwriting the existing data at these cache line s with the data associated with the write request block . Then the flow diagram proceeds to block .

If no matching cache tag is found no branch of block then the cache control facilitates writing the data associated with the write request to empty available cache line s in the cache modules block .

Next at a block the data associated with the write request is additionally copied to empty available cache line s in the cache modules associated with the other FPGA . This mirroring of data between the cache modules and occurs via the inter FPGA interface and the crossover path connecting the FPGA to FPGA . In some embodiments block is optional when the crossover path is omitted from the storage system . In other embodiments the mirroring of data associated with the write request in both cache modules and is initiated before the write request is received at a given FPGA . The write request from the I O circuit is received by the I O circuit switch and is split into two identical requests one going to the FPGA and the other to the FPGA . Then the cache control in each of the FPGAs and can store the data associated with the write request also referred to as write data in its respective cache modules and . At a block the cache table included in the tags and matching logic module is updated to reflect the addition of the data associated with the write request into certain cache line s of the cache modules .

Because flash modules comprise the primary or permanent data storage medium for the storage system the data associated with the write request although already written to the cache modules see blocks and is eventually written to the flash modules . Nevertheless the cache management subsystem is configured to intelligently perform data writes to the flash modules taking into account the characteristics of the flash modules . In order to prolong the usability of flash modules which are limited to a certain number of writes before degrading the cache management subsystem accumulates certain type of data corresponding to a plurality of write requests and then performs a single write of the accumulated data to flash modules rather than performing a write to flash modules for each write request. This means that if for example there are 25 write requests instead of writing to flash modules 25 times once for each of the 25 write requests the data corresponding to these 25 write requests may be written at the same time and once e.g. a single write operation to the flash modules .

After the data associated with the write request is written to cache module and cache table updated accordingly the cache control determines whether the data associated with the write request and data associated with a previous write request are associated with consecutive block s of the flash modules block . Both the data associated with the write request and data associated with a previous write request are handled by the same cache control . If both data are associated with consecutive block s of the flash modules yes branch of block then the cache control waits to write data associated with the write request and the data associated with previous write request to flash modules block . The cache control accumulates data to be written to the flash modules . If the two data are associated with non consecutive block s of flash modules no branch of block then the cache control sends data associated with the previous write request to be written in flash modules block .

Accordingly the cache management subsystem is configured to act as a middleman between the I O circuits and flash modules for every read and write requests from the I O circuit . For all read and write requests the presence of data associated with the read or write request in the cache modules is checked before the flash modules are involved. Based on the presence or absence of such data in the cache modules the cache management subsystem performs optimization operations to complete the data requests significantly faster than is possible with flash modules alone. The cache management subsystem also prolongs the useful lifespan of flash modules by minimizing the number of writes to flash modules without sacrificing completeness of data being stored in the flash modules . All data associated with write requests are written to cache modules prior to be written to flash modules unless the data fits an exception. All data associated with read requests that are retrieved from the flash modules may or may not be written to cache modules depends upon system or user setting . All data associated with write requests similarly may or may not be written to cache modules corresponding to both FPGAs depends upon system or user setting . The cache management subsystem actively maintains the used storage capacity level of the cache modules at or below a pre set capacity level e.g. 70 80 etc. by evicting data stored in the cache modules that fit one or more eviction rules as needed. An example of an eviction rule comprises evicting data that has the least amount of write activity and moving it to the flash modules .

The storage system may experience performance variations over time such as bottlenecks at certain times. The performance of the storage system may be improved or tweaked over time based on measurements of actual system performance. In some embodiments the storage system offers a plurality of potential measurement or monitoring events e.g. via a set of menu options for selection by a system analyst or diagnostician. Each of the plurality of potential measurement or monitoring events comprises a particular behavior characteristic attribute or event that occurs at a particular location within the system . The behavior control included in each of the FPGAs is configured to facilitate handling of event monitoring as directed by the system analyst or diagnostician. For example the particular event of interest may be to find out how many data packets are received by the system within the next 10 hours that are 213 bytes in size. As another example the system may be experiencing an unexpected decrease in performance and in order to diagnosis where the bottleneck is occurring one or more points within the system may be targeted for event monitoring.

The system may monitor hundreds thousands or tens of thousands of particular behaviors characteristics attributes or events that provide detail logs of various system operations referred to as low level behavior monitoring or measurements. In addition the behavior control comprising reconfigurable hardware circuitry can be configured to provide higher level behavior monitoring or measurements. The hardware circuitry permits measurements to keep up with real time activities within the system in which software based measurements can t keep up . And the reconfigurability of the hardware circuitry permits different behaviors characteristics attributes or events to be captured over time as needed. Although low level behavior monitoring provides a large amount of detailed data the data tends to be discrete event type data and may not correlate to each other in a manner useful to the system analyst or diagnostician. The higher level behavior monitoring aims to correlate behavior monitoring data in a meaningful way that is useful for troubleshooting future system upgrades system fine tuning and the like.

For each of the events involved in a given compound event comparative type logic also referred to as comparative logic comparer logic or comparator element and a counter are configured in the system component to be monitored for the given event. For example if an event involves monitoring the size of all incoming data packets to the system comparative type logic and counter may be provided within each of the I O circuits . As another example if an event involves monitoring input or output characteristics of the reconfigurable application engine then comparative type logic and counter may be located within the reconfigurable application engine . For each system component e.g. system component a system component b . . . system component n involved in a compound event appropriate comparative type logic counter and other measurement detection elements are configured therein. Each of the comparative type logics is provided with the appropriate pre set event condition or comparer value e.g. look for 213 byte size data packet greater than 90 compression image output iSCSI protocol etc. .

At a system component a e.g. I O circuit I O circuit switch FPGA reconfigurable application engine etc. the comparative type logic is set up to monitor a first event of a given compound event. If no event is detected no branch of block then the monitoring continues. If an event is detected yes branch of block then the comparative type logic compares the detected event to the pre set event condition or comparer value at a block . If the condition is not met no branch of block then the flow diagram returns to block . Otherwise the condition is met yes branch of block and the flow diagram proceeds to block . The counter associated with the first event comparative type logic is incremented at the block

Continuing the example above system component a may be the I O circuit the comparative type logic may be configured to detect incoming data packets and the pre set condition may be 213 byte size. Thus a successful first event comprises a data packet received by the I O circuit that is 213 bytes in size.

Once the counter is incremented the system component a sends notification to the behavior control block . The notification informs the behavior control that an instance of the first event of the compound event has occurred. The notification also provides additional information about the first event. For example the data packet may be associated with a unique identifier and the unique identifier is provided to the behavior control . The unique identifier will be used by the behavior control to correlate other event notifications to each other. For example the unique identifier permits tracing of a particular data packet s pathway through the system or at least to certain system components within the system .

Next at a block a check is performed as to whether to continue monitoring for the event. The comparative type logic may be configured to monitor only for a specific time period e.g. 12 hours or until some other condition is met. If monitoring period should continue yes branch of block then the flow diagram returns to block . Otherwise no branch of block the monitoring stops.

Each of the system components a b . . . n involved in monitoring the compound event performs operations similar to those discussed for blocks . As shown in system component b performs blocks system component n performs blocks and the like. It is understood that each of the system components a b . . . n involved in the compound event monitors for respective first event second event and so forth of the compound event.

At a block the notification sent in each of blocks . . . are received by the behavior control . In response the behavior control determines whether the compound event has occurred by correlating the received notification information at a block . Continuing the example of the 213 byte size data packet let s assume the second event of the compound event is that such data packet is destined for RAID group seven. If the behavior control receives a notification from the system component a of a 213 byte size data packet having a certain unique identifier and also a notification from the system component b of a data packet received at RAID group seven having the same certain unique identifier then the behavior control is able to correlate the two notifications together based on the common unique identifier. The pathway of the particular data packet within the system is traced by the behavior control . Upon detection of the compound event the behavior control increments a counter associated with the compound event at a block .

The operations of may be performed for each of a plurality of compound events being monitored by the behavior control . In this manner the behavior control monitors one or more behavior attributes of at least a portion of the system .

The RAID compute engine included in each of the FPGAs comprises reconfigurable or non reconfigurable hardware circuitry to facilitate redundant grouping of flash modules to improve chances of data recovery in failure scenarios. In some embodiments all of the RAID functionalities may be controlled by the RAID compute engine . In other embodiments the storage tier may include some RAID functionalities and other or master functionalities may be handled by the RAID compute engine .

Hardware circuitry included in FPGAs are used to offload the software based processing performed by the servers in the root complexes during data transfers in read and write operations. The FPGAs use reconfigurable hardware circuits for read and write data transfers that are faster than the equivalent data transfer software executing on a processor. The CPU associated with the FPGA in the interconnection fabric or a master CPU may be involved in providing instructions to configure the FPGA to handle the offloading of data transfers. The FPGA is configured by code executing on the CPU at boot up and may be reconfigured anytime the component configuration in the system is altered e.g. anytime there is a change in the configuration of memory devices .

The offloading of data transfer software executing on any CPU to the FPGA involves the FPGA being configured to perform translations of logic unit number LUN and logic block address LBA in the Internet small computer system interface iSCSI domain into PCIe configuration addresses to transfer data to flash memory. Data addresses such as those involved in the iSCSI protocol and maintained in the Internet environment are managed in terms of LUNs and LBAs. However in the domain of storage devices maintained in the PCIe P2P connectivity environment addresses corresponding to the storage devices are managed according to the PCIe address configuration space see . A master server or CPU server boards in the system executes boot up software that determines the size and location of all data structures utilized in an address translation and configures the FPGA with this information. The configuration process of FPGAs includes populating tables with address conversion information for establishing LUN maps and LBA maps to convert data addresses from the iSCSI standard to flash memory addresses for example. Address configuration space is determined according to the enumeration process above . Once the enumeration process has discovered endpoints in the switch fabric the FPGA can be configured with the tables and mappings that provide LUN and LBA translation to PCIe addresses at will.

More particularly the system in accordance with some embodiments reduces processor e.g. CPU and software involvement and intervention in the control and throughput of dataflow between an external network environment and the storage system. The system receives I O requests from the external network. Typically a basic amount of software must execute in a CPU before a given I O request may be directed to an appropriate read or write hardware procedure e.g. PCI read or write cycles to be carried out. According to some embodiments most if not all of the typical software execution on the CPUs in the I O request types may be bypassed through offloading the processing of these requests to the protocol offload engine included in the FPGAs . In common cases of reads or writes to blocks or data objects in some embodiments the CPU e.g. server boards may be bypassed entirely.

For instance the iSCSI approach to data transfer between remote devices requires that the fundamental SCSI commands for controlling the I O requests each be encapsulated in various levels of IP or TCP layers. Each one of these layers of encapsulation must be unwrapped by the CPU and DRAM in the software intensive approach to I O protocol servicing. This iSCSI approach further exacerbates the problem under the software intensive approach to I O processing. Instead iSCSI read and write commands are offloaded to the FPGA for processing. All other command types may be processed in the server boards according to the iSCSI target software.

The I O circuits sends all other command types to the iSCSI target software that has its own dedicated descriptor rings in server memory e.g. in server boards . The iSCSI target software executes in the CPU. Descriptor rings according to example embodiments are a circular natured portion of memory that may be shared between a processor and a PCIe device to buffer information handed off between the two components during data transfers. Particular descriptor rings may be receive rings and transmit rings for example. ISCSI allows multiple protocol data units PDUs to be placed in a single transmission control protocol TCP payload. The network interface controller or card NIC e.g. I O circuit switch or places the entire TCP payload on the FPGA s descriptor ring only if all PDUs contained in that payload encapsulate either a read or a write command if any other command types are inter mixed with read or write commands then the NIC will place the entire payload on the iSCSI target descriptor ring.

The FPGA manages the caching of data involved in read and write transactions. The instructions provided to the FPGA from the execution of configuration programs operating in the CPU above can configures the FPGA to implement cache policies. Following the indexing of the LUN map and the LBA map the translated PCIe address is used to determine an address match in cache tags maintained within the tags and matching logic module or . If there is a cache tag match the data sought in the data transfer request is resident in the cache modules and may be provided from cache modules . The access performance for cache memory is significantly greater than the access times for flash memory. Each cache hit cache tag match in a data transaction significantly improves performance compared to accessing flash memory directly. In this way data transactions may be completed entirely from cache and accomplished significantly faster than would acquiring the data from flash memory. In this way as much of the data involved in read and write transactions as possible is provided from cache and cached respectively. In certain example embodiments it may be possible to accomplish nearly all data transactions from cache.

Thus the cache modules are accessible by any other peer component through memory to memory transfers utilizing the interconnect address space . This is possible due to the enumeration process including the cache modules associated with the FPGA in the same manner that all other peer devices associated with the switch fabric are enumerated.

The FPGAs include the following offload operations data structures which in conjunction with the cache tags in the tags and matching logic modules cached data in the cache modules and the protocol offload engine are capable of performing offloaded read and write operations 

At a block the Ethernet packet also referred to as a TCP IP packet including a read command arrives at the network interface tier and more specifically at the input of the NIC 1 corresponding to numerical operational steps denoted in . In response at a block the NIC reads the descriptor and forwards that to the iSCSI receiver ring 2 . And at a block the NIC writes the payload e.g. PDU of the packet into the packet buffer 3 . Next at a block the NIC writes a receive status into the iSCSI receiver ring 4 . The NIC also sends a new packet received interrupt signal 5 block . Next at a block the receive status is checked 6 and if there is a receive error the error indication is sent to the iSCSI target. If there is no receive error at a block the payload in the packet buffer is read the header fields parsed and the LUN field extracted to index into the LUN map 7 .

The LUN base is mapped to a fixed state of the FPGA . At a block the LBA map base in the LUN map is used to determine the LUN s the LUN identified in the LUN field LBA map 8 . The LBA field is used to index into the LBA map which was determined according to the LUN map 9 block . If there is an LBA map miss an error indicator is sent to the iSCSI target. LBA map entries are regions with a base or start LBA and region length indicated in units of blocks. The PCI addr is the starting address of the region.

Next at a block from the SCSI LBA map the PCI address plus block offsets are used for a cache tag lookup in the cache table 10 . A single read command may require multiple cache tag lookups. If there is a cache hit the LRU field of the cache tag corresponding to the data requested is updated e.g. incremented to indicate that a read of the data stored in that cache line is occurring 11 block . The lock bit field of the cache tag corresponding to the data requested is also set to lock to ensure that data to be read is not modified during the rest of the read process.

Next at a block the cache frame address is written into a descriptor at the head of the transmit ring 12 . The head pointer is incremented and the cache frame may serve as packet buffers. The head pointer for the transmit ring is updated 13 block . The NIC reads the descriptor from the iSCSI transmit ring and cache data 14 block . Next the NIC reads the packet buffer from the cached data corresponding to the requested data 15 block .

TCP IP and ethernet headers are prepended to the data read from storage in the headers and retrieved data are transmitted as a packet containing the requested data 16 block . The NIC writes a transmit status to the iSCSI transmit ring 17 block . The transmit status is checked in a block and if a transmit error is detected an error indication is sent to the iSCSI target 18 . If no transmit error is detected the lock bit field is cleared from the priority field in the cache tags corresponding to the requested data.

In some embodiments the iSCSI target software may need to be kept informed of the existence and progress of offloaded read and write commands so it can maintain its data structures including performance counters.

Although the present invention has been described in connection with some embodiments it is not intended to be limited to the specific form set forth herein. One skilled in the art would recognize that various features of the described embodiments may be combined in accordance with the invention. Moreover it will be appreciated that various modifications and alterations may be made by those skilled in the art without departing from the spirit and scope of the invention.

The Abstract is provided to allow the reader to quickly ascertain the nature of the technical disclosure. It is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the claims. In addition in the foregoing Detailed Description it can be seen that various features are grouped together in a single embodiment for the purpose of streamlining the disclosure. This method of disclosure is not to be interpreted as reflecting an intention that the claimed embodiments require more features than are expressly recited in each claim. Rather as the following claims reflect inventive subject matter lies in less than all features of a single disclosed embodiment. Thus the following claims are hereby incorporated into the Detailed Description with each claim standing on its own as a separate embodiment.

