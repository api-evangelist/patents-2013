---

title: Erasure coded storage aggregation in data centers
abstract: Embodiments of erasure coded storage aggregation are disclosed. The erasure coded storage aggregation includes storing a data file as erasure coded fragments in a plurality of nodes of one or more data centers. The erasure coded storage aggregation further includes monitoring an access frequency of the data file. Based on the comparison between the access frequency and a predetermined threshold, the data file is either reconstructed from the erasure coded fragments and stored in a storage node or retained as erasure coded fragments in the plurality of nodes of the one or more data centers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08918478&OS=08918478&RS=08918478
owner: Microsoft Corporation
number: 08918478
owner_city: Redmond
owner_country: US
publication_date: 20130603
---
This application is a continuation of and claims priority from U.S. patent application Ser. No. 12 534 024 titled Erasure Coded Storage Aggregation in Data Centers filed on Jul. 31 2009 which is herein incorporated by reference in its entirety.

 Cloud computing refers to the access of computing resources and data via a network infrastructure such as the Internet. The computing resources and data storage may be provided by linked data centers of the cloud . Each of the data centers may include a plurality of servers. The servers in a typical data center may provide computation nodes and storage nodes. Computation nodes provide data processing resources while storage nodes provide data storage and retrieval resources. However the computation nodes may occasionally use large amounts of storage capacity to perform computation operations. Thus a data center may set aside a significant amount of reserve storage capacity for such occasions.

Further the storage nodes generally store duplicate copies of the same data to guard against data loss due to hardware and or software failure. As a result the actual storage capacity of the storage nodes of a data center may be reduced. The need to set aside reserve storage capability and storage capacity loss due to redundant data storage may reduce the operating efficiency of a data center as well as result in the need to build new data centers that consume additional energy financial resources and natural resources.

Described herein are erasure coded storage aggregation techniques for reducing the amount of storage capacity or number of storage nodes used to offer data redundancy for guarding against data loss due to hardware and or software failure. Further the techniques may also enable some of the computation nodes of a data center to provide storage capacity for storing third party data while simultaneously maintaining the ability to buffer their own data for the performance of computation operations. In this way the storage capacity of existing data centers may be increased without the addition of servers switches and or other hardware infrastructure. Moreover increased efficiency in the usage of existing storage capacity of a data center may result in significant energy savings and operating cost reduction.

In at least one embodiment the erasure coded storage aggregation includes storing a data file as erasure coded fragments in a plurality of nodes of one or more data centers. The erasure coded storage aggregation further includes monitoring an access frequency of the data file. Based on the comparison between the access frequency and a predetermined threshold the data file is either reconstructed from the erasure coded fragments and stored in a storage node or retained as erasure coded fragments in the plurality of nodes of the one or more data centers. Other embodiments will become more apparent from the following detailed description when taken in conjunction with the accompanying drawings.

This Summary is provided to introduce a selection of concepts in a simplified form that is further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

This disclosure is directed to the implementation of erasure coded storage aggregation in one or more data centers to increasing the usage of existing storage capacity of storage nodes in the one or more data centers. In various embodiments the implementation of erasure coded storage aggregation on a data center may reduce the amount storage capacity or number of storage nodes used to offer data redundancy for guarding against data loss due to hardware and or software failure. Further the implementation of erasure coded storage aggregation may also enable some of the computation nodes of a data center to provide storage capacity for storing third party data while simultaneously maintaining the ability to buffer their own data for the performance of computation operations. In this way the implementation of erasure coded storage aggregation may increase the storage capacity of one or more existing data centers without the addition of servers switches and or other hardware infrastructure.

Moreover increased efficiency in the usage of existing storage capacity of the one or more data centers may result in significant energy savings and operating cost reduction as well as alleviate expenditure for new data centers. Various examples for implementing erasure coded storage aggregation on one or more data centers in accordance with the embodiments are described below with reference to .

The example architecture may be implemented on a computing cloud which may include a plurality of data centers such as the data centers and . The data center may be equipped with servers that are interconnected via an intranet infrastructure e.g. a local area network within the data center . Likewise the data center may also be equipped with servers that are interconnected via an intranet infrastructure within the data center . Moreover the data centers and may be interconnected to each other and one or more clients via a network infrastructure such as the Internet.

Each of the servers of a data center may provide computing resources and data storage retrieval resources to one or more clients such as the client via the network infrastructure . The one or more clients may include any computing device that has network access capabilities such as a desktop computer a laptop computer a tablet computer smart phone personal digital assistants PDAs and other wired and wireless communication devices. As shown the servers of the data center may provide computing and data storage retrieval resources and the servers of the data center may provide computing and data storage retrieval resources .

The computing resources of the data centers such as the data centers and may be organized into computation nodes. One or more computation node may reside on a server such as a one of the servers . Conversely a computation node may also span multiple servers such as a plurality of the servers . Furthermore the data storage retrieval resources of the data centers such as the data centers and may be organized into storage nodes. One or more storage nodes may reside on a server such as one of the servers .

The computation nodes and the storage nodes on the data centers may enable each data center to run one or more workloads. The workloads may include the execution of one or more software applications. For example the workloads may include the execution of line of business LOB software applications. In other examples the workloads may also include database and communication applications such as a Structured Query Language SQL database or Microsoft Exchange Server as developed by the Microsoft Corporation of Redmond Wash. The workloads may further include virtual machines VM or software implementations of machines that execute program applications. In additional examples the workloads may also include virtual hard disks VHD .

In various embodiments at least one of the data centers may contain available storage resources. In some of these embodiments the available storage resources may be unfilled storage nodes that reside on at least one of the data centers. In other embodiments the available storage resources may be storage nodes that have been intentionally set aside to ensure the proper operations of the data centers. For example the available storage resources may include empty backup storage nodes for storing redundant data copies. In other examples the available storage resources include the storage capacity of computation nodes maintained by these nodes to buffer their own data for the performance of computation operations.

Thus the resources of the data center may include available storage resources and the resources of the data center may include available storage resources . The available storage resources and may include a plurality of available nodes which illustrates as example available nodes . . . where the actual total number of available nodes may vary in different embodiments. By way of example but not as a limitation available nodes and may be part of the available storage resources while available nodes and may be part of the available storage resources . In other embodiments the available nodes may reside entirely on servers of a single data center e.g. servers of the data center .

As shown in erasure coded storage aggregation may be implemented to backup data stored in the storage resources of a data center into one or more available storage resources. For example such techniques may be used to backup a data file that is stored in the resources into at least some of the available nodes .

In various embodiments erasure coded storage aggregation may be implemented based on the use of erasure coding. Erasure coding involves the breaking up of a data file such as the data file into k subset of fragments. The k subset of fragments is further transformed with the mathematical generation and incorporation of new fragments to produce N fragments. For example but not as a limitation a subset k of 100 fragments may be transformed into 105 fragments where N 105. The transformation of k subset of fragments into N fragments is such that the original data file such as the data file may be reconstructed from any subset of N fragments that has the same size as the k subset. Thus referring to the example above each of the any subset of N fragments would have 100 fragments. In various embodiments the N fragments may be further stored at different locations e.g. different nodes within the same data center different nodes on different data centers etc. .

Thus erasure coded storage aggregation may offer robust storage and backup of data as an N k subset of fragments may be lost without affecting the integrity of the original data. That is since each of the N fragments may be stored on different nodes the failure of some number of the nodes e.g. N k nodes may not impact the integrity of the original data file. This robustness is due to the fact that erasure coding enables the reconstruction of the original data file using any subset of N fragments that has the same size as the k subset. Moreover a higher N to k ratio may increases the robustness of the storage i.e. may create a higher recovery rate as this means that more erasure coded fragments are available for the reconstruction of the data file.

In certain embodiments the erasure coding of a data file such as the data file may be performed iteratively by a technique known as hierarchal erasure coding. In hierarchal erasure coding each of the N fragments e.g. fragments N N of the original data file as described above may be further processed in the same manner as the original data file. For example a fragment N may be further broken down into m subset of fragments. The m subset of fragments is subsequently transformed to produce P fragments. Similar to describe above the transformation of m subset of fragments into P fragments is such that the original fragment Nmay be reconstructed from any subset of P fragments that has the same size as the m subset. Accordingly rather than storing the N fragments in different nodes the fragments e.g. P fragments derived from each of the fragments N Nmay be stored in a different node. Thus it will be appreciated that the descriptions herein related to the use and benefits of erasure coding may also be applicable to hierarchal erasure coding .

In the various embodiments of backing up data using erasure coded storage aggregation at least one data file e.g. the data file stored in the resources of the data center may be received by a front end . In such embodiments the front end may include hardware and or software e.g. APIs that selects and transfers data files from the storage nodes of the data centers for backup.

The front end may subsequently contact an allocation coordinator to select nodes from the available nodes such as the available nodes to store the erasure coding fragments. The allocation coordinator may be an application that is part of a metadata server that is responsible for file indexing operations. This metadata server may reside in one of the data centers although it need not. The allocation coordinator may use one or more factors to determine the quantity of nodes to be selected from the available nodes as well as select nodes from the available nodes. These factors may include the size of the data file possibly amongst other factors. For example a large data file e.g. a 256 gigabyte file is generally broken down into more fragments than is a smaller data file e.g. a 128 gigabyte file . Therefore a greater number of available nodes are used for a large file than a smaller file. Further the quantity of available nodes used to store the erasure coding fragment may also depend on the amount of storage capacity on each available node. For example since the erasure coding generally breaks down a data file into equal size fragments the available node with the lowest storage capacity may limit the size of each fragment which may in turn determine the number of available nodes desirable to perform erasure coded aggregation storage of the data file.

The quantity of available nodes used to store the erasure coding fragment may further depend on a predetermined fraction of the N fragments used for the reconstruction of the data file . For example given that a data file is divided into k fragments a larger N may mean that the backup of the data file is more robust when compared to a smaller N as the pool of available nodes that may be accessed to reconstruct the data file is larger. However a larger N also means that more available nodes are used to store the backup for the data file . In various embodiments a user may designate the N to k ratio using a user interface.

Furthermore the factors may also include the estimated availability of each available node. For example if a computation node frequently uses its own storage capacity e.g. allocated hard disk space the allocation coordinator may determine that the computation node is not suitable for storing one or more backup fragments if such frequency is above a predetermined frequency threshold. In various embodiments the predetermined frequency threshold may be the number of times in a given time period that the data file is accessed e.g. a preset number of times per day per week or per month etc. .

The factors may also include the estimated health status e.g. reliability of each available node as well as the proximity of each available node to other available node. For example the allocation coordinator may only select a predetermined number of available nodes from a single server or a single data center so as to reduce or eliminate the risk of simultaneous failure of an unacceptable number of nodes. In this way the allocation coordinator may provide geo redundancy to the data file.

In some embodiments the allocation coordinator may select each node such as the available node to store a plurality of erasure coded fragments. While this reduces the robustness of the erasure coded aggregation storage this may also beneficially reduce the number of available nodes used. For example the allocation coordinator may implement the storage of a plurality of erasure coded fragment on a single selected node when there are an insufficient number of available nodes for carrying out one to one storage of the fragments.

Following the selection of nodes from the available nodes for storing the erasure coding fragments the allocation coordinator may cause an erasure coder to perform erasure coding on the data file such as the data file . In some embodiments the erasure coder may be a single application that executes on a single server e.g. one of the servers . In alternative embodiments the erasure coder may be a distributed application that performs erasure coding of the data file simultaneously on a plurality of servers. Here the servers may be located in a single data center e.g. data center or across a plurality of data centers e.g. data centers . The erasure coder may produce a plurality of erasure coded fragments from the data file which are illustrated in as fragments . However the actual number of fragments may correspond to the number of nodes selected by the allocation coordinator from the available nodes.

When the erasure coder has performed the erasure code fragmentation of the data file such as the data file the allocation coordinator may contact one or more aggregation agents . The one or more aggregation agents may be background service applications that enable the allocation coordinator to distribute the erasure coded fragment to the selected nodes for storage e.g. nodes as well as facilitate the retrieval of the erasure coded fragments . As further described below the one or more aggregation agents may also calculate file integrity validation values such as checksum values for each of the erasure coded fragments.

The allocation coordinator may keep track of the distribution of the erasure coded fragments to the selected nodes e.g. some of available nodes . In various embodiments the allocation coordinator may maintain metadata regarding the addresses of the nodes that received the erasure coded fragments one or more locations of the erasure coded fragments in each node and the number of fragments stored on each node. Moreover the allocation coordinator may further maintain metadata regarding the current estimated availability and health of each node as well other pertinent information that enables the allocation coordinator to eventually retrieve a sufficient number of erasure coded fragments to reconstruct the data file such as the data file .

The allocation coordinator may also deal with contingencies that may arise during the distribution of the erasure coded fragments to the selected nodes. In some scenarios the one or more selected nodes may become unavailable during the distribution process. For example one or more aggregation agents associated with the selected nodes e.g. some of the available nodes may fail to respond to requests to store one or more of the erasure coded fragments within a predetermined time period. In another example the one or more aggregation agents may report to the allocation coordinator that the one or more selected nodes lack the requisite storage capacity. Accordingly the allocation coordinator may assign the one or more affected erasure coded fragments to alternative nodes of the available nodes and or selected nodes of the available nodes that already store erasure coded fragments. The allocation coordinator may also update its metadata according to the changes in the distribution of the erasure coded fragments .

In addition the allocation coordinator may also monitor the erasure coded fragments of a data file such as the data file which are stored on the selected nodes. In various embodiments the allocation coordinator may calculate a checksum value e.g. a hash value for each erasure coded fragment generated by the erasure coder . The allocation coordinator may store the checksum values in its metadata. Following the distribution of the erasure coded fragments to the selected nodes of the available nodes the allocation coordinator may ask the respective aggregation agents to calculate and return a new checksum value for each of the erasure coded fragments .

In such embodiments the allocation coordinator may compare the returned checksum value for each of the erasure coded fragments against its own calculated checksum values. Thus an erasure coded fragment e.g. the fragment may be deemed by the allocation coordinator to have a valid status when the checksum values match. Conversely the allocation coordinator may deem the status of an erasure coded fragment as invalid for any other response to its checksum queries. For example the allocation coordinator may not receive a checksum value for an erasure coded fragment within a predetermined time period. In another example the allocation coordinator may receive a checksum value for an erasure coded fragment e.g. the fragment that does not match the original check sum value computed by the allocation coordinator . In each of these instances the allocation coordinator may deem invalid the erasure coded fragment.

For each instance where an invalid status for an erasure coded fragment is found the allocation coordinator may command the responsible aggregation agent to request a repair by the node storing invalid erasure coded fragment. For example the responsible aggregation agent may request an error check and correction e.g. a parity check and data recovery by the storing node. Following repair the aggregation agent may return another checksum value for the erasure coded fragment so that the allocation coordination may once again compare the checksum values for the fragment.

Nevertheless if the number of invalid erasure coded fragments for a data file e.g. data file reaches a predetermined minimum threshold value the allocation coordinator may initiate a reconstruction of the data file by gathering a sufficient number of the remaining valid erasure coded fragments from the respective storing nodes. The predetermined minimum threshold value may be a number that is determined based on factors such as the current estimated availability of the nodes that are storing the erasure coded fragments of the data file the health status of such nodes the proportion of the nodes that are storing more than one erasure coded fragments the k to N ratio for the erasure coded fragments of the data file and or the like. Following the reconstruction of the data file e.g. the data file the allocation coordinator may once again repeat the selection of available nodes and use the erasure coder and the aggregation agents to implement erasure coded storage aggregation using the selected nodes.

As further shown in erasure coded storage aggregation may also be implemented to backup or store data such as data file received from the client . In such embodiments the front end may include hardware and or software that interact with the client to receive the data file . In some embodiments the front end may include components that determine whether the data file is suited for erasure coded storage. For example data files that are accessed frequently e.g. a continuously updated word processing document may be ill suited for erasure coded storage in some instances. This lack of suitability may be at least due in part to the time delay and computing resources expended to fragment and reconstruct data files which may hinder quick and convenient access to the document. On the other hand archived data files e.g. old emails or retained business records may be especially suited for erasure coded storage. This suitability may be due at least to the fact that such data files are infrequently accessed and thus that the time and computing costs associated with fragmentation and reconstruction of such data files is not a significant consideration or hindrance.

Accordingly the front end may include hardware and or software components that query a user of a data file e.g. the data file to determine the frequency that the data file is likely to be accessed in the future. If this estimated frequency is below a predetermined frequency threshold the front end may trigger the erasure coding of the data file. In other embodiments the front end may predict the likely future access frequency based on the frequency of past access activities on the data file. In various embodiments the predetermined frequency threshold may be the number of times that the data file is accessed in a given time period e.g. a preset number of times per day per week per month etc. . In additional embodiments the front end may determine whether to erasure code store a data file e.g. data file based on data file type. For example a user may have designated to the front end that email data files are to be erasure coded for storage. Thus in at least one embodiment with respect to the data files that are not suited and or designated for erasure coding storage the front end may pass each data file intact for storage at a storage node e.g. storage in the resources of a suitable data center such as the data center .

However with respect to data files e.g. data file that are suitable and or designated for erasure coding storage the front end may interact with the allocation coordinator the erasure coder and the aggregation agents to erasure code the data file into fragments such as the fragments for storage on at least some of the available nodes . The allocation coordinator may also monitor and maintain the erasure coded fragments of the data file as described above with respect to data file .

In further embodiments the front end may facilitate reconstruction and retrieval of data files that are stored as erasure coded fragments such as the fragments that are stored on the selected nodes such as available nodes . In such embodiments the front end may receive a request to retrieve a data file. The request may be from a client such as the client . The request may also be from a storage node of a data center such as the data center when the storage node is unable to locate the original data file .

The front end may pass the request to the allocation coordinator . The allocation coordinator may consult its metadata to locate the nodes such as at least some of the available nodes that are storing the corresponding erasure coded fragments. The allocation coordinator may further command the erasure coder to contact the aggregation agents responsible for the located nodes. In turn at least some of the aggregation agents may return erasure coded fragments of the requested data file. When a sufficient number erasure coded fragments have been received by the erasure coder the erasure coder may reconstruct the data file from the erasure coded fragments. The erasure coder may further pass the reconstructed data file to the allocation coordinator . The allocation coordinator may note the successful reconstruction of the data file in its metadata and pass the reconstructed data file to the front end . The front end may subsequently deliver the reconstructed data file to the requesting client e.g. the client or the requesting storage node.

The front end may also include hardware and or software components that monitor the access and or actual access frequency of each data file e.g. the data file that is stored as erasure coded fragments e.g. the fragments on the selected nodes such as available nodes . Further the front end may initiate the reconstruction of the data file from the erasure coded fragments based on an access request or the frequency of access.

In some embodiments when the front end receives a request to access the data file the front end may dynamically retrieve a sufficient number of the erasure coded fragments from storage in the available nodes to reconstruct the data file. In various embodiments the front end may complete the retrieval and reconstruction with assistance from the allocation coordinator the erasure coder and the aggregation agents as described above.

The front end may also transfer the reconstructed data file for intact storage at a storage node e.g. storage in the resources of a suitable data center such as the data center . In this way further access to the data file may be achieved directly from the storage node without time delay that may be associated with reconstructing the data file from erasure coded fragments.

In other embodiments if the actual access frequency of a data file meets or is above a predetermined frequency threshold the front end may dynamically retrieve a sufficient number of the erasure coded fragments from storage in the available nodes to reconstruct the data file. In various embodiments the predetermined frequency threshold may be the number of times in a given time period that the data file is accessed e.g. a preset number of times per day per week per month etc. . The front end may complete the retrieval and reconstruction with assistance from the allocation coordinator the erasure coder and the aggregation agents as described above.

The front end may further transfer the reconstructed data file for storage at a storage node e.g. storage in the resources of a suitable data center such as the data center . Thus as described above further access to the data file may be achieved directly from the storage node without any reconstruction related time delay.

However if the actual access frequency of a data file subsequently falls below the predetermined frequency threshold the front end may once again trigger the erasure coding of the data file. Accordingly the front end may monitor and transit each of one or more data files e.g. the data file from an intact state to a erasure coded fragments state and vice versa depending on the access pattern.

It will be appreciated that while erasure coded storage aggregation is described above with respect to data centers the techniques of erasure coded storage aggregation is application to any plurality of data centers that provide computation nodes and storage nodes. Thus the description of erasure coded storage aggregation with respect to data centers is illustrative rather than limiting.

The components may be stored in the memory . The memory may include volatile and or nonvolatile memory removable and or non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Such memory may include but is not limited to random accessory memory RAM read only memory ROM electrically erasable programmable read only memory EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices RAID storage systems or any other medium which can be used to store the desired information and is accessible by a computer system. Further the components may be in the form of routines programs objects and data structures that cause the performance of particular tasks or implement particular abstract data types.

In various embodiments the components may include the front end interface the allocation coordinator the erasure coder and the aggregation agents . The front end interface may be the software portion of the front end and may receive data files from the client e.g. client as well provide retrieved data files to the client. Moreover the front end interface may select and transfer data files e.g. data file from the storage nodes of the data centers such as the data center . The front end interface may be a single instance program that executes on a single server such as one of the servers .

The allocation coordinator meanwhile may be a single instance program that executes on a single server e.g. a metadata server or a distributed program that simultaneously executes on multiple servers. In some embodiments the multiple servers may be located in different data centers. The allocation coordinator may select available nodes such as computation nodes and or storage nodes for the storage of the erasure coded fragments of data files. For example the allocation coordinator may select nodes that correspond to available storage resources . Further the allocation coordinator may monitor the status of the erasure coded fragments as well as initiate data file recovery and retrieval when appropriate. In various embodiments the allocation coordinator may store metadata on the various available nodes selected nodes and erasure coded fragments in a metadata database that resides on a portion of the memory .

The erasure coder may be a single instance program that executes on a single server or a distributed program that simultaneously executes on multiple servers. The erasure coder may perform erasure coding or hierarchical erasure coding on data files e.g. the data file . Further the erasure coder may also reconstruct data files from erasure coded fragments.

The aggregation agents may be background service applications that interface with the nodes that reside on the servers such as the servers . The aggregation agents may enable erasure coded fragments to be placed in the storage nodes as well as enable erasure coded fragments to be placed in the computation nodes without interfering with the operations of the computation nodes.

As shown in the placement of the one or more aggregation agents for the performance of such background services may be described with respect to data center architecture . As shown the data center architecture typically includes by order of stacking a hardware layer a driver layer a root operating system OS a virtual machine VM scheduler and applications .

In some embodiments the one or more aggregation agents may reside on the driver layer that runs on top of the hardware layer . In other embodiments the one or more aggregation agents may reside in the root operating system OS . In still other embodiments the one or more aggregation agents may be implemented in the VM scheduler .

However in an erasure coded storage aggregation scenario the same level of data integrity may be achieved without the use of triplicates. For example the erasure coding each data file stored in the example storage node may result in N fragments that are only 5 more numerous than the k fragments. Thus assuming that all of the N fragments of each data file are stored on the example storage node the effective storage capacity of the example storage node is 95 . As shown portion may represents the 95 storage of k fragments while portion may represent the 5 of N k fragments used by erasure coded storage aggregation to achieve data redundancy. Nevertheless it will be appreciated that in actual implementation the erasure coded fragments are stored on different storage nodes to maximize robustness. Thus the scenario shown in is only intended to illustrate the significant advantages of erasure coded storage aggregation over triplicate data storage.

However with the implementation of erasure coded storage aggregation n scenario the allocation coordinator may may safely store one or more erasure coded fragments such as the one or more of the fragments in the storage capacity portion . As stated above an original data file such as the data file may be reconstructed from any subset of N fragments that has the same size as the k subset. Thus the loss of the one or more fragments stored in the storage capacity portion may not impact the overall integrity of the data file as long as a sufficient number of erasure coded fragments is available in other nodes. In this way the implementation of erasure coded storage aggregation may increase utilization of previously unavailable storage resources.

At block the front end may receive a storage request for a data file e.g. data file or the data file . In some embodiments the storage request may be from a client . In other embodiments the storage request may be from a backup service application that is responsible for backing up a storage node or a computation node.

At block the front end may determine a storage option for the data file. In various embodiments the front end may determine the storage option based on the frequency of past access activities on the data file predicted future access frequency or user storage designation related to the type of the data file. For example a user may designate data files of one or more types for erasure coded storage.

At decision block if the front end determines that the data file should not be stored as erasure coded data no at decision block the process may proceed to block .

At block the front end may store the data file in a data center without using erasure coded storage. Rather the data file may be stored intact. In various embodiments the front end may store the data file in a storage node that uses traditional data redundancy techniques e.g. RAIDS redundant array . At block the front end may update the allocation coordinator with the storage status of the data file. The allocation coordinator may store the status of the data file as metadata.

Returning to decision block if the front end determines that the data file should be stored as erasure coded data yes at decision block the process may proceed to block .

At block the allocation coordinator may determine the quantity of erasure coded fragments to effectively store the data file based on a plurality of factors as described above. The allocation coordinator may then selected the desired nodes from a plurality of available nodes to store erasure coded fragments e.g. the fragments of the data file. In various embodiments the selected nodes may include storage nodes and or computation nodes and the nodes may reside in one or more data centers.

At block the allocation coordinator may command the erasure coder to erasure code the data file into the determined number of erasure coded fragments. In various embodiments as described above the erasure coding may also include hierarchical erasure coding. Following erasure coding of the data file the allocation coordinator may then activate one or more aggregation agents to store the erasure coded fragments on the selected nodes e.g. at least some of the nodes . In various embodiments if any of the selected nodes should fail to store its assigned erasure coded fragment the corresponding aggregation agent may report back to the allocation coordinator so that the allocation coordinator may select alternative nodes for storing the affected erasure coded fragment.

At block the allocation coordinator may store metadata on the status of the erasure coded fragments. In various embodiments the status of the erasure coded fragments may include the locations of the nodes that store the erasure coded fragments the size of the erasure coded fragments and the availability and health of the nodes etc.

At block the allocation coordinator may monitor and safeguard the integrity of the erasure coded fragments that are stored on the nodes. In various embodiments the allocation monitor may periodically query the nodes regarding the integrity of the erasure coded fragments. In some embodiments the allocation coordinator may initiate data recovery by activating the corresponding node to repair the one or more affected erasure coded fragments if the integrity of the one or more fragments is compromised. In other embodiments the allocation coordinator may initiate recovery by reconstructing the data file create new erasure coded fragments using the reconstructed data file and redistribute erasure coded fragments to newly selected nodes.

At block the front end may receive a retrieval request for a data file. In some embodiments the retrieval request may be from a client . In other embodiments the retrieval request may be from a backup service application that is responsible for backing up a storage node or a computation node.

At block the front end may determine a status of the data file based on the metadata store by the allocation coordinator . In various embodiments the status of the data file may include whether the data file is stored as erasure coded fragments the location where the data file is stored and the like.

At decision block if the front end determines that the data file is not stored as erasure coded fragments no at decision block the process may proceed to block .

At block the front end may retrieve an intact copy of the data file from a storage node that uses traditional data redundancy techniques e.g. RAIDS redundant array . At block the front end may provide the retrieved data file to a requestor such as the client .

Returning to decision block if the front end determines that the data file is stored as erasure coded fragments yes at decision block the process may proceed to block .

At block the allocation coordinator may send requests to the aggregation agents to retrieve at least some of the erasure coded fragments e.g. the fragments of the data file. Once a sufficient number of the erasure coded fragments for assembling the data file is retrieved the allocation coordinator may terminate erasure coded fragment retrieval. Subsequently the erasure coder may reconstruct the data file using the erasure coded fragments.

At block the allocation coordinator may use the front end to provide the reconstructed data file to a requestor such as the client or the backup service application for the storage node or the computation node.

At block the front end may store a data file e.g. the data file as erasure coded fragments such as the fragments . The fragments may be stored in a plurality of selected nodes such as available nodes . In various embodiments the front end may store the data file as erasure coded fragments with assistance from the allocation coordinator the erasure coder and the aggregation agents as described above.

At block the front end may monitor the access frequency of the data file. In various embodiments the predetermined frequency threshold may be the number of times in a given time period that the data file is accessed e.g. a preset number of times per day per week per month etc. .

At decision block the front end may determine whether the access frequency of the data file meets or exceeds a predetermined frequency threshold. If the front end determines that the access frequency of the data file does not meet or exceed the predetermined frequency threshold no at decision block the process may loop back to block so that the access frequency of the data file may continue to be monitored.

However if the front end determines that the access frequency of the data file does meet or exceed the predetermined frequency threshold yes at decision block the process may proceed to block .

At block the front end may dynamically retrieve a sufficient number of the erasure coded fragments from storage in the available nodes and reconstruct the data file. In various embodiments the front end may complete the retrieval and reconstruction with assistance from the allocation coordinator the erasure coder and the aggregation agents as described above. Further the front end may transfer the reconstructed data file for storage at a storage node e.g. storage in the resources of a suitable data center such as the data center . At block the front end may continue to monitor the access frequency of the data file.

At decision block the front end may determine whether the access frequency of the data file falls below the predetermined frequency threshold. If the front end determines that the access frequency of the data file does fall below the predetermined frequency threshold yes at decision block the process may loop back to block . Upon returning to block the front end may once again erasure code the data file for storage in the plurality of selected nodes.

However if the front end determines that the access frequency of the data file is not below the predetermined frequency threshold no at decision block the process may loop back to block so that the access frequency of the data file may continue to be monitored.

In at least one configuration computing device typically includes at least one processing unit and system memory . Depending on the exact configuration and type of computing device system memory may be volatile such as RAM non volatile such as ROM flash memory etc. or some combination thereof. System memory may include an operating system one or more program modules and may include program data . The operating system includes a component based framework that supports components including properties and events objects inheritance polymorphism reflection and provides an object oriented component based application programming interface API such as but by no means limited to that of the .NET Framework manufactured by the Microsoft Corporation Redmond Wash. The computing device is of a very basic configuration demarcated by a dashed line . Again a terminal may have fewer components but may interact with a computing device that may have such a basic configuration.

Computing device may have additional features or functionality. For example computing device may also include additional data storage devices removable and or non removable such as for example magnetic disks optical disks or tape. Such additional storage is illustrated in by removable storage and non removable storage . Computer storage media may include volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. System memory removable storage and non removable storage are all examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by Computing device . Any such computer storage media may be part of device . Computing device may also have input device s such as keyboard mouse pen voice input device touch input device etc. Output device s such as a display speakers printer etc. may also be included.

Computing device may also contain communication connections that allow the device to communicate with other computing devices such as over a network. These networks may include wired networks as well as wireless networks. Communication connections are some examples of communication media. Communication media may typically be embodied by computer readable instructions data structures program modules etc.

It is appreciated that the illustrated computing device is only one example of a suitable device and is not intended to suggest any limitation as to the scope of use or functionality of the various embodiments described. Other well known computing devices systems environments and or configurations that may be suitable for use with the embodiments include but are not limited to personal computers server computers hand held or laptop devices multiprocessor systems microprocessor base systems set top boxes game consoles programmable consumer electronics network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and or the like.

The implementation of erasure coded storage aggregation on one or more data centers may reduce the amount storage capacity or number of storage nodes used to offer data redundancy for guarding against data loss due to hardware and or software failure. Further the implementation of erasure coded storage aggregation may also enable some of the computation nodes of a data center to provide storage capacity for storing third party data while simultaneously maintaining the ability to buffer their own data for the performance of computation operations. In this way the implementation of erasure coded storage aggregative may increase the storage capacity of one or more existing data centers without the addition of servers switches and or other hardware infrastructure.

In closing although the various embodiments have been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended representations is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as exemplary forms of implementing the claimed subject matter.

