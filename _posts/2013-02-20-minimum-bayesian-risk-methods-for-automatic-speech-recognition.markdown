---

title: Minimum bayesian risk methods for automatic speech recognition
abstract: A hypothesis space of a search graph may be determined. The hypothesis space may include n hypothesis-space transcriptions of an utterance, each selected from a search graph that includes t>n transcriptions of the utterance. An evidence space of the search graph may also be determined. The evidence space may include m evidence-space transcriptions of the utterance that are randomly selected from the search graph, where t>m. For each particular hypothesis-space transcription in the hypothesis space, an expected word error rate may be calculated by comparing the particular hypothesis-space transcription to each of the evidence-space transcriptions. Based on the expected word error rates, a lowest expected word error rate may be obtained, and the particular hypothesis-space transcription that is associated with the lowest expected word error rate may be provided.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09123333&OS=09123333&RS=09123333
owner: Google Inc.
number: 09123333
owner_city: Mountain View
owner_country: US
publication_date: 20130220
---
This application is entitled to the benefit of provisional U.S. patent application Ser. No. 61 700 098 which is hereby incorporated by reference in its entirety.

Search graphs or lattices in automatic speech recognition ASR systems can be used to map an input utterance to one or more output text strings. Some implementations attempt to select text string s that have a minimal probability of having one or more word errors. It may be more accurate to instead select text strings that have the smallest expected word error rate e.g. the smallest edit distance to a correct transcription of the utterance. However doing so can be computationally expensive.

In a first example embodiment n hypothesis space transcriptions of an utterance may be selected from a search graph that includes t n transcriptions of the utterance. Additionally m evidence space transcriptions of the utterance may also be randomly selected from the search graph where t m. For particular hypothesis space transcriptions an expected word error rate may be calculated by comparing the particular hypothesis space transcription to each of the evidence space transcriptions. Based on the expected word error rates a lowest expected word error rate may be obtained and the particular hypothesis space transcription that is associated with the lowest expected word error rate may be provided.

A second example embodiment may include a non transitory computer readable storage medium having stored thereon program instructions that upon execution by a computing device cause the computing device to perform operations in accordance with the first and or second example embodiments.

A third example embodiment may include a computing device comprising at least a processor and data storage. The data storage may contain program instructions that upon execution by the processor operate in accordance with the first and or second example embodiments.

These as well as other aspects advantages and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description with reference where appropriate to the accompanying drawings. Further it should be understood that the description provided in this summary section and elsewhere in this document is intended to illustrate the claimed subject matter by way of example and not by way of limitation.

One method of selecting a transcription of an utterance is to use a maximum a posteriori MAP technique. Formally given an utterance x MAP techniques attempt to find a transcription y that maximizes P y x the probability that the transcription is correct given the utterance.

However the MAP technique selects a transcription that minimizes the likelihood of transcription level error. This transcription may be referred to as y. For example if yis a sentence the MAP technique minimizes the probability of yhaving at least one word error.

A potentially more accurate metric of ASR performance is word error rate WER . The WER between a particular transcription and the correct transcription y may be defined as the number of edits substitutions deletions or insertions needed to change the particular transcription into y divided by the number of words in y . This number of edits may also be referred to as the Levenshtein edit distance.

Minimizing WER rather than transcription level error is motivated by the observation that some transcription level errors matter more than others. For example the best transcription as given by the MAP technique may have more word errors when compared to a correct transcription of the utterance than other possible transcriptions with lower probabilities than y. Put another way WER gives partial credit for correctly recognizing portions of sentences whereas MAP techniques generally do not.

Minimum Bayesian Risk MBR techniques can be used to take WER or a more general loss function into consideration when selecting a transcription. MBR techniques can be tuned to select a transcription that is optimal with respect to a loss function that represents a cost when the ASR system erroneously produces a transcription y instead of y . If this loss function is the Levenshtein edit distance then the MBR technique selects a transcription that is optimal with respect to WER.

Nonetheless determining a transcription using the MBR technique is expensive compared to using the MAP technique. For instance a sum of weighted probabilities over the entire language is calculated. Also each pair of possible transcriptions in the language is compared and the loss function between them is determined. For the Levenshtein edit distance the running time of the loss function over strings y and z is O y z .

One way of improving the running time of the MBR technique is to limit the search graph. The embodiments herein disclose several ways of doing so and include a number of ways of representing the search graph that may facilitate efficient implementations.

The above processes and example embodiments thereof will be described in detail in Sections 5 6 and 7. However in order to further embody possible ASR system implementations the next four sections describe respectively example computing systems and devices that may support ASR systems an overview of ASR system components and functions an overview of ASR system operation and an overview of speaker adaptation.

The methods devices and systems described herein can be implemented using client devices and or so called cloud based server devices. Under various aspects of this paradigm client devices such as mobile phones tablet computers and or desktop computers may offload some processing and storage responsibilities to remote server devices. At least some of the time these client services are able to communicate via a network such as the Internet with the server devices. As a result applications that operate on the client devices may also have a persistent server based component. Nonetheless it should be noted that at least some of the methods processes and techniques disclosed herein may be able to operate entirely on a client device or a server device.

Furthermore the server devices described herein may not necessarily be associated with a client server architecture and therefore may also be referred to as computing devices. Similarly the client devices described herein also may not necessarily be associated with a client server architecture and therefore may be interchangeably referred to as user devices. In some contexts client devices may also be referred to as computing devices. 

This section describes general system and device architectures for such client devices and server devices. However the methods devices and systems presented in the subsequent sections may operate under different paradigms as well. Thus the embodiments of this section are merely examples of how these methods devices and systems can be enabled.

Network may be for example the Internet or some other form of public or private Internet Protocol IP network. Thus client devices and may communicate using packet switching technologies. Nonetheless network may also incorporate at least some circuit switching technologies and client devices and may communicate via circuit switching alternatively or in addition to packet switching.

A server device may also communicate via network . Particularly server device may communicate with client devices and according to one or more network protocols and or application level protocols to facilitate the use of network based or cloud based computing on these client devices. Server device may include integrated data storage e.g. memory disk drives etc. and may also be able to access a separate server data storage . Communication between server device and server data storage may be direct via network or both direct and via network as illustrated in . Server data storage may store application data that is used to facilitate the operations of applications performed by client devices and and server device .

Although only three client devices one server device and one server data storage are shown in communication system may include any number of each of these components. For instance communication system may comprise millions of client devices thousands of server devices and or thousands of server data storages. Furthermore client devices may take on forms other than those in .

User interface may comprise user input devices such as a keyboard a keypad a touch screen a computer mouse a track ball a joystick and or other similar devices now known or later developed. User interface may also comprise user display devices such as one or more cathode ray tubes CRT liquid crystal displays LCD light emitting diodes LEDs displays using digital light processing DLP technology printers light bulbs and or other similar devices now known or later developed. Additionally user interface may be configured to generate audible output s via a speaker speaker jack audio output port audio output device earphones and or other similar devices now known or later developed. In some embodiments user interface may include software circuitry or another form of logic that can transmit data to and or receive data from external user input output devices.

Communication interface may include one or more wireless interfaces and or wireline interfaces that are configurable to communicate via a network such as network shown in . The wireless interfaces if present may include one or more wireless transceivers such as a BLUETOOTH transceiver a Wifi transceiver perhaps operating in accordance with an IEEE 802.11 standard e.g. 802.11b 802.11g 802.11n a WiMAX transceiver perhaps operating in accordance with an IEEE 802.16 standard a Long Term Evolution LTE transceiver perhaps operating in accordance with a 3rd Generation Partnership Project 3GPP standard and or other types of wireless transceivers configurable to communicate via local area or wide area wireless networks. The wireline interfaces if present may include one or more wireline transceivers such as an Ethernet transceiver a Universal Serial Bus USB transceiver or a similar transceiver configurable to communicate via a twisted pair wire a coaxial cable a fiber optic link or other physical connection to a wireline device or network.

Processor may include one or more general purpose processors e.g. microprocessors and or one or more special purpose processors e.g. digital signal processors DSPs graphical processing units GPUs floating point processing units FPUs network processors or application specific integrated circuits ASICs . Processor may be configured to execute computer readable program instructions that are contained in data storage and or other instructions to carry out various functions described herein.

Thus data storage may include one or more non transitory computer readable storage media that can be read or accessed by processor . The one or more computer readable storage media may include volatile and or non volatile storage components such as optical magnetic organic or other memory or disc storage which can be integrated in whole or in part with processor . In some embodiments data storage may be implemented using a single physical device e.g. one optical magnetic organic or other memory or disc storage unit while in other embodiments data storage may be implemented using two or more physical devices.

Data storage may also include program data that can be used by processor to carry out functions described herein. In some embodiments data storage may include or have access to additional data storage components or devices e.g. cluster data storages described below .

Server device and server data storage device may store applications and application data at one or more places accessible via network . These places may be data centers containing numerous servers and storage devices. The exact physical location connectivity and configuration of server device and server data storage device may be unknown and or unimportant to client devices. Accordingly server device and server data storage device may be referred to as cloud based devices that are housed at various remote locations. One possible advantage of such cloud based computing is to offload processing and data storage from client devices thereby simplifying the design and requirements of these client devices.

In some embodiments server device and server data storage device may be a single computing device residing in a single data center. In other embodiments server device and server data storage device may include multiple computing devices in a data center or even multiple computing devices in multiple data centers where the data centers are located in diverse geographic locations. For example depicts each of server device and server data storage device potentially residing in a different physical location.

In some embodiments each of the server clusters A B and C may have an equal number of server devices an equal number of cluster data storages and an equal number of cluster routers. In other embodiments however some or all of the server clusters A B and C may have different numbers of server devices different numbers of cluster data storages and or different numbers of cluster routers. The number of server devices cluster data storages and cluster routers in each server cluster may depend on the computing task s and or applications assigned to each server cluster.

In the server cluster A for example server devices A can be configured to perform various computing tasks of server device . In one embodiment these computing tasks can be distributed among one or more of server devices A. Server devices B and C in server clusters B and C may be configured the same or similarly to server devices A in server cluster A. On the other hand in some embodiments server devices A B and C each may be configured to perform different functions. For example server devices A may be configured to perform one or more functions of server device and server devices B and server device C may be configured to perform functions of one or more other server devices. Similarly the functions of server data storage device can be dedicated to a single server cluster or spread across multiple server clusters.

Cluster data storages A B and C of the server clusters A B and C respectively may be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives. The disk array controllers alone or in conjunction with their respective server devices may also be configured to manage backup or redundant copies of the data stored in cluster data storages to protect against disk drive failures or other types of failures that prevent one or more server devices from accessing one or more cluster data storages.

Similar to the manner in which the functions of server device and server data storage device can be distributed across server clusters A B and C various active portions and or backup redundant portions of these components can be distributed across cluster data storages A B and C. For example some cluster data storages A B and C may be configured to store backup versions of data stored in other cluster data storages A B and C.

Cluster routers A B and C in server clusters A B and C respectively may include networking equipment configured to provide internal and external communications for the server clusters. For example cluster routers A in server cluster A may include one or more packet switching and or routing devices configured to provide i network communications between server devices A and cluster data storage A via cluster network A and or ii network communications between the server cluster A and other devices via communication link A to network . Cluster routers B and C may include network equipment similar to cluster routers A and cluster routers B and C may perform networking functions for server clusters B and C that cluster routers A perform for server cluster A.

Additionally the configuration of cluster routers A B and C can be based at least in part on the data communication requirements of the server devices and cluster storage arrays the data communications capabilities of the network equipment in the cluster routers A B and C the latency and throughput of the local cluster networks A B C the latency throughput and cost of the wide area network connections A B and C and or other factors that may contribute to the cost speed fault tolerance resiliency efficiency and or other design goals of the system architecture.

As shown in client device may include a communication interface a user interface a processor and data storage all of which may be communicatively linked together by a system bus network or other connection mechanism .

Communication interface functions to allow client device to communicate using analog or digital modulation with other devices access networks and or transport networks. Thus communication interface may facilitate circuit switched and or packet switched communication such as POTS communication and or IP or other packetized communication. For instance communication interface may include a chipset and antenna arranged for wireless communication with a radio access network or an access point. Also communication interface may take the form of a wireline interface such as an Ethernet Token Ring or USB port. Communication interface may also take the form of a wireless interface such as a Wifi BLUETOOTH global positioning system GPS or wide area wireless interface e.g. WiMAX or LTE . However other forms of physical layer interfaces and other types of standard or proprietary communication protocols may be used over communication interface . Furthermore communication interface may comprise multiple physical communication interfaces e.g. a Wifi interface a BLUETOOTH interface and a wide area wireless interface .

User interface may function to allow client device to interact with a human or non human user such as to receive input from a user and to provide output to the user. Thus user interface may include input components such as a keypad keyboard touch sensitive or presence sensitive panel computer mouse trackball joystick microphone still camera and or video camera. User interface may also include one or more output components such as a display screen which for example may be combined with a presence sensitive panel CRT LCD LED a display using DLP technology printer light bulb and or other similar devices now known or later developed. User interface may also be configured to generate audible output s via a speaker speaker jack audio output port audio output device earphones and or other similar devices now known or later developed. In some embodiments user interface may include software circuitry or another form of logic that can transmit data to and or receive data from external user input output devices. Additionally or alternatively client device may support remote access from another device via communication interface or via another physical interface not shown .

Processor may comprise one or more general purpose processors e.g. microprocessors and or one or more special purpose processors e.g. DSPs GPUs FPUs network processors or ASICs . Data storage may include one or more volatile and or non volatile storage components such as magnetic optical flash or organic storage and may be integrated in whole or in part with processor . Data storage may include removable and or non removable components.

Processor may be capable of executing program instructions e.g. compiled or non compiled program logic and or machine code stored in data storage to carry out the various functions described herein. Therefore data storage may include a non transitory computer readable medium having stored thereon program instructions that upon execution by client device cause client device to carry out any of the methods processes or functions disclosed in this specification and or the accompanying drawings. The execution of program instructions by processor may result in processor using data .

By way of example program instructions may include an operating system e.g. an operating system kernel device driver s and or other modules and one or more application programs e.g. address book email web browsing social networking and or gaming applications installed on client device . Similarly data may include operating system data and application data . Operating system data may be accessible primarily to operating system and application data may be accessible primarily to one or more of application programs . Application data may be arranged in a file system that is visible to or hidden from a user of client device .

Application programs may communicate with operating system through one or more application programming interfaces APIs . These APIs may facilitate for instance application programs reading and or writing application data transmitting or receiving information via communication interface receiving or displaying information on user interface and so on.

In some vernaculars application programs may be referred to as apps for short. Additionally application programs may be downloadable to client device through one or more online application stores or application markets. However application programs can also be installed on client device in other ways such as via a web browser or through a physical interface e.g. a USB port on client device .

Before describing speaker adaptation in detail it may be beneficial to understand overall ASR system operation. Thus this section describes ASR systems in general including how the components of an ASR system may interact with one another in order to facilitate speech recognition and how some of these components may be trained.

It should be noted that the discussion in this section and the accompanying figures are presented for purposes of example. Other ASR system arrangements including different components different relationships between the components and or different processing may be possible.

Feature analysis module may receive utterance . This utterance may include an analog or digital representation of human speech and may possibly contain background noise as well. Feature analysis module may convert utterance to a sequence of one or more feature vectors . Each of feature vectors may include temporal and or spectral representations of the acoustic features of at least a portion of utterance . For instance a feature vector may include mel frequency cepstrum coefficients of such a portion.

The mel frequency cepstrum coefficients may represent the short term power spectrum of a portion of utterance . They may be based on for example a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency. A mel scale may be a scale of pitches subjectively perceived by listeners to be about equally distant from one another even though the actual frequencies of these pitches are not equally distant from one another. 

To derive these coefficients feature analysis module may sample and quantize utterance divide it into overlapping or non overlapping frames of s milliseconds and perform spectral analysis on the frames to derive the spectral components of each frame. Feature analysis module may further perform noise removal and convert the standard spectral coefficients to mel frequency cepstrum coefficients and then calculate first order and second order cepstral derivatives of the mel frequency cepstrum coefficients.

The first order cepstral coefficient derivatives may be calculated based on the slopes of linear regressions performed over windows of two or more consecutive frames. The second order cepstral coefficient derivatives may be calculated based on the slopes of linear regressions performed over windows of two or more consecutive sets of first order cepstral coefficient derivatives. However there may be other ways of calculating the first order and second order cepstral coefficient derivatives.

In some embodiments one or more frames of utterance may be represented by a feature vector of mel frequency cepstrum coefficients first order cepstral coefficient derivatives and second order cepstral coefficient derivatives. For example the feature vector may contain 13 coefficients 13 first order derivatives and 13 second order derivatives therefore having a length of 39. However feature vectors may use different combinations of features in other possible embodiments.

Pattern classification module may receive a sequence of feature vectors from feature analysis module and produce as output one or more text string transcriptions of utterance . Each transcription may be accompanied by a respective confidence level indicating an estimated likelihood that the transcription is correct e.g. 80 confidence 90 confidence etc. .

To produce this output pattern classification module may include or incorporate aspects of acoustic model dictionary and or language model . In some embodiments pattern classification module may also use a search graph that represents sequences of word or sub word acoustic features that appear in spoken utterances. The behavior of pattern classification module will be described below in the context of these modules.

Acoustic model may determine probabilities that a particular sequence of feature vectors were derived from a particular sequence of spoken words and or sub word sounds. This may involve mapping sequences of feature vectors to one or more phonemes and then mapping sequences of phonemes to one or more words.

A phoneme may be considered to be the smallest segment of an utterance that encompasses a meaningful contrast with other segments of utterances. Thus a word typically includes one or more phonemes. For purposes of simplicity phonemes may be thought of as utterances of letters but this is not a perfect analogy as some phonemes may present multiple letters. An example phonemic spelling for the American English pronunciation of the word cat is k ae t consisting of the phonemes k ae and t . Another example phonemic spelling for the word dog is d aw g consisting of the phonemes d aw and g .

Different phonemic alphabets exist and these alphabets may have different textual representations for the various phonemes therein. For example the letter a may be represented by the phoneme ae for the sound in cat by the phoneme ey for the sound in ate and by the phoneme ah for the sound in beta. Other phonemic representations are possible.

Common phonemic alphabets for American English contain about 40 distinct phonemes. Each of these phonemes may be associated with a different distribution of feature vector values. Thus acoustic model may be able to estimate the phoneme s in a feature vector by comparing the feature vector to the distributions for each of the 40 phonemes and finding one or more phonemes that are most likely represented by the feature vector.

One way of doing so is through use of a hidden Markov model HMM . An HMM may model a system as a Markov process with unobserved i.e. hidden states. Each HMM state may be represented as a multivariate Gaussian distribution that characterizes the statistical behavior of the state. Additionally each state may also be associated with one or more state transitions that specify the probability of making a transition from the current state to another state.

When applied to an ASR system the combination of the multivariate Gaussian distribution and the state transitions for each state may define a time sequence of feature vectors over the duration of one or more phonemes. Alternatively or additionally the HMM may model the sequences of phonemes that define words. Thus some HMM based acoustic models may also consider phoneme context when a mapping a sequence of feature vectors to one or more words.

Acoustic model may represent a word by concatenating the respective 3 state HMMs for each phoneme in the word together with appropriate transitions. These concatenations may be performed based on information in dictionary as discussed below. In some implementations more or fewer states per phoneme may be used in an acoustic model.

An acoustic model may be trained using recordings of each phoneme in numerous contexts e.g. various words and sentences so that a representation for each of the phoneme s states can be obtained. These representations may encompass the multivariate Gaussian distributions discussed above.

In order to train the acoustic model a possibly large number of utterances containing spoken phonemes may each be associated with transcriptions. These utterances may be words sentences and so on and may be obtained from recordings of everyday speech or some other source. The transcriptions may be high accuracy automatic or manual human made text strings of the utterances.

The utterances may be segmented according to their respective transcriptions. For instance training of the acoustic models may involve segmenting the spoken strings into units e.g. using either a Baum Welch and or Viterbi alignment method and then using the segmented utterances to build distributions for each phoneme state.

Consequently as more data utterances and their associated transcriptions are used for training a more accurate acoustic model is expected to be produced. However even a well trained acoustic model may have limited accuracy when used for ASR in a domain for which it was not trained. For instance if an acoustic model is trained by utterances from a number of speakers of American English this acoustic model may perform well when used for ASR of American English but may be less accurate when used for ASR of e.g. British English.

Also if an acoustic model is trained using utterances from a number of speakers it will likely end up representing each phoneme as a statistical average of the pronunciation of this phoneme across all of the speakers. Thus an acoustic model trained in this fashion may represent the pronunciation and usage of a hypothetical average speaker rather than any particular speaker.

For purposes of simplicity throughout this specification and the accompanying drawings it is assumed that acoustic models represent phonemes as context dependent phonemic sounds. However acoustic models that use other types of representations are within the scope of the embodiments herein.

As noted above dictionary may define a pre established mapping between phonemes and words. This mapping may include a list of tens or hundreds of thousands of phoneme pattern to word mappings. Thus in some embodiments dictionary may include a lookup table such as Table 1. Table 1 illustrates how dictionary may list the phonemic sequences that pattern classification module uses for the words that the ASR system is attempting to recognize. Therefore dictionary may be used when developing the phonemic state representations of words that are illustrated by acoustic model .

Language model may assign probabilities to sequences of phonemes or words based on the likelihood of that sequence of phonemes or words occurring in an input utterance to the ASR system. Thus for example language model may define the conditional probability of w the nth word in a phrase transcribed from an utterance given the values of the pattern of n 1 previous words in the phrase. More formally language model may define 

In general a language model may operate on n grams which for example may be sequences of n phonemes or words that are represented in pattern classification module . In practice language models with values of n greater than 5 are rarely used because of their computational complexity and also because smaller n grams e.g. 3 grams which are also referred to as tri grams tend to yield acceptable results. In the example described below tri grams are used for purposes of illustration. Nonetheless any value of n may be may be used with the embodiments herein.

Language models may be trained through analysis of a corpus of text strings. This corpus may contain a large number of words e.g. hundreds thousands millions or more. These words may be derived from utterances spoken by users of an ASR system and or from written documents. For instance a language model can be based on the word patterns occurring in human speech written text e.g. emails web pages reports academic papers word processing documents etc. and so on.

From such a corpus tri gram probabilities can be estimated based on their respective number of appearances in the training corpus. In other words if C w w w is the number of occurrences of the word pattern w w win the corpus then

Thus a language model may be represented as a table of conditional probabilities. Table 2 illustrates a simple example of such a table that could form the basis of language model . Particularly Table 2 contains tri gram conditional probabilities.

For the 2 gram prefix cat and Table 2 indicates that based on the observed occurrences in the corpus 50 of the time the next 1 gram is dog. Likewise 35 of the time the next 1 gram is mouse 14 of the time the next 1 gram is bird and 1 of the time the next 1 gram is fiddle. Clearly in a fully trained ASR system the language model would contain many more entries and these entries would include more than just one 2 gram prefix.

Nonetheless using the observed frequencies of word patterns from a corpus of speech and or from other sources is not perfect as some acceptable tri grams may not appear in the corpus and may therefore be assigned a probability of zero. Consequently when given a zero probability tri gram at run time the language model may attempt to find another n gram associated with a non zero probability.

In order to address this issue the language model may be smoothed so that zero probability tri grams have small non zero probabilities and the probabilities of the tri grams in the corpus are reduced accordingly. In this way tri grams not found in the corpus can still be recognized by the language model. Alternatively the language model may back off when encountering a zero probability tri gram and consider a related non zero probability bi gram or uni gram instead.

Once acoustic model and language model are appropriately trained feature analysis model and pattern classification module may be used to perform ASR. Provided with an input utterance the ASR system can search the space of valid word sequences from the language model to find the word sequence with the maximum likelihood of having been spoken in the utterance. A challenge with doing so is that the size of the search graph can be quite large and therefore performing this search may take an excessive amount of computing resources e.g. processing time and memory utilization . Nonetheless there are some heuristic techniques that can be used to reduce the complexity of the search potentially by one or more orders of magnitude.

For instance a finite state transducer FST can be used to compactly represent multiple phoneme patterns that map to a single word. Some words such as data either tomato and potato have multiple pronunciations. The phoneme sequences for these pronunciations can be represented in a single FST per word.

This process of creating efficient phoneme level FSTs can be carried out for words in dictionary and the resulting word FSTs can be combined into sentence FSTs using the language model . Ultimately a very large network of states for phonemes words and sequences of words can be developed and represented in a compact search graph.

Each circle in search graph may represent a state associated with the processing of an input utterance that has been mapped to phonemes. For purposes of simplicity each phoneme in search graph is represented with a single state rather than multiple states. Also self transitions are omitted from search graph in order to streamline .

The states in search graph are named based on the current phoneme context of the input utterance using the format x y z to indicate that the current phoneme being considered y has a left context of the phoneme x and a right context of the phoneme z. In other words the state x y z indicates a point in processing an utterance in which the current phoneme being considered is y the previously phoneme in the utterance is x and the next phoneme in the utterance is z. The beginning of an utterance and the end of an utterance are represented by the character and also may be referred to as null phonemes.

Terminal states may be represented by a recognized word or phrase in quotes. Search graph includes five terminal states representing recognition of the words or phrases catapult cat and mouse cat and dog cat and cap. 

Transitions from one state to another may represent an observed ordering of phonemes in the corpus. For instance the state k ae represents the recognition of a k phoneme with a left context of a null phoneme and a right context of an ae phoneme. There are two transitions from the state k ae one for which the next phoneme the phoneme after the ae is a t and another for which the next phoneme is a p. 

Based on acoustic model dictionary and language model costs may be assigned to one or more of the states and or transitions. For example if a particular phoneme pattern is rare a transition to a state representing that phoneme pattern may have a higher cost than a transition to a state representing a more common phoneme pattern. Similarly the conditional probabilities from the language model see Table 2 for examples may also be used to assign costs to states and or transitions. For instance in Table 2 given a phrase with the words cat and the conditional probability of the next word in the phrase being dog is 0.5 while the conditional probability of the next word in the phrase being mouse is 0.35. Therefore the transition from state ae n d to state n d m may have a higher cost than the transition from state ae n d to state n d d. 

Search graph including any states transitions between states and associated costs therein may be used to estimate text string transcriptions for new input utterances. For example pattern classification module may determine a sequence of one or more words that match an input utterance based on search graph . Formally pattern classification module may attempt to find max where a is a stream of feature vectors derived from the input utterance P a w represents the probability of those feature vectors being produced by a word sequence w and P w is the probability assigned to w by language model . For example P w may be based on n gram conditional probabilities as discussed above as well as other factors. The function argmaxmay return the value of w that maximizes P a w P w .

To find text strings that may match utterance pattern classification module may attempt to find paths from an initial state in search graph to a terminal state in search graph based on feature vectors . This process may involve pattern classification module performing a breadth first search depth first search beam search or some other type of search on search graph . Pattern classification module may assign a total cost to one or more paths through search graph based on costs associated with the states and or transitions of associated with each path. Some of these costs may be based on for instance a confidence level that a particular segment of the utterance maps to a particular sequence of phonemes in the path.

As an example suppose that utterance is the phrase cat and dog. In a possible scenario pattern classification module would step through search graph phoneme by phoneme to find the path beginning with initial state k ae and ending with terminal state cat and dog. Pattern classification module may also find one or more additional paths through search graph . For example pattern classification module may also associate utterance with the path with initial state k ae and ending with terminal state cat and mouse and with the path with initial state k ae and ending with terminal state catapult. Nonetheless pattern classification module may assign a lower cost to the path with terminal state cat and dog than to other paths. Consequently the path with terminal state cat and dog may be selected as the best transcription for the input utterance.

It should be understood that ASR systems can operate in many different ways. The embodiments described above are presented for purposes of illustration and may not be the only way in which an ASR system operates.

As noted above FSTs may be used to represent both dictionary and language model . Particularly weighted FSTs WFSTs can be used to represent transitions and their associated costs e.g. probabilities between states. For sake of simplicity the term FST will be used herein to refer to both FSTs and WFSTs.

Formally an FST can be represented as a type of finite state machine and may include a start state and one or more final e.g. terminal states. Each final state may be associated with a weight. A transition between two states may be associated with an input label an output label and a weight. Each input label and each output label may consist of one or more symbols.

A path from the start state to the final state includes a sequence of transitions that represents a sequential mapping from the input label symbols on the transitions of the path to the output label symbols on these transitions. For any particular transition an input label and or an output label may be the empty symbol E which indicates that no symbol is consumed and or produced respectively for the particular transition. The weight of a path may be calculated as the sum or product of the weights on each transition in the path plus the weight associated with the final state if a final state has a weight of 0 this weight may be omitted from the FST . By following a path of an FST an input symbol sequence can be mapped to an output symbol sequence and the weight of the path may be associated with the mapping.

In some embodiments the search graph may include one or more distinct super final states. Super final states may be included for computational convenience and may include one or more incoming transitions with input and output symbols of and the weight of an associated final state. Super final states will be discussed in more detail below.

Example FSTs are given in . By convention the initial state has a bold circle and a final state has a double circle. Also a transition is annotated with the term i o w indicating that input symbol i maps to output symbol o with weight w. Each weight may represent a probability a logarithm of a probability a cost etc.

In FST includes states A0 A1 A2 and A3. Transition from state A0 to A1 is labeled with input symbol a output symbol b and weight 0.1. Thus transition represents a mapping from symbols a to b associated with a weight of 0.1. Similarly transition represents a mapping from symbols b to a associated with a weight of 0.2. State A1 has a self transition transition which maps zero or more instances of symbol c to the same number of instances of symbol a each mapping associated with a weight of 0.3.

When considered as a whole FST maps the input symbol string aca to the output symbol string baa where x 0 and the notation srepresent x instances of the symbol s. Assuming that path weights are additive the weight of this mapping is 1.1 1.4 1.7 etc. depending on the value of x e.g. the number of c s in the input symbol string . FST also maps the input symbol string bb to the output symbol string ab with a weight of 1.3.

In FST maps the input symbol string bad to the output symbol string cbbwith weights of 1.4 2.0 2.6 etc. based on the value of x again additive path weights are assumed . It should be understood that FST and FST are merely simple examples of FSTs. Actual FSTs used in ASR systems may be much larger and more complex.

As an example FST in maps the phoneme strings d ay t ah d aa t ah d ay d ah and d aa d ah to the text string data . FST also maps the phoneme string d ew to the text string dew . In this case unlike FST and the weight associated with each transition is a probability and the path weight may be calculated as the product of the weights of each transition in the path. For instance FST indicates that the text string data is pronounced as d ay t ah 15 of the time d aa t ah 15 of the time d ay d ah 35 of the time and d aa d ah 35 of the time. On the other hand there is only one pronunciation of dew that is supported.

One way of understanding FST is that it transduces a phoneme sequence to a word with a particular weight. The word corresponding to a pronunciation is output by the transition that consumes the first phoneme for that pronunciation. The transitions that consume the remaining phoneme do not output any symbols as indicated by empty symbol as the transition s output label. Since words can be demarked by output symbol it may be possible to combine the FSTs for more than one word without losing track of where each word begins and ends. Another advantage of FSTs as can be seen from is that a large ASR dictionary can be represented in a relatively compact amount of space.

Additionally as shown in FSTs can also represent language models. FST represents a grammar fragment that includes the text strings the data is corrupted and the data are corrupted with associated probabilities of 0.4 and 0.6 respectively.

Although not shown in both FST and FST may include weights associated with one or more of their final states. Also it should be clear that FSTs used in ASR systems may be far larger and more complex for example including thousands millions or billions of states. The FST examples shown herein are for purposes of illustration and therefore have been simplified.

An additional possible benefit of FSTs is that two or more FSTs can be combined through a process referred to as composition. Composition allows for instance the dictionary represented by FST to be combined with the grammar represented by FST . The resulting search graph may represent the grammar of FST including the several ways of pronouncing data as represented by FST .

It should be noted that strictly speaking the composition of FST with FST may result in an empty FST because FST does not have paths for The is are and corrupted . Additionally FST may require a transition from the final state back to the initial state in order to have paths corresponding to the sequence of words in FST . However for sake of illustration and simplicity FST represents just one word.

In general a grammar that contains m words may be composed with one or more dictionary FSTs that represent one or more ways of pronouncing each of the m words. Clearly such a composition of FSTs even for a small grammar with few words can be difficult to visualize.

In order to illustrate composition using simpler FSTs represents the composition of FSTs and . FST maps sequences of symbols that would be valid input for FST to sequences of symbols that would be valid output for FST and would be valid input for FST . Then FST maps this result to sequences of symbols that valid output for FST . In other words the composition of two FSTs Tand T denoted T T may be thought of as using the input symbols of FST Tand the output symbols of FST T.

Formally T Thas exactly one path mapping string u to string w for each pair of paths the first in Tmapping u to some string v and the second in Tmapping v to w. The weight of a path in T Tmay be computed from the weights of the two corresponding paths in Tand Twith the same operation that computes the weight of a path from the weights of its transitions. If the transition weights represent probabilities that operation may be multiplication. If instead the weights represent log probabilities or negative log probabilities the operation may be addition.

In FST each state represents the composition of two states one from each of FST and FST . Thus for instance state A0 BO is the composition of state A0 from FST and state BO from FST . Each transition is also a composition of transitions from FST and FST . Thus each transition includes an input symbol from a state of FST and an output symbol from FST such that the input symbol is mapped to an intermediate symbol by FST and that intermediate symbol when used as the input symbol in an associated state of FST produces the output symbol.

Thus for transition which is the composition of transition from FST and transition of FST the input symbol a is also the input symbol for transition . Transition maps this input symbol a to the output symbol b which is the intermediate symbol for transition . By convention intermediate symbols do appear in composed FSTs. Transition takes the intermediate symbol b as an input symbol and maps it to output symbol c. Therefore transition maps input symbol a to output symbol c.

Additionally transition is associated with a weight of 0.1 and transition is associated with a weight of 0.3. Assuming these weights represent negative log probabilities the weight associated with transition is 0.4 the sum of these two weights. Similar logic applies to the other transitions and states in FST .

Given the components of an ASR system such as those described above one method of selecting a transcription of an utterance is to use a maximum a posteriori MAP technique. Formally given an utterance x MAP techniques attempt to find a transcription y that maximizes P y x which is the probability that the transcription is correct given the utterance. It should be noted that when describing MAP and MBR techniques herein a slightly different notation is used than introduced in Section 4.

Therefore the transcription that maximizes P y P x y also maximizes P y x . While the values of P y and P x y may not be known they can be approximated.

For example P y the probability of transcription y can be estimated from a language model. This estimation may be based on the observation that some sequences of words are more likely to appear in a given language than others. For instance the sentence this sentence is normal will have a relatively high probability in the language model because the sequence of words follows English grammar rules. However the sentence colorless green ideas sleep furiously will have a relatively low probability in the language model because each word is unexpected given the previously observed words.

P x y the probability of utterance x given transcription y can be determined by an acoustic model. Doing so may involve the use of pronunciation dictionaries to map the word to sequences of phonemes each associated with a probability.

It should be noted that the MAP technique may select a transcription that minimizes the likelihood of transcription level error. This transcription will be called y. For example if yis a sentence the MAP technique minimizes the probability of yhaving at least one word error.

However a potentially more useful metric of ASR performance is word error rate WER . The WER between a particular transcription and the correct transcription y may be defined as the number of edits substitutions deletions or insertions needed to change the particular transcription into y divided by the number of words in y . This number of edits may also be referred to as the Levenshtein edit distance.

Minimizing WER rather than transcription level error is motivated by the observation that some transcription level errors matter more than others. For example the best transcription as given by the MAP technique may have more word errors when compared to a correct transcription of the utterance than other possible transcriptions with lower probabilities than y. Put another way WER gives partial credit for correctly recognizing portions of sentences.

Minimum Bayesian Risk MBR techniques can also be used to select a transcription. One possible advantage of using MBR techniques is that they can be tuned to select a transcription that is optimal with respect to a general loss function L. It is assumed that the loss function generates a cost when the system erroneously produces a transcription y instead of y . If this loss function is the Levenshtein edit distance then the MBR technique selects a transcription that is optimal with respect to WER.

In other words the selected transcription exhibits the lowest expected WER of all transcripts considered.

Nonetheless the computation of yis expensive compared to using the MAP technique. For instance a sum of weighted probabilities over the entire language is calculated. Also each pair of transcriptions is compared and the loss function between them is determined. When the loss function is the Levenshtein edit distance the running time of L y z is O y z .

One way of improving the running time of the MBR technique is to limit the search graph. Accordingly the evidence space W is defined to be the part of the language over which the weighted probabilities are calculated. Additionally the hypothesis space W is the subset of the search graph e.g. the language from which yis chosen.

One way of limiting the hypothesis space is to restrict Wto an n best list. In other words y is selected from the n best transcriptions of the utterance. The n best list may be determined by evaluating the transcriptions based on one or more metrics such as highest probability highest confidence lowest cost etc. In some embodiments the n best list may be obtained using a MAP technique.

The evidence space may be limited by restricting Wto sample paths in the search graph. Recall that the search graph can be interpreted as a probability distribution over transcripts. Thus this probability distribution can be sampled by the following procedure 

Then the MBR calculation can be performed. This calculation may involve for each hypothesis path considering all evidence paths computing the sum of the weights on these paths and returning the best hypothesis path.

The following subsections describe in more detail variations and operations that may be involved in embodiments of this procedure.

Some of the operations discussed herein involve the use of mathematical constructs known as semirings. Formally a semiring R has two binary operators and called addition and multiplication respectively. The properties of a semiring are that is commutative and has identity value is associative and has identity value distributes over and both of x by and of by x are .

There are several types of semirings each with potentially different characteristics. For instance in the probability semiring R is the set of real numbers is normal addition and is normal multiplication.

The log semiring is a transform of the probability semiring in which each value of x in the probability semiring takes on a value of log x in the log semiring. The operator denotes normal addition and the operator is defined such that x y log e e . Although it is mathematically equivalent to the probability semiring the log semiring may be preferred because it can provide computational efficiency and numerical stability.

The tropical semiring is the same as the log semiring except that x y is approximated as min x y . This result may be used for computational efficiency and is based on the Viterbi approximation log max min 

Additionally the signed log semiring operates over the set of tuples that have either 1 or 1 as the first element and a real number as the second element and is defined by the following relationships log if log if 1 1 0 

As noted above the search graph may be prepared by pushing the weights associated with some transitions toward the initial state such that the total weight of each path is unchanged. In some embodiments this operation provides that for every state the sum of all outgoing edges including the final weight which can be seen as an E transition to a super final state is equal to 1.

Formally let d x be the sum of weights of all transitions between state x and a final state per the appropriate semiring. Thus d S0 10 d S1 5 d S2 2 and d S3 0. Also let w y be the weight of transition y. Thus w t1 1 w t2 5 w t3 2 and w t4 2. Further let p y be the state at which transition y begins and let q y be the state at which transition y ends. The weight pushing algorithm re weights each transition according to the equation

Applying this algorithm to FST w t1 6 10 w t2 1 w t3 4 10 and w t4 1. The result is shown as FST in . Essentially the weight of transition t2 is pushed to transition t1 and the weight of transition t4 is pushed to transition t2 and the weights are normalized into probabilities. For sake of illustration and convenience in this example FST is converted from being in the log semiring to the probability semiring.

If this pushing is not performed the sampling algorithm above would start at state S0 and prefer the transition with the highest weight t3. This would result in the path traversing S0 S2 and S3 being taken. However the path traversing S0 S1 and S3 has a higher weight and therefore should be preferable over the path traversing S0 S2 and S3. The weight pushing algorithm addresses this issue. Further in FST the weights of the outgoing transitions from each state sum to 1 which implies that from each state paths can be sampled according to their respective probabilities.

The MBR technique given above computes the Levenshtein distance between every pair of hypothesis and evidence paths. It may be possible to make this technique more efficient by factoring out the common steps of the computation and performing these common steps fewer times e.g. perhaps just once . Doing so may lead to a performance gain because even different hypothesis and evidence paths can be similar to one another.

FSTs can also be used to efficiently calculate edit distances. For instance a set of text strings represented as unweighted transducers can be composed with an edit transducer that calculates the edit distance between strings. More formally for any two strings s1 and s2 assume that their respective FST representations are S1 and S2. Then let T be an edit transducer such that S1 T S2 is a transducer where all paths have s1 as an input label and s2 as an output label and where the shortest path has weight L s1 s2 .

An example edit transducer T for the words w1 and w2 is depicted in . All transitions in T are self transitions and for sake of convenience and clarity are provided to the right of T. Each of these transitions may represent an edit distance between the input and output symbols. Thus transducing w1 to w1 and w2 to w2 have an edit distance of 0 indicating that no edits are required. On the other hand all other transitions represent an insertion substitution or deletion and therefore have an edit distance of 1. Therefore given two sentences s1 and s2 taken from the dictionary consisting of w1 and w2 and given S1 and S2 the FST representations of s1 and s2 respectively the weight of the shortest path of S1 T S2 is the Levenshtein edit distance between s1 and s2.

As an example let S1 be an FST for the sentence w1 w2. Then the composition of S1 T is shown in . The FST in can be used to map an input sentence to an edit distance between that input sentence and w1 w2. For example let S2 be an FST for the sentence w2 w1. Then the composition of S1 T S2 represents the edit distance between w1 w2 and w2 w1. This composition S1 T S2 is shown as FST in and provides a path for each combination of edits deletion insertion substitution leading from w1 w2 to w2 w1 with an associated edit distance. Of course the minimum edit distance is 2 representing two substitutions.

The size of edit transducer T is quadratic with respect to the number of words in the language. One possible way of reducing the complexity and size of T is to decompose T into two transducers T1 and T2. Transducer T1 may represent the left half of T and may map symbols to edit operations. Transducer T2 may represent the right half of T and may map edit operations back to symbols. A decomposition of T is shown as FSTs and in . Note that for these FSTs the insert operation is denoted as the delete operation is denoted as and the substitute operation is denoted as .

The composition S1 T o S2 can now be expressed as S1 T1 T2 S2 due to the associative property of composition. Doing so replaces the quadratic sized T with the linear sized T1 and T2.

This construction may be used to compute the MBR value of a given path represented by FST S in W. Particularly S T can be computed where is the unweighted version of W. The result may accept strings of Wwith a weight equal to the cost of their alignments with the given path represented by FST S. This results in a compact representation of the distance of the given path to every path in W.

Edit transducer factorization in the log semiring is similar to that of the edit transducer factorization in the tropical semiring. However some possible differences may include use of a counting transducer as a way to count edit operations without eliminating alignments use of a synchronization operation as a way to equate the labels of the various alignments for each pair of strings and the use of automata determinization. This process may involve encoding the input and output symbols as a pair determinizing the resulting FST as a finite state automaton on a pair of symbols and decoding the pairs back to input and output symbols. It should be understood that determinization of a non deterministic automaton results in a deterministic automaton that recognizes exactly the same language.

Furthermore unlike the tropical semiring method which computes the MBR value of only one hypothesis string the log semiring method can be used to compute the values for all hypothesis paths simultaneously.

Instead of generating one path for each alignment with the cost of the alignment as weight in FST the path for each alignment may be generated in multiple copies so that the operation in the log semiring yields the correct cost for the alignment. As a computational convenience perfect alignments may be modeled explicitly giving them a large but finite weight N. Otherwise these alignments might not be taken into account.

Like for the tropical semiring FST is quadratic in the size of the word list A. Thus it may be beneficial to factor FST . However in doing so complications may arise because unlike in the tropical semiring adding bogus edges for identity substitutions and relying on the operation to eliminate them later may not work. Instead the set of non identity substitutions may be modeled explicitly. In particular the unneeded edges may be added but then cancelled out by adding a negated version of them in the signed log semiring see above for a definition of the signed log semiring .

For this factorization of FST let T be FST T1 be FST and T2 be FST . Thus T T1 T2. Wand Wmay be mapped from the log semiring to the signed log semiring by converting each weight x to 1 x . W T1 T2 W may be calculated to determine the edit distance between multiple evidence and hypothesis strings. After determinizing to eliminate negative weights these values can be mapped back to the log semiring by converting 1 x to x.

At step n hypothesis space transcriptions of an utterance may be selected from a search graph that includes t n transcriptions of the utterance. Determining the n hypothesis space transcriptions may involve determining the n best transcriptions of the utterance according to a maximum a posteriori MAP technique. At step m evidence space transcriptions of the utterance may be randomly selected from the search graph where t m.

The search graph may be implemented as an FST. This FST may include a composition of a language model FST and a dictionary FST. Alternatively or additionally this FST may include a decomposed edit transducer.

At step for each particular hypothesis space transcription an expected word error rate may be calculated by comparing the particular hypothesis space transcription to each of the evidence space transcriptions. These comparisons may involve for each particular evidence space transcription further steps of determining a probability that the particular evidence space transcription is a correct transcription of the utterance based on the utterance determining an edit distance between the particular hypothesis space transcription and the particular evidence space transcription and calculating a product of the probability and the edit distance. In some embodiments the edit distance may be a Levenshtein edit distance. Additionally the expected word error rate may be the sum of each of the products associated with the particular evidence space transcriptions.

Determining the edit distance may involve factoring an edit transducer in the tropical semiring into a left half transducer and a right half transducer. The edit transducer may include a finite state transducer that maps edit operations to associated costs. At least one of the n hypothesis space transcriptions may be composed with the left half transducer into a first composed transducer. Additionally the right half transducer may be composed with an evidence space transducer into a second composed transducer. The evidence space transducer may represent the m evidence space transcriptions. Then the first composed transducer may be composed with the second composed transducer.

In other embodiments determining the edit distance may involve factoring an edit transducer in the log semiring into a left half transducer and a right half transducer. The edit transducer may include a finite state transducer that maps edit operations to associated costs. A hypothesis space transducer may be composed with the left half transducer into a first composed transducer. The hypothesis space transducer may represent the n hypothesis space transcriptions. Additionally the right half transducer may be composed with an evidence space transducer into a second composed transducer. The evidence space transducer may represent the m evidence space transcriptions. Then the first composed transducer may be composed with the second composed transducer.

At step based on the expected word error rates a lowest expected word error rate may be determined. At step the particular hypothesis space transcription that is associated with the lowest expected word error rate may be provided.

The search graph may include a directed acyclic graph each edge of the directed acyclic graph associated with a probability. Randomly selecting the evidence space transcriptions from the search graph may involve traversing the directed acyclic graph from a source node to a terminal node by repeatedly selecting edges according to the probability associated with each edge.

Some implementations may include receiving a representation of the utterance from a client device. Providing the particular hypothesis space transcription that is associated with the lowest expected word error rate may involve providing the particular hypothesis space transcription that is associated with the lowest expected word error rate to the client device. Alternatively or additionally providing the particular hypothesis space transcription that is associated with the lowest expected word error rate may involve providing the particular hypothesis space transcription that is associated with the lowest expected word error rate on an output display of a computing device that performs at least one of the steps of .

The above detailed description describes various features and functions of the disclosed systems devices and methods with reference to the accompanying figures. In the figures similar symbols typically identify similar components unless context indicates otherwise. The illustrative embodiments described in the detailed description figures and claims are not meant to be limiting. Other embodiments can be utilized and other changes can be made without departing from the spirit or scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure as generally described herein and illustrated in the figures can be arranged substituted combined separated and designed in a wide variety of different configurations all of which are explicitly contemplated herein.

For situations in which the systems discussed here collect personal information about users the users may be provided with an opportunity to opt in out of programs or features that may collect personal information e.g. information about a user s preferences or a user s utterances made to an ASR system . In addition certain data may be anonymized in one or more ways before it is stored or used so that personally identifiable information is removed. For example a user s identity may be anonymized so that no personally identifiable information can be determined for the user and so that any identified user preferences or user interactions are generalized for example generalized based on user demographics rather than associated with a particular user.

With respect to any or all of the message flow diagrams scenarios and flow charts in the figures and as discussed herein each step block and or communication may represent a processing of information and or a transmission of information in accordance with example embodiments. Alternative embodiments are included within the scope of these example embodiments. In these alternative embodiments for example functions described as steps blocks transmissions communications requests responses and or messages may be executed out of order from that shown or discussed including in substantially concurrent or in reverse order depending on the functionality involved. Further more or fewer steps blocks and or functions may be used with any of the message flow diagrams scenarios and flow charts discussed herein and these message flow diagrams scenarios and flow charts may be combined with one another in part or in whole.

A step or block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein described method or technique. Alternatively or additionally a step or block that represents a processing of information may correspond to a module a segment or a portion of program code including related data . The program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique. The program code and or related data may be stored on any type of computer readable medium such as a storage device including a disk drive a hard drive or other storage media.

The computer readable medium may also include non transitory computer readable media such as computer readable media that stores data for short periods of time like register memory processor cache and or random access memory RAM . The computer readable media may also include non transitory computer readable media that stores program code and or data for longer periods of time such as secondary or persistent long term storage like read only memory ROM optical or magnetic disks and or compact disc read only memory CD ROM for example. The computer readable media may also be any other volatile or non volatile storage systems. A computer readable medium may be considered a computer readable storage medium for example or a tangible storage device.

Moreover a step or block that represents one or more information transmissions may correspond to information transmissions between software and or hardware modules in the same physical device. However other information transmissions may be between software modules and or hardware modules in different physical devices.

While various aspects and embodiments have been disclosed herein other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting with the true scope and spirit being indicated by the following claims.

