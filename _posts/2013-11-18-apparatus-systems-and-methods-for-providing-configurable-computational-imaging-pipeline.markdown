---

title: Apparatus, systems, and methods for providing configurable computational imaging pipeline
abstract: The present application relates generally to a parallel processing device. The parallel processing device can include a plurality of processing elements, a memory subsystem, and an interconnect system. The memory subsystem can include a plurality of memory slices, at least one of which is associated with one of the plurality of processing elements and comprises a plurality of random access memory (RAM) tiles, each tile having individual read and write ports. The interconnect system is configured to couple the plurality of processing elements and the memory subsystem. The interconnect system includes a local interconnect and a global interconnect.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09146747&OS=09146747&RS=09146747
owner: LINEAR ALGEBRA TECHNOLOGIES LIMITED
number: 09146747
owner_city: Dublin
owner_country: IE
publication_date: 20131118
---
This application claims benefit of the earlier priority date of U.K. Patent Application No. GB 1314263.3 entitled CONFIGURABLE AND COMPOSABLE COMPUTATIONAL IMAGING PIPELINE filed on Aug. 8 2013 by Linear Algebra Technologies Limited and of the Romanian Patent Application identified as OSIM Registratura A 00812 entitled APPARATUS SYSTEMS AND METHODS FOR PROVIDING CONFIGURABLE AND COMPOSABLE COMPUTATIONAL IMAGING PIPELINE filed on Nov. 6 2013 by Linear Algebra Technologies Limited both of which are expressly incorporated herein by reference in their entirety. This application is also related to U.S. patent application Ser. No. 14 082 396 entitled APPARATUS SYSTEMS AND METHODS FOR PROVIDING COMPUTATIONAL IMAGING PIPELINE filed on an even day herewith by Linear Algebra Technologies Limited which is also incorporated herein by reference in its entirety.

This present application relates generally to processing devices suitable for image and video processing.

Computational image and video processing is very demanding in terms of memory bandwidth as image resolutions and frame rates are high with aggregate pixel rates in the high hundreds of megapixels per second being common place. Furthermore as this field is in its relative infancy algorithms are in constant flux. Therefore it is difficult to implement them entirely in hardware as changes to the algorithms can mean hardware is unable to adapt. At the same time a software approach relying on implementation in processors alone is unrealistic. Accordingly it is generally desirable to provide a flexible architecture infrastructure which can accommodate processors and hardware accelerators.

At the same time the demand for such video and image processing is coming to a large extent from portable electronic devices for example tablet computers and mobile devices where power consumption is a key consideration. As a result there is a general need for a flexible infrastructure to couple programmable multicore processors and hardware accelerators with a high bandwidth memory subsystem that allows them to deliver a sustained data transfer rate at low power levels necessary for portable electronic devices.

In accordance with the disclosed subject matter apparatus systems and methods are provided for providing configurable and composable computational imaging pipeline.

Disclosed subject matter includes a parallel processing device. The processing device includes a plurality of processing elements each configured to execute instructions and a memory subsystem comprising a plurality of memory slices including a first memory slice associated with one of the plurality of processing elements. The first memory slice comprises a plurality of random access memory RAM tiles each having individual read and write ports. The parallel processing device can include an interconnect system configured to couple the plurality of processing elements and the memory subsystem. The interconnect system can include a local interconnect configured to couple the first memory slice and the one of the plurality of processing elements and a global interconnect configured to couple the first memory slice and the remaining of the plurality of processing elements.

In some embodiments the one of the plurality of RAM tiles is associated with an arbitration block wherein the arbitration block is configured to receive memory access requests from one of the plurality of processing elements and to grant to the one of the plurality of processing elements an access to the one of the plurality of RAM tiles.

In some embodiments the arbitration block is configured to grant access to the one of the plurality of RAM tiles in a round robin manner.

In some embodiments the arbitration block comprises a clash detector configured to monitor memory access requests to the one of the plurality of RAM tiles and to determine whether two or more of the plurality of processing elements are attempting to access the one of the plurality of RAM tiles simultaneously.

In some embodiments the clash detector is coupled to a plurality of address decoders wherein each of the plurality of address decoders is coupled to one of the plurality of processing elements and is configured to determine whether the one of the plurality of processing elements is attempting to access the one of the plurality of RAM tiles associated with the arbitration block.

In some embodiments the plurality of processing elements comprises at least one vector processor and at least one hardware accelerator.

In some embodiments the parallel processing device includes a plurality of memory slice controllers each configured to provide access to one of the plurality of memory slices.

In some embodiments the interconnect system comprises a first bus configured to provide communication between the at least one vector processor and the memory subsystem.

In some embodiments the interconnect system comprises a second bus system configured to provide communication between the at least one hardware accelerator and the memory subsystem.

In some embodiments the second bus system comprises a slice address request filter configured to mediate communication between the at least one hardware accelerator and the memory subsystem by receiving a memory access request from the at least one hardware accelerator and by granting to the at least one hardware accelerator access to the memory subsystem.

In some embodiments one of the plurality of processing devices comprises a buffer to increase a throughput of the memory subsystem wherein a number of elements in the buffer is greater than a number of cycles for retrieving data from the memory subsystem.

Disclosed subject matter includes a method for operating a parallel processing system. The method includes providing a plurality of processing elements including a first processing element and a second processing element wherein each of the plurality of processing elements is configured to execute instructions. The method also includes providing a memory subsystem comprising a plurality of memory slices including a first memory slice associated with the first processing element wherein the first memory slice comprises a plurality of random access memory RAM tiles each having individual read and write ports. The method further includes receiving by an arbitration block associated with one of the plurality of RAM tiles via a local interconnect of an interconnect system a first memory access request from the first processing element. The method additionally includes sending by the arbitration block via the global interconnect a first authorization message to the first processing element to authorize the first processing element to access the one of the plurality of RAM tiles.

In some embodiments the method further includes receiving by the arbitration block via a global interconnect of the interconnect system a second memory access request from a second processing element and sending by the arbitration block via the global interconnect a second authorization message to the second processing element to authorize the second processing element to access the one of the plurality of RAM tiles.

In some embodiments the method further includes sending by the arbitration block a plurality of authorization messages to the plurality of processing elements to authorize access to the one of the plurality of RAM tiles in a round robin manner.

In some embodiments the method further includes monitoring by a clash detector in the arbitration block memory access requests to the one of the plurality of RAM tiles and determining whether two or more of the plurality of processing elements are attempting to access the one of the plurality of RAM tiles simultaneously.

In some embodiments the plurality of processing elements comprises at least one vector processor and at least one hardware accelerator.

In some embodiments the method further includes providing a plurality of memory slice controllers each configured to provide access to one of the plurality of memory slices.

In some embodiments the method further includes providing communication between the at least one vector processor and the memory subsystem via a first bus system of the interconnect system.

In some embodiments the method further includes providing communication between the at least one hardware accelerator and the memory subsystem via a second bus system of the interconnect system.

In some embodiments the second bus system comprises a slice address request filter configured to mediate communication between the at least one hardware accelerator and the memory subsystem by receiving a memory access request from the at least one hardware accelerator and by granting to the at least one hardware accelerator access to the memory subsystem.

Disclosed subject matter includes an electronic device. The electronic device includes a parallel processing device. The processing device includes a plurality of processing elements each configured to execute instructions and a memory subsystem comprising a plurality of memory slices including a first memory slice associated with one of the plurality of processing elements. The first memory slice comprises a plurality of random access memory RAM tiles each having individual read and write ports. The parallel processing device can include an interconnect system configured to couple the plurality of processing elements and the memory subsystem. The interconnect system can include a local interconnect configured to couple the first memory slice and the one of the plurality of processing elements and a global interconnect configured to couple the first memory slice and the remaining of the plurality of processing elements. The electronic device also includes a processor in communication with the parallel processing device configured to run a module stored in memory. The module is configured to receive a flow graph associated with a data processing process wherein the flow graph comprises a plurality of nodes and a plurality of edges connecting two or more of the plurality of nodes wherein each node identifies an operation and each edge identifies a relationship between the connected nodes and assign a first node of the plurality of nodes to a first processing element of the parallel processing device and a second node of the plurality of nodes to a second processing element of the parallel processing device thereby parallelizing operations associated with the first node and the second node.

In some embodiments the module is configured to assign the first node of the plurality of nodes to the first processing element based on a past performance of a memory subsystem in the parallel processing device.

In some embodiments the memory subsystem of the parallel processing device comprises a counter that is configured to count a number of memory clashes over a predetermined period of time and the past performance of the memory subsystem comprises the number of memory clashes measured by the counter.

In some embodiments the module is configured to assign the first node of the plurality of nodes to the first processing element while the parallel processing device is operating at least a portion of the flow graph.

In some embodiments the module is configured to receive a plurality of flow graphs and assign all operations associated with the plurality of flow graphs to a single processing element in the parallel processing device.

In some embodiments the module is configured to stagger memory accesses by the processing elements to reduce memory clashes.

In some embodiments the flow graph is specified using an application programming interface API associated with the parallel processing device.

In some embodiments the module is configured to provide input image data to the plurality of processing elements by dividing the input image data into strips and providing one strip of the input image data to one of the plurality of processing elements

In some embodiments a number of the strips of the input image data is the same as a number of the plurality of processing elements.

Disclosed subject matter includes a method. The method includes receiving at a processor in communication with a parallel processing device a flow graph associated with a data processing process wherein the flow graph comprises a plurality of nodes and a plurality of edges connecting two or more of the plurality of nodes wherein each node identifies an operation and each edge identifies a relationship between the connected nodes. The method also includes assigning a first node of the plurality of nodes to a first processing element of the parallel processing device and a second node of the plurality of nodes to a second processing element of the parallel processing device thereby parallelizing operations associated with the first node and the second node. The parallel processing device also includes a memory subsystem comprising a plurality of memory slices including a first memory slice associated with the first processing element wherein the first memory slice comprises a plurality of random access memory RAM tiles each having individual read and write ports and an interconnect system configured to couple the first processing element the second processing element and the memory subsystem. The interconnect system includes a local interconnect configured to couple the first memory slice and the first processing element and a global interconnect configured to couple the first memory slice and the second processing element.

In some embodiments assigning the first node of the plurality of nodes to the first processing element of the parallel processing device comprises assigning the first node of the plurality of nodes to the first processing element based on a past performance of a first memory slice in the parallel processing device.

In some embodiments the method also includes counting at a counter in the memory subsystem a number of memory clashes in the first memory slice over a predetermined period of time and the past performance of the first memory slice comprises the number of memory clashes in the first memory slice.

In some embodiments assigning the first node of the plurality of nodes to the first processing element is performed while the parallel processing device is operating at least a portion of the flow graph.

In some embodiments the method also includes staggering memory accesses by the processing elements to the first memory slice in order to reduce memory clashes.

In some embodiments the flow graph is specified using an application programming interface API associated with the parallel processing device.

In some embodiments the method also includes providing an input image data to the plurality of processing elements by dividing the input image data into a plurality of strips and providing one of the plurality of strips of the input image data to one of the plurality of processing elements.

In some embodiments a number of the plurality of strips of the input image data is the same as a number of the plurality of processing elements.

One possible way of interconnecting such different processing resources e.g. processors and hardware accelerators is to use a bus as outlined in the computational photography engine of Chimera developed by NVidia. illustrates the computational photography engine of Chimera. The Chimera computational photography engine includes multiple Graphics Processing Unit GPU cores that are connected to a Multicore ARM processor sub system and hardware HW Image Signal Processing ISP accelerators via a flat level bus infrastructure e.g. a single hierarchy bus system that connects all processing elements . The Chimera computational photography engine is presented generally as a software framework that abstracts the details of the underlying GPU cores CPUs and ISP blocks from the programmer. Furthermore the Chimera computational photography engine describes dataflow through their computational photography engine via two information busses the first bus carrying image or frame data and the second bus carrying state information associated with each frame.

The use of a flat level bus infrastructure as in Chimera can be cheap and convenient to implement. However the use of a flat level bus infrastructure can have a number of notable disadvantages as a means of interconnecting heterogeneous processing elements e.g. processing elements of various types such as GPU cores CPUs and ISP blocks . First the use of a bus to interconnect computational resources means that memory can be distributed throughout the system local to each central processing unit CPU a graphics processing unit GPU and or an image signal processor ISP block . Therefore memory cannot be allocated flexibly within the processing pipeline in accordance with the requirements of the computational photography pipeline the programmer wishes to implement. This lack of flexibility may mean that certain image or video processing are either difficult or impossible to implement or that an implementation is limited in terms of frame rate image quality or otherwise.

Second the use of a flat level bus infrastructure also means that different computational resources CPUs GPUs and ISP blocks have to contend for bus bandwidth. This contention requires arbitration which reduces the amount of available bus bandwidth. Therefore progressively less of the theoretical bandwidth is available for actual work. The reduction of bandwidth may mean that a processing pipeline fails to meet the performance requirements of the application in terms of a frame rate image quality and or power.

Third insufficient memory in proximity to a particular computational resource may mean that data has to be transferred back and forth between memory associated with a given GPU CPU or hardware ISP block and another computational resource. This lack of locality means that additional bus bandwidth and arbitration overhead can be incurred. Furthermore the lack of locality also means that additional power is consumed. Therefore it may be difficult or impossible to support a particular algorithm at a particular target frame rate.

Fourth the use of a flat level bus infrastructure may also compound difficulties in constructing a pipeline from heterogeneous processing elements each of which may have different latency characteristics. For instance GPU cores are designed to tolerate latency by running multiple overlapping process threads to handle multiple outstanding accesses to memory usually external DRAM to cover latency whereas normal CPUs and hardware ISP blocks are not designed to be latency tolerant.

Another way to interconnect different processing resources is provided by a Cell processor architecture developed by IBM which is illustrated in . The Cell processor architecture includes a local storage LS available to each processor also known as a synergistic execution unit SXU . The Cell processor relies on a time shared infrastructure and direct memory access DMA transfers to programmatically schedule data transfers between one processor s LS and another processor s LS . The difficulty with the Cell architecture is the complexity faced by the programmer to explicitly schedule background data transfers many hundreds of cycles in advance due to the high latencies in the Cell architecture so as to ensure that shared data is available to each processor when required. If the programmer does not explicitly schedule background data transfers processors would stall which would degrade the performance.

Another way to interconnect different processing resources is to use a shared multicore memory subsystem to share data efficiently between processors in a multicore processing system. This shared multicore memory subsystem is used in the Efficient Low power Microprocessor ELM system. illustrates the ELM system. The ELM system includes an ensemble the primary physical design unit for compute resources in an ELM system. The ensemble includes a cluster of four processors that are loosely coupled. The cluster of four processors share local resources such as an ensemble memory and an interface to the interconnection network. The ensemble memory captures instruction and data working sets close to the processors and is banked to allow the local processors and network interface controller to access it concurrently. Each processor within an ensemble is assigned a preferred bank in the ensemble memory . Accesses by a processor to its preferred bank are prioritized ahead of accesses and will block accesses by other processors and the network interface. Instructions and data that are private to a single processor may be stored in its preferred bank to provide deterministic access times. The arbiters that control access to the read and write ports are biased to establish an affinity between processors and memory banks . This allows software to make stronger assumptions about bandwidth availability and latency when accessing data that may be shared by multiple processors.

However the ELM architecture can consume a lot of power due to the physically large random access memory RAM blocks. Furthermore the ELM architecture can suffer from low throughput where there is a lot of data sharing between processors . In addition no provision is made for data sharing between processors and hardware accelerators which can be advantageous in terms of power and performance in certain cases.

The present disclosure relates to apparatus systems and methods for enabling multiple processors and hardware accelerators to access shared data simultaneously with other processors and hardware accelerators. The present disclosure provides apparatus systems and methods for accessing shared data simultaneously without being blocked by a local processor that has a higher affinity e.g. a higher priority to access local storage.

The disclosed apparatus systems and methods provide substantial benefits over existing multicore memory architectures. Existing multicore memory architectures use a single monolithic block of RAM per processor which can limit the bandwidth at which data can be accessed. The disclosed architecture can provide a mechanism for accessing memory at a substantially higher bandwidth compared to existing multicore memory architectures that use a single monolithic block of RAM. The disclosed architecture obtains this higher bandwidth by instantiating multiple physical RAM blocks per processor instead of instantiating a single large RAM block per processor. Each RAM block can include a dedicated access arbitration block and a surrounding infrastructure. Therefore each RAM block in the memory subsystem can be accessed independently of others by multiple processing elements in the system for example vector processors reduced instruction set computing RISC processors hardware accelerators or DMA engines.

This is somewhat counter intuitive that the use of multiple small RAM instances is beneficial compared to using a single large RAM instance because a memory bank based on a single large RAM instance is more area efficient than a memory bank based on multiple smaller RAM instances. However the power dissipation for smaller RAM instances is typically significantly lower than for a single large RAM instance. Furthermore if a single large physical RAM instance were to achieve the same bandwidth as multi instance RAM blocks the single large physical RAM instance would incur at a substantially higher power than one composed of multiple physical RAM instances. Therefore at least from the power dissipation perspective the memory subsystem can benefit from using multiple physical RAM instances than using a single large RAM instance.

The memory subsystem with multiple physical RAM instances can have an added advantage in that the cost per RAM access for example the memory access time or the power consumption of smaller RAM blocks is typically a lot lower than that of larger RAM blocks. This is due to the shortened bit lines used to read write data from the RAMs. Furthermore the access time for reads and writes for smaller RAM blocks is also lower due to the reduced resistance capacitance RC time constants associated with shorter bit lines . Therefore the processing elements coupled to the multi RAM based memory subsystem can operate at a higher frequency which in turn reduces static power due to static leakage current. This can be useful particularly when the processors and memory are isolated into power domains. For example when a given processor or filter accelerator has completed its task the power domain associated with the given processor or filter accelerator can be advantageously gated off. Therefore the memory subsystem in the disclosed architecture has superior characteristics in terms of available bandwidth and power dissipation.

In addition a memory subsystem with multiple RAM instances each with arbitrated accesses can provide many ways for data to be shared between processors and hardware accelerators without dedicating a RAM block to a particular processor by locking the RAM block. In principle if a larger RAM is subdivided into N sub blocks then the available data bandwidth is increased by approximately a factor of N. This is based on the assumption that data can be opportunely partitioned to reduce contemporaneous sharing e.g. an access clash by multiple processing elements. For example when a consumer processor or a consumer accelerator reads data from a data buffer that is being filled by a producer processor or a producer accelerator then there is a contemporaneous sharing of a data buffer resulting in an access clash.

In some embodiments the disclosed architecture can provide mechanisms for reducing a contemporaneous sharing of data. In particular the disclosed architecture can be amenable for reducing the contemporaneous sharing via a static memory allocation mechanism and or a dynamic memory allocation mechanism. For example in the static memory allocation mechanism data is mapped to different portions of memory before a program is started e.g. during the program compilation stage in order to reduce the contemporaneous sharing of the data. On the other hand in the dynamic memory allocation scheme data is mapped to different portions of memory during the program execution. Static memory allocation mechanism provides a predictable mechanism for allocating memory to data and it does not incur any substantial overhead in terms of power or performance.

As another example the disclosed architecture can be used in conjunction with a runtime scheduler running on a controller e.g. a supervising RISC processor or one or more processors that mediate access to data structures partitioned across multiple RAM blocks. The runtime scheduler can be configured to stagger the start times of different processing elements operating on parts e.g. lines or tiles of data e.g. an image frame in order to reduce simultaneous access to shared data.

In some embodiments the runtime scheduler can be complemented with a hardware arbitration block. For example the hardware arbitration block can be configured to mediate shared memory accesses by processors such as vector processors via a shared deterministic interconnect designed to reduce stalling. In some cases the hardware arbitration block can be configured to perform a cycle oriented scheduling. The cycle oriented scheduling can include scheduling a use of a resource at a clock cycle granularity as opposed to scheduling a use of a resource at a task level granularity which may require multiple clock cycles. Scheduling resource allocations at a clock cycle granularity can provide higher performance.

In other embodiments the runtime scheduler can be complemented with a multiplicity of hardware accelerators each of which can include an input buffer and an output buffer to store data. The input and output buffers can be configured to absorb or hide the variance of delays in accessing external resources such as external memory. The input and output buffers can include a first in first out FIFO buffer and the FIFO buffer can include a sufficient number of slots to store sufficient amount of data and or instructions to absorb the variance of delays in accessing external resources.

In some embodiments the disclosed apparatus systems and methods provide a parallel processing device. The parallel processing device can include a plurality of processors such as a parallel processor each of which may execute instructions. The parallel processing device can also include a plurality of memory slices each memory slice being h associated with one of the parallel processing devices and giving preferential access to that processor over other processing devices in the parallel processing device. Each memory slice can include a plurality of RAM tiles where each RAM tile can include a read port and a write port. In some cases each memory slice may be provided with a memory slice controller for providing access to a related memory slice. The processors and the RAM tiles can be coupled to one another via a bus. In some cases the bus can couple any of the processors with any of the memory slices. Suitably each RAM tile can include a tile control logic block for granting access to the tile. The tile control logic block is sometimes referred to as a tile control logic or an arbitration block.

In some embodiments the parallel processing device can further include at least one hardware accelerator configured to perform a predefined processing function for example image processing. In some cases the predefined processing function can include a filtering operation.

In some embodiments at least one hardware accelerator can be coupled to the memory slices via a separate bus. The separate bus can include an associated accelerator memory controller AMC which is configured to receive requests from at least one hardware accelerator and to grant to the hardware accelerator an access to a memory slice through the related memory slice controller. It will thus be appreciated that the memory access path employed by the hardware accelerators can be different to the path employed by the vector processors. In some embodiments the at least one hardware accelerator can include an internal buffer e.g. a FIFO memory to account for delays in accessing the memory slices.

In some embodiments the parallel processing device can include a host processor. The host processor can be configured to communicate with an AMC via a host bus. The parallel processing device can also be provided with an application programming interface API . The API provides a high level interface to the vector processors and or hardware accelerators.

In some embodiments the parallel processing device can operate in conjunction with a compiler that provides instructions for the parallel processing device. In some cases the compiler is configured to run on a host processor which is distinct from the processing elements such as a vector processor or a hardware accelerator. In some cases the compiler is configured to receive a flow graph via the image video API specifying an image processing process. The compiler can be further configured to map one or more aspects of the flow graph to one or more of the processing elements such as a vector processor or a hardware accelerator. In some embodiments a flow graph can include nodes and edges where each node identifies an operation and each edge identifies relationships between nodes e.g. operations such as an order in which the operations are carried out. The compiler can be configured to assign a node e.g. an operation to one of the processing elements to parallelize the computation of the flow graph. In some embodiments the flow graph may be provided in an extensible mark up language XML format. In some embodiments the compiler can be configured to allocate multiple flow graphs to a single processing element.

In some embodiments the parallel processing device can be configured to measure its performance and provide the information to the compiler. Therefore the compiler can use the past performance information received from the parallel processing device to determine the allocation of current tasks to processing elements in the parallel processing device. In some embodiments the performance information can be indicative of a number of access clashes experienced by one or more processing elements in the processing device.

In some cases the parallel processing device can be used in video applications which may be computationally expensive. To address the computational demand of video applications the parallel processing device can configure its memory subsystem to reduce the access clashes between processing units during memory access. To this end as discussed previously the parallel processing device can subdivide monolithic memory banks into multiple physical RAM instances instead of using the monolithic memory banks as a single physical block of memory. With this subdivision each physical RAM instance can be arbitrated for read and write operations thereby increasing the available bandwidth by the number of physical RAM instances in the memory bank.

In some embodiments the hardware cycle oriented arbitration can also provide multiple traffic classes and programmable scheduling masks. The multiple traffic classes and programmable scheduling masks can be controlled using the runtime scheduler. The hardware cycle oriented arbitration block can include a port arbitration block which can be configured to allocate multiple requesters of a single shared resource in a round robin scheduling scheme. In the round robin scheduling scheme requesters e.g. processing elements are granted access to a resource e.g. memory in the order the request was received from the requesters. In some cases the port arbitration block can augment the round robin scheduling scheme to account for the multiple traffic classes. The single shared resource can include a RAM tile shared registers or other resources that vector processors filter accelerators and RISC processors can access to share data. Additionally the arbitration block can allow for overriding the round robin allocation of resources with a priority vector or super priority vector. The priority vector or the super priority vector can be provided by a runtime scheduler in order to prioritize certain traffic classes e.g. video traffic classes as needed by a particular application of interest.

In some embodiments a processing element can include one or more of a processor such as a vector processor or a streaming hybrid architecture vector engine processor a hardware accelerator and a hardware filter operator.

The parallel processing device also includes an interconnection system that couples the processors and the memory slices . The interconnection system is referred to herein as an inter shave interconnect ISI . The ISI can include a bus through which processors can read or write data to any part of any one of the memory slices .

In some embodiments each memory slice N can include a plurality of RAM tiles or physical RAM blocks . . . N. For instance a memory slice N having the size of 128 kB can include four 32 kB single ported RAM tiles e.g. physical RAM elements organized as 4k 32 bit words. In some embodiments a tile can also be referred to as a logical RAM block. In some embodiment a tile can include a single ported complementary metal oxide semiconductor CMOS RAM. The advantage of a single ported CMOS RAM is that it is generally available in most semiconductor processes. In other embodiments a tile can include a multi ported CMOS RAM.

In some embodiments each tile can be associated with a tile control logic . The tile control logic is configured to receive requests from processors and provides access to the individual read and write ports of the associated tile . For example when a processing element N wants to access data in a RAM tile before the processing element N sends the memory data request to the RAM tile directly the processing element N can send a memory access request to the tile control logic associated with the RAM tile . The memory access request can include a memory address of data requested by the processing element N. Subsequently the tile control logic can analyze the memory access request and determine whether the processing element N can access the requested memory. If the processing element N can access the requested memory the tile control logic can send an access grant message to the processing element N and subsequently the processing element N can send a memory data request to the RAM tile .

As there is potential for simultaneous access by multiple processing elements in some embodiments the tile control logic can include a clash detector which is configured to detect an instance in which two or more processing elements such as a processor or an accelerator attempt to access any one of the tiles in a memory slice. The clash detector can monitor access to each tile for an attempted simultaneous access. The clash detector can be configured to report to the runtime scheduler that an access clash has occurred and needs to be resolved.

If the memory access request is for data stored in the RAM tile associated with the one hot address encoder then the one hot address encoder can provide a bit value 1 to the particular RAM tile s clash detector if the memory access request is not for data stored in the RAM tile associated with the one hot address encoder then the one hot address encoder can provide a bit value 0 to the particular RAM tile s clash detector .

In some embodiments the one hot address encoder is configured to determine whether the memory access request is for data stored in the RAM tile associated with the one hot address encoder by analyzing the target address of the memory access request. For example when the RAM tile associated with the one hot address encoder is designated with a memory address range of 0x0000 and 0x00ff then the one hot address encoder can determine whether the target address of the memory access request falls within the range of 0x0000 and 0x00ff. If so the memory access request is for data stored in the RAM tile associated with the one hot address encoder if not the memory access request is not for data stored in the RAM tile associated with the one hot address encoder . In some cases the one hot address encoder can use a range compare block to determine whether the target address of the memory access request falls within the address range associated with a RAM tile .

Once the clash detector receives bit values from all one hot address encoders the clash detector can count the number of 1 s in the received bit values e.g. sum the bit values to determine whether there is more than one processing elements currently requesting access to the same RAM tile . If there is more than one processing element currently requesting access to the same RAM tile the clash detector can report a clash.

For example when a particular RAM tile can be accessed by 64 processing elements a first clash detector can receive a memory access request from 32 processing elements and the second clash detector can receive a memory access request from the remaining 32 processing elements. The first clash detector can be configured to analyze one or more memory access requests from the 32 processing elements coupled to itself and determine a first number of elements of the 32 processing elements coupled to itself that are requesting access to a particular RAM tile . In parallel the second clash detector can be configured to analyze one or more memory access requests from the 32 processing elements coupled to itself and determine a second number of elements of the 32 processing elements coupled to itself that are requesting access to the particular RAM tile . Then the second clash detector can add the first number and the second number to determine how many of the 64 processing elements are requesting access to the particular RAM tile .

Once a clash detection system detects a clash the clash detection system can send a halt signal to a requester . shows an arbitration block for reporting a clash signal to a requester in accordance with some embodiments. More particularly the outputs of the range compare blocks in clash detection systems are combined using an OR gate to generate a halt signal to the requester. The half signal indicates that more than one processing element is attempting to access the same physical RAM sub block within the memory slice associated with the requester. Upon receiving the halt signal the requester can halt the memory access operation until the clash has cleared. In some embodiments the clash can be cleared by the hardware independently of the program code.

In some embodiments the arbitration block can operate at a cycle granularity. In such embodiments the arbitration block allocates resources at a clock cycle granularity rather than at a task level granularity which may include multiple clock cycles. Such cycle oriented scheduling can improve the performance of the system. The arbitration block can be implemented in hardware so that the arbitration block can perform the cycle oriented scheduling in real time. For example at any particular instance the arbitration block implemented in hardware can be configured to allocate resources for the next clock cycle.

The first port selection block includes a first leading one detector LOD and a second LOD . The first LOD is configured to receive a client request vector which can include a plurality of bits. Each bit in the client request vector indicates whether or not a message access request has been received from a requestor associated with that bit position. In some cases the client request vector operates in an active high mode. Once the first LOD receives the client request vector the first LOD is configured to detect a bit position counting from the left to the right at which the request becomes non zero for the first time thereby identifying the first memory access request counting from the left to right to the first port selection block . In parallel the client request vector can be masked by an AND logic operator to generate a masked client request vector using a mask generated by a mask register and a mask left shifter . The mask register can be set by a processor in communication with the mask register and the mask left shifter can be configured to shift to the left the mask represented by the mask register . The second LOD can receive the masked client request vector from the AND logic operator and detect the leading in the masked client request vector.

The output from the first LOD and the second LOD are then provided to the port 0 winner selection block . The port 0 winner selection block further receives two additional inputs a priority vector and a super priority vector. The port 0 winner selection block is configured to determine which one of the received memory access requests should be assigned to the slice port 0 based on the priorities of the inputs. In some embodiments the priorities of the inputs can be ranked as follows starting with super priority vector which has the highest priority priority vector which divides the masked LOD vector into priority and non priority requests followed by the unmasked LOD vector which has the lowest priority. In other embodiments other priorities can be specified.

While the first port selection block can be configured to determine whether the client request vector can be assigned to the slice port 0 the second port selection block can be configured to determine whether the client request vector can be assigned to the slice port 1 . The second port selection block includes a first trailing one detector TOD a second TOD a mask register a mask right shifter a port 1 winner selection block and a masking AND logic block . The TOD is configured to receive a client request vector which can include a plurality of bits and detect a bit position counting from the right to the left at which the vector becomes non zero for the first time. The operation of the second port selection block is substantially similar to the first port selection block except that it operates from right to left of the input vector selecting the trailing one in the input request vector using a trailing one detector .

The outputs of the port winner selection blocks are also provided to the same winner detection block which is configured to determine if the same memory access request has won access to both slice port 0 and slice port 1 . If the same client request vector has won access to both slice port 0 and slice port 1 the same winner detection block selects one or the slice ports to route the request and allocates the other port to the next highest ranking request in the input vector. This avoids over allocation of resources to a particular request thereby improving the allocation of resources to competing requesters.

The operation of the port arbitration block works by starting from the left hand side of the 32 bit client request vector and masked LOD outputs the position of the first masked request vector if this masked request vector is not superseded by a higher priority input via the priority or super priority vectors the requester corresponding to the LOD position wins and is granted access to port 0 . The LOD position is also used to advance the mask position via the 32 bit left shifter and is also used to compare with the port 1 LOD assignment to check if the same requester has been given access to both ports and in this case only one of the ports is granted with a flip flop being toggled to grant access on an alternating basis between ports 0 and 1 in the case of successive same winner detections. In the case the LOD output from the masked detector has been assigned priority via a corresponding one bit in the priority vector the requesting client is granted 2 back to back cycles access to port 0. In the case that there is no leading one in the masked client request vector and no higher priority request exists the unmasked LOD wins and is assigned access to port 0. In the case of any of the above cases a 1 bit in the super priority vector will override any of the previous requests and grant unrestricted access to port 0 to the requester.

The logic in the lower part of the diagram starts from the right hand side of the request vector and otherwise operates in the same manner as the upper part which starts from the left hand side of the request vector. In this case operation of the port 1 arbitration block in terms of priorities etc. is identical to that of the port 0 portion of the logic.

In some embodiments a processing element can include a buffer to reduce a latency of memory access due to memory access arbitration. illustrates a mechanism for reducing a memory access latency due to memory access arbitration in accordance with some embodiments. In a typical memory access arbitration scheme the memory access arbitration block is pipelined which leads to a fixed overhead arbitration penalty when allocating a shared resource such as a RAM tile to one of the plurality of processing elements e.g. requesters . For example when a requester sends a memory access request to an arbitration block it takes at least four cycles for the requester to receive an access grant message because it takes at least one cycle in each of the following steps 1 analyze the memory access request at the one hot address encoder 2 analyze the output of the one hot address encoder at the arbitration block 3 send an access grant message to the one hot address encoder by the arbitration block and 4 send the access grant message to the requester by the one hot address encoder . Then subsequently the requester has to send a memory data request to the RAM tile and receive data from the RAM tile each of which takes at least one cycle. Therefore a memory access operation has a latency of at least six cycles. This fixed penalty would reduce the bandwidth of the memory subsystem.

This latency issue can be addressed with a memory access request buffer maintained in the processing element . For example the memory access request buffer can receive memory access requests from the processing element every clock cycle and store the received memory access requests until they are ready to be sent to the memory arbitration block . The buffer in effect synchronizes the rate at which memory access requests are sent to the memory arbitration block and the rate at which the data is received from the memory subsystem. In some embodiments the buffer can include a queue. The number of elements in the buffer e.g. the depth of the buffer can be greater than the number of cycles for retrieving data from the memory subsystem. For example when the RAM access latency is 6 cycles the number of elements in the buffer can be 10. The buffer can reduce the arbitration latency penalty and improve the throughput of the memory subsystem. With the memory access request buffer in principle up to 100 of total memory bandwidth can be allocated between requesters.

It will be understood that a potential problem with using multiple RAM instances is that by allowing simultaneous access by multiple processing elements to sub instances within a bank a memory contention may result.

The present disclosure provides at least two approaches to address the memory contention. First care is taken in the software design as will be described later to avoid a memory contention and or a memory clash by carefully laying out the data in the memory subsystem so as to reduce the contention and or the memory clash. Furthermore the software development tools associated with the parallel processing device can allow the memory contention or the memory clash to be reported during the software design phase. Therefore the memory contention issues or the memory clash issues can be corrected by improving the data layout in response to the memory contention or the memory clash reported during the software design phase.

Second as described further below the ISI block within the architecture is configured to detect port clashing contention in hardware and stall processing elements with lower priorities. For example the ISI block is configured to analyze memory access requests from processing elements service the sequence of memory access requests and route memory access requests in accordance with the priority order so that all data reads or writes from all processing elements are completed in the priority order.

The priority order amongst the processing elements can be established in a number of ways. In some embodiments the priority order may be defined statically at system design time. For example the priority order can be coded as a reset state for system registers so that when a system powers up the system powers up with a set of pre assigned priorities. In other embodiments the priority order can be dynamically determined via user programmable registers.

In some embodiments programmers may plan the data layout for their software applications in order to reduce contention for shared sub blocks of memory within a memory slice. In some cases the planning of the data layout can be assisted by an arbitration block. For example the arbitration block can detect a memory contention grant access to the memory on the basis of priority to a processing element associated with the highest priority task stall other processing elements which are contending and unroll the contention process by process until the contention has been resolved.

In some embodiments the element 2 can include a processing element e.g. a vector processor or a hardware accelerator that can filter an input with a 3 3 blur filter. The element 2 can be configured to receive an input from the shared buffer which temporarily maintains an output of the element 1 . In order to apply a 3 3 blur kernel to an input the element 2 can receive at least 3 lines of data from the shared input buffer before it can commence operation. Thus the SW scheduler which can run on a RISC processor can detect that the correct number of lines of data is contained in the shared buffer before signalling to the element 2 that it can commence the filtering operation.

Following the initial signal that 3 lines of data are present the SW scheduler can be configured to signal to the element 2 when each additional new line has been added to the rolling 3 line buffer . In addition to the line by line synchronisation cycle by cycle arbitration and synchronisation is performed for each element in the pipeline. For instance the element 1 can include a hardware accelerator which produces one complete output pixel per cycle. In order to achieve this throughput the hardware accelerator can keep the input buffer full so the processing block has sufficient data to continue its operations. This way the processing block can produce sufficient output to keep the throughput of the element 1102 as high as possible.

In some embodiments a software tool chain can predict memory conflicts from analyzing software program using the memory subsystem. The software tool chain can include a graphic user interface GUI based integrated development environment IDE e.g. Eclipse based IDE from which the developer can edit code call the compiler assembler and perform source level debugging when required. The software tool chain can be configured to predict memory conflicts via dynamic analysis of programs running on multiple processors using a system simulator which models all processing bus memory elements and peripherals. The software tool chain can also be configured to log in a log file or a display device if different programs running on different processors or hardware resources attempt contemporaneous access to a particular block of a memory slice. The software tool chain can be configured to log in a cycle by cycle basis.

In some embodiments the pipeline can also include one or more hardware counters e.g. one counter for each memory instance which are incremented each time a memory clash occurs. These counters may then be read by a hardware debugger e.g. JTAG and displayed on a screen or logged to a file. Subsequent analysis of the log files by the system programmer can allow memory accesses to be scheduled differently so as to reduce possibility for memory port clashing.

One key difficulty for programmers of IBM s Cell architecture illustrated in is programmatically scheduling data transfers hundreds of cycles in advance so that data can be controlled by the DMA and be stored in local storage LS by the time a vector processor accesses the data. Some embodiments of the disclosed architecture can address this issue by handling the arbitration and scheduling of accesses in hardware and logging clashes in user readable hardware counters. This allows the disclosed architecture to be used to create a high performance video image processing pipeline.

In some embodiments the disclosed parallel processing device can be configured to operate in conjunction with a pipeline description tool e.g. a software application which allows image processing pipelines to be described as a flow graph. The pipeline description tool is capable of describing image vision processing pipelines in a flexible way that is independent of the underlying hardware software platform. In particular the flow graph used by the pipeline description tool allows tasks to be described independently of the processing elements e.g. the processor and filter accelerator resources that may be used to implement the flow graph. The resulting output of the pipeline description tool can include a description of the directed acyclic graph DAG or flow graph. The description of the DAG or the flow graph can be stored in a suitable format such as XML.

In some embodiments the description of the DAG or the flow graph can be accessible to all other tools in the system and can be used to control operations of the parallel processing device in accordance with the DAG. illustrates how the description of the DAG or the flow graph can be used to control operations of a parallel processing device in accordance with some embodiments.

Prior to the actual operation of the computing device a compiler for the parallel processing device can take 1 a description of the flow graph and 2 a description of available resources and generate a task list indicating how the DAG can be performed across multiple processing elements. For example when the task cannot be performed on a single processing element the compiler can split the task across multiple processing elements when the task can be performed on a single processing element the compiler can assign the task to a single processing element.

In some cases when the task would only use a portion of the capabilities of a processing element the compiler can fuse and schedule multiple tasks to be executed on a single processing element in a sequential manner up to the limit that can be supported by a processing element. illustrates the scheduling and the issuing of tasks by the compiler and the scheduler in accordance with some embodiments. The advantage of scheduling tasks using a compiler and the scheduler is that the compiler and the scheduler can automatically schedule tasks based on operations performed by the tasks. This is a big advantage to prior art in which a programmer had to determine manually the schedule for code running on a processing element or a group of processing elements running a particular task including when to schedule data transfers by DMA from peripherals to CMX from CMX to CMX block and from CMX back to peripherals. This was a laborious and error prone task and the usage of DFGs allows this process to be automated saving time and increasing productivity.

During runtime of the computing device the runtime scheduler can dynamically schedule the task across the available processing elements based on the task list generated by the compiler . The runtime scheduler can operate on the host RISC processor in a multicore system and can schedule tasks across the processing elements such as a plurality of vector processors filter accelerators and Direct Memory Access DMA engines using the statistics from the hardware performance monitors and timers . In some embodiments the hardware performance monitors and timers can include stall counters CMX clash counters bus cycle counters ISI APB and AXI and cycle counters which may be read by the runtime scheduler .

In some embodiments the runtime scheduler can assign tasks to available processing elements based on statistics from hardware performance monitors and timers . The hardware performance monitors and timers can be used to increase the efficiency of the processing elements or to perform a task using fewer number of processing elements in order to save power or allow other tasks to be computed in parallel.

To this end the hardware performance monitors and timers can provide a performance metric. The performance metric can be a number that indicates the activity level of a processing element. The performance metric can be used to control the number of instantiated processing elements for performing a task. For example when the performance metric associated with a particular processing element is greater than a predetermined threshold then the runtime scheduler can instantiate an additional processing element of the same type as the particular processing element thereby distributing the task over more processing elements. As another example when the performance metric associated with a particular processing element is less than a predetermined threshold then the runtime scheduler can remove one of the instantiated processing elements of the same type as the particular processing element thereby reducing the number of processing elements performing a certain task.

In some embodiments the runtime scheduler can prioritize the use of processing elements. For example the runtime scheduler can be configured to determine whether the task should be preferably assigned to a processor or a hardware filter accelerator.

In some embodiments the runtime scheduler can be configured to change the CMX buffer layout in the memory subsystem so that the system can comply with runtime configuration criteria. The runtime configuration criteria can include for example image processing throughput frames per second the energy consumption the amount of memory used by the system the number of operating processors and or the number of operating filter accelerators.

An output buffer can be laid out in memory in one of several ways. In some cases the output buffer can be physically contiguous in memory. In other cases the output buffer can be chunked or sliced. For example the output buffer can be split into N vertical strips where N is the number of processors assigned to the image processing application. Each strip is located in a different CMX slice. This layout can favour processors since each processor can locally access input and output buffers. However this layout can be detrimental for filter accelerators because such a layout can cause a lot of clashes for filter accelerators. Filter accelerators often process data from left to right. Therefore all filter accelerators would initiate their processes by accessing the first strip of image which could cause a lot clashes from the start. In other cases the output buffer can be interleaved. For example the output buffer can be split across all 16 CMX slices with a predetermined size interleaving. The predetermined size can be 128 bits. The interleaved layout of the output buffer can favour filter accelerators because spreading the accesses across CMX reduces the likelihood of clashes.

In some embodiments a buffer such as an input buffer or an output buffer can be allocated based on whether its producers and consumers are hardware and or software. Consumers are more important since they typically need more bandwidth filters usually read multiple lines and output one line . The hardware filters are programmed accordingly to the layout of the buffers they support contiguous interleaved and sliced memory addressing .

In steps the runtime compiler is configured to issue the tasks to associated hardware components as the associated hardware components become available for new tasks. For example in step when the DMA becomes available to perform a new task the runtime compiler is configured to dequeue the first queue for the DMA and provide the dequeued task to the DMA. Likewise in step when the processor becomes available to perform a new task the runtime compiler is configured to dequeue the second queue for the processor and provide the dequeued task to the processor. Also in step when the hardware filter becomes available to perform a new task the runtime compiler is configured to dequeue the third queue for the hardware filter and provide the dequeued task to the hardware filter.

In some embodiments the runtime scheduler may use the counter values from the hardware performance monitors and timers to adjust the usage of the processing elements especially where more than one pipeline e.g. a software application is running on the array of processing elements simultaneously as these pipelines have not necessarily been co designed. For instance if the effective bus bandwidth allocated to each pipeline is less than expected and a number of clashes occurring in accessing the CMX memory is large the runtime scheduler may use this information to stagger execution of the 2 pipelines by modifying the order in which tasks are taken from the 2 pipeline queues thereby reducing memory clashes.

In some embodiments the DAG compiler can operate in real time e.g. online . illustrates an operation of a real time DAG compiler in accordance with some embodiments. The real time DAG compiler can be configured to receive an input XML description of the DAG the description of the available processing elements and any user defined constraints such as the number of processors frame rate power dissipation target etc. Then real time DAG compiler can be configured to schedule the DAG components across the processing elements including for example the DMA engine a processor a hardware filter and memory to ensure that the DAG as specified can satisfy the user defined constraints when mapped to the system resources. In some embodiments the real time DAG compiler can determine whether the tasks in the DAG can be performed in parallel in a breath first manner. If the breadth of the DAG is larger than the number of processing elements available to perform the tasks in parallel e.g. the amount of available processing power is less than the parallelism of the DAG the real time DAG compiler can fold the tasks so that the tasks are performed sequentially on the available processing elements.

In some embodiments when a task would only use a portion of the capabilities of a processing element the runtime scheduler can also be configured to fuse and schedule multiple tasks to be executed on a single processing element in a sequential manner up to the limit that can be supported by a processing element as illustrated in .

In an image processing application a scheduler can be configured to divide up processing tasks amongst processors by dividing an image into strips. For example the image can be divided into vertical strips or horizontal strips of a predetermined width.

In some embodiments the scheduler can predetermine the number of processors used for a particular image processing application. This allows the scheduler to predetermine the number of strips for the image. In some embodiments a filtering operation can be performed by the processors in series. For example when there are 5 software filters executed by the application the processors can each be configured to execute the first software filter simultaneously at a first time instance the second software filter simultaneously at a second time instance etc. This means that the computational load is more evenly balanced among the processors assigned to the particular image processing application. This is because the processors are configured to simultaneously execute the same list of filters in the same order.

When too many processors are assigned to the image processing application the processors can spend a lot of time idling waiting on hardware filter accelerators to complete the tasks. On the other hand when too few processors are assigned to the application the hardware filter accelerators can spend a lot of time idling. In some embodiments the run scheduler can be configured to detect these situations and adapt accordingly. In other embodiments the scheduler can be configured to over assign processors to the particular image processing applications and allow the processors to power down once the processors have completed its tasks ahead of the hardware filter accelerators.

In some embodiments the scheduler can use a barrier mechanism to synchronize processing elements such as hardware filter accelerators and processors. The output of the scheduler can include a stream of commands. These commands can include 1 start commands for processing elements such as hardware filter accelerators and processors and 2 barrier commands. A barrier command indicates that the processing elements should wait from proceeding with a next set of commands until all processing elements in the group have reached the barrier command even if some of the processing elements have actually completed their tasks. In some embodiments the scheduler may provide the barrier command based on dependencies between tasks performed by the processing elements.

In some embodiments the barrier mechanism is implemented in hardware using interrupt signals . For example the scheduler can program a bit mask which specifies which processing elements belong to a group. As the processing elements complete the assigned tasks interrupt signals associated with the respective processing elements are asserted. Once all interrupt signals associated with the processing elements in the group have been asserted the controller of the processing elements can receive a global interrupt signal indicating that all processing elements have reached the barrier command.

In some embodiments the interrupt sources can include SHAVE vector processors RISC processors hardware filters or external events. In particular the hardware filters can support various modes including a non circular buffer mode where the input output buffer contains a frame and the filter can be configured to issue a single interrupt either when it has processed the whole input frame or written the complete corresponding output frame. The filters are also programmable to operate on lines patches or tiles from frames using appropriate settings for image dimensions buffer base address line stride etc.

One important challenge with a complex parallel processing device how to program the processing elements in the parallel processing device particularly for embedded systems which are very power sensitive and scarce in terms of resources like computational resources and memory. Computational imaging video and image processing in particular are very demanding in terms of performance on embedded systems as the frame dimensions and rates are very high and increasing strongly from year to year.

The solution to this problem presented herein is to provide an Application Programming Interface API which allows applications to be written at high level by a programmer without intimate knowledge of the details of the multicore processor architecture . Using the software API the programmer can rapidly create new image or video processing pipelines without knowing the intimate details of the implementation as details of whether functions are implemented in software on programmable processors or in hardware are abstracted away from the programmer. For instance an implementation of a blur filter kernel is provided as a reference software implementation running on one or more processors or hardware accelerator filters. The programmer can initially use a software blur filter implementation and can switch to using the hardware filter with no change to the overall pipeline implementation as the ISI AMC and CMX arbitration block deal with which processor and HW resources get access to the physical memory blocks and in which order not the programmer.

While the multi ported memory approach described above is adequate for sharing memory at high bandwidth and low latency between identical processors it is not ideal for sharing bandwidth with other devices. These other devices may be hardware accelerators and other processors with differing latency requirements particularly in very high bandwidth applications such as computational video and image processing.

The disclosed architecture can be used in conjunction with a multi ported memory subsystem to provide higher bandwidth to support more simultaneous accesses from a multiplicity of programmable VLIW processors with highly deterministic latency requirements a large cohort of programmable image video processing hardware filters and bus interfaces to allow control and data access by a conventional host processor and peripherals. illustrates the parallel processing device having different types of processing elements in accordance with some embodiments. The parallel processing device includes a plurality of processors and a plurality of filter accelerators and the plurality of processors and the plurality of filter accelerators can be coupled to the memory subsystem via the ISI and the accelerator memory controller AMC respectively.

The subsystem of AMC and Multicore Memory CMX subsystem provides on chip storage facilitating low power streaming digital signal processing in software on the processors as well as hardware filter accelerators for particular image video processing applications. In some embodiments the CMX memory is organized into 16 slices of 128 kB organised as 64 bit words 2 MB in total . Each processor can have direct access to a slice in the memory subsystem and indirect higher latency access to all other slices in the memory subsystem . The processors may use the CMX memory for storing instructions or data while hardware filter accelerators use CMX memory to store data.

In order to facilitate data sharing between heterogeneous processing elements whilst allowing latency intolerant processors to achieve high performance when accessing a shared CMX memory with HW filter accelerators the HW filter accelerators are designed to be latency tolerant. This is achieved by providing each HW filter accelerators filters with local FIFOs which make timing more elastic as well as a cross bar switch to share access to the CMX leaving the ISI free to support inter SHAVE communication without contention from HW filter accelerators as illustrated in in accordance with some embodiments.

In addition to external going port clashes it is possible to clash with an incoming ISI port access. If more than one external slice attempts to access the same slice of memory in any one cycle then a port clash may occur. The mapping of LSU port to ISI interconnect port is fixed so it is possible for SHAVE 0 to access slice 2 through LSU port 0 and SHAVE 11 to access slice 2 through LSU port 1 without any clashes. The ISI matrix can allow for 8 2 ports 64 bits of data to be transferred every cycle. For example SHAVEN can access Slice N 1 through LSU port 0 and 1 and all 8 SHAVE processors can access simultaneously without any stalling.

In some embodiments the memory subsystem can be logically divided into slices blocks . illustrates the proposed multicore memory subsystem in accordance with some embodiments. shows a detailed bus inter connection between AXI AHB SHAVEs ISI and CMX as well as filter accelerators AMC and CMX. The diagram shows two AMC input and two AMC output ports 2 ISI input and 2 ISI output ports connections to L2 cache and mutex as well as internal read write arbitration and source multiplexing to address the 4 memory tiles and FIFO as well as output destination selection of the 4 memory block outputs to ISI and AMC.

Each slice can connect to two of 16 possible ISI input sources including 12 SHAVEs DMA Texture Management Unit TMU and AHB bus interface to the on board host processor. Similarly each slice has 2 output ISI ports which allow a slice to send data to 2 of 16 possible destinations including 12 SHAVEs DMA Texture Management Unit TMU and AXI and AHB bus interfaces to the on board host processor. In the preferred implementation the slice contains 4 physical RAM blocks with input arbitration block which in turn connects to the local SHAVE processor 2 LSUs and 2 64 bit instruction ports 2 ISI input ports 2 AMC input ports and FIFO used for inter SHAVE messaging as well as a messaging FIFO L2 cache and mutual exclusion mutex blocks.

On the output path from a CMX slice the input to the destination selection block is connected to the 4 RAM instances along with the L2 cache and hardware mutex blocks. The outputs from the destination selection block as illustrated in as the block connects to the 2 local LSU ports and Instruction ports SP1 and SP0 as well as 2 ISI output ports and 2 AMC output ports. The 2 ISI ports allow a local slice to be connected to two destinations from the 12 possible processors DMA TMU AXI and AHB host busses. processors are provided with access to the memory via the Inter SHAVE Interconnect ISI which connects to the 2 64 bit ISI input and 2 64 bit ISI output ports contained in a multicore memory subsystem slice. The high bandwidth low latency deterministic access provided by the ISI interconnect reduces stalling in the processors and provides high computational throughput.

In some embodiments a slice port controller is provided in the AMC which communicates with the 2 read and 2 write ports in the associated CMX memory slice as shown in . Seen from the filter accelerator side of the CMX memory subsystem each hardware filter is connected to a port on the accelerator memory controller AMC crossbar . The AMC has a pair of 64 bit read and a pair of 64 bit write ports which connect it to each slice of CMX memory there are 16 slices for a total of 2 MB in the preferred implementation . Connecting the image processing hardware accelerators to the AMC via read or write client interfaces and local buffering within the accelerators allows more relaxed latency requirements leaving more bandwidth available to the ISI and processors with highly deterministic timing allowing stalling of the processors to be reduced.

In addition to arbitrating between 64 requesters on each of the 2 AMC ports in a slice shown in an additional 2 1 arbiter is provided to arbitrate between AMC ports 1 and 0. The purpose of this 2 1 arbiter is to prevent one or other of the 2 AMC ports from saturating all of the AMC port bandwidth which could lead to excessive stalling on one of the requesting ports. This enhancement provides more balanced allocation of resources in the presence of multiple heavy requesters of port bandwidth and hence higher sustained throughput for the overall architecture. Similarly a 2 1 arbiter arbitrates between the 2 processor ports SP1 and SP 0 for similar reasons.

The arbitration and multiplexing logic also controls access by the processors either directly or via the ISI to a shared L2 cache via a second level of arbitration which shares access on a strictly round robin basis between 16 possible sources where one 64 bit port is connected between the second level arbiter and each of the 16 CMX slices. Similarly the same logic allows access to the 32 hardware mutexes which are used for inter processor negotiation of mutual exclusion among threads running on the 12 on board processors and 2 32 bit RISC processors via the AHB and AXI bus connections on the ISI .

The priority in the preferred implementation is that SP1 and SP0 have the highest priority then LSU1 and LSU0 followed by ISIout1 and ISIout0 and finally AMCout1 and AMCout0 and finally the FIFO have the lowest priority. The reason for this priority assignment is that SP1 and SP0 control program access by the processor to CMX and the processor will stall immediately if the next instruction is not available followed by LSU1 and LSU0 which will again cause the processor to stall similarly ISIout1 and ISIout0 come from other processors and will cause them to stall if data is not immediately available. The AMCout1 and AMCout0 ports have the lowest priority as they have built in FIFOs and hence can tolerate a lot of latency before stalling. The processor FIFO is only required for low bandwidth inter processor messaging and hence has the lowest priority of all.

Once the arbiter has allowed access to up to 4 sources to the 4 SRAM tiles L2 cache mutexes and FIFO the output data from the six read data sources including 4 SRAM tiles L2 cache and mutexes is selected and directed to up to 4 out of 8 possible 64 bit destination ports 4 on the processor associated with the slice SP1 SP0 LSU1 and LSU0 2 associated with the ISI ISIout1 and ISIout0 and finally 2 associated with the AMC AMCout1 and AMCout0 . No prioritisation is required in the output multiplexer as only 4 64 bit sources need be distributed to 8 destination ports.

The number of read and write client interfaces to the CMX are separately configurable. Any client may address any slice or slices of CMX. With 16 slices in CMX 2 ports per slice and a system clock frequency of 600 MHz the maximum total data memory bandwidth which may be supplied to the clients is 143 GB s Max bandwidth 600 MHz 64 8 2 16 1.536e11 B s 143 GB s.

At the higher clock rate of 800 MHz the bandwidth rises to 191 GB sec. The AMC arbitrates between simultaneous accesses on its client read write interfaces of the hardware accelerator blocks connected to it. A maximum of two read write accesses may be granted per slice per clock cycle giving a maximum slice memory bandwidth of 8.9 GB s at a system clock frequency of 600 MHz. Client access is not restricted to CMX address space. Any access falling outside of CMX address space is forwarded to the AMC s AXI bus master.

The Streaming Image Processing Pipeline SIPP software framework used in conjunction with the exemplary arrangement of provides a flexible approach to implementing image processing pipelines using CMX memory for scan line buffers frame tiles subsections of frames or indeed entire frames at high resolution with the adjunct of an external DRAM die in package wire bonded to a substrate to which the image video processing die is attached. The SIPP framework takes care of complexities such as handling image borders replication of pixels and circular line buffer management making the implementation of ISP Image Signal Processing functions in software on the processor simpler and more generic.

The Accelerators include a collection of hardware image processing filters that can be used in the SIPP software framework . The Accelerators can enable some of the most computationally intensive functionality to be offloaded from the processing elements . The diagram shows how a plurality of filter accelerators may be connected to the AMC which performs Address filtering arbitration and multiplexing. Also connected to the AMC may be multiple MIPI camera serial interfaces in the preferred implementation a total of 12 MIPI serial lanes are connected in 6 groups of 2 lanes. The AMC also is connected to AXI and APB interfaces to allow the 2 system RISC processors in the reference implementation to access the CMX memory via the AMC. The final element of the diagram is the CMX which the AMC arbitrates access to allowing simultaneous access by multiple hardware filter accelerators to the physical RAM instances in the CMX memory . A reference filter accelerator is also shown in this case a 5 5 2D filter kernel which contains an fp16 IEEE754 like 16 bit floating point format arithmetic pipeline an associated pipeline stall controller line buffer read client to store a line of input to the fp16 pipeline a line start control input and line buffer write client to store output from the fp16 pipeline. To allow the Accelerators to fit within the SIPP framework they require high bandwidth access to CMX memory this is provided by the Accelerator Memory Controller AMC .

In some embodiments the CMX memory subsystem can be split into 128 kB blocks or slices associated with their neighbouring processing element for high speed low power access. Within a slice the memory is organized as a number of smaller tiles for example 3 32 kB 1 16 kB and 2 8 KB independent SRAM blocks. The physical RAM size can be chosen as a trade off of area utilization and configuration flexibility. Any processing element can access physical RAM anywhere in the memory subsystem CMX with the same latency 3 cycles but access outside of a local processor slice is limited in bandwidth and will consume more power than an access a local slice. In general to decrease the power consumption and to increase performance a processing element can store data locally in a dedicate memory slice.

In some embodiments each physical RAM can be 64 bit wide. If more than one processing element attempts to access a single physical RAM a clash can occur resulting in a processor stall. The CMX will automatically arbitrate between port clashes and ensure that no data is lost. For every port clash a processing element is stalled for a cycle resulting in lower throughput. With careful data layout by the programmer within the CMX port clashes can be avoided and processor cycles better utilized.

It can be seen from the image processing HW architecture shown in that each filter accelerator can include at least one AMC read and or write client interface to access CMX memory . The number of read write client interfaces on the AMC is suitably configurable. The AMC can include a pair of 64 bit ports into each slice of CMX memory . The AMC routes requests from its clients to the appropriate CMX slice by partial address decode . Simultaneous requests to the same slice from different clients can be arbitrated in round robin manner Read data returned from CMX is routed back to the requesting AMC read clients.

AMC clients accelerators present a full 32 bit address to the AMC . Accesses from clients which do not map to CMX memory space are forwarded to the AMC s AXI master. Simultaneous accesses outside CMX memory space from different clients are arbitrated in a round robin manner.

The AMC is not limited to providing CMX access to filter accelerators any hardware accelerator or 3rd party elements may use the AMC to access CMX and the wider memory space of the platform if its memory interfaces are suitably adapted to the read write client interfaces on the AMC.

The hardware image processing pipeline SIPP can include the filter accelerators an arbitration block MIPI control APB and AXI interfaces and connections to the CMX multiport memory as well as an exemplary hardware 5 5 filter kernel. This arrangement allows a plurality of processors and hardware filter accelerators for image processing applications to share a memory subsystem composed of a plurality of single ported RAM Random Access Memory physical blocks.

The use of single ported memories increases the power and area efficiency of the memory subsystem but limits the bandwidth. The proposed arrangement allows these RAM blocks to behave as a virtual multi ported memory subsystem capable of servicing multiple simultaneous read and write requests from multiple sources processors and hardware blocks by using multiple physical RAM instances and providing arbitrated access to them to service multiple sources.

The use of an Application Programming Interface API and data partitioning at the application level is important in order to ensure that contention for physical RAM blocks between processors and filter accelerators or processors among themselves is reduced and hence the data bandwidth delivered to processors and hardware is increased for a given memory subsystem configuration.

In some embodiments the parallel processing device can reside in an electronic device. illustrates an electronic device that includes a parallel processing device in accordance with some embodiments. The electronic device can include a processor memory one or more interfaces and a parallel processing device .

The electronic device can have memory such as a computer readable medium flash memory a magnetic disk drive an optical drive a programmable read only memory PROM and or a read only memory ROM . The electronic device can be configured with one or more processors that process instructions and run software that may be stored in memory . The processor can also communicate with the memory and interfaces to communicate with other devices. The processor can be any applicable processor such as a system on a chip that combines a CPU an application processor and flash memory or a reduced instruction set computing RISC processor.

In some embodiments the compiler and the runtime scheduler can be implemented in software stored in memory and operate on the processor . The memory can be a non transitory computer readable medium flash memory a magnetic disk drive an optical drive a programmable read only memory PROM a read only memory ROM or any other memory or combination of memories. The software can run on a processor capable of executing computer instructions or computer code. The processor might also be implemented in hardware using an application specific integrated circuit ASIC programmable logic array PLA field programmable gate array FPGA or any other integrated circuit.

In some embodiments the compiler can be implemented in a separate computing device in communication with the electronic device over the interface . For example the compiler can operate in a server in communication with the electronic device .

The interfaces can be implemented in hardware or software. The interfaces can be used to receive both data and control information from the network as well as local sources such as a remote control to a television. The electronic device can also provide a variety of user interfaces such as a keyboard a touch screen a trackball a touch pad and or a mouse. The electronic device may also include speakers and a display device in some embodiments.

In some embodiments a processing element in the parallel processing device can include an integrated chip capable of executing computer instructions or computer code. The processor might also be implemented in hardware using an application specific integrated circuit ASIC programmable logic array PLA field programmable gate array FPGA or any other integrated circuit.

In some embodiments the parallel processing device can be implemented as a system on chip SOC . In other embodiments one or more blocks in the parallel processing device can be implemented as a separate chip and the parallel processing device can be packaged in a system in package SIP . In some embodiments the parallel processing device can be used for data processing applications. The data processing applications can include image processing applications and or video processing applications. The image processing applications can include an image processing process including an image filtering operation the video processing applications can include a video decoding operation a video encoding operation a video analysis operation for detecting motion or objects in videos. Additional applications of the present invention include machine learning and classification based on sequence of images objects or video and augmented reality applications including those where a gaming application extracts geometry from multiple camera views including depth enabled cameras and extracts features from the multiple views from which wireframe geometry for instance via a point cloud can be extracted for subsequent vertex shading by a GPU.

The electronic device can include a mobile device such as a cellular phone. The mobile device can communicate with a plurality of radio access networks using a plurality of access technologies and with wired communications networks. The mobile device can be a smart phone offering advanced capabilities such as word processing web browsing gaming e book capabilities an operating system and a full keyboard. The mobile device may run an operating system such as Symbian OS iPhone OS RIM s Blackberry Windows Mobile Linux Palm WebOS and Android. The screen may be a touch screen that can be used to input data to the mobile device and the screen can be used instead of the full keyboard. The mobile device may have the capability to run applications or communicate with applications that are provided by servers in the communications network. The mobile device can receive updates and other information from these applications on the network.

The electronic device can also encompasses many other devices such as televisions TVs video projectors set top boxes or set top units digital video recorders DVR computers netbooks laptops tablet computers and any other audio visual equipment that can communicate with a network. The electronic device can also keep global positioning coordinates profile information or other location information in its stack or memory.

It will be appreciated that whilst several different arrangements have been described herein that the features of each may be advantageously combined together in a variety of forms to achieve advantage.

In the foregoing specification the application has been described with reference to specific examples. It will however be evident that various modifications and changes may be made therein without departing from the broader spirit and scope of the invention as set forth in the appended claims. For example the connections may be any type of connection suitable to transfer signals from or to the respective nodes units or devices for example via intermediate devices. Accordingly unless implied or stated otherwise the connections may for example be direct connections or indirect connections.

It is to be understood that the architectures depicted herein are merely exemplary and that in fact many other architectures can be implemented which achieve the same functionality. In an abstract but still definite sense any arrangement of components to achieve the same functionality is effectively associated such that the desired functionality is achieved. Hence any two components herein combined to achieve a particular functionality can be seen as associated with each other such that the desired functionality is achieved irrespective of architectures or intermediate components. Likewise any two components so associated can also be viewed as being operably connected or operably coupled to each other to achieve the desired functionality.

Furthermore those skilled in the art will recognize that boundaries between the functionality of the above described operations are merely illustrative. The functionality of multiple operations may be combined into a single operation and or the functionality of a single operation may be distributed in additional operations. Moreover alternative embodiments may include multiple instances of a particular operation and the order of operations may be altered in various other embodiments.

However other modifications variations and alternatives are also possible. The specifications and drawings are accordingly to be regarded in an illustrative rather than in a restrictive sense.

In the claims any reference signs placed between parentheses shall not be construed as limiting the claim. The word comprising does not exclude the presence of other elements or steps than those listed in a claim. Furthermore the terms a or an as used herein are defined as one or more than one. Also the use of introductory phrases such as at least one and one or more in the claims should not be construed to imply that the introduction of another claim element by the indefinite articles a or an limits any particular claim containing such introduced claim element to inventions containing only one such element even when the same claim includes the introductory phrases one or more or at least one and indefinite articles such as a or an. The same holds true for the use of definite articles. Unless stated otherwise terms such as first and second are used to arbitrarily distinguish between the elements such terms describe. Thus these terms are not necessarily intended to indicate temporal or other prioritization of such elements. The mere fact that certain measures are recited in mutually different claims does not indicate that a combination of these measures cannot be used to advantage.

