---

title: Architecture for agentless service insertion
abstract: An example method for service insertion in a network environment is provided in one example and includes configuring a service node by tagging one or more interface ports of a virtual switch function to which the service node is connected with one or more policy identifiers. When data traffic associated with a policy identifier is received on a virtual overlay path the virtual switch function may then terminate the virtual overlay path and direct raw data traffic to the interface port of the service node that is tagged to the policy identifier associated with the data traffic.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09178828&OS=09178828&RS=09178828
owner: CISCO TECHNOLOGY, INC.
number: 09178828
owner_city: San Jose
owner_country: US
publication_date: 20130426
---
The present disclosure relates to network services and more particularly to a system and method for service insertion and provisioning in a network.

Data centers may host applications and store large amounts of data for an organization or multiple organizations. An enterprise data center may be privately owned and discretely provide services for a number of customers with each customer using data center resources by way of private networks. In addition these data centers provide server and desktop virtualization that is dramatically changing the enterprise network by creating many virtual networks which connect virtual machines and the physical networks through virtual switches. In this new network paradigm many new network service requirements are imposed on modern Information Technology IT network infrastructure.

The service requirements e.g. load balancing wide area network WAN acceleration network security etc. have traditionally been provided by inline network appliances referred to as service nodes. These inline network appliances do not scale well in a virtualized network environment in which end point applications are provided by virtual machines. At any given point in time virtual machines are being instantiated stopped and dynamically migrated to another physical host. In a virtual network the network services should support this dynamic provisioning scalability and virtual machine mobility.

According to embodiments of the disclosure a system and a method is provided to implement a service node configured to provide network traffic services for one or more hosts in a network. The service node may be inserted e.g. dynamically and transparently into the network by associating one or more interface ports of a switch function to which the service node is connected with one or more service policies of the service node. When data traffic associated with a service policy is received on an overlay path the overlay path may be terminated. The term terminated in this sense can include any activity associated with ending interrupting breaking pausing or otherwise stopping a communication. The data traffic may then be directed as raw data traffic to the interface port associated with the service policy and the service node providing the service receives the raw data. This allows insertion of the service node without requiring insertion of control functions that would otherwise be necessary to couple the service node to data and control plane mechanisms used for delivery of data traffic in the network. The embodiments may be implemented to handle dynamic characteristics of virtual environments such as virtual machine or service node migration. The example embodiments also allow service insertion in the physical switch and network environment or in a hybrid environment.

According to an example embodiment of the disclosure a system and method is provided to implement a service node configured to provide network traffic services for one or more virtual machines in a virtual network. The virtual service node may be dynamically and transparently inserted into the virtual network by tagging one or more interface ports of a virtual switch function to which the service node is connected with one or more policy identifiers. Data traffic associated with a policy identifier and received on a virtual overlay path may then be directed as raw data traffic to the interface port of the service node that is tagged to the policy identifier associated with the data traffic.

The system and method may be implemented in a virtual network having a service classifier dispatcher VPATH function that controls switching and routing in the data plane of the virtual network and a service controller in the form of a Virtual Services Module VSM or Virtual Network Management Center VNMC that controls configuration and programming of the VPATH function for data plane control. The VPATH function implements switching and routing on virtual overlays through a virtual network including one or more virtual switch functions or Virtual Ethernet Modules VEMs implemented in one or more switches or routers in the network and implements control of service node interaction with the network in accordance with the embodiment. In the embodiment one or more service nodes may be inserted into the network as agentless service nodes so that an agentless service node is able to send and receive data traffic on a basic switch interface e.g. Ethernet or VLAN interface rather than require that a VPATH function such as an API or agent be embedded within the service node. When data traffic associated with a policy identifier and intended for an agentless service node is received on the VPATH overlay the VPATH overlay is terminated in the VEM to which the service node is connected by the virtual switch interface rather than having the overlay terminated in the service node itself. To direct the data traffic VPATH virtual overlay information in the form of a policy identifier may be mapped onto the switch interface to which the service node is connected. When data traffic associated with a policy identifier is received at a VEM the VPATH overlay is terminated and the data traffic is delivered natively to the switch interface mapped to the policy identifier. The service node receives the data traffic on the interface and then implements service on the data traffic according to the appropriate policy mapped to the interface.

The VPATH function provides configuration programming through the VSM that implements the system functions for the agentless VSNs. Upon configuration of an agentless VSN or at other appropriate times the VPATH function discovers the switch ports used by agentless service nodes and maps one or more policy identifiers to the interfaces to which the service nodes are connected. VPATH identifies the interface by a required agentless service node identifier that is configured in a port profile configuration. The service node identifier triggers VPATH to learn the IP address of the VEM of the service node and the interface on which the VSN is connected. The switch ports associated with interfaces of the corresponding service node are tagged as agentless ports. In the example embodiment VPATH also supports service nodes that integrate embedded agents as well as agentless service nodes that do not include embedded agent. In order to support both agentless service nodes and service nodes with embedded agents the VPATH function determines which traffic on the overlay needs to be terminated at the overlay and which traffic needs to continue on the overlay into a service. Only traffic exiting or entering service nodes without agents needs its overlay terminated by the VPATH function. Once an overlay carrying traffic to an agentless service node is terminated by the VPATH at the VEM to which a service node is connected only raw data traffic e.g. raw data packets are forwarded on the appropriate interfaces agentless ports to the service node. The appropriate interfaces are identified by the tags on the interface being associated with a policy identifier. The VEM terminating the overlay maintains traffic flow information so that when the service node sends traffic back to the VPATH terminating the overlay the return traffic can be sent onward to the sending VPATH of the switch from where the traffic originated in the virtual network.

In another example embodiment of the disclosure the system and method may be implemented in a network having a virtual switching function and a service overlay path implemented according to a Locator Identifier Separation Protocol LISP Mapping System. A virtual service node may be dynamically and transparently inserted into the network by associating one or more interface ports of a virtual switch function to which the service node is connected with one or more service policies of the service node. The association is done by using the LISP Mapping System to associate a Routing Locator RLOC to an appropriate service node and service policy. When data traffic associated with a service policy is received at a switching function in the network on the overlay path the overlay path is terminated and the data traffic is sent as raw data to the interface of the service node that implements the service defined by the policy. Data traffic may be identified by its source and destination and the LISP Mapping System may be used to define a service chain path with one or more service nodes as a sequence of RLOCs the data traffic should go through from source to destination.

Referring now to an example system is shown for a data center and end user environment. System comprises a data center and an end user which may include a data center other networks or individual users etc. Data center and end user can communicate with each other by way of links over a network . Data center can may employ one or more edge switches for traffic ingressing or egressing the data center. Data center may also comprise a plurality of servers and storage devices . The servers e.g. blade servers may host application services e.g. World Wide Web center applications or remotely hosted virtual machine VM applications . The storage devices may be part of a storage area network SAN in certain embodiments of the present disclosure.

In operation system can allow dynamic service insertion in virtual networks where the service insertion could be done in a transparent manner that does not require modification of the service itself. This stands in contrast to methods that require the services to be aware of the insertion logic by necessitating a coupling of the control functions of the service with the control and data plane mechanisms of the virtual network. This coupling of the control and data plane mechanisms with a service often requires modification of the service itself. When third party services and third party service providers are involved the requirements for modification are burdensome or provide a potential barrier to implementation of the service. For example the modification could require embedding modifications in the form of user space libraries kernel loadable modules Application Programming Interfaces APIs etc. which provide the functions of agents into the service in order to allow the service to interact with the virtual network.

Data center further comprises core switches aggregator switches and access switches collectively shown at reference numeral to aggregate and distribute ingress upstream traffic and egress downstream traffic . A plurality of switches is provided at each access aggregation and core level to achieve redundancy within data center . In this example the plurality of servers and storage devices employ a plurality of virtual switches and a plurality of virtual service nodes VSNs to service network traffic for VMs operating therewith. Typical services provided for network traffic may consist of load balancing and applications control WAN acceleration network security and network analysis and monitoring. WAN acceleration typically involves application and data acceleration e.g. using compression techniques to reduce hardware throughput requirements. According to the techniques provided herein selected VSNs may be created in such a manner that they are started instantiated stopped or migrated within the virtual network dynamically and transparently to the service. Accordingly network services can be scaled to meet demand without requiring that the services be changed or modified for operation in the network.

Referring now to therein is a simplified block diagram illustrating example details of a portion of the network of in accordance with one embodiment of the disclosure. illustrates an example data center which could be for example data center . The environment comprises a virtual supervisor module VSM switches and a virtual network management center VNMC and a plurality of servers . Each server hosts hypervisor respectively. As used herein the value n represents any quantity of items referred to and may vary from item to item. Hypervisors are hardware abstract layers that provide operating system independence for application services provided by VMs. Hypervisor supports VSN and VMs and hypervisor supports VSNs and and VMs and and hypervisor supports VSNs and and VMs . VMs may provide applications and services e.g. one or more private networks in a private cloud for end users e.g. end user . Each hypervisor has a virtual switch or Virtual Ethernet Module VEM shown as VEMs that performs switching at layer 2 of the Open Systems Interconnect OSI model. The VEMs also performs some Quality of Service Qos security and monitoring functions. In this example the VEMs also perform layer 3 switching and can operate as a single unit across hypervisors. Each hypervisor also has a virtual service data path VPATH respectively.

VEMs provide switching and other service for VMs e.g. inter VM routing and switching for VM ingress and egress network traffic while the VDSPs provide a dedicated traffic pathway for services provided by VSN . The VDSPs may be in the form of a service overlay path e.g. a layer 2 3 overlay tunnel. The VPATHs also give the VSNs mobility that allows them to migrate from one server to another without routing traffic through an inline appliance. By use of a VEM and VPATH architecture any of the VSNs may provide services for any of the VMs . Accordingly when a VM mobility event occurs e.g. if a VM is moved from one server to another VSNs that were providing network traffic services for the moved VM will also provide those same services when the moved VM is running on its new server.

In the embodiment of VSNs may be implemented as having an agent VA for example VSN and VSN have agents VA and VA respectively or without an agent for example VSN and VSNs . For VSN and the service overlay extends directly into VSN and . When the VSNs are communicating with other entities in the virtual network including VSMs the agents VAs and function internally in VSN and VSN respectively to provide an API for the VSNs to interface with VPATH extract metadata received from VPATH such as for example policy identifiers and provide the data to the VSNs. For VSNs and VSNs which will be referred to collectively in the example embodiments as agentless VSNs the service overlay terminates in the VEM VPATH to which an interface of the VSN is connected. The agentless VSNs and VSNs may be coupled to the VPATH VEM through a basic interface in the infrastructure in which the virtual network is implemented for example through a VLAN interface. Each interface that is coupled to an input of an agentless VSN may be tagged with a policy identifier identifying the service policy of the VSN and traffic on the virtual overlay associated with the policy identifier may then be sent to the interface tagged with that policy identifier and therefore to the appropriate VSN. Each interface that is coupled to the output of an agentless VSN may also be tagged with a policy identifier and traffic received on that interface from the VSN may then be identified as serviced output traffic associated with the policy identifier. The need for agents embedded in the VSN has been avoided through this manner of policy identifier propagation.

For example in the embodiment of in communications between VSN which includes agent and other entities in the virtual network the service overlay path terminates in VSN . For communications between VSN which is agentless and other entities in the virtual network the service overlay may terminate in VPATH of VEM . In one embodiment this may be implemented by tagging the inputs and outputs of VSN with policy identifiers t1 and t2 respectively. VPATH may then provide communications between other entities in the virtual network and VSN by sending and receiving raw data traffic to and from VSN by utilizing the tags t1 and t2. Traffic received on VPATH and associated with the policy identifier tagged to t1 may be directed to interface t1 as raw data traffic. The interfaces of the other agentless VSNs may be similarly tagged with policy identifiers. In the embodiment of VSN is connected to VPATH of VEM through input and output interfaces t1 and t2 respectively VSN is connected to VPATH through input and output interfaces t3 and t4 respectively and VSN is connected to VPATH through input and output interfaces t5 and t6 respectively.

The VPATH function can provide the programming through VSM that implements the system functions of agentless VSNs. Upon configuration of an agentless VSN or at other appropriate times the VPATH function discovers the switch ports used by the agentless VSN and maps one or more policy identifiers to the interfaces to which the service nodes are connected. VPATH identifies the interface by a required agentless service node identifier that is configured in a port profile configuration. The service node identifier triggers VPATH to learn the IP address of VEM of the service node and the interface on which the VSN is connected. The switch ports associated with interfaces of the corresponding service node are tagged in VPATH as agentless ports. This scheme of discovery allows migration of agentless VSNs to other VEMs transparently as the destination VPATH in the VEM is automatically programmed to learn the migrated agentless VSN and becomes the terminating VPATH for that agentless VSN. In the embodiment of in order to support both agentless service nodes and service nodes with embedded agents the VPATH function determines which traffic on the overlay needs to be terminated at the overlay and which traffic needs to continue on the overlay into a service. Only traffic exiting or entering service nodes without agents needs its overlay terminated by the VPATH function. Once an overlay carrying traffic to an agentless service node is terminated by the VPATH at the VEM to which a service node is connected only raw data traffic e.g. raw data packets are forwarded on the appropriate interfaces agentless ports to the service node. The appropriate interfaces are identified by the tags on the interface being associated with a policy identifier. The VEM terminating the overlay maintains traffic flow information so that when the service node sends traffic back to the VPATH terminating the overlay the return traffic can be sent onward to the sending VPATH of the switch from where the traffic originated in the virtual network.

In the embodiment of the entities other than the agentless service nodes that comprise the network VSN VSN and VSMs may be implemented to operate in conjunction with VPATHs hypervisors VSM VNMC switches and VEMs in accordance with techniques and method disclosed in co pending U.S. patent application Ser. No. 13 337 379 Architecture For Scalable Virtual Network Service filed on Dec. 27 2011 and commonly assigned to the assignee of the present application which is hereby incorporated by reference in its entirety. The network may be modified to allow interaction of the network with the agentless service nodes according to this disclosure. According to example embodiments of this disclosure the VPATHs hypervisors VSM VNMC switches and VEMs may also be adapted as necessary to allow implementation of the agentless VSNs and into the network and allow appropriate communications between the agentless VSNs and and other entities in the network.

The VSM allows a network team to manage the virtualization environment and to manage network policies by way of port profiles. The VSM employs a service agent SA to facilitate the functions of the VSM . The VNMC is used by a security server team to manage the various VMs security policies and VSNs e.g. virtual firewalls using service profile that encompasses security policies. For ease of use VNMC provides a graphical user interface GUI for device and policy management. VSM and VNMC may also be adapted as necessary according to the present disclosure to allow implementation of the agentless VSNs and .

Referring now to therein is a simplified block diagram illustrating example details of a portion of a network using service insertion techniques in accordance with the techniques described in relation to the embodiment of of the disclosure. shows a network comprising distributed virtual switch DVS access net VEMs and VMs and VSNs . DVS and VEMs provide virtual connections represented by overlay between any of the VMs and VSNs through the access net and DVS . VEMs and VMs and VSNs may be implemented analogously to the VEMs VMs and VSNs shown and described in and are connected through a network of switches to a VSM and a VNMC which are not shown in . VEMs provide the functions provided by VEMs and VPATH of and it may be considered that the VEMs each include a VPATH function. Access net represents the functions provided by the control and data planes of the embodiment of that send and route data traffic between the VEMs VMs and VSNs.

According to embodiment of VSNs implemented in network may be implemented as agentless VSNs for example VSNs and . Other VSNs may be implemented as VSNs having agents that are integrated into the functions implementing the virtual overlay path such as VSN having agent and VSN shown having agent VA . The interfaces to each of the input and output ports of agentless VSNs and may be tagged with VLAN tags. The interfaces to the input and output ports of are tagged t1 and t2 respectively the interfaces to the input and output ports of are tagged t3 and t4 respectively and the interfaces to the input and output ports of are tagged t5 and t6 respectively. The use and insertion of services provided by agentless VSNs in the virtual network does not require modification of the services for example use and insertion of service nodes does not require implementation of any API into the VSNs and . This is because instead of passing policy identifiers via APIs as is done with the agents VA and in VSNs and respectively for example the policy identifiers are mapped to the tags on the interfaces of agentless service nodes and . In the example shown in VSN is an agentless VSN providing a service according to policy identifier ID P1.

As an example when data traffic for VM is received in VEM at port A a port profile for port A programmed in VPATH indicates that the service identified by policy identifier P1 should be applied to the received traffic. The profile indicates to VEM that the data traffic should be intercepted and forwarded to VSN along with policy identifier P1 as shown at block B. VEM will include the policy identifier P1 in a service header of the data traffic encapsulate the service header and data for transmission on the virtual overlay path and send the encapsulated data traffic to VEM . Upon receiving the data traffic on the virtual overlay path VEM determines that VSN is an agentless VSN terminates the virtual overlay path de encapsulates the data traffic and removes the service header to generate raw data traffic. VEM then maps the policy identifier P1 to the interface tagged with t6 at as shown at block to determine the appropriate interface to which it should send the raw data traffic. The mapping may be done using for example a policy mapping table programmed in VPATH. The raw data traffic is sent to the interface tagged t6. VSN will know to apply policy P1 to the raw data traffic from the fact that it is received on the interface tagged t6. When serviced raw data is received by VEM from interface t5 VEM may operate to return this raw data traffic to the destination VEM by utilizing for example a flow table in which it has cached the source VEM and related metadata when the traffic from and to VEM is layer 2 traffic. Enhanced techniques using information on static switch configurations of the network to determine where to send the return traffic may be used when layer 3 traffic and physical switches are involved. VEM then encapsulates the raw data traffic and sends it back to VEM .

Referring now to therein are simplified flow diagrams of an example process for a VSM using an agentless VSN which can be described with reference to . At a virtual service node is started that is configured to provide network traffic services for one or more virtual machines the virtual service node is connected to the virtual network on an interface. The input and output ports of the VSN e.g. VSN are connected to VLAN input and output interfaces coupled to VEM . No modification is required internally of VSN . At the interfaces connected to VSN are tagged to a policy identifier in the virtual network. At a VM e.g. VM comes online and is associated with a virtual interface. At a port profile is attached to the virtual interface where the port profile comprises information configured to identify the service profile to VEM connected to the virtual service node in the form of a policy identifier P1 attached to port A. At information is provided VPATH that informs VPATH of the VLAN interfaces of VSN and the policy identifiers tagged to the VLAN interfaces connected to VSN . At data traffic to VM is intercepted and at the data traffic along with its policy identifier is redirected to the VEM of VSN . At the virtual overlay for the data traffic is terminated in VEM the received data traffic is de encapsulated and raw data traffic is sent to input interface t6 which is tagged to the policy identifier in the received data traffic. At step VSN then services the raw data traffic according to the policy associated with the input port t6 on which the data traffic was received and returns raw data traffic on the output interface tagged as the output with the policy identifier. The described operations of are illustrative only and may not need to follow in the order presented. In various embodiments of the disclosures additional operations may be added or operations may be removed from the operations described.

Services provided by agentless VSNs according to embodiments of the disclosure could also be provided in a chained fashion. Referring to therein is illustrated a simplified block diagram illustrating example details of a portion of a network using service chaining in accordance with the techniques described in relation to the embodiment of of the disclosure. shows a network comprising distributed virtual switch DVS access net VEMs and VMs and VSNs . DVS and VEMs provide virtual connections represented by overlay between any of the VMs and VSNs through the access net and DVS . VEMs and VMs VSNs VEMs and Access net may be implemented analogously to the VEMs VMs and VSNs shown and described in . In an example of service chaining traffic sent to VM may be intercepted at port A and steered through an ordered list of service functions that includes the service functions provided by agentless VSN and agentless VSN . The intercepted data traffic is first sent to VEM on overlay where the first policy in the ordered list mapped to the interfaces connected to input t1 and output t2 of VSN . The data traffic received at VEM is converted to raw data and sent to VSN on the interface mapped to t1. The serviced raw data is then received on the interface mapped to output t5 encapsulated for transmission and returned to VEM on overlay . VEM then sends the data traffic to VEM where the second policy in the ordered list is mapped to interfaces connected to input t6 and output t5 of VSN . The data traffic received at VEM is converted to raw data and sent to VSN on the interface mapped to t6. The serviced raw data is then received on the interface mapped to output t2 encapsulated for transmission and in this example sent back on overlay to VM .

Referring now to therein are simplified block diagrams that illustrate agentless service node network traffic service at a virtual switch in accordance with an embodiment of the disclosure. In agentless service node provides a virtual firewall function. In traffic is received on VPATH of VEM from end user over network on the network overlay path. VEM determines that the traffic is directed toward agentless service node de encapsulates the packet flow into raw data packets maps the policy identifier received with the data traffic to interface t1 and steers the raw data packets to the interface tagged with t1. VSN services the raw data packets and returns the raw data packets on the interface tagged t2 which is mapped to the policy identifier as an output. VEM receives the service raw data packets on t2 encapsulates the serviced raw data packets for transmission on the overlay and sends the traffic onward toward target VM . shows the identical service components functioning in the same manner except that agentless service node services the traffic by blocking the raw data packets from passing.

Referring now to therein is a simplified block diagram illustrating an example of a network using service insertion in accordance with a further embodiment of the disclosure. The network comprises LISP mapping system servers and switch and server which may be a physical server. Server includes hypervisor vswitch and virtual machine Client VM1 . Server includes hypervisor vswitch and service node FW VM and virtual machine VM3 . Virtual machine is connected to hypervisor vswitch by interface . Service node is connected to hypervisor vswitch by interface and virtual machine is connected to hypervisor vswitch by interface . Interfaces and are each shown as a single line connection but include both input and output connections. Network is a network operating according to the LISP protocol which provides an overlay transmission path for communications between servers and other entities in the network. The LISP protocol uses the EID endpoint identifier and RLOC routing locator name spaces for sending data packets. The EID is the IP address of a host and the RLOC is the IP address of the LISP router for the host. LISP provides an overlay transmission path by encapsulating traffic at the RLOC to RLOC transmission level. In the embodiment of virtual machine of server initiates a transmission of data traffic from VM1 at EID1 to destination EID2 on server . Hypervisor vswitch queries the LISP Mapping System at with a request as to which route a transmission from EID1 to EID2 should take through the system. Mapping System returns information on the routing at which includes LISP overlay paths defined by RLOC addresses RLOCof server and RLOCN7K of switch . The Mapping System has been programmed to send traffic sent from EID1 to EID2 to be processed by service node on its route to EID2. The programming of mapping system has associated the data traffic with the service policy of service node by the insertion of RLOCin the routing information and by the mapping of RLOCto the interface of service node at EID. Hypervisor vswitch encapsulates the data according to the LISP protocol for transmission to RLOCon overlay path . When the data traffic is received at hypervisor vswitch of server at RLOCthe encapsulated data traffic is de encapsulated and sent as shown at to EIDon interface as raw data traffic. The service node services the raw data and returns raw data on interface to hypervisor vswitch . Hypervisor vswitch then queries Mapping System at with a request as to which route the data traffic should take next. Mapping System returns a reply that includes route information at that includes the LISP overlay paths defined by RLOC addresses RLOCof server and RLOCof switch . Hypervisor vswitch then knows that the next RLOC destination is RLOCof switch . The data is then capsulated according to the LISP Protocol for transmission to RLOCof switch where it is received de encapsulated and sent as raw data to host ENG of 710 at EID2.

Mapping system associates the data traffic with the service policy of service node by the insertion of RLOCin the routing information and by the mapping of RLOCto interface of service node at EID. The programming of Mapping System and connection of RLOCto interface of service node according to the embodiment allows transparent and dynamic insertion of a service node such as service node into the system without modification of the service node. The data traffic format is not modified and includes the original packet a tenant ID if necessary and the RLOC header as specified by the LISP protocol. When a service node is inserted in the network a new RLOC is added to the mapping system. There should be at least one RLOC associated per every service node. A new EID may also be added if it is desired to virtualize the service or have multiple instances of the same service. A service node can be implemented without modifying the service node or inserting any functions or agents into the service node to interact with the LISP control and data planes.

Referring now to therein are simplified block diagrams illustrating an example network having service node insertion and service chaining in accordance with a further embodiment of the disclosure. illustrate how a server may be dynamically and transparently inserted into the network . In system functions to implement a transmission from VM1 to host ENG of server through service node basically identically to the process of by which VM1 sends data traffic to ENG of server through service node with the reference numerals in the description of the process . . . replaced with . . . . respectively. Turning now to in service node IPS VM which provides an IPS service in server has been inserted into the network and into the service chain for servicing data traffic sent from VM to host ENG of server . Inserting service node into the network did not require any modifications to the service node . The LISP Mapping System may be reprogrammed so that after servicing of the data traffic at service node the overlay path at sends the data traffic to RLOC. RLOCis the address that effectively is used to associate received data traffic at hypervisor vswitch with the service node . In other words in the LISP Mapping associates the data traffic sent to hypervisor switch with the service policy of service node by the insertion of RLOCin the routing information and by the mapping of RLOCto the interface of service node at EID. When service node has sent raw data traffic back on interface to hypervisor vswitch the LISP Mapping System sets the next overlay path to route the data traffic to RLOCat switch and then to host ENG at EDI2 at server .

Referring now to B and C therein are shown example block diagrams of servers in which virtual switch functions according to the various example embodiments of the disclosure may be implemented. For example servers of the servers in which VEMs of are implemented or the servers and of with the vswitch function in place of VEM and VPATH may be implemented as shown in . The service nodes in may be implemented as agentless service nodes by connecting the service nodes to appropriate interfaces. Referring to a server host is shown. The server comprises one or more network interface units a processor and a hypervisor with memory for storage. The network interface units are coupled to the processor and are configured to transmit or receive messages over one or more networks or between servers. Additionally the network interface units are configured to support the VEM and VPATH across multiple servers according to the techniques described herein.

Processor is coupled to the network interface units and to the hypervisor . Processor is a microprocessor or microcontroller that is for example configured to execute program logic instructions i.e. software for carrying out various operations and tasks described herein. Processor may be configured with memory or other storage. Hypervisor is also configured with one or more processors similar to processor and may be referred to herein generally as processors . Memory used in server may comprise read only memory ROM random access memory RAM magnetic disk storage devices optical storage media devices flash memory devices electrical optical or other physical tangible memory storage devices.

The functions of the processors may be implemented by logic encoded in one or more tangible computer non transitory readable storage media e.g. embedded logic such as an application specific integrated circuit ASIC digital signal processor DSP instructions software that is executed by a processor etc. where memory stores data used for the operations described herein and stores software or processor executable instructions that are executed to carry out the operations described herein. The various functions and components of the embodiments may take any of a variety of forms so as to be encoded in one or more tangible readable memory media or storage device for execution such as fixed logic or programmable logic. e.g. software computer instructions executed by a processor and the processors may be ASICs that comprises fixed digital logic or a combination thereof. For example the processors may be embodied by digital logic gates in a fixed or programmable digital logic integrated circuit which digital logic gates are configured to perform various functions described herein. In general the components may be embodied in one or more computer readable storage media encoded with software comprising computer executable instructions and when the software is executed operable to perform the operations described herein.

Referring now to therein is a second example of a block diagram of a server into which virtual switch functions according to the various example embodiments of the disclosure may be implemented. In a network interface card is shown that may be one of the interfaces of . The functions of the hypervisor and the components shown in remain relatively the same with the exception that VPATH functionality has been moved from the hypervisor to the network card and is labeled with reference number .

Referring now to therein is another example of a block diagram of a server into which virtual switch functions according to the various example embodiments of the disclosure may be implemented. A physical hardware switch is deployed in a data center. Switch comprises a supervisor module and switch fabric . VEM VPATH is hosted within the switch fabric . Switch interfaces with VNMC physical services node and physical hosts . VNMC resides in a virtual network as a VM. Physical hosts support a plurality of VMs in a virtual network. VPATH forwards traffic to physical service node in the same way it would forward VM traffic to a VSN.

Note that in this Specification references to various features e.g. elements structures modules components operations characteristics etc. included in one embodiment example embodiment an embodiment another embodiment some embodiments various embodiments other embodiments alternative embodiment and the like are intended to mean that any such features are included in one or more embodiments of the present disclosure but may or may not necessarily be combined in the same embodiments.

It is also important to note that the operations and steps described with reference to the preceding FIGURES illustrate only some of the possible scenarios that may be executed by or within the system. Some of these operations may be deleted or removed where appropriate or these steps may be modified or changed considerably without departing from the scope of the discussed concepts. In addition the timing of these operations may be altered considerably and still achieve the results taught in this disclosure. The preceding operational flows have been offered for purposes of example and discussion. Substantial flexibility is provided by the system in that any suitable arrangements chronologies configurations and timing mechanisms may be provided without departing from the teachings of the discussed concepts.

Although the present disclosure has been described in detail with reference to particular arrangements and configurations these example configurations and arrangements may be changed significantly without departing from the scope of the present disclosure. For example although the present disclosure has been described with reference to particular communication exchanges involving certain network access and protocols communications within the systems of the embodiments may be applicable to other exchanges or routing protocols. Moreover although the systems of the embodiments have been illustrated with reference to particular elements and operations that facilitate the communication process these elements and operations may be replaced by any other suitable architecture or process that achieves the intended functionality of the embodiments.

Numerous other changes substitutions variations alterations and modifications may be readily ascertained to one skilled in the art and it is intended that the present disclosure encompass all such changes substitutions variations alterations and modifications as falling within the scope of the appended claims. In order to assist the United States Patent and Trademark Office USPTO and additionally any readers of any patent issued on this application in interpreting the claims appended hereto Applicant wishes to note that the Applicant a does not intend any of the appended claims to invoke paragraph six 6 of 35 U.S.C. section 112 as it exists on the date of the filing hereof unless the words means for or step for are specifically used in the particular claims and b does not intend by any statement in the specification to limit this disclosure in any way that is not otherwise reflected in the appended claims.

