---

title: Conditional master election in distributed databases
abstract: Methods and apparatus for conditional master election in a distributed database are described. A plurality of replicas of a database object are stored by a distributed database service. Some types of operations corresponding to client requests directed at the database object are to be coordinated by a master replica. Client access to the database object is enabled prior to election of a master replica. In response to a triggering condition, a particular replica is elected master. The master coordinates implementation of operations with one or more other replicas in response to client requests.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09569513&OS=09569513&RS=09569513
owner: Amazon Technologies, Inc.
number: 09569513
owner_city: Reno
owner_country: US
publication_date: 20130910
---
Several leading technology organizations are investing in building technologies that sell software as a service . Such services provide access to shared storage e.g. database systems and or computing resources to clients or subscribers. Within multi tier e commerce systems combinations of different types of resources may be allocated to subscribers and or their applications such as whole physical or virtual machines CPUs memory network bandwidth or I O capacity.

One of the many benefits of using the software as a service approach is that providing the desired levels of availability data durability and scalability becomes the responsibility of the service operator. Clients of the services may simply decide what levels of availability durability and performance they wish to pay for and leave the implementation details to the services. The service operators may consequently establish numerous data centers often geographically distributed across different cities states or even countries and populate the data centers with computing networking and storage infrastructure based on expectations of client usage levels for the various services. The specific resources used for a given client may be selected from several different data centers for example to achieve desired levels of fault tolerance and data durability.

In at least some scenarios internal services may be set up to manage some of the common components of functionality underlying various client accessible services. For example a large provider may implement a number of different database related services e.g. relational database management services object oriented database services NoSQL or non relational databases and the like each targeted to different market segments several of which may require state management for database objects such as tables or table partitions. A general purpose state management service may be implemented for internal use within the provider s network for use by each of the different database related services. Such a state management service may also be used for managing states of resources used by other types of services such as virtualized computing services where for example health state transitions and overall responsiveness of various virtualization hosts and or compute instances may need to be monitored.

The use of such internal state management services may reduce the need for different client accessible services to re implement similar pieces of underlying technology thereby helping reduce costs for the service provider. However at least under some circumstances e.g. especially during periods of recovery from infrastructure outages when a large number of state transitions may have to be handled within a short time period it may be possible for the state management service itself to become overloaded which can potentially lead to cascading problems for the client accessible services.

While embodiments are described herein by way of example for several embodiments and illustrative drawings those skilled in the art will recognize that embodiments are not limited to the embodiments or drawings described. It should be understood that the drawings and detailed description thereto are not intended to limit embodiments to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope as defined by the appended claims. The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description or the claims. As used throughout this application the word may is used in a permissive sense i.e. meaning having the potential to rather than the mandatory sense i.e. meaning must . Similarly the words include including and includes mean including but not limited to.

Various embodiments of methods and apparatus for conditional master election in distributed databases are described. According to some embodiments a distributed database service may be set up in a provider network in which various types of database objects such as tables or table partitions are replicated. Different replicas of a replica group for a given object may be assigned different roles with respect to the way in which writes are propagated and or the manner in which a desired level of consistency for the data is attained. As described below in further detail in some embodiments one replica of a replica group may be elected a master replica in response to a determination that a triggering condition has been met and that master replica may subsequently coordinate the implementation of one or more types of operations with the remaining replicas. Various aspects of the techniques used for master election e.g. the selection of the triggering conditions may be designed in such a way as to minimize the overall overhead associated with state management for the distributed database service as a whole e.g. by avoiding at least some master elections until they become necessary or until the estimated probability that a master election will be required within a short time reaches a threshold.

Networks set up by an entity such as a company or a public sector organization to provide one or more network accessible services such as various types of cloud based database computing or storage services accessible via the Internet and or other networks to a distributed set of clients may be termed provider networks herein. In the remainder of this document the term client when used as the source or destination of a given communication may refer to any of the computing devices processes hardware modules or software modules that are owned by managed by or allocated to an entity such as an organization a group with multiple users or a single user that is capable of accessing and utilizing at least one network accessible service of the provider network. A given provider network may include numerous data centers which may be distributed across different geographical regions hosting various resource pools such as collections of physical and or virtualized computer servers storage servers with one or more storage devices each networking equipment and the like needed to implement configure and distribute the infrastructure and services offered by the provider. A number of different hardware and or software components some of which may be instantiated or executed at different data centers or in different geographical regions may collectively be used to implement each of the services in various embodiments.

A distributed database service DDS may be offered by a provider network in some embodiments enabling clients to use the provider network s compute and storage capabilities to implement database applications over network connections e.g. using one or more programmatic interfaces such as a web services interface. For example using such a programmatic interface a client of the distributed database service may issue a respective request to create a table to insert data into a table to read data from a table and so on. The distributed database service may be implemented using several logical layers or groups of resources of the provider network in some embodiments including for example a front end layer comprising nodes used for receiving and responding to client requests a back end layer comprising storage nodes at which the data is stored and or an administrative layer from which resources of other layers are managed. In at least some such embodiments direct accesses from the clients may not be permitted to the back end layer. The distributed database service may implement any of various types of database technologies and models in different embodiments such as a relational database an object oriented database a key value based non relational e.g. a NoSQL database a document oriented database and so on.

According to one embodiment the DDS may support a high level of data durability and or availability for at least some database objects e.g. tables using object level replication. In one example implementation N replicas R R . . . RN of the data of a given table T may be stored in respective availability containers of the provider network where N may be selectable by the DDS e.g. in response to clients durability or availability requirements . The set of replicas of a given table may be termed a replica group herein. A given availability container may comprise a portion or all of one or more data centers and may be engineered in such a way as to prevent various types of failures within a given availability container from impacting operations at other availability containers. Thus for example a given availability container AC may comprise a set of hardware software and infrastructure components such as power supplies power distribution units cooling heating equipment networking equipment and the like that has a failure profile e.g. a probability distribution over time of various types of faults or errors that is not expected to be correlated with the failure profile of a different availability container AC. Each availability container may itself be designed to have very low failure rates along with the isolation characteristics that prevent cascading faults across multiple availability containers. The implementation of numerous availability containers capable of containing failures within their boundaries may thus help support highly reliable and durable services. Thus in the case where N replicas of T are stored in respective availability containers T s data would remain accessible even if N 1 availability containers happen to become unavailable at once reducing the probability of a complete outage to very near zero. Availability containers may also be referred to as availability zones in some implementations. In at least some embodiments the resources of a geographically distributed provider network may be organized into a hierarchy of geographical regions with each region comprising some number of availability containers and each availability container comprising part or all of one or more data centers.

The DDS may support various data consistency models in different embodiments. For example according to one consistency model a write or update to a given table may have to be persisted at a quorum number of replicas before the write is deemed complete while a read may be performed at any of the replicas and as a result it may be the case that a given read request may return a slightly out of date version of the data if for example a recent update has not been propagated to the replica from which the read is satisfied . In some embodiments two or more categories of reads may be supported consistent reads versus eventually consistent reads for example. A consistent read directed at a given table or object may return a result that reflects all the writes that completed successfully prior to the read request in such an embodiment while an eventually consistent read may not necessarily reflect the result of a recently completed write for a short time period e.g. typically on the order of a second or less in some implementations .

Different approaches to update propagation may be taken in different implementations of the DDS. In one embodiment one of the replicas of the table s replica group may be elected to a master role using any of various techniques described below. The DDS may support various types of client work requests e.g. different types of requests to view and or modify client data or metadata in different embodiments. A given client work request may require one or more operations e.g. reads or writes at the back end storage nodes. When a client work request e.g. a client submitted update or insert command that involves a write is received e.g. at a front end node of the DDS a corresponding write operation may first be performed at the master and the master may then coordinate the propagation of the write to the other replicas of the replica group. Such coordination may involve for example directing write propagation operations to one or more non master replicas and waiting for acknowledgements from a sufficient number of non master replicas before determining that the write has been successfully completed. Consistent read operations may also be directed to and coordinated by the master in at least some embodiments as the master is the replica that is aware of the state of the most recent write directed to the target of the consistent read. In response to a consistent read request the master may in some cases have to wait for a short period of time until the most recent write completes i.e. until one or more acknowledgement of outstanding write propagations are received from non master replicas . In other cases if the latest write to the targeted data has been acknowledged from some number of non master replicas the master may not have to wait in response to a request for a consistent read. Eventually consistent reads may be handled by any of the replicas in some embodiments e.g. by sending a read request to the nearest replica from the front end node that receives the client request. Thus in at least some embodiments one or more types of operations corresponding to client work requests directed at database objects may have to be coordinated by a master replica while other types of operations may not need such coordination. In some embodiments more than one master may be elected for a given replica group e.g. one master for handling writes and a different master for handling consistent reads using conditional election techniques similar to those described herein.

In at least one embodiment a distributed state management service SMS which may also be referred to herein as a distributed state manager DSM implemented within the provider network may be used for master election. The DSM may be used by one or more other services of the provider network in at least some embodiments and not just by the DDS. Even the DDS may use the DSM not just for master election but also for other types of state management operations in some embodiments such as ensuring via heartbeat monitoring that the various nodes of the DDS layers remain responsive. In some implementations the DSM itself may comprise a plurality of nodes or clusters of nodes distributed across different availability containers and or data centers. Especially in large provider networks in which numerous other services use the DSM the resource utilization levels of the DSM itself may reach high levels especially after events such as large scale outages e.g. when power outages or weather emergencies result in hundreds or thousands of provider network resources becoming unavailable or inaccessible at the same time followed by a period of recovery in which the resources all come on line . Consider a scenario in which a given DSM node is configured to receive and respond to heartbeat requests from thousands of resources in accordance with a health monitoring protocol as well as to handle master election requests from some number of those same resources. If the DSM node becomes overwhelmed with handling master election requests during an especially busy period e.g. during recovery from an outage it may not be able to respond to heartbeat messages from some set of resources quickly enough. This in turn may lead the resources that do not receive heartbeat responses quickly enough to determine that their network links to the DSM node have been compromised or have failed and such resources may in turn transmit heartbeats to other DSM nodes which may also be overloaded and may also not respond quickly enough. As a result a positive feedback loop may be established as a result of which more and more workload is generated for already overloaded DSM nodes.

Techniques for conditional master election may accordingly be implemented in some embodiments e.g. to reduce the workload associated with the elections of master replicas or at least to spread election related workload more evenly over time . According to one such technique a replica group for a given database object may be deemed to be in one of several states with respect to master election at any given point in time. In on embodiment a replica group may be in an active state if it currently has a master elected and in a dormant or master less state if none of the replicas is currently designated as the master. For example when a replica group is created e.g. in response to a create table request from a client the replica group may be initially configured in master less or dormant state in some embodiments and client access may be enabled to the table prior to the election of the master. In response to receiving a client work request directed at the table which does not require coordination by a master replica such as an eventually consistent read operations corresponding to the work request may be implemented at a selected replica of the replica group such as a particular randomly selected replica or a replica selected based on locality or low latency.

If a determination is made by the DDS that a triggering condition for master election at a master less replica group has been met the process of master election may be initiated in such an embodiment. A number of different conditions may trigger master election such as for example a client s submission of a write request or a consistent read request either of which may require coordination by a master or a determination by an administrative node of the DDS that coordination requiring operations directed at the table are likely to commence in the near future. According to some implementations a quorum based master election protocol may be implemented in which one replica Rj initially sends a request for a token or a lock to a DSM. If the DSM grants the token lock to the requesting replica Rj Rj or the DSM itself may notify the other replicas of the replica group that Rj is a candidate for the master role and is awaiting acknowledgement of its election as master. If a quorum of the remaining replicas transmit acknowledgement messages to Rj and or to the DSM indicating their approval of Rj being elected master the election process may be deemed complete in some embodiments. Subsequently in response to work requests that require coordination the master replica Rj may coordinate the implementation of the corresponding work operations. If Rj is not elected master e.g. if the DSM does not grant the token to Rj or a sufficient number of acknowledgments is not received a different replica may eventually be elected or Rj itself may be elected after retrying for the master role . In some implementations more than one replica may attempt to become master in response to the triggering condition and one of the attempting replicas may be selected by the DSM. In at least one embodiment the master election process may not involve the use of a quorum based protocol e.g. a particular replica may be selected as master without requiring the assent of other replicas.

In some embodiments any of several conditions or scenarios may lead to a given active replica group i.e. one that currently has an elected master replica becoming dormant or master less. For example in one embodiment if the current master replica fails or its master status is revoked due to failure to respond quickly enough to heartbeat messages due to network partitioning or due to some other reason the election of a new master may be deferred until one of the triggering conditions is met leaving the replica group dormant for some period of time. In some embodiments the DDS may monitor the number of work requests directed at the replica group that do require master led coordination and if the number of such requests within a given time period falls below a threshold the replica group may be moved to the dormant state by revoking the master s status.

The DDS in the depicted embodiment may comprise several layers of resources. A front end layer comprising one or more request router RR nodes such as RRs A B and C may serve as the service endpoints to which clients submit work requests e.g. read and write requests of various types directed to database objects and from which clients may receive responses to the work requests. Database objects such as tables indexes and the like may be stored in back end layer of the DDS which comprises a plurality of storage nodes such as storage nodes A B C K M P Q and R. To provide desired levels of availability durability and or performance tables and or other database objects may be replicated at the back end layer in the depicted embodiment. A number of replicas of a given table for example may each be stored at a respective storage node . The set of replicas of a given object such as a table may be referred to as a replica group RG . At a given point in time an RG may be in an active state in which case it currently has an elected master replica or in a dormant or master less state in the depicted embodiment. Other states such as short lived states during the implementation of a master election protocol may also be supported in at least some embodiments. 

Three example replica groups are shown in active replica groups ARGs A and B and dormant replica group DRG C. ARG A includes a master replica A Ra of a database object stored at storage node A as well as two non master replicas A Rb and A Rc stored respectively at storage nodes B and C. Similarly ARG B comprises master replica B Ra at storage node K and non master replicas B Rb and B Rc at storage nodes L and M. DRG C comprises three non master replicas C Ra C Rb and C Rc at storage nodes P Q and R. It is noted that although availability container boundaries and data center boundaries are not shown in . in at least some implementations each of the replicas of an RG may be stored in a different availability container or data center than the others. The RRs and or nodes of administrative layer may also be distributed across different data centers and availability containers in at least some embodiments. Although only a single replica is shown at a given storage node in each storage node may comprise a plurality of replicas typically belonging to different RGs . The number of replicas in different RGs may differ in some embodiments e.g. depending on clients data durability and or availability needs some database objects may have more replicas set up than others.

A number of administrative nodes such as nodes A B and C may collectively form an administrative layer of the DDS in the depicted embodiment. Admin nodes may be responsible for such tasks as maintaining the mappings between database objects and the storage nodes at which the objects replicas are located acquiring and releasing resources for the storage nodes and RRs as needed and so on. In at least some embodiments as described below in further detail when a client work request directed to a particular database object is first received at an RR the RR may determine which specific storage node or nodes are to be contacted by querying an admin node . A cache of object to SN mappings may be maintained at an RR in at least some embodiments.

Master election may be accomplished with the help of the DSM in the depicted embodiment. In other embodiments other state management machinery may be used such as a distributed or centralized state manager implemented within the DDS itself. In some embodiments when an RG is first created e.g. in response to a request from a client to create a table in the DDS a replica group with some number of replicas may be created at selected storage nodes in dormant or master less state. In other embodiments a master node may be elected by default immediately upon or shortly after RG creation e.g. with the help of the DSM .

RG state transitions from dormant state to active state and from active state to dormant state may be initiated by a number of types of triggering conditions in different embodiments. For example in order to respond to a client work request that includes a write operation in some embodiments coordination by a master replica may be required. Accordingly the arrival of a coordination requiring work request at an RR may itself serve as a trigger for master election if the targeted database object s replica group happens to be in master less state at the time the work request is received. In at least some embodiments an admin node or an RR may proactively initiate the master election process e.g. in response to a determination that coordination requiring operations are likely to commence within some time window. For example the timing of changes in some application workloads may be at least somewhat predictable based on monitored metrics and or based on client input. If an admin node or an RR determines that the probability that write or consistent read operations directed to a currently dormant RG are going to be received within the next hour master election may be triggered in some embodiments.

Conversely in at least some embodiments if an admin node or an RR is able to determined that the rate of work requests requiring master led coordination at a given RG has fallen below a threshold level for some time interval this may lead to an inference that the corresponding database object is not likely to require master led coordination anytime in the near future. Accordingly if the RG is currently active but there is a low estimated probability of master led coordination being needed the master status of the current master replica may be revoked and the RG may be placed in dormant state at least temporarily in some implementations.

In at least some embodiments the DSM may be responsible for tracking the health status of the storage nodes and or of tracking the individual health states of specific replicas stored on the storage nodes e.g. using a heartbeat message exchange protocol. If the DSM determines that the master replica of an ARG is not responsive or cannot be contacted via the network the DSM may also revoke the master status of the replica and in some cases the corresponding RG may be left for at least some time without a master. In some scenarios a master replica may itself voluntarily give up its master role e.g. if abnormal conditions or overload is detected at the storage node at which the master is resident . Thus master elections as well as master status revocations may in general be initiated due to a number of different reasons and by a number of different sources in various embodiments.

In at least some embodiments clients may be enabled to access to a given database object e.g. by being provided with a logical identifier or name for the object which can be used to direct work requests to the object even if none of the corresponding replicas has been elected master. Depending on the types of operations the client wishes to perform a given client work request may either be satisfied without electing a master e.g. an eventually consistent read may be performed at any non master replica or the response to the given client request may have to wait until master election is completed e.g. in the case of a write or a consistent read . The master election process itself may comprise several possibly asynchronous steps in different embodiments and the order in which the various steps are completed may in some cases be unpredictable because of the distributed nature of the entities involved. In one embodiment the master election process may begin by a particular replica such as a replica targeted by an RR to respond to a client s work request requesting a token or lock from the DSM . If the token lock is granted the remaining replicas or some number of remaining replicas may be informed that the token has been granted to the particular replica. The particular replica may then have to wait for acknowledgements from at least some of the other replicas before it and the DSM consider master election completed. After the master has been elected the master may coordinate work operations for those work requests that require it such as for writes or consistent reads. In some embodiments each replica of a given replica group may be configured to submit its own request for the token or lock whenever it discovers that another replica has submitted a similar request thus allowing the DSM greater flexibility in the selection of the next master as described below in further detail with reference to . Such an approach may help reduce the average time taken to elect a master for example since the DSM may become aware of all the candidates for master fairly quickly.

By letting at least some replica groups remain master less for some periods of time especially during recovery following a failure event the workload imposed on the DSM may be substantially reduced and or smoothed over time compared to the alternative of always immediately electing masters as soon as possible. Such an approach may thus reduce the likelihood of the DSM becoming overloaded. This in turn may help the services relying on the DSM from encountering problematic scenarios such as positive feedback loops involving missed heartbeat responses followed by repeated new heartbeat messages which in turn lead to further missed heartbeat responses and so on.

As noted earlier in at least some embodiments a number of different client work request types may be supported by the DDS some of which may require coordination by a master replica while others may not. respectively illustrate high level overviews of write operations consistent read operations and eventually consistent read operations at a distributed database implementing multiple replicas of objects according to at least some embodiments. shows a client s write work request e.g. an insert update create or modify operation directed to a table or to table metadata being submitted by a client to an RR . The RR in turn submits an internal representation of a write request to the master replica of the replica group of the targeted data object. In cases in which there is no master at the time the write request is received master election may have to be completed before the write operation can actually be performed at any of the replicas. The master replica may implement the write operation locally and then propagate the writes to one or more non master replicas A and B as indicated by arrows A and B. The master may wait for one or more acknowledgements e.g. A and or B before an internal write response indicating write completion at the back end is sent to the RR . In turn after receiving the internal write response the RR may transmit a client write response to the client . The number of non master replicas from which acknowledgements are required before a write is considered successful may vary in different implementations. In some implementations no such acknowledgements may be required as long as the local write at the master replica has been completed successfully.

In a client sends a consistent read CR work request to an RR . The CR work request may be directed to a table or to metadata about the table which may also be replicated together with the data for example. The RR once again identifies the master replica of the RG of the targeted data object which may in some cases require a master election which is to respond to the corresponding internal CR request . The master verifies that the latest write directed to the data object has been successfully acknowledged by at least some of the non master replicas which may or may not require any waiting by the master depending on how recently the latest write was propagated and sends an internal CR response comprising the requested data back to the RR . The RR in turn may send a client CR response back to the requesting client.

Various other types of work requests in addition to or instead of the illustrated combination of writes consistent reads and eventually consistent reads may be implemented in different embodiments. In some embodiments only writes and consistent reads may be supported while in other embodiments only writes and eventually consistent reads may be supported. In some embodiments different consistency levels may be supported for writes as well as reads e.g. in an embodiment in which N replicas are maintained for a given object writes that are considered complete when they have been successfully propagated to 1 K L and N replicas may be supported where 1

If there are no requests ahead of the one submitted by the replica A in token request queue the token may be granted to replica A and a token response may be provided to indicate the grant in the depicted embodiment. The master candidate A may then submit respective election requests A and B to the remaining replicas B and C respectively in effect asking the remaining replicas to acknowledge A s election as master. In at least some embodiments when a given replica such as B or C receives an indication that a different replica such as A is attempting to become the master the replica that receives the indication may also submit its own token request to the DSM such as token request A or B to DSM server nodes B and C respectively which may also be added to the token request queue . This may be done as a precautionary measure to shorten the overall time taken to elect a master for example. If the DSM cannot grant master status to the first candidate replica A in this example for some reason or if the original master candidate A fails or becomes disconnected before the election protocol completes and the DSM already has enqueued requests from other replicas this may speed up the time taken to choose one of the other replicas as the master. In addition it should be noted that in a distributed environment such as that shown in the order in which requests and responses are received processed and or sent by the DSM nodes and the replicas may in general be unpredictable so that sending multiple master candidacy requests may represent a conservative approach to completing master election as soon as possible. In the depicted embodiment therefore a master election may be triggered at a particular replica as a result of determining that a different replica has initiated the master election protocol.

After receiving the election request from the master candidate A in the depicted embodiment the other replicas B and C may each eventually respond with a respective election acknowledgement A and B indicating their confirmation that they view A as the elected master. In some embodiments only a subset of the remaining non master replicas may be required to acknowledge the master s election for the election protocol to be considered complete. Thus in one such embodiment if the replica group comprises three replicas as shown in the candidate A to whom the token is granted may require only one election acknowledgment either from B or C to complete the election protocol. In at least some implementations the confirmed master A may send an election complete message to the DSM and or the remaining replicas B and C may send messages to the DSM confirming the completion of the master election. In some embodiments when master election is confirmed a lock indicating master status i.e. a different primitive than the token used to start the election may be granted to the master replica. Upon receiving the election completion indications any remaining queued token requests for the token granted to the master replica A may be removed from token request queue in some implementations. In some embodiments messages among the replicas and the DSM server nodes may be transmitted and or received in a different order than that described above during master election. In addition although each replica is shown as communicating with a different DSM server node such a one to one mapping between replicas and DSM server nodes may not be a requirement in at least some embodiments. In some implementations whether master election is occurring or not the replicas may each be configured to receive and respond to heartbeat messages from the DSM e.g. so that the DSM can monitor the health status as opposed to for example the master versus non master status of the replicas. In some such implementations some of the messages related to master election may be piggybacked on included within the heartbeat messages or the heartbeat messages may be piggybacked on the election related messages.

In at least some embodiments as indicated in and in a distributed state manager may be used to implement aspects of the master election mechanism. illustrates an example of a distributed state manager that may be used by a distributed database according to at least some embodiments. In the illustrated embodiment the DSM comprises a server cluster with a plurality of server nodes e.g. A B C and D . The replica group may be considered a distributed client of the DSM in the illustrated embodiment with each replica comprising one or more processes including at least one process configured to communicate with the DSM server cluster . In the illustrated embodiment each replica such as A may comprise an execution of database service code A and client library components A. Similarly replica B executes database service code B and client library component B. In general different client processes may execute different application level or user level code and or library components in some embodiments. The client library component of the DSM may in some embodiments comprise a software library that exposes one or more programmatic interfaces to the database service code for interacting with the DSM server cluster . In various embodiments database service of a replica may invoke various methods of the client library component to interact with the DSM server cluster over a network e.g. to request tokens or locks access different constants and or variables of client library components and or otherwise access data and functionality of the client library components . In some embodiments the client library components may read data from the DSM update data in the DSM and or listen for events notifications from the DSM.

In some embodiments each node of the DSM server cluster may be implemented using a different physical and or virtual machine. In other embodiments two or more of nodes may be different software instances or processes executing on the same physical or virtual machine. The set of server nodes may be referred to as a collective in some embodiments.

A given client process of the DSM e.g. a process implementing some or the functionality of a replica may communicate with the collective via one of the nodes in the depicted embodiment. Different clients may communicate with different nodes in some embodiments. The particular node chosen by a client may depend on the identity of the client the type of request and or other factors. In some embodiments a client may maintain affinity to a given node once communication and or another relationship has been established. It may also be possible for a client that has established affinity with a first node of the DSM collective to switch to another node at any time. This may be done arbitrarily in response to a node failure in response to the node becoming overloaded in response to a scheduler component indicating that the client should switch to another node and or in response to various other events. As shown in the illustrated embodiment the various nodes may communicate with one another via network connections . These network connections may be implemented using various types of networks e.g. Myrinet Ethernet Gigabit Ethernet etc. in various topologies e.g. ring grid Torus bus etc. .

For simplicity of explanation many of the embodiments described herein comprise a DSM implemented on a fully connected cluster of computers where each node is a different physical machine in the cluster executes a separate instance of the DSM node software and can communicate directly with every other node in the collective via a network connection. However those skilled in the art will appreciate that various other configurations are possible using different physical and or virtual machines connected by different network types and or topologies as described above.

According to nodes of the collective may work together to maintain a shared state e.g. for various clients including replica groups in a logical registry . The logical registry may not necessarily be implemented as a separate physical entity but rather as a logical entity that may be implemented across multiple nodes of the DSM. For example in the depicted embodiment each node may keep a respective local copy e.g. local registry copy A for node A local registry copy B for node B and so on of the logical registry . Through a consensus protocol the nodes may agree on state transitions for each node to apply to its local registry copy thereby collectively maintaining a single logical registry . Each node may thus maintain a cached copy of the registry that is valid as of the last registry transition i.e. update known at the node. In some embodiments each transition may be associated with a registry logical timestamp such as in a monotonically increasing 64 bit integer or counter agreed upon by the collective. This timestamp may be a physical or logical time in different embodiments and may be referred to as the DSM time herein. In embodiments where the DSM time is maintained as a counter it may be incremented each time the registry is updated in some implementations e.g. each change to the logical registry may result in a change to the DSM time and each change in the DSM time may indicate that at least one element of the registry was updated. Each node may maintain its own registry logical timestamp e.g. logical timestamp A for node A logical timestamp B for node B and so on indicative of the most recent transition of the logical registry that is reflected in the local registry copy at that node. At any point in time in some implementations the value of the local logical registry timestamp at a given node may differ from the value of the local logical registry timestamp of another node however if and when two nodes have the same local logical registry timestamp values the data in their respective local registry copies may be identical i.e. both local copies of the registry may be guaranteed to have applied the same set of updates . In at least some implementations each node may also maintain an independent system clock separate from the registry logical timestamps.

Logical registry may include information that may be accessed in a consistent manner by a plurality of the nodes . In some embodiments the logical registry may include several types of primitive and compound elements and associated metadata such as tokens lock objects session objects representing connections to client processes and the like. In some embodiments the DSM may maintain multiple logical registries. In such embodiments each logical registry may be identified by a unique name. Different logical registries may be used to store data relevant to a corresponding client application or set of client applications in some embodiments. For example different distributed applications may use respective logical registries separate from one another. In other embodiments a single logical registry may include elements representing state information of a plurality of client applications. In some embodiments each of the elements of a given registry such as tokens locks other data entries and or sessions may be identified by a respective pathname e.g. companyA databaseB tableC within a namespace e.g. each element may be identified via a string concatenated from substrings where each substring represents a respective hierarchical component named using a directory like naming convention starting with a root substring e.g. companyA in the case of the element with the identifier companyA databaseB tableC .

In some embodiments at least some entries within the logical registry may include a name value creation time a modification time and or some other timestamp. The time related information stored in the registry such as creation time or modification time may be expressed using DSM time in some embodiments and or using system clock time in other embodiments. For example a global timestamp value based on DSM time may be stored for a token representing master status of a given replica group in at least some embodiments. The global timestamp value may indicate when the corresponding token request was received at the collective in some implementations or when the corresponding token request was processed at the collective in other implementations. The time a token request was processed may be somewhat later than the time the request was received depending on various factors in different implementations and or on how busy the node s were when the request was received. In some embodiments a logical registry may also list named client processes and or client sessions e.g. representations of connections between client processes and the server cluster recognized by the DSM. Such listings may also include configuration parameters for those client processes and or sessions.

The DSM server cluster may act as a mediator between the clients such as replicas and one or more logical registries . The client may interact with a logical registry by submitting transactions to the DSM server cluster which may interact with the logical registry on behalf of the client process. Through a read transaction a client process may read information such as token records locks entries or sessions from the logical registry . Using a write transaction a client process may update information in the logical registry . A request for a token such as an invocation of a Queue For Token or Queue For Lock API by a replica may represent a write transaction in some implementations as it may lead to the creation and or storage of a corresponding token record in the logical registry and the global timestamp corresponding to the commit of the transaction may be saved in the permit record. The granting of the token or lock may also represent a write transaction in some embodiments. Similarly in some embodiments a token revocation or release operation may be treated as a write transaction since it may typically involve removing a record from the logical registry.

Each transaction may have different possible outcomes. In some embodiments these outcomes may correspond to success write success read abort and fail. A success write outcome may indicate that a transaction that includes at least one write operation was executed successfully and the registry has been updated. A success read outcome may indicate that the transaction executed successfully but did not make any changes to the logical registry. An abort outcome may indicate that the transaction was aborted for some reason other than the particular contents of the transaction. In various embodiments the DSM may abort and or reattempt a transaction for different reasons. A fail outcome may indicate that the transaction failed for example because an object that was to be created already exists the request contained a syntax error and or for various other reasons.

The DSM may determine the outcome of various transactions and route event notifications e.g. as indicated by the arrows labeled in indicating the outcomes to interested client processes. Client processes may register to receive some or all events in some events in some embodiments e.g. using a transactional watch mechanism that may be implemented by the DSM. The transactional watch mechanism may allow a given client process to indicate a watch target comprising one or more objects e.g. tokens within the registry and to receive notifications whenever any of those objects is modified. In some implementations a consistent cache mechanism may be implemented by the state manager e.g. using the watch mechanism enabling multiple client processes to maintain respective read only caches that are updated by the state manager whenever a modification occurs to a cache data set comprising registry objects. Internally within the cluster notifications corresponding to the transaction outcomes may be sent to some or all of the server nodes . In some embodiments the DSM may be configured to forward all event notifications to every client by default but to allow each client to set up various filters to indicate the types of events in which it is interested. Each event notification may be labeled with a physical and or logical timestamp from which the clients may be able to infer bounds on the age of the event in one embodiment.

As indicated above in some embodiments clients may use transactions to read insert remove and or modify data stored in the logical registry . In some embodiments a transaction may be specified using a stack based language. Such a transaction may be specified in terms of data tokens and operations where the data tokens are pushed directly onto a stack and the operations read their inputs from the stack and push results to the stack. After such a transaction is complete the resulting stack may be sent to the client process.

In some embodiments the DSM may execute a failure detection service to detect failures among different nodes . For example if a given node crashed or became otherwise unresponsive the failure detection service may determine this. In some embodiments a failure detection service may be executed across the server cluster in a distributed manner. In one embodiment the failure detection service may also determine whether particular client processes e.g. processes of replicas have become unresponsive for example by monitoring heartbeat messages received from various client processes. In at least some embodiments if connectivity is lost between a client process and a given DSM node the client library component of the client process may automatically attempt to establish a connection to a different node. In at least some implementations the client process may not be aware of the identity of the node to which it is connected.

As noted earlier in at least some embodiments the distributed database service may include a front end layer comprising request router nodes that receive client work requests and transmit corresponding internal requests for work operations to the appropriate replicas at storage nodes where the targeted data is located. illustrates example interactions between a request router node of a distributed database service and the members of a replica group in response to a client work request according to at least some embodiments. In the illustrated embodiment each replica e.g. A B and C comprises a respective plurality of subcomponents an operation request handler ORH e.g. A B and C a group communications module GCM e.g. A B and C and a local database process e.g. A B and C . In some embodiments the subcomponents may each be implemented as a separate process or thread of execution for example. The ORH at a replica may be configured to receive internal work operation requests e.g. requests for writes consistent reads or eventually consistent reads from RRs as indicated by arrows e.g. A B and C and provide corresponding responses e.g. A B and C to the RRs. If coordination with other members of the replica group and or interactions with the DSM are required the ORH at a replica may request the GCM of that replica to perform the desired coordination interactions as indicated by arrows e.g. A B and C . GCMs may thus represent client processes of the DSM in the depicted embodiment. Primitive operations such as local reads or writes may be implemented at the local database process in response to local requests e.g. A B and C from the ORH and corresponding local responses e.g. A B and C may be provided to the ORHs.

In the depicted embodiment RR node may include a cache of membership information regarding various replica groups e.g. including mappings between database table names or identifiers and network addresses and or storage device addresses at which the replicas of the tables are located. When a work request whose implementation requires master led coordination is received directed to a particular database object with a corresponding replica group the RR node may first check its cache to see if the cache contains an indication of a current master replica for the targeted data object. If the cache contains an entry identifying the master the internal work request may be transmitted to that master. If however the cache does not contain an entry for a master the RR may submit a query to an administrative node of the DDS requesting information identifying the master and or other replicas of the targeted data object. The administrative node may send a query response which may include the requested information. In some cases e.g. if the replica group is known not to have a master or if the administrative node is unaware whether a master has been elected or not the query response may not identify the master replica.

If the RR is unable to identify a master replica in the depicted embodiment the RR may begin a process of contacting each of the members of the targeted replica group in some selected order requesting the replicas in turn to perform the operation that requires master led coordination. In effect by submitting such requests in at least some implementations the RR may induce each of the replicas in turn to attempt to become the master replica until one of them succeeds. Thus for example if the internal operation request A to replica A requires master led coordination at a point in time at which there is no replica designated as master GCM A of replica A may initiate the process of master election by requesting the token associated with master status for RG from the DSM. However because replica A has not yet been elected master the response A may indicate to the RR that replica A is not yet master cannot coordinate the requested operation and hence must reject the requested operation. The RR may then submit the same request to replica B in effect asking replica B if it can perform the master led coordination. In turn replica B s GCM B may send a token request to the DSM in an attempt to get B elected master and the response B may also indicate that replica B is not yet the master and must reject the requested operation. Next the RR may send the same request to replica C inducing C in turn to attempt to become the master. If the RR cannot find a master even though it has contacted all the replicas of RG at least once the RR may again request the operation from the replicas one by one e.g. in the same order as before until the master election eventually completes and one of the replicas achieves master status and sends a response to the RR indicating that it can coordinate or has already coordinated the requested operation. After a master has been elected the RR may update cache to indicate which particular replica was successful in attaining the master status in the depicted embodiment. In some implementations the roles of the GCMs and the ORHs may be combined. In at least one implementation some kinds of internal requests e.g. writes which may require communications with other replicas or with a state manager may be sent by RRs directly to the GCMs instead of via an ORH.

If the received work request requires master less coordination as detected in element e.g. if it involves one or more writes or consistent reads the front end node may attempt to identify the master replica. If an elected master can be identified as detected in element an internal operation request corresponding to the client s work request may be transmitted to the master and the needed operations may be coordinated from the master element . Such coordination may involve one or more steps which may depend on the specifics of the work operations for example the coordination may include waiting for and receiving acknowledgements of write propagation from one or more non master replicas.

If an elected master cannot be identified as also detected in element master election may be initiated element e.g. by a replica that receives the internal request from the front end node in a manner similar to operations illustrated in and . For example a quorum based election protocol that includes a token request being sent from the master candidate to a state manager such as a DSM and election acknowledgements being received by the master candidate to which the token is granted may be used. After the master election process completes back end operations may be coordinated for the work request by the elected master element .

If the client s work request does not require any operations coordinated by a master replica as also determined in element in the depicted embodiment one of the replicas may be selected to implement the necessary operations e.g. eventually consistent read operations element . The internal request from the front end node may be directed to the selected replica and the operations may be performed at the selected node element . In either case i.e. whether master election was required or not a response to the client s work request may be provided in the depicted embodiment element . Thus in the embodiment depicted in the reception of a client work request that requires master led coordination may itself serve as a triggering condition for master election if a master has not already been elected . Work requests that do not require master led coordination may be completed without electing a master i.e. while the replica group remains in dormant state. Thus depending on the types of work requests that are received and the sequences in which they are received replica groups may remain master less for substantial periods of time in such embodiments. Accordingly the state manager workload associated with master election may be reduced or at least time shifted compared to scenarios in which the active state is the default state for replica groups or scenarios in which masters are elected as soon as possible.

If RR is not able to identify a master also in operations corresponding to element for example because the replica group is in a dormant state or if the replica that was elected master is not responsive RR may start sending an internal work request to the replica group members in order until RR is eventually able to identify the master replica on the basis of a response from one of the members. As shown in element the next replica to target may be selected by RR e.g. based on a randomly determined order in which the replicas of the RG are to be contacted or based on various heuristics . The internal work request may be transmitted to the selected replica element . If the targeted replica acknowledges that it is the master as determined in element RR may update its cache with the identification of the master element and may await the next client work request element . If the targeted replica does not acknowledge that it is the replica in at least some embodiments it may be able to respond to RR with the correct master replica s identification information element or a hint or suggestion regarding which replica it believes to be the master. Non master replicas that participate in master election may learn which replica has been elected as soon as the election process terminates in some embodiments and may thus be able to inform the RRs regarding the identity of the master. If RR receives such a master identification it may update its cache accordingly element and send the internal work request to the identified master element . If RR is not provided an identification of the master as also detected in element the next target replica may be selected and operations corresponding to elements onwards may be repeated until a master is eventually found. As indicated in the reception by a non master replica of an internal work request that requires master led coordination may serve as an incentive for that replica to attempt to become elected master in at least some implementations. Accordingly in such implementations the loop of operations illustrated by elements onwards may eventually lead to a master being elected which may in turn lead to an exit from the loop.

If the number of such requests is below a threshold as detected in element e.g. if the number is very small or zero the DDS component may determine that there is no need for RG to continue to have a replica designated as master. Accordingly in the depicted embodiment the master status of R may be revoked and RG may be hibernated or moved back into master less state element . Such proactive hibernation may involve for example requesting the DSM to revoke a token or lock associated with master status from R in some embodiments. Subsequently if and when a master election triggering condition is detected master election may again be initiated for RG element . For example master election may be triggered when a work request targeted at D that requires master led coordination arrives or if the probability that a master led coordination requiring work request is likely to be received within a time window is estimated to exceed a threshold. Thus over time a given replica group may move in and out of active state in the depicted embodiment based on metrics and or estimates of different types of work request arrivals.

It is noted that in various embodiments operations other than those illustrated in the flow diagrams of may be implemented to support conditional master election and that some of the operations shown may not be implemented or may be implemented in a different order or in parallel rather than sequentially.

The techniques described above of deferring or postponing master election in a distributed database in which data objects are replicated may be useful in a variety of different scenarios. For example for large database services that comprise tens of thousands or hundreds of thousands of tables and or table replicas it may be the case that a substantial number of the tables may not be accessed very frequently. If after a large scale outage in such an environment masters had to be elected as soon as possible for all the replica groups for all the tables the state management machinery that is used for master election may become overwhelmed. This may be particularly likely if the state management machinery is also used for other purposes than master election alone e.g. for health status monitoring. In such environments therefore it may be advantageous to avoid electing or designating replicas until and unless the need for a master becomes apparent or it becomes possible to predict that a master is likely to be required within a short time.

In at least some embodiments a server that implements a portion or all of one or more of the technologies described herein including the techniques to implement database object replicas storage nodes front end nodes such as request routers administrative nodes or state manager nodes may include a general purpose computer system that includes or is configured to access one or more computer accessible media. illustrates such a general purpose computing device . In the illustrated embodiment computing device includes one or more processors coupled to a system memory via an input output I O interface . Computing device further includes a network interface coupled to I O interface .

In various embodiments computing device may be a uniprocessor system including one processor or a multiprocessor system including several processors e.g. two four eight or another suitable number . Processors may be any suitable processors capable of executing instructions. For example in various embodiments processors may be general purpose or embedded processors implementing any of a variety of instruction set architectures ISAs such as the x86 PowerPC SPARC or MIPS ISAs or any other suitable ISA. In multiprocessor systems each of processors may commonly but not necessarily implement the same ISA. In some embodiments graphics processing units GPUs may be used in addition to or instead of conventional CPUs.

System memory may be configured to store instructions and data accessible by processor s . In various embodiments system memory may be implemented using any suitable memory technology such as static random access memory SRAM synchronous dynamic RAM SDRAM nonvolatile Flash type memory or any other type of memory. In the illustrated embodiment program instructions and data implementing one or more desired functions such as those methods techniques and data described above are shown stored within system memory as code and data .

In one embodiment I O interface may be configured to coordinate I O traffic between processor system memory and any peripheral devices in the device including network interface or other peripheral interfaces such as various types of persistent and or volatile storage devices used to store physical replicas of data object partitions. In some embodiments I O interface may perform any necessary protocol timing or other data transformations to convert data signals from one component e.g. system memory into a format suitable for use by another component e.g. processor . In some embodiments I O interface may include support for devices attached through various types of peripheral buses such as a variant of the Peripheral Component Interconnect PCI bus standard or the Universal Serial Bus USB standard for example. In some embodiments the function of I O interface may be split into two or more separate components such as a north bridge and a south bridge for example. Also in some embodiments some or all of the functionality of I O interface such as an interface to system memory may be incorporated directly into processor .

Network interface may be configured to allow data to be exchanged between computing device and other devices attached to a network or networks such as other computer systems or devices as illustrated in through for example. In various embodiments network interface may support communication via any suitable wired or wireless general data networks such as types of Ethernet network for example. Additionally network interface may support communication via telecommunications telephony networks such as analog voice networks or digital fiber communications networks via storage area networks such as Fibre Channel SANs or via any other suitable type of network and or protocol.

In some embodiments system memory may be one embodiment of a computer accessible medium configured to store program instructions and data as described above for through for implementing embodiments of the corresponding methods and apparatus. However in other embodiments program instructions and or data may be received sent or stored upon different types of computer accessible media. Generally speaking a computer accessible medium may include non transitory storage media or memory media such as magnetic or optical media e.g. disk or DVD CD coupled to computing device via I O interface . A non transitory computer accessible storage medium may also include any volatile or non volatile media such as RAM e.g. SDRAM DDR SDRAM RDRAM SRAM etc. ROM etc. that may be included in some embodiments of computing device as system memory or another type of memory. Further a computer accessible medium may include transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as a network and or a wireless link such as may be implemented via network interface . Portions or all of multiple computing devices such as that illustrated in may be used to implement the described functionality in various embodiments for example software components running on a variety of different devices and servers may collaborate to provide the functionality. In some embodiments portions of the described functionality may be implemented using storage devices network devices or special purpose computer systems in addition to or instead of being implemented using general purpose computer systems. The term computing device as used herein refers to at least all these types of devices and is not limited to these types of devices.

Various embodiments may further include receiving sending or storing instructions and or data implemented in accordance with the foregoing description upon a computer accessible medium. Generally speaking a computer accessible medium may include storage media or memory media such as magnetic or optical media e.g. disk or DVD CD ROM volatile or non volatile media such as RAM e.g. SDRAM DDR RDRAM SRAM etc. ROM etc. as well as transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as network and or a wireless link.

The various methods as illustrated in the Figures and described herein represent exemplary embodiments of methods. The methods may be implemented in software hardware or a combination thereof. The order of method may be changed and various elements may be added reordered combined omitted modified etc.

Various modifications and changes may be made as would be obvious to a person skilled in the art having the benefit of this disclosure. It is intended to embrace all such modifications and changes and accordingly the above description to be regarded in an illustrative rather than a restrictive sense.

