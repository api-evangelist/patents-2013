---

title: Leveraging array operations at virtualized storage processor level
abstract: A technique for managing data storage operations in a data storage system having a file-based front end system and a block-based back end array includes provisioning a set of storage resources on the array for use exclusively with one or more virtualized storage processors (VSPs), where each VSP identifies multiple file systems. In response to a management command to perform a data storage operation on a VSP, the back-end array performs the specified operation on the provisioned set of storage resources. Thus, the back-end array performs the data operation exclusively on the VSP (or on multiple VSPs) and therefore on all of its constituent file systems as one.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09535630&OS=09535630&RS=09535630
owner: EMC IP Holding Company LLC
number: 09535630
owner_city: Hopkinton
owner_country: US
publication_date: 20130627
---
Data storage systems are arrangements of hardware and software that include one or more storage processors coupled to non volatile storage devices. In typical operation the storage processors service storage requests that arrive from users. The storage requests specify data elements to be written read created or deleted for example. The storage processors each may run software that manages incoming storage requests and performs various data processing tasks to organize and secure user data stored on the non volatile storage devices.

Some data storage systems take the form of file based systems such as NAS network attached storage systems. Such systems communicate with users of host computing devices hosts using one or more file based protocols such as NFS Network File System CIFS Common Internet File System and or SMB 3.0 server message block for example. File based systems allow users to create and store file systems and to export those file systems for access over a computer network. File based systems typically provide advanced features such as replication and snap for protecting data and for allowing hosts to roll back their data to earlier points in time. These advanced features may be operated at the file system level such that users may specify and direct performance of replication and snap operations on individual file systems.

Other data storage systems take the form of block based arrays. Such arrays communicate with hosts using one or more block based protocols such as Fibre Channel iSCSI and or InifiniBand for example. In some examples block based arrays include multiple disk drives electronic flash drives and the like and allow hosts to provision logical storage volumes from such drives. The arrays may export such volumes called logical unit numbers or LUNs to hosts using one or more of the above protocols or some other protocol. Hosts may then access data on LUNs e.g. for reading or writing by specifying the particular LUN and a block offset range relative to the LUN. Arrays may also provide advanced features such as replication and snap for example which operate at the level of individual LUNs.

In some arrangements a file based system provides a front end and connects to a block based array which operates as a back end. Hosts access one or more storage processors in the file based system using a file based protocol and the file based system accesses the array using a block based protocol. Thus the file based system services requests from hosts while the block based array efficiently stores host data.

Users of file based systems e.g. NAS systems often create and access tens hundreds or even more file systems. Unfortunately however conventional file based systems typically require users to specify advanced features e.g. replication and snap on a per file system basis. For users of hundreds or more file systems the burden of specifying these features for so many files systems becomes great. Users of combined systems which employ a file based front end with a block based back end typically face the same burdens as the file based systems on the front end continue to manage the file systems.

In contrast with the conventional approach of managing data storage operations on a file based system an improved technique for managing data storage operations in a data storage system having a file based front end system and a block based back end array includes provisioning a set of storage resources on the array for use exclusively with one or more virtualized storage processors VSPs where each VSP is associated with multiple file systems. Users or internal processes may issue management commands to perform data storage operations such as replication and or snap on a designated VSP. In response to a management command to perform a data storage operation on a VSP the back end array performs the specified operation on the provisioned set of storage resources. Thus the back end array performs the data operation exclusively on the VSP or on multiple VSPs and therefore on all of its constituent file systems as one. Users may thus define and perform advanced features at the VSP level rather than at the individual file system level greatly reducing the burden placed on users to manage the advanced features. An additional benefit of the above described technique is that it also leverages the advanced features of the back end array which may provide high performance data processing and high efficiency.

Certain embodiments are directed to a method of managing data storage operations. The method includes directing a data storage array coupled to a storage processor to provision a set of storage resources on the array for storing multiple file systems of a VSP virtualized storage processor operated by the storage processor. The method further includes in response to the storage processor receiving file based requests to access files of the file systems of the VSP sending block based requests to the array to access content of the files from the set of storage resources. The method still further includes in response to an issuance of a management command to perform a data operation on the VSP directing the array to perform the data operation on each of the set of storage resources thereby effecting the data operation by the array on the file systems of the VSP together as a group.

Other embodiments are directed to computerized apparatus and computer program products. Some embodiments involve activity that is performed at a single location while other embodiments involve activity that is distributed over a computerized environment e.g. over a network .

Embodiments of the invention will now be described. It is understood that such embodiments are provided by way of example to illustrate various features and principles of the invention and that the invention hereof is broader than the specific example embodiments disclosed.

An improved technique for managing data storage operations in a data storage system having a file based front end system and a block based back end array includes provisioning a set of storage resources on the array for use exclusively with one or more virtualized storage processors VSPs where each VSP identifies multiple file systems. In response to a management command to perform a data storage operation on a VSP the back end array performs the specified operation on the provisioned set of storage resources. Thus the back end array performs the data operation exclusively on the VSP or on multiple VSPs and therefore on all of its constituent file systems as one.

Efforts are underway to develop virtualized storage processors VSPs . VSPs allow users to aggregate any number of file systems within a single construct and to associate the aggregated file systems with a particular set of file based servers such as NFS servers and or CIFS servers for example. Earlier constructs for aggregating file systems are known in the art as virtual data movers or VDMs. More information about VSPs may be found in copending U.S. patent application Ser. No. 13 828 294 filed Mar. 14 2013 the contents and teachings of which are hereby incorporated by reference in their entirety.

The front end data storage apparatus is seen to include a storage processor or SP . It is understood that he front end data storage apparatus may include multiple SPs like the SP . For example multiple SPs may be provided as circuit board assemblies or blades which plug into a chassis that encloses and cools the SPs. The chassis has a backplane for interconnecting the SPs and additional connections may be made among SPs using cables. It is understood however that no particular hardware configuration is required as any number of SPs including a single one can be provided and the SP can be any type of computing device capable of communicating with hosts N and with array .

The back end block based array the array includes a set of storage devices a management interface and operations . The set of storage devices may include for example hard disk drives electronic flash drives solid state drives and or other types of storage devices. The management interface exposes a set of application programming interfaces APIs which allow the SP or another computing device to configure the array . The operations provide advanced features such as replication and snap for example. Such operations may act locally e.g. within the array and or may act in coordination with a remote appliance or other equipment e.g. to store replicated data and or snaps at a different site . Without limiting the generality of the foregoing an example of a suitable array for use in connection with the improvements hereof is the VMAX family of storage arrays which are available from EMC Corporation of Hopkinton Mass. Suitable non limiting examples of advanced features include SRDF Symmetrix Remote Data Facility for replication and TimeFinder for snap. It is understood that the array may include many more components which are omitted from the figure for simplicity such as its own storage processor s network interface and user interface for example.

The network can be any type of network such as a local area network LAN wide area network WAN the Internet and or some other type of network or combination of networks. The hosts N connect to the SP over the network using various file based protocols such as NFS SMB 3.0 and CIFS for example. Any number of hosts N may connect to the SP over the network using any of the above protocols some subset thereof or other protocols besides those shown.

The interconnect connects the SP to the array and supports one or more block based protocols such as Fibre Channel iSCSI or InfiniBand for example. Although the interconnect is shown as a single point to point connection it is understood that the interconnect may itself be realized as a network e.g. a TCP IP network or a storage area network . Using a network multiple front end data storage systems like the system may connect to the array and multiple arrays like the array may connect to one or more front end data storage systems.

The SP is seen to include one or more communication interfaces a set of processing units and memory . The communication interfaces include for example adapters for converting electronic and or optical signals received from the network and from the interconnect to electronic form for use by the SP and for converting electronic signals from the SP to electronic and or optical signals for transmission over the network and over the interconnect . The set of processing units includes one or more processing chips and or assemblies. In a particular example the set of processing units includes multiple multi core CPU chips. The memory includes both volatile memory e.g. RAM and non volatile memory such as one or more ROMs disk drives solid state drives SSDs and the like. The set of processing units and the memory together form control circuitry which is constructed and arranged to carry out various methods and functions as described herein. Also the memory includes a variety of software constructs realized in the form of executable instructions. When the executable instructions are run by the set of processing units the set of processing units are made to carry out the operations of the software constructs. Although certain software constructs are specifically shown and described it is understood that the memory typically includes many other software constructs which are not shown such as an operating system various applications processes and daemons.

The memory of the SP is seen to provide multiple VSPs M. Each of the VSPs M is associated with a set of file systems a set of servers e.g. NFS and or CIFS servers and various properties and settings. In an example each VSP has internal file systems e.g. a configuration file system and a root file system as well as any number of user file systems.

In an example the SP includes or has access to a configuration database . The configuration database includes a set of records that store for each VSP information that identifies the VSP s file systems servers and settings. The configuration database may reside on the SP as shown or at some other location in the data storage apparatus accessible to the SP .

The memory also provides a VSP manager . The VSP manager manages lifecycle events of VSPs M such as VSP creation and deletion as well as data operations performed on VSPs such replication and snap.

The memory further provides a front end TO stack . The front end TO stack TO stack receives and processes incoming requests from hosts N to perform file based operations on file systems accessed through the SP . In an example all file systems accessed through the SP belong to a particular VSP i.e. one of the VSPs M . Each file system can belong to only a single VSP. The TO stack processes incoming file based requests e.g. file reads and writes by mapping files identified in the requests to particular block based storage locations on the array . Likewise the TO stack processes block based input from the array and maps the input to file based information relative to particular file systems. Thus the TO stack manages the mapping of requests and responses between the hosts N and the array .

The memory of the SP still further provides a pool manager . The pool manager manages within the SP the provisioning of storage for use by the file systems of the VSPs M.

In example operation a user operating one of the hosts e.g. host issues a request to the SP to create a new VSP. The request may specify a VSP name e.g. VSP as well as other configuration information. In response to the SP receiving the request the VSP manager establishes a new set of records in the configuration database for VSP and provisions storage to VSP e.g. from the array .

For example the VSP manager communicates with the array via the management interface and directs the array to provision a set of storage resources for supporting the new VSP. For example the VSP manager directs the array to provision a set of LUNs one or more LUNs from the storage devices and to make the newly provisioned LUNs available e.g. to export the LUNs over Fiber Channel iSCSI or InfiniBand for example so that the data storage apparatus can discover the LUNs and access them using block based requests via the interconnect . Although the word LUNs is presented in the plural form this is for presentation purposes and it is understood that the LUNs may be realized as a single LUN.

Once the set of LUNs are provisioned on the array the VSP manager creates a storage pool for VSP and assigns the newly provisioned LUNs to the storage pool. The pool manager then renders the LUNs in the form of storage slices e.g. 256 MB units of storage and adds the slices to the storage pool. File systems of VSP are then created on the storage pool with the pool manager provisioning slices derived from the LUNs on the array to the new file systems to meet their storage demands.

To summarize the VSP manager may perform the following activities in response to receiving the request to create a new VSP 

The VSP manager can change the size of the storage pool in response to usage. For example as users create new file systems of VSP and or store more content on their existing file systems the VSP manager can direct the array to extend any of the provisioned LUNs so that more slices can be made available for supporting the additional file content. The VSP manager may also direct the array to provision a new LUN for supporting the new VSP. The VSP manager can further shrink the storage pool if the provisioned storage is not fully utilized. For example the VSP manager can direct the array to remove LUNs to free portions of LUNs and or to shrink LUNs.

The LUNs supporting any VSP above may be assigned to a group such that data operations e.g. replication and snap may be performed by operations of the array on the group of LUNs together. For example a host e.g. the host N may issue a command to perform a snap operation on VSP . In response to the command the VSP manager may respond in different ways.

In one example the VSP manager identifies each of the LUNs in the group of LUNs provisioned on the array for VSP . For example the VSP manager may identify the particular pool assigned to VSP e.g. by referencing the set of records in the configuration database for VSP and then access a separate data store to identify the LUNs on the array that have been assigned to the identified pool. The VSP manager then directs the array via operations to perform the designated operation here a snap on each of the LUNs in the designated group.

In another example the VSP manager directs the array to form a group itself from the set of LUNs provisioned to the new VSP. For example the VSP manager communicates with the array via the management interface and directs the array to establish a consistency group that includes the set of LUNs. Consistency groups are known in the art and may be used by arrays to enforce consistent treatment of LUNs. Thus for example forming a consistency group from the set of LUNs ensures that all data operations e.g. replication and snap are performed on all members of the consistency group together or that no operations are performed on any members at all. Further the consistency group enforces consistency in time such that any snap taken of a consistency group reflects the state of all elements of the consistency group at the same point in time.

In an example the array forms a consistency group from the set of LUNs gives the consistency group an identifier e.g. Group A and stores the identifier along with its member LUNs internally. The VSP manager may then communicate with the array to obtain the group identifier and stores the group identifier on the SP or in a location readily accessible to the SP such as in the set of records for VSP in the configuration database . Thereafter the VSP manager may direct the array to perform a data storage operation by retrieving its local copy of the group identifier e.g. Group A and directing the array to perform the requested operation on the identified consistency group e.g. via a command to Snap Group A. 

It is understood that the VSP manager may direct the array to create the consistency group at any suitable time. For instance in one example the VSP manager directs the array to form a consistency group when the VSP is first being created e.g. as one of the implicit acts performed in response to receiving the command to create a VSP. In another example the VSP manager waits until the SP receives a command to perform a data operation on the VSP and then direct the array to form a consistency group in response to receiving the command .

Once VSP has been created the SP may service requests from any of hosts N authorized to access VSP to read and or write its files and directories to create and delete file systems and to perform other functions. Read and write requests are processed by the IO stack .

It should be appreciated that specifying a particular set of LUNs for a particular VSP and providing the LUNs in a consistency group or simply in a group ensures that data storage operations such as replication and snap are performed at the VSP level of granularity. Thus users can build any number of file systems on a VSP and be assured that the file systems will all receive the same treatment as far as data storage operations are concerned by virtue of their belonging to the same VSP. It should further be appreciated that it is the array with its efficient and high performance data processing which performs the data storage operations applied at the VSP level of granularity. This is the case in spite of the array not being required to have any awareness of the existence of VSPs. Rather the disclosed technique provides VSP level data storage operations by virtue of its mapping of particular VSPs to particular groups of LUNs.

The array pool on the array includes array slices which are derived from the underlying storage devices . In response to requests from the VSP manager the array creates LUNs on the array from the array slices . For example the array composes a set of LUNs and . In this example each LUN is composed of five array slices for illustrative purposes LUNs typically include more than five slices each and the number of slices in each LUN may be different . The array has brought the LUNs and together into a consistency group so that the array may perform data operations such as replication and snap consistently across the entire group of LUNs.

It is understood that the array may support many VSPs as well as other data objects and that the array pool may include other consistency groups. For example shows another consistency group formed from three LUNs and . The LUNs and may support storage for another VSP such as VSP and the consistency group ensures that the array performs data operations such as replication and snap consistently across the entire group of LUNs and that support VSP .

Once the array has created and exported the LUNs and for discovery by the SP the pool manager may render the LUNs and in the form of slices and add the slices to the storage pool as shown by the arrows . The slices need not be the same size as the array slices . In an example the array slices are each 1 GB in size whereas the slices in the pool are each 256 MB the figure is not drawn to scale . In an example the pool manager renders the LUNs and in the form of slices by carving the block offset ranges of the LUNs and into slice sized segments where the slice sized segments each have the size of the slices .

With slices provisioned to the pool from the set of LUNs and in the array the pool manager running on the SP may provision the slices to any number of lower deck file systems . For example the pool manager may provision slices S to S to a first lower deck file system LDFS . The pool manager may further provision slices S to S to a second lower deck file system LDFS and may provision slices S S to a third lower deck file system LDFS . These provisioned slices of the pool collectively form slices which together provide storage for supporting the lower deck file systems .

In an example the data storage apparatus manages the lower deck file systems internally i.e. the hosts N do not generally access the lower deck file systems directly. Each of the lower deck file systems includes one or more files with each file typically representing a data object such as an upper deck file system e.g. one of the upper deck file systems . Thus each file of a lower deck file system may store a representation of a complete upper deck file system. For example LDFS includes one file that stores a complete representation of a root file system Root FS and another file that stores a complete representation of a configuration file system Config FS . Similarly LDFS includes a file that stores a complete representation of a first user file system User FS and LDFS includes a file that stores a complete representation of a second user file system User FS . Mapping expresses the lower deck files as complete file systems and manages the correspondence between blocks of the upper deck file systems and blocks of the lower deck file systems .

It should be appreciated that the lower deck file systems may also include files that store snaps of the primary objects they represent e.g. LDFS may store files that represent snaps of User FS . However because embodiments hereof call for the array to snap LUNs that support a VSP the lower deck file systems need not themselves include files that represent snaps and each lower deck file system except LDFS in the example shown may include only a single file.

Each lower deck file system includes metadata such as inodes and indirect blocks not shown which describe the file s that the respective lower deck file system stores. Also it is understood that the data within a file of a lower deck file system includes both data and metadata of the upper deck file system that the file represents. Lower deck file systems may store other types of data objects besides upper deck file systems. Additional information about lower deck file systems and upper deck file systems may be found in copending U.S. patent application Ser. No. 13 828 322 filed on Mar. 14 2013 the contents and teachings of which are hereby incorporated by reference in their entirety.

In an example the pool provisions slices to the lower deck file systems as storage is required consistently with a thin provisioning storage model. Thus for example if LDFS requires additional storage to support user files being stored therein the pool manager may provision another one of the slices to LDFS. Likewise if the slices of LDFS are underutilized e.g. if there is more than one slices worth of free space the pool manager may move content off a slice i.e. evacuate the slice and return the evacuated slice to the pool . The returned slice may then be repurposed to any of the lower deck file systems i.e. to any lower deck file systems that share the same pool and that require a new slice.

Previously the repurposing of slices to other file systems that share the same pool was as much as could be done to reclaim storage. For example slices could not be repurposed to data objects in other pools or to data objects that do not use a pool. Additional improvements hereof overcome such limitations in repurposing storage however.

In the pool manager has freed the slice S which has been made available for repurposing. Slices S S and S continue to support LDFS. If the pool as a whole is underutilized then the slice S may be returned to the array rather than being used to support another data object served by the pool . Thus the pool as a whole may be made to shrink. Arrow shows the slice returning operation. Here pool manager sends a message to the management interface on the array . The message identifies a particular LUN here LUN and a particular offset range within the identified LUN i.e. the range from which the pool manager derived the slice S. The pool manager then directs the array to free the specified offset range within the identified LUN. The free space is indicated by the unshaded region of the LUN . The array may then return the freed region of the LUN to the array pool i.e. the free portion thereof but the process for returning the freed region to the array pool and repurposing the freed region varies depending on whether the LUN is itself thinly provisioned or thickly densely provisioned from the array pool .

As shown in the array slice is in the array pool and the LUN has been made smaller. The array may then repurpose the slice via the operation shown by arrow to extend the LUN used by VSP . For example the VSP manager directs the array to extend LUN so that the pool supporting VSP can grow. The pool manager may carve the extended space of LUN into new slices and add the slices to the pool used by VSP . In this manner storage that was once provisioned for use by one VSP is repurposed for use by another VSP even though the two VSPs are built upon different pools and are formed from LUNs that belong to different consistency groups.

The VSP manager may direct the array to redefine the LUN to be a smaller LUN by first moving any content VSP data and metadata stored on the LUN at locations beyond a predetermined offset and then truncating the LUN to the predetermined offset . Preferably the offset is chosen to be an integer number of array slice units from the end of the LUN . In the example shown in the LUN includes a region which includes VSP data and metadata. As the region is located at a higher offset range than the predetermined offset the VSP manager directs the array to evacuate the region as indicated by arrow by moving VSP data and metadata stored therein to locations below the predetermined offset . Once all VSP data and metadata in the region have been evacuated the VSP manager directs the array to truncate the LUN to the offset .

As shown in the LUN has been truncated and the regions and have been returned to the array pool . In this example it is assumed for simplicity that the size of the array slices is the same as the size of the slices . The slices and may then be repurposed as shown by arrow by extending the LUN in substantially the same way as described above in connection with . Thus storage that was once provisioned for use by one VSP is repurposed for use by another VSP regardless of whether the LUNs that provide the underlying storage are thinly provisioned or thickly provisioned.

At step a data storage array coupled to a storage processor is directed to provision a set of storage resources on the array for storing multiple file systems of a VSP virtualized storage processor operated by the storage processor. For example in response to one of the hosts N issuing a request to create a VSP the VSP manager directs the array to provision a set of LUNs on the array for storing multiple file systems such as a root file system a configuration file system and any number of user file systems of a VSP e.g. VSP operated by the SP .

At step in response to the storage processor receiving file based requests to access files of the file systems of the VSP block based requests are sent to the array to access content of the files from the set of storage resources. For example in response to receiving file based requests e.g. via NFS CIFS SMB 3.0 to access files of one of the VSPs M the IO stack processes the requests to generate block based requests and the SP sends the block based requests to the array over the interconnect .

At step in response to an issuance of a management command to perform a data operation on the VSP the array is directed to perform the data operation on each of the set of storage resources thereby effecting the data operation by the array on the file systems of the VSP together as a group. For example in response to the command to Snap VSP the VSP manager directs the array to use the operations to take a snap of the set of LUNs . Taking a snap of the set of LUNs effectively snaps the file systems of the VSP that is provisioned from the LUNs.

An improved technique for managing data storage operations in a data storage system has been described for a data storage system having a file based front end system and a block based back end array . The technique includes provisioning a set of storage resources e.g. LUNs on the array for use exclusively with one or more virtualized storage processors VSPs where each VSP is associated with multiple file systems. In response to a management command to perform a data storage operation on a VSP the back end array performs the specified operation on the provisioned set of storage resources. Thus the back end array performs the data operation exclusively on the VSP or on multiple VSPs and therefore on all of its constituent file systems as one. The above described technique also leverages the advanced features of the back end array which may provide high performance data processing and high efficiency. Further the improved technique allows storage used by one VSP to be repurposed for use with another VSP even if the two VSPs are served from different pools.

Having described certain embodiments numerous alternative embodiments or variations can be made. For example it has been shown and described that one VSP is associated with one pool which derives its resources from one set of LUNs on the array . However this is merely one effective example of a way in which the invention can be practiced. In other examples multiple VSPs may be provided on a single pool which derives its resources from a single set of LUNs on the array . Providing multiple VSPs on a single pool results in all such VSPs being treated the same way from the standpoint of data storage operations. For example snap and replication settings and activities operated by the array will be common to all VSPs provided on the same pool derived from the same set of LUNs.

It is also possible and within the scope of the invention hereof for a single VSP to be built over multiple pools where each pool derives its storage resources from a different set of LUNs on the array . This example may be handled in various ways. According to one approach the consistency group or simply the group for the VSP can be defined to include the sets of LUNs supporting all of the multiple pools. Data storage operation settings and actions are then performed together over all the sets of LUNs supporting the multiple pools on which the VSP is built. According to another approach each pool has its own consistency group or simply group and file systems of the VSP hosted from different pools are allowed to have different data storage operation settings and actions.

Also although replication and snap have been described as examples of data storage operations that may be leveraged by the array data storage operations are not limited to these operations. For example compression encryption de duplication and other operations may be applied in the same manner as has been described above for replication and snap e.g. with the array performing the operations on a set of storage resources from which one or more VSPs are provisioned.

Also although the improvements hereof have been described in connection with hosts connecting to a front end data storage system which in turn connects to a backend array these systems may alternatively be combined or their constituents can be rearranged or repackaged in other forms. For example the hosts N can be loaded with software that enables them to perform some or all of the functions disclosed for the SP .

Also although the storage resources provisioned for storing the file systems of a VSP have been described as LUNs e.g. LUNs this is merely an example. Another example would be to directly provision array slices to the pool without forming LUNs. Consistency groups or simply groups could then be formed from aggregations of array slices rather than from groups of LUNs.

Also the request to create a new VSP and the command to snap a VSP have been shown and described as originating from hosts. This again is merely an example as such requests and commands may originate from anywhere including within the SP within some other SP from a separate administrative console from a remote system or from any other source or location.

Further although features are shown and described with reference to particular embodiments examples and variants hereof such features may be included and hereby are included in any of the disclosed embodiments examples and variants. Thus it is understood that features disclosed in connection with any embodiment are included as variants of any other embodiment.

Further still the improvement or portions thereof may be embodied as a non transient computer readable storage medium such as a magnetic disk magnetic tape compact disk DVD optical disk flash memory Application Specific Integrated Circuit ASIC Field Programmable Gate Array FPGA and the like shown by way of example as medium in . Multiple computer readable media may be used. The medium or media may be encoded with instructions which when executed on one or more computers or other processors perform methods that implement the various processes described herein. Such medium or media may be considered an article of manufacture or a machine and may be transportable from one machine to another.

As used throughout this document the words comprising including and having are intended to set forth certain items steps elements or aspects of something in an open ended fashion. Also as used herein and unless a specific statement is made to the contrary the word set means one or more of something. Although certain embodiments are disclosed herein it is understood that these are provided by way of example only and the invention is not limited to these particular embodiments.

Those skilled in the art will therefore understand that various changes in form and detail may be made to the embodiments disclosed herein without departing from the scope of the invention.

