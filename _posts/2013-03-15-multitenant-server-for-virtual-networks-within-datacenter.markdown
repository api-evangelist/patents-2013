---

title: Multitenant server for virtual networks within datacenter
abstract: In general, techniques are described for facilitating multi-tenancy of a server accessed by virtual networks of a data center. A device included within a data center comprising one or more processors may perform the techniques. The processors may be configured to execute a virtual switch that supports a number of virtual networks executing within the data center. The virtual switch may be configured to receive a request regarding data associated with an identifier that is unique within one of the virtual networks that originated the request. The virtual switch may then translate the identifier included within the request to generate a globally unique identifier that is unique within the plurality of virtual networks, update the request to replace the identifier included within the request with the globally unique identifier, and transmit the updated request to a server of the data center.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08959185&OS=08959185&RS=08959185
owner: Juniper Networks, Inc.
number: 08959185
owner_city: Sunnyvale
owner_country: US
publication_date: 20130315
---
This application claims the benefit of U.S. Provisional Application No. 61 723 685 filed Nov. 7 2012 U.S. Provisional Application No. 61 722 696 filed Nov. 5 2012 U.S. Provisional Application No. 61 721 979 filed Nov. 2 2012 U.S. Provisional Application No. 61 721 994 filed Nov. 2 2012 U.S. Provisional Application No. 61 718 633 filed Oct. 25 2012 U.S. Provisional Application No. 61 656 468 filed Jun. 6 2012 U.S. Provisional Application No. 61 656 469 filed Jun. 6 2012 and U.S. Provisional Application No. 61 656 471 filed Jun. 6 2012 the entire content of each of which being incorporated herein by reference.

Techniques of this disclosure relate generally to computer networks and more particularly to virtual networks.

In a typical cloud data center environment there is a large collection of interconnected servers that provide computing and or storage capacity to run various applications. For example a data center may comprise a facility that hosts applications and services for subscribers i.e. customers of data center. The data center may for example host all of the infrastructure equipment such as networking and storage systems redundant power supplies and environmental controls. In a typical data center clusters of storage systems and application servers are interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers. More sophisticated data centers provide infrastructure spread throughout the world with subscriber support equipment located in various physical hosting facilities.

In general techniques are described to facilitate multi tenancy in the context of a vendor neutral server such as an Interface for Metadata Access Point IF MAP server. The various techniques described in this disclosure may provide for a multi tenant IF MAP server in the context of a number of virtual networks executing within a data center where each of the virtual networks are associated with a different tenant and access the IF MAP server to maintain session data. Virtual switches that support these virtual networks may ensure identifiers associated with the session data may be globally unique across the virtual networks so as to effectively maintain separate or non overlapping virtual IF MAP server spaces for each of the virtual networks. To ensure this separation the virtual switches may translate identifiers in responses from the virtual networks from locally unique identifiers that are unique within the context of the virtual network to globally unique identifiers that are unique across all of the virtual networks. Typically this translation involves appending a namespace to the identifier in the response to create a globally unique identifier in the form of namespace identifier. In this manner the techniques may facilitate multi tenancy within vendor neutral databases.

In one embodiment a method comprises receiving with a virtual switch that supports a plurality of virtual networks executing within a data center a request regarding data associated with an identifier that is unique within one of the plurality of virtual networks that originated the request and translating the identifier included within the request to generate a globally unique identifier that is unique within the plurality of virtual networks. The method also comprises updating the request to replace the identifier included within the request with the globally unique identifier and transmitting the updated request to a server of the data center.

In another embodiment A device included within a data center comprises one or more processors configured to execute a virtual switch that supports a plurality of virtual networks executing within the data center. The virtual switch is configured to receive a request regarding data associated with an identifier that is unique within one of the plurality of virtual networks that originated the request translate the identifier included within the request to generate a globally unique identifier that is unique within the plurality of virtual networks update the request to replace the identifier included within the request with the globally unique identifier and transmit the updated request to a server of the data center.

In another embodiment a non transitory computer readable storage medium has stored thereon instructions that when executed cause one or more processors of a device included within a data center to execute a virtual switch that supports a plurality of virtual networks executing within the data center. The virtual switch is configured to receive a request regarding data associated with an identifier that is unique within one of the plurality of virtual networks that originated the request translate the identifier included within the request to generate a globally unique identifier that is unique within the plurality of virtual networks update the request to replace the identifier included within the request with the globally unique identifier and transmit the updated request to a server.

The details of one or more embodiments of the techniques are set forth in the accompanying drawings and the description below. Other features objects and advantages of the techniques will be apparent from the description and drawings and from the claims.

In some examples data center may represent one of many geographically distributed network data centers. As illustrated in the example of data center may be a facility that provides network services for customers . Customers may be collective entities such as enterprises and governments or individuals. For example a network data center may host web services for several enterprises and end users. Other exemplary services may include data storage virtual private networks traffic engineering file service data mining scientific or super computing and so on. In some embodiments data center may be individual network servers network peers or otherwise.

In this example data center includes set of storage systems and application servers A X herein servers interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers. Switch fabric is provided by a set of interconnected top of rack TOR switches A BN TOR switches coupled to a distribution layer of chassis switches . Although not shown data center may also include for example one or more non edge switches routers hubs gateways security devices such as firewalls intrusion detection and or intrusion prevention devices servers computer terminals laptops printers databases wireless mobile devices such as cellular phones or personal digital assistants wireless access points bridges cable modems application accelerators or other network devices.

In this example TOR switches and chassis switches provide servers with redundant multi homed connectivity to IP fabric and service provider network . Chassis switches aggregates traffic flows and provides high speed connectivity between TOR switches . TOR switches A and B may be network devices that provide layer 2 MAC address and or layer 3 IP address routing and or switching functionality. TOR switches and chassis switches may each include one or more processors and a memory and that are capable of executing one or more software processes. Chassis switches are coupled to IP fabric which performs layer 3 routing to route network traffic between data center and customers using service provider network .

Virtual network controller VNC provides a logically centralized controller for facilitating operation of one or more virtual networks within data center in accordance with one or more embodiments of this disclosure. In some examples virtual network controller may operate in response to configuration input received from network administrator .

Interface for Metadata Access Point IF MAP server IF MAP server may represent an intermediate network device that stores information in accordance with a vendor neutral data model. IF MAP originally referred to an authorization data model that provides a standardized authorization data model that vendors may adopt so as to reduce communication or interoperability issues that arise between vendor specific or proprietary authorization data models. The group responsible for introducing IF MAP known as the Trusted Computing Group TCG is encouraging vendors to accept this new IF MAP standard and vendors are releasing devices compliant with this standard.

The IF MAP standard provides not only a vendor neutral or cross vendor data model but also provides an IF MAP protocol by which to access the authorization information stored according to this standard vendor neutral authorization data model. The IF MAP protocol supports various IF MAP messages or communications by which to publish authorization information search authorization information stored within the IF MAP server subscribe to authorization information stored within the IF MAP server and poll the IF MAP server for authorization information to which a given device is subscribed. More information concerning the IF MAP cross vendor or vendor neutral data model and protocol can be found in a specification entitled TNC IF MAP Binding for SOAP Specification Version 2.1 Revision 15 dated May 7 2012 the contents of which are hereby incorporated by reference as if set forth in its entirety.

IF MAP has since been expanded or modified to accommodate different technologies including cloud computing which may involve data centers such as data center that host cloud computing applications . IF MAP server may represent an IF MAP server that implements a data model that conforms to IF MAP in this cloud computing context and that supports the IF MAP protocol for publishing polling accessing and or receiving data stored to this cloud computing version of the IF MAP data model.

While described herein with respect to this particular vendor neutral authorization data model set forth by the IF MAP standard the techniques may be implemented with respect to any standard or accepted authorization data model. Moreover while described as a separate device e.g. a standalone database IF MAP server may be integrated within any one of the network devices shown as residing within data center in . For example virtual network controller may include an integrated IF MAP server . The techniques of this disclosure therefore should not be limited to the example of in this respect.

A virtual network may be used with multi tenancy. The term multi tenancy may refer to a system in which a single hardware and software platform simultaneously supports multiple tenants e.g. customers clients from a common data store. The shared platform in the multi tenant architecture is usually designed to virtually partition data and operations so that each tenant works with a unique virtual application instance. In one implementation each subnet shown in may serve one tenant for example one company. In some examples a first virtual network would belong to CompanyA a second virtual network would belong to CompanyB etc.

IF MAP server may not however support multi tenancy when identifiers from different companies overlap. To illustrate by way of example both CompanyA and CompanyB may have an employee identified as Employee 5 where this identifier may be used to retrieve authorization or other information concerning Employee 5. IF MAP server may not be able to resolve to which of CompanyA and CompanyB the request for information concerning Employee 5 is to be returned. In this sense IF MAP server may be unable to maintain different records for Employee 5 at CompanyA and Employee 5 at CompanyB which may result in a single record used for both of Employee 5 at CompanyA and CompanyB. Mingling of information across CompanyA and CompanyB in this manner may represent a substantial breach of security especially given that IF MAP server may store authorization information that may result in Employee 5 receiving authorization to access data for a CompanyA at which Employee 5 may not work.

In accordance with various aspects of the techniques described in this disclosure one or more of servers may translate what may be referred to as locally unique identifiers into globally unique identifiers that are unique within the context of data center . In other words servers may translate identifiers used for accessing data stored to IF MAP server that are locally unique within the context of the company into identifiers that are unique across all of the companies accessing data center thereby potentially ensuring that data stored to IF MAP server remains secure in the sense that the data is only accessibly by the particular CompanyA and not by another company or customer of data center .

In operation server A may as one example receive a request regarding data associated with an identifier that is unique within one of a number of customers of data center where as described in more detail below each customer as is often represented within data center by a corresponding one of a number of virtual networks that originated the request. In the example of the request conforms to the vendor neutral IF MAP protocol but the techniques may be performed with respect to any vendor neutral or vendor specific protocol.

Server A may then translate the identifier e.g. Employee 5 included within the request to generate a globally unique identifier that is unique within the plurality of virtual networks. Typically this translation is transparent to the customer which is another way of saying that the customer CompanyA in this example is not aware of this translation given that the identifier is locally unique within the context of CompanyA. That is server A may translate the identifier in such a way that users and devices of CompanyA are not be able to detect that the translation occurred.

Typically server A may store an association between each customer or its virtual network representation of the customer and a namespace. As one example the namespace may comprise the name of the customer e.g. CompanyA . Server A may then append this namespace to the identifier to generate the globally unique identifier. To illustrate server A may receive a request from CompanyA having an identifier Employee 5. Server A may translate identifier Employee 5 to a globally unique identifier by appending CompanyA to Employee 5 with the resulting globally unique identifier of CompanyA Employee 5. Appending namespaces in this manner is similar to how namespaces are employed in various computer programming languages such as C .

In any event after generating the globally unique identifier server A may update the request to replace the identifier included within the request with the globally unique identifier. Server A may then transmit the updated request to IF MAP server . This request may comprise any form of communication including data publish requests data polling requests data retrieval requests or any other form of IF MAP request or communication. The term request should therefore not be restricted in this sense to a request for data but may refer to any request or communication.

IF MAP server may issue a response to the request where the response may include the globally unique identifier. Given that the customer CompanyA in this example is often not aware of the translation performed by server A server A may translate this globally unique identifier to recover the original identifier which again is locally unique within the context of CompanyA. Server A may then update the response to replace the globally unique identifier with the recovered locally unique identifier. Server A may then transmit the updated response to CompanyA which may process the response in any number of ways but often in the context of authorizing the identifier to access data stored by or applications executed by one or more of servers .

While described above as using namespaces the techniques may generally involve translation or transformation of the identifier from a locally unique identifier to a globally unique identifier. Using namespaces in the manner described above is merely one example of this translation and the techniques may be performed with respect to other forms of translation. For example the translation may be performed using a mapping between customer identifier combination and a globally unique identifier which may be a number or index into the table . In other words while namespaces are appended to the identifier to form the globally unique identifier the techniques may be performed with respect to other forms of translation that do not reuse the locally unique identifier in the manner described above with respect to namespaces.

In this way each tenant would be assigned its own namespace within its own subnet such as CompanyA and CompanyB. The tenants maintain identifiers that are unique in its respective context subnet but the identifiers may not be globally unique. For example CompanyA may have an identifier Employee 5 and CompanyB may also have the same identifier Employee 5 in their respective namespace. As each company has its own namespace the same identifier may be used by multiple companies without a conflict. Each tenant may then only know about the identifiers in its own namespace.

Each virtual switch may execute within a hypervisor a host operating system or other component of each of servers . In the example of virtual switch executes within hypervisor also often referred to as a virtual machine manager VMM which provides a virtualization platform that allows multiple operating systems to concurrently run on a corresponding one of host servers . In the example of virtual switch A manages or otherwise supports execution of virtual networks each of which provides a network environment for execution of one or more virtual machines VMs on top of the virtualization platform provided by hypervisor . Each VM is associated with one of the virtual subnets VN VN managed by the hypervisor .

In general each VM may be any type of software application and may be assigned a virtual address for use within a corresponding virtual network where each of the virtual networks may be a different virtual subnet provided by virtual switch A. A VM may be assigned its own virtual layer three L3 IP address for example for sending and receiving communications but may be unaware of an IP address of the server A on which the virtual machine is executing. In this way a virtual address is an address for an application that differs from the logical address for the underlying physical computer system i.e. switch A in the example of .

In one implementation each of servers includes a virtual network agent VN agent A X VN agents that controls the overlay of virtual networks and that coordinates the routing of data packets within server . In general VN agent communicates with virtual network controller which generates commands to control routing of packets through data center . VN agent may operate as a proxy for control plane messages between virtual machines and virtual network controller . For example a VM may request to send a message using its virtual address via the VN agent and VN agent may in turn send the message and request that a response to the message be received for the virtual address of the VM that originated the first message. In some cases a VM may invoke a procedure or function call presented by an application programming interface of VN agent and the VN agent may handle encapsulation of the message as well including addressing.

In one example network packets e.g. layer three L3 IP packets or layer two L2 Ethernet packets generated or consumed by the instances of applications executed by virtual machines within the virtual network domain may be encapsulated in another packet e.g. another IP or Ethernet packet that is transported by the physical network. The packet transported in a virtual network may be referred to herein as an inner packet while the physical network packet may be referred to herein as an outer packet. Encapsulation and or de capsulation of virtual network packets within physical network packets may be performed within virtual switches e.g. within the hypervisor or the host operating system running on each of servers . As another example encapsulation and de capsulation functions may be performed at the edge of switch fabric at a first hop TOR switch that is one hop removed from the application instance that originated the packet. This functionality is referred to herein as tunneling and may be used within data center to create one or more overlay networks. Other example tunneling protocols may be used including IP over GRE VxLAN MPLS over GRE etc.

As noted above virtual network controller provides a logically centralized controller for facilitating operation of one or more virtual networks within data center . Virtual network controller may for example maintain a routing information base e.g. on or more routing tables that store routing information for the physical network as well as the overlay network of data center . Similarly switches and virtual switches maintain routing information such as one or more routing and or forwarding tables. In one example implementation virtual switch A of hypervisor implements a network forwarding table NFT for each virtual network . In general each NFT stores forwarding information for the corresponding virtual network and identifies where data packets are to be forwarded and whether the packets are to be encapsulated in a tunneling protocol such as with one or more outer IP addresses.

The routing information may for example map packet key information e.g. destination IP information and other select information from packet headers to one or more specific next hops within the networks provided by virtual switches and switch fabric . In some case the next hops may be chained next hop that specify a set of operations to be performed on each packet when forwarding the packet such as may be used for flooding next hops and multicasting replication. In some cases virtual network controller maintains the routing information in the form of a radix tree having leaf nodes that represent destinations within the network. U.S. Pat. No. 7 184 437 provides details on an exemplary embodiment of a router that utilizes a radix tree for route resolution the contents of which is incorporated herein by reference in its entirety.

As shown in each virtual network provides a communication framework for encapsulated packet communications for the overlay network established through switch fabric . In this way network packets associated with any of virtual machines may be transported as encapsulated packet communications via the overlay network. In addition in the example of each virtual switch includes a default network forwarding table NFTand provides a default route that allows packet to be forwarded to virtual subnet VN without encapsulation i.e. non encapsulated packet communications per the routing rules of the physical network of data center . In this way subnet VN and virtual default network forwarding table NFTprovide a mechanism for bypassing the overlay network and sending non encapsulated packet communications to switch fabric .

Moreover virtual network controller and virtual switches may communicate using virtual subnet VN in accordance with default network forwarding table NFTduring discovery and initialization of the overlay network and during conditions where a failed link has temporarily halted communication via the overlay network. Once connectivity with the virtual network controller is established the virtual network controller updates its local routing table to take into account new information about any failed links and directs virtual switches to update their local network forwarding tables . For example virtual network controller may output commands to virtual network agents to update one or more NFTs to direct virtual switches to change the tunneling encapsulation so as to re route communications within the overlay network for example to avoid a failed link.

When link failure is detected a virtual network agent local to the failed link e.g. VN Agent A may immediately change the encapsulation of network packet to redirect traffic within the overlay network and notifies virtual network controller of the routing change. In turn virtual network controller updates its routing information any may issues messages to other virtual network agents to update local routing information stored by the virtual network agents within network forwarding tables .

As further shown in the example of multiple servers have access to the IF MAP server via virtual network controller . Also multiple tenants have access to the same IF MAP server . Each server has multiple tenants VN VN VN and a Translator in the form of VN that knows all the tenants managed by its server . Upon receiving a query which may be another way of referring to the requests referenced above from a subnet such as VN that belongs to CompanyA the translator VN converts the query into a globally unique query by adding the unique namespace of the querying tenant in this case CompanyA .

For example suppose a query is generated by CompanyA about Employee 5. Without the Translator VN a query about Employee 5 may not be processed because CompanyB and Company C may also have an identifier Employee 5 in their respective namespaces and the IF MAP server would not know to which Employee 5 the query pertains. The Translator VN translates the query Employee 5 into CompanyA Employee 5 by identifying the connection through which the query came in i.e. whether it came from VN VN etc. in the example of . With the translated query that is globally unique the techniques may promote for efficient use of IF MAP server in that IF MAP server may be shared by multiple tenants .

The query result which is another way of referring to the response referenced above generated by IF MAP server may include the namespace CompanyA. If needed the translator VN may strip the namespace from the globally unique identifier and thereby recover the locally unique identifier before forwarding the result to the subnet VN which is the subnet that originated the request query because the subnet VN may not recognize the result that includes the namespace CompanyA. 

While the tenants are not virtualization aware or in other words aware of the multi tenancy of data center IF MAP server is virtualization aware and may use a format such as namespace identifier when associating the session data for a particular identifier to the namespace identifier in the database.

In this sense VN which may represent the translator of virtual switch A as one example may receive a request regarding data associated with an identifier that is unique within one of the VNs that originated the request. VN may then translate the identifier included within the request to generate a globally unique identifier that is unique within the VNs . VN may also update the request to replace the identifier included within the request with the globally unique identifier and transmit the updated request to IF MAP server via virtual network controller .

In some instances as noted above VN may perform the translation by appending a namespace assigned to the one of the virtual networks that originated the request i.e. VN in the example above to the identifier to generate the globally unique identifier.

As described above the techniques may promote more efficient usage of IF MAP server by facilitating multi tenancy. As a result of performing the techniques VN may receive another request from a different one of VNs than the one that originated what may be referred to as the first request described above i.e. VN in the above example where this second request may concern data associated with the same identifier as that included in the first request. This same identifier may also be unique within this other one of VNs e.g. VN. VN may then translate the identifier included within the second request to generate a second globally unique identifier that is unique within VNs and different from the other globally unique identifier generated for the first request. VN may update this second request to replace the second identifier included within the second request with the second globally unique identifier and transmit the updated second request to IF MAP server . In this manner the techniques may promote multi tenancy within IF MAP server without having to virtualize or otherwise create and maintain multiple instances of an IF MAP server such as one per customer or tenant .

Upon receiving a query from a subnet VN that belongs to for example CompanyA the translator VN converts the query into a globally unique query by adding the unique namespace of the querying tenant in this case CompanyA . For example suppose a query is generated by Company A about Employee 5. Without the translator VN a query regarding Employee 5 may not be processed because CompanyB and CompanyC may also have an identifier Employee 5 in their respective namespaces and IF MAP server would not know to which Employee 5 the query pertains. The translator VN translates the query Employee 5 into CompanyA Employee 5 by identifying the connection through which the query came in. With the translated query that is globally unique IF MAP server can be used efficiently given that multiple tenants may share the same IF MAP server i.e. IF MAP server in the example of .

The query result generated by IF MAP server may include the namespace CompanyA. The translator VN may strip the namespace before forwarding the result to the subnet VN because the subnet VN may not recognize the result that includes the namespace CompanyA. 

Typically VNs forward all packets that are directed to resources internal to data center to VN which represents the VN for data center . In other words when configuring VNs each of VNs are configured to forward requests for resources internal to data center such as IF MAP server to VN . VN may then process these requests which in the case of IF MAP server may include translating this identifier which is layer seven L7 data in terms of the Open Systems Interconnection OSI model and modifying the request to replace this identifier with the globally unique identifier.

Although not shown in the example of VN may also include a table or other data structure that identifies which of VNs have been configured to use a namespace so as to automatically generate globally unique identifiers and which of VNs have not been configured to use a namespace so as to automatically generation globally unique identifiers. That is some of VNs may be configured to use a namespace when formatting requests for session data associated with a particular identifier while others of VNs may not be configured to use a namespace in this manner. For those of VNs not configured to use a namespace translator may transparently translate the identifier in these requests to generate an updated request having the globally unique identifier in place of the identifier generated by those of VNs that do not use namespaces. VN may maintain a data structure to identify those of VNs that use namespaces and those that do not use namespaces using this table to process both requests sent to and responses from IF MAP server .

Virtual network controller VNC of illustrates a distributed implementation of a VNC that includes multiple VNC nodes A N collectively VNC nodes to execute the functionality of a data center VNC including managing the operation of virtual switches for one or more virtual networks implemented within the data center. Each of VNC nodes may represent a different server of the data center e.g. any of servers of or alternatively on a server or controller coupled to the IP fabric by e.g. an edge router of a service provider network or a customer edge device of the data center network. In some instances some of VNC nodes may execute as separate virtual machines on the same server.

Each of VNC nodes may control a different non overlapping set of data center elements such as servers individual virtual switches executing within servers individual interfaces associated with virtual switches chassis switches TOR switches and or communication links. VNC nodes peer with one another using peering links to exchange information for distributed databases including distributed databases A K collectively distributed databases and routing information e.g. routes for routing information bases A N collectively RIBs . Peering links may represent peering links for a routing protocol such as a Border Gateway Protocol BGP implementation or another peering protocol by which VNC nodes may coordinate to share information according to a peering relationship.

VNC nodes of VNC include respective RIBs each having e.g. one or more routing tables that store routing information for the physical network and or one or more overlay networks of the data center controlled by VNC . In some instances one of RIBs e.g. RIB A may store the complete routing table for any of the virtual networks operating within the data center and controlled by the corresponding VNC node e.g. VNC node A .

In general distributed databases define the configuration or describe the operation of virtual networks by the data center controlled by distributed VNC . For instance distributes databases may include databases that describe a configuration of one or more virtual networks the hardware software configurations and capabilities of data center servers performance or diagnostic information for one or more virtual networks and or the underlying physical network the topology of the underlying physical network including server chassis switch TOR switch interfaces and interconnecting links and so on. Distributed databases may each be implemented using e.g. a distributed hash table DHT to provide a lookup service for key value pairs of the distributed database stored by different VNC nodes .

As illustrated in the example of distributed virtual network controller VNC includes one or more virtual network controller VNC nodes A N collectively VNC nodes . Each of VNC nodes may represent any of VNC nodes of virtual network controller of . VNC nodes that peer with one another according to a peering protocol operating over network . Network may represent an example instance of switch fabric and or IP fabric of . In the illustrated example VNC nodes peer with one another using a Border Gateway Protocol BGP implementation an example of a peering protocol. VNC nodes provide to one another using the peering protocol information related to respective elements of the virtual network managed at least in part by the VNC nodes . For example VNC node A may manage a first set of one or more servers operating as virtual network switches for the virtual network. VNC node A may send information relating to the management or operation of the first set of servers to VNC node N by BGP A. Other elements managed by VNC nodes may include network controllers and or appliances network infrastructure devices e.g. L2 or L3 switches communication links firewalls and VNC nodes for example. Because VNC nodes have a peer relationship rather than a master slave relationship information may be sufficiently easily shared between the VNC nodes . In addition hardware and or software of VNC nodes may be sufficiently easily replaced providing satisfactory resource fungibility.

Each of VNC nodes may include substantially similar components for performing substantially similar functionality said functionality being described hereinafter primarily with respect to VNC node A. VNC node A may include an analytics database A for storing diagnostic information related to a first set of elements managed by VNC node A. VNC node A may share at least some diagnostic information related to one or more of the first set of elements managed by VNC node A and stored in analytics database as well as to receive at least some diagnostic information related to any of the elements managed by others of VNC nodes . Analytics database A may represent a distributed hash table DHT for instance or any suitable data structure for storing diagnostic information for network elements in a distributed manner in cooperation with others of VNC nodes . Analytics databases A N collectively analytics databases may represent at least in part one of distributed databases of distributed virtual network controller of .

VNC node A may include a configuration database A for storing configuration information related to a first set of elements managed by VNC node A. Control plane components of VNC node A may store configuration information to configuration database A using interface A which may represent an Interface for Metadata Access Points IF MAP protocol implementation. VNC node A may share at least some configuration information related to one or more of the first set of elements managed by VNC node A and stored in configuration database A as well as to receive at least some configuration information related to any of the elements managed by others of VNC nodes . Configuration database A may represent a distributed hash table DHT for instance or any suitable data structure for storing configuration information for network elements in a distributed manner in cooperation with others of VNC nodes . Configuration databases A N collectively configuration databases may represent at least in part one of distributed databases of distributed virtual network controller of .

Virtual network controller may perform any one or more of the illustrated virtual network controller operations represented by modules which may include orchestration user interface VNC global load balancing and one or more applications . VNC executes orchestration module to facilitate the operation of one or more virtual networks in response to a dynamic demand environment by e.g. spawning removing virtual machines in data center servers adjusting computing capabilities allocating network storage resources and modifying a virtual topology connecting virtual switches of a virtual network. VNC global load balancing executed by VNC supports load balancing of analytics configuration communication tasks e.g. among VNC nodes . Applications may represent one or more network applications executed by VNC nodes to e.g. change topology of physical and or virtual networks add services or affect packet forwarding.

User interface includes an interface usable to an administrator or software agent to control the operation of VNC nodes . For instance user interface may include methods by which an administrator may modify e.g. configuration database A of VNC node A. Administration of the one or more virtual networks operated by VNC may proceed by uniform user interface that provides a single point of administration which may reduce an administration cost of the one or more virtual networks.

VNC node A may include a control plane virtual machine VM A that executes control plane protocols to facilitate the distributed VNC techniques described herein. Control plane VM A may in some instances represent a native process. In the illustrated example control VM A executes BGP A to provide information related to the first set of elements managed by VNC node A to e.g. control plane virtual machine N of VNC node N. Control plane VM A may use an open standards based protocol e.g. BGP based L3VPN to distribute information about its virtual network s with other control plane instances and or other third party networking equipment s . Given the peering based model according to one or more aspects described herein different control plane instances e.g. different instances of control plane VMs A N may execute different software versions. In one or more aspects e.g. control plane VM A may include a type of software of a particular version and the control plane VM N may include a different version of the same type of software. The peering configuration of the control node devices may enable use of different software versions for the control plane VMs A N. The execution of multiple control plane VMs by respective VNC nodes may prevent the emergence of a single point of failure.

Control plane VM A communicates with virtual network switches e.g. illustrated VM switch executed by server using a communication protocol operating over network . Virtual network switches facilitate overlay networks in the one or more virtual networks. In the illustrated example control plane VM A uses Extensible Messaging and Presence Protocol XMPP A to communicate with at least virtual network switch by XMPP interface A. Virtual network route data statistics collection logs and configuration information may in accordance with XMPP A be sent as XML documents for communication between control plane VM A and the virtual network switches. Control plane VM A may in turn route data to other XMPP servers such as an analytics collector or may retrieve configuration information on behalf of one or more virtual network switches. Control plane VM A may further execute a communication interface A for communicating with configuration virtual machine VM A associated with configuration database A. Communication interface A may represent an IF MAP interface.

VNC node A may further include configuration VM A to store configuration information for the first set of element to and manage configuration database A. Configuration VM A although described as a virtual machine may in some aspects represent a native process executing on an operating system of VNC node A. Configuration VM A and control plane VM A may communicate using IF MAP by communication interface A and using XMPP by communication interface A. In some aspects configuration VM A may include a horizontally scalable multi tenant IF MAP server and a distributed hash table DHT based IF MAP database that represents configuration database A. In some aspects configuration VM A may include a configuration translator which may translate a user friendly higher level virtual network configuration to a standards based protocol configuration e.g. a BGP L3VPN configuration which may be stored using configuration database A. Communication interface may include an IF MAP interface for communicating with other network elements. The use of the IF MAP may make the storage and management of virtual network configurations very flexible and extensible given that the IF MAP schema can be dynamically updated. In some instances aspects of virtual network controller may be flexible for new applications .

VNC node A may further include an analytics virtual machine VM A to store diagnostic information and or visibility information related to at least the first set of elements managed by VNC node A. Control plane VM and analytics VM may communicate using an XMPP implementation by communication interface A. Analytics VM A although described as a virtual machine may in some aspects represent a native process executing on an operating system of VNC node A.

Analytics VM A may include analytics database A which may represent an instance of a distributed database that stores visibility data for virtual networks such as one of distributed database of distributed virtual network controller of . Visibility information may describe visibility of both distributed VNC itself and of customer networks. The distributed database may include an XMPP interface on a first side and a REST JASON XMPP interface on a second side.

Virtual network switch may implement the layer forwarding and policy enforcement point for one or more end points and or one or more hosts. The one or more end points or one and or one or more hosts may be classified into a virtual network due to configuration from control plane VM A. Control plane VM A may also distribute virtual to physical mapping for each end point to all other end points as routes. These routes may give the next hop mapping virtual IP to physical IP and encapsulation technique used e.g. one of IPinIP NVGRE VXLAN etc. . Virtual network switch may be agnostic to actual tunneling encapsulation used. Virtual network switch may also trap interesting layer 2 L2 packets broadcast packets and or implement proxy for the packets e.g. using one of Address Resolution Protocol ARP Dynamic Host Configuration Protocol DHCP Domain Name Service DNS etc.

In some cases different VNC nodes may be provided by different suppliers. However the peering configuration of VNC nodes may enable use of different hardware and or software provided by different suppliers for implementing the VNC nodes of distributed VNC . A system operating according to the techniques described above may provide logical view of network topology to end host irrespective of physical network topology access type and or location. Distributed VNC provides programmatic ways for network operators and or applications to change topology to affect packet forwarding and or to add services as well as horizontal scaling of network services e.g. firewall without changing the end host view of the network.

In accordance with various aspects of the techniques described in this disclosure the translator of one or more of configuration VMs may provide translation to facilitate multi tenancy within the IF MAP server database. That is these translators discussed above may perform the techniques described in this disclosure to translate identifiers unique to the originating one of configuration VMs but not unique globally across all of configuration VMs to globally unique identifiers to facilitate multi tenancy within the IF MAP server database in the manner described above. These translators may perform translation that is similar in this respect to the translation performed by VN as described above with respect to the example of .

The translator VN receives a query e.g. for data associated with the identifier Employee 5 from one of the tenants step such as CompanyA that is hosted by subnet VN. If the query does not include a namespace step the translator VN globalizes the query by adding a namespace to it based on from which tenant the query came step . If the query already includes a namespace then no namespace is added and step may be bypassed. The query with the added namespace may now be CompanyA Employee 5. This query with the added namespace is then forwarded to IF MAP server step and a result is received step . The result includes the namespace CompanyA as the IF MAP server is universal and handles queries from different subnets . For example the result may be CompanyA Miami. If the original query did not include a namespace the translator VN takes out the namespace step and forwards the result Miami to the tenant step . If the original query included a namespace the Translator VN may forward the query result to the tenant without taking out the namespace.

With the query translation process server A may automatically add the namespace to an identifier without the client or tenant being aware of the naming scheme. Server A may in this sense effectively translate between a locally unique identifier and a globally unique identifier allowing multiple tenants to share the IF MAP server .

As shown in the specific example of computing device includes one or more processors one or more communication units one or more input devices one or more output devices and one or more storage devices . Computing device in the specific example of further includes operating system virtualization module and one or more applications A N collectively applications . Each of components and may be interconnected physically communicatively and or operatively for inter component communications. As one example in components and may be coupled by one or more communication channels . In some examples communication channels may include a system bus network connection interprocess communication data structure or any other channel for communicating data. Virtualization module and applications as well as operating system may also communicate information with one another as well as with other components in computing device .

Processors in one example are configured to implement functionality and or process instructions for execution within computing device . For example processors may be capable of processing instructions stored in storage devices . Examples of processors may include any one or more of a microprocessor a controller a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or equivalent discrete or integrated logic circuitry.

One or more storage devices may be configured to store information within computing device during operation. Storage devices in some examples are described as a computer readable storage medium. In some examples storage devices are a temporary memory meaning that a primary purpose of storage devices is not long term storage. Storage devices in some examples are described as a volatile memory meaning that storage devices do not maintain stored contents when the computer is turned off. Examples of volatile memories include random access memories RAM dynamic random access memories DRAM static random access memories SRAM and other forms of volatile memories known in the art. In some examples storage devices are used to store program instructions for execution by processors . Storage devices in one example are used by software or applications running on computing device e.g. operating system virtualization module and the like to temporarily store information during program execution.

Storage devices in some examples also include one or more computer readable storage media. Storage devices may be configured to store larger amounts of information than volatile memory. Storage devices may further be configured for long term storage of information. In some examples storage devices include non volatile storage elements. Examples of such non volatile storage elements include magnetic hard discs tape cartridges or cassettes optical discs floppy discs flash memories or forms of electrically programmable memories EPROM or electrically erasable and programmable memories EEPROM .

Computing device in some examples also includes one or more communication units . Computing device in one example utilizes communication units to communicate with external devices. Communication units may communicate in some examples by sending data packets over one or more networks such as one or more wireless networks via inbound and outbound links. Communication units may include one or more network interface cards IFCs such as an Ethernet card an optical transceiver a radio frequency transceiver or any other type of device that can send and receive information. Other examples of such network interfaces may include Bluetooth 3G and WiFi radio components.

Computing device in one example also includes one or more input devices . Input devices in some examples are configured to receive input from a user through tactile audio or video feedback. Examples of input devices include a presence sensitive display a mouse a keyboard a voice responsive system video camera microphone or any other type of device for detecting a command from a user. In some examples a presence sensitive display includes a touch sensitive screen.

One or more output devices may also be included in computing device . Output devices in some examples are configured to provide output to a user using tactile audio or video stimuli. Output devices in one example include a presence sensitive display a sound card a video graphics adapter card or any other type of device for converting a signal into an appropriate form understandable to humans or machines. Additional examples of output devices include a speaker a cathode ray tube CRT monitor a liquid crystal display LCD or any other type of device that can generate output to a user.

Computing device may include operating system . Operating system in some examples controls the operation of components of computing device . For example operating system facilitates the communication of modules applications with processors communication units input devices output devices and storage devices . Virtualization module may represent a hypervisor such as hypervisor of or other module that provides the virtual environment in which the virtual switch such as virtual switches may execute. Applications may each include program instructions and or data that are executable by computing device . As one example application A may include instructions that cause computing device to perform one or more of the operations and actions described in the present disclosure.

In accordance with techniques of the present disclosure virtualization module of computing device may receive a request regarding data associated with an identifier that is unique within one of the plurality of virtual networks that originated the request. Virtualization module may as described above translate the identifier included within the request to generate a globally unique identifier that is unique within the plurality of virtual networks. Virtualization module may as described above update the request to replace the identifier included within the request with the globally unique identifier and transmit the updated request to an IF MAP server such as IF MAP server .

The techniques described in this disclosure may be implemented at least in part in hardware software firmware or any combination thereof. For example various aspects of the described techniques may be implemented within one or more processors including one or more microprocessors digital signal processors DSPs application specific integrated circuits ASICs field programmable gate arrays FPGAs or any other equivalent integrated or discrete logic circuitry as well as any combinations of such components. The term processor or processing circuitry may generally refer to any of the foregoing logic circuitry alone or in combination with other logic circuitry or any other equivalent circuitry. A control unit including hardware may also perform one or more of the techniques of this disclosure.

Such hardware software and firmware may be implemented within the same device or within separate devices to support the various techniques described in this disclosure. In addition any of the described units modules or components may be implemented together or separately as discrete but interoperable logic devices. Depiction of different features as modules or units is intended to highlight different functional aspects and does not necessarily imply that such modules or units must be realized by separate hardware firmware or software components. Rather functionality associated with one or more modules or units may be performed by separate hardware firmware or software components or integrated within common or separate hardware firmware or software components.

The techniques described in this disclosure may also be embodied or encoded in an article of manufacture including a computer readable storage medium encoded with instructions. Instructions embedded or encoded in an article of manufacture including a computer readable storage medium encoded may cause one or more programmable processors or other processors to implement one or more of the techniques described herein such as when instructions included or encoded in the computer readable storage medium are executed by the one or more processors. Computer readable storage media may include random access memory RAM read only memory ROM programmable read only memory PROM erasable programmable read only memory EPROM electronically erasable programmable read only memory EEPROM flash memory a hard disk a compact disc ROM CD ROM a floppy disk a cassette magnetic media optical media or other computer readable storage media. In some examples an article of manufacture may include one or more computer readable storage media.

A computer readable storage medium comprises a non transitory medium. The term non transitory indicates that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM or cache .

Various examples have been described. These and other examples are within the scope of the following claims.

