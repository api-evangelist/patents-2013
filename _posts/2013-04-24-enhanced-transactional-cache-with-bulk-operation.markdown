---

title: Enhanced transactional cache with bulk operation
abstract: Described herein is a technology for providing enhanced transactional caching. In accordance with one aspect, a transactional cache associated with a database is configured. Execution of a write operation on the database is delayed until a flush is determined to be necessary. The write operation is delayed by writing to the transactional cache. The flush is invoked by writing inserted or updated records in the transactional cache to the database via a bulk operation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09477609&OS=09477609&RS=09477609
owner: SAP SE
number: 09477609
owner_city: Walldorf
owner_country: DE
publication_date: 20130424
---
The present disclosure relates generally to data management in enterprise applications that access database systems. More particularly the present disclosure relates to enhanced transactional caching in enterprise applications that access database systems.

Databases are often used to support transactional data processing applications such as financials sales order fulfillment manufacturing human resources and enterprise resource planning applications. A system that implements a transaction application and its associated database e.g. relational database is referred to as an online transaction processing OLTP system. In an OLTP system there is a need for fast reads and writes of the transactional data to update many portions of the database simultaneously.

OLTP systems are optimized for processing very small amounts of detailed data but are generally not suited for analytical tasks that involve ad hoc analysis of large data amounts. In order to perform queries without negatively impacting the performance of the OLTP system online analytical processing OLAP systems were developed. OLAP systems are optimized for complex analytical and ad hoc queries with a rapid execution time. OLAP systems retrieve data using pre calculated data summaries and are not efficient when handling small numbers of users doing custom analytical processing over very large volumes of non indexed data.

Running both OLTP and OLAP on the same database has become an efficient way to reduce product total cost of ownership TCO and improve performance. However due to the different types of operations involved a database architecture that is suitable for OLAP optimization may not be suitable for OLTP. Conversely a database architecture that is suitable for OLTP may not be suitable for OLAP.

For example OLTP is characterized by a large number of row based write operations e.g. INSERT AND UPDATE while OLAP typically optimizes performance by compressing attribute or columns with the help of dictionaries. The use of column based databases for OLAP has become quite popular. However due to the model gap between OLTP row based operations and the column based architecture such column based databases may be the bottleneck of some operations that are critical to OLTP. This is especially true for OLTP applications that involve many row based write operations e.g. INSERT and UPDATE that make up about 50 of the total transaction time in a typical enterprise resource planning ERP transaction e.g. adding 100 line invoice with serial and batch items .

Thus a need exists for systems methods and apparatuses to address the shortfalls of current technology and to provide other new and innovative features.

A computer implemented technology for providing enhanced transaction caching is described herein. In accordance with one aspect a transactional cache associated with a database is configured. Execution of a write operation on the database is delayed until a flush is determined to be necessary. The write operation is delayed by writing to the transactional cache. The flush is invoked by performing a bulk operation to write inserted or updated records in the transactional cache to the database.

In accordance with another aspect a transactional cache associated with a column based in memory database is configured. Execution of a write operation on the database is delayed until a flush is determined to be necessary. The write operation is delayed by writing to the transactional cache. The flush is invoked by performing a column wise bulk operation that writes inserted or updated records in the transactional cache to the database.

This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the following detailed description. It is not intended to identify features or essential features of the claimed subject matter nor is it intended that it be used to limit the scope of the claimed subject matter. Furthermore the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure.

In the following description for purposes of explanation specific numbers materials and configurations are set forth in order to provide a thorough understanding of the present frameworks and methods and in order to meet statutory written description enablement and best mode requirements. However it will be apparent to one skilled in the art that the present frameworks and methods may be practiced without the specific exemplary details. In other instances well known features are omitted or simplified to clarify the description of the exemplary implementations of present frameworks and methods and to thereby better explain the present frameworks and methods. Furthermore for ease of understanding certain method steps are delineated as separate steps however these separately delineated steps should not be construed as necessarily order dependent in their performance.

The following description sets forth one or more implementations of systems and methods for facilitating transactional caching. One implementation of the present framework provides various enhancements to a transactional cache so as to support both read and write operations e.g. INSERT UPDATE etc. . Write operations may be delayed until a flush operation is necessary. The flush operation may be invoked by collecting newly inserted or updated records in the enhanced transactional cache and then writing them to the database using a bulk operation e.g. column wise .

Caching generally refers to the temporary storage of data to achieve higher performance in software systems. A transactional cache temporarily stores the results of a database transaction i.e. an atomic work unit of database access to reduce the number of database accesses. Conventional transactional caches typically achieve performance gain by reducing the number of accesses to the original record of data but do not reduce the number of write operations e.g. INSERT UPDATE . In one implementation of the present framework a bulk operation is combined with an enhanced transactional cache to reduce both read and write operations in a column based database thereby achieving greater performance improvement e.g. 20 in a typical transaction .

By minimizing the model gap between OLTP s row based operations and column oriented storage OLTP performance can be improved dramatically. The present framework advantageously increases the applicability of bulk operations for invoking a flush. All INSERT and UPDATE operations executed on the same table in a transaction may benefit from the bulk operation even if they are distributed in different locations. In addition the transactional cache is transparent to the OLTP application developer. This means that the performance of the OLTP application may be improved without changing the existing business logic design and implementation.

The framework described herein may be implemented as a method computer controlled apparatus a computer process a computing system or as an article of manufacture such as a computer usable medium. These and various other features will be apparent from the following description.

Computing system includes a central processing unit CPU an input output I O unit and a non transitory memory device . Other support circuits such as a cache power supply clock circuits and a communications bus may also be included in computing system . In addition any of the foregoing may be supplemented by or incorporated in application specific integrated circuits. Examples of computing system include a handheld device a mobile device a personal digital assistance PDA a workstation a server a portable laptop computer another portable device a mini computer a mainframe computer a storage system a dedicated digital appliance a device a component other equipment or some combination of these capable of responding to and executing instructions in a defined manner.

Memory device may be any form of non transitory computer readable media including but not limited to dynamic random access memory DRAM static random access memory SRAM Erasable Programmable Read Only Memory EPROM Electrically Erasable Programmable Read Only Memory EEPROM flash memory devices magnetic disks internal hard disks removable disks magneto optical disks Compact Disc Read Only Memory CD ROM any other volatile or non volatile memory or a combination thereof.

Memory device serves to store machine executable instructions data and various software components for implementing the techniques described herein all of which may be processed by CPU . As such the computer system is a general purpose computer system that becomes a specific purpose computer system when executing the machine executable instructions. Alternatively the various techniques described herein may be implemented as part of a software product which is executed via an application server . Each computer program may be implemented in a high level procedural or object oriented programming language e.g. C C Java Advanced Business Application Programming ABAP from SAP AG etc. or in assembly or machine language if desired. The language may be a compiled or interpreted language. The machine executable instructions are not intended to be limited to any particular programming language and implementation thereof. It will be appreciated that a variety of programming languages and coding thereof may be used to implement the teachings of the disclosure contained herein.

In one implementation the memory module of the computer system includes an application server and a data server . The application server may store an online transaction processing OLTP unit an Enhanced Transactional Cache ETC controller an ETC and an online analytical processing OLAP unit .

OLTP unit may include source code and executable machine code for performing the functions of a business application. OLTP unit may be designed to perform various transaction oriented functions such as customer relationship management CRM enterprise resource management ERP application human resource management enterprise content management ECM business process management BPM product lifecycle management and so forth. OLTP unit may be coded using a high level programming language such as Java C ABAP etc. Other types of programming languages are also useful. OLTP unit generates data referred to as transaction data that may be stored in the computing system . Transaction data includes for example information relating to the sale of goods e.g. location time price quantity etc. production of goods e.g. quantity supplier raw materials cost etc. telemarketing data customer support data and so forth.

Application server may include an OLAP unit including source code and executable machine code for performing analysis of data. OLAP unit may be designed to perform various business functions such as business reporting e.g. for sales marketing management financial etc. business process management BPM budgeting and forecasting and so forth. OLAP unit may be coded using a high level programming language such as Java C ABAP etc. Other types of programming languages are also useful.

In accordance with one implementation OLTP Unit is communicatively coupled to ETC controller . ETC controller manages ETC . During initialization ETC controller creates and configures ETC . More particularly ETC controller may provide configuration lifecycle management status management flush management query order management cache size management version control and or other services which will be described in more detail in the following description.

In one implementation ETC is a transaction level cache with a life cycle that is the same as a database transaction. In other words its contents will be cleared when the transaction ends e.g. commit roll back etc. . Usually ETC is a component of a persistence layer which is a group of software classes that make it easier for a program to persist its state. Since ETC is temporary consuming a large amount of memory space may be acceptable. However to avoid using up too much memory space e.g. when the query has a non unique key ETC controller may limit the number of records to be cached e.g. maximum of 100 .

Data server may include a database management system DBMS and a database . DBMS may include a set of programs to define administer and process the database . A user at the client computer may interact with a user interface UI to communicate with the database via the application server and the DBMS . UI may be a graphical user interface and include optional user interface components such as windows menus buttons check boxes charts icons etc.

In one implementation database is an in memory database that relies primarily on the system s main memory for efficient computer data storage. More particularly the data in the in memory database resides in volatile memory and is not persistently stored on a hard drive thereby allowing the data to be instantly accessed and scanned at a speed of several megabytes per millisecond. Some data of the in memory database such as the transaction log file may still be persistently stored for recovery purposes. The in memory database allows seamless access to and propagation of high volumes of data in real time. Parallel processing may further be achieved by using a multicore processor in conjunction with the in memory database . In memory database technology includes systems such as SAP s HANA high performance analytic appliance in memory computing engine.

In one implementation in memory database is optimized for both OLAP and OLTP. By consolidating OLAP and OLTP into a single database a lower total cost of ownership TCO may be achieved. Column based data storage may further be implemented in the in memory database where data tables are stored as columns of data in sequence and in compressed memory blocks. This may facilitate faster aggregation of data when calculations are performed on single columns. Alternatively row based data storage is also possible where a table is stored as a sequence of records each of which contains the fields of one row. In some implementations instead of updating entire rows only fields that have changed will be updated. This avoids having to lock entire data tables during updates to prevent conflicting modifications to a set of data. High levels of parallelization may be achieved which is critical to real time processing of live data streams and performing constant and substantially simultaneous updates.

It should be noted that the different components of the computer system may be located on different physical machines. More particularly components of the application server and the data server may be implemented on different physical machines or computer systems connected on the network . For instance the OLTP unit ETC controller and ETC may be implemented on one machine while the OLAP unit and data server may be implemented on another two different physical machines. It should further be appreciated that the different components of the client computer may also be located on the computer system .

In one implementation the OLTP unit communicates with the persistence layer . The persistence layer may include an ETC that serves as a proxy for database and delays execution of both read and write accesses until a flush is determined to be necessary. A Flush generally refers to the action of writing dirty records in the ETC to the database . To perform a flush a bulk operation interface may be implemented. A bulk operation generally refers to the execution of multiple database access operations as a batch or single unit of work . The bulk operation may be performed to write all newly inserted or old updated records in the ETC to the database . By using a bulk operation the number of round trips of data between the application server and the database is reduced and performance is advantageously enhanced.

If the database is column based as shown in the bulk operation may be column wise binding. A column wise binding bulk operation binds one array to each column for which data is returned from the ETC . For example a column wise binding bulk insert operation adds columns of data in the ETC into respective columns of a table in the column based database . Column wise binding is particularly useful in improving the performance of column oriented in memory databases.

The bulk operation interface may be implemented using Open Database Connectivity ODBC which is a standard C programming language middleware Application Programming Interface API for accessing the DBMS . Other types of interfaces may also be used. An exemplary Open Database Connectivity ODBC routine may be invoked with the following statements 

As shown by the status transition diagram there are six possible statuses for each record New Clean New Dirty New Delete Old Clean Old Dirty and Old Delete. The transition from one status to another may be initiated in response to an event indicated along each transition arrow . An event may triggered by receiving a database access operation e.g. Insert Delete etc. .

It should be appreciated that the status transition diagram is a simplified implementation of the status transition diagram . Since the Delete operation is rarely used in a business application the New Delete and Old Delete statuses and may be removed. A Write through DB policy may be defined for the Delete operation. The Write through DB policy indicates that the entry is deleted directly from the database in response to receiving a Delete instruction. In addition the corresponding entry may also be removed from the ETC . The New Clean and New Dirty statuses and may be combined into a New status . A Write Delay policy may be implemented for Insert and Update operations. The Write Delay policy indicates that the entry is temporarily stored in the ETC and its status set to New. 

At a request to execute one or more database operations is received. The ETC controller handles each operation and takes actions accordingly to the various sub routines shown.

At a Query by Key operation is received. The key used in such query may be a primary key or partial key. At when a Query by Key operation is executed the ETC controller looks up the record in the ETC . If the record is not found in the ETC at it is loaded from the database . At the record is added into the ETC and its status is set to Clean. If the record is found in the ETC the cache record is returned to the user or business application or OLTP unit at .

At a Delete by Key statement is received. As mentioned previously a Write through DB policy may be defined for the Delete operation. At the record corresponding to the key is deleted directly from the database . At the corresponding entry in the ETC may also be removed.

At an Insert statement is received. At the new record is added to the ETC and its status is marked as New. 

At an Update by Key statement is received. At the ETC controller looks up the record corresponding to the key in the ETC . If the corresponding record is not found at the ETC controller adds the record into the ETC . If the corresponding record is found at the record in the ETC is updated. At if the previous status is Clean or Dirty the record in the ETC is marked as Dirty to indicate that it needs to be used to update an existing record in the database. If the previous status is New the status remains unchanged.

At a General Query Stored Procedure Batch Update not by key or a Batch Delete not by key is received. At the ETC controller checks to see if a Flush is required. To make sure that the flush is effective and the flush order is correct the ETC controller may implement a policy to determine the necessity of a flush.

For instance a Flush may be invoked before some general query is executed because some query results may be affected by any newly inserted or old dirty records in the ETC . The ETC controller may check to see whether a flush is necessary before the general query is executed. A flush may be necessary if the general query is related to a view. A view generally refers to a query accessible as a virtual table in the database. In addition a flush may also be necessary if there is any newly inserted or old dirty record in the ETC that is associated with a table that is related to the general query e.g. SQL table JOIN sub query etc. . It should be noted that these conditions may also be checked before a Stored Procedure Batch Update not by key or a Batch Delete not by key is executed.

If a Flush is determined to be necessary at the ETC controller collects all newly inserted and old dirty records in the ETC according to for example table name. The ETC controller then inserts or updates the database with the collected records so as to synchronize the database with the ETC . This may be achieved by a bulk operation e.g. column wise binding bulk operation . The newly inserted and old dirty records in the ETC are then marked as clean. If a Flush is determined to be unnecessary the process continues at step where the General Query Stored Procedure Batch Update or Batch Delete is executed on the database . At in the case of a Stored Procedure the entire ETC is cleared. In the case of a Batch Update or a Batch Delete the associated table in ETC is cleared.

At a COMMIT is initiated to complete the transaction and retain the changes in the database . Committing the transaction causes the ETC to be flushed at . The ETC is then cleared at and the transaction is successfully committed at .

At a ROLLBACK is initiated to return the database to the state it was in before the transaction began. Rolling back the transaction causes the ETC to be cleared at . The transaction is then successfully rolled back at .

As discussed previously the ETC controller may execute a bulk operation to flush the ETC . A flush may occur in the following cases 1 When a transaction is committing 2 Before some general query is executed 3 Before some batch update is executed 4 Before some batch delete is executed 5 Before all stored procedures are executed and 6 When developer requests to flush by calling Flush explicitly.

Since a flush delays the execution of Insert and Update operations the execution order of query statements may also change. The modification of the execution order may cause some issues with order sensitive query statements and raise a constraint violation or expression evaluation error .

A constraint may refer to a rule that restricts values in a database. For instance an SQL database server may allow five types of constraints to be defined 1 NOT NULL constraint 2 unique index constraint 3 primary key constraint 4 reference integrity or foreign key constraint and 5 check constraint. Other types of constraints may also be defined. A NOT NULL constraint prohibits a database value from being null. A unique index constraint prohibits multiple rows from having the same value in the same column or combination of columns but allows some values to be null. A primary key constraint combines a NOT NULL constraint and a unique constraint in a single declaration. A reference integrity constraint requires values in one table to match values in another table. A check constraint requires a value in the database to comply with a specified condition.

Examples of order sensitive query statements that may raise constraint violations include Update statements that change either the primary key unique index or foreign key Insert and Delete statements. Modifying the execution order of these statements may cause the database server to throw an exception.

For instance if the execution order of Update and Insert statements is modified three types of scenarios may occur. In the first scenario the column of a unique index may be updated as follows 

If statement S1 is executed before statement S2 the statements will be executed successfully. However if S1 is executed after S2 S2 will raise an exception due to a unique index constraint violation.

If statement S1 is executed before statement S2 the statements will be executed successfully. However if S1 is executed after S2 S2 will raise an exception due to a foreign key constraint violation.

If statement S1 is executed after statement S2 the statements will be executed successfully. However if S1 is executed before S2 S1 will raise an exception due to a foreign key constraint violation.

In another example modifying the execution order of different Update operations may also raise constraint violations. For instance the column of a unique index may be updated as follows 

If statement S1 is executed after statement S2 the statements will be executed successfully. However if S1 is executed before S2 S1 will raise an exception due to a unique index constraint violation.

If statement S1 is executed after statement S2 the statements will be executed successfully. However if S1 is executed before S2 S1 will raise an exception due to a foreign key constraint violation. For Update and Delete operations similar issues may occur if the column of unique index or foreign key is updated.

To address such possible constraint violations the ETC controller may apply a Write through DB policy for order sensitive Delete statements. The Write through DB policy directly removes the record from the database . More particularly the Write through DB policy executes database access operations directly on the database instead of first accessing the cache e.g. marking record as delete and delaying access operations until a flush is invoked. For order sensitive Update and Insert statements the ETC controller may maintain the original execution order of the statements when they first entered the ETC . As for other issues caused by incorrect flush order the user may explicitly call Flush to ensure correct business logic. Such cases are typically very rare in most business applications.

The ETC controller may also perform other functions such as data version management. One data version management strategy is to use backup buffers for version check control. Backup buffers serve to save a copy of the original contents of the data associated with the database . Since the contents of the backup buffers are loaded from database they may be used to check which fields are modified by comparing the data and backup buffers. Backup buffers may also be used to check whether the record is updated by other clients when a flush happens. If the backup buffers contents are different from the database records in the data buffers it means that a newer version has been committed after such records are cached. The current transaction should be a rollback and a concurrent exception should be thrown.

More particularly shows an exemplary method of version check control during an update of only normal fields. A normal field generally refers to a column of a relational table that is not a primary key. In this example the first column of the tables stored in the first and second data buffers and and first and second backup buffers and holds the primary key which identifies a record in the tables.

At Step 1 a record e.g. row in a table or other data element may be loaded from the database to the ETC . When this happens the first data buffer may store the database record while the contents of the first data buffer are copied to the first backup buffer . The status of the record is set to Clean.

At Step 2 a GetByKey statement may be called by the user to Query by Key or retrieve the record whose key field matches a specified key expression e.g. . Since the status of the record is Clean the ETC record may be returned to the OLTP unit and stored in the second data buffer . The second backup buffer may also store a copy of the record.

At Step 3 a Dag SetColStr statement may be called by the user to change a string value in a normal field of the table contained in the second data buffer. In such case the second backup buffer may contain a different or old copy of the record.

At Step 4 a Dag Update statement may be called by the user to update the ETC or database . In response to this statement the ETC controller may invoke an UpdateInCache statement. If the record is found in the ETC the record in the second data buffer is copied to the first data buffer . The status of the record is then set to Dirty. If the Checkbackup flag is true the second backup buffer is then compared with the first data buffer . If the contents of the second backup buffer and the first data buffer are different an error is reported.

At Step 5 a GetbyKey statement may be called by the user to retrieve the record again. In response the record in the first data buffer is returned and stored in the second data buffer . A copy of the record is copied from the second data buffer to the second backup buffer . At Step 6 a DAG SetColLong statement may be called by the user to change a long integer value in a normal field of the table contained in the second data buffer . The contents in the second backup buffer may then be different from the contents in the second data buffer .

At Step 7 a Dag Update statement may be called again by the user to update the ETC or database . In response to this statement the ETC controller may invoke an UpdateInCache statement. If the record is found in the ETC the record in the second data buffer is copied to the first data buffer . The status of the record remains as Dirty. If the Checkbackup flag is true the second backup buffer is then compared with the first data buffer . If the contents of the second backup buffer and the first data buffer are different an error is reported.

At Step 8 a Flush to database statement is invoked. The first backup buffer is first checked to determine if it contains the same dirty record as the first data buffer . If it does not the contents of the first data buffer are copied to the first backup buffer . The dirty record in the first data buffer is flushed to the database and the status of the record is set to Clean.

At Step 1 a record e.g. row in a table or other data element may be loaded from the database to the ETC . When this happens the first data buffer may store the record while the contents of the first backup buffer may be copied from the first data buffer to the first backup buffer . The status of the record is set to Clean.

At Step 2 a GetByKey statement may be called by the user to Query by Key or retrieve the record whose key field matches a specified key expression e.g. . Since the status of the record is Clean the record may be returned to the OLTP unit and stored in the second data buffer . The second backup buffer may also contain a copy of the record.

At Step 3 a Dag SetColStr statement may be called by the user to change a string value in a normal field of the table contained in the second data buffer . In such case the second backup buffer may contain a different or an old copy of the record.

At Step 4 a Dag Update statement may be called by the user to update the ETC or database . In response to this statement the ETC controller may invoke an UpdateInCache statement. If the record is found in the ETC the record in the second data buffer is copied to the first data buffer . The status of the record is then set to Dirty. If the Checkbackup flag is true the second backup buffer is then compared with the first data buffer . If the contents of the second backup buffer and the first data buffer are different an error is reported.

At Step 5 the second backup buffer may also be updated with the contents of the second data buffer in response to the Dag Update statement.

At Step 6 a DAG SetColLong statement may be called by the user to change a long integer value in a key field e.g. from 001 to 002 and a normal field e.g. from 10 to 5 of the table contained in the second data buffer . The contents in the second backup buffer may then be different from the contents in the second data buffer .

At Step 7 a Dag Update statement may be called again by the user to update the ETC or database . In response the ETC controller may invoke an UpdateInCache statement. When the ETC controller finds that the primary key field has been updated it compares the second backup buffer with the contents of the first data buffer to check if they are the same. If they are the same it means that the original version of the record in the OLTP unit is the current version and it is therefore safe to override the cached record in the ETC with the record in the OLTP unit . Accordingly the first data buffer in the ETC will be updated by the second data buffer on the OLTP unit side and this ETC record may be used to execute update statements when flush is invoked.

At Step 8 not shown the ETC controller may execute the update query statement for the new record. To improve performance the query statement may be executed with a checking condition e.g. where PK 001 and Col2 a and Col3 10 .

At Step 9 the first data buffer is updated with the new record and its status is set to Clean. The contents of the first data buffer are then copied to the first backup buffer .

Although the one or more above described implementations have been described in language specific to structural features and or methodological steps it is to be understood that other implementations may be practiced without the specific features or steps described. Rather the specific features and steps are disclosed as preferred forms of one or more implementations.

