---

title: Connection redistribution in load-balanced systems
abstract: Methods and apparatus for connection redistribution in load-balanced systems that include multiple load balancers each serving multiple nodes. In the connection redistribution method, each node estimates a connection close rate, which may be based on an estimation of the percentage of the overall client traffic received by the respective load balancer that is being handled by the node. The node generates close requests for connections between the respective load balancer and clients according to the connection close rate. The node sends the close requests to its load balancer, which forwards the close requests to the appropriate clients. Upon receiving a close request, a client may close the connection(s) indicated by the request, obtain a public IP address for a load balancer, and initiate new connection(s) to the respective load balancer via the public IP address.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09432305&OS=09432305&RS=09432305
owner: Amazon Technologies, Inc.
number: 09432305
owner_city: Reno
owner_country: US
publication_date: 20130626
---
The advent of virtualization technologies for commodity hardware has provided benefits with respect to managing large scale computing resources for many clients with diverse needs allowing various computing resources to be efficiently and securely shared by multiple clients. For example virtualization technologies may allow a single physical computing machine to be shared among multiple users by providing each user with one or more virtual machines hosted by the single physical computing machine with each such virtual machine being a software simulation acting as a distinct logical computing system that provides users with the illusion that they are the sole operators and administrators of a given hardware computing resource while also providing application isolation and security among the various virtual machines. As another example virtualization technologies may allow data storage hardware to be shared among multiple users by providing each user with a virtualized data store e.g. a virtualized database which may be distributed across multiple data storage devices or storage nodes with each such virtualized data store acting as a distinct logical data store that provides users with the illusion that they are the sole operators and administrators of the data storage resource.

As the scale and scope of network based applications and network based services such as virtualized or cloud computing services have increased network based applications or services may include hundreds or even thousands of hardware or software nodes to which client traffic from many clients of the service or application may need to be load balanced. In an example data center or network implementing a large scale service or application multiple load balancers in a load balancer layer may front a fleet of nodes with each load balancer fronting a subset of the nodes. Client connections to the service or application may be pseudorandomly distributed among the load balancers in the load balancer layer for example by a Domain Name Server DNS accessible by the clients that provides the IP addresses of the load balancers to the clients upon request. Connections from clients e.g. Transmission Control Protocol TCP connections may thus be established to particular load balancers the load balancer to which particular client connections are established may distribute client traffic on those connections among its respective nodes according to a load balancing technique.

Load balancers are typically single dedicated devices that include multiple network interface controllers NICs for example eight NICs with some of the NICs handling inbound traffic from outbound traffic to clients and the other NICs handling outbound traffic from inbound traffic to the nodes that are being load balanced. Bandwidth or throughput on conventional load balancers is typically in the range of 40 Gigabits per second Gbps on the client side and 40 Gbps on the server side. Load balancers typically use techniques such as max connections or max conns round robin and or least connections least conns applied to data collected from the host devices to select which node will handle a connection. In addition load balancers typically serve as proxies to the nodes that they front and thus terminate connections from the clients and send the client traffic to the nodes on connections established between the nodes and the load balancer. Thus a node and a client typically do not communicate over a direct connection when using load balancers.

While embodiments are described herein by way of example for several embodiments and illustrative drawings those skilled in the art will recognize that embodiments are not limited to the embodiments or drawings described. It should be understood that the drawings and detailed description thereto are not intended to limit embodiments to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope as defined by the appended claims. The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description or the claims. As used throughout this application the word may is used in a permissive sense i.e. meaning having the potential to rather than the mandatory sense i.e. meaning must . Similarly the words include including and includes mean including but not limited to.

Various embodiments of methods and apparatus for connection redistribution in load balanced systems that include multiple load balancers each fronting multiple nodes are described. illustrates an example network based service environment in which embodiments of a connection redistribution method may be implemented. A network based service also referred to as a web based service or simply web service may be implemented in a service network . Examples of network based services may include but are not limited to data store services database services and computation services. The service network may implement at least a load balancer layer and a production layer . Note that the service network may be implemented in a single data center or across two or more data centers. The data centers may be geographically dispersed.

The load balancer layer may include multiple load balancers . The load balancers may for example be commercially available devices provided by a vendor. As an example in some implementations the load balancers may be NetScaler load balancers provided by Citrix Systems Inc. However other load balancers may be used in various implementations. Each load balancer may be provided with one or more unique public IP address of the network based service each public IP address associated with a particular port e.g. TCP port of the load balancer . The public IP addresses of the load balancers may be published for example to a Domain Name Server DNS or to some other type of endpoint discovery service or method.

The production layer may include multiple nodes . Each node may for example be a device such as a server device. For example in some implementations the nodes may be rack mounted computing devices such as blade servers installed in one or more racks within a data center or across two or more data centers. Alternatively nodes may be implemented in software e.g. as different virtual machines processes threads etc. with one or more instances of a software node implemented on each of one or more devices. In many implementations there may be hundreds or thousands of nodes on service network within a data center or across two or more data centers. Each node may include software and or hardware that implements functionality of the network based service.

The nodes may be arranged in node groups with each node group fronted by one of the load balancers in load balancer layer . Each node group may include tens or hundreds of nodes . For example in load balancer A fronts nodes A in node group A load balancer B fronts nodes B in node group B and so on. The nodes in a node group may each be assigned a local or private address on service network that is known to the respective load balancer so that the load balancer may connect to and communicate with the nodes in the node group . For example in at least some implementations the nodes in a node group may each be assigned a unique subnet address of a public IP address assigned to the respective load balancer . In at least some implementations the nodes within a node group may communicate node group information e.g. presence information health information etc. among the nodes in the group for example according to a gossip protocol. In some embodiments the nodes within a node group may also communicate with nodes in other node groups .

While shows each load balancer fronting a single node group in some implementations at least one load balancer may front two or more node groups with each node group corresponding to a different public IP address of the respective load balancer .

Clients on an external network may access the network based service implemented on service network . In at least some embodiments to access the service a given client may resolve a domain name of the service e.g. a URL for example via DNS . The client sends the domain name of the service to the DNS which selects one of the public IP addresses of the load balancers and returns the selected IP address to the client . The DNS may implement a randomizing technique to select from among the public IP address of the load balancers so that the IP addresses are pseudorandomly provided to the clients . While shows DNS on external network DNS may be located on service network . In at least some implementations the DNS technology may provide weighting for the different public IP addresses so that different ones of the addresses can be provided to requesting clients at different rates. For example an address with a weight of 0.2 may be provided to clients approximately twice as often as an address with a weight of 0.1. In these implementations the weight of a particular address may be set to zero 0.0 so that the address is not provided to the clients .

Note that other methods and or other types of endpoint discovery services or technologies than DNS technology may be used for clients to obtain public IP addresses of the load balancers in the network based service.

Once the client has obtained a public IP address of a load balancer the client may initiate a connection with the load balancer via the public IP address according to a network communications protocol or network protocol. In at least some implementations the network protocol may be Transmission Control Protocol TCP although other network protocols may be used.

Once a connection e.g. a TCP connection has been established between the client and a load balancer according to the network protocol the client may send one or more requests e.g. Hypertext Transport Protocol HTTP requests to the service via the connection. The load balancer may select node s from among the nodes in a respective node group to receive the requests from the client according to a load balancing technique e.g. max connections or max conns round robin least connections least conns etc. . A node that receives a request from a client via a load balancer may generate a response which is sent by the node to the load balancer which then forwards the response to the respective client typically via the connection on which the respective request was received from the client .

The load balancers may serve as proxies to the nodes that they front and thus may terminate the connections from the clients and send the client traffic to the nodes over a network substrate or fabric between the load balancers and the nodes . In some implementations the requests from a client may be encapsulated according to an IP tunneling protocol and routed over the network infrastructure to the target nodes . However the client traffic may be sent from the load balancers to the nodes via other methods for example as Hypertext Transport Protocol HTTP traffic via TCP connections between the load balancers and the nodes . Note that a load balancer may distribute requests from a given client connection to two or more of the nodes in its node group . Thus a node and a client typically do not communicate over a direct connection and there is typically no affinity between a given client and a given node of the service.

There may be many clients for example tens hundreds or thousands of clients that access the network based service on service network via connections to the load balancers in the load balancer layer . Each client may initiate one two or more connections with the load balancers as described above. Note that at least some clients may locally cache the public IP addresses of the load balancers obtained from the DNS or other discovery service for a period typically for minutes or hours thus additional connections or reconnections may in some cases be made to the same load balancer by a given client .

There may be a wide range of usage on the connections between the load balancers and the clients . At least some connections from some clients may be heavily used with lots of traffic e.g. HTTP request messages being generated by the respective clients . Other connections may be less used or lightly used. Also when an application on a given client starts up one two or more connections with a particular load balancer may be established. Conventionally a client may hold its connection s with the load balancer for an extended period which may be days weeks or even months for example until the application is restarted. Thus in conventional load balanced systems an application on a client may establish and hold connection s with a particular load balancer for an extended period and may generate a lot of traffic on the connection s . These usage characteristics of such load balanced systems may result in situations where particular ones of the load balancers in the load balancer layer are heavily loaded with many busy persistent connections to one or more clients and thus receive many requests to distribute among the respective nodes while others of the load balancers are less used or even underutilized even though the DNS or other discovery service may pseudorandomly distribute the public IP addresses of the load balancers in the load balancer layer to the clients .

Also occasionally the service provider may need to take a load balancer or the nodes in the node group fronted by a load balancer offline for example for maintenance or upgrade. The rest of the fleet in the service network can take over the load when the load balancer and its group are offline. However conventionally there has not been a graceful method for taking a load balancer and its node group out of service.

Embodiments of a connection redistribution method are described in which each of the nodes in each node group may periodically initiate connection closes of one or more connections between their respective load balancers and the clients . In at least some embodiments of the connection redistribution method each node may calculate or estimate a connection close rate according to one or more metrics and then select random connections to which connection close requests are to be sent according to the connection close rate. In at least some embodiments the connection close rate at each node may be based on an estimation of the percentage of the overall client traffic at the respective load balancer that the node is handling. In at least some embodiments the connection close rate at each node may provide an indication of a number of connections that the node should issue connection close requests for in an hour although other time periods may be used. In at least some embodiments each node may recalculate its connection close rate at a specified interval e.g. once per hour and or upon receiving updated information. Methods for calculating or estimating a connection close rate are described later in this document.

In at least some embodiments to select the connections to be closed each node may select random request messages e.g. HTTP request messages from among those received at the node from its respective load balancer according to the connection close rate. In at least some embodiments to request that a connection be closed a node modifies the header of a response message e.g. an HTTP response message to a received request message from a client to include a connection close request. The response message is sent to the respective load balancer which then forwards the response message to the respective client over a connection between the client and the load balancer .

Note that some load balancers may in some modes e.g. client keep alive CKA mode mangle headers on response packets before forwarding the packets to the clients . Thus in some implementations settings of the load balancers may be selected so that the connection close requests are passed through to the clients in the packet headers.

In response to the message including the connection close request the client may close the connection obtain the public IP address of one of the load balancers from the DNS or other discovery service and open a new connection. Since the public IP addresses of the load balancers in the load balancer layer are pseudorandomly distributed to the clients generally the public IP address of the load balancer returned to the client will be a different load balancer than the one for which the connection was closed.

As previously noted at least some clients may locally cache the public IP addresses of the load balancers obtained from the DNS or other discovery service for a period. When attempting to establish a new connection to a load balancer after closing a connection a client may use a public IP address from its local cache. Thus in some cases if a client has not recently flushed or refreshed its local cache of public IP addresses the client may attempt to open a connection to the same load balancer for which a connection was closed in response to a connection close request.

The connection redistribution method causes the clients to redistribute their connections among the load balancers rather than holding connections for extended periods which tends to spread the client connections and client traffic from all clients and from particular clients among the load balancers in the load balancer layer thus helping to avoid situations where particular ones of the load balancers in the load balancer layer are heavily loaded with many busy persistent connections to particular clients .

In addition in at least some embodiments the connection close rate at each node that is used in the connection redistribution method may be selected so that all client connections at each load balancer and thus for the service as a whole tend to be recycled at a given period e.g. an hour . In at least some embodiments to take a load balancer or the nodes in a node group s fronted by a load balancer offline new connections to one or more of its public IP addresses of the load balancer may be prevented by directing the DNS or other discovery service to stop resolving connection requests to the public IP address es of the load balancer for example by setting the weight s of the public IP address es to zero. Within the given period or a bit longer e.g. within an hour or two the nodes in the node group s corresponding to the public IP address es may issue connection close requests for all of the active client connections to the public IP address es . The clients may re establish the connections with other public IP address es in the load balancer layer via the DNS or other discovery service. Thus a load balancer or a node group fronted by a load balancer may be easily and gracefully taken out of service within a relatively short period.

Some embodiments may provide a method or methods via which a load balancer or node group may be taken offline more quickly than by the above method for example within 10 minutes rather than within an hour. For example instead of waiting for the nodes in a node group to close the connections according to the current connection close rates as calculated at the nodes the nodes may be manually directed or programmatically directed to issue connection close requests at an accelerated rate. Manually directing the nodes may for example involve a human operator at a console issuing command s to the nodes via a user interface to a control program. Programmatically directing the nodes may for example involve the nodes or some other process on the network monitoring the weights of the public IP addresses at the DNS if the weight of a public IP address that corresponds to a node group goes to zero indicating that new connections are not being made to the public IP address the nodes in that node group may automatically begin to issue close connection requests at an accelerated rate.

For example if at the current connection close rate a node includes a connection close request in a response once every 12 seconds based on a time period of one hour the node may be manually or programmatically directed to instead include a connection close request in a response once every 2 seconds based on a time period of 10 minutes. Thus in this example instead of waiting one hour or more for all the connections to a public IP address to be closed the connections may be closed within 10 minutes or a bit more.

As an alternative in some embodiments the nodes may be manually or programmatically directed to include a connection close request in the response to each Nth request received from the load balancer. For example N can be set to 1 to cause the nodes to include a connection close request in the response to each request received from the load balancer or to 10 to cause the nodes to include a connection close request in the response to each 10request received from the load balancer .

In some embodiments the nodes may be manually or programmatically directed to throttle down the rate at which a connection close request is included in responses from an initial rate e.g. the current connection close rate over a period e.g. 10 minutes so that connection close requests are issued at an accelerating rate over the period until at the end a connection close request is issued in a response to every request received from the clients via the load balancer .

As indicated at upon obtaining public IP addresses of the load balancers from the DNS the clients initiate connections to the load balancers using the obtained addresses according to a network protocol for example TCP. Once a client establishes a connection e.g. a TCP connection to a public IP address of a load balancer the client may begin sending requests e.g. HTTP requests to the system via the load balancer. The connections may be persistent that is a client may hold a given connection until the client closes the connection.

As indicated at the load balancers each distribute the client requests received via the client connections to the load balancer to the nodes in the node group s fronted by the load balancer. The load balancers may serve as proxies to the nodes that they front and thus may terminate the connections from the clients and send the client traffic to the nodes over a network substrate or fabric between the load balancers and the nodes. A load balancer may distribute requests received on a given client connection to two or more of the nodes in a respective node group. Thus a node and a client typically do not communicate over a direct connection and there is typically no affinity between a given client and a given node.

As indicated at the nodes each generate connection close requests for connections according to a connection close rate. In at least some embodiments each node may generate connection close requests according to a connection close rate. In at least some embodiments each node calculates or estimates a connection close rate for the node and then selects random connections to which connection close requests are to be sent according to the connection close rate. In at least some embodiments the connection close rate at each node may be based on an estimation of the percentage of the overall client traffic at the respective load balancer that the node is handling. In at least some embodiments the connection close rate at each node may provide an indication of a number of connections that the node should issue connection close requests for in a specified time period for example per hour. Methods for calculating or estimating a connection close rate are described later in this document. In at least some embodiments to select the connections to be closed each node may select random request messages e.g. HTTP request messages from among those received at the node from its respective load balancer according to the connection close rate.

In at least some embodiments to request that a connection be closed a node modifies the header of a response message e.g. an HTTP response message to a received request message from a client to include a connection close request. The response message is then sent to the load balancer which forwards the response message with the header including the connection close request to the respective client via the connection. In response to the connection close request the client may close the connection and attempt to open a new connection to a public IP address of a load balancer. The client may obtain the public IP address from an endpoint discovery service such as DNS or in some cases may obtain the public IP address from its local cache.

Alternatively some network communications protocols may allow a client to establish a separate control connection to a load balancer via which control or other packets may be exchanged with the load balancer. In these implementations a connection close request indicating one or more connections to be closed may be generated by a node sent to the load balancer and forwarded to the client via the control connection. The client may then close the connection s indicated by the connection close requests.

As shown by the arrow returning from element to element in the connection redistribution method causes the clients to periodically recycle connections to the load balancers and thus tends to redistribute the client connections among the load balancers. Thus rather than holding connections for extended periods and thus potentially overloading particular load balancers the client connections and client traffic from all clients and from particular clients tend to be more evenly spread among the load balancers. In at least some embodiments the connection close rate at each node may be selected so that all client connections at each load balancer and thus for the system as a whole tend to be recycled at a given period e.g. an hour . This allows a given load balancer to be gracefully taken offline by stopping new connections to the load balancer and allowing the active connections to the load balancer to be closed by the respective nodes according to the connection redistribution method. As previously mentioned in some embodiments instead of waiting for the nodes in a node group to close the connections according to the current connection close rates as calculated at the nodes the nodes may be manually or programmatically instructed to issue connection close requests at an accelerated rate.

In some implementations each node in the load balanced system may be a computing device or storage device. Each node may for example be a server device. For example in some implementations the nodes may be rack mounted computing devices such as blade servers installed in one or more racks within a data center or across two or more data centers. Alternatively nodes may be implemented in software with one or more instances of node implemented on each of one or more devices. Each node may include several components or modules implemented in hardware and or software on the node . In at least some implementations on each node one or more components implement functionality of the load balanced system and one or more components implement node group functionality. In addition each node may include a component or module that implements an embodiment of a connection close method as described herein for example as shown in . As an example shows node C that includes service module s that implement functionality of a network based service on the node C node group module s that implement node group functionality on the node C and a connection close module that implements the connection close method on the node C. Note that each node in each node group may be similarly configured.

Clients may obtain the public IP addresses of the load balancers from an endpoint discovery service for example from a Domain Name Server DNS . In at least some embodiments when a client wants to establish a connection to the load balanced system the client resolves a domain name of the system e.g. a URL via the endpoint discovery service to obtain a public IP address of one of the load balancers for example a public IP address of load balancer A. The endpoint discovery service may implement a randomizing technique to select from among the public IP address of the load balancers so that the IP addresses are pseudorandomly provided to the clients . Note that in some embodiments other methods may be used for clients to obtain public IP addresses of the load balancers .

Upon obtaining a public IP address of load balancer A for example from the endpoint discovery service a client may initiate one or more connections to the load balancer A according to a network protocol for example TCP. Once the client establishes a connection e.g. a TCP connection to load balancer A via the public IP address the client may begin sending requests e.g. HTTP requests to the system via the load balancer A. The connections may be persistent that is the client may hold a given connection until the client closes the connection.

Load balancer A distributes the client requests e.g. HTTP requests received via the client connections to the public IP address among the nodes A in node group A. The load balancer A may serve as a proxy to the nodes in node group A and thus may terminate the connections from the clients . Load balancer A distributes the client requests received via the connections among the nodes according to a load balancing technique and sends the client requests to the nodes over connections between the load balancer A and the nodes .

Thus node C receives a portion of the client requests e.g. HTTP requests that are received by load balancer A at the public IP address. Service module s on node A may handle each received request to perform one or more functions of the service or application implemented by the load balanced system according to the respective request. For example in a database service a request may be a database query request and the service module s may process the database query according to the request. Node C may generate a response message or messages e.g. HTTP response message s for at least some of the received and processed requests. The response messages are sent by the node C to the load balancer A which then routes the response messages to the appropriate client via the respective connection.

Node group module s on node C may communicate with other nodes in the node group A to exchange or propagate node group information such as presence information and health information. In at least some embodiments the nodes may propagate node group information among the node group A according to a gossip protocol. In some embodiments nodes A in node group A may also communicate with nodes in other node groups B via node group modules to obtain information about the other node groups.

Connection close module on node C may calculate or estimate a connection close rate for node C according to one or more metrics obtained by the module for example one or more metrics related to the request rate at node C and one or more metrics obtained from the node group information collected by node group module s . In at least some embodiments the connection close rate for node C may be based on an estimation of the percentage of the overall client traffic received by load balancer A that the node C is handling. In at least some embodiments the connection close rate at node C may provide an indication of a number of connections that node C should issue connection close requests for in a specified time period for example X connection close requests per hour.

After calculating the connection close rate connection close module may then generate connection close requests for selected connections according to the connection close rate. In at least some embodiments to select the connections to be closed connection close module may select random request messages e.g. HTTP request messages from among those received at the node C from load balancer A according to the connection close rate. In at least some embodiments to request that a connection be closed connection close module modifies the header of a response message e.g. an HTTP response message to a received request message from a client to include a connection close request.

Upon receiving a response message that includes a connection close request from node C load balancer A forwards the response message with the connection close request to the appropriate client via a connection between load balancer A and the client . In some implementations settings of the load balancer A may be selected so that the connection close request in the header is passed through to the client . Upon receiving the response message the client may close the connection contact the endpoint discovery service to obtain a new public IP address for a load balancer and then initiate a new connection to the respective load balancer via the public IP address.

In some embodiments the nodes in a node group may communicate with the nodes in its node group and with the nodes in other node groups to obtain information about all the nodes and node groups in the system. The node may use this information to determine how many total nodes and node groups are in the system and may use the average number of nodes in the node groups as an estimate. For example if there are 10 node groups and 1000 nodes in the system then the average number of nodes in a node group is 100.

In some embodiments each node may collect information from and share information with the other nodes in its node group for example via a gossip protocol. For example each node may share its health and presence information with other nodes in the node group and may collect health and presence information from other nodes in the node group. The node may use this information to determine how many healthy nodes are in its node group.

In some embodiments a central or authoritative service may maintain membership information for each node group or for all the node groups in the system and each node may obtain information about its node group e.g. how many healthy nodes are in the node group from the service.

As indicated at of the node calculates a connection close rate according to the collected information and the request rate at the node. In at least some embodiments the connection close rate for the node may be based on an estimation of the percentage of the overall client traffic received by the respective load balancer that the node is handling. The connection close rate for a node may provide an indication of a number of connections that the given node should request to be closed in a specified time period for example one hour. As indicated at the node generates close requests for connections according to the connection close rate. As shown by the arrow returning from to the node may periodically or aperiodically recalculate the connection close rate for example once per hour.

As previously mentioned the connection close rate for a node provides an indication of a number of connections that the respective node should request to be closed in a specified time period. In at least some embodiments each node in a node group may calculate or estimate a connection close rate for the respective node. In at least some embodiments the connection close rate at each node in a node group may be determined according to a function of the number of healthy nodes in the node group the number of requests per second handled at that node and some configured or specified time period for example one hour. The following describes some example methods for calculating or estimating the connection close rate that may be used in some embodiments. Note however that other methods for calculating or estimating the connection close rate may be used in some embodiments.

The following is an example of a method for calculating a connection close rate at a node that may be used in some embodiments and is not intended to be limiting. The node may track requests received at the node and estimate how many requests are received in a second at the node. The node may also obtain or estimate a total number of healthy nodes in the node group for example according to one of the methods described above in relation to . The number of requests received in a second at the node times the total number of healthy nodes in the node group gives an estimated request rate per second for the node group. The estimated request rate per second for the node group may be used as the total number of connections to be closed by the node group in an hour. The node calculates an estimate of the percentage of the overall traffic that it is serving. For example if there are 100 nodes in the node group the node may be serving 1 100 1 of the traffic. The number of connections that the node is to close in an hour is then this percentage of the total number of connections to be closed in an hour. For example if 100 000 connections are to be closed in an hour and the node is serving 1 of the traffic then the node may request that 1000 connections be closed in an hour. Since there are 3600 seconds in an hour this would be 3600 1000 1 connection close request every 3.6 seconds issued by the node the connection close rate for the node . The following formalizes this example method for calculating a connection close rate at a node 

In some embodiments the above formula may be modified by weighting the total number of connections to be closed in an hour with a weight W for example as 

Note that different ones of the nodes in a node group may determine different values for R in the above equations and thus the different nodes may calculate different connection close rates. Less busy nodes may tend to determine lower values for R while busier nodes may tend to determine higher values for R. Because of this the busier nodes may tend to generate connection close requests more frequently than the less busy nodes.

The following is an example of another method for calculating a connection close rate at a node that may be used in some embodiments and is not intended to be limiting. In this method a number of client connections per second at the load balancer may be used to determine how many connections are to be closed in a specified time period e.g. one hour . This value may be a specified value that is input to or acquired by the nodes and may be the same for all nodes in a node group but may be different for different node groups . A weight W may also be used the weight may be applied to the number of client connections per second at the load balancer 

This yields one connection to be closed every Y WX hours. The request rate at the node is taken into account 

In Y WX hours the node will serve approximately R Y WX 3600 requests. The node thus issues a connection close request every R Y WX 3600 requests the connection close rate for the node . As an example given X 300 Y 218 W 2 and R 200 at a node the connection close rate for the node is 23600 200 218 600 3600 261600

In other words the node may issue a connection close request approximately every 261600 requests. Since the request rates for the nodes are used in the calculations at the nodes clients making a higher number of requests to the load balancer will tend to have their connections recycled more frequently and thus move to a different load balancer for subsequent requests.

The above describes some example methods for calculating the connection close rate at each node in a node group as some function of the number of healthy nodes in the node group the number of requests per second handled at that node and a specified time period. The connection close rate calculated at each node using these methods may be an estimate of the number of connection close requests that the node should issue in the specified time period. As previously noted different ones of the nodes in a node group may determine different values for R request rate per second at the node in the above methods and thus the different nodes may calculate different connection close rates. However the connection close rates calculated at all nodes may collectively tend to achieve an overall connection close rate for the node group via which all connections at the respective load balancer may be closed within some range e.g. some number of minutes or seconds of the specified time period.

In some embodiments a method for calculating the connection close rate at each node in a node group such as the examples given above may introduce randomness to the calculations by using a random factor Z in the calculations at the nodes. For example the connection close rate estimated for each node may be multiplied by a Z value that is randomly generated at each node. Z may be randomly generated within some specified range for example between 0.9 and 1.1. As an example given X 300 Y 218 W 2 R 200 and Z 1.05 at a node the connection close rate for the node is 23600 200 218 600 3600 261600 1.1 287760

Randomness may instead or also be introduced at other points in the calculations. For example the weight W that is applied to the total number of connections to be closed in an hour may be randomly generated within a specified range at each node or multiplied by a random factor Z at each node.

In some embodiments instead of or in addition to introducing a random factor or factors into the calculations of the connection close rate the connection close rate may be calculated and then randomness may be introduced in its application. As indicated above the connection close rate may be given as either some number of requests n e.g. every n requests or as some time interval t e.g. every t seconds . In various embodiments the nodes may either issue close requests based on the number of requests i.e. every nth request or based on the time interval e.g. every t seconds . To introduce randomness n or t may be randomly varied within some range or percentage for example 10 .

The above methods for calculating or estimating a connection close rate and for introducing randomness into the connection close rate are given by way of example and are not intended to be limiting. One of ordinary skill in the art will recognize that other methods for calculating or estimating a connection close rate and for introducing randomness into the calculations may be used.

In some embodiments each node may collect and use information about particular clients or connections and apply that information to make decisions about which connections to close and or about how often to close connections for particular clients. For example in some embodiments each node may use information in the requests e.g. the client s IP address and or the client s identity to track requests per second per client. The node may then calculate or estimate a close request rate per client according to the information collected per client or alternatively may distribute or allocate the calculated overall close request rate for the node among the clients according to the information collected per client. For example in some embodiments the node may generate connection close requests for busier clients at a lower rate or less frequently than connections for less busy clients as determined from the information collected per client. In some embodiments a randomizing factor may be used in allocating the calculated overall close request rate for the node across the clients.

In some embodiments the collected client information may be shared with other nodes in the node group e.g. via a gossip protocol and thus the nodes may determine close request rates for particular clients based on global information for the clients. Since there may be many clients and tracking the clients may be expensive in some embodiments the request rate for only a subset of the clients e.g. the B busiest clients may be tracked and used to determine connection close request rates for the subset of the clients.

In some embodiments in addition to propagating e.g. via a gossip protocol presence and health information among the node group which the nodes use to determine the number of healthy nodes and thus the total request rate the nodes may also propagate their local request rate information to the other nodes in the node group. Each node may then calculate its close request rate so that the node closes a number of connections proportional to its load relative to that of the other nodes.

Alternatively some network communications protocols may allow a client to establish a separate control connection to a load balancer via which control or other packets may be exchanged with the load balancer. In these implementations a connection close request indicating one or more connections to be closed may be generated by a node sent to the load balancer and forwarded to the client via the control connection. The client may then close the connection s indicated by the connection close requests.

The connection redistribution method causes the clients to redistribute their connections among the load balancers rather than holding connections for extended periods which tends to spread the client connections and client traffic from all clients and from particular clients among the load balancers in the load balancer layer of the system thus helping to avoid situations where particular ones of the load balancers are heavily loaded with many busy persistent connections to particular clients. In addition to take a load balancer or the nodes in the node group fronted by a load balancer offline new client connections may be prevented to the load balancer for example by directing the endpoint discovery service e.g. a DNS to not resolve connection requests to the public IP address es of that load balancer and within a given period or a bit longer e.g. within an hour or two the nodes in the node group fronted by that load balancer may issue connection close requests for all of the active client connections at the load balancer. Since new client connections are not being made to the load balancer after most or all of the client connections are closed the load balancer and or nodes can be serviced as needed.

While embodiments of the connection redistribution method may be implemented in any load balanced system that includes multiple load balancers each fronting multiple nodes the following describes as an example of such a load balanced system a network based distributed database service in which embodiments may be implemented as illustrated in . The distributed database service may for example provide relational and or non relational database functionality to clients in single tenant and or multi tenant environments. The clients may receive the database services from a web based service provider and may negotiate with the service provider to obtain access to resources for those services including storage resources and input output throughput capacity. The distributed database service may include a load balancer layer a front end or request routing layer and a back end or storage layer .

The load balancer layer may include multiple load balancer devices. The load balancers may for example be commercially available devices provided by a vendor. Each load balancer may be provided with a unique public IP address of the distributed database service the public IP addresses of the load balancers may be published for example to a Domain Name Server DNS .

The storage layer may include tens hundreds or thousands of storage nodes . Each storage node may include data software and or hardware that implements at least database storage functionality of the distributed database service .

The request routing layer may include tens hundreds or thousands of request routers . Each request router may include data software and or hardware that implements at least request routing functionality of the distributed database service . The request routers may be arranged in groups with each group fronted by a different one of the load balancers in load balancer layer . Each group may include tens or hundreds of request routers . In some implementations the request routers and storage nodes may be implemented as or on separate physical devices. Alternatively the request routers may be co located on physical devices that implement the storage nodes . In at least some implementations each request router in request router layer may be configured to route requests e.g. HTTP requests received from clients via a respective load balancer to appropriate ones of the storage nodes in storage layer . Each request router may also include a module that implements an embodiment of the connection close method as described herein for example as illustrated in .

Request router may also include a connection close module that implements an embodiment of the connection close method as described herein for example as illustrated in . Connection close module may calculate a connection close rate for the request router according to one or more metrics for example one or more metrics related to the request rate at request router and one or more metrics obtained from collected group information. In at least some embodiments the connection close rate may be based on an estimation of the percentage of the overall client traffic received by the load balancer that the request router is handling. In at least some embodiments the connection close rate at request router may provide an indication of a number of connections that request router should issue connection close requests for in a specified time period for example X connection close requests per hour. After calculating the connection close rate connection close module may generate connection close requests for selected connections according to the connection close rate. In at least some embodiments to select the connections to be closed connection close module may select random request messages e.g. HTTP request messages from among those received at request router from its load balancer according to the connection close rate. In at least some embodiments to request that a connection be closed connection close module modifies the header of a response message e.g. an HTTP response message to a received request message from a client to include a connection close request.

As illustrated in a storage node instance may include one or more modules configured to provide partition management shown as to implement replication and failover processes shown as and or to provide an application programming interface API to underlying storage shown as . As illustrated in this example each storage node instance may include a storage engine which may be configured to maintain i.e. to store and manage one or more tables and associated table data in storage which in some embodiments may be a non relational database on behalf of one or more clients . In some embodiments each storage node instance may also include a request logging and data aggregation component which may perform some or all of request logging data collection and or data aggregation functions for generating e.g. computing and presenting metrics and or reports. In addition to these component specific modules storage node instance may include components that are common to the different types of computing nodes that collectively implement a Web services platform such as a message bus shown as and or a dynamic configuration module shown as .

In at least some embodiments a server that implements a portion or all of the connection redistribution methods and apparatus for load balanced systems as described herein may include a general purpose computer system that includes or is configured to access one or more computer accessible media such as computer system illustrated in . In the illustrated embodiment computer system includes one or more processors coupled to a system memory via an input output I O interface . Computer system further includes a network interface coupled to I O interface .

In various embodiments computer system may be a uniprocessor system including one processor or a multiprocessor system including several processors e.g. two four eight or another suitable number . Processors may be any suitable processors capable of executing instructions. For example in various embodiments processors may be general purpose or embedded processors implementing any of a variety of instruction set architectures ISAs such as the x86 PowerPC SPARC or MIPS ISAs or any other suitable ISA. In multiprocessor systems each of processors may commonly but not necessarily implement the same ISA.

System memory may be configured to store instructions and data accessible by processor s . In various embodiments system memory may be implemented using any suitable memory technology such as static random access memory SRAM synchronous dynamic RAM SDRAM nonvolatile Flash type memory or any other type of memory. In the illustrated embodiment program instructions and data implementing one or more desired functions such as those methods techniques and data described above for the connection redistribution methods and apparatus are shown stored within system memory as code and data .

In one embodiment I O interface may be configured to coordinate I O traffic between processor system memory and any peripheral devices in the device including network interface or other peripheral interfaces. In some embodiments I O interface may perform any necessary protocol timing or other data transformations to convert data signals from one component e.g. system memory into a format suitable for use by another component e.g. processor . In some embodiments I O interface may include support for devices attached through various types of peripheral buses such as a variant of the Peripheral Component Interconnect PCI bus standard or the Universal Serial Bus USB standard for example. In some embodiments the function of I O interface may be split into two or more separate components such as a north bridge and a south bridge for example. Also in some embodiments some or all of the functionality of I O interface such as an interface to system memory may be incorporated directly into processor .

Network interface may be configured to allow data to be exchanged between computer system and other devices attached to a network or networks such as other computer systems or devices as illustrated in for example. In various embodiments network interface may support communication via any suitable wired or wireless general data networks such as types of Ethernet network for example. Additionally network interface may support communication via telecommunications telephony networks such as analog voice networks or digital fiber communications networks via storage area networks such as Fibre Channel SANs or via any other suitable type of network and or protocol.

In some embodiments system memory may be one embodiment of a computer accessible medium configured to store program instructions and data as described above for for implementing embodiments of a connection redistribution method in a load balanced system. However in other embodiments program instructions and or data may be received sent or stored upon different types of computer accessible media. Generally speaking a computer accessible medium may include non transitory storage media or memory media such as magnetic or optical media e.g. disk or DVD CD coupled to computer system via I O interface . A non transitory computer accessible storage medium may also include any volatile or non volatile media such as RAM e.g. SDRAM DDR SDRAM RDRAM SRAM etc. ROM etc that may be included in some embodiments of computer system as system memory or another type of memory. Further a computer accessible medium may include transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as a network and or a wireless link such as may be implemented via network interface .

Various embodiments may further include receiving sending or storing instructions and or data implemented in accordance with the foregoing description upon a computer accessible medium. Generally speaking a computer accessible medium may include storage media or memory media such as magnetic or optical media e.g. disk or DVD CD ROM volatile or non volatile media such as RAM e.g. SDRAM DDR RDRAM SRAM etc. ROM etc as well as transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as network and or a wireless link.

The various methods as illustrated in the Figures and described herein represent exemplary embodiments of methods. The methods may be implemented in software hardware or a combination thereof. The order of method may be changed and various elements may be added reordered combined omitted modified etc.

Various modifications and changes may be made as would be obvious to a person skilled in the art having the benefit of this disclosure. It is intended to embrace all such modifications and changes and accordingly the above description to be regarded in an illustrative rather than a restrictive sense.

