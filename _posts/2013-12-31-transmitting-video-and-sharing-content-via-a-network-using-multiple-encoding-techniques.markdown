---

title: Transmitting video and sharing content via a network using multiple encoding techniques
abstract: Embodiments disclose systems and methods for transmitting user-extracted video and content more efficiently by recognizing that user-extracted video provides the potential to treat parts of a single frame of a user-extracted video differently. An alpha mask of the image part of the user-extracted video is used when encoding the image part so that it retains a higher quality upon transmission than the remainder of the user-extracted video.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09386303&OS=09386303&RS=09386303
owner: PERSONIFY, INC.
number: 09386303
owner_city: Chicago
owner_country: US
publication_date: 20131231
---
Embodiments of the present invention relate to transmitting video and sharing content via a network and in particular to more efficiently transmitting video and content via a network by transmitting them separately using optimized protocols.

Some video transmission systems merge video and content to be shared into one video stream. In such systems the video stream may be transmitted using standard video codecs and streaming protocols. Upon receipt the video and content are displayed for view on a web browser. These systems require no processing of the video stream at the viewer site aside from the processes related to receiving and displaying. Such systems typically treat the combined video and shared content similarly regarding methods of compression transmission reception and display even though different methods may be more efficient or otherwise more suitable for each of the components that went into the video stream.

Where transmission systems send video and content separately the video itself is typically transmitted using processes that treat the pixels of the video uniformly. Thus such current transmission systems do not exploit the potential provided by user extracted video to differentiate between an image part and a background part of the user extracted video or between an image part and a non image part of a user extracted video combined with another video or other content.

Also current video transmission systems do not support the use of an alpha mask also known as an alpha channel though there have been efforts to modify current systems to support WebM video with an alpha channel for VP8 video.

Embodiments of the claimed subject matter disclose methods and systems related to transmitting user extracted video and content more efficiently. These embodiments recognize that user extracted video provides the potential to treat parts of a single frame of the user extracted video differently e.g. the image part of the user extracted video may be encoded to retain a higher quality upon decoding than the remainder of the user extracted video. Such different treatment of the parts of a user extracted video may allow more efficient transmission.

According to such embodiments a user extracted video is created along with an associated alpha mask which identifies the image part of the user extracted video. If the image part is more important than the remainder of the user extracted video e.g. if it is a higher priority to have a high resolution image part it is processed for transmission using methods that preserve its quality or resolution in comparison to the remainder of the user extracted video. During this processing the alpha mask is used to differentiate between the image part and the remainder of the user extracted video. The processed video is then sent to a receiving computer.

In an embodiment content is also selected and combined with the user extracted video to create a composite video. During processing the alpha mask is then used to differentiate between the image part and in this embodiment the remainder of the composite video.

In an embodiment a chroma key is employed to include the alpha mask in the encoded video. Dechroma keying is then used to re generate the alpha mask from the sent and decoded video. The re generated alpha mask is used to determine an alpha value for each pixel of each frame of the decoded video with the alpha value for a pixel being based on the difference between the pixel color in the decoded video and a key color. The alpha value is then used to determine whether to display that pixel color on the pixel.

In an embodiment control information regarding a dynamic chroma key is sent. The control information represents a dynamic chroma key represents a key color that is not found within the associated image part of the video. This key color was used to replace the remainder of the associated user extracted video. Should the image part of the video change and a pixel color changes to match the key color a new key color is chosen to replace the remainder of the associated user extracted video. The control information is then changed to represent the new key color.

In the following description numerous details and alternatives are set forth for purpose of explanation. However one of ordinary skill in the art will realize that embodiments can be practiced without the use of these specific details. In other instances well known structures and devices are shown in block diagram form to not obscure the embodiments with unnecessary detail. And the methods described within may be described in one order but one of skill will realize the methods may be employed in a number of different orders.

In receiver receives content control information and user extracted video . Receiver processes content and video according to control information . An exemplary result of such processing is illustrated in .

Still regarding user extracted video and shared content may be streamed separately in different ways. This results in potentially three different types of transmitted data with the differences between the data providing opportunities to individually tailor and optimize the transmission of each type separately from the others.

First regarding user extracted video data chroma keying processing may be used to embed an alpha mask in the video frame. Such embedding is typically performed in real time. An alpha mask represents a video frame using 0 or 1 for each pixel of that frame. Where the alpha mask contains a 0 that pixel is part of the background part of the user extracted video. Where the alpha mask contains a 1 that pixel is part of the image part of the user extracted video. An alpha mask is created during the extraction of the user from the video which is discussed within. Video data may then be compressed using a standard encoder or an encoder according to an embodiment Z encoder see the discussion of . Subsequently video data may be sent peer to peer or broadcast by a media streaming server using network application layer protocols such as Real Time Transport Protocol RTP Real Time Messaging Protocols RTMP Real Time Messaging Protocols Tunneled RTMPT HTTP Live Streaming HLS or HTTP Dynamic Streaming HDS .

Second regarding control information this information is used to synchronize the sharing of content between host sender and receiver viewer displays. For example should content be a document and have been sent ahead of video data then control information would need information necessary to synchronize the page number of the document with video data . Control information also contains rendering information e.g. the relative position size and degree of transparency of the user extracted video for rendering that video with the shared content .

Third regarding content such content may include e.g. documents photos presentation slides video clips and web pages which may be uploaded from e.g. a user computer and also from shared cloud services like Google Docs Microsoft Office 365 YouTube Vimeo and SlideShare . By splitting the data and handling different video streams with codecs and protocols that are matched to or optimized for the specific streaming data e.g. still image or video various system embodiments help to minimize transmission bit rate requirements while retaining visual quality. Codecs and protocols may for example be optimized to improve the resolution and frame rate of video since video typically contains movement. And codecs and protocols for content may be optimized to improve content details. In embodiments smart strategies are employed that automatically choose different protocols based on the type of data e.g. video document etc. being transmitted.

In some embodiments the sender processing flow may be as follows. First a user persona is extracted from a video see . Also an alpha mask is created to define an image part and a background part of the user persona. And content may be selected for sharing. Then a dynamic chroma keying technique is applied to the background part. With a dynamic chroma keying technique a color not initially found within the image part is used to replace the background part as defined by the alpha mask. Subsequently should that initial background color be found in the image part a new background color is chosen from the colors not appearing in the image part see for further discussion . After chroma keying the image part and the background part are compressed for transmission. Because it is preferable to have a detailed image part while the level of detail in the background part is of less importance the image part may be compressed with methods that preserve more detail while losing smoothness of an object s motion. The background part may be compressed with methods that lose detail i.e. lossy methods which generally lose detail but maintain smoothness of motion . Thus using different compression techniques may decrease the bandwidth required to transmit the video while maintaining its overall quality. Information regarding the compression of the image and background parts is preferably included in the control information to facilitate accurate decoding. Control information is then created regarding the persona and content. Such control information preferably facilitates the transmission reception and display of the persona and content. And dynamic chroma key information is added to the control information. Then the user persona content and control information are transmitted to a receiving computer. In some embodiments the protocols for each are chosen to optimize the efficiency of transmission which may result in reduced bandwidth and or an improved resolution. This may not require entirely different protocols. For example the content and persona are similar data types and thus the same protocol may be used to transmit the persona and content. Then the persona and content are displayed according to the control information by the receiving computer.

Still regarding content combined with user extracted video may be displayed by a software client. However since shared content on the cloud is usually rendered from web pages web browsers may be used to process blend and display content and video . In some embodiments the receiver processing flow may be as follows. First the user extracted video stream may be decoded and dechroma keyed to extract the alpha mask. Second the background pixels may be set to be transparent when the frame is rendered on a HTML5 canvas . Third shared content is displayed on a Web iFrame object in a web page that viewers open on their web browsers. Finally canvas which contains video image part is rendered on the top of the iFrame object . The location and size of the canvas is specified in the control information control and signaling data . Subsequently additional canvasses containing additional video image parts may be rendered on top of iFrame object depending on the number of users participating in the session.

In additional embodiments the method may further include the following. Content may be selected to accompany the user extracted video. This content may be combined with the user extracted video to create a composite video. In such a case at the priority of the image part would be determined in relation to the remainder of the composite video at the alpha mask would be used to encode the image part and the remainder of the composite video differently based in part on the priority of the image part and at the encoded composite video would be sent to the at least one receiving computer.

Regarding step in some embodiments the background part is not displayed at the receiver. Thus it would be inefficient for the whole video frame to be compressed and transmitted for subsequent discarding of the background part at the receiver. Embodiments disclosed herein mitigate this inefficiency by embedding alpha mask information in the color frame and then executing a chroma keying technique to separate the video frame into an image part and a background part. In such embodiments the background part may be encoded or transmitted differently these include for example its not being encoded or transmitted at all . Such is the case for example with conferencing applications where only the user s image and not their surrounding environment is to be combined or shared for embedding with virtual content. This treatment of the background part saves bandwidth by not transmitting unnecessary pixel data.

The choice of key color preferably satisfies the following requirements 1 no pixel in the foreground area has the same color as the key color 2 there is some safe Lnorm distance between the key color and the closest color in the foreground pixel and 3 the key color does not require frequent change and is chosen to minimize the size of the encoded video packets. The safe Lnorm distance is chosen based on considerations such as data type compression methods and decoding methods.

Regarding the second requirement 2 the reason for the safe distance Lis that after applying encoding to the video and sending through the network e.g. the Internet the color values may not be preserved correctly when uncompressed and decoded into the video for display. Rather the decoder may give out color values that are similar to but not the same as the uncompressed ones. Thus the presence of a safe Lnorm distance ensures that the decoded key color values of the background part are always separated from decoded color values of the image part or foreground area of the user extracted video.

Almost all codecs such as VP8 or H264 prefer the input video in YUV color space for the ease and efficiency of video compression. Thus regarding the static chroma key technique to convert from RGB to YUV color space a fixed point approximation is applied in most digital implementations.

Since the value range of the output YUV is normally scaled to 16 235 it is possible to use the 0 0 0 value for key color. This key color selection satisfies requirements 1 3 above. However it is not always the case for all codec implementations that the range of YUV is limited to 16 235 . In such cases an embodiment proposes a dynamic chroma key technique.

Still regarding should static chroma keying be used at a key color is chosen and at the background is replaced.

At should no empty box of the chosen dimensions be found the key color y u v is chosen to minimize the expression 

If at E 0 which means that there is at least one pixel value in the image part foreground area that has its color inside the center box and its neighboring boxes then that pixel color value is modified so that it no longer lies inside the box. This works to avoid ambiguity in dechroma keying step.

Compared to the static chroma key method the dynamic chroma key method requires more computation and bandwidth. Therefore it is preferable to use the dynamic method only when the static method cannot be applied.

Regarding quantization block at an alpha mask from the user extraction process may be used to drive the quality of a quantization block so that macro blocks in the user image or user extracted region of a video frame i.e. the more important sections are quantized with more bits than the background. Alpha mask allows the encoder to identify the location of the image part in the canvas . This information is added to the quantization block allowing the encoder method to avoid encoding blocks that do not contain elements of image part . This preserves the quality of the image part or user region during encoding. And by using fewer bits to quantize the background part in the case of a video of a user extracted image or content in the case of a composite video with content and a user extracted image it reduces the bandwidth required to transmit the encoded video stream. Skipping i.e. not encoding the background part of a user extracted video also saves additional processing time.

Efficiencies are gained in compression by addressing the different requirements of the content. When content is shared the changes in content that accompany a change in video frame are typically small. In such case the Z encoder may compress only those changes in the content following the method described above with respect to video frame . In an additional embodiment should it be determined that the background or content portion of video frame is actually more important than the user extracted image then the alpha mask from the user extraction process may be used to drive the quality of a quantization block so that macro blocks in the background or content region of a video frame are quantized with more bits than the user extracted image using the method described. And in general method does not require that video frame has gone through the chroma keying process. Furthermore in an embodiment alpha mask may be used to drive the quality of a quantization block with the information from alpha mask added through optional path to prediction block .

After dechroma keying the frame of decoded video and the generated alpha mask are sent to the alpha blending block to make the image for display block . Alpha blending block may combine decoded video with any additional content or additional user extracted video . Alpha mask contains an alpha value for each pixel of the decoded video frame that specifies how much the pixel color value contributes to the blended color value of the output display. Side information may be used to modify alpha mask according to control input from the user at the sender side. The alpha value may then range from 0 to 1 or 0 to 100 . The alpha blending formula is as follows where C C and Cequal the color values of the blended pixel video pixel and content pixel respectively 1 

The following contains sample Javascript HTML5 code for implementing aspects of the embodiments such as streaming live video initializing video and canvas sizes and binding post processing actions to video during streaming.

Creating a persona by extracting a user image from a video will now be described regarding . illustrates an example video . In general the example video comprises a background portion and a persona . For example the background portion may comprise a wall outdoor scene or any other background scene and the persona may comprise a human user or presenter. However the persona may comprise any identifiable object or entity. Thus the example video may be divided into at least two portions a background and a persona . For example if the video comprises a user typing on a keyboard then the user may comprise the persona and a wall of the room behind may comprise the background portion .

As seen in a camera is connected to a computer . The camera may comprise a three dimensional 3D camera depth camera z camera and or range camera. In some embodiments the camera may be comprised of a color or RGB camera and a depth camera or may comprise of a single camera with an RGB sensor and depth sensor. As such the camera receives color information and depth information. The received color information may comprise information related to the color of each pixel of a video. In some embodiments the color information is received from a Red Green Blue RGB sensor . As such the RGB sensor may capture the color pixel information in a scene of a captured video image. The camera may further comprise an infrared sensor and an infrared illuminator . In some embodiments the infrared illuminator may shine an infrared light through a lens of the camera onto a scene. As the scene is illuminated by the infrared light the infrared light will bounce or reflect back to the camera . The reflected infrared light is received by the infrared sensor . The reflected light received by the infrared sensor results in depth information of the scene of the camera . As such objects within the scene or view of the camera may be illuminated by infrared light from the infrared illuminator . The infrared light will reflect off of objects within the scene or view of the camera and the reflected infrared light will be directed towards the camera . The infrared sensor may receive the reflected infrared light and determine a depth or distance of the objects within the scene or view of the camera based on the reflected infrared light.

In some embodiments the camera may further comprise a synchronization module to temporally synchronize the information from the RGB sensor infrared sensor and infrared illuminator . The synchronization module may be hardware and or software embedded into the camera . In some embodiments the camera may further comprise a 3D application programming interface API for providing an input output IO structure and interface to communicate the color and depth information to a computer system . The computer system may process the received color and depth information and comprise and perform the systems and methods disclosed herein. In some embodiments the computer system may display the foreground video embedded into the background feed onto a display screen .

Any node of the network may comprise a general purpose processor a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof capable to perform the functions described herein. A general purpose processor may be a microprocessor but in the alternative the processor may be any conventional processor controller microcontroller or state machine. A processor may also be implemented as a combination of computing devices e.g. a combination of a DSP and a microprocessor a plurality of microprocessors one or more microprocessors in conjunction with a DSP core or any other such configuration etc. .

In some embodiments a node may comprise a machine in the form of a virtual machine VM a virtual server a virtual client a virtual desktop a virtual volume a network router a network switch a network bridge a personal digital assistant PDA a cellular telephone a web appliance or any machine capable of executing a sequence of instructions that specify actions to be taken by that machine. Any node of the network may communicate cooperatively with another node on the network. In some embodiments any node of the network may communicate cooperatively with every other node of the network. Further any node or group of nodes on the network may comprise one or more computer systems e.g. a client computer system a server computer system and or may comprise one or more embedded computer systems a massively parallel computer system and or a cloud computer system.

The computer system includes a processor e.g. a processor core a microprocessor a computing device etc. a main memory and a static memory which communicate with each other via a bus . The machine may further include a display unit that may comprise a touch screen or a liquid crystal display LCD or a light emitting diode LED display or a cathode ray tube CRT . As shown the computer system also includes a human input output I O device e.g. a keyboard an alphanumeric keypad etc. a pointing device e.g. a mouse a touch screen etc. a drive unit e.g. a disk drive unit a CD DVD drive a tangible computer readable removable media drive an SSD storage device etc. a signal generation device e.g. a speaker an audio output etc. and a network interface device e.g. an Ethernet interface a wired network interface a wireless network interface a propagated signal interface etc. .

The drive unit includes a machine readable medium on which is stored a set of instructions i.e. software firmware middleware etc. embodying any one or all of the methodologies described above. The set of instructions is also shown to reside completely or at least partially within the main memory and or within the processor . The set of instructions may further be transmitted or received via the network interface device over the network bus .

It is to be understood that embodiments may be used as or to support a set of instructions executed upon some form of processing core such as the CPU of a computer or otherwise implemented or realized upon or within a machine or computer readable medium. A machine readable medium includes any mechanism for storing information in a form readable by a machine e.g. a computer . For example a machine readable medium includes read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices electrical optical or acoustical or any other type of media suitable for storing information.

Although the present embodiment has been described in terms of specific exemplary embodiments it will be appreciated that various modifications and alterations might be made by those skilled in the art without departing from the spirit and scope of the invention. The previous description of the disclosed embodiments is provided to enable any person skilled in the art to make or use the present invention. Various modifications to these embodiments will be readily apparent to those skilled in the art and the generic principles defined herein may be applied to other embodiments without departing from the spirit or scope of the invention. Thus the present invention is not intended to be limited to the embodiments shown herein but is to be accorded the widest scope consistent with the principles and novel features disclosed herein.

