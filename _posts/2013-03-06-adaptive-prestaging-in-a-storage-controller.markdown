---

title: Adaptive prestaging in a storage controller
abstract: In one aspect of the present description, at least one of the value of a prestage trigger and the value of the prestage amount, may be modified as a function of the drive speed of the storage drive from which the units of read data are prestaged into a cache memory. Thus, cache prestaging operations in accordance with another aspect of the present description may take into account storage devices of varying speeds and bandwidths for purposes of modifying a prestage trigger and the prestage amount. Other features and aspects may be realized, depending upon the particular application.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08874840&OS=08874840&RS=08874840
owner: International Business Machines Corporation
number: 08874840
owner_city: Armonk
owner_country: US
publication_date: 20130306
---
The present application is a continuation application of copending application Ser. No. 13 018 305 filed Jan. 31 2011 entitled ADAPTIVE PRESTAGING IN A STORAGE CONTROLLER assigned to the assignee of the present application and incorporated by reference in its entirety.

The present description relates to a method system and program product for prestaging data into cache from a storage system in preparation for data transfer operations.

In current storage systems a storage controller manages data transfer operations between a direct access storage device DASD which may be a string of hard disk drives or other non volatile storage devices and host systems and their application programs. To execute a read operation presented from a host system i.e. a data access request DAR the storage controller physically accesses the data stored in tracks in the DASD. A DAR is a set of contiguous data sets such as tracks records fixed blocks or any other grouping of data. The process of physically rotating the disk in the DASD to the requested track then physically moving the reading unit to the disk section to read data is often a time consuming process. For this reason current systems frequently stage data into a cache memory of the storage controller in advance of the host requests for such data.

In addition storage systems may also include solid state storage devices such as flash memory. However flash memory tends to be substantially more expensive than DASD storage.

A storage controller typically includes a large buffer managed as a cache to buffer data accessed from the attached storage. In this way data access requests DARs can be serviced at electronic speeds directly from the cache thereby avoiding the delays caused by reading from storage such as DASD storage. Prior to receiving the actual DAR the storage controller receives information on a sequence of tracks in the DASD involved in the upcoming read operation or that data is being accessed sequentially. The storage controller will then proceed to stage the sequential tracks into the cache. The storage controller would then process DARs by accessing the staged data in the cache. In this manner the storage controller can return cached data to a read request at the data transfer speed in the storage controller channels as opposed to non cached data which is transferred at the speed of the DASD device or other storage from which the data was transferred to the cache.

During a sequential read operation an application program such as a batch program will process numerous data records stored at contiguous locations in the storage device. It is desirable during such sequential read operations to prestage the sequential data into cache in anticipation of the requests from the application program. As a result of prestaging often one of the tracks being staged is already in cache. Present techniques used to prestage sequential blocks of data include sequential caching algorithms systems such as those described in the commonly assigned patent entitled CACHE DASD Sequential Staging and Method having U.S. Pat. No. 5 426 761. A sequential caching algorithm detects when a device is requesting data as part of a sequential access operation. Upon making such a detection the storage controller will begin prestaging sequential data records following the last requested data record into cache in anticipation of future sequential accesses.

U.S. Pat. No. 5 426 761 discloses an algorithm for sequential read operations to prestage multiple tracks into cache when the tracks are in the extent range of the sequential read operation the track is not already in the cache and a maximum number of tracks have not been prestaged into cache. The cached records may then be returned to the application performing the sequential data operations at speeds substantially faster than retrieving the records from a non volatile storage device.

Another prestaging technique includes specifying a block of contiguous data records to prestage into cache in anticipation of a sequential data request. For instance the Small Computer System Interface SCSI provides a prestage or prefetch command PRE FETCH that specifies a logical block address where the prestaging operation begins and a transfer length of contiguous logical blocks of data to transfer to cache. The SCSI PRE FETCH command is described in the publication Information Technology Small Computer System Interface 2 published by ANSI on Apr. 19 1996 reference no. X3.131 199x Revision 10L which publication is incorporated herein by reference in its entirety.

Yet another prestaging technique is referred to as AMP Adaptive Multi stream Prefetching in a shared cache. This technique seeks to provide an algorithm which can adapt both the amount of data being prefetched in a prefetch operation and the value of a trigger for triggering the prefetch operation on a per stream basis in response to evolving workloads.

As set forth above when a host reads tracks sequentially it benefits greatly to pre stage tracks so that the tracks are already in cache before the host requests access to those tracks. However a storage controller has a limited Cache and limited bandwidth. If pre stage stages too few tracks then the host will get more cache misses since fewer tracks will be in cache. Conversely if the host pre stages too many tracks then it can starve other kinds of I O in the storage controller.

Tracks staged into cache may be demoted dropped from cache according to a Least Recently Used LRU algorithm to insure that the staged data in cache does not exceed a predetermined threshold. The LRU algorithm discards the least recently used items first which are generally the items which have been in the cache the longest without being used.

The data when first placed into cache may be designated time stamped MRU most recently used data. As the data remains unused in cache the timestamp for that data is updated. The longer the data stays in cache unused the closer the data moves toward an LRU timestamp. The LRU algorithm assumes that the oldest unused data is the data least likely to be used and therefore may be discarded from the cache to make room in the cache for additional data which is more likely to be accessed. In some LRU algorithms LRU data may be redesignated with an MRU timestamp to give that data another chance to be used before being demoted that is discarded from cache should the timestamp for that data again arrive at LRU status.

In one aspect of the present description at least one of the value of a prestage trigger and the value of a prestage amount may be modified as a function of the drive speed of the storage drive from which the units of read data are prestaged into a cache memory. In one embodiment the operations include reading units of data of a sequential stream of read data stored in at least one of a first storage drive having a drive speed and a second storage drive having a drive speed. A prestage trigger is initialized to initiate prestaging of units of read data. In response to reaching the trigger as the units of the sequential stream of read data are read units of data of the sequential stream of read data are prestaged from a storage drive into a cache memory in anticipation of a read request. In addition at least one of the value of the prestage trigger and the value of the prestage amount may be modified as a function of the drive speed of the storage drive from which the units of read data are prestaged into the cache memory. In some embodiments the drive speed of the first storage drive may be faster than that of the second storage drive. However it is appreciated that in other embodiments the drive speeds of various storage drives of the system may be the same depending upon the particular application.

In another aspect upon demoting a unit of data from the cache before the unit being demoted is read from cache in response to a read request at least one of the value of the prestage trigger and the value of the prestage amount may be reduced as a function of the drive speed in association with the demoting of a unit of data from the cache.

In yet another aspect the prestaging operations may further include comparing the modified value of the prestage trigger to the amount of data to be prestaged in response to the trigger and prestaging additional units of data of the sequential stream of read data into the cache memory as a function of the comparison.

In still another aspect the prestaging operations may further include staging units of data of the sequential stream of read data into the cache memory and determining if a cache miss occurs because a staging has not completed. In one embodiment the value of the prestage trigger may be modified by increasing the value of the prestage trigger as a function of the drive speed of the storage drive from which the units of read data are prestaged into the cache memory.

In yet another embodiment the value of the prestage trigger may be modified by increasing the value of the prestage trigger by a first value if the read data being prestaged is from a hard disk drive and may be modified by increasing the value of the prestage trigger by a second value higher than the first value if the read data being prestaged is from a solid state drive.

In still another aspect the prestaging operations may further include determining if a cache hit occurs in connection with a unit of read data which was a candidate for demotion from the cache wherein the value of the prestage amount may be modified by decreasing the value of the prestage amount as a function of the drive speed of the storage drive from which the units of read data are prestaged into the cache memory in response to a determination that a cache hit occurs in connection with a unit of read data which was a candidate for demotion from the cache. In one embodiment the value of the prestage amount may be decreased by decreasing the value of the prestage trigger by a first value if the read data being prestaged is from a hard disk drive and by decreasing the value of the prestage trigger by a second value higher than the first value if the read data being prestaged is from a solid state storage drive.

In the following description reference is made to the accompanying drawings which form a part hereof and which is shown by way of illustration several embodiments of the present invention. It is understood that other embodiments may be utilized and structural changes may be made without departing from the scope of the present invention.

A trigger track is identified at a trigger distance g from the end of the unaccessed tracks of the prestaged set . Once the tracks being accessed reaches the trigger track another set of the next p sequential tracks of the stream of sequential read data is prefetched or prestaged into the cache. Once the tracks being accessed reaches the trigger track of that next set yet another set of the next p sequential tracks of the stream of sequential read data is prefetched or prestaged into the cache and so on.

A prior art algorithm called AMP Adaptive Multi stream Prefetching can adapt both the amount p of data being prestaged in a prestage operation and the value of the trigger g for triggering the prestage operation on a per stream basis in response to changing workloads. The AMP algorithm is based upon the theorem that improved performance may be obtained where for each individual stream of sequential read data the prestageAmount p should be p r L where r is the request rate of the stream measured in tracks second and L is the life of the shared cache MRU Most Recently Used timestamp minus LRU Least Recently Used timestamp . Accordingly it is believed that when the prestageAmount p r L that data in cache will likely be demoted before being used.

If it is determined block that the track being read is on a sequential read stream and it is determined block that the track being read is at the last track of a stage group the prestage amount p is increased block by 1. By increasing the prestage amount p it is intended to decrease the chances of running out of prestaged data and causing a cache miss. However if it is previously determined block that the prestage amount p has reached a maximum such as a maximum of 84 for example the prestage amount p is not increased.

If it is determined block that the track being read is not at the last track of a stage group and it is determined block that the track being read is a cache miss the prestage trigger distance g is increased block by 1. By increasing the prestage trigger distance g it is intended to trigger the prestaging of additional data sooner to reduce the chances of future cache misses. However if it determined block that the prestage trigger g plus 2 g 2 has exceeded the prestage amount p the prestage trigger is reduced block to the prestage amount p minus 2 p 2 .

If it is determined block that the track that was read was not a cache miss that is it was a cache hit a determination is made block as to whether that track was prestaged in cache and whether it was a candidate for demotion from cache.

Referring back to if it is determined block that the track being read was prestaged in cache and was a candidate for demotion from cache both the prestage amount p and the prestage trigger g are each decreased block by 1. By decreasing the prestage amount p and the trigger g it is intended to reduce the amount of data being prestaged each time and to delay the prestaging of additional data into cache to reduce the number of candidates for demotion and to reduce demotion of data from cache.

The storage controller further includes a cache in which data is prestaged and staged. In accordance with one aspect of the present description and as explained in greater detail below the storage controller can prestage data in such a manner that one or both of the prestage amount and the prestage trigger may be modified as a function of the speeds and bandwidths of the various storage devices coupled to the storage controller . Thus in this embodiment the solid state storage has a substantially faster access time as compared to that of the hard drive storage . Consequently when cache conditions indicate that it is appropriate to modify the prestage amount or prestage trigger the prestage amount or prestage trigger for a stream of data being read from the faster flash memory storage may be modified a lower amount for example as compared to a modification of the prestage amount or prestage trigger for a stream of data being read from slower hard drive storage . Other features and advantages will also be explained in connection with the present description.

The host systems may be any host system known in the art such as a mainframe computer workstations etc. including an operating system such as WINDOWS AIX UNIX MVS etc. AIX is a registered trademark of IBM MVS is a trademark of IBM WINDOWS is a registered trademark of Microsoft Corporation and UNIX is a registered trademark licensed by the X Open Company LTD. A plurality of channel paths in the host systems provide communication paths to the storage controller . The storage controller and host systems may communicate via any network or communication system known in the art such as LAN TCP IP ESCON SAN SNA Fibre Channel SCSI etc. ESCON is a registered trademark of International Business Machines Corporation IBM . The host system executes commands and receives returned data along a selected channel . The storage controller issues commands via a path to physically position the electromechanical devices to read the storage . The storage controller also issues commands via a path to read data from the solid state storage .

In preferred embodiments the cache is implemented in a high speed volatile storage area within the storage controller such as a DRAM RAM etc. The length of time since the last use of a record in cache is maintained to determine the frequency of use of cache . The least recently used LRU data in cache may be demoted according to LRU or other cache memory management techniques known in the art. In alternative embodiments the cache may be implemented in alternative storage areas accessible to the storage controller . Data can be transferred between the channels and the cache between the channels and the storage and between the storage and the cache . In alternative embodiments with branching data retrieved from the storage in response to a read miss can be concurrently transferred to both the channel and the cache and a data write can be concurrently transferred from the channel to both a non volatile storage unit and cache . It is appreciated that the number of hosts storage controllers caches and storage drives may vary depending upon the particular application.

A determination is also made as to whether the conditions in the cache are such that the prestage trigger or prestage amount should be modified block . If so the prestage trigger may be modified block as a function of the different speeds of the different drives coupled to the storage controller. discussed below shows an example of such a determination and an example of such a prestage trigger or amount modification as a function of different drives speeds.

In another operation a determination is made as to whether to stage tracks block . If so additional tracks are staged block . discussed below shows an example of such a determination.

In one operation the prestage amount p and the value of the prestage trigger g are initialized block . In this example the prestage amount p may be initialized at 8 tracks and the value of the prestage trigger g may be initialized at distance of 3 tracks for example. Thus it will be seen that when a track is read the prestage trigger of this track is the larger one of 3 or the prestage trigger of the previous track. It is appreciated that other values may be selected for initialization of the prestage amount p and the prestage trigger g depending upon the particular application

A track is read block and a determination is made as to whether block the track being read is on a sequential read stream. If so another determination is made as to whether block the track being read was in connection with a cache miss. If so a determination is made as to whether block a stage issued in connection with the track being read. If so the prestage trigger is increased block as a function of different speeds of different drives and tracks are staged block .

If a stage had issued in connection with the read which resulted in a cache miss the cache miss may have occurred as a result of waiting for that issued stage to complete. In this example the prestage trigger of this track may be increased based on the speed of the drives. For example if the track was read from the hard drive storage the prestage trigger may be increased by 1 for example. If the track was read from a faster drive such as the solid state storage the prestage trigger may be increased by a greater amount such as by 2 for example. Such differentiated increments may facilitate aiding the completion of the issued stages before a cache miss occurs. It is appreciated that other values may be selected for incrementing the prestage trigger g as a function of different drive speeds depending upon the particular application.

Alternatively if a stage had not issued block in connection with the read which resulted in a cache miss the cache miss may have occurred as a result of that track having been demoted before being used. As a result it may be appropriate to delay prestaging additional tracks or to reduce block the amount being prestaged.

In this example both the prestage trigger and the prestage amount may be decreased block as a function of the different speeds of the different drives. For example if the track was read from the hard drive storage the prestage trigger may be decreased by 1 for example and the prestage amount may be reduced by 1 for example. If the track was read from a faster drive such as solid state storage the prestage trigger may be decreased by a greater amount such as by 2 for example and the prestage amount may be reduced by 2 for example. Such differentiated decrements may facilitate reducing the number of candidate tracks for demotion. It is appreciated that other values may be selected for decrementing the prestage trigger g and the prestage amount p as a function of different drive speeds depending upon the particular application. In addition tracks are staged block .

If it is determined block the track being read was a cache hit rather than a cache miss another determination is made as to whether block the track being read is a prestage track candidate for demotion. If so it may be appropriate to reduce the amount of data being prestaged to reduce the number of candidates for demotion. In this embodiment the prestage amount is decreased block as a function of different speeds of different drives.

For example if the track was read from the hard drive storage which is slower than the solid state drive the prestage amount may be decreased by 1 for example. If the track was read from a faster drive such as solid state storage the prestage amount may be decreased by a greater amount such as by 2 for example. Such differentiated decrements may facilitate reducing the number of candidate tracks for demotion. It is appreciated that other values may be selected for decrementing the prestage amount p as a function of different drive speeds depending upon the particular application.

If it is determined block that the track being read is at the last track of a stage group the prestage amount p may be increased block as a function of different speeds of different drives. For example if the track was read from the hard drive storage which is slower than the solid state drive the prestage amount may be increased by 1 for example. If the track was read from a faster drive such as solid state storage the prestage amount may be increased by a larger amount such as by 2 for example. Such differentiated increments may facilitate prestaging sufficient data to reduce cache misses. It is appreciated that other values may be selected for incrementing the prestage amount p as a function of different drive speeds depending upon the particular application.

By increasing the prestage amount p it is intended to decrease the chances of running out of prestaged data and causing a cache miss. However if it is previously determined block that the prestage amount p has reached a maximum such as a maximum of 84 for example the prestage amount p is not increased. It is appreciated that other values may be selected for a maximum depending upon the particular application.

In the illustrated embodiment another determination is made as to whether the size of the prestage trigger has reached or exceeded block the prestage amount p. If so additional tracks may prestaged block . It is believed that issuing additional prestages will permit the prestage trigger g to continue to grow to facilitate avoiding a cache miss due to prestage triggers of insufficient size. It is appreciated that other values may be selected for determining when to prestage additional tracks as a function of prestage trigger size depending upon the particular application.

It is seen that cache prestaging operations in accordance with one aspect of the present description may take into account storage devices of varying speeds and bandwidths for purposes of modifying one or both of a prestage trigger and the prestage amount. Such a capability may be particularly suitable for storage controllers having a mix of solid state storage and hard drive storage for example.

Still further a cache prestaging operation in accordance with another aspect of the present description may decrease one or both of the prestage trigger and the prestage amount in circumstances such as a cache miss which may have resulted from prestaged tracks being demoted before they are used. Conversely a cache prestaging operation in accordance with another aspect of the present description may increase one or both of the prestage trigger and the prestage amount in circumstances such as a cache miss which may have resulted from waiting for a stage to complete.

In yet another aspect the prestage trigger may not be limited by the prestage amount p. Instead the pre stage trigger may be permitted to expand as conditions warrant it by prestaging additional tracks and thereby effectively increasing the potential range for the prestage trigger.

The described operations may be implemented as a method apparatus or computer program product using standard programming and or engineering techniques to produce software firmware hardware or any combination thereof. Accordingly aspects of the embodiments may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore aspects of the embodiments may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon.

Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device interconnected storage devices an array of storage devices multiple memory or storage devices or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable RF etc. or any suitable combination of the foregoing.

Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects of the present invention are described below with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

In certain embodiments the system of may be implemented as a cloud component part in a cloud computing environment. In the cloud computing environment the systems architecture of the hardware and software components involved in the delivery of cloud computing may comprise a plurality of cloud components communicating with each other over a network such as the Internet. For example in certain embodiments the system of may provide clients and other servers and software and or hardware components in the networked cloud with scheduling services.

The terms an embodiment embodiment embodiments the embodiment the embodiments one or more embodiments some embodiments and one embodiment mean one or more but not all embodiments of the present invention s unless expressly specified otherwise.

The terms including comprising having and variations thereof mean including but not limited to unless expressly specified otherwise.

The enumerated listing of items does not imply that any or all of the items are mutually exclusive unless expressly specified otherwise.

Devices that are in communication with each other need not be in continuous communication with each other unless expressly specified otherwise. In addition devices that are in communication with each other may communicate directly or indirectly through one or more intermediaries.

A description of an embodiment with several components in communication with each other does not imply that all such components are required. On the contrary a variety of optional components are described to illustrate the wide variety of possible embodiments of the present invention.

Further although process steps method steps algorithms or the like may be described in a sequential order such processes methods and algorithms may be configured to work in alternate orders. In other words any sequence or order of steps that may be described does not necessarily indicate a requirement that the steps be performed in that order. The steps of processes described herein may be performed in any order practical. Further some steps may be performed simultaneously.

When a single device or article is described herein it will be readily apparent that more than one device article whether or not they cooperate may be used in place of a single device article. Similarly where more than one device or article is described herein whether or not they cooperate it will be readily apparent that a single device article may be used in place of the more than one device or article or a different number of devices articles may be used instead of the shown number of devices or programs. The functionality and or the features of a device may be alternatively embodied by one or more other devices which are not explicitly described as having such functionality features. Thus other embodiments of the present invention need not include the device itself.

The illustrated operations of shows certain events occurring in a certain order. In alternative embodiments certain operations may be performed in a different order modified or removed. Moreover steps may be added to the above described logic and still conform to the described embodiments. Further operations described herein may occur sequentially or certain operations may be processed in parallel. Yet further operations may be performed by a single processing unit or by distributed processing units.

In some embodiments a solid state storage was described as having a substantially faster drive speed or access time as compared to that of a hard drive storage . However it is appreciated that features of the present description are also applicable or beneficial to additional embodiments in which the drive speeds of various drives of the system may be the same as well depending upon the particular application.

The foregoing description of various embodiments of the invention has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise form disclosed. Many modifications and variations are possible in light of the above teaching. It is intended that the scope of the invention be limited not by this detailed description but rather by the claims appended hereto. The above specification examples and data provide a complete description of the manufacture and use of the composition of the invention. Since many embodiments of the invention can be made without departing from the spirit and scope of the invention the invention resides in the claims hereinafter appended.

