---

title: Realtime acoustic adaptation using stability measures
abstract: Methods, systems, and computer programs encoded on a computer storage medium for real-time acoustic adaptation using stability measures are disclosed. The methods include the actions of receiving a transcription of a first portion of a speech session, wherein the transcription of the first portion of the speech session is generated using a speaker adaptation profile. The actions further include receiving a stability measure for a segment of the transcription and determining that the stability measure for the segment satisfies a threshold. Additionally, the actions include triggering an update of the speaker adaptation profile using the segment, or using a portion of speech data that corresponds to the segment. And the actions include receiving a transcription of a second portion of the speech session, wherein the transcription of the second portion of the speech session is generated using the updated speaker adaptation profile.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08849664&OS=08849664&RS=08849664
owner: Google Inc.
number: 08849664
owner_city: Mountain View
owner_country: US
publication_date: 20130716
---
This application is a continuation of U.S. patent application Ser. No. 13 622 576 filed on Sep. 19 2012 which claims the benefit of U.S. Provisional Application Ser. No. 61 655 855 filed Jun. 5 2012 the entirety of which are hereby incorporated by references as if fully set forth therein.

An automated speech recognizer ASR may use a speaker dependent adaptation profile to increase speech recognition performance. A profile may be periodically updated for example at the end of a user s speech session.

In some implementations instead of or in addition to performing profile adaptation at the end of a speech session profile adaptation may be performed within a speech session when useful adaptation data is available. In doing so the ASR may adapt more quickly to current conditions and may improve recognition performance as the speech session progresses. According to an innovative aspect of the present disclosure profile adaptation may be triggered during a speech session when a segment of recognition data becomes stable.

In general one aspect of the subject matter described in this specification may be embodied in methods that include the actions of receiving a transcription of a first portion of a speech session wherein the transcription of the first portion of the speech session is generated using a speaker adaptation profile. The actions further include receiving a stability measure for a segment of the transcription and determining that the stability measure for the segment satisfies a threshold. Additionally the actions include triggering an update of the speaker adaptation profile using the segment or using a portion of speech data that corresponds to the segment. And the actions include receiving a transcription of a second portion of the speech session wherein the transcription of the second portion of the speech session is generated using the updated speaker adaptation profile.

Another aspect of the subject matter may be embodied in methods that include the actions of receiving a first portion of a speech session and decoding the first portion of the speech session to generate a transcription of the first portion of the speech session using a speaker adaptation profile. Further actions include identifying a segment of the transcription of the first portion of the speech session and then determining a stability measure of the segment. Additionally the actions include determining that the stability measure for the segment satisfies a threshold. The actions also include triggering an update of the speaker adaptation profile using the segment or using a portion of speech data that corresponds to the segment. Then the actions include receiving a second portion of the speech session and decoding the second portion of the speech session to generate a transcription of the second portion of the speech session using the updated speaker adaptation profile.

Other embodiments of these aspects include corresponding systems apparatus and computer programs configured to perform the actions of the methods encoded on computer storage devices.

These and other embodiments may each optionally include one or more of the following features. For instance in some implementations a segment comprises a word sub word or group of words. And in some cases the speech session is longer than 1 minute in duration. And in some embodiments the speech session comprises one utterance whereas in other embodiments the speech session comprises multiple utterances.

In certain embodiments the stability measure is based on one or more of an age metric a right context metric and a regression. In certain embodiments the stability measure represents a probability.

In some embodiments triggering an update of the speaker adaptation profile comprises adding the segment to an adaptation queue.

Some embodiments include the further steps of receiving the first portion of the speech session and decoding the first portion of the speech session to generate the transcription of the first portion of the speech session using the speaker adaptation profile. In some aspects the actions further include determining the stability measure of the segment.

In certain embodiments the method further includes receiving the second portion of the speech session and decoding the second portion of the speech session to generate the transcription of the second portion of the speech session using the updated speaker adaptation profile. Certain embodiments further include the action of updating the speaker adaptation profile. In some embodiments updating the speaker adaptation profile includes modifying an acoustic model.

The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features aspects and advantages of the subject matter will become apparent from the description the drawings and the claims.

The process can occur in real time while a speech session is ongoing. The process starts when an automated speech recognizer ASR outputs a transcription of part of the speech session of a user . In this example the transcription of the user s first utterance is The time has come the Walrus said to talk of many things. The ASR uses an initial speaker adaptation profile to generate the transcription .

The ASR then divides the transcription into speech segments and determines a stability measure for each speech segment. As noted below the stability of a speech segment is specific to the time when it is determined and the stability of a given speech segment may increase over time. From the transcription example speech segments and evaluated before time t T 2 are The time with an associated stability measure of 0.90 has with an associated stability measure of 0.95 and come with an associated stability measure of 0.75.

A segment stability evaluator compares the stability measure for each speech segment and with a threshold which in this example has a value of 0.80. The speech segments that satisfy the threshold are then input to an adaptation queue . For example speech segments and have associated stability measures and that are greater than the sample threshold value of 0.80. These speech segments and have been added to the adaptation queue . In contrast by time t T 2 speech segment has an associated stability measure that is still less than the threshold value of 0.80 and therefore was not added to the adaptation queue . An adaptation profile updater retrieves speech segments and or data corresponding to these segments from the adaptation queue to update the speaker adaptation profile.

Based on the stable speech segments and the updated speech adaptation profile can then be used to decode a later portion of the same speech session. Indeed in some implementations the updated speech adaptation profile may be used to decode a later portion of the same utterance. This may allow the ASR to rapidly adapt to current conditions and provide a better user experience. For example the ASR can use the updated speaker adaptation profile to provide a transcription of the second part of the user s speech session. In this example the transcription of the user s second utterance is Of shoes and ships and sealing wax of cabbages and kings. As before the ASR divides the transcription into speech segments and determines a stability measure for each. Example speech segment from the transcription is cabbages with an associated stability measure of 0.85. Advantageously later portions of the speech session such as speech segment may be recognized more accurately based on the updated speaker adaptation profile. The stability measure of the speech segment is then evaluated in the segment stability evaluator and input to the adaptation queue .

The sample process occurs over a time period shown as time t 0 to t T. The transcriptions and are divided to form speech segments and between time t 0 and t T. Each speech segment is then evaluated to determine a stability measure during the corresponding time frame. In the example the sample stability measures were determined between time t 0 and t T 2 and sample stability measure was determined between time t T 2 and time t T.

The stability measures associated with each speech segment may change over time. The stability measure determined for speech segment for example is shown as 0.75 between t 0 and t T 2 but this may change over time as additional speech segments are output. For example at some time after t T 2 the stability measure may be re evaluated and may then exceed the sample threshold of 0.80 at which time the speech segment can be input to the adaptation queue .

The process can continue until the speech session is finished. Moreover as long as the speech session continues stable segments can be identified and used to update the speaker adaptation profile which means that the accuracy of the speaker adaptation profile can continually be improved during the course of the speech session. This may particularly advantageous for longer speech sessions e.g. greater than 1 minute such as dictation or video conferencing applications.

Specific aspects of the process shown in will now be described with reference to the system of . A user speaks into a user device that communicates with an ASR . The user device may be any type of computing device including but not limited to a mobile phone smart phone PDA music player e book reader tablet computer laptop or desktop computer or other stationary or portable device that includes one or more processors and non transitory computer readable media. In some versions one or more components of the ASR may be external to the user device in which case communication between the ASR and the user device may occur over phone and or computer networks including wireless cellular networks wireless local area networks WLAN or Wi Fi networks wired Ethernet networks other wired networks or any combination thereof.

The user s speech session is converted into a series of audio signals that are communicated to the ASR . In particular each portion of the speech session may be encoded into an individual audio signal. For example when the user begins to utter a sentence e.g. The time has come the Walrus said . . . the first portion of the speech session is encoded and sent to the ASR as a first audio signal . The speech session can represent any type of voice communication such as voice based instructions search engine query terms dictation dialogue systems or any other input that uses transcribed speech or that invokes a software application using transcribed speech to perform an action. The speech session can include one or multiple utterances.

The ASR receives and processes the audio signal . For example the ASR may be configured to execute application code associated with a variety of software components e.g. modules objects libraries and services for implementing a speaker adaptation profile a speech decoder a segment stability evaluator and an output module .

As the ASR receives the audio signal the speech decoder recognizes and transcribes the speech session . Periodically the speech decoder also divides the transcription into speech segments and determines stability measures for each of the segments. A speech segment may be for example a sub word a word or a group of words. The segment stability evaluator then compares the stability measures for each of the segments with a threshold to determine whether each segment is stable. Stable segments are then sent to an adaptation profile updater which uses the segments to update the speaker adaptation profile .

Aspects of the ASR will now be described in further detail. The speaker adaptation profile as described herein is a set of models that are adapted to a particular speaker. The set of models may include for example acoustic models language models and vocabulary models that can be adapted to specific speech characteristics of the speaker. In certain embodiments the speaker adaptation profile can start as a set of generic recognition models that are then adapted to the speaker as described herein. In other versions the speaker adaptation profile can be initially trained using a set of known speech inputs so that it is pre adapted to speech characteristics of the speaker and it can then be improved as described herein. The speaker adaptation profile can be any suitable data structure or object and can be stored in any suitable computer readable media. For example the speaker adaptation profile may be a data table or tables or a Java or C object stored in a memory that is accessible by the ASR . A speaker adaptation profile is typically retrieved and loaded into memory by the ASR for each recognized speaker so that it can be used by the speech decoder .

Once the speaker adaptation profile has been loaded by the ASR speech recognition is performed of the audio signal by applying transforms based on the speaker adaptation profile to the signal . The speech decoder may be any suitable module configured to process an input audio signal for speech recognition. In some versions for example the speech decoder may be a computer program executing on the ASR . In other versions the speech decoder may be hardware software or firmware components that communicate with the ASR . For example it may be a server that communicates with the ASR via the Internet or via a wired e.g. Ethernet or wireless e.g. Wi Fi connection.

In operation the speech decoder receives the audio signal and recognizes and transcribes the speech session as text. In some aspects the speech decoder stores the text as speech recognition hypotheses in a buffer as the audio signal is received. As more of the audio signal is received these speech recognition hypotheses also referred to as incremental speech recognition results may be updated in the buffer over time to reflect the additional information. The speech decoder can then use these hypotheses to identify speech segments which may be sub words words or groups of words from the hypotheses. The speech decoder identifies speech segment boundaries based on predetermined parameters of the speech decoder and characteristics of the speaker adaptation profile as would be known to one of skill in the art.

The speech decoder also determines stability measures associated with the segments. One or more stability characteristics can be combined weighted and or normalized to produce a single stability measure for a segment or several stability measures can be used for any given segment. For example one stability characteristic may be based on how long a particular segment e.g. word has persisted as the most likely hypotheses. This may be referred to as an age or persistence metric. Another characteristic may be based on how close in time to the end of the most recent speech recognition hypothesis the segment resides. This can be referred to as a right context of the segment. A further characteristic may be based on a regression used to estimate the probability that a given word is stable given a set of features associated with that word. For example a logistic regression can be used to estimate the probability that a portion of an incremental result is stable. In some aspects one or more of the stability measures can be used to determine segment boundaries and segment stability. It should also be noted that using one or more of these stability characteristics will cause the stability measure of any given segment to vary over time.

The stability measures can be any suitable value or set of values e.g. integers or floating point numbers . In certain aspects the stability measure can represent a probability or a confidence that the associated speech segment is stable which can be derived from one or more of the stability characteristics described above. For example sample stability measures and from are shown as probabilities i.e. having values of 0.0 to 1.0 .

The speech segments and their associated stability measures can be stored by the speech decoder in any suitable memory accessible by the segment stability evaluator . For example they may be stored as Java or C objects in a heap a stack a queue or a collection. While the segment stability evaluator is illustrated in as separate from the speech decoder in some implementations it may be integrated with the speech decoder for example as a component library method or function of the speech decoder .

The segment stability evaluator receives the stability measures from the speech decoder and compares them with a threshold. The threshold may be any suitable value or set of values e.g. integers or floating point numbers that is comparable with the stability measures. In certain aspects the threshold can represent a minimum probability that a speech segment with a given stability measure is stable. The sample threshold from for example is shown as a probability i.e. 0.80 . Non limiting sample values for a threshold probability may be for example about 0.50 about 0.75 about 0.80 about 0.85 about 0.90 about 0.95 or about 0.99. The determination of the threshold is implementation specific and may be for example determined empirically or statistically modeled. However it should be noted that determination of a proper threshold involves a trade off i.e. higher thresholds will more accurately predict stability but will cause less frequent updates of the speaker adaptation profile . Segments that satisfy the threshold e.g. segments that have a stability measure greater than the threshold are referred to as stable segments.

The segment stability evaluator then triggers an update of the speaker adaptation profile for each of the stable segments and or speech data corresponding to those segments. Triggering the update may be done using any suitable technique and may performed synchronously i.e. as the stable segments are identified or asynchronously i.e. queued and processed as resources become available . For example the segment stability evaluator may push the stable segments and or speech data corresponding to those segments to an adaptation queue associated with the speaker adaptation profile. The adaptation queue may be a first in first out queue such as the sample adaptation queue shown in or may be any other suitable component such as a stack. When the adaptation profile updater has processing resources available it can retrieve the segments and or speech data from the adaptation queue. In certain implementations the update may be triggered using any suitable message queuing service such as Java messaging service or Microsoft message queuing. As another example the segment stability evaluator may make synchronous application programming interface API requests to the adaptation profile updater including the stable speech segments and or speech data as parameters.

The adaptation profile updater may be any suitable module configured to receive the stable segments and update the speaker adaptation profile . In some versions for example the adaptation profile updater may be a computer program executing on the ASR . In other versions the adaptation profile updater may be hardware software or firmware components that communicate with the ASR . For example it may be a server that communicates with the ASR via the Internet or via a wired e.g. Ethernet or wireless e.g. Wi Fi connection.

In operation the adaptation profile updater uses the stable segments to update one or more of the models that form the speaker adaptation profile . In certain implementations only the acoustic models of the speaker adaptation profile are updated. For example adaptation profile updater can re adapt the speaker s acoustic model using accumulated stable speech segments based on feature space adaptation model space adaptation or any other suitable technique. In other implementations the language models and or vocabulary models may be updated.

Once the adaptation profile updater generates an updated speaker adaptation profile it loads the updated profile into a suitable memory so that the profile is available for use by the speech decoder to recognize additional speech. An advantage of this approach may be that the accuracy of the speaker adaptation profile can be continually improved and adapted to current acoustic conditions during a speech session. In some implementations the accuracy of the speaker adaptation profile can even be improved and adapted during a single utterance.

The ASR also sends speech segments to an output module that can transmit signals encoding these segments as transcriptions to the user device which can render representations of the transcriptions for the user . While the speech segments are shown in being transmitted from the segment stability evaluator to the output module in some implementations the speech segments may be sent directly from the speech decoder to the output module . The transcriptions can be sent to the user device at particular time intervals or in real time as they are generated. In some implementations both stable and unstable segments are sent whereas in other implementations only stable segments are sent.

In more detail the process begins when the segment stability evaluator receives a transcription of a first portion of a speech session . The speech session may be for example longer than 1 minute in duration with the first portion being some fraction of that duration e.g. 30 seconds . In some instances the speech session may be one utterance and in other cases it may be multiple utterances. In particular a speech decoder receives the first portion of the speech session decodes it to generate the transcription using a speaker adaptation profile and then communicates the transcription to the segment stability evaluator .

As explained above the speech decoder identifies speech segments from the transcription and determines stability measures associated with the speech segments. The speech segments may be a word sub word or group of words identified from the transcription. The segment stability evaluator receives the stability measures associated with segments of the transcription . Although shows this step as being performed after receiving the speech segments it should be appreciated that it may occur before or simultaneously with receiving the speech segments. For example the speech segments and the associated stability measures may both be received in the same data structures. It should also be noted that while the process is described as being performed by the segment stability evaluator one or more steps could be performed by the speech decoder or any other appropriate component of the ASR . Indeed in certain implementations the segment stability evaluator may be a component of the speech decoder .

The segment stability evaluator then determines whether the stability measures for the speech segments satisfy a threshold . In general the threshold may be any suitable value or set of values e.g. integers or floating point numbers that is comparable with the stability measures.

As an example the stability measures may represent probabilities that the associated speech segment is stable and determining whether the threshold is satisfied may involve a comparison of each stability measure with the threshold. In other implementations the stability measures may be time based stability characteristics such as persistence or right context. The stability measures may also be based on a normalized and or weighted combination of for example an age metric a right context metric and or a regression.

When the segment stability evaluator determines that a given segment has a stability measure that satisfies the threshold it triggers an update of the speaker adaptation profile using the segment or a portion of speech data that corresponds to the segment . Triggering the update may be performed synchronously i.e. as the stable segments are identified or asynchronously i.e. queued and processed as resources become available . For example the segment stability evaluator may push the stable segments and or speech data corresponding to those segments to an adaptation queue associated with the speaker adaptation profile. When the adaptation profile updater has resources available it can retrieve the segments and or speech data from the adaptation queue and update the speaker adaptation profile using the segments and or speech data. In some versions the adaptation profile updater updates the speaker adaptation profile by modifying an acoustic model associated with the speaker.

When the segment stability evaluator determines that a given segment is not stable i.e. does not satisfy the threshold it determines whether there are additional segments to evaluate . If so then it receives the additional segments and processes them as described above. If not it determines whether additional transcriptions of portions of the speech session are available . When no additional transcriptions are available the process is complete .

But when additional transcriptions are available the process repeats although it should be noted that in some implementations the process can loop back before reaching the end if additional speech sessions exist. In particular the segment stability evaluator receives a transcription of another portion of the speech session which may be generated using an updated speaker adaptation profile. For example in some instances e.g. long speech sessions for example greater than 1 minute the segment stability evaluator will determine that a speech segment is stable and push the segment to the adaptation queue and the adaptation profile updater will retrieve the segment process the segment and load an updated speaker adaptation profile all prior to the speech decoder receiving an additional portion of the speech session and decoding that additional portion to generate a transcription. In such instances the speech decoder will use the updated speaker adaptation profile for decoding the additional portion of the speech session. For example if the speech session is longer than 1 minute in duration the second portion may be some fraction of that duration e.g. 30 seconds from the latter part of the speech session.

Embodiments of the subject matter the functional operations and the processes described in this specification can be implemented in digital electronic circuitry in tangibly embodied computer software or firmware in computer hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs i.e. one or more modules of computer program instructions encoded on a tangible nonvolatile program carrier for execution by or to control the operation of data processing apparatus. Alternatively or in addition the program instructions can be encoded on an artificially generated propagated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. The computer storage medium can be a machine readable storage device a machine readable storage substrate a random or serial access memory device or a combination of one or more of them.

The term data processing apparatus encompasses all kinds of apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can include special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit . The apparatus can also include in addition to hardware code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them.

A computer program which may also be referred to or described as a program software a software application a module a software module a script or code can be written in any form of programming language including compiled or interpreted languages or declarative or procedural languages and it can be deployed in any form including as a standalone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program may but need not correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.

The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Computers suitable for the execution of a computer program include by way of example can be based on general or special purpose microprocessors or both or any other kind of central processing unit. Generally a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Moreover a computer can be embedded in another device e.g. a mobile telephone a personal digital assistant PDA a mobile audio or video player a game console a Global Positioning System GPS receiver or a portable storage device e.g. a universal serial bus USB flash drive to name just a few.

Computer readable media suitable for storing computer program instructions and data include all forms of nonvolatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user embodiments of the subject matter described in this specification can be implemented on a computer having a display device e.g. a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input. In addition a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user for example by sending web pages to a web browser on a user s client device in response to requests received from the web browser.

Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a front end component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification or any combination of one or more such back end middleware or front end components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN e.g. the Internet.

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

While this specification contains many specific implementation details these should not be construed as limitations on the scope of what may be claimed but rather as descriptions of features that may be specific to particular embodiments. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a subcombination or variation of a subcombination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multitasking and parallel processing may be advantageous. Moreover the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example the processes depicted in the accompanying figures do not necessarily require the particular order shown or sequential order to achieve desirable results. In certain implementations multitasking and parallel processing may be advantageous. Other steps may be provided or steps may be eliminated from the described processes. Accordingly other implementations are within the scope of the following claims.

