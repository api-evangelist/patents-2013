---

title: System, method, and computer program product for management of dependency between tasks
abstract: A system, method, and computer program product for management of dynamic task-dependency graphs. The method includes the steps of generating a first task data structure in a memory for a first task, generating a second task data structure in the memory, storing a pointer to the second task data structure in a first output dependence field of the first task data structure, setting a reference counter field of the second task data structure to a threshold value that indicates a number of dependent events associated with the second task, and launching the second task when the reference counter field stores a particular value. The second task data structure is a placeholder for a second task that is dependent on the first task.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09286119&OS=09286119&RS=09286119
owner: NVIDIA Corporation
number: 09286119
owner_city: Santa Clara
owner_country: US
publication_date: 20130213
---
The present invention relates to task management and more particularly to management of dependency between tasks.

Modern operating systems enable various processors to be multi tasking processors. In other words two or more tasks can be executed substantially simultaneously. Typically the operating system implements a task scheduling kernel that manages which tasks are executed by the processor. The priority scheduling algorithm may implement round robin scheduling fixed priority pre emptive scheduling or other types of scheduling algorithms well known in the art. Conventionally the scheduling algorithm is implemented via task objects i.e. data structures that are managed by a software kernel. Many operating systems implement a process priority with each process currently active within the operating system. For example Microsoft Windows assigns one of 32 priority levels to each launched process and a task manager kernel implements a multi level feedback queue to manage the scheduling of the active processes for execution.

Many modern processors implement hardware based scheduling units that enable multiple tasks to be queued in hardware and dispatched for execution based on a particular scheduling algorithm implemented by the scheduling unit. However in most systems prioritizing processes for execution is still managed by software running in the background. For example a hardware scheduling unit may be configured to execute whichever process has the highest priority. However a software kernel is configured to adjust the priority level of each process to ensure that each process is allowed to execute and that dependent processes are executed in the correct order.

It will be appreciated that latency is introduced by the management of scheduling priority in software. The task scheduling kernel must be executed periodically to update priority levels in order to adjust which processes are being executed by the processor. In addition by including software management of priority levels the hardware scheduling unit must repeatedly make memory access requests to update priority levels of the different tasks. This latency creates a bottleneck that slows down the efficiency of the processor. Thus there is a need for addressing this issue and or other issues associated with the prior art.

A system method and computer program product for management of dynamic task dependency graphs. The method includes the steps of generating a first task data structure in a memory for a first task generating a second task data structure in the memory storing a pointer to the second task data structure in a first output dependence field of the first task data structure setting a reference counter field of the second task data structure to a threshold value that indicates a number of dependent events associated with the second task and launching the second task when the reference counter field stores a particular value. The second task data structure is a placeholder for a second task that is dependent on the first task.

A hardware scheduling mechanism for a multi threaded processor is described below. The hardware scheduling mechanism provides a means to implement a variety of different parallelization techniques such as scheduling streams and events. The hardware scheduling mechanism implements a dynamic task dependency graph that includes a plurality of task data structures implemented as a linked list for each stream of tasks executed by the processor. Each task in a stream is launched when a reference counter reaches a trigger value such as zero. The tasks in each stream may be generated remotely by a central processing unit that generates tasks in memory and then sends a link to the task to the processor or the tasks in each stream may be generated locally by threads executing in the processor. The streams each include a placeholder data structure that reserves a place in the stream for new tasks to be added to the stream. Each task in a stream is only launched once software has generated the task associated with the task data structure and when one or more predecessor tasks in the stream have finished executing. Synchronization between streams may be implemented by creating dependencies for a task in one stream to an event executed by another stream. The dependent task will not be executed until the occurrence of the event in the other stream.

At step a pointer to the second task data structure is stored in a first output dependence field of the first task data structure. The pointer indicates that the reference counter field of the second task data structure should be decremented when execution of the first task has completed. At step a reference counter field of the second task data structure is set to an initial value that indicates a number of input dependencies associated with the second task. In one embodiment the reference counter for the second task data structure is initialized to two indicating that two dependencies are associated with the second task. The second task data structure is dependent on software to generate the second task in the memory. The second task data structure is also dependent on the completed execution of the first task. In some embodiments the second task data structure may include additional dependencies such as dependencies related to the occurrence of events in another stream.

At step the second task is launched when the reference counter field in the second task data structure stores a particular value. In one embodiment tasks are launched when the reference counter field in the task data structure for the task reaches zero. The reference counter field is decremented by software when the task associated with the task data structure is stored in memory i.e. when software is done generating the task and when any predecessor tasks that store a pointer to the task data structure in one of the predecessor task s output dependence field has completed execution.

It should be noted that while various optional features are set forth herein in connection with managing dynamic task dependency graphs such features are for illustrative purposes only and should not be construed as limiting in any manner. In one embodiment the scheduling mechanism described above is implemented in a parallel processing unit.

In one embodiment the PPU includes an input output I O unit configured to transmit and receive communications i.e. commands data etc. from a central processing unit CPU not shown over the system bus . The I O unit may implement a Peripheral Component Interconnect Express PCIe interface for communications over a PCIe bus. In alternative embodiments the I O unit may implement other types of well known bus interfaces.

The PPU also includes a host interface unit that decodes the commands and transmits the commands to the task management unit or other units of the PPU e.g. memory interface as the commands may specify. The host interface unit is configured to route communications between and among the various logical units of the PPU .

In one embodiment a program encoded as a command stream is written to a buffer by the CPU. The buffer is a region in memory e.g. memory or system memory that is accessible i.e. read write by both the CPU and the PPU . The CPU writes the command stream to the buffer and then transmits a pointer to the start of the command stream to the PPU . The host interface unit provides the task management unit TMU with pointers to one or more streams. The TMU selects one or more streams and is configured to organize the selected streams as a pool of pending grids. The pool of pending grids may include new grids that have not yet been selected for execution and grids that have been partially executed and have been suspended.

A work distribution unit that is coupled between the TMU and the SMs manages a pool of active grids selecting and dispatching active grids for execution by the SMs . Pending grids are transferred to the active grid pool by the TMU when a pending grid is eligible to execute i.e. has no unresolved data dependencies. An active grid is transferred to the pending pool when execution of the active grid is blocked by a dependency. When execution of a grid is completed the grid is removed from the active grid pool by the work distribution unit . In addition to receiving grids from the host interface unit and the work distribution unit the TMU also receives grids that are dynamically generated by the SMs during execution of a grid. These dynamically generated grids join the other pending grids in the pending grid pool.

In one embodiment the CPU executes a driver kernel that implements an application programming interface API that enables one or more applications executing on the CPU to schedule operations for execution on the PPU . An application may include instructions i.e. API calls that cause the driver kernel to generate one or more grids for execution. In one embodiment the PPU implements a SIMD Single Instruction Multiple Data architecture where each thread block i.e. warp in a grid is concurrently executed on a different data set by different threads in the thread block. The driver kernel defines thread blocks that are comprised of k related threads such that threads in the same thread block may exchange data through shared memory. In one embodiment a thread block comprises 32 related threads and a grid is an array of one or more thread blocks that execute the same stream and the different thread blocks may exchange data through global memory.

In one embodiment the PPU comprises X SMs X . For example the PPU may include 15 distinct SMs . Each SM is multi threaded and configured to execute a plurality of threads e.g. 32 threads from a particular thread block concurrently. Each of the SMs is connected to a level two L2 cache via a crossbar or other type of interconnect network . The L2 cache is connected to one or more memory interfaces . Memory interfaces implement 16 32 64 128 bit data buses or the like for high speed data transfer. In one embodiment the PPU comprises U memory interfaces U where each memory interface U is connected to a corresponding memory device U . For example PPU may be connected to up to 6 memory devices such as graphics double data rate version 5 synchronous dynamic random access memory GDDR5 SDRAM .

In one embodiment the PPU implements a multi level memory hierarchy. The memory is located off chip in SDRAM coupled to the PPU . Data from the memory may be fetched and stored in the L2 cache which is located on chip and is shared between the various SMs . In one embodiment each of the SMs also implements an L1 cache. The L1 cache is private memory that is dedicated to a particular SM . Each of the L1 caches is coupled to the shared L2 cache . Data from the L2 cache may be fetched and stored in each of the L1 caches for processing in the functional units of the SMs .

In one embodiment the PPU comprises a graphics processing unit GPU . The PPU is configured to receive commands that specify shader programs for processing graphics data. Graphics data may be defined as a set of primitives such as points lines triangles quads triangle strips and the like. Typically a primitive includes data that specifies a number of vertices for the primitive e.g. in a model space coordinate system as well as attributes associated with each vertex of the primitive. The PPU can be configured to process the graphics primitives to generate a frame buffer i.e. pixel data for each of the pixels of the display . The driver kernel implements a graphics processing pipeline such as the graphics processing pipeline defined by the OpenGL API.

An application writes model data for a scene i.e. a collection of vertices and attributes to memory. The model data defines each of the objects that may be visible on a display. The application then makes an API call to the driver kernel that requests the model data to be rendered and displayed. The driver kernel reads the model data and writes commands to the buffer to perform one or more operations to process the model data. The commands may encode different shader programs including one or more of a vertex shader hull shader geometry shader pixel shader etc. For example the TMU may configure one or more SMs to execute a vertex shader program that processes a number of vertices defined by the model data. In one embodiment the TMU may configure different SMs to execute different shader programs concurrently. For example a first subset of SMs may be configured to execute a vertex shader program while a second subset of SMs may be configured to execute a pixel shader program. The first subset of SMs processes vertex data to produce processed vertex data and writes the processed vertex data to the L2 cache and or the memory . After the processed vertex data is rasterized i.e. transformed from three dimensional data into two dimensional data in screen space to produce fragment data the second subset of SMs executes a pixel shader to produce processed fragment data which is then blended with other processed fragment data and written to the frame buffer in memory . The vertex shader program and pixel shader program may execute concurrently processing different data from the same scene in a pipelined fashion until all of the model data for the scene has been rendered to the frame buffer. Then the contents of the frame buffer are transmitted to a display controller for display on a display device.

The PPU may be included in a desktop computer a laptop computer a tablet computer a smart phone e.g. a wireless hand held device personal digital assistant PDA a digital camera a hand held electronic device and the like. In one embodiment the PPU is embodied on a single semiconductor substrate. In another embodiment the PPU is included in a system on a chip SoC along with one or more other logic units such as a reduced instruction set computer RISC CPU a memory management unit MMU a digital to analog converter DAC and the like.

In one embodiment the PPU may be included on a graphics card that includes one or more memory devices such as GDDR5 SDRAM. The graphics card may be configured to interface with a PCIe slot on a motherboard of a desktop computer that includes e.g. a northbridge chipset and a southbridge chipset. In yet another embodiment the PPU may be an integrated graphics processing unit iGPU included in the chipset i.e. Northbridge of the motherboard.

As described above the work distribution unit dispatches active grids for execution on one or more SMs of the PPU . The scheduler unit receives the grids from the work distribution unit and manages instruction scheduling for one or more thread blocks of each active grid. The scheduler unit schedules threads for execution in groups of parallel threads where each group is called a warp. In one embodiment each warp includes 32 threads. The scheduler unit may manage a plurality of different thread blocks allocating the thread blocks to warps for execution and then scheduling instructions from the plurality of different warps on the various functional units i.e. cores DPUs SFUs and LSUs during each clock cycle.

In one embodiment each scheduler unit includes one or more instruction dispatch units . Each dispatch unit is configured to transmit instructions to one or more of the functional units. In the embodiment shown in the scheduler unit includes two dispatch units that enable two different instructions from the same warp to be dispatched during each clock cycle. In alternative embodiments each scheduler unit may include a single dispatch unit or additional dispatch units .

Each SM includes a register file that provides a set of registers for the functional units of the SM . In one embodiment the register file is divided between each of the functional units such that each functional unit is allocated a dedicated portion of the register file . In another embodiment the register file is divided between the different warps being executed by the SM . The register file provides temporary storage for operands connected to the data paths of the functional units.

Each SM comprises L processing cores . In one embodiment the SM includes a large number e.g. 192 etc. of distinct processing cores . Each core is a fully pipelined single precision processing unit that includes a floating point arithmetic logic unit and an integer arithmetic logic unit. In one embodiment the floating point arithmetic logic units implement the IEEE 754 2008 standard for floating point arithmetic. Each SM also comprises M DPUs that implement double precision floating point arithmetic N SFUs that perform special functions e.g. copy rectangle pixel blending operations and the like and P LSUs that implement load and store operations between the shared memory L1 cache and the register file . In one embodiment the SM includes 64 DPUs 32 SFUs and 32 LSUs .

Each SM includes an interconnect network that connects each of the functional units to the register file and the shared memory L1 cache . In one embodiment the interconnect network is a crossbar that can be configured to connect any of the functional units to any of the registers in the register file or the memory locations in shared memory L1 cache .

In one embodiment the SM is implemented within a GPU. In such an embodiment the SM comprises J texture units . The texture units are configured to load texture maps i.e. a 2D array of texels from the memory and sample the texture maps to produce sampled texture values for use in shader programs. The texture units implement texture operations such as anti aliasing operations using mip maps i.e. texture maps of varying levels of detail . In one embodiment the SM includes 16 texture units .

The PPU described above may be configured to perform highly parallel computations much faster than conventional CPUs. Parallel computing has advantages in graphics processing data compression biometrics stream processing algorithms and the like.

The TDR unit is configured to resolve dependencies. The TDR unit tracks input dependencies i.e. preceding tasks or events on which the current task is dependent . Once the input dependencies have been resolved the task may then be sent to the SL unit to be scheduled and launched. The SL unit is configured to schedule available tasks for execution on one of the SMs . In one embodiment the SL unit may schedule tasks based on a priority level associated with the task. The priority level may be stored in a field of the TMD for the task. When a task is received by the SL unit the SL unit readies the task to be passed down the pipe to the WDU . The SL unit may fetch task state data including data specified by the TMD into the task cache . Data may be fetched from memory and temporarily stored in task cache . In one embodiment the SL unit pre fetches task state data into the task cache when a task is transmitted to the SL unit by the TDR unit . In other words once a task s dependencies have been resolved and the task is ready for scheduling the SL unit is configured to pre fetch task state data from memory to prepare for the task to be scheduled and launched. In another embodiment the TDR unit may fetch task state data into the task cache .

In one embodiment the task data structure includes a TMD pointer field TMDPointer that stores a pointer to a TMD for the task associated with the task data structure . The TMD is a separate data structure that stores the information associated with a task including but not limited to a task identifier i.e. a unique index for the task a pointer to the location of a program kernel associated with the task a pointer to the location of data to be processed by the program kernel and so forth. In another embodiment various fields of the task data structure may be included within the TMD data structure. For example the reference counter field and the output dependence fields may be included as additional fields within the TMD data structure rather than having separate data structures for the tasks i.e. TMDs and the task state data for managing dependencies between tasks i.e. scheduler objects . In some embodiments the task data structure may include one or more additional fields such as a priority level for the tasks per the desires of the user.

The TDR unit is configured to manage task dependencies by incrementing and decrementing reference counters in each of the tasks and linking dependent tasks using the output dependence fields . The TDR unit may manage one or more streams of tasks concurrently. A stream of tasks is an ordered list of tasks where each subsequent task in the stream is dependent on a predecessor task in the stream. When a task in a stream has completed execution i.e. the TDR unit receives a message from an SM that indicates the SM has completed execution of the task the TDR unit is configured to follow each of the pointers in the output dependence fields for the completed task and decrement the reference counter fields in each of the dependent tasks.

When a reference counter field associated with a task data structure is equal to a trigger value e.g. 0 the TDR unit transmits the task associated with the task data structure to the SL unit to be scheduled and launched. The reference counter field enables a task to be dependent on the occurrence of one or more conditions delaying the launch of the task until each of the conditions has been met. For example when a task data structure is generated the task may not have been populated in memory by software. The task may also be dependent on the completed execution of a predecessor task. Thus when the task data structure is initialized the reference counter field may be set to an initial value of two to indicate that the task has two dependencies i.e. software must populate the task in memory and the predecessor task must complete execution .

In addition tasks may be dependent on additional events. For example a task in one stream can be synchronized based on the occurrence of an event in another stream. If the event has not occurred then the dependent task will not be launched by the TMU until the event has occurred. Software may increment the reference counter field for each event that the task is dependent on. For example in addition to the task being dependent on software populating the task and a predecessor task in the stream completing execution a task may also be dependent on one or more events being recorded in one or more additional streams executing concurrently in PPU . For each additional event that a task is dependent on software may increment the reference counter field in the task data structure associated with the task.

As shown in the reference counter field for the first task data structure is one at this time because software has already generated the TMD for the first task and the Task  is dependent on one predecessor task. Once the predecessor task in the stream has completed execution the reference counter field for the first task data structure will be decremented to zero and the first task will be scheduled and launched by the SL unit . It will be appreciated that if Task  is the first task in the stream then the reference counter field may be initialized to one and decremented to zero once software has generated the TMD for the first task thereby allowing the first task to be scheduled and launched by the SL unit .

It will be appreciated that the dynamic task dependency graphs shown in are essentially linked lists that store task state information for different tasks in each entry of the linked list. In some embodiments PPU may track and manage multiple streams concurrently and the multiple streams may be interdependent. For example the execution of a task in one stream may depend on the occurrence of an event in another stream. An event mechanism is described below which implements interdependencies among different streams.

As shown in a first stream includes a first task data structure Task  and a placeholder task data structure Task . A pointer to the placeholder task data structure is stored in the first output dependence field of the first task data structure . The first task data structure in the first stream includes a pointer to a TMD for the first task in the first stream TMD . The reference counter field for the first task data structure has been decremented to one which indicates that the first task will be ready to launch once a predecessor task has completed execution. Similarly a second stream includes a first task data structure Task  and a placeholder task data structure Task . The first task data structure in the second stream includes a pointer to a TMD for the first task in the second stream TMD . The reference counter field for the first task data structure has been decremented to one which indicates that the first task will be ready to launch once a predecessor task has completed execution. illustrates two concurrently executing streams. As shown in the streams are not dependent on each other.

In order to manage dependencies between concurrently executing streams record events may be added to a stream. As shown in a first record event may be added to the placeholder task data structure in stream . In one embodiment a pointer to a record event object Event  may be added to the TMD pointer field in the placeholder task data structure . The record event object may be a small kernel of code that is executed by the SM when a record event occurs. In another embodiment a special value may be stored in the TMD pointer field that indicates that the task is a record event and no kernel of code is executed for the task. In such cases the TDR unit does not transmit the task to the SL unit for scheduling and launch. In addition a new placeholder task data structure Task  is added to the end of the first stream and a pointer to the new task data structure is added to the first output dependence field of the record event task data structure .

When a record event is added to a stream an event stream is generated such that one or more additional streams may synchronize upon the occurrence of the record event. When the event stream is generated i.e. when software adds a record event to a stream a placeholder task data structure Task  is generated and added to the event stream . The reference counter field for the placeholder task data structure is initiated to a value of two. In addition a pointer to the placeholder task data structure is stored in the second output dependence field of the record event task data structure . Consequently after the record event is executed i.e. the reference counter field in the record event task data structure reaches the trigger value the reference counter fields for both the new placeholder task data structure and the wait event placeholder task data structure are decremented.

As shown in tasks may then be added to other streams which depend on wait events in the event stream that are triggered by the occurrence of the record event in the first stream . In order to synchronize a task in a second stream with the occurrence of the record event in the first stream a wait event Wait  is added to the event stream in the wait event placeholder task data structure . A task dependent upon the occurrence of the wait event is added to stream in the placeholder task data structure and a new placeholder task data structure is appended to the end of the second stream . In addition a new wait event placeholder task data structure is appended to the end of the event stream . A pointer to the new wait event placeholder task data structure is stored in the first output dependence field of the wait event task data structure . In addition a pointer to the dependent task data structure is stored in the second output dependence field of the wait event task data structure . The reference counter field of the dependent task is incremented by one to indicate that the task is dependent upon the occurrence of the wait event.

Multiple tasks may be dependent upon the occurrence of the same record event. For each task that is dependent upon the event a new wait event is appended to the associated event stream . Furthermore a single task may be dependent upon multiple events. For example Task  may also be dependent on the occurrence of another event in a third stream not explicitly shown . In that case a separate event stream associated with the other record event may be generated and a second wait event task data structure may be pointed to Task  . The reference counter field of Task  may be incremented again i.e. from 3 to 4 to indicate that the task is dependent on the other wait event.

In one embodiment software may implement an application programming interface API that can synchronize PPU using different API calls. In one API call DeviceSync each stream executing on PPU must finish executing before any additional streams are launched. DeviceSync may be implemented by the host interface unit waiting to receive an idle signal from the TMU that indicates that all pending streams have finished execution. In another API call StreamSync software may indicate the end of a stream of execution by generating a null task in the last placeholder task data structure for the stream. When the TDR unit encounters a null task the TDR unit may reference a semaphore field in the task data structure which stores a pointer to a semaphore location. The TDR unit may then release the semaphore associated with the stream indicating to the host interface unit that the stream has completed execution. In yet another API call EventSync when the TDR unit encounters a record event the TDR unit may reference a semaphore field in the task data structure which stores a pointer to a semaphore location. The TDR unit may then release the semaphore associated with the event indicating to the host interface unit that the event has occurred. These three mechanisms allow software executing on the host machine to synchronize with the execution of tasks on the PPU . Although semaphores have been implemented as a method of communication between the hardware and software other techniques may be implemented in lieu of using semaphores. For example a generic memory operation may be used to transfer information between software executing on a CPU and hardware executing the tasks.

The system also includes input devices a graphics processor and a display i.e. a conventional CRT cathode ray tube LCD liquid crystal display LED light emitting diode plasma display or the like. User input may be received from the input devices e.g. keyboard mouse touchpad microphone and the like. In one embodiment the graphics processor may include a plurality of shader modules a rasterization module etc. Each of the foregoing modules may even be situated on a single semiconductor platform to form a graphics processing unit GPU .

In the present description a single semiconductor platform may refer to a sole unitary semiconductor based integrated circuit or chip. It should be noted that the term single semiconductor platform may also refer to multi chip modules with increased connectivity which simulate on chip operation and make substantial improvements over utilizing a conventional central processing unit CPU and bus implementation. Of course the various modules may also be situated separately or in various combinations of semiconductor platforms per the desires of the user.

The system may also include a secondary storage . The secondary storage includes for example a hard disk drive and or a removable storage drive representing a floppy disk drive a magnetic tape drive a compact disk drive digital versatile disk DVD drive recording device universal serial bus USB flash memory. The removable storage drive reads from and or writes to a removable storage unit in a well known manner.

Computer programs or computer control logic algorithms may be stored in the main memory and or the secondary storage . Such computer programs when executed enable the system to perform various functions. The memory the storage and or any other storage are possible examples of computer readable media.

In one embodiment the architecture and or functionality of the various previous figures may be implemented in the context of the central processor the graphics processor an integrated circuit not shown that is capable of at least a portion of the capabilities of both the central processor and the graphics processor a chipset i.e. a group of integrated circuits designed to work and sold as a unit for performing related functions etc. and or any other integrated circuit for that matter.

Still yet the architecture and or functionality of the various previous figures may be implemented in the context of a general computer system a circuit board system a game console system dedicated for entertainment purposes an application specific system and or any other desired system. For example the system may take the form of a desktop computer laptop computer server workstation game consoles embedded system and or any other type of logic. Still yet the system may take the form of various other devices including but not limited to a personal digital assistant PDA device a mobile phone device a television etc.

Further while not shown the system may be coupled to a network e.g. a telecommunications network local area network LAN wireless network wide area network WAN such as the Internet peer to peer network cable network or the like for communication purposes.

While various embodiments have been described above it should be understood that they have been presented by way of example only and not limitation. Thus the breadth and scope of a preferred embodiment should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

