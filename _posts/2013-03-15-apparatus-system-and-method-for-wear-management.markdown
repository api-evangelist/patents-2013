---

title: Apparatus, system, and method for wear management
abstract: A storage module is configured to determine a health metric of a storage division of a solid-state storage medium. The health metric may comprise a combination of factors, including, but not limited to: wear level, performance (e.g., program time, erase time, and the like), error rate, and the like. A wear level module may configure storage operations to reduce the wear rate of storage divisions having poor health metrics and/or heath metrics that are degrading more quickly than other storage divisions. Reducing wear rate may include deferring grooming operations, delaying use for storage operations, temporarily retiring the storage division, or the like. Storage divisions may be brought back into service at normal use rates in response determining that other portions of the storage media have been worn to the point that they exhibit similar health and/or reliability characteristics.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09063874&OS=09063874&RS=09063874
owner: SanDisk Technologies, Inc.
number: 09063874
owner_city: Plano
owner_country: US
publication_date: 20130315
---
This application is a continuation in part of and claims priority to U.S. patent application Ser. No. 12 616 124 entitled Apparatus System and Method for Predicting Failures in Solid State Storage filed Nov. 10 2009 for David Flynn et al. which claims priority to U.S. Provisional Patent Application No. 61 112 955 entitled Apparatus System and Method for Predicting Errors in Solid State Storage filed Nov. 10 2008 for David Flynn et al. this application also claims priority to and is a continuation in part of U.S. patent application Ser. No. 13 724 812 entitled Apparatus System and Method for Managing Solid State Storage Reliability filed Dec. 21 2012 for Warner Losh et al. and which claims priority to U.S. Provisional Patent Application No. 61 652 745 entitled Apparatus System and Method for Managing Storage Division Retirement filed on May 29 2012 each of which are hereby incorporated by reference.

This disclosure relates to apparatus systems and methods for managing a solid state storage medium and in particular to managing wear of various portions of the solid state storage medium.

Wear leveling a solid state storage medium may comprise adapting storage operations to evenly distribute storage operations between different portions of the medium. However different portions of the solid state storage medium may wear at different rates due to inter alia physical characteristics of the solid state storage medium quality signal path attenuation manufacturing variations and so on. Accordingly the wear level alone may not be an accurate gauge of the reliability and or remaining useful life.

Disclosed herein are embodiments of a method for managing wear on a solid state storage medium. The disclosed methods may comprise one or more machine executable operations and or steps. The disclosed operations and or steps may be embodied as program code stored on a computer readable storage medium. Accordingly embodiments of the methods disclosed herein may be embodied as a computer program product comprising a computer readable storage medium storing computer usable program code executable to perform one or more method operations and or steps.

Embodiments of the disclosed method may comprise determining an overall reliability metric of a solid state storage medium based on reliability metrics of a plurality of different sections of the solid state storage medium identifying a section of the solid state storage medium having a reliability metric that differs from the overall reliability metric by more than a threshold and managing storage operations performed on the solid state storage medium to adjust wear of the identified section in response to the storage operations. Managing the storage operations may include relocating valid data from the identified section and deferring a reclamation operation on the identified section and or identifying a storage request pertaining to cold data and storing the cold data on the identified section.

In some embodiments the method includes indicating that the identified section is out of service. The method may further comprise indicating that the identified section is in service and available for storing data in response to determining that a differential between the reliability metric of the identified section and a new overall reliability metric of the solid state storage medium determined after performing a plurality of storage operations on the solid state storage medium is within the threshold.

Embodiments of the method may comprise determining an updated overall reliability metric of the solid state storage medium subsequent to identifying the section and managing storage operations performed on the solid state storage medium to adjust a wear rate of the identified section in response to determining that the reliability metric of the identified section is within the threshold of the updated overall reliability metric.

In some embodiments the method includes recording indications of reduced wear sections of the solid state storage medium accessing the indications to schedule storage operations on the solid state storage medium and or recording the indications on a non volatile storage medium. The reliability metric may be based on one or more of a raw bit error rate of the section a program time of the section an erase time of the section and a program count of the section.

Disclosed herein are embodiments of an apparatus comprising a reliability analysis module configured to determine an aggregate health metric corresponding to respective health metrics of storage divisions of a solid state storage medium and to identify a set of one or more storage divisions having health metrics that deviate from the aggregate health metric by more than a threshold and a wear management module configured to adapt storage operations performed on the identified set of storage divisions to manage usage of the identified set of storage divisions.

The apparatus may further comprise a reliability monitoring module configured to determine the respective health metrics of a plurality of storage divisions of a solid state storage device. The storage division health metrics are based on performance characteristics of the storage divisions. In some embodiments the apparatus further includes a groomer module configured to prepare storage divisions for use by relocating valid data from the storage divisions and initializing the storage divisions. The groomer module may be configured to relocate valid data from one of the identified set of storage divisions and to delay initializing the storage division.

Embodiments of the apparatus may include a log storage module configured to queue storage divisions for data storage operations. The log storage module may be configured to remove one or more of the identified set of storage divisions from the queue. Alternatively or in addition the log storage module may be configured to queue one or more of the identified storage divisions in a reduced usage queue and to schedule a storage operation on a storage division in the reduced usage queue in response to receiving a storage request pertaining to low usage data. The log storage module may be configured to queue the storage divisions in an ordered queue and to reorder one or more of the identified set of storage divisions to an end of the ordered queue.

The reliability analysis module may be configured to remove a storage division from the identified set of storage divisions in response to determining that a deviation between the health metric of the storage division and an updated aggregate health metric is less than the threshold.

Disclosed herein are embodiments of a system comprising means for identifying a first set of a plurality of solid state storage sections having reliability characteristics that differ from reliability characteristics of a second set of the plurality of solid state storage sections by more than a threshold means for receiving a plurality of requests to perform storage operations on the solid state storage sections and means for modifying a sequential storage pattern for the requested storage operations to vary usage of the first set of solid state storage sections as compared to usage of the second set of solid state storage sections. Embodiments of the system may further include means for determining reliability characteristics of the storage sections comprising means for monitoring one or more of error rates of the storage sections program times of the storage sections and erase times of the storage sections.

The disclosed system may further comprise means for determining updated reliability characteristics of the others of the solid state storage sections and means for selecting a storage section for removal from the first set and inclusion in the second set in response to determining that reliability characteristics of the selected storage section are within a deviation threshold of the updated reliability characteristics of the solid state storage divisions in the second set.

Embodiments of the disclosed system may include means for grooming solid state storage sections for reuse comprising means for delaying reuse of storage sections in the first set and or means for queuing the storage operations on the solid state storage sections configured to prevent storage operations from being queued to the solid state storage sections in the first set.

The solid state storage medium may comprise non volatile solid state storage media such as flash memory nano random access memory nano RAM or NRAM nanocrystal wire based memory silicon oxide based sub 10 nanometer process memory graphene memory Silicon Oxide Nitride Oxide Silicon SONOS Resistive Random Access Memory RRAM Programmable Metallization Cell PMC Conductive Bridging RAM CBRAM Magneto Resistive RAM MRAM Dynamic RAM DRAM Phase change RAM PRAM or the like. The storage controller may be configured to write data to and or read data from the solid state storage medium via a bus . The bus may comprise a storage I O bus for communicating data to from the solid state storage medium and may further comprise a control I O bus for communicating addressing and other command and control information to the solid state storage medium .

The solid state storage medium may comprise a plurality of storage units which may be organized and or partitioned into respective sections . As used herein a storage unit or physical storage unit refers to one or more physical storage locations on the solid state storage medium . A storage unit may refer to any unit of storage including but not limited to a page a group collection or set of pages e.g. a logical page a sector a block or the like. The storage module may be configured to manage groups of storage units in various sections which may be referred to as storage divisions. As used herein a storage division refers to a particular portion or section of a solid state storage medium which may include a group collection and or set of storage units. Accordingly a storage division may refer to one or more of an erase block a group collection and or set of erase blocks e.g. logical erase block or the like. As illustrated in the solid state storage medium may comprise a plurality of storage sections A N each of which may exhibit different reliability and or health characteristics.

As disclosed in further detail herein the storage module may be configured to perform storage operations on logical storage units. As used herein a logical storage unit refers to a group of two or more storage units such as a group of physical pages. The storage module may be configured to perform storage operations on the two or more storage units in parallel. In some embodiments the storage module may be configured to store data structures such as data segments packets ECC codewords or the like on two or more of the physical storage units of a logical storage unit.

The storage module may comprise and or be implemented on a computing device . The computing device may comprise a processor volatile memory and or persistent storage . The processor may comprise one or more general and or special purpose processing elements. The processor may be configured to execute instructions loaded into the volatile memory from the persistent storage . Portions of one or more of the modules of the storage module may be embodied as machine readable instructions stored on the persistent storage . The instructions may be configured for execution by the processor to implement one or more of the modules and or methods described herein.

Portions of the storage module and or the components and modules thereof may be implemented using hardware components connected to the computing device using a system bus such as a peripheral component interconnect express PCI e bus a Serial Advanced Technology Attachment serial ATA bus USB connection an Institute of Electrical and Electronics Engineers IEEE 1394 bus FireWire an external PCI bus Infiniband and or the like. Portions of the storage module and or the components and modules thereof may be embodied as computer readable instructions configured for execution by the processor of the computing device. The storage module and or storage controller may comprise one or more kernel level applications drivers and or services user level applications libraries interfaces and or the like.

The storage module may be configured to provide storage services to one or more storage clients through inter alia the storage interface . The storage interface may comprise a block device interface a storage layer a virtual storage interface VSL or other suitable storage interface and or Application Programming Interface API for providing I O services.

In some embodiments the storage module includes a logical to physical translation layer to map and or associate identifiers of the storage client with storage units of the solid state storage medium . The logical to physical translation layer may provide for any to any mappings between logical identifiers and physical storage locations such that data may be written and or updated out of place on the solid state storage medium . As used herein a physical address refers to an address or other reference capable of referencing a particular storage location on the solid state storage medium . Accordingly a physical address may be a media address. 

The storage module may be configured to maintain metadata pertaining to solid state storage medium including but not limited to an index comprising the any to any mappings between logical identifiers and physical storage locations of the solid state storage medium a reverse index pertaining to the contents of the solid state storage medium one or more validity bitmaps reliability characteristics health characteristics status metadata and so on. The metadata may be stored on the volatile memory and or may be periodically stored on a persistent storage medium such as the persistent storage and or solid state storage medium .

The solid state storage medium may be subject to error and or failure conditions. These conditions may result in data errors as data is written to and or read from the solid state storage medium . Such errors may arise due to a number of factors which may include but are not limited to wear over programming read disturb write disturb erase disturb programming errors charge gain charge loss charge leaking de trapping and so on. Moreover the solid state storage medium may have a limited useable lifetime. The projected useful lifetime of the solid state storage medium may be expressed in terms of the maximum number of program erase PE cycles the solid state storage medium can endure before failing a manufacturer may represent for example that the mean time to failure for storage the solid state storage medium is 200 000 PE cycles. Therefore as used herein the wear level of storage section may correspond to the number of PE cycles that have been performed on the section and or a ratio of PE cycles to a PE cycle threshold. The PE cycle threshold may correspond to a mean PE cycles to failure for the solid state storage medium or other threshold value.

The storage module may comprise a health management module configured to manage the reliability and or health of the solid state storage medium . The health management module may comprise a reliability monitoring module configured monitor acquire and or obtain characteristics indicative of the health and or reliability of the solid state storage medium and or different sections thereof. In some embodiments the health management module is configured to quantify the health and or reliability of storage sections using health metrics and or one or more health characteristics which may correspond to the solid state storage medium as a whole to particular storage sections to groups of storage sections and or portions of the storage sections or the like. As used herein health metrics refer to metrics configured to quantify the reliability of a storage section which may correspond to the reliability of the solid state storage medium likelihood of successfully obtaining data stored on the solid state storage medium remaining useful life an error rate rate of change in health and or reliability and or the like.

The health metrics and or characteristics determined by the health management module may be configured to quantify the remaining useful life and or likelihood of failure of the solid state storage medium . Accordingly a health metric may comprise and or correspond to reliability characteristics such as a bit error rate BER and or raw bit error rate RBER or the like. BER and or RBER characteristics may be derived at least in part from the number of errors encountered during one or more storage operations on a storage section as compared to the total amount of data transferred to and or from the section in the one or more storage operations the RBER of a read operation may correspond to the number of bit errors encountered in the read operation as compared to the total amount of data transferred in the read operation.

The health management module may further comprise a wear management module . As disclosed above a manufacturer may represent that the solid state storage medium is capable of enduring a particular amount of wear before failing e.g. a maximum PE count or threshold . In some embodiments the wear management module is configured to distribute storage operations e.g. program erase operations evenly between sections of the solid state storage medium such that the wear level PE count of the storage divisions is generally uniform.

Through testing experience and research the inventors have determined that wear level is not always an accurate measure and or estimate of the reliability and or remaining useful life of the solid state storage medium . In some embodiments different sections of the solid state storage medium may have different reliability and or endurance characteristics and or wear rates. Some sections may be capable of enduring higher levels of wear more PE cycles than other sections due to inter alia different quality levels of different portions of the solid state storage media physical characteristics of the solid state storage media e.g. the location of various die planes etc. different operating conditions within different portions of the solid state storage medium e.g. different temperatures voltages signal attenuation etc. and so on.

In some embodiments the wear management module is configured to manage storage operations on the solid state storage media according to the health metrics determined by health management module as disclosed above. The health metrics may incorporate additional characteristics and or properties of the solid state storage medium to more accurately predict the reliability and or remaining useful life. As disclosed above a health metric or reliability metric refers to a value configured to quantify the health e.g. remaining useful life and or reliability of the solid state storage medium and or portions thereof e.g. sections . The disclosed health metrics may be based on various different characteristics which may be combined in any suitable manner. A health metric may be based on one or more of performance characteristics of the storage section such as program time e.g. the time required to program data to the storage section erase time e.g. the time required to erase data from the storage section read time e.g. the time required to perform read operations on the storage section error rates including but not limited to the reliability metric s disclosed above wear level e.g. PE count PE ratio or the lie rate of change in various characteristics such as error rate reliability or the like and so on. In some embodiments a health metric H may comprise a weighted combination of one or more factors and or characteristics as illustrated below 

In Equation 1 the health metric H comprises a weighted average of N different factors each having a respective weight w . In some embodiments the health metric H comprises a weighted combination of program time P weighted by w erase time E weighted by w wear level PE weighted by w and reliability or error rate E weighted by w as illustrated below 

In some embodiments the wear management module may be configured to manage storage operations according to a rate of change of health metrics. The rate of change of health metrics may be used to identify storage sections B that are more susceptible to wear and as such may not be capable of implementing as many PE cycles as other more robust sections of the solid state storage medium . The rate of change in various health and or reliability characteristics may therefore be used as an indicator of storage health and may be used as a health metric itself . In some embodiments the reliability monitoring module may be configured to determine the rate of change of various health related characteristics. As illustrated below a rate of change health metric H may comprise a rate of change per storage operation SO which may comprise a PE cycle read operation.

The rate of change of various health metrics and or characteristics may be indicative of which sections of the solid state storage medium are degrading more quickly than other sections which may allow the wear management module to distribute storage operations between the sections accordingly.

The health management module may comprise a retirement module configured to identify portions and or sections of the solid state storage medium that should be retired or taken out of service OOS . As used herein retiring a section of the solid state storage medium refers to indicating that the section should not be used to store data. Portions of the solid state storage medium may be taken out of service in response to various conditions including but not limited to failure conditions partial failures inaccessibility unacceptable performance e.g. long read program and or erase times programming errors read errors and the like. The OOS module may be configured to manage retirement conditions within the solid state storage medium which as disclosed in further detail herein may comprise avoiding the sections replacing certain sections and or the like. The OOS management module may be configured to avoid portions of the solid state storage medium that have been taken OOS. Avoiding an OOS storage location may comprise replacing OOS storage resources with replacement resources e.g. remapping masking OOS storage resources e.g. mapping nonce and or padding data to the OOS storage location a hybrid approach combining remapping and masking or the like. Further embodiments of apparatus systems and methods for managing OOS conditions are disclosed in U.S. patent application Ser. No. 13 354 215 entitled Apparatus System and Method for Managing Out of Service Conditions filed Jan. 19 2011 which is hereby incorporated by reference.

The storage module may further comprise a groomer module which is configured to perform grooming operations on the solid state storage medium . Grooming operations may include but are not limited to reclaiming storage resources erasure refreshing data stored on the solid state storage medium and so on. The groomer module may operate outside of the path for servicing other higher priority storage operations and or requests. Therefore the groomer module may operate as an autonomous background process which may be suspended and or deferred while other storage operations are in process. Alternatively the groomer module may operate in the foreground while other storage operations are being serviced. The groomer may wear level the non volatile storage media such that data is systematically spread throughout different storage locations which may improve performance data reliability and avoid overuse and or underuse of particular storage locations thereby lengthening the useful life of the solid state storage medium . Recovering and or reclaiming a storage section such as an or logical erase block may comprise relocating valid data if any from the section erasing the section and or initializing the section for use in servicing other storage operations. Initializing may comprise marking the storage section with a sequence number and the like. In some embodiments initializing the storage section may further comprise placing a reference and or link to the storage section in a pool of storage sections available store data.

As disclosed above the reliability monitoring module may be configured to monitor health and or reliability characteristics of the solid state storage medium . In some embodiments the reliability monitoring module may monitor such information during normal operation of the storage module e.g. while the storage module is servicing storage requests of the storage clients . The reliability monitoring module may query and or be communicatively coupled with the storage controller monitor storage operations performed on the solid state storage medium . The reliability monitoring module may be configured to gather information pertaining to erase operations to determine erase time program operations to determine program time and or program error rates read operations to determine error rates and or read time and so on. In some embodiments the reliability monitoring module is further configured to perform test operations to determine one or more health characteristics. In some embodiments the reliability monitoring module is configured to perform periodic test read operations on the solid state storage medium . A test operation may comprise reading one or more packets and or data segments from particular storage sections . The test operations may further comprise determining whether the operation s resulted in an error e.g. errors detected and or corrected using the ECC correction module a parity module or the like . The reliability monitoring module may be configured to perform test read operations according to a scan pattern configured to cover different sections of the solid state storage medium .

The health management module may comprise a reliability analysis module configured to determined an overall aggregate and or combined health metric and or health model of the solid state storage medium . The overall health or reliability metric may comprise an average mean and or other combination of the health and or reliability characteristics of the sections comprising the solid state storage medium. The reliability analysis module may be configured to periodically update the aggregate health metric in response to information acquired by the reliability monitoring module failure conditions and so on.

As disclosed above the wear management module may be configured to manage the distribution of storage operations to the sections of the solid state storage medium in accordance with health metrics of the solid state storage medium and or the sections themselves as determined by the reliability monitoring module reliability analysis module and or the like . As used herein distributing and or managing storage operations may comprise managing usage wear and or wear rate on the solid state storage medium which may include but is not limited to managing the frequency of program erase cycles on various sections of the solid state storage medium managing the type of data stored on various sections of the solid state storage medium managing storage operations within particular regions of the solid state storage medium to avoid read disturb write disturb and or other conditions scheduling sequential storage operations on various sections of the solid state storage medium and the like. The wear management module may be configured to manage storage operations to mitigate the effects of wear on various sections of the solid state storage medium .

The wear level module may be configured to manage usage of the solid state storage medium to equalize the health metrics across the solid state storage medium as opposed to equalizing PE count and or other operational metrics. In some embodiments the wear level module is configured to identify sections having different reliability characteristics which may comprise identifying clusters groups and or bands of sections based on the health and or reliability metrics thereof. In some embodiments identifying sets groups and or bands of sections comprises determining an overall aggregate and or combined health and or reliability metric of the solid state storage medium . The overall reliability metric may correspond to reliability metrics across all of the sections A N that are currently in service of the solid state storage medium . The reliability analysis module may be configured to identify sections that differ from the overall reliability metric may more than a threshold. The threshold may be a pre determined value or may be adapted according to the health and or reliability characteristics of the solid state storage medium . In some embodiments the reliability analysis module may be configured to identify sections having reliability characteristics in the bottom or to 10 differ from the average by more than a set amount or the like. Alternatively or in addition the reliability analysis module may be configured to identify sections having a rate of change in health and or reliability that exceeds the rate of change of other sections by more than a threshold.

In the embodiment the health characteristics of storage sections B and N may be lower than those of the other storage sections which as disclosed above may indicate that the storage sections B and N are more susceptible to wear than other storage sections . In response the wear management module may be configured to manage storage sections to vary the wear applied to B and N. Accordingly the sections B and N may be identified as low us gray or fragile sections. As used herein a low use reduced usage gray or fragile storage sections refer to storage sections that have health and or reliability characteristics that differ from the health and or reliability characteristics of other sections by a threshold amount and or exhibit a rate of change in heath characteristics that differ from a rate of change of the health characteristics of other sections . Other sections may be classified as normal use. In response to identifying a low use the wear management module may manage storage operations to reduce the wear and or usage of the sections B and N as well as other fragile sections as compared to the other sections . In some embodiments the wear management module may be configured to temporarily retire the low use sections B and N. As used herein temporarily retiring a section may comprise removing the section from a queue or pool of available storage sections marking the section as OOS in Health metadata or other storage metadata or the like.

As disclosed above the reliability monitoring module may be configured to periodically and or continually monitor health and or reliability characteristics of the solid state storage medium . The reliability analysis module may update the overall reliability metrics and or characteristics of the sections in accordance with updated information provided by the monitoring module . Since health and or reliability of the sections degrades over time and in proportion to use rates of the sections the health and or reliability metrics of the low use sections D and N may eventually converge with the overall reliability metric of the solid state storage medium due inter alia to the wear management module reducing wear and or usage of the sections D and N . In response to the reliability and or health metrics of one or more of the low use sections D or N converging with the overall reliability and or health characteristics of the rest of the solid state storage medium differential less than a threshold the wear management module may be configured to return the sections D or N to the general population of the solid state storage medium which may comprise managing storage operations to wear and or use the section s D and or N as normal. If the section D and or N was temporarily retired the wear management module may un retire the section D and or N which may comprise placing the section into a queue and or pool of available storage sections marking section D and or N as available or the like.

The reliability analysis module may determine that another storage section C has health and or reliability characteristics that exceed the overall reliability and or health metric such sections e.g. section C may be identified as a robust or healthy. In response the wear management module may be configured to manage storage operations to increase the wear and or usage rate on the section C and or other robust sections . Increased wear and or usage may result in decreasing the health and or reliability of the section C more rapidly than other sections . As such the health and or reliability metric of the section may converge with the other sections over time and in response the wear management module may be configured to manage storage operations to reduce the load on the section C to normal levels.

As illustrated above the wear level management module may be configured to use health and or reliability metrics to evenly wear and or use the solid state storage medium . The wear level management module may manage the distribution and or allocation of storage based on health and or reliability metrics as opposed to the PE count alone. Accordingly the overall health of the media may be evenly distributed based on actual observed performance and or reliability metrics as opposed to PE cycle count alone.

The wear level management module may be configured to manage storage operations in various different ways including but not limited to managing the pool and or queue of available storage sections managing the operation of a groomer module disclosed in further detail herein and or managing the operation of the retirement module and or OOS management module to inter alia temporarily retire one or more sections .

The solid state media controller may comprise a request module configured to receive storage requests from the storage module and or other storage clients . The request module may be configured to perform storage operations on the solid state storage media in response to the requests which may comprise transferring data to from the storage module and or storage clients . Accordingly the request module may comprise one or more direct memory access DMA modules remote DMA modules bus controllers bridges buffers and the like.

The solid state media controller may comprise a write module that is configured to process data for storage on the solid state storage media . In some embodiments the write module comprises one or more data processing stages which may include but are not limited to compression encryption packetization media encryption error encoding and so on.

Error encoding may comprise encoding data packets or other data containers in an error correcting code ECC using inter alia the ECC write module . ECC encoding may comprise generating ECC codewords each of which may comprise a data segment of length N and a syndrome of length S. For example the ECC write module may be configured encode data segments into 240 byte ECC chunks each ECC chunk comprising 224 bytes of data and 16 bytes of ECC data. In this embodiment the ECC encoding may be capable of correcting more bit errors than the manufacturer of the solid state storage media requires. In other embodiments the ECC write module may be configured to encode data in a symbolic ECC encoding such that each data segment of length N produces a symbol of length X. The ECC write module may encode data according to a selected ECC strength. As used herein the strength of an error correcting code refers to the number of errors that can be detected and or corrected by use of the error correcting code. In some embodiments the strength of the ECC encoding implemented by the ECC write module may be adaptive and or configurable. In some embodiments the strength of the ECC encoding may be selected according to the reliability and or error rate of the solid state storage media .

The ECC write module may be further configured to calculate parity data for one or more data segments or other data structures . The parity data may be used with or in place of the ECC encoding described above. Parity data may be used to detect and or correct errors in data stored on the solid state storage medium e.g. using parity substitution as described below .

The write module may be configured to store data in a contextual format on the solid state storage media . As used herein a contextual format refers to a data format in which a logical interface of a data segment is associated with the data segment on the solid state storage media . For example a contextual packet format may include a packet header comprising one or more logical identifiers of a data segment or the like. The contextual format may be used to reconstruct the logical to physical translation layer and or storage metadata of the storage module in the event storage metadata e.g. forward index of the storage module is lost or corrupted.

The write buffer may be configured to buffer data for storage on the solid state storage media . In some embodiments the write buffer may comprise one or more synchronization buffers to synchronize a clock domain of the solid state media controller with a clock domain of the solid state storage media and or bus .

The log storage module may be configured to select media storage location s for data storage and or may provide addressing and or control information to the non volatile storage media via the bus . Accordingly the log storage module may provide for storing data sequentially at an append point within the physical address space of the solid state storage media . The physical address at which a particular data segment is stored may be independent of the logical interface e.g. logical identifier of the data segment. The logical to physical translation layer may be configured to associate the logical interface of data segments e.g. logical identifiers of the data segments with the physical address es of the data segments on the solid state storage media . In some embodiments the logical to physical translation layer may comprise storage metadata which may include a forward index comprising arbitrary any to any mappings between logical identifiers and media addresses. The storage metadata may be maintained in volatile memory such as the volatile memory . In some embodiments the storage module is configured to periodically store portions of the storage metadata on a persistent storage medium such as the solid state storage media persistent storage or the like.

The solid state media controller may further comprise a read module that is configured to read data from the solid state storage media in response to requests received via the request module . The requests may comprise and or reference the logical interface of the requested data such as a logical identifier a range and or extent of logical identifiers a set of logical identifiers or the like. The physical addresses associated with data of the request may be determined based at least in part upon the logical to physical translation layer and or storage metadata maintained by the storage module . Data may stream into the read module via the read buffer and in response to addressing and or control signals provided via the bus . The read buffer may comprise one or more read synchronization buffers for clock domain synchronization as described above.

The read module may be configured to process data read from the non volatile storage media and provide the processed data to the storage module and or a storage client . The read module may comprise one or more data processing stages which may include but are not limited to error correction media decryption depacketization decryption decompression and so on. Data processed by the read module may flow to the storage module and or storage client via the request module and or other interface or communication channel e.g. the data may flow directly to from a storage client via a DMA or remote DMA module of the storage module .

The read module may comprise an ECC read module configured to detect and or correct errors in data read from the solid state storage media using inter alia the ECC encoding of the data e.g. as encoded by the ECC write module parity data e.g. using parity substitution and so on. The ECC encoding may be capable of detecting and or correcting a pre determined number of bit errors in accordance with the strength of the ECC encoding. The ECC read module may be capable of detecting more bit errors than can be corrected.

The ECC read module may be configured to correct any correctable errors using the ECC encoding. In some embodiments the ECC read module may attempt to correct errors that cannot be corrected using the ECC encoding using other techniques such as parity substitution or the like. Alternatively or in addition the ECC read module may attempt to recover data comprising uncorrectable errors from another source. For example in some embodiments data may be stored in a RAID configuration. In response to detecting an uncorrectable error the ECC read module may attempt to recover the data from the RAID or other source of redundant data e.g. a mirror backup copy or the like .

In some embodiments the ECC read module may be configured to generate an interrupt in response to reading data comprising uncorrectable errors. The interrupt may comprise a message indicating that the requested data is in error and may indicate that the ECC read module cannot correct the error using the ECC encoding. The message may comprise the data that includes the error e.g. the corrupted data . The interrupt may be caught by the storage module or other process.

In some embodiments the storage module may correct errors in corrupted data using alternative error correction techniques such as parity substitution or the like. Parity substitution may comprise iteratively replacing portions of the corrupted data with a parity mask e.g. all ones until a parity calculation associated with the data is satisfied. The masked data may comprise the uncorrectable errors and may be reconstructed using other portions of the data in conjunction with the parity data. Alternatively the storage module may replace the corrupted data with another copy of the data such as a backup or mirror copy and then may use the replacement data of the requested data packet or return it to the read module . In another embodiment the storage module stores data in a RAID configuration from which the corrupted data may be recovered as described above.

Further embodiments of apparatus systems and methods for detecting and or correcting data errors are disclosed in United States Patent Application Publication No. 2009 0287956 Ser. No. 12 467 914 entitled Apparatus System and Method for Detecting and Replacing a Failed Data Storage filed May 18 2009 which is hereby incorporated by reference in its entirety. The solid state media controller may further comprise a multiplexer that is configured to selectively route data and or commands between the write module and read module and solid state storage media . In some embodiments solid state media controller may be configured to read data while filling the write buffer and or may interleave one or more storage operations on one or more banks of solid state storage elements as described below in conjunction with . Further embodiments of write and or read modules are disclosed in United States Patent Application Publication No. 2008 0141043 Ser. No. 11 952 091 entitled Apparatus System and Method for Managing Data using a Data Pipeline filed Dec. 6 2007 which is hereby incorporated by reference in its entirety.

As disclosed above the groomer module may be configured to reclaim storage resources on the solid state storage media . The groomer module may operate as an autonomous background process which may be suspended and or deferred while other storage operations are in process. The wear manager may be configured to leverage the groomer module to manage wear and or usage of sections of the solid state storage media based on health and or reliability metrics as disclosed above.

In some embodiments the groomer module may interleave grooming operations with other storage operations and or requests. For example reclaiming a storage resource such as an erase block or logical erase block e.g. set of two or more erase blocks may comprise relocating valid data stored on the logical erase block to other storage locations on the solid state storage media . The groomer write and groomer read bypass modules and may be configured to allow data packets to be read into the read module and then be transferred directly to the write module without being routed out of the storage media controller .

The groomer read bypass module may coordinate reading data to be relocated from the storage resource that is being reclaimed. The groomer module may be configured to interleave the relocation data with other data being written to the non volatile storage media via the groomer write bypass . Accordingly data may be relocated without leaving the solid state media controller . In some embodiments the groomer module may be configured to fill the remainder of the write buffer with relocation data which may improve groomer efficiency while minimizing the performance impact of grooming operations.

As disclosed above the write module and log storage module may be configured to store data sequentially within the solid state storage media . Referring to the storage controller and or log storage module may be configured to store data sequentially within a physical address space of the solid state storage media . The physical address space of the solid state storage medium may be partitioned into a plurality of sections A N which as disclosed above may comprise erase blocks logical erase blocks or the like. The each section A N may comprise a plurality of storage units P Z which may comprise pages logical pages or the like. The log storage module may configure the write module to store data sequentially at an append point . The log storage module may sequentially write data to the storage units P PZ of the storage section until the end of the storage section PZ is reached. The log storage module may then advance the append point to a next available storage section B N. After reaching the end of the physical address space PZ of section N the append point resumes back at storage section A P if available .

The groomer module may be configured to identify sections A N for recovery. As disclosed above recovery may comprise relocating valid data from a section and initializing the section which may include erasing the section and placing the section in a pool of sections that are available for storing data. When the append point reaches the end of the section A the log storage module may determine the next section to use to store data. In the embodiment the next storage section may be B.

The wear management module may be configured to manage and or adapt the sequential storage operations to change the wear and or usage of the sections A N in accordance with their respective health and or reliability metrics. In the embodiment the section B may be considered to be a fragile low use storage division. Accordingly the wear management module may remove the entry B from the pool as such the append point may skip section B and resume at C. Alternatively or in addition the pool may comprise an ordered and or prioritized queue and the sections therein may be associated with corresponding priority metadata B . . . M. The priority metadata B associated with the low use section B may indicate that the section is to be skipped a particular number of times modifying the normal sequential of storage operations within the physical address space . The wear management module may be configured to skip the storage section B N times before setting the append point to B. The iteration and or skip count associated with the section B may be maintained in the priority metadata B.

The priority metadata of other sections such as section C may indicate that the section is a robust section based on the health and or reliability characteristics thereof . In response the log storage module may be configured to prioritize selection of the section C over other sections in the pool which may increase wear and or usage of the section C as disclosed above.

In some embodiments the pool may comprise a secondary low priority pool comprising sections identified as being fragile and or low use as disclosed above. The log storage module may select storage sections from the low priority pool less frequently than the pool to reduce wear and or usage on the sections therein. In some embodiments the pool may further comprise a high priority pool not shown comprising robust sections . The log storage module may be configured to select storage sections from the high priority pool more frequently to increase the wear and or usage on the sections therein.

In some embodiments the storage module may be configured to maintain access metadata pertaining data stored on the solid state storage medium . The access metadata may identify data that is low use or cold which may include data that is infrequently accessed written and or modified. Other data may be identified as high use or hot which may include data that is frequently accessed written and or modified. The wear management module may be configured to manage wear and or usage based on the access metadata associated with various storage operations. In some embodiments the wear management module is configured to cause data identified as low use and or cold to be stored on low use and or fragile storage sections . Accordingly the wear management module may be configured to instruct the log storage module to modify the append point to store data of one or more storage requests in a storage section in the low priority queue and or having low priority metadata B. The wear management module may be further configured to configure the write module to store high use hot data on robust storage sections which may comprise configuring the log storage module to select one or more identified storage sections section C to service particular storage requests.

The wear management module may be configured to manage wear and or usage of the storage sections by use of the groomer module . As disclosed above the groomer module may be configured to iterate through the physical address space to identify sections to reclaim. A storage section may be selected for reclamation based on the amount of invalid data on the section age of the data on the section capacity constraints and the like. Recovering a storage section may comprise relocation valid data from the section to other sections via the groomer bypass and or disclosed above . The recovery operations may further comprise erasing the section and placing the section the pool as disclosed above. The wear management module may be configured to delay recovery of low use and or fragile storage sections by inter alia allowing the groomer module to relocate data from the storage section but instructing the groomer module to deter initializing the section e.g. defer erasing marking and or placing the section into the pool . The wear management module may be configured to instruct the groomer module to defer initialization for a particular time period and or a particular number of grooming passes through the physical address space . Deferring initialization may reduce the frequency at which fragile sections are used without requiring modifications to the pool and or queue .

The solid state storage elements may be embodied on separate chips packages die or the like. Alternatively or in addition one or more of the solid state storage elements may share the same package and or chip e.g. be separate die and or planes on the same chip . The solid state storage elements comprise respective erase blocks each comprising a plurality of storage units e.g. pages . However the disclosure could be adapted to use different types of solid state storage media comprising different media partitioning schemes and as such should not be read as limited in this regard.

The storage module may be configured to perform storage operations on logical storage units and or logical erase blocks of the logical storage element . In the embodiment each logical erase block comprises an erase block of a respective storage element 0 through 24 and each logical page comprises a physical page of a respective storage element 0 through 24. Accordingly each logical erase block may comprise as many as twenty five 25 erase blocks and each logical page may comprise as many as twenty five 25 physical pages . Although the logical erase block of includes erase blocks within a single logical storage element the disclosure is not limited in this regard in some embodiments described below the logical erase block may span a plurality of logical storage elements and or banks of storage elements .

The storage module may be configured to perform storage operations on logical storage element which may operate across the constituent solid state storage elements an operation to read a logical page comprises reading from as many as twenty five 25 physical pages e.g. one storage unit per solid state storage element an operation to program a logical page comprises programming as many as twenty five 25 physical pages an operation to erase a logical erase block comprises erasing as many as twenty five 25 erase blocks and so on. Accordingly the effective read write bandwidth of the logical storage element may be proportional to the number of solid state storage elements included therein.

Arranging solid state storage elements into logical storage elements may be used to address certain properties of the solid state storage media . For example the solid state storage media may have asymmetric properties it may take ten 10 times as long to program data on a solid state storage element as it takes to read data from the solid state storage element . Moreover in some cases data may only be programmed to erase blocks that have been initialized e.g. erased . An erase operation may take ten 10 times as long as a program operation and by extension one hundred 100 times or more longer than a read operation .

The arrangement of the solid state storage elements into logical storage elements and or interleaved banks as described herein may allow the storage module to address the asymmetric properties of the solid state storage media . In some embodiments the asymmetry in read program and or erase operations is addressed by performing these operations on many elements in parallel e.g. on a logical storage element . In the embodiment programming asymmetry may be addressed by programming twenty five 25 physical pages in a logical page in parallel. Performing multiple program operations in parallel may increase the effective write or programming bandwidth. The effective program bandwidth of the logical storage element depicted in may be as much as twenty five 25 times that of the program bandwidth of the same twenty five 25 solid state storage elements in serial. The increase to programming bandwidth may be used to mask the asymmetry between write program and read operations. Erase operations may be performed on a multiple erase blocks e.g. logical erase blocks . Erasing a logical erase block may therefore comprise erasing twenty five 25 separate erase blocks in parallel. Like the logical programming operations described above implementing erase operations on logical erase blocks in parallel may allow the storage module to manage asymmetry between erase program and read operations.

In some embodiments a certain portion of a logical storage element may be configured to store error detection and or recovery data. For example one of the storage elements denoted in may be used to store parity data. In this embodiment the effective capacity and or bandwidth of the logical storage element may be reduced e.g. reduced from twenty five 25 physical pages to twenty four 24 physical pages the first twenty four 24 physical pages are used to store data and physical page is dedicated to storing parity data. As used herein effective capacity and or bandwidth refers to the number of storage units or divisions that are available to store data and or the total amount of data that can be stored and or read in parallel. The operational mode described above may be referred to as a 24 1 configuration denoting that twenty four 24 physical storage units are available to store data and one 1 of the physical storage units is used for parity data. The logical storage element could be configured to operate in any number of operational modes in which any proportion of the solid state storage elements are used to store error detection and or recovery data and as such the disclosure should not be read as limited in this regard.

As illustrated above the storage module may be configured to perform storage operations on logical storage units logical pages of the solid state storage media each of which may comprise as many as twenty five erase blocks . The health management module may be configured to track reliability metrics of the solid state storage medium at a corresponding level of granularity. Accordingly the health management module may be configured to determine reliability characteristics of storage sections that correspond to the logical erase blocks which as disclosed herein may comprise combining the reliability metrics of individual erase blocks .

Although particular embodiments of logical storage elements as disclosed herein the disclosure is not limited in this regard and could be adapted to incorporate logical storage elements of differing sizes and or configurations. The size and number of erase blocks pages planes or other logical and physical divisions within the solid state storage elements are expected to change over time with advancements in technology it is to be expected that many embodiments consistent with new configurations are possible and are consistent with the embodiments disclosed herein. Moreover the storage controller may be configured to layout data onto the logical storage element according to different adaptive configurations as disclosed in U.S. patent application Ser. No. 13 830 652 entitled Systems and Methods for Adaptive Error Control Coding filed Mar. 14 2013 for Jeremy Fillingim et al and which is hereby incorporated by reference.

Referring back to as disclosed herein the storage controller may be configured to continue operating when storage units in the solid state storage media are taken out of service and or to manage wear for health conditions as instructed by the wear management module .

As disclosed above the health module may be configured to gather profiling information pertaining to the solid state storage media by use of the reliability monitor module the reliability information may include but is not limited to error information e.g. RBER performance metrics wear levels e.g. PE cycles and so on. The reliability information may be used by the reliability analysis module to determine reliability and or health characteristics and corresponding metrics for various sections of the solid state storage medium e.g. logical erase blocks erase block and so on . The wear management module may use the reliability and or health metrics to inter alia identify low use and or high use sections of the solid state storage medium and may manage storage operations accordingly. The reliability module may leverage to reliability and or health metrics to identify portions of the solid state storage medium that should be retired. In some embodiments the wear management module may configure the reliability module and or OOS management module to manage low use sections as if the sections were OOS to thereby reduce wear on the sections. The wear management module may be further configured to bring the sections back into service in response to overall health of the solid state storage medium degrading to the same or sufficiently similar level as the low use sections as disclosed above.

The OOS management module may be configured to track storage resources that have been taken out of service and or are being managed as low wear and or fragile. In some embodiments the OOS management module tracks OOS conditions in the solid state storage media using health metadata . The health metadata may be detected and or tracked at varying levels of granularity health conditions may be tracked and or maintained by page logical page erase block logical erase blocks die chips planes and or according to other storage partitions or divisions. The health management module may be configured to maintain reliability information for storage divisions comprising a plurality of erase blocks in accordance with the logical storage element and or logical pages of . The disclosure should not be read as limited in this regard however and could be applied to any size and or organization of non volatile storage media . The storage module may be configured to manage OOS conditions using one or more of a remapping approach masking approach hybrid approach or the like.

In some embodiments the storage module is configured to manage OOS and or health conditions using a remapping approach in which the bus includes addressing information for each solid state storage element in the logical storage element e.g. each storage element may receive a respective physical address via the bus . The storage module may leverage the separate addressing information to remap replacements for one or more OOS storage resources from other portions of the solid state storage media . The OOS management module may use remapping to prevent a few OOS erase blocks from taking an entire logical erase block out of service. As disclosed above in some embodiments the wear management module is configured to use the OOS management module to manage low usage conditions within logical erase blocks . In some embodiments the OOS management module may be configured to treat low wear portions of a logical erase block as temporarily OOS to reduce wear on the portion while allowing other portions of the logical erase block to continue being used to store data.

The OOS management module may be configured to manage OOS and or health conditions using an masking approach. in which OOS and or health conditions are managed by masking physical storage units that are OOS if any . As used herein masking an OOS storage location such as an erase block may comprise configuring the write module to inject padding data into the write buffer such that the padding data is mapped to the OOS storage locations on the bus during programming operations. Masking may further comprise configuring the read module to ignore or otherwise avoid data read from OOS storage locations during read operations. Masking OOS storage units may reduce the storage capacity and or effective bandwidth of portions of the logical storage element while allowing the remaining in service storage divisions to continue in operation. As used herein padding or masking data refers to any data that is used in place of valid data. Accordingly padding data may be actively added as a particular data pattern e.g. ones zeros or other patterns or may be added passively by reusing whatever data is on the bus or write module allowing portions of the bus to float or the like.

In some embodiments the OOS management module is configured to manage OOS conditions using a hybrid approach in which OOS and or health conditions are managed by masking the OOS and or low use storage units if any as disclosed above. The masking approach may be used until the number of OOS and or low use storage locations reaches a threshold. When the threshold is reached the storage module may be configured to implement the bad block remapping approach to replace one or more of the OOS physical storage units from other portions of the solid state media as described above. OOS storage units for which there are no available replacements may continue to be managed using the masking approach. Further embodiments of apparatus systems and methods for managing OOS conditions are disclosed in U.S. patent application Ser. No. 13 354 215 entitled Apparatus System and Method for Managing Out of Service Conditions filed Jan. 19 2011 which is hereby incorporated by reference in its entirety.

In the embodiment the solid state media controller may comprise an OOS write module configured to manage OOS conditions in the write module e.g. remap and or mask OOS storage resources . During write operations the OOS write module may be configured to identify storage resources that are OOS using inter alia the health metadata . The OOS write module may access the health metadata from an internal metadata storage unit driver storage module or the like. Alternatively or in addition the OOS management module may be configured to push health metadata to the solid state media controller via the request receiver module e.g. health metadata may be included with storage requests .

The OOS write module may be configured to manage OOS and or health conditions using one or more of a remapping approach masking approach hybrid approach or the like as disclosed above. The OOS write module or other command and control module may be configured to implement a remapping approach to replace OOS and or low use storage resources with other available storage resources. The remapping approach may comprise identifying other available storage resources and modifying one or more addresses and or command signals on the bus to replace OOS and or low use storage resources with the identified replacement resources e.g. using the log storage module . The OOS write module may be further configured to implement a masking approach which may comprise injecting padding data into the write buffer or other portions of the write module such that the padding data is mapped to the OOS and or low use storage resources identified by the Health metadata . The OOS write module may be further configured to implement a hybrid approach in which the OOS write module masks a threshold number of OOS and or low use storage resources and then implements bad block remapping where available thereafter.

The OOS read module may be configured to manage OOS and or health conditions in the read module using one or more of a remapping approach masking approach hybrid approach or the like as described above. In a bad block remapping approach the OOS read module may be configured to identify the replacement addresses for OOS and or low use storage resources if any and set addressing and or control signals on the bus accordingly e.g. by use of the log storage module . In a masking approach the OOS read module may be configured to strip or otherwise ignore data read corresponding to OOS and or low use storage resources e.g. strip padding data from the read buffer before the data is processed through the rest of the read module . In a hybrid approach the OOS read module may be configured to selectively remap storage resources and or strip data from the read buffer in accordance with the health metadata and as described above.

Referring back to the wear management module may be configured to identify groups of storage sections that have similar health characteristics. The groups may include a general population group average health metrics a low usage group poor health metrics and or a high usage group robust health metrics . The groups may be identified using fixed threshold values based on characteristics of the solid state storage medium e.g. known and or expected error rates testing experience preferences or the like. Alternatively or in addition the groups may be identified by use of adaptive thresholds. In some embodiments the wear management module identifies the storage sections in the bottom 10 as low use and or fragile and the top 10 as high use or robust.

In some embodiments the reliability analysis module determines a health model of the solid state storage medium which may comprise a statistical distribution of health and or reliability metrics of the storage sections . depicts one embodiment of a plot of a statistical health metric distribution. The embodiment depicts a Normal or Gaussian type distribution. However the disclosure is not limited in this regard and could use any suitable distribution and or statistical model. The axis may represent the distribution of storage sections A N having various levels of health and or reliability characteristics .

As depicted in the mean A of the distribution may correspond to the average overall and or aggregate health of the solid state storage medium . Low usage and or fragile sections may be identified as sections having reliability and or health metrics that are lower than a threshold value A. The threshold may be based on inter alia properties of the model such as a weighted value of the standard deviation A. Storage sections exhibiting lower health and or reliability than A w times the standard deviation A may be identified as low use fragile storage sections A. Other sections that exhibit higher health characteristics may be placed in a normal use A category. In some embodiments the model may further comprise a high usage threshold configured to identify robust storage sections for increased wear as disclosed herein. As illustrated in sections B and N are identified as low use A storage sections A is identified as normal use A and section C is identified as high use A. Storage sections having reliability characteristics that do not satisfy a reliability threshold may be retired e.g. storage section X .

The reliability and or health characteristics of the solid state storage media may change over time becoming less reliable health. The reliability analysis module may be configured to periodically or continually update reliability information in response to storage operations performed on the solid state storage medium and or test operations. The criteria for identifying and or categorizing the health of storage sections may change accordingly. depicts a model of the health of the solid state storage medium subsequent to performing a plurality of storage operations on the solid state storage medium . The model of is included for reference. As illustrated in the overall health of the medium has decreased as indicated by the lower mean B as compared to A . The thresholds for classifying storage sections as low use normal use and or high use may be adjusted in accordance with the model . In the example the health of the high use section C has degraded as compared to the rest of the population due to inter alia higher wear on the section C imposed by the wear management module . The health characteristics of the low use sections B and N may improved relative to the other sections and threshold B may be reduced such that the section B is identified as part of the normal usage group B. The storage section N however may remain in the low usage category B. The retirement threshold may correspond that to a minimum reliability setting that may not change in response the models and or and as such the storage section X may remain OOS.

In some embodiments the reliability analysis module may be configured to identify low use and or high use sections based on the rate of change in health and or reliability metrics. depicts one embodiment a plot comprising a distribution of Hr health rate of change and or derivative values for storage sections . The axis may indicate increasing rates of change to the health and or reliability metrics of the storage sections degrading at increasing rates . A threshold may correspond to a point at which the reliability analysis module determines that the health and or reliability metrics of the sections in the region A are degrading at rate that is sufficiently higher than the rest of the normal usage population A to warrant reducing the wear and or usage rate of the sections . The derivative approach of may be more responsive to changes such that low wear conditions can be addressed before storage sections exhibit extreme drops in health and or reliability.

The derivative based model of may change in response to changes within the solid state storage media . In some embodiments the health and or reliability characteristics of the storage sections may degrade non linearly e.g. an exponential decay or other characteristic function . depicts another embodiment of a model corresponding the rage of change in storage section health metrics. As illustrated in the rate of change of the storage sections has rapidly increased which may be indicative of an overall aging of the solid state storage medium . The threshold B may be adjusting accordingly to include storage sections that are changing at a significantly higher rate B than the rest of the population B. Although not depicted in the models and could further include criteria e.g. thresholds for identifying health robust storage sections as disclosed above. The robust storage sections may include storage sections having a wear rate that is lower by a threshold value than the other storage sections .

In some embodiments the reliability analysis module may be configured to combine a plurality of different models and or selection criteria to identify low high and or normal usage storage sections including the models of and A B.

Step may comprise determining the reliability and or health of a solid state storage medium solid state storage media and or logical storage element . Step may comprise determining an overall reliability and or health metric of a plurality of storage sections logical storage divisions erase blocks and the like referred to generally as storage sections . Step may comprise a reliability monitoring module monitoring the storage sections to determine reliability and or health characteristics of the sections . Monitoring may comprise accessing and or determining performance metrics such as erase time program time read time and the like reliability metrics such as BER RBER and the like wear levels such as PE count and so on. Step may further comprise a weighted combination of a plurality of factors to generate one or more health and or reliability metrics for the storage sections . Step may further comprise combining aggregating and or fusing the reliability and or health metrics of the plurality of storage sections to determine an overall metric. In some embodiments step further comprises determining a model of the health and or reliability of the storage sections such as a statistical model a derivative model or the like. In some embodiments step may further comprise storing storage metadata pertaining to the reliability health metrics and or characteristics and or the identified sets of storage sections described below on a non volatile storage medium for use following a restart operation crash or the like.

Step may comprise identifying a set of one or more storage sections having health and or reliability metrics that diverge from others of the plurality of storage sections . Divergent section s may include sections having health and or reliability metrics that differ from the overall health and or reliability metric by more than a threshold. Alternatively or in addition the divergent section s may correspond to sections that having a health and or reliability metric that is changing more or less rapidly than other sections . Step may comprise identifying low wear sections . Step may further comprise identifying a general population of sections and or robust sections . The divergent sections may be identified by comparing health and or reliability metrics of the storage sections to the overall health and or reliability metric one or more threshold or the like. In some embodiments step comprises determining a deviation and or other statistical property to categorize one or more storage sections as disclosed herein.

Step may comprise managing storage operations performed on the storage sections to adjust a wear and or usage on the set of storage sections identified at step . The wear management operations of step may be configured to mitigate differences in health and or reliability characteristics between different the plurality of storage sections based on inter alia characteristics corresponding to the reliability of the storage sections the performance of the storage sections and the like. The characteristics may further include PE count and or other wear related factors as disclosed above. Accordingly the management operations of step may be configured to increase the overall useful life and or health of the storage sections by distributing operations to the storage sections according to health and or reliability characteristics thereof as opposed to PE cycles alone.

Managing storage operations may comprise reducing the wear and or usage to the set of storage sections which may include but is not limited to configuring a groomer module to reduce the rate at which the storage sections are made available for storing data by inter alia relocating data from the storage sections and delaying and or deferring one or more initialization operations such as an erasure operation and or insertion into a pool of available storage sections configuring the log storage module to skip bypass and or delay data storage to the identified sections removing the one or more storage sections from the pool of available storage sections reordering the one or more storage sections in a queue of available storage sections updating priority metadata in the pool to defer use of the storage section placing the storage sections in a low usage queue temporarily retiring the storage sections by inter alia updating health metadata to indicate that the storage sections are OOS and or storing low use and or cold data on one or more of the storage sections .

Step may comprise updating the sets of divergent solid state storage section s as disclosed above. Step may comprise evaluating the health and or reliability characteristics of the plurality of solid state storage sections against the updated aggregate characteristics of step . Step may further comprise reclassifying one or more of the solid state storage sections in accordance with the updates of step as disclosed above e.g. a low use solid state storage section B as a normal use solid state storage section B as in .

Step may comprise managing storage operations in accordance with the updates of step . Step may include but is not limited to reclassifying a low use solid state storage section as a normal use solid state storage section by inter alia configuring the groomer module to resume normal grooming operations on the solid state storage section placing the solid state storage section in the pool including the solid state storage section in a queue updating priority metadata associated with the solid state storage section un retiring the solid state storage section e.g. updating health metadata to indicate that the solid state storage section is no longer retired and so on as disclosed above.

Step may comprise un retiring the storage section which may comprise configuring the OOS management module to use the storage section to store data as opposed to avoiding the section remapping the section or the like . Step may comprise updating health metadata to indicate that the storage section is no longer OOS and or retired. Step may further comprise performing storage operations on the storage sections including the un retired storage section .

The above description provides numerous specific details for a thorough understanding of the embodiments described herein. However those of skill in the art will recognize that one or more of the specific details may be omitted or other methods components or materials may be used. In some cases operations are not shown or described in detail.

Furthermore the described features operations or characteristics may be combined in any suitable manner in one or more embodiments. It will also be readily understood that the order of the steps or actions of the methods described in connection with the embodiments disclosed may be changed as would be apparent to those skilled in the art. Thus any order in the drawings or Detailed Description is for illustrative purposes only and is not meant to imply a required order unless specified to require an order.

Embodiments may include various steps which may be embodied in machine executable instructions to be executed by a general purpose or special purpose computer or other electronic device . Alternatively the steps may be performed by hardware components that include specific logic for performing the steps or by a combination of hardware software and or firmware.

Embodiments may also be provided as a computer program product including a computer readable storage medium having stored instructions thereon that may be used to program a computer or other electronic device to perform processes described herein. The computer readable storage medium may include but is not limited to hard drives floppy diskettes optical disks CD ROMs DVD ROMs ROMs RAMs EPROMs EEPROMs magnetic or optical cards solid state memory devices or other types of medium machine readable medium suitable for storing electronic instructions.

As used herein a software module or component may include any type of computer instruction or computer executable code located within a memory device and or computer readable storage medium. A software module may for instance comprise one or more physical or logical blocks of computer instructions which may be organized as a routine program object component data structure etc. that perform one or more tasks or implements particular abstract data types.

In certain embodiments a particular software module may comprise disparate instructions stored in different locations of a memory device which together implement the described functionality of the module. Indeed a module may comprise a single instruction or many instructions and may be distributed over several different code segments among different programs and across several memory devices. Some embodiments may be practiced in a distributed computing environment where tasks are performed by a remote processing device linked through a communications network. In a distributed computing environment software modules may be located in local and or remote memory storage devices. In addition data being tied or rendered together in a database record may be resident in the same memory device or across several memory devices and may be linked together in fields of a record in a database across a network.

It will be understood by those having skill in the art that many changes may be made to the details of the above described embodiments without departing from the underlying principles of the disclosure.

