---

title: Bit-count texture format
abstract: A system, method, and computer program product are provided for using a bit-count texture format. A rasterized coverage bit mask is received by a texture processing unit from a bit-count format texture map, the rasterized coverage bit mask is converted to a scalar value, and the scalar value is processed while the rasterized coverage bit mask is retained in the bit-count format texture map. The coverage bit mask may be converted by computing a count of samples that are covered by at least one graphics primitive according to the rasterized coverage bit mask.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09305388&OS=09305388&RS=09305388
owner: NVIDIA Corporation
number: 09305388
owner_city: Santa Clara
owner_country: US
publication_date: 20130823
---
Conventional graphics processors may be configured to perform voxel based global illumination or ambient occlusion calculations where a voxel is a volumetric pixel. Some voxel based global illumination and ambient occlusion calculations compute an opacity value for each voxel. Computing the opacity values typically requires accessing memory multiple times to read per pixel coverage data and store intermediate data. The multiple memory accesses may reduce processing performance for voxel based global illumination and ambient occlusion calculations.

Thus there is need for addressing this issue and or address other issues associated with the prior art.

A system method and computer program product are provided for using a bit count texture format. A rasterized coverage bit mask is received by a texture processing unit from a bit count format texture map the rasterized coverage bit mask is converted to a scalar value and the scalar value is processed while the rasterized coverage bit mask is retained in the bit count format texture map. The coverage bit mask may be converted by computing a count of samples that are covered by at least one graphics primitive according to the rasterized coverage bit mask.

A new bit count texture format may be used to store rasterized coverage bit masks for a display surface such as a plane of a voxel representation. When a rasterized coverage bit mask is read from a bit count format texture map the rasterized coverage bit mask is interpreted as an opacity value. More specifically in one embodiment the rasterized coverage bit mask is converted to a scalar value by computing a count of samples that are covered by at least one graphics primitive. For example when half of the samples for a pixel are covered according to the rasterized coverage bit mask the scalar value is computed as 0.5. The rasterized coverage bit mask is retained in the bit count format texture map during the conversion process and while the scalar value is processed.

In one embodiment the scalar value may be represented in a floating point format e.g. 16 or 32 bits per pixel . The computed scalar values for the pixels within a voxel may be combined to produce an opacity value for the voxel. Additionally the bit count texture format may be used for performing multi sample operations voxel based global illumination and ambient occlusion calculations.

At operation the rasterized coverage bit mask for a pixel is converted to a scalar value. The rasterized coverage bit mask may be converted by computing a count of the samples that are covered according to the rasterized coverage bit mask. In one embodiment the scalar value may represent a per pixel opacity. At operation the scalar value is processed. The rasterized coverage bit mask for the pixel may be retained within the texture map. In one embodiment the scalar value may be filtered to produce a per pixel region opacity or a per voxel opacity.

More illustrative information will now be set forth regarding various optional architectures and features with which the foregoing framework may or may not be implemented per the desires of the user. It should be strongly noted that the following information is set forth for illustrative purposes and should not be construed as limiting in any manner. Any of the following features may be optionally incorporated with or without the exclusion of other features described.

In one embodiment the PPU includes an input output I O unit configured to transmit and receive communications i.e. commands data etc. from a central processing unit CPU not shown over the system bus . The I O unit may implement a Peripheral Component Interconnect Express PCIe interface for communications over a PCIe bus. In alternative embodiments the I O unit may implement other types of well known bus interfaces.

The PPU also includes a host interface unit that decodes the commands and transmits the commands to the task management unit or other units of the PPU e.g. memory interface as the commands may specify. In one embodiment the PPU comprises U memory interfaces U where each memory interface U is connected to a corresponding memory device U . The host interface unit is configured to route communications between and among the various logical units of the PPU .

In one embodiment a program encoded as a command stream is written to a buffer by the CPU. The buffer is a region in memory e.g. memory or system memory that is accessible i.e. read write by both the CPU and the PPU . The CPU writes the command stream to the buffer and then transmits a pointer to the start of the command stream to the PPU . The host interface unit provides the grid management unit GMU with pointers to one or more streams. The GMU selects one or more streams and is configured to organize the selected streams as a pool of pending grids. In one embodiment a thread block comprises 32 related threads and a grid is an array of one or more thread blocks that execute the same stream and the different thread blocks may exchange data through global memory. The pool of pending grids may include new grids that have not yet been selected for execution and grids that have been partially executed and have been suspended.

A work distribution unit that is coupled between the GMU and the SMs manages a pool of active grids selecting and dispatching active grids for execution by the SMs . Pending grids are transferred to the active grid pool by the GMU when a pending grid is eligible to execute i.e. has no unresolved data dependencies. An active grid is transferred to the pending pool when execution of the active grid is blocked by a dependency. When execution of a grid is completed the grid is removed from the active grid pool by the work distribution unit . In addition to receiving grids from the host interface unit and the work distribution unit the GMU also receives grids that are dynamically generated by the SMs during execution of a grid. These dynamically generated grids join the other pending grids in the pending grid pool.

In one embodiment the CPU executes a driver kernel that implements an application programming interface API that enables one or more applications executing on the CPU to schedule operations for execution on the PPU . An application may include instructions i.e. API calls that cause the driver kernel to generate one or more grids for execution. In one embodiment the PPU implements a SIMT Single Instruction Multiple Thread architecture where each thread block i.e. warp in a grid is concurrently executed on a different data set by different threads in the thread block. The driver kernel defines thread blocks that are comprised of k related threads such that threads in the same thread block may exchange data through shared memory.

In one embodiment the PPU may include 15 distinct SMs . Each SM is multi threaded and configured to execute a plurality of threads e.g. 32 threads from a particular thread block concurrently. Each of the SMs is connected to a level two L2 cache via a crossbar or other type of interconnect network . The L2 cache is connected to one or more memory interfaces . Memory interfaces implement 16 32 64 128 bit data buses or the like for high speed data transfer. In one embodiment the PPU may be connected to up to 6 memory devices such as graphics double data rate version 5 synchronous dynamic random access memory GDDR5 SDRAM .

In one embodiment the PPU implements a multi level memory hierarchy. The memory is located off chip in SDRAM coupled to the PPU . Data from the memory may be fetched and stored in the L2 cache which is located on chip and is shared between the various SMs . In one embodiment each of the SMs also implements an L1 cache. The L1 cache is private memory that is dedicated to a particular SM . Each of the L1 caches is coupled to the shared L2 cache . Data from the L2 cache may be fetched and stored in each of the L1 caches for processing in the functional units of the SMs .

In one embodiment the PPU comprises a graphics processing unit GPU . The PPU is configured to receive commands that specify shader programs for processing graphics data. Graphics data may be defined as a set of primitives such as points lines triangles quads triangle strips and the like. Typically a primitive includes data that specifies a number of vertices for the primitive e.g. in a model space coordinate system as well as attributes associated with each vertex of the primitive. The PPU can be configured to process the graphics primitives to generate a frame buffer i.e. pixel data for each of the pixels of the display . The driver kernel implements a graphics processing pipeline such as the graphics processing pipeline defined by the OpenGL API.

An application writes model data for a scene i.e. a collection of vertices and attributes to memory. The model data defines each of the objects that may be visible on a display. The application then makes an API call to the driver kernel that requests the model data to be rendered and displayed. The driver kernel reads the model data and writes commands to the buffer to perform one or more operations to process the model data. The commands may encode different shader programs including one or more of a vertex shader hull shader geometry shader pixel shader etc. For example the GMU may configure one or more SMs to execute a vertex shader program that processes a number of vertices defined by the model data. In one embodiment the GMU may configure different SMs to execute different shader programs concurrently. For example a first subset of SMs may be configured to execute a vertex shader program while a second subset of SMs may be configured to execute a pixel shader program. The first subset of SMs processes vertex data to produce processed vertex data and writes the processed vertex data to the L2 cache and or the memory . After the processed vertex data is rasterized i.e. transformed from three dimensional data into two dimensional data in screen space to produce fragment data the second subset of SMs executes a pixel shader to produce processed fragment data which is then blended with other processed fragment data and written to the frame buffer in memory . The vertex shader program and pixel shader program may execute concurrently processing different data from the same scene in a pipelined fashion until all of the model data for the scene has been rendered to the frame buffer. Then the contents of the frame buffer are transmitted to a display controller for display on a display device.

The PPU may be included in a desktop computer a laptop computer a tablet computer a smart phone e.g. a wireless hand held device personal digital assistant PDA a digital camera a hand held electronic device and the like. In one embodiment the PPU is embodied on a single semiconductor substrate. In another embodiment the PPU is included in a system on a chip SoC along with one or more other logic units such as a reduced instruction set computer RISC CPU a memory management unit MMU a digital to analog converter DAC and the like.

In one embodiment the PPU may be included on a graphics card that includes one or more memory devices such as GDDR5 SDRAM. The graphics card may be configured to interface with a PCIe slot on a motherboard of a desktop computer that includes e.g. a northbridge chipset and a southbridge chipset. In yet another embodiment the PPU may be an integrated graphics processing unit iGPU included in the chipset i.e. Northbridge of the motherboard.

As described above the work distribution unit dispatches active grids for execution on one or more SMs of the PPU . The scheduler unit receives the grids from the work distribution unit and manages instruction scheduling for one or more thread blocks of each active grid. The scheduler unit schedules threads for execution in groups of parallel threads where each group is called a warp. In one embodiment each warp includes 32 threads. The scheduler unit may manage a plurality of different thread blocks allocating the thread blocks to warps for execution and then scheduling instructions from the plurality of different warps on the various functional units i.e. cores DPUs SFUs and LSUs during each clock cycle.

In one embodiment each scheduler unit includes one or more instruction dispatch units . Each dispatch unit is configured to transmit instructions to one or more of the functional units. In the embodiment shown in the scheduler unit includes two dispatch units that enable two different instructions from the same warp to be dispatched during each clock cycle. In alternative embodiments each scheduler unit may include a single dispatch unit or additional dispatch units .

Each SM includes a register file that provides a set of registers for the functional units of the SM . In one embodiment the register file is divided between each of the functional units such that each functional unit is allocated a dedicated portion of the register file . In another embodiment the register file is divided between the different warps being executed by the SM . The register file provides temporary storage for operands connected to the data paths of the functional units.

Each SM comprises L processing cores . In one embodiment the SM includes a large number e.g. 192 etc. of distinct processing cores . Each core is a fully pipelined single precision processing unit that includes a floating point arithmetic logic unit and an integer arithmetic logic unit. In one embodiment the floating point arithmetic logic units implement the IEEE 754 2008 standard for floating point arithmetic. Each SM also comprises M DPUs that implement double precision floating point arithmetic N SFUs that perform special functions e.g. copy rectangle pixel blending operations and the like and P LSUs that implement load and store operations between the shared memory and the register file via the J texture unit L1 caches and the interconnect network . The J texture unit L1 caches are coupled between the interconnect network and the shared memory and are also coupled to the crossbar . In one embodiment the SM includes 64 DPUs 32 SFUs and 32 LSUs . In another embodiment the L1 cache is not included within the texture unit and is instead included with the shared memory with a separate direct connection to the crossbar .

Each SM includes an interconnect network that connects each of the functional units to the register file and to the shared memory through the interconnect network . In one embodiment the interconnect network is a crossbar that can be configured to connect any of the functional units to any of the registers in the register file to any of the J texture unit L1 caches or the memory locations in shared memory .

In one embodiment the SM is implemented within a GPU. In such an embodiment the SM comprises J texture unit L1 caches . The texture unit L1 caches include texture processing circuitry that is configured to access texture maps i.e. a 2D array of texels from the memory and sample the texture maps to produce sampled texture values for use in shader programs. The texture unit L1 caches implement texture operations using mip maps i.e. texture maps of varying levels of detail . In one embodiment the SM includes 16 texture unit L1 caches . As described further herein the texture unit L1 caches are also configured to receive load and store requests from the LSUs and to coalesce the texture accesses and the load and store requests to generate coalesced memory operations that are output to a memory system that includes the shared memory . The memory system may also include the L2 cache memory and a system memory not shown .

The PPU described above may be configured to perform highly parallel computations much faster than conventional CPUs. Parallel computing has advantages in graphics processing data compression biometrics stream processing algorithms and the like.

As shown in the graphics processing pipeline comprises a pipeline architecture that includes a number of stages. The stages include but are not limited to a data assembly stage a vertex shading stage a primitive assembly stage a geometry shading stage a viewport scale cull and clip SCC stage a rasterization stage a fragment shading stage and a raster operations stage . In one embodiment the input data comprises commands that configure the processing units to implement the stages of the graphics processing pipeline and geometric primitives e.g. points lines triangles quads triangle strips or fans etc. to be processed by the stages. The output data may comprise pixel data i.e. color data that is copied into a frame buffer or other type of surface data structure in a memory.

The data assembly stage receives the input data that specifies vertex data for high order surfaces primitives or the like. The data assembly stage collects the vertex data in a temporary storage or queue such as by receiving a command from the host processor that includes a pointer to a buffer in memory and reading the vertex data from the buffer. The vertex data is then transmitted to the vertex shading stage for processing.

The vertex shading stage processes vertex data by performing a set of operations i.e. a vertex shader or a program once for each of the vertices. Vertices may be e.g. specified as a 4 coordinate vector associated with one or more vertex attributes. The vertex shading stage may manipulate properties such as position color texture coordinates and the like. In other words the vertex shading stage performs operations on the vertex coordinates or other vertex attributes associated with a vertex. Such operations commonly including lighting operations i.e. modifying color attributes for a vertex and transformation operations i.e. modifying the coordinate space for a vertex . For example vertices may be specified using coordinates in an object coordinate space which are transformed by multiplying the coordinates by a matrix that translates the coordinates from the object coordinate space into a world space or a normalized device coordinate NCD space. The vertex shading stage generates transformed vertex data that is transmitted to the primitive assembly stage .

The primitive assembly stage collects vertices output by the vertex shading stage and groups the vertices into geometric primitives for processing by the geometry shading stage . For example the primitive assembly stage may be configured to group every three consecutive vertices as a geometric primitive i.e. a triangle for transmission to the geometry shading stage . In some embodiments specific vertices may be reused for consecutive geometric primitives e.g. two consecutive triangles in a triangle strip may share two vertices . The primitive assembly stage transmits geometric primitives i.e. a collection of associated vertices to the geometry shading stage .

The geometry shading stage processes geometric primitives by performing a set of operations i.e. a geometry shader or program on the geometric primitives. Tessellation operations may generate one or more geometric primitives from each geometric primitive. In other words the geometry shading stage may subdivide each geometric primitive into a finer mesh of two or more geometric primitives for processing by the rest of the graphics processing pipeline . The geometry shading stage transmits geometric primitives to the viewport SCC stage .

The viewport SCC stage performs viewport scaling culling and clipping of the geometric primitives. Each surface being rendered to is associated with an abstract camera position. The camera position represents a location of a viewer looking at the scene and defines a viewing frustum that encloses the objects of the scene. The viewing frustum may include a viewing plane a rear plan and four clipping planes. Any geometric primitive entirely outside of the viewing frustum may be culled i.e. discarded because the geometric primitive will not contribute to the final rendered scene. Any geometric primitive that is partially inside the viewing frustum and partially outside the viewing frustum may be clipped i.e. transformed into a new geometric primitive that is enclosed within the viewing frustum. Furthermore geometric primitives may each be scaled based on depth of the viewing frustum. All potentially visible geometric primitives are then transmitted to the rasterization stage .

The rasterization stage converts the 3D geometric primitives into 2D fragments. The rasterization stage may be configured to utilize the vertices of the geometric primitives to setup a set of plane equations from which various attributes can be interpolated. The rasterization stage may also compute a coverage mask for a plurality of pixels that indicates whether one or more sample locations for the pixel intercept the geometric primitive. In one embodiment z testing may also be performed to determine if the geometric primitive is occluded by other geometric primitives that have already been rasterized. The rasterization stage generates fragment data i.e. interpolated vertex attributes associated with a particular sample location for each covered pixel that are transmitted to the fragment shading stage .

The fragment shading stage processes fragment data by performing a set of operations i.e. a fragment shader or a program on each of the fragments. The fragment shading stage may generate pixel data i.e. color values for the fragment such as by performing lighting operations or sampling texture maps using interpolated texture coordinates for the fragment. The fragment shading stage generates pixel data that is transmitted to the raster operations stage .

The raster operations stage may perform various operations on the pixel data such as performing alpha tests stencil tests and blending the pixel data with other pixel data corresponding to other fragments associated with the pixel. When the raster operations stage has finished processing the pixel data i.e. the output data the pixel data may be written to a render target such as a frame buffer a color buffer or the like.

The rasterized coverage bit masks that are generated by the rasterization stage may pass through the fragment shading stage and raster operations stage and be written to a texture map that is stored in memory . In another embodiment the rasterization stage writes the texture map directly. The texture map may be represented in a bit count texture format so that each rasterized coverage bit mask that is read by the fragment shading stage is converted into a scalar value that represents opacity. In one embodiment the rasterized coverage masks stored in the texture map are retained and are not overwritten with the scalar values. The rasterized coverage bit masks may be very compact compared with the corresponding scalar values. For example each rasterized coverage bit mask may be as small as one bit per pixel compared with 16 or more bits that represent a floating point format scalar value.

It will be appreciated that one or more additional stages may be included in the graphics processing pipeline in addition to or in lieu of one or more of the stages described above. Various implementations of the abstract graphics processing pipeline may implement different stages. Furthermore one or more of the stages described above may be excluded from the graphics processing pipeline in some embodiments such as the geometry shading stage . Other types of graphics processing pipelines are contemplated as being within the scope of the present disclosure. Furthermore any of the stages of the graphics processing pipeline may be implemented by one or more dedicated hardware units within a graphics processor such as PPU . Other stages of the graphics processing pipeline may be implemented by programmable hardware units such as the SM of the PPU .

A conventional technique that is used to generate opacity data for voxels stores rasterized coverage bit masks in a conventional texture format such as R32 UINT or R10G10B10A2 UNORM or other formats defined in the Microsoft DirectX applications programming interface . While configured in a compute mode the SM may be configured to read the rasterized coverage bit masks compute alpha values representing the number of samples that are covered for each pixel and write intermediate data e.g. opacity values represented as unsigned integers without any further processing back to the texture map. The rasterized coverage bit masks are overwritten by the unsigned integer values so that the rasterized coverage bit masks are not retained in the texture map. The intermediate data is written to the same texture map because storing the intermediate data would consume twice as much memory and the rasterized coverage bit masks are not needed after they are converted.

The conventional conversion technique reads the texture map and also writes intermediate data to the texture map thereby consuming twice as much memory bandwidth as is needed to just read the texture map. In contrast when a bit count texture format is used the bit count format texture map is read and is not written during the conversion process which is performed while the SM is configured in a graphics mode. Each rasterized coverage bit mask is converted to a scalar value when the rasterized coverage bit mask is read and the rasterized coverage bit masks are retained in the bit count format texture map . Therefore the format of the scalar value is not limited to a number of bits that can be stored in the bit count format texture map . The scalar value may be represented by a number of bits that is less than equal to or greater than the number of bits used to represent a corresponding rasterized coverage bit mask. Some operations such as those used to implement voxel based global illumination or ambient occlusion calculations use 16 or 32 bit floating point format scalar values that represent opacity. When the bit count format is used for the texture map the number of bits representing the rasterized coverage bit mask for each pixel may be less than the number of bits representing the scalar value for the pixel. For example when a 4 AA sample mode is used 4 bits represent the rasterized coverage bit mask for each pixel and 16 or 32 bits may be used to represent a floating point format scalar value for each pixel.

The conventional conversion technique performs the conversion process when configured in a compute mode by reading the rasterized coverage bit masks from memory by the SM and computing an opacity value for each pixel. The opacity value is an unsigned integer value that is written back to the memory overwriting the rasterized coverage bit masks in the memory. When configured in the compute mode as opposed to a graphics mode at least some of the graphics processing operations may not be performed. In particular operations performed by the fragment shading stage and the raster operations stage may not be performed. At a later time the opacity values are read from the memory to compute an opacity value for each voxel. In sum the conventional technique reads the texture map writes the texture map and then reads the texture map again. In contrast when a bit count texture format is used the texture map is read once when an SM is configured in a graphics mode the rasterized bit mask is converted to a scalar value and then processed by the fragment shading stage and or the raster operations stage so the memory access bandwidth is reduced compared with the conventional technique.

The bit mask conversion unit receives the rasterized coverage bit mask and computes a scalar value that indicates the number of samples that are covered according to the rasterized coverage bit mask. The scalar value represents an opacity corresponding to each rasterized coverage bit mask. The filter unit may be configured to combine the scalar values using filtering operations for one or more pixels to produce a scalar value that represents an opacity corresponding to a voxel or multiple pixels. For example the filter unit may be configured to sum the per pixel scalar values and divide by the number of pixels included in a voxel for a voxel plane. The filtered scalar values are output by the texture unit L1 cache as filtered texels .

The system also includes input devices a graphics processor and a display i.e. a conventional CRT cathode ray tube LCD liquid crystal display LED light emitting diode plasma display or the like. User input may be received from the input devices e.g. keyboard mouse touchpad microphone and the like. In one embodiment the graphics processor may include a plurality of shader modules a rasterization module etc. Each of the foregoing modules may even be situated on a single semiconductor platform to form a graphics processing unit GPU .

In the present description a single semiconductor platform may refer to a sole unitary semiconductor based integrated circuit or chip. It should be noted that the term single semiconductor platform may also refer to multi chip modules with increased connectivity which simulate on chip operation and make substantial improvements over utilizing a conventional central processing unit CPU and bus implementation. Of course the various modules may also be situated separately or in various combinations of semiconductor platforms per the desires of the user.

The system may also include a secondary storage . The secondary storage includes for example a hard disk drive and or a removable storage drive representing a floppy disk drive a magnetic tape drive a compact disk drive digital versatile disk DVD drive recording device universal serial bus USB flash memory. The removable storage drive reads from and or writes to a removable storage unit in a well known manner.

Computer programs or computer control logic algorithms may be stored in the main memory and or the secondary storage . Such computer programs when executed enable the system to perform various functions. For example a compiler program that is configured to examiner a shader program and enable or disable attribute buffer combining may be stored in the main memory . The compiler program may be executed by the central processor or the graphics processor . The main memory the storage and or any other storage are possible examples of computer readable media.

In one embodiment the architecture and or functionality of the various previous figures may be implemented in the context of the central processor the graphics processor an integrated circuit not shown that is capable of at least a portion of the capabilities of both the central processor and the graphics processor a chipset i.e. a group of integrated circuits designed to work and sold as a unit for performing related functions etc. and or any other integrated circuit for that matter.

Still yet the architecture and or functionality of the various previous figures may be implemented in the context of a general computer system a circuit board system a game console system dedicated for entertainment purposes an application specific system and or any other desired system. For example the system may take the form of a desktop computer laptop computer server workstation game consoles embedded system and or any other type of logic. Still yet the system may take the form of various other devices including but not limited to a personal digital assistant PDA device a mobile phone device a television etc.

Further while not shown the system may be coupled to a network e.g. a telecommunications network local area network LAN wireless network wide area network WAN such as the Internet peer to peer network cable network or the like for communication purposes.

While various embodiments have been described above it should be understood that they have been presented by way of example only and not limitation. Thus the breadth and scope of a preferred embodiment should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

