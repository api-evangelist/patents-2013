---

title: Instructions to mark beginning and end of non transactional code region requiring write back to persistent storage
abstract: A processor in described having an interface to non-volatile random access memory and logic circuitry. The logic circuitry is to identify cache lines modified by a transaction which views the non-volatile random access memory as the transaction's persistence storage. The logic circuitry is also to identify cache lines modified by a software process other than a transaction that also views said non-volatile random access memory as persistence storage.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09547594&OS=09547594&RS=09547594
owner: Intel Corporation
number: 09547594
owner_city: Santa Clara
owner_country: US
publication_date: 20130315
---
This invention relates generally to the field of computer systems. More particularly the invention relates to an apparatus and method for implementing a multi level memory hierarchy including a non volatile memory tier.

One of the limiting factors for computer innovation today is memory and storage technology. In conventional computer systems system memory also known as main memory primary memory executable memory is typically implemented by dynamic random access memory DRAM . DRAM based memory consumes power even when no memory reads or writes occur because it must constantly recharge internal capacitors. DRAM based memory is volatile which means data stored in DRAM memory is lost once the power is removed. Conventional computer systems also rely on multiple levels of caching to improve performance. A cache is a high speed memory positioned between the processor and system memory to service memory access requests faster than they could be serviced from system memory. Such caches are typically implemented with static random access memory SRAM . Cache management protocols may be used to ensure that the most frequently accessed data and instructions are stored within one of the levels of cache thereby reducing the number of memory access operations and improving performance.

With respect to mass storage also known as secondary storage or disk storage conventional mass storage devices typically include magnetic media e.g. hard disk drives optical media e.g. compact disc CD drive digital versatile disc DVD etc. holographic media and or mass storage flash memory e.g. solid state drives SSDs removable flash drives etc. . Generally these storage devices are considered Input Output I O devices because they are accessed by the processor through various I O adapters that implement various I O protocols. These I O adapters and I O protocols consume a significant amount of power and can have a significant impact on the die area and the form factor of the platform. Portable or mobile devices e.g. laptops netbooks tablet computers personal digital assistant PDAs portable media players portable gaming devices digital cameras mobile phones smartphones feature phones etc. that have limited battery life when not connected to a permanent power supply may include removable mass storage devices e.g. Embedded Multimedia Card eMMC Secure Digital SD card that are typically coupled to the processor via low power interconnects and I O controllers in order to meet active and idle power budgets.

With respect to firmware memory such as boot memory also known as BIOS flash a conventional computer system typically uses flash memory devices to store persistent system information that is read often but seldom or never written to. For example the initial instructions executed by a processor to initialize key system components during a boot process Basic Input and Output System BIOS images are typically stored in a flash memory device. Flash memory devices that are currently available in the market generally have limited speed e.g. 50 MHz . This speed is further reduced by the overhead for read protocols e.g. 2.5 MHz . In order to speed up the BIOS execution speed conventional processors generally cache a portion of BIOS code during the Pre Extensible Firmware Interface PEI phase of the boot process. The size of the processor cache places a restriction on the size of the BIOS code used in the PEI phase also known as the PEI BIOS code .

Phase change memory PCM also sometimes referred to as phase change random access memory PRAM or PCRAM PCME Ovonic Unified Memory or Chalcogenide RAM C RAM is a type of non volatile computer memory which exploits the unique behavior of chalcogenide glass. As a result of heat produced by the passage of an electric current chalcogenide glass can be switched between two states crystalline and amorphous. Recent versions of PCM can achieve two additional distinct states.

PCM provides higher performance than flash because the memory element of PCM can be switched more quickly writing changing individual bits to either 1 or 0 can be done without the need to first erase an entire block of cells and degradation from writes is slower a PCM device may survive approximately 100 million write cycles PCM degradation is due to thermal expansion during programming metal and other material migration and other mechanisms .

In the following description numerous specific details such as logic implementations opcodes means to specify operands resource partitioning sharing duplication implementations types and interrelationships of system components and logic partitioning integration choices are set forth in order to provide a more thorough understanding of the present invention. It will be appreciated however by one skilled in the art that the invention may be practiced without such specific details. In other instances control structures gate level circuits and full software instruction sequences have not been shown in detail in order not to obscure the invention. Those of ordinary skill in the art with the included descriptions will be able to implement appropriate functionality without undue experimentation.

References in the specification to one embodiment an embodiment an example embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is submitted that it is within the knowledge of one skilled in the art to effect such feature structure or characteristic in connection with other embodiments whether or not explicitly described.

In the following description and claims the terms coupled and connected along with their derivatives may be used. It should be understood that these terms are not intended as synonyms for each other. Coupled is used to indicate that two or more elements which may or may not be in direct physical or electrical contact with each other co operate or interact with each other. Connected is used to indicate the establishment of communication between two or more elements that are coupled with each other.

Bracketed text and blocks with dashed borders e.g. large dashes small dashes dot dash dots are sometimes used herein to illustrate optional operations components that add additional features to embodiments of the invention. However such notation should not be taken to mean that these are the only options or optional operations components and or that blocks with solid borders are not optional in certain embodiments.

Memory capacity and performance requirements continue to increase with an increasing number of processor cores and new usage models such as virtualization. In addition memory power and cost have become a significant component of the overall power and cost respectively of electronic systems.

Some embodiments solve the above challenges by intelligently subdividing the performance requirement and the capacity requirement between memory technologies. The focus of this approach is on providing performance with a relatively small amount of relatively higher speed memory such as DRAM while implementing the bulk of the system memory using significantly denser non volatile random access memory NVRAM . Embodiments of the invention described below define platform configurations that enable hierarchical memory subsystem organizations for the use of NVRAM. The use of NVRAM in the memory hierarchy also enables new usages such as expanded boot space and mass storage implementations.

Some embodiments solve the above challenges by intelligently subdividing the performance requirement and the capacity requirement between memory technologies. The focus of this approach is on providing performance with a relatively small amount of relatively higher speed memory such as DRAM while implementing the bulk of the system memory using significantly denser non volatile random access memory NVRAM . Embodiments of the invention described below define platform configurations that enable hierarchical memory subsystem organizations for the use of NVRAM. The use of NVRAM in the memory hierarchy also enables new usages such as expanded boot space and mass storage implementations.

 1 It maintains its content even if power is removed similar to FLASH memory used in solid state disks SSD and different from SRAM and DRAM which are volatile 

 4 rewritable and erasable at a lower level of granularity e.g. byte level than FLASH found in SSDs which can only be rewritten and erased a block at a time minimally 64 Kbyte in size for NOR FLASH and 16 Kbyte for NAND FLASH 

 6 capable of being coupled to the processor over a bus using a protocol that supports identifiers IDs to support out of order operation and allowing access at a level of granularity small enough to support operation of the NVRAM as system memory e.g. cache line size such as 64 or 128 byte . For example the bus may be a non out of order memory bus e.g. a DDR bus such as DDR3 DDR4 etc. . As another example the bus may be PCI express PCIE bus desktop management interface DMI bus or any other type of bus utilizing an out of order protocol and a small enough payload size e.g. cache line size such as 64 or 128 byte and

As mentioned above in contrast to FLASH memory which must be rewritten and erased a complete block at a time the level of granularity at which NVRAM is accessed in any given implementation may depend on the particular memory controller and the particular memory bus or other type of bus to which the NVRAM is coupled. For example in some implementations where NVRAM is used as system memory the NVRAM may be accessed at the granularity of a cache line e.g. a 64 byte or 128 Byte cache line notwithstanding an inherent ability to be accessed at the granularity of a byte because cache line is the level at which the memory subsystem accesses memory. Thus when NVRAM is deployed within a memory subsystem it may be accessed at the same level of granularity as the DRAM e.g. the near memory used in the same memory subsystem. Even so the level of granularity of access to the NVRAM by the memory controller and memory bus or other type of bus is smaller than that of the block size used by Flash and the access size of the I O subsystem s controller and bus.

NVRAM may also incorporate wear leveling algorithms to account for the fact that the storage cells at the far memory level begin to wear out after a number of write accesses especially where a significant number of writes may occur such as in a system memory implementation. Since high cycle count blocks are most likely to wear out in this manner wear leveling spreads writes across the far memory cells by swapping addresses of high cycle count blocks with low cycle count blocks. Note that most address swapping is typically transparent to application programs because it is handled by hardware lower level software e.g. a low level driver or operating system or a combination of the two.

The far memory of some embodiments of the invention is implemented with NVRAM but is not necessarily limited to any particular memory technology. Far memory is distinguishable from other instruction and data memory storage technologies in terms of its characteristics and or its application in the memory storage hierarchy. For example far memory is different from 

Far memory may be used as instruction and data storage that is directly addressable by a processor and is able to sufficiently keep pace with the processor in contrast to FLASH magnetic disk optical disc applied as mass storage. Moreover as discussed above and described in detail below far memory may be placed on a memory bus and may communicate directly with a memory controller that in turn communicates directly with the processor .

Far memory may be combined with other instruction and data storage technologies e.g. DRAM to form hybrid memories also known as Co locating PCM and DRAM first level memory and second level memory FLAM FLASH and DRAM . Note that at least some of the above technologies including PCM PCMS may be used for mass storage instead of or in addition to system memory and need not be random accessible byte addressable or directly addressable by the processor when applied in this manner.

For convenience of explanation most of the remainder of the application will refer to NVRAM or more specifically PCM or PCMS as the technology selection for the far memory . As such the terms NVRAM PCM PCMS and far memory may be used interchangeably in the following discussion. However it should be realized as discussed above that different technologies may also be utilized for far memory. Also that NVRAM is not limited for use as far memory.

 Near memory is an intermediate level of memory configured in front of a far memory that has lower read write access latency relative to far memory and or more symmetric read write access latency i.e. having read times which are roughly equivalent to write times . In some embodiments the near memory has significantly lower write latency than the far memory but similar e.g. slightly lower or equal read latency for instance the near memory may be a volatile memory such as volatile random access memory VRAM and may comprise a DRAM or other high speed capacitor based memory. Note however that the underlying principles of the invention are not limited to these specific memory types. Additionally the near memory may have a relatively lower density and or may be more expensive to manufacture than the far memory .

In one embodiment near memory is configured between the far memory and the internal processor caches . In some of the embodiments described below near memory is configured as one or more memory side caches MSCs to mask the performance and or usage limitations of the far memory including for example read write latency limitations and memory degradation limitations. In these implementations the combination of the MSC and far memory operates at a performance level which approximates is equivalent or exceeds a system which uses only DRAM as system memory. As discussed in detail below although shown as a cache in the near memory may include modes in which it performs other roles either in addition to or in lieu of performing the role of a cache.

Near memory can be located on the processor die as cache s and or located external to the processor die as caches e.g. on a separate die located on the CPU package located outside the CPU package with a high bandwidth link to the CPU package for example on a memory dual in line memory module DIMM a riser mezzanine or a computer motherboard . The near memory may be coupled in communicate with the processor using a single or multiple high bandwidth links such as DDR or other high bandwidth links as described in detail below .

The caches illustrated in may be dedicated to a particular system memory address range or a set of non contiguous address ranges. For example cache is dedicated to acting as an MSC for system memory address range 1 and caches and are dedicated to acting as MSCs for non overlapping portions of system memory address ranges 2 and 3 . The latter implementation may be used for systems in which the SPA space used by the processor is interleaved into an address space used by the caches e.g. when configured as MSCs . In some embodiments this latter address space is referred to as a memory channel address MCA space. In one embodiment the internal caches perform caching operations for the entire SPA space.

System memory as used herein is memory which is visible to and or directly addressable by software executed on the processor while the cache memories may operate transparently to the software in the sense that they do not form a directly addressable portion of the system address space but the cores may also support execution of instructions to allow software to provide some control configuration policies hints etc. to some or all of the cache s . The subdivision of system memory into regions may be performed manually as part of a system configuration process e.g. by a system designer and or may be performed automatically by software.

In one embodiment the system memory regions are implemented using far memory e.g. PCM and in some embodiments near memory configured as system memory. System memory address range 4 represents an address range which is implemented using a higher speed memory such as DRAM which may be a near memory configured in a system memory mode as opposed to a caching mode .

As indicated near memory may be implemented to operate in a variety of different modes including a first mode in which it operates as a cache for far memory near memory as cache for FM B a second mode in which it operates as system memory A and occupies a portion of the SPA space sometimes referred to as near memory direct access mode and one or more additional modes of operation such as a scratchpad memory or as a write buffer . In some embodiments of the invention the near memory is partitionable where each partition may concurrently operate in a different one of the supported modes and different embodiments may support configuration of the partitions e.g. sizes modes by hardware e.g. fuses pins firmware and or software e.g. through a set of programmable range registers within the MSC controller within which for example may be stored different binary codes to identify each mode and partition .

System address space A in is used to illustrate operation when near memory is configured as a MSC for far memory B. In this configuration system address space A represents the entire system address space and system address space B does not exist . Alternatively system address space B is used to show an implementation when all or a portion of near memory is assigned a portion of the system address space. In this embodiment system address space B represents the range of the system address space assigned to the near memory A and system address space A represents the range of the system address space assigned to NVRAM .

In addition when acting as a cache for far memory B the near memory may operate in various sub modes under the control of the MSC controller . In each of these modes the near memory address space NMA is transparent to software in the sense that the near memory does not form a directly addressable portion of the system address space. These modes include but are not limited to the following 

 1 Write Back Caching Mode In this mode all or portions of the near memory acting as a FM cache B is used as a cache for the NVRAM far memory FM B. While in write back mode every write operation is directed initially to the near memory as cache for FM B assuming that the cache line to which the write is directed is present in the cache . A corresponding write operation is performed to update the NVRAM FM B only when the cache line within the near memory as cache for FM B is to be replaced by another cache line in contrast to write through mode described below in which each write operation is immediately propagated to the NVRAM FM B .

 2 Near Memory Bypass Mode In this mode all reads and writes bypass the NM acting as a FM cache B and go directly to the NVRAM FM B. Such a mode may be used for example when an application is not cache friendly or requires data to be committed to persistence at the granularity of a cache line. In one embodiment the caching performed by the processor caches A and the NM acting as a FM cache B operate independently of one another. Consequently data may be cached in the NM acting as a FM cache B which is not cached in the processor caches A and which in some cases may not be permitted to be cached in the processor caches A and vice versa. Thus certain data which may be designated as uncacheable in the processor caches may be cached within the NM acting as a FM cache B.

 3 Near Memory Read Cache Write Bypass Mode This is a variation of the above mode where read caching of the persistent data from NVRAM FM B is allowed i.e. the persistent data is cached in the near memory as cache for far memory B for read only operations . This is useful when most of the persistent data is Read Only and the application usage is cache friendly.

 4 Near Memory Read Cache Write Through Mode This is a variation of the near memory read cache write bypass mode where in addition to read caching write hits are also cached. Every write to the near memory as cache for FM B causes a write to the FM B. Thus due to the write through nature of the cache cache line persistence is still guaranteed.

When acting in near memory direct access mode all or portions of the near memory as system memory A are directly visible to software and form part of the SPA space. Such memory may be completely under software control. Such a scheme may create a non uniform memory address NUMA memory domain for software where it gets higher performance from near memory relative to NVRAM system memory . By way of example and not limitation such a usage may be employed for certain high performance computing HPC and graphics applications which require very fast access to certain data structures.

In an alternate embodiment the near memory direct access mode is implemented by pinning certain cache lines in near memory i.e. cache lines which have data that is also concurrently stored in NVRAM . Such pinning may be done effectively in larger multi way set associative caches.

Thus as indicated the NVRAM may be implemented to operate in a variety of different modes including as far memory B e.g. when near memory is present operating whether the near memory is acting as a cache for the FM via a MSC control or not accessed directly after cache s A and without MSC control just NVRAM system memory not as far memory because there is no near memory present operating and accessed without MSC control NVRAM mass storage A BIOS NVRAM and TPM NVRAM . While different embodiments may specify the NVRAM modes in different ways describes the use of a decode table .

By way of example operation while the near memory as cache for FM B is in the write back caching is described. In one embodiment while the near memory as cache for FM B is in the write back caching mode mentioned above a read operation will first arrive at the MSC controller which will perform a look up to determine if the requested data is present in the near memory acting as a cache for FM B e.g. utilizing a tag cache . If present it will return the data to the requesting CPU core or I O device through I O subsystem . If the data is not present the MSC controller will send the request along with the system memory address to an NVRAM controller . The NVRAM controller will use the decode table to translate the system memory address to an NVRAM physical device address PDA and direct the read operation to this region of the far memory B. In one embodiment the decode table includes an address indirection table AIT component which the NVRAM controller uses to translate between system memory addresses and NVRAM PDAs. In one embodiment the AIT is updated as part of the wear leveling algorithm implemented to distribute memory access operations and thereby reduce wear on the NVRAM FM B. Alternatively the AIT may be a separate table stored within the NVRAM controller .

Upon receiving the requested data from the NVRAM FM B the NVRAM controller will return the requested data to the MSC controller which will store the data in the MSC near memory acting as an FM cache B and also send the data to the requesting processor core or I O Device through I O subsystem . Subsequent requests for this data may be serviced directly from the near memory acting as a FM cache B until it is replaced by some other NVRAM FM data.

As mentioned in one embodiment a memory write operation also first goes to the MSC controller which writes it into the MSC near memory acting as a FM cache B. In write back caching mode the data may not be sent directly to the NVRAM FM B when a write operation is received. For example the data may be sent to the NVRAM FM B only when the location in the MSC near memory acting as a FM cache B in which the data is stored must be re used for storing data for a different system memory address. When this happens the MSC controller notices that the data is not current in NVRAM FM B and will thus retrieve it from near memory acting as a FM cache B and send it to the NVRAM controller . The NVRAM controller looks up the PDA for the system memory address and then writes the data to the NVRAM FM B.

In the NVRAM controller is shown connected to the FM B NVRAM mass storage A and BIOS NVRAM using three separate lines. This does not necessarily mean however that there are three separate physical buses or communication channels connecting the NVRAM controller to these portions of the NVRAM . Rather in some embodiments a common memory bus or other type of bus is used to communicatively couple the NVRAM controller to the FM B NVRAM mass storage A and BIOS NVRAM . For example in one embodiment the three lines in represent a bus such as a memory bus e.g. a DDR3 DDR4 etc bus over which the NVRAM controller implements a e.g. out of order protocol to communicate with the NVRAM . The NVRAM controller may also communicate with the NVRAM over a bus supporting a native protocol such as a PCI express bus desktop management interface DMI bus or any other type of bus utilizing an out of order protocol and a small enough payload size e.g. cache line size such as 64 or 128 byte .

In one embodiment computer system includes integrated memory controller IMC which performs the central memory access control for processor which is coupled to 1 a memory side cache MSC controller to control access to near memory NM acting as a far memory cache B and 2 a NVRAM controller to control access to NVRAM . Although illustrated as separate units in the MSC controller and NVRAM controller may logically form part of the IMC .

In the illustrated embodiment the MSC controller includes a set of range registers which specify the mode of operation in use for the NM acting as a far memory cache B e.g. write back caching mode near memory bypass mode etc described above . In the illustrated embodiment DRAM is used as the memory technology for the NM acting as cache for far memory B. In response to a memory access request the MSC controller may determine depending on the mode of operation specified in the range registers whether the request can be serviced from the NM acting as cache for FM B or whether the request must be sent to the NVRAM controller which may then service the request from the far memory FM portion B of the NVRAM .

In an embodiment where NVRAM is implemented with PCMS NVRAM controller is a PCMS controller that performs access with protocols consistent with the PCMS technology. As previously discussed the PCMS memory is inherently capable of being accessed at the granularity of a byte. Nonetheless the NVRAM controller may access a PCMS based far memory B at a lower level of granularity such as a cache line e.g. a 64 bit or 128 bit cache line or any other level of granularity consistent with the memory subsystem. The underlying principles of the invention are not limited to any particular level of granularity for accessing a PCMS based far memory B. In general however when PCMS based far memory B is used to form part of the system address space the level of granularity will be higher than that traditionally used for other non volatile storage technologies such as FLASH which can only perform rewrite and erase operations at the level of a block minimally 64 Kbyte in size for NOR FLASH and 16 Kbyte for NAND FLASH .

In the illustrated embodiment NVRAM controller can read configuration data to establish the previously described modes sizes etc. for the NVRAM from decode table or alternatively can rely on the decoding results passed from IMC and I O subsystem . For example at either manufacturing time or in the field computer system can program decode table to mark different regions of NVRAM as system memory mass storage exposed via SATA interfaces mass storage exposed via USB Bulk Only Transport BOT interfaces encrypted storage that supports TPM storage among others. The means by which access is steered to different partitions of NVRAM device is via a decode logic. For example in one embodiment the address range of each partition is defined in the decode table . In one embodiment when IMC receives an access request the target address of the request is decoded to reveal whether the request is directed toward memory NVRAM mass storage or I O. If it is a memory request IMC and or the MSC controller further determines from the target address whether the request is directed to NM as cache for FM B or to FM B. For FM B access the request is forwarded to NVRAM controller . IMC passes the request to the I O subsystem if this request is directed to I O e.g. non storage and storage I O devices . I O subsystem further decodes the address to determine whether the address points to NVRAM mass storage A BIOS NVRAM or other non storage or storage I O devices. If this address points to NVRAM mass storage A or BIOS NVRAM I O subsystem forwards the request to NVRAM controller . If this address points to TMP NVRAM I O subsystem passes the request to TPM to perform secured access.

The presence of a new memory architecture such as described herein provides for a wealth of new possibilities. Although discussed at much greater length further below some of these possibilities are quickly highlighted immediately below.

According to one possible implementation NVRAM acts as a total replacement or supplement for traditional DRAM technology in system memory. In one embodiment NVRAM represents the introduction of a second level system memory e.g. the system memory may be viewed as having a first level system memory comprising near memory as cache B part of the DRAM device and a second level system memory comprising far memory FM B part of the NVRAM .

According to some embodiments NVRAM acts as a total replacement or supplement for the flash magnetic optical mass storage B. As previously described in some embodiments even though the NVRAM A is capable of byte level addressability NVRAM controller may still access NVRAM mass storage A in blocks of multiple bytes depending on the implementation e.g. 64 Kbytes 128 Kbytes etc. . The specific manner in which data is accessed from NVRAM mass storage A by NVRAM controller may be transparent to software executed by the processor . For example even through NVRAM mass storage A may be accessed differently from Flash magnetic optical mass storage A the operating system may still view NVRAM mass storage A as a standard mass storage device e.g. a serial ATA hard drive or other standard form of mass storage device .

In an embodiment where NVRAM mass storage A acts as a total replacement for the flash magnetic optical mass storage B it is not necessary to use storage drivers for block addressable storage access. The removal of storage driver overhead from storage access can increase access speed and save power. In alternative embodiments where it is desired that NVRAM mass storage A appears to the OS and or applications as block accessible and indistinguishable from flash magnetic optical mass storage B emulated storage drivers can be used to expose block accessible interfaces e.g. Universal Serial Bus USB Bulk Only Transfer BOT 1.0 Serial Advanced Technology Attachment SATA 3.0 and the like to the software for accessing NVRAM mass storage A.

In one embodiment NVRAM acts as a total replacement or supplement for firmware memory such as BIOS flash and TPM flash illustrated with dotted lines in to indicate that they are optional . For example the NVRAM may include a BIOS NVRAM portion to supplement or replace the BIOS flash and may include a TPM NVRAM portion to supplement or replace the TPM flash . Firmware memory can also store system persistent states used by a TPM to protect sensitive system information e.g. encryption keys . In one embodiment the use of NVRAM for firmware memory removes the need for third party flash parts to store code and data that are critical to the system operations.

Continuing then with a discussion of the system of in some embodiments the architecture of computer system may include multiple processors although a single processor is illustrated in for simplicity. Processor may be any type of data processor including a general purpose or special purpose central processing unit CPU an application specific integrated circuit ASIC or a digital signal processor DSP . For example processor may be a general purpose processor such as a Core i3 i5 i7 2 Duo and Quad Xeon or Itanium processor all of which are available from Intel Corporation of Santa Clara Calif. Alternatively processor may be from another company such as ARM Holdings Ltd of Sunnyvale Calif. MIPS Technologies of Sunnyvale Calif. etc. Processor may be a special purpose processor such as for example a network or communication processor compression engine graphics processor co processor embedded processor or the like. Processor may be implemented on one or more chips included within one or more packages. Processor may be a part of and or may be implemented on one or more substrates using any of a number of process technologies such as for example BiCMOS CMOS or NMOS. In the embodiment shown in processor has a system on a chip SOC configuration.

In one embodiment the processor includes an integrated graphics unit which includes logic for executing graphics commands such as 3D or 2D graphics commands. While the embodiments of the invention are not limited to any particular integrated graphics unit in one embodiment the graphics unit is capable of executing industry standard graphics commands such as those specified by the Open GL and or Direct X application programming interfaces APIs e.g. OpenGL 4.1 and Direct X 11 .

The processor may also include one or more cores although a single core is illustrated in again for the sake of clarity. In many embodiments the core s includes internal functional blocks such as one or more execution units retirement units a set of general purpose and specific registers etc. If the core s are multi threaded or hyper threaded then each hardware thread may be considered as a logical core as well. The cores may be homogenous or heterogeneous in terms of architecture and or instruction set. For example some of the cores may be in order while others are out of order. As another example two or more of the cores may be capable of executing the same instruction set while others may be capable of executing only a subset of that instruction set or a different instruction set.

The processor may also include one or more caches such as cache which may be implemented as a SRAM and or a DRAM. In many embodiments that are not shown additional caches other than cache are implemented so that multiple levels of cache exist between the execution units in the core s and memory devices B and B. For example the set of shared cache units may include an upper level cache such as a level 1 L1 cache mid level caches such as level 2 L2 level 3 L3 level 4 L4 or other levels of cache an LLC and or different combinations thereof. In different embodiments cache may be apportioned in different ways and may be one of many different sizes in different embodiments. For example cache may be an 8 megabyte MB cache a 16 MB cache etc. Additionally in different embodiments the cache may be a direct mapped cache a fully associative cache a multi way set associative cache or a cache with another type of mapping. In other embodiments that include multiple cores cache may include one large portion shared among all cores or may be divided into several separately functional slices e.g. one slice for each core . Cache may also include one portion shared among all cores and several other portions that are separate functional slices per core.

The processor may also include a home agent which includes those components coordinating and operating core s . The home agent unit may include for example a power control unit PCU and a display unit. The PCU may be or include logic and components needed for regulating the power state of the core s and the integrated graphics unit . The display unit is for driving one or more externally connected displays.

As mentioned in some embodiments processor includes an integrated memory controller IMC near memory cache MSC controller and NVRAM controller all of which can be on the same chip as processor or on a separate chip and or package connected to processor . DRAM device may be on the same chip or a different chip as the IMC and MSC controller thus one chip may have processor and DRAM device one chip may have the processor and another the DRAM device and these chips may be in the same or different packages one chip may have the core s and another the IMC MSC controller and DRAM these chips may be in the same or different packages one chip may have the core s another the IMC and MSC controller and another the DRAM these chips may be in the same or different packages etc.

In some embodiments processor includes an I O subsystem coupled to IMC . I O subsystem enables communication between processor and the following serial or parallel I O devices one or more networks such as a Local Area Network Wide Area Network or the Internet storage I O device such as flash magnetic optical mass storage B BIOS flash TPM flash and one or more non storage I O devices such as display keyboard speaker and the like . I O subsystem may include a platform controller hub PCH not shown that further includes several I O adapters and other I O circuitry to provide access to the storage and non storage I O devices and networks. To accomplish this I O subsystem may have at least one integrated I O adapter for each I O protocol utilized. I O subsystem can be on the same chip as processor or on a separate chip and or package connected to processor .

I O adapters translate a host communication protocol utilized within the processor to a protocol compatible with particular I O devices. For flash magnetic optical mass storage B some of the protocols that I O adapters may translate include Peripheral Component Interconnect PCI Express PCI E 3.0 USB 3.0 SATA 3.0 Small Computer System Interface SCSI Ultra 640 and Institute of Electrical and Electronics Engineers IEEE 1394 Firewire among others. For BIOS flash some of the protocols that I O adapters may translate include Serial Peripheral Interface SPI Microwire among others. Additionally there may be one or more wireless protocol I O adapters. Examples of wireless protocols among others are used in personal area networks such as IEEE 802.15 and Bluetooth 4.0 wireless local area networks such as IEEE 802.11 based wireless protocols and cellular protocols.

In some embodiments the I O subsystem is coupled to a TPM control to control access to system persistent states such as secure data encryption keys platform configuration information and the like. In one embodiment these system persistent states are stored in a TMP NVRAM and accessed via NVRAM controller .

In one embodiment TPM is a secure micro controller with cryptographic functionalities. TPM has a number of trust related capabilities e.g. a SEAL capability for ensuring that data protected by a TPM is only available for the same TPM. TPM can protect data and keys e.g. secrets using its encryption capabilities. In one embodiment TPM has a unique and secret RSA key which allows it to authenticate hardware devices and platforms. For example TPM can verify that a system seeking access to data stored in computer system is the expected system. TPM is also capable of reporting the integrity of the platform e.g. computer system . This allows an external resource e.g. a server on a network to determine the trustworthiness of the platform but does not prevent access to the platform by the user.

In some embodiments I O subsystem also includes a Management Engine ME which is a microprocessor that allows a system administrator to monitor maintain update upgrade and repair computer system . In one embodiment a system administrator can remotely configure computer system by editing the contents of the decode table through ME via networks .

For convenience of explanation the application may sometimes refers to NVRAM as a PCMS device. A PCMS device includes multi layered vertically stacked PCM cell arrays that are non volatile have low power consumption and are modifiable at the bit level. As such the terms NVRAM device and PCMS device may be used interchangeably in the following discussion. However it should be realized as discussed above that different technologies besides PCMS may also be utilized for NVRAM .

It should be understood that a computer system can utilize NVRAM for system memory mass storage firmware memory and or other memory and storage purposes even if the processor of that computer system does not have all of the above described components of processor or has more components than processor .

In the particular embodiment shown in the MSC controller and NVRAM controller are located on the same die or package referred to as the CPU package as the processor . In other embodiments the MSC controller and or NVRAM controller may be located off die or off CPU package coupled to the processor or CPU package over a bus such as a memory bus like a DDR bus e.g. a DDR3 DDR4 etc a PCI express bus a desktop management interface DMI bus or any other type of bus.

Processor designers are currently designing enhanced instruction sets that enable transactional support of multi threaded software. In conventional i.e. non transactional multithreaded software programs protect data with locks. Only one thread can hold a lock at any one time so it can ensure that no other thread is modifying the data at the same time. This tends to be pessimistic the thread with the lock prevents any threads from taking the lock even if they only want to read the data or make a non conflicting update to it.

With transactional support referring to threads no longer need to take out locks when manipulating data. They start a transaction make their changes and when they ve finished commit the transaction or roll back the changes made at step if the transaction cannot be committed. While the thread is making its changes over the course of the transaction referring to special hardware within the processor takes note of any all cache and near memory B locations that the thread reads from and writes to.

Typically any all data writes made by a transaction are present in cache simply because a cache holds a system s most recent changes. That is if a transaction needs to change a data item the data item is called up from deeper storage if it is not in cache already changed and then written into cache. Thus assuming the amount of data changes made by a transaction are limited to being less than the cache size available for each data address all changes made by a transaction will be present in cache. Hardware within the processor prevents write back of these changed data items to persistence until commitment of the transaction. In a first embodiment the cache referred to above includes processor caches and near memory. In a second embodiment the cache referred to above only includes processor caches i.e. near memory is not included . For simplicity the remainder of the document will refer mainly to the first embodiment.

In an embodiment there is an instance of special hardware for each CPU core within the processor and or each instruction execution pipeline within each CPU core within the processor . Here the special hardware instance e.g. as implemented with logic circuitry of the core pipeline that is executing the transactional thread takes note of the transaction s cache and near memory reads and writes as described above. Note that some caching levels within processor may service multiple cores e.g. a last level cache while other caching levels within processor may service only a single core e.g. a core s L1 cache .

When the transaction is ready to be committed the special hardware checks that while the transaction was executing no other thread made any changes to or read from these same locations. If this condition is met the transaction is committed and the thread continues. Here committing the changes means the changes are written into persistence storage. If this condition is not met the transaction is aborted and all its changes are undone . In one embodiment in order to undo the changes fresh data representing the state of the data before any changes made by the transaction are called up from persistence storage and rewritten into cache or the cache lines that were changed are invalidated. The thread can then retry the operation try a different strategy for example one that uses locks or give up entirely.

In an implementation NVRAM far memory B corresponds to persistence storage to which committed data changes are stored upon commitment of a transaction while near memory B and any all caches above near memory correspond to the cache locations where a thread is able to make changes prior to commitment of its transaction.

The concept of persistence storage in various cases however can be extended to other types of software processes that do not technically meet the definition of a transaction as discussed above. Persistence storage according to various different information processing paradigms may be a writable data store whose records reflect the formally recognized state of some process or data structure and is therefore globally visible e.g. and or have some expectation of being needed over a extended time span e.g. multiple on off cycles of the computing system . Notably many such software processes may also choose to implement persistence storage in NVRAM far memory B.

For those non transactional software processes that recognize the existence of persistence storage the software has to have embedded precautions that ensure modified data that needs to be persisted is flushed from cache and stored to persistent storage before any subsequent changes are made to it. Here for example if a change is made to an item of data and the software views the change as needing to be reflected in persistent storage the software will insert a cache line flush instruction e.g. CLFLUSH followed by a memory fence instruction e.g. MFENCE . The cache line flush instruction will cause the newly changed data to be written back to persistence storage B. The memory fence instruction will prevent other operations of the same thread from accessing the data until it has been written to persisted storage B.

In more complicated approaches the thread s software includes complicated book keeping tasks to keep track of what data items in cache need to be persisted to persistence storage B. Here for example certain data items may be recognized by the thread s software as requiring persistence the book keeping software will keep track of these data items and at an appropriate moment in the code s execution execute appropriate cache line and memory fence instructions.

As observed in the software is only asked to define a persistent region of code. This definition is marked at the beginning of the region with a PBEGIN instruction and at the end of the region with a PEND instruction . The PBEGIN instruction essentially turns on the functionality of the special hardware . While the code is executing after the PBEGIN instruction the special hardware tracks which cache lines were changed. When the PEND instruction is executed it causes the cache lines identified by the special hardware to be flushed to persistence B and turns off the special hardware . No other instructions are permitted to be executed after the PEND instruction until all cache lines are flushed to effect the memory fence.

Thus the special hardware tracks cache accesses not only during transactional operations but also during non transactional operations. shows a representation of an instruction execution pipeline within a core that is coupled to the special hardware . Here the coupling is used to turn on the special hardware in response to a PBEGIN instruction and turn off the special hardware in response to a PEND instruction. The instruction execution pipeline is also designed with logic to prevent issuance of a next instruction until the cache flushing is complete. Cache flushing logic is also coupled to the instruction execution pipeline and the special hardware but is not drawn for convenience. The cache flushing logic is triggered into action by the PEND instruction and refers to the special hardware to understand which cache lines need to be flushed. Other features of are as described above with respect to .

If a transaction operation begins the specialized hardware is enabled and begins tracking which cache lines are modified by the transaction . When the transaction is complete transactional hardware within the processor checks to see whether any other transactions have written to or read from these same cache lines . If none have the changes are committed to far memory NVRAM B otherwise the cache lines are replaced with content from persistence NVRAM B or invalidated .

If a PBEGIN instruction is executed the specialized hardware is enabled and begins tracking which cache lines are modified by the software process . When a PEND instruction is executed all modified cache data is written back to persistence NVRAM B and no other instructions are permitted to execute until the write back is complete .

Processes taught by the discussion above may be performed with program code such as machine executable instructions which cause a machine such as a virtual machine a general purpose CPU processor or processing core disposed on a semiconductor chip or special purpose processor disposed on a semiconductor chip to perform certain functions. Alternatively these functions may be performed by specific hardware components that contain hardwired logic for performing the functions or by any combination of programmed computer components and custom hardware components.

A storage medium may be used to store program code. A storage medium that stores program code may be embodied as but is not limited to one or more memories e.g. one or more flash memories random access memories static dynamic or other optical disks CD ROMs DVD ROMs EPROMs EEPROMs magnetic or optical cards or other type of machine readable media suitable for storing electronic instructions. Program code may also be downloaded from a remote computer e.g. a server to a requesting computer e.g. a client by way of data signals embodied in a propagation medium e.g. via a communication link e.g. a network connection .

In the foregoing specification the invention has been described with reference to specific exemplary embodiments thereof. It will however be evident that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense.

