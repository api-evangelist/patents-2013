---

title: Storage tiering in cloud environment
abstract: A plurality of performance-based storage nodes and a plurality of capacity-based storage nodes of a data storage system in a network environment are allocated to one or more tiered resource pools such that the performance-based storage nodes and the capacity-based storage nodes allocated to each one of the one or more tiered resource pools are addressable via a given virtual address for each tiered resource pool. Access to the performance-based storage nodes and the capacity-based storage nodes in the one or more tiered resource pools by a plurality of compute nodes is managed transparent to the compute nodes via a given storage policy. At least portions of the compute nodes, the performance-based storage nodes, and the capacity-based storage nodes are operatively coupled via a plurality of network devices. One or more of the allocating and managing steps are automatically performed under control of at least one processing device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09619429&OS=09619429&RS=09619429
owner: EMC IP Holding Company LLC
number: 09619429
owner_city: Hopkinton
owner_country: US
publication_date: 20130927
---
The field relates generally to data storage system environments and more particularly to techniques for storage tiering in a data storage system environment.

Storage tiering is a technique of establishing a hierarchy of different storage types whereby different types of storage can be allocated to different tiers of the hierarchy. This enables storing the appropriate data to the appropriate tier based on service level requirements at a minimal cost. Each tier has different levels of protection performance and cost. For example high performance solid state drives SSDs or Fibre Channel FC drives can be configured as tier 1 storage to keep frequently accessed data and low cost Serial Advanced Technology Attachment SATA drives as tier 2 storage to keep the less frequently accessed data. Keeping frequently used data in SSD or FC drives improves application performance. Moving less frequently accessed data to SATA drives can free up storage capacity in high performance drives and reduce the cost of storage. This movement of data happens based on different tiering policies. The tiering policy might be based on parameters such as file type size frequency of access etc.

However existing storage tiering methods do not typically account for network latency which can cause performance problems in the data storage system. Such problems can lead to violations of a service level agreement between a data storage service provider and its customers tenants .

Embodiments of the invention provide techniques for storage tiering in a data storage system environment.

For example in one embodiment a method comprises the following steps. A plurality of performance based storage nodes and a plurality of capacity based storage nodes of a data storage system in a network environment are allocated to one or more tiered resource pools such that the performance based storage nodes and the capacity based storage nodes allocated to each one of the one or more tiered resource pools are addressable via a given virtual address for each tiered resource pool. Access to the performance based storage nodes and the capacity based storage nodes in the one or more tiered resource pools by a plurality of compute nodes is managed transparent to the compute nodes via a given storage policy. At least portions of the compute nodes the performance based storage nodes and the capacity based storage nodes are operatively coupled via a plurality of network devices. One or more of the allocating and managing steps are automatically performed under control of at least one processing device.

In another embodiment an article of manufacture is provided which comprises a processor readable storage medium having encoded therein executable code of one or more software programs. The one or more software programs when executed by the at least one processing device implement steps of the above described method.

In yet another embodiment an apparatus comprises a memory and a processor configured to perform steps of the above described method.

Advantageously embodiments described herein provide storage tiering techniques that take into account network latency in a cloud or data center environment.

These and other features and advantages of the present invention will become more readily apparent from the accompanying drawings and the following detailed description.

Embodiments of the present invention will be described herein with reference to exemplary information processing systems computing systems data storage systems and associated servers computers storage units and devices and other processing devices. It is to be appreciated however that embodiments of the invention are not restricted to use with the particular illustrative system and device configurations shown. Moreover the phrases information processing system computing system and data storage system as used herein are intended to be broadly construed so as to encompass for example private or public cloud computing or storage systems as well as other types of systems comprising distributed virtual and or physical infrastructure. However a given embodiment may more generally comprise any arrangement of one or more processing devices.

As used herein the term enterprise refers to a business company firm venture organization operation concern corporation establishment partnership a group of one or more persons or some combination thereof.

Storage tiering can be implemented as a manual or an automated process. Manual storage tiering is a traditional method where the storage administrator monitors the storage workloads periodically and moves the data between the tiers. Manual storage tiering is complex and time consuming.

Automated storage tiering automates the storage tiering process such that data movement between the tiers is performed non disruptively. In automated storage tiering the application workload is proactively monitored and active data is automatically moved to a higher performance tier and the inactive data to a higher capacity lower performance tier. One example of automated storage tiering is the Fully Automated Storage Tiering FAST technology commercially available from EMC Corporation Hopkinton Mass. . Data movements between various tiers can happen within intra array or between inter array storage arrays.

The traditional storage tiering is effective and efficient in the enterprise environment since most applications use a dedicated storage network such as a Storage Area Network SAN or Network Attached Storage NAS to connect to the storage array directly. Applications can make full use of the bandwidth of the dedicated network. Thus compared to the storage input output I O latency the storage network latency can be ignored. Therefore the network latency is not considered in the traditional storage tiering technology.

But in a cloud or data center environment compute nodes and storage nodes are parts of resource pools and are allocated to different tenants on demand. Thus so as to avoid occupying a resource exclusively application compute nodes do not use a dedicated network to connect to storage nodes. Application nodes and storage nodes are typically connected via a converged network and the converged network uses unified Internet Protocol IP centric protocols. For the storage related network traffic Internet Small Computer System Interface iSCSI and Fibre Channel over Ethernet FCoE are frequently used as block level storage and Network File System NFS and Common Internet File System CIFS are frequently used as file level storage. Because the network latency between application node and storage node can not be ignored in a converged network the traditional storage tiering technology may fail to comply with a Service Level Agreement SLA for example response time because it only considers storage itself but ignores the network.

Thus as shown an application node is operatively connected to a high performance storage node through a minimum number of switches routers here one switch router in network . In contrast the application node is operatively coupled to a high capacity storage node through more switches routers than high performance storage node e.g. through switch router and . Of course more switches routers can be employed than what is shown in so long as the high performance storage node is connected to the application node through a minimum of switches routers.

It is to be appreciated that a high performance storage node could for example be a storage array with flash drives or FC drives while a high capacity storage node could be a storage array with SATA drives. Thus a high performance storage node is considered an example of a performance based storage node and a high capacity storage node is considered an example of a capacity based storage node. For storage products that support inter array storage tiering such products could be configured on demand by an administrator as a high performance storage node or a high capacity storage node.

This approach can reduce the network latency for the given application node and storage node. However in a cloud environment server virtualization is widely adopted and application instances of the tenants will be created moved and destroyed on demand. Thus the approach in may not be effective when the tenant s application is moved to another compute node i.e a new location .

Moving storage nodes close to application nodes is advantageous but a non disruptive and automatic adaptive way is needed. In order to understand the network topology and control network devices switches routers one or more embodiments of the invention leverage Software Defined Network SDN technology. In one embodiment a standard protocol is used i.e. OpenFlow to query the topology information for example which node is connected to which port and set rules for example routing rules from switches and routers. OpenFlow is managed by the Open Networking Foundation ONF and is a standard communications interface defined between the control and forwarding layers of an SDN architecture. OpenFlow allows direct access to and manipulation of the forwarding plane of network devices such as switches and routers both physical and virtual hypervisor based .

Embodiments of the invention provide an automatic approach which abstracts the storage nodes into a pool and automatically provisions high performance storage from storage nodes close to an application node and high capacity storage from appropriate storage nodes to tiered storage pools. That is embodiments of the invention decouple the application node and high performance storage nodes via a network abstraction whereby the allocated tiered storage pool has a virtual IP address and application nodes access the tiered storage pool via the virtual IP address. Embodiments of the invention also automatically migrate the high performance storage to a new storage node in the pool when the tenant s application moves to a new location. As will be explained in detail below a center control plane implements these functions along with a plurality of agents thus controlling the storage nodes and network.

This approach modifies the physical deployment in a SDN enabled data center wherein most switches support OpenFlow protocol . The approach deploys at least a majority of the high performance storage nodes close to the application nodes for example at the edge switches which are the accessing switches of the application nodes or distribution switches on top of edge switches. For the high capacity storage nodes they can be integrated into the data center as usual. All storage nodes and network devices are managed by a center control plane.

As shown in the approach in data storage system environment employs two major sub systems a center control plane and a plurality of agents . The agents are deloyed at each of the storage nodes in the data storage system. For example as shown agent is deployed at high performance storage node agent is deployed at high performance storage node agent is deployed at high capacity storage node and agent is deployed at high capacity storage node . The storage nodes and application nodes are connected through a switch router network . For example high performance storage node application node and application node are connected to network via switch router high performance storage node application node and application node are connected to network via switch router high capcity storage node is connected to network via switch router and high capcity storage node is connected to network via switch router . It is to be appreciated that the data storage system environment may have more or less storage nodes application nodes and or switches routers than what is illustrated in .

Center control plane manages all the storage nodes and network devices e.g. switches and routers by selecting appropriate high performance capacity storage nodes and allocating storage for serving requests from application nodes. The center control plane also transparently moves high performance storage data to one or more new storage locations when an application is moved to another location i.e. from one application node to another application node . In order to avoid a single point of failure and a performance bottleneck well known clustering techniques can be applied to the center control plane .

As shown in this embodiment the center control plane includes four main components a storage controller a network controller a policy engine and a meta data repository .

Storage controller is used to perform storage operations. Such operations can be controlled via a standard protocol e.g. Storage Management Initiative Specification SMI S in the case of storage arrays. Main operations of the storage controller include but are not limited to creating a volume deleting a volume attaching a volume and detaching a volume. An example volume is depicted in .

Network controller employs the OpenFlow protocol to communicate with OpenFlow supported switches directly or uses a vendor specific Application Programming Interface API to communicate with a third party network controller to operate OpenFlow supported switches indirectly. Main operations of the network controller include but are not limited to querying the network topology and setting rules on OpenFlow switches.

Policy engine selects the appropriate storage node based on predefined policies or customized policies implemented by an administrator. Selection policies are based on for example network topology distance network traffic workloads high performance storage node capacity usage or combinations thereof.

Meta data repository maintains the tiered storage s virtual IP mapping relationships. Also meta data repository maintains the tiered virtual LUN s mapping data for fault tolerance.

The agents deployed in each storage node are used for the center control plane to communicate with the storage nodes. If the storage node supports a standard storage management protocol such as SMI S the control plane could communicate with the storage node without an agent. If the storage node does not support a standard storage management protocol an agent is employed. An agent is preferably in the form of program code or software sent to and executed by the storage node to provide the standard storage management interface thus acting as an adapter for the storage node. Note that in a control path between components is depicted as a solid line while a data path is depicted as a solid dotted line.

There are two use cases which illustrate advantages of embodiments of the storage tiering abstraction approach over existing storage tiering approaches. While embodiments of the invention are not limited thereto for detailed steps of each use case note that the deploy environment is assumed to be a slightly modified version of the one shown and described in the context of namely 

The high performance storage node can manage the virtual LUN mapping meta data so that the data requests from the application route to the high performance storage node. The high performance storage node determines which data requests that it can serve and which data requests should be forwarded to a high capacity storage node.

Also automated storage tiering technology such as the above mentioned FAST technology can be applied to the virtualized volume to enable storage tiering automatically. shows a storage tiering example 400 with a high performance storage node and a high capacity storage node utilizing an automated storage tiering algorithm e.g. FAST to access a virtualized volume .

It is to be appreciated that the various components and steps illustrated and described in can be implemented in a distributed virtual infrastructure or cloud infrastructure. illustrates a cloud infrastructure . The data storage system environment of as well as the data storage system environment of can be implemented in whole or in part by the cloud infrastructure .

As shown the cloud infrastructure comprises virtual machines VMs . . . M implemented using a hypervisor . The hypervisor runs on physical infrastructure . The cloud infrastructure further comprises sets of applications . . . M running on respective ones of the virtual machines . . . M utilizing associated logical storage units or LUNs under the control of the hypervisor .

As used herein the term cloud refers to a collective computing infrastructure that implements a cloud computing paradigm. For example as per the National Institute of Standards and Technology NIST Special Publication No. 800 145 cloud computing is a model for enabling ubiquitous convenient on demand network access to a shared pool of configurable computing resources e.g. networks servers storage applications and services that can be rapidly provisioned and released with minimal management effort or service provider interaction.

Although only a single hypervisor is shown in the example of a given embodiment of cloud infrastructure configured in accordance with an embodiment of the invention may include multiple hypervisors each running on its own physical infrastructure. Portions of that physical infrastructure might be virtualized.

As is known virtual machines are logical processing elements that may be instantiated on one or more physical processing elements e.g. servers computers or other processing devices . That is a virtual machine generally refers to a software implementation of a machine i.e. a computer that executes programs in a manner similar to that of a physical machine. Thus different virtual machines can run different operating systems and multiple applications on the same physical computer. Virtualization is implemented by the hypervisor which as shown in is directly inserted on top of the computer hardware in order to allocate hardware resources of the physical computer physical infrastructure dynamically and transparently. The hypervisor affords the ability for multiple operating systems to run concurrently on a single physical computer and share hardware resources with each other.

An example of a commercially available hypervisor platform that may be used to implement portions of the cloud infrastructure in one or more embodiments of the invention is vSphere which may have an associated virtual infrastructure management system such as vCenter both commercially available from VMware Inc. Palo Alto Calif. . The underlying physical infrastructure may comprise one or more distributed processing platforms that include storage products such as VNX and Symmetrix VMAX both commercially available from EMC Corporation Hopkinton Mass. . A variety of other storage products may be utilized to implement at least a portion of the cloud infrastructure .

An example of a processing platform on which the cloud infrastructure may be implemented is processing platform shown in . The processing platform in this embodiment comprises a plurality of processing devices denoted . . . K which communicate with one another over a network . One or more of the components shown and described in may therefore each run on one or more storage arrays one or more hosts servers computers or other processing platform elements each of which may be viewed as an example of what is more generally referred to herein as a processing device. As illustrated in such a device generally comprises at least one processor and an associated memory and implements one or more functional modules for controlling certain features of components shown in . Again multiple elements or modules may be implemented by a single processing device in a given embodiment.

The processing device in the processing platform comprises a processor coupled to a memory . The processor may comprise a microprocessor a microcontroller an application specific integrated circuit ASIC a field programmable gate array FPGA or other type of processing circuitry as well as portions or combinations of such circuitry elements.

The memory or other storage devices having program code embodied therein is an example of what is more generally referred to herein as a processor readable storage medium. Articles of manufacture comprising such processor readable storage media are considered embodiments of the invention. A given such article of manufacture may comprise for example a storage device such as a storage disk a storage array or an integrated circuit containing memory. The term article of manufacture as used herein should be understood to exclude transitory propagating signals. Furthermore memory may comprise electronic memory such as random access memory RAM read only memory ROM or other types of memory in any combination. One or more software programs program code when executed by a processing device such as the processing device causes the device to perform functions associated with one or more of the elements components of system environment . One skilled in the art would be readily able to implement such software given the teachings provided herein. Other examples of processor readable storage media embodying embodiments of the invention may include for example optical or magnetic disks.

Also included in the processing device is network interface circuitry which is used to interface the processing device with the network and other system components. Such circuitry may comprise conventional transceivers of a type well known in the art.

The other processing devices of the processing platform are assumed to be configured in a manner similar to that shown for processing device in the figure.

The processing platform shown in may comprise additional known components such as batch processing systems parallel processing systems physical machines virtual machines virtual switches storage volumes logical units etc. Again the particular processing platform shown in is presented by way of example only and components and steps shown and described in may include additional or alternative processing platforms as well as numerous distinct processing platforms in any combination.

Also numerous other arrangements of servers computers storage devices or other components are possible for implementing components shown and described in through . Such components can communicate with other components over any type of network such as a wide area network WAN a local area network LAN a satellite network a telephone or cable network a storage network e.g. Fibre Channel iSCSI Ethernet a converged network e.g. FCoE or Infiniband or various portions or combinations of these and other types of networks.

It should again be emphasized that the above described embodiments of the invention are presented for purposes of illustration only. Many variations may be made in the particular arrangements shown. For example although described in the context of particular system and device configurations the techniques are applicable to a wide variety of other types of information processing systems computing systems data storage systems processing devices and distributed virtual infrastructure arrangements. In addition any simplifying assumptions made above in the course of describing the illustrative embodiments should also be viewed as exemplary rather than as requirements or limitations of the invention. Numerous other alternative embodiments within the scope of the appended claims will be readily apparent to those skilled in the art.

