---

title: Method and system for load balancing at a data network
abstract: A method of load balancing implemented at a data network is disclosed. The data network contains a number of data plane nodes and a number of clusters of a control node. The method starts with deriving a graph from a topology of the data plane nodes, where the graph contains vertices, each representing one of the data plane nodes, and edges, each representing a connection between a pair of data plane nodes. The method continues with partitioning the graph into a number of sub-graphs, where the partition aims at minimizing connectivity among the number of sub-graphs, and where the number of sub-graphs equal to the number of clusters. The control node then assigns each cluster to one of the data plane nodes, where each cluster is assigned to one or more data plane node partitioned into the same sub-graph.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09338097&OS=09338097&RS=09338097
owner: TELEFONAKTIEBOLAGET L M ERICSSON (PUBL)
number: 09338097
owner_city: Stockholm
owner_country: SE
publication_date: 20131031
---
The embodiments of the invention are related to the field of networking. More specifically the embodiments of the invention relate to a method and system for load balancing at a data network particularly a network implemented using Software Defined Network SDN or network functions virtualization VFN .

In a traditional data network the functions relating forwarding traffic and determining where to send traffic is done by a single network device. The single network device is commonly said to contain a control plane and a data plane. The traditional integrated approach of the control plane and the data plane being tightly coupled in a single box may result in an overly complicated control plane and complex network management. Due to high complexity equipment vendors and network operators are often reluctant to initiate changes and the network itself can become fragile and hard to manage over time. This is known to create a large burden and high bather to new protocol and technology developments.

Software Defined Network SDN is a network architecture that aims at decoupling control plane functions from data plane functions such that separate apparatuses may be utilized for different functions. In the SDN architecture network intelligence and states are logically centralized and the underlying network infrastructure is abstracted from the applications. As a result networking may be simplified and new applications become feasible. For example network virtualization can be accomplished by implementing it in a software application where the control plane is separated from the data plane. Also a network administrator of a SDN system may have programmable central control of network traffic without requiring physical access to the system s hardware devices. With these benefits SDN architecture based systems referred to as SDN systems or SDN networks exchangeably herein below are gaining popularity among carriers and enterprises.

The middle layer of the SDN architecture is the control layer . Control layer contains SDN control software . Control layer is generally implemented in a SDN controller and it contains centralized intelligence of the SDN network. Instead of hand coding configurations scattered among thousands of devices a network operator may configure the SDN controller using SDN control software to deploy new applications and change network behavior in real time. By centralizing network state in the control layer the SDN architecture gives a network operator the flexibility to configure manage secure and optimize network resources through managing the SDN controller. In addition the network operator may change SDN control software for her specific need without relying on a vendor of the underlying network devices to change its proprietary software.

The upper layer of the SDN architecture is service layer . Service layer may interface with control layer via a set of application programming interfaces APIs . Service layer contains virtual services that may be implemented in the SDN network. The virtual services include routing multicast security access control bandwidth management traffic engineering quality of service processor and storage optimization energy use and all forms of policy management. Thus a service provider may provide virtual services to a SDN network without being tied to the details of implementation in SDN software and or network devices. With the abstraction of service layer the service provider may provide unified services across networks with different implements by different vendors.

Besides SDN another emerging networking trend is Network Function Virtualization NFV . NFV leverages standard IT virtualization technology to consolidate many network equipment types onto industrial standard high volume servers switches and storage which could be located in datacenters network nodes and in the end user premises. Network function then can be implemented in software running on a range of industrial standard hardware devices which can be moved to or instantiated in various locations in the network as required. NFV can be implemented using SDN where the network functions are implemented in the server layer such as service layer in and the industrial standard hardware devices are implemented in the SDN control layer and the infrastructure layer.

Within the layered structure of SDN or layered implementation of NFV a control layer such as SDN control layer plays a central role to manage traffic forwarding and also provide the virtual services to the underlying data plane nodes. Hardware in the control layer such as SDN controller in control layer may be implemented in a number of hardware devices and how to balance load between the hardware devices is a challenge.

A method of load balancing implemented at a data network is disclosed. The data network contains a number of data plane nodes carrying user traffic and a number of clusters of a control node managing the data plane nodes. The method starts with deriving a graph from a topology of the data plane nodes where the graph contains vertices each representing one of the data plane nodes and edges each representing a connection between a pair of data plane nodes. The method continues with partitioning the graph into a number of sub graphs each sub graph containing one or more vertices and edges representing data plane nodes and connections between the data plane nodes respectively where the partition aims at minimizing connectivity among the number of sub graphs and where the number of sub graphs equal to the number of clusters. The control node then assigns each cluster to one of the data plane nodes where each cluster is assigned to one or more data plane node partitioned into the same sub graph.

A method of load balancing implemented at a data network is disclosed. The data network contains a number of data plane nodes carrying user traffic and at least a cluster of a control node managing the data plane nodes. The cluster contains a number of physical servers. The method starts with deriving a graph from a topology of the data plane nodes where the graph contains vertices each representing one of the data plane nodes and edges each representing a connection between a pair of data plane nodes. The method continues with partitioning the graph into a number of sub graphs each sub graph containing one or more vertices and edges representing data plane nodes and connections between the data plane nodes respectively where the partition aims at minimizing connectivity among the number of sub graphs and where the number of sub graphs equal to the number of physical servers. The control node then assigns each physical server to one of the data plane nodes where each physical server is assigned to one or more data plane node partitioned into the same sub graph.

A network device for of load balancing at a data network is disclosed. The data network contains a number of data plane nodes carrying user traffic and a number of clusters of a control node managing the data plane nodes. The network device contains a graph partitioner which is configured to derive a graph from a topology of the data plane nodes where the graph contains vertices each representing one of the data plane nodes and edges each representing a connection between a pair of data plane nodes. The graphic partitioner is further configured to partition the graph into a number of sub graphs each sub graph containing one or more vertices and edges representing data plane nodes and connections between the data plane nodes respectively where the partition aims at minimizing connectivity among the number of sub graphs and where the number of sub graphs equal to the number of clusters. The network device also contains a control allocator configured to assign each cluster to one of the data plane nodes where each cluster is assigned to one or more data plane node partitioned into the same sub graph.

A network device for of load balancing at a data network is disclosed. The data network contains a number of data plane nodes carrying user traffic and at least a cluster of a control node managing the data plane nodes. The cluster contains a number of physical servers. The network device contains a control assignment processor. The control assignment processor contains a graph partitioner and a control allocator. The graph partitioner is configured to derive a graph from a topology of the data plane nodes where the graph contains vertices each representing one of the data plane nodes and edges each representing a connection between a pair of data plane nodes. The graphic partitioner is further configured to partition the graph into a number of sub graphs each sub graph containing one or more vertices and edges representing data plane nodes and connections between the data plane nodes respectively where the partition aims at minimizing connectivity among the number of sub graphs and where the number of sub graphs equal to the number of physical servers. The control allocator of the control assignment processor is configured to assign each physical server to one of the data plane nodes where each physical server is assigned to one or more data plane node partitioned into the same sub graph. The network device further contains a data plane node topology unit including at least one of a data plane node registration unit configured to register the plurality of data plane nodes at the control node and a data plane node topology discovery unit configured to automatically discover the data plane nodes at the control node where topology of the data plane nodes is learned from the registration or discovery. The network device further contains a control node database configured to save the learned topology of the data plane nodes.

Embodiments of the invention aims at reducing inter cluster traffic and intra cluster traffic by considering topology of data plane nodes. Through spreading workload based on topology of data plane node instead of ignoring the topology a SDN system may be more efficient in its operations such as maintaining the topology setting up path implementing protocols and running network applications.

In the following description numerous specific details are set forth. However it is understood that embodiments of the invention may be practiced without these specific details. In other instances well known circuits structures and techniques have not been shown in detail in order not to obscure the understanding of this description. It will be appreciated however by one skilled in the art that the invention may be practiced without such specific details. Those of ordinary skill in the art with the included descriptions will be able to implement appropriate functionality without undue experimentation.

References in the specification to one embodiment an embodiment an example embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is submitted that it is within the knowledge of one skilled in the art to effect such feature structure or characteristic in connection with other embodiments whether or not explicitly described.

In the following description and claims the terms coupled and connected along with their derivatives may be used. It should be understood that these terms are not intended as synonyms for each other. Coupled is used to indicate that two or more elements which may or may not be in direct physical or electrical contact with each other co operate or interact with each other. Connected is used to indicate the establishment of communication between two or more elements that are coupled with each other. A set as used herein refers to any positive whole number of items including one item.

An electronic device e.g. an end station a network device stores and transmits internally and or with other electronic devices over a network code composed of software instructions and data using machine readable media such as non transitory machine readable media e.g. machine readable storage media such as magnetic disks optical disks read only memory flash memory devices phase change memory and transitory machine readable transmission media e.g. electrical optical acoustical or other form of propagated signals such as carrier waves infrared signals . In addition such electronic devices include hardware such as a set of one or more processors coupled to one or more other components e.g. one or more non transitory machine readable storage media to store code and or data and network connections to transmit code and or data using propagating signals as well as user input output devices e.g. a keyboard a touchscreen and or a display in some cases. The coupling of the set of processors and other components is typically through one or more interconnects within the electronic devices e.g. busses and possibly bridges . Thus a non transitory machine readable medium of a given electronic device typically stores instructions for execution on one or more processors of that electronic device. One or more parts of an embodiment of the invention may be implemented using different combinations of software firmware and or hardware.

As used herein a network device e.g. a router a switch or a controller is a piece of networking equipment including hardware and software that communicatively interconnects other equipment on the network e.g. other network devices end systems . Some network devices are multiple services network devices that provide support for multiple networking functions e.g. routing bridging VLAN virtual LAN switching Layer 2 aggregation session border control Quality of Service and or subscriber management and or provide support for multiple application services e.g. data voice and video . A network device is generally identified by its media access MAC address Internet protocol IP address subnet network sockets ports and or upper OSI layer identifiers.

Note while the illustrated examples in the specification discuss mainly on SDN system embodiments of the invention may be implemented in non SDN system. It can be implemented in a layered network architecture such as a Network Function Virtualization NFV architecture. Unless specified otherwise the embodiments of the invention apply to a control node of a layered network architecture i.e. they are NOT limited to SDN controller.

As discussed herein a SDN controller handles the intelligent actions relating to data plane node traffic forwarding such as topology discovery link monitoring traffic steering and path setup in a SDN network. The SDN controller may be built using commercial off the shelf COTS hardware with software. For example the SDN controller may be built using COTS computer clusters or simply clusters . A cluster consists of a set of coupled computers that work together so that externally it may be viewed as a single logical node. The SDN controller may be built in a scaled out manner using same or similar clusters where each cluster is responsible for forwarding traffic and providing services to a set of data plane nodes of the SDN network. In one embodiment such kind load balancing is implemented utilizing a load balancer.

The main task of data plane nodes A G is to forward packets within the data plane nodes from an ingress port to an egress port according to the rules in flow tables programmed by the one or more SDN controllers. Each flow entry contains a set of actions such as forwarding packets to a given port modifying certain bits in a packet header encapsulating packets to the network controller or dropping the packets. For the first packet in a new flow the forwarding element often forwards the packet to the network controller to trigger the new flow being programmed and new dynamic next hop route being set. It can also be used to forward slow path packets such as Internet Control Message Protocol ICMP packets to the SDN controllers for processing. Note that the concept of a flow can be defined broadly e.g. a TCP connection or all traffic from a particular MAC address or IP address. Also note that a packet within network is defined broadly and it may be an Ethernet frame an IP network packet or a message in a proprietary format.

In one embodiment data plane node can be viewed logically as containing two main components. One is a control plane and the other is a forwarding plane or data plane plane . A zoom in view of data plane node C at reference illustrates the two planes. Data plane node C contains control plane and forwarding plane . Control plane coordinates management and configuration of data plane node C. Configuration of forwarding plane is achieved by running applications on host processor . Host processor usually runs an operating system in order to provide a well known development environment. Commands from host processor are sent to forwarding processor using an interconnect e.g. a peripheral component interconnect PCI bus . Exception packets e.g. packet for routing and management are often processed on host processor . Forwarding processor interacts with various forwarding ports of data plane node C to forward and otherwise process incoming and outgoing packets.

Forwarding plane is responsible for forwarding traffic e.g. forwarding operations includes switching routing and learning . It contains forwarding processor that is designed to provide high throughput at the detriment of a more complex and flexible development environment. Different types high performance memory and hardware accelerator are often found on board of forwarding processor for achieving the high throughput. In contrast host processor can be more complex and flexible at the detriment of providing high throughput as it processes more control packets often referred to as slow path packets than data packet thus throughput is less mission critical. When data plane node C receives a packet associated with a new flow it does not know where to forward the packet. Thus it sends the packet to its managing SDN controller SDN controller in this example . SDN controller receives the packet and it programs a new flow entry based on its associated prefix and dynamic next hop. It then sends the new flow entry to data plane node C. Data plane node C then forwards the packet according to the new flow entry.

SDN controller adds and removes flow entries from a flow table. It defines the interconnection and routing among a set of traffic forward elements and other network devices. It also handles network state distribution such as collecting information from the set of traffic forward elements and distributing forwarding routing instructions to them. SDN controller can also be programmed to support new addressing routing and complex packet processing applications. A data plane node needs to be coupled to and managed by at least one SDN controller to function correctly.

SDN controller contains load balancer and clusters and at references and respectively. A SDN controller may be front ended by more than one load balancer and it may contain a much higher number of clusters. For simplicity of illustration each of data plane nodes A G is coupled to SDN controller through load balancer . In one embodiment each of the data plane nodes A G communicates SDN controller through a virtual Internet Protocol address VIP . The VIP is an IP address assigned to multiple applications and software residing clusters of SDN controller rather than IP addresses to be individually assigned to each application or cluster. In one embodiment each cluster runs the same applications and each of the application is stateless with respect to SDN network state and network state is kept separately in a database which has the responsibility to maintain data consistency and coherency. Each data plane node is coupled to and managed by at least one cluster of a SDN controller often referred to as homed to the at least one cluster when the data plane node is coupled to and managed by the SDN controller.

Through the VIP each data plane node communicates and registers itself with SDN controller . Each data plane node may also register its ports to the SDN controller. With the information SDN controller can discover the topology of the data plane nodes. One way of the discovery is through link layer discovery protocol LLDP where SDN controller injects a LLDP packet with correct data path identifier and port identifier through a data plane node. The data plane node then passes the packet to the next data plane node which then returns the packet to SDN controller . Through a series of LLDP packet transmission SDN controller will be able to build the topology of the data plane nodes A G.

When load balancer performs load balancing it typically considers workloads at various clusters and tries to distribute the workloads optimally without considering topology of data plane nodes. The approach is suboptimal in many ways. For example 

Thus embodiments of the invention aims at reducing inter cluster traffic and intra cluster traffic by considering topology of data plane nodes.

Referring to each of data plane nodes A H is coupled to a single cluster the coupling is illustrated by a single dotted line at the top of each box representing a data plane node and the coupling is determined by load balancer and it may be based on workload distribution of the clusters e.g. load balancer couples a data plane node to a cluster with the least workload . Through the coupling SDN controller learns the topology of the data plane nodes. From the topology SDN controller derives a graph. The graph contains vertices each representing a data plane node and edges each representing a connection between a pair of data plane nodes.

Out of the graph SDN controller creates a number of connected sub graphs through one of a number of graph partitioning algorithms. The graph partitioning algorithms can aim at creating maximally intra connected sub graphs and or least inter connected sub graphs. The number of sub graphs equals to the number of clusters so that each cluster can be assigned to one sub graphs consist of data plane nodes . At data plane nodes A B D and E form sub graph and data plane nodes C F G and H form sub graph . The partitioning is based on one of the graph partitioning algorithm to have the maximum intra connections within the sub graphs and or minimum inter connections between the sub graphs. The number of sub graphs equals to the number of clusters at two. Thus cluster is assigned to sub graph while cluster is assigned to sub graph .

In one embodiment load balancer then re couples data plane nodes to their respective clusters according to the graph partitioning algorithm adopted. The load balancing now is optimized based on the topology of data plane nodes. Whenever the topology of data plane nodes changes the process can be repeated so that the coupling between the cluster and data plane node is updated to reflect the updated topology.

Through spreading workload based on topology of data plane node instead of ignoring the topology a SDN system may be more efficient in its operations such as maintaining the topology setting up path implementing protocols and running network applications.

One main step in the load balancing illustrated in is to partition the topology based graph. The graph can be a unidirectional connected graph represented as G V E where V is the set of vertices that represent the data plane node and E is the set of edges that represent links connecting two data plane nodes. The efficiency of a graph partitioning algorithm largely dependents on minimizing dependencies between sub graphs. The algorithm may aim at partitioning the graph with the constraint that the edges between the sub graphs are kept to a minimum. In addition the algorithm may also aim at the number of vertices within each sub graph is roughly equal to each other.

Mathematically graph partitioning is the process by which a graph G V E is partitioned into k sub graphs V V . . . V such that the parts are disjoint and are of equal size and the number of edges with endpoints in different parts is minimized Graph partitioning is considered as a non deterministic polynomial time hard NP hard problem and algorithms to effectively partition a graph are based on heuristics.

Known heuristic algorithms can be used for graph partitioning at a SDN system or other similar systems includes the Kernighan Lin algorithm multi level method spectral partitioning and spectral bisection and etc. The detail implementation of various heuristic algorithms is understood by the person in the ordinary skill of the art thus not details in this specification.

The method optionally starts with learning the topology of data plane nodes of the data network at reference . The topology may be learned through registration of the data plane nodes at the control node commonly done at a SDN controller for example . It may also be learned through dynamic discovery of the data plane node through a topology maintenance process.

From the topology of data plane nodes the control node derives a graph at reference . The graph contains vertices each representing a data plane node and edges each representing a connection between a pair of data plane nodes of the data network. Note in one embodiment the edges carry weights based on bandwidth capacity and latency of the represented links connecting the data plane nodes. That is connections may be treated differently depending on the characteristics of the links making the connections.

At reference the graph is partitioned into a number of sub graphs where the number of sub graphs equals the number of available clusters and where the objective of the partition is to minimize connectivity among the sub graphs. As discussed herein above graph partition is a NP hard problem mathematically and a number of heuristic algorithms may be implemented. For example the algorithm may be one of Kernighan Lin algorithm multi level method and spectral partitioning and spectral bisection. Note one additional restriction of the partition is that the sub graphs contain substantially the same number of vertices.

While the graph partition is based on minimizing connectivity among the sub graphs other criteria may also be considered. For example a control node may also consider existing workloads of the clusters as a tie breaker when different sub graph partitions yield similar inter sub graph connectivity but one yields more even workload among the clusters.

At reference the control node assigns each available cluster to data plane nodes based on the graph partition so that each available cluster is assigned to a set of data plane nodes partitioned into the same sub graph. After the assignment the control node may rearrange coupling of data plane nodes to the clusters so that each data plane node is managed controlled by the assigned cluster.

In some embodiment load balancing may be accomplished using embodiments of the invention without a load balancer. illustrates load balancing of a SDN system without load balancer according to one embodiment of the invention. is similar to and the same or similar references indicate elements or components having the same or similar functionalities. One difference between the two figures is that SDN controller of does not contain a load balancer. Instead of having a single virtual IP address for the SDN controller SDN controller corresponds to multiple VIPs each for a physical server of cluster node .

As illustrated each data plane node has two VIP based connections to cluster one each for servers and at references and . Servers and serve as a master server or slave server for each data plane node. Servers and together offer 1 1 high availability HA . Note in some embodiment one server may serve as the master server while multiple servers serve as the slave servers and together they may operate in N 1 HA mode. At 1 1 or N 1 HA mode the master and slave servers coordinate with each other through state replication and synchronization through heartbeat and other cluster operations. While each physical server serves a master or slave role a physical server may be a master server for one data plane node and slave server for another data plane node. In normal operation the workload of a data plane node is handled mainly by the master server. Thus to balance workload between servers and each can be assigned to be the sole master node of a data plane node so that each data plane node is coupled to one single master node and one slave node.

One difference between and is that SDN controller does not utilize load balancer for workload balancing and servers and are in high availability mode and serve as master and slave servers for each data plane node. In other words each data plane node has VIP access to both servers although for a given data plane node only one master server manages operations of the data plane node. In the coupling of each data plane node with both servers is illustrated with two dotted lines at the top of each box representing a data plane node. The processes of topology discovery and graph partitioning of are similar to the processes illustrated in and discussed herein above. The difference is that after the sub graph is partition the servers are assigned each data plane node in instead of clusters in . For data plane nodes within sub graph i.e. nodes A B D and E server is assigned to be the master server and server is assigned to be the slave server. For data plane nodes within sub graph i.e. nodes C F G and H server is assigned to be the master server and server is assigned to the slave server. When the prior coupling of master slave servers is different from the assignment SDN controller will cause rearrange so that the master slave server coupling complies with the assignment.

Referring to operations within references are similar to operations within references thus are not repeated here. One difference is that the number of sub graphs equal to the number of available physical servers for load balancing at reference . At reference the control node assigns each available physical server to data plane nodes based on the graph partition so that each available physical server is assigned to a set of data plane nodes partitioned to the same sub graph. The assignment means that the assigned physical server serves as the master server for the set of data plane nodes and the other physical servers serve only as the slave servers. After the assignment the control node may rearrange coupling of data plane nodes to the physical servers so that each data plane node is served by the newly assigned master slave servers.

Network device contains graph partitioner and control allocator . Data plane node topology unit contains at least one of data plane node registration unit and data plane node topology discovery unit . A control node database is coupled to both network device and data plane node topology unit .

In one embodiment network device is a load balancer of a data network such as a SDN controller . The load balancer works with data plane node topology unit and control node database to balance workload. The process starts at data plane node topology unit where it contains data plane node registration unit configured to register the number of data plane nodes at the control node and or data plane node topology discovery unit configured to automatically discover the data plane nodes. The topology of the data plane nodes is learned from the registration or discovery at data plane node registration unit and data plane node topology discovery unit respectively. The learned topology is saved at control node database .

Graph partitioner and control allocator may contain processor individually or share a single processor. The processor can be a general purpose or special purpose processors and the modules may be implemented as a single unit. Graph partitioner is configured to derive a graph from the topology of the data plane nodes saved in control node database . The graph contains vertices each representing one of the data plane nodes and edges each representing a connection between a pair of data plane nodes. The edges may carry weight as discussed herein above. The graph partitioner is further configured to partition the graph into a number of sub graphs where each sub graph contains one or more vertices and edges representing data plane nodes and connections between the data plane nodes respectively. The number of sub graphs equals to the number of available clusters for load sharing within the control node. The partitioning aims at minimizing connectivity among the number of sub graphs. As discussed herein above a number of graph partitioning algorithms may be implemented. After the partitioning the controller allocator is configured to assign each cluster to one of the data plane nodes where each cluster is assigned to one or more data plane node partitioned into the same sub graph. In one embodiment the number of data plane node within each sub graph is substantially the same.

In one embodiment network device is not a load balancer and there are multiple physical servers associated with each of one or more clusters. Within the multiple physical servers within one cluster one serves as the master server and another serves as the slave server and data plane nodes have connectivity with their respective master and slave servers.

In that embodiment network device works with data plane node topology unit and control node database to balance workload too. The process also starts at data plane node topology unit where it contains data plane node registration unit configured to register the number of data plane nodes at the control node and or data plane node topology discovery unit configured to automatically discover the data plane nodes. The topology of the data plane nodes is learned from the registration or discovery at data plane node registration unit and data plane node topology discovery unit respectively. The learned topology is saved at control node database .

Graph partitioner and control allocator may contain processor individually or share a single processor. The processor can be a general purpose or special purpose processors and the modules may be implemented as a single unit. Graph partitioner is configured to derive a graph from the topology of the data plane nodes saved in Control node database . The graph contains vertices each representing one of the data plane nodes and edges each representing a connection between a pair of data plane nodes. The edges may carry weight as discussed herein above. The graph partitioner is further configured to partition the graph into a number of sub graphs where each sub graph contains one or more vertices and edges representing data plane nodes and connections between the data plane nodes respectively. The number of sub graphs equals to the number of available physical servers for load sharing within the control node. The partitioning aims at minimizing connectivity among the number of sub graphs. As discussed herein above a number of graph partitioning algorithms may be implemented. After the partitioning the controller allocator is configured to assign each physical server to one of the data plane nodes where each physical server is assigned to one or more data plane node partitioned into the same sub graph. In one embodiment the number of data plane node within each sub graph is substantially the same.

Note the operations of the flow diagrams in are described with reference to the exemplary embodiment of . However it should be understood that the operations of flow diagrams can be performed by embodiments of the invention other than those discussed with reference to and the embodiments discussed with reference to can perform operations different than those discussed with reference to the flow diagrams of .

While the flow diagrams in the figures herein above show a particular order of operations performed by certain embodiments of the invention it should be understood that such order is exemplary e.g. alternative embodiments may perform the operations in a different order combine certain operations overlap certain operations etc. .

Different embodiments of the invention may be implemented using different combinations of software firmware and or hardware. Thus the techniques shown in the figures can be implemented using code and data stored and executed on one or more electronic devices e.g. an end system a network device or a cloud centralized management system at a data center . Such electronic devices store and communicate internally and or with other electronic devices over a network code and data using computer readable media such as non transitory computer readable storage media e.g. magnetic disks optical disks random access memory read only memory flash memory devices phase change memory and transitory computer readable transmission media e.g. electrical optical acoustical or other form of propagated signals such as carrier waves infrared signals digital signals . In addition such electronic devices typically include a set of one or more processors coupled to one or more other components such as one or more storage devices non transitory machine readable storage media user input output devices e.g. a keyboard a touchscreen and or a display and network connections. The coupling of the set of processors and other components is typically through one or more busses and bridges also termed as bus controllers . Thus the storage device of a given electronic device typically stores code and or data for execution on the set of one or more processors of that electronic device.

While the invention has been described in terms of several embodiments those skilled in the art will recognize that the invention is not limited to the embodiments described can be practiced with modification and alteration within the spirit and scope of the appended claims. The description is thus to be regarded as illustrative instead of limiting.

