---

title: Sharing non-page aligned memory
abstract: A method for sharing memory between a central processing unit (CPU) and an input/output (I/O) device of a computing device is described. The method may include creating an allocation of memory for the I/O device to operate on. The method includes detecting whether the allocation is not page-aligned, wherein an allocation is page-aligned when its base address and size be evenly divisible by the applicable page-size. The allocation may be successfully shared, even if not page-aligned, even if an operating system of the computing device doesn't support sharing of non-page-aligned allocations.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09478000&OS=09478000&RS=09478000
owner: Intel Corporation
number: 09478000
owner_city: Santa Clara
owner_country: US
publication_date: 20130927
---
Computing devices and their operating systems often manage memory at a level of granularity greater than their smallest addressable element i.e. they divide the device s memory address spaces into fixed sized blocks called pages . Memory addresses evenly divisible by the page size are called page boundaries . Operating systems often use page granular management at certain levels while doing finer grained management at other levels. In some cases fine grain allocations don t necessarily begin and or end on page boundaries. Operating systems often allow system memory to be shared with GPU devices but such implementations tend to only support such sharing at page granularity rather than fine grain allocations including allocations that do not being and or end on page boundaries.

The same numbers are used throughout the disclosure and the figures to reference like components and features. Numbers in the 100 series refer to features originally found in numbers in the 200 series refer to features originally found in and so on.

In embodiments a unified memory architecture UMA enables memory sharing between the CPU and GPU by providing both the CPU and the GPU with the same physical memory. Thus the physical memory and the corresponding physical address space of the CPU and GPU are one and the same. In some cases the physical memory may be partitioned between the CPU and the GPU. Further the physical memory can be implemented as a paged system memory that is allocated by an operating system of the computing device. A paged virtual memory address space of the CPU may be mapped to the same physical memory pages as the graphics virtual memory address space of the GPU. However the operating system restricts the GPU to accessing page aligned system memory allocations. Computing languages such as OpenCL may provide application programming interfaces API s to enable the GPU to access non page aligned system memory allocations. OpenCL may also provide API s that enable data to be copied between the separate physical address domains of the CPU and the GPU so that non page aligned data can be operated on by the GPU. In such a scenario after the GPU has finished processing a set of data the data is copied back to the physical address domain of the CPU. The data transfers that occur when using OpenCL to copy data between the separate physical address domains of the CPU and the GPU may reduce any efficiency gained by offloading tasks to the GPU. Accordingly embodiments described herein relate to the sharing of non page aligned system memory between the CPU and the GPU of a computing device. The memory may be shared via a unified memory architecture UMA . Non page aligned buffers of the UMA may re use the same backing or underlying physical memory regardless of restrictions imposed by the operating system.

In the following description and claims the terms coupled and connected along with their derivatives may be used. It should be understood that these terms are not intended as synonyms for each other. Rather in particular embodiments connected may be used to indicate that two or more elements are in direct physical or electrical contact with each other. Coupled may mean that two or more elements are in direct physical or electrical contact. However coupled may also mean that two or more elements are not in direct contact with each other but yet still co operate or interact with each other.

Some embodiments may be implemented in one or a combination of hardware firmware and software. Some embodiments may also be implemented as instructions stored on a machine readable medium which may be read and executed by a computing platform to perform the operations described herein. A machine readable medium may include any mechanism for storing or transmitting information in a form readable by a machine e.g. a computer. For example a machine readable medium may include read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices or electrical optical acoustical or other form of propagated signals e.g. carrier waves infrared signals digital signals or the interfaces that transmit and or receive signals among others.

An embodiment is an implementation or example. Reference in the specification to an embodiment one embodiment some embodiments various embodiments or other embodiments means that a particular feature structure or characteristic described in connection with the embodiments is included in at least some embodiments but not necessarily all embodiments of the inventions. The various appearances of an embodiment one embodiment or some embodiments are not necessarily all referring to the same embodiments. Elements or aspects from an embodiment can be combined with elements or aspects of another embodiment.

Not all components features structures characteristics etc. described and illustrated herein need be included in a particular embodiment or embodiments. If the specification states a component feature structure or characteristic may might can or could be included for example that particular component feature structure or characteristic is not required to be included. If the specification or claim refers to a or an element that does not mean there is only one of the element. If the specification or claims refer to an additional element that does not preclude there being more than one of the additional element.

It is to be noted that although some embodiments have been described in reference to particular implementations other implementations are possible according to some embodiments. Additionally the arrangement and or order of circuit elements or other features illustrated in the drawings and or described herein need not be arranged in the particular way illustrated and described. Many other arrangements are possible according to some embodiments.

In each system shown in a figure the elements in some cases may each have a same reference number or a different reference number to suggest that the elements represented could be different and or similar. However an element may be flexible enough to have different implementations and work with some or all of the systems shown or described herein. The various elements shown in the figures may be the same or different. Which one is referred to as a first element and which is called a second element is arbitrary.

The computing device may also include a graphics processing unit GPU . As shown the CPU may be connected through a bus to the GPU . However in some embodiments the GPU is located on the same die as the CPU within the computing device . In this manner the CPU and the GPU are physically connected in such a manner that the connection between the CPU and the GPU via the bus may be eliminated. Furthermore in embodiments the CPU and the GPU may be included within a unified memory architecture of the computing device as discussed with respect to .

The GPU may be configured to perform any number of graphics operations within the computing device . For example the GPU may be configured to render or manipulate graphics images graphics frames videos or the like to be displayed to a user of the computing device . In some embodiments the GPU includes a number of graphics engines not shown wherein each graphics engine is configured to perform specific graphics tasks or to execute specific types of workloads.

The computing device may also include a memory device . The memory device can include random access memory RAM read only memory ROM flash memory or any other suitable memory systems. For example the memory device may include dynamic random access memory DRAM . The memory may include device drivers that are configured to execute the instructions for implementing the memory sharing procedure. The device drivers may be configured to execute instructions for reducing corruption of the system memory. The device drivers may be software an application program application code or the like. In embodiments a device driver is a graphics driver that is embedded with functionality that implements the memory sharing procedure for non page aligned memory. The device driver may include a user mode module. The user mode module of a device driver enables the device driver to execute in the user mode space of a computing system rather than in the privileged space of the kernel mode. By executing in the user mode the device driver may call an application programming interface API to access system hardware.

Additionally in embodiments the CPU and GPU can access any level of memory. However data from other levels of memory may be stale while the LLC includes the most recent data. Furthermore in embodiments the CPU and GPU can employ any mutually accessible storage location to perform the memory sharing procedure for non page aligned memory.

The memory contains any number of applications that are configured to run on the computing device . In some cases when an application is executed by CPU the application may request that an allocation be allocated by the device driver . The allocation may be a designated portion of physical memory. An allocation is said to be page aligned if both its base address and size are evenly divisible by the page size otherwise it is considered non aligned.

In some cases an application may execute on the CPU and request the allocation in order to perform the operations such as processing data. When an application requests the allocation the operating system may perform a Probe for Write operation to check that the application has write permissions to the allocation . The probe for write operation may result in the operating system reading and writing to the first byte of each page used by the allocation . However the probe for write operation by the CPU is not atomic and is interruptible. When an allocation is not page aligned it does not fully own the pages it partially resides on and more specifically if an allocation does not begin on a page boundary it does not own the first byte of its first page that can belong to a neighboring allocation. If a non page aligned allocation is forced into a probe for write execution path only intended for page aligned allocations data corruption can occur when the non atomic probe for write overwrites data the GPU is writing to the neighboring allocation.

A memory management unit MMU may be used to manage access to the pages of data that back the surface . The MMU can divide the virtual address space of the CPU and the GPU into various pages of address space. The CPU and the GPU each have their own virtual address spaces. The virtual address space allows for protection of the data contained within the surface by isolating the various applications executing within a computing system to a particular subset of virtual addresses. Through the use of virtual address spaces one application will not access the data of another application . Accordingly the MMU includes a CPU page table and a GPU page table . The CPU page table maps the virtual addresses of the CPU to the physical addresses associated with the allocation . Similarly the GPU page table maps the virtual addresses of the GPU to the physical addresses associated with the allocation .

In various embodiments the virtual memory addresses from the CPU page table and the graphics virtual memory addresses from the GPU page table are mapped to the physical memory pages of the allocation . Before the allocation is accessed by the GPU the allocation may be pinned. Pinning the allocation refers to locking the allocation so that the physical locations and the corresponding physical addresses are unchangeable. The pinning may include the operating system performing a Probe and Lock operation. Pinning is required where GPUs do not support page faults. Once the GPU has finished accessing the allocation the pinned allocation can be unlocked and evicted from the page tables. Before the accessing workload is submitted to the GPU the operating system can probe and lock the surface by reading and writing to the first byte of each involved page which produces the same issue as in 0027 for misaligned allocations. In embodiments described herein the device driver may synchronize access or manage operations associated with misaligned allocations to avoid the stated issues.

In embodiments described herein the device driver may synchronize operations by the GPU and the CPU manage misaligned allocations in the driver rather than the operating system or any combination thereof. The synchronization technique may delay operations such as allocation creation by the CPU via the operating system if a related page is currently in the GPU domain. The delay technique may be referred to synchronization and may avoid corruption of misaligned allocations. Additionally or alternatively the management technique may manage a misaligned allocation by having the operating system manage only the page aligned portions of the allocation and having the driver manage any non aligned page fragments. These techniques are discussed in more detail below.

The computing device may also include an input output I O device interface . The CPU may be connected through the bus to the input output I O device interface adapted to connect the computing device to one or more I O devices . The I O devices may include for example a keyboard and a pointing device wherein the pointing device may include a touchpad or a touchscreen among others. The I O devices may be built in components of the computing device or may be devices that are externally connected to the computing device .

The CPU may also be linked through the bus to a display interface adapted to connect the computing device to a display device . The display device may include a display screen that is a built in component of the computing device . The display device may also include a computer monitor television or projector among others that is externally connected to the computing device .

A network interface controller NIC may be adapted to connect the computing device through the bus to a network . The network may be a wide area network WAN local area network LAN or the Internet among others.

The block diagram of is not intended to indicate that the computing device is to include all of the components shown in . Further the computing device may include any number of additional components not shown in depending on the details of the specific implementation.

The UMA may enable direct memory sharing between the CPU and the GPU without any type of data copying or data transfer between the CPU and the GPU . This may be accomplished by allowing the CPU and the GPU to share the allocation .

The CPU page table of the UMA may include a number of CPU virtual memory addresses and the GPU page table may include a number of graphics virtual memory addresses . The CPU virtual memory addresses form the CPU virtual address space while the graphics virtual memory addresses form the graphics virtual address space. Each address space is mapped to a physical address in each page table. For shared allocations the CPU virtual memory addresses and the graphics virtual memory addresses both map to the same set of physical addresses within the CPU page table and the GPU page table respectively.

The physical addresses enable the CPU and the GPU to process data stored at physical locations within the allocation . In various embodiments the allocation is made based on the specific CPU virtual addresses accessed by an application such as an application discussed in . Once the allocation has been made each physical address is mapped to a corresponding CPU virtual address within the CPU page table as shown in . The graphics virtual memory addresses within the GPU page table may be synchronized with the CPU page table such that the CPU virtual addresses and the GPU virtual memory addresses are mapped to the same set of physical addresses . The physical addresses correspond to physical locations within the allocation . Accordingly the allocation may be directly shared between the CPU and the GPU .

As described above an allocation is associated with one or more pages. If either the base address or size of an allocation is not evenly divisible by the applicable page size the allocation is not page aligned. The device driver may successfully share an unalligned allocation between the CPU and GPU even if the operating system doesn t naturally support unaligned sharing.

In embodiments the device driver may monitor the allocation to determine the whether a given surface is associated with an I O device such as the GPU by tracking read or write operations from the GPU to the allocation . The device driver may prevent an operating system associated with the CPU from writing to the allocation when the allocation is being written to by the GPU . Tracking the association of the GPU to an allocation may be referred to herein as allocation affinity tracking. In embodiments allocation affinity may be tracked using mail box writes with unique signatures to track the GPU s state and usage. Every command executed by the GPU is assigned a unique identifier and that identifier is used to track the completion of a command. If a command is not yet complete the allocation affinity may be in the GPU s domain rather than the CPU s domain. Preventing an operating system associated with the CPU from writing to the allocation when it is in the GPU s domain may reduce the stated issues with sharing non aligned allocations.

In embodiments the device driver may reduce the stated issues with sharing non aligned allocations by reporting only aligned pages to the operating system of the computing device . The operating system may restrict the GPU from accessing any misaligned pages. Therefore any partial pages may be managed by the driver rather than the operating system of the computing device . Unlike with an operating system s aligned only management the driver may probe lock and independently manage any misaligned page fragments.

The schematic of is not intended to indicate that the UMA is to include all of the components shown in . Further the UMA may include any number of additional components not shown in depending on the details of the specific implementation.

In some embodiments the method may be executed on a computing device such as the computing device where the CPU and the GPU are connected by a bus . In other embodiments the CPU and the GPU may be included in a UMA such as the UMA discussed above with respect to . Further the method may executed by a driver of the computing device such as the device driver of the computing device .

The method begins at block with creation of an allocation within a physical memory. As discussed above during allocation creation the operating system may perform a probe for write to verify write access to allocation resulting in the aforementioned corruption.

At block a determination may be made as to whether the allocation is in the GPU domain. In embodiments the determination of whether the allocation is in the GPU domain may be determined by referencing a database configured to store a list of the pages and associated addresses that the GPU is currently working on. The database configured to store the pages and associated addresses that the GPU is currently working on may be referred to herein as the GPU affinity database. 

If the page is not in the GPU domain at block the allocation may continue. If the page is in the GPU domain at block the allocation is delayed until the GPU has completed any pending operational commands.

Each address may include pointers to a page referred to as reference counts. At block once the GPU operational commands have completed the reference count is decremented. If the reference count is greater than zero then the pages and associated addresses may remain in the GPU affinity database. If the reference count is zero then the pages and associated addresses may be removed from the GPU affinity database. The allocation may then continue to be processed at .

Before executing a workload at the GPU everything that the GPU may work on will need to be entered into the GPU affinity database. The method may begin at block by initiating a flush finish operation indicating allocations to be worked on by the GPU. At block pages associated with the allocation may already be in the GPU domain as a consequence of other operations being associated with the pages. If the page is not in the GPU domain then the page is provided to the GPU affinity database and the reference count is incremented at block . If the page is in the GPU domain then the allocation of the surface is created at block . The method may include waiting at block until the GPU command has completed. Once the GPU command has completed the reference count is decremented at block . Further if the decremented reference count is zero the page associated with the surface creation operation that finished at block will be removed from the GPU affinity database. At block the flush finish operation is provided to the GPU for processing and addresses associated with the operation is inserted into the GPU affinity database at block incrementing the reference count.

The process flow diagram of are not intended to indicate that the blocks of methods and are to be executed in any particular order or that all of the blocks are to be included in every case. Further any number of additional blocks may be included within the methods and depending on the details of the specific implementation. Additionally while the methods described herein include a GPU the memory may be shared between any I O device such as another CPU or a direct memory access DMA controller.

In embodiments the driver may manage misaligned allocations by reporting only aligned pages to the operating system of the computing device. In embodiments reporting only aligned pages may include increasing the address of a base page to an aligned address of a page next to the base page. For an allocation with start address an increased base page start address may be provided to the operating system. Alternatively reporting only aligned pages may include reducing a reported allocation size. Although not illustrated in the limit page may indicate the end of the allocation such as the boundary . The limit page may be provided to the operating system. If after the above increase reduction the reported size would be zero the driver may have the operating system handle the allocation as a non shared single page allocation for which the driver will use its own managed shared page in place of the one provided by the operating system.

The various software components discussed herein may be stored on the tangible non transitory computer readable media as indicated in . For example a surface creation module may be configured to creating an allocation associated with a page of memory for the I O device to operate on. A detection module may be configured to detecting whether the allocation is not page aligned. Further a module for handling the misalignment may be configured to reducing corruption of the surface based on the detection.

The block diagram of is not intended to indicate that the tangible non transitory computer readable media is to include all of the components shown in . Further the tangible non transitory computer readable media may include any number of additional components not shown in depending on the details of the specific implementation.

In embodiments the CPU does not have to marshal data between the CPU address space and the GPU address space. Furthermore the CPU is not tasked with ensuring that no other processing cores are working on the particular set of data that the CPU wants the GPU to handle thus preventing processing races between processing cores.

In various embodiments the system comprises a platform coupled to a display . The platform may receive content from a content device such as content services device s or content delivery device s or other similar content sources. A navigation controller including one or more navigation features may be used to interact with for example the platform and or the display . Each of these components is described in more detail below.

The platform may include any combination of a chipset a central processing unit CPU a memory device a storage device a graphics subsystem applications and a radio . The chipset may provide intercommunication among the CPU the memory device the storage device the graphics subsystem the applications and the radio . For example the chipset may include a storage adapter not shown capable of providing intercommunication with the storage device .

The CPU may be implemented as Complex Instruction Set Computer CISC or Reduced Instruction Set Computer RISC processors x86 instruction set compatible processors multi core or any other microprocessor or central processing unit CPU . In some embodiments the CPU includes dual core processor s dual core mobile processor s or the like.

The memory device may be implemented as a volatile memory device such as but not limited to a Random Access Memory RAM Dynamic Random Access Memory DRAM or Static RAM SRAM . The storage device may be implemented as a non volatile storage device such as but not limited to a magnetic disk drive optical disk drive tape drive an internal storage device an attached storage device flash memory battery backed up SDRAM synchronous DRAM and or a network accessible storage device. In some embodiments the storage device includes technology to increase the storage performance enhanced protection for valuable digital media when multiple hard drives are included for example.

The graphics subsystem may perform processing of images such as still or video for display. The graphics subsystem may include a graphics processing unit GPU such as the GPU or a visual processing unit VPU for example. An analog or digital interface may be used to communicatively couple the graphics subsystem and the display . For example the interface may be any of a High Definition Multimedia Interface DisplayPort wireless HDMI and or wireless HD compliant techniques. The graphics subsystem may be integrated into the CPU or the chipset . Alternatively the graphics subsystem may be a stand alone card communicatively coupled to the chipset .

The graphics and or video processing techniques described herein may be implemented in various hardware architectures. For example graphics and or video functionality may be integrated within the chipset . Alternatively a discrete graphics and or video processor may be used. As still another embodiment the graphics and or video functions may be implemented by a general purpose processor including a multi core processor. In a further embodiment the functions may be implemented in a consumer electronics device.

The radio may include one or more radios capable of transmitting and receiving signals using various suitable wireless communications techniques. Such techniques may involve communications across one or more wireless networks. Exemplary wireless networks include wireless local area networks WLANs wireless personal area networks WPANs wireless metropolitan area network WMANs cellular networks satellite networks or the like. In communicating across such networks the radio may operate in accordance with one or more applicable standards in any version.

The display may include any television type monitor or display. For example the display may include a computer display screen touch screen display video monitor television or the like. The display may be digital and or analog. In some embodiments the display is a holographic display. Also the display may be a transparent surface that may receive a visual projection. Such projections may convey various forms of information images objects or the like. For example such projections may be a visual overlay for a mobile augmented reality MAR application. Under the control of one or more applications the platform may display a user interface on the display .

The content services device s may be hosted by any national international or independent service and thus may be accessible to the platform via the Internet for example. The content services device s may be coupled to the platform and or to the display . The platform and or the content services device s may be coupled to a network to communicate e.g. send and or receive media information to and from the network . The content delivery device s also may be coupled to the platform and or to the display .

The content services device s may include a cable television box personal computer network telephone or Internet enabled device capable of delivering digital information. In addition the content services device s may include any other similar devices capable of unidirectionally or bidirectionally communicating content between content providers and the platform or the display via the network or directly. It will be appreciated that the content may be communicated unidirectionally and or bidirectionally to and from any one of the components in the system and a content provider via the network . Examples of content may include any media information including for example video music medical and gaming information and so forth.

The content services device s may receive content such as cable television programming including media information digital information or other content. Examples of content providers may include any cable or satellite television or radio or Internet content providers among others.

In some embodiments the platform receives control signals from the navigation controller which includes one or more navigation features. The navigation features of the navigation controller may be used to interact with the user interface for example. The navigation controller may be a pointing device that may be a computer hardware component specifically human interface device that allows a user to input spatial e.g. continuous and multi dimensional data into a computer. Many systems such as graphical user interfaces GUI and televisions and monitors allow the user to control and provide data to the computer or television using physical gestures. Physical gestures include but are not limited to facial expressions facial movements movement of various limbs body movements body language or any combination thereof. Such physical gestures can be recognized and translated into commands or instructions.

Movements of the navigation features of the navigation controller may be echoed on the display by movements of a pointer cursor focus ring or other visual indicators displayed on the display . For example under the control of the applications the navigation features located on the navigation controller may be mapped to virtual navigation features displayed on the user interface . In some embodiments the navigation controller may not be a separate component but rather may be integrated into the platform and or the display .

The system may include drivers not shown that include technology to enable users to instantly turn on and off the platform with the touch of a button after initial boot up when enabled for example. Program logic may allow the platform to stream content to media adaptors or other content services device s or content delivery device s when the platform is turned off. In addition the chipset may include hardware and or software support for 5.1 surround sound audio and or high definition 7.1 surround sound audio for example. The drivers may include a graphics driver for integrated graphics platforms. In some embodiments the graphics driver includes a peripheral component interconnect express PCIe graphics card.

In various embodiments any one or more of the components shown in the system may be integrated. For example the platform and the content services device s may be integrated the platform and the content delivery device s may be integrated or the platform the content services device s and the content delivery device s may be integrated. In some embodiments the platform and the display are an integrated unit. The display and the content service device s may be integrated or the display and the content delivery device s may be integrated for example.

The system may be implemented as a wireless system or a wired system. When implemented as a wireless system the system may include components and interfaces suitable for communicating over a wireless shared media such as one or more antennas transmitters receivers transceivers amplifiers filters control logic and so forth. An example of wireless shared media may include portions of a wireless spectrum such as the RF spectrum. When implemented as a wired system the system may include components and interfaces suitable for communicating over wired communications media such as input output I O adapters physical connectors to connect the I O adapter with a corresponding wired communications medium a network interface card NIC disc controller video controller audio controller or the like. Examples of wired communications media may include a wire cable metal leads printed circuit board PCB backplane switch fabric semiconductor material twisted pair wire co axial cable fiber optics or the like.

The platform may establish one or more logical or physical channels to communicate information. The information may include media information and control information. Media information may refer to any data representing content meant for a user. Examples of content may include for example data from a voice conversation videoconference streaming video electronic mail email message voice mail message alphanumeric symbols graphics image video text and the like. Data from a voice conversation may be for example speech information silence periods background noise comfort noise tones and the like. Control information may refer to any data representing commands instructions or control words meant for an automated system. For example control information may be used to route media information through a system or instruct a node to process the media information in a predetermined manner. The embodiments however are not limited to the elements or the context shown or described in .

As described above examples of a mobile computing device may include a personal computer PC laptop computer ultra laptop computer tablet touch pad portable computer handheld computer palmtop computer personal digital assistant PDA cellular telephone combination cellular telephone PDA television smart device e.g. smart phone smart tablet or smart television mobile internet device MID messaging device data communication device and the like.

An example of a mobile computing device may also include a computer that is arranged to be worn by a person such as a wrist computer finger computer ring computer eyeglass computer belt clip computer arm band computer shoe computer clothing computer or any other suitable type of wearable computer. For example the mobile computing device may be implemented as a smart phone capable of executing computer applications as well as voice communications and or data communications. Although some embodiments may be described with a mobile computing device implemented as a smart phone by way of example it may be appreciated that other embodiments may be implemented using other wireless mobile computing devices as well.

As shown in the device may include a housing a display an input output I O device and an antenna . The device may also include navigation features . The display may include any suitable display unit for displaying information appropriate for a mobile computing device. The I O device may include any suitable I O device for entering information into a mobile computing device. For example the I O device may include an alphanumeric keyboard a numeric keypad a touch pad input keys buttons switches rocker switches microphones speakers a voice recognition device and software or the like. Information may also be entered into the device by way of microphone. Such information may be digitized by a voice recognition device.

A method for sharing memory between a central processing means CPM and graphics processing means GPM of a computing device is described herein. For example the CPM may be a central processing unit CPU configured to process memory allocations. The GPM may be a graphics processing unit GPU configured to work with the CPM in a unified memory architecture scheme. The method may include creating an allocation of memory for the GPM to operate on and detecting whether the allocation is not page aligned. An allocation is page aligned when the base address of the allocation and size of the allocation are evenly divisible by the applicable page size. The method may include successfully sharing the allocation even if not page aligned and even if an operating system of the computing device doesn t support sharing of non page aligned allocations.

A computing device is described herein. The computing device may include a central processing means CPM configured to execute stored instructions and graphics processing means GPM and a GPM page table. For example the CPM may be a central processing unit CPU configured to process memory allocations. The GPM may be a graphics processing unit GPU configured to work with the CPM in a unified memory architecture scheme using the GPM page table. The computing device may include a storage means such as a storage device including processor executable code that when executed by the CPM is configured to create a shared allocation of memory for the GPM to operate on and detect whether the allocation is not page aligned. An allocation is page aligned when the base address of the allocation and size of the allocation are evenly divisible by the applicable page size. The storage means includes processor executable code that when executed by the CPM is configured to share the allocation even if not page aligned and even if an operating system of the computing device doesn t support sharing of non page aligned allocations.

At least one machine readable medium is described herein. The at least one machine readable medium having instructions stored therein that in response to being executed on a computing device cause the computing device to create a shared memory allocation for a general processing unit GPU to operate on and detect whether the allocation is not page aligned. The instructions may cause the computing device to share the allocation compensating for any lack of operating system support for non aligned sharing based on the detection.

It is to be understood that specifics in the aforementioned examples may be used anywhere in one or more embodiments. For instance all optional features of the computing device described above may also be implemented with respect to either of the methods or the computer readable medium described herein. Furthermore although flow diagrams and or state diagrams may have been used herein to describe embodiments the inventions are not limited to those diagrams or to corresponding descriptions herein. For example flow need not move through each illustrated box or state or in exactly the same order as illustrated and described herein.

The inventions are not restricted to the particular details listed herein. Indeed those skilled in the art having the benefit of this disclosure will appreciate that many other variations from the foregoing description and drawings may be made within the scope of the present inventions. Accordingly it is the following claims including any amendments thereto that define the scope of the inventions.

