---

title: Text-to-speech synthesis
abstract: The present disclosure describes example systems, methods, and devices for generating a synthetic speech signal. An example method may include determining a phonemic representation of text. The example method may also include identifying one or more finite-state machines (“FSMs”) corresponding to one or more phonemes included in the phonemic representation of the text. A given FSM may be a compressed unit of recorded speech that simulates a Hidden Markov Model. The example method may further include determining a selected sequence of models that minimizes a cost function that represents a likelihood that a possible sequence of models substantially matches a phonemic representation of text. Each possible sequence of models may include at least one FSM. The method may additionally include generating a synthetic speech signal based on the selected sequence that includes one or more spectral features generated from at least one FSM included in the selected sequence.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09082401&OS=09082401&RS=09082401
owner: Google Inc.
number: 09082401
owner_city: Mountain View
owner_country: US
publication_date: 20130109
---
Unless otherwise indicated herein the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.

A text to speech system TTS may be employed to generate synthetic speech based on text. A first example TTS system may concatenate one or more recorded speech units to generate synthetic speech. A second example TTS system may concatenate one or more statistical models of speech to generate synthetic speech. A third example TTS system may concatenate recorded speech units with statistical models of speech to generate synthetic speech. In this regard the third example TTS system may be referred to as a hybrid TTS system.

A method is disclosed. The method may include determining a phonemic representation of text that includes one or more linguistic targets. Each of the one or more linguistic targets may include one or more phonemes. The method may also include identifying one or more finite state machines FSMs that correspond to one of the one or more phonemes included in the one or more linguistic targets. Each of the one or more FSMs may be a compressed recorded speech unit that simulates a Hidden Markov Model HMM by averaging one or more spectral features of a recorded speech unit over N states. N may be a positive integer. The method may further include determining one or more possible sequences of synthetic speech models based on the phonemic representation of text. Each of the one or more possible sequences may include at least one FSM. The method may additionally include determining from the one or more possible sequences of synthetic speech models a selected sequence of models that minimizes a value of a cost function. The cost function may represent a likelihood that one of the one or more possible sequences substantially matches the phonemic representation of text. The method may additionally include generating by a computing system having a processor and a memory a synthetic speech signal based on the selected sequence. The synthetic speech signal may include information indicative of one or more spectral features generated from at least one FSM included in the selected sequence.

A computer readable memory having stored therein instructions executable by a computing system is disclosed. The instructions may include instructions for determining a phonemic representation of text that includes one or more linguistic targets. Each of the one or more linguistic targets may include one or more phonemes. The instructions may also include instructions for identifying one or more finite state machines FSMs that correspond to one of the one or more phonemes included in the one or more linguistic targets. A given FSM may be a compressed recorded speech unit that simulates a HMM by averaging one or more spectral features of a recorded speech unit over N states. N may be a positive integer. The instructions may further include instructions for determining one or more possible sequences of synthetic speech models based on the phonemic representation of text. Each of the one or more possible sequences may include at least one FSM. The instructions may additionally include instructions for determining from the one or more possible sequences of synthetic speech models a selected sequence of models that minimizes a value of a cost function. The cost function may represent a likelihood that one of the one or more possible sequences substantially matches the phonemic representation of text. The instructions may additionally include instructions for generating a synthetic speech signal based on the selected sequence. The synthetic speech signal may include information indicative of one or more spectral features generated from at least one FSM included in the selected sequence.

A computing system is disclosed. The computing system may include a data storage having stored therein program instructions and a plurality of FSMs. Each FSM in the plurality of FSMs may be a compressed recorded speech unit that simulates an HMM by averaging one or more spectral features of a recorded speech unit over N states. N may be a positive integer. The computing system may also include a processor. Upon executing the program instructions stored in the data storage the processor may be configured to determine a phonemic representation of text that includes one or more linguistic targets. Each of the one or more linguistic targets may include one or more phonemes. The processor may also be configured to identify one or more FSMs included in the plurality of FSMs that correspond to one of the one or more phonemes included in the one or more linguistic targets. The processor may be further configured to determine one or more possible sequences of synthetic speech models based on the phonemic representation of text. Each of the one or more possible sequences may include at least one FSM. The processor may be further configured to determine from the one or more possible sequences of synthetic speech models a selected sequence that minimizes a value of a cost function. The cost function may represent a likelihood that one of the one or more possible sequences substantially matches the phonemic representation of the text. The processor may also be configured to generate a synthetic speech signal based on the selected sequence. The synthetic speech signal may include information indicative of one or more spectral features generated from an FSM included in the selected sequence.

These as well as other aspects advantages and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description with reference where appropriate to the accompanying drawings.

In the following detailed description reference is made to the accompanying figures which form a part thereof. In the figures similar symbols typically identify similar components unless context dictates otherwise. The illustrative embodiments described in the detailed description figures and claims are not meant to be limiting. Other embodiments may be utilized and other changes may be made without departing from the spirit or scope of the subject matter presented herein. It will be readily understood that aspects of the present disclosure as generally described herein and illustrated in the figures can be arranged substituted combined separated and designed in a wide variety of different configurations all of which are contemplated herein.

Disclosed herein are methods systems and devices for generating a synthetic speech signal using a hybrid text to speech TTS system. An example method may include determining a phonemic representation of text. As used herein the term phonemic representation may refer to text represented as one or more phonemes indicative of a pronunciation of the text perhaps by representing the text as a sequence of one or more linguistic targets. Each linguistic target may include a prior phoneme a current phoneme and a next phoneme. The linguistic target may also include information indicative of one or more phonetic features that provide information indicative of how the phoneme is pronounced. The one or more linguistic targets may be determined using any algorithm method and or process suitable for parsing text in order to determine the phonemic representation of text.

The example method may also include identifying one or more finite state machines FSMs that correspond to a current phoneme of one of the one or more linguistic targets. In one aspect an FSM may be a compressed recorded speech unit that simulates a Hidden Markov Model HMM . Those of skill in the art will understand that an HMM is a statistical model that may be used to determine state information for a Markov Process when the states of the process are not observable. A Markov Process undergoes successive transitions from one state to another with the previous and next states of the process depending to some measurable degree on the current state. In the context of speech synthesis in the HMM training process speech parameters such as spectral envelopes are extracted from speech waveforms as described above and then their time sequences are modeled as context dependent HMMs.

An FSM may differ from an HMM in that a given FSM is based on a single recorded speech unit as opposed to being estimated from a corpus of recorded speech units. In this regard a given FSM may include information for substantially reproducing an associated recorded speech unit. Since an FSM simulates an HMM a synthetic speech generator may substantially reproduce a recorded speech unit directly from the FSM in the same manner in which a synthetic speech signal would be generated from an HMM. Thus generating synthetic speech using one or more FSMs may result in higher quality synthetic speech as compared to a TTS system only using HMMs. Additionally a plurality of FSMs may require less data storage space than a corpus of recorded speech units thereby providing more flexibility in the implementation of the hybrid TTS system. In another example an FSM may be trained using a forced Viterbi algorithm using L recorded speech units included in a corpus of recorded speech units where L is an integer significantly less than that the number of recorded speech units included in the corpus. For instance L may be an integer between 1 and 10. In contrast an HMM may be trained using the entire corpus of recorded speech units.

The example method may include identifying one or more FSMs corresponding to a current phoneme of one of the one or more linguistic targets. Each FSM in a plurality of FSMs may be mapped to a current phoneme. For each linguistic target a computing system may identify one or more FSMs that are mapped to the current phoneme of the linguistic target. The example method may further include determining one or more possible sequences of synthetic speech models based on the phonemic representation of text. As used herein the term synthetic speech model may refer to a mathematical model that may be used to generate synthetic speech such as an FSM or an HMM. Each possible sequence may include a model that corresponds to one of the linguistic targets. One or more models may be joined or concatenated together to form the possible sequence. Each of the one or more possible sequences may include at least one FSM. In some examples the one or more possible sequences may include other synthetic speech models such as HMMs.

The example method may include determining a selected sequence that minimizes a cost function. The cost function may indicate a likelihood that a possible sequence of models substantially matches the phonemic representation of the text. The example method may additionally include generating by a computing system having a processor and a data storage a synthetic speech signal based on the selected sequence. Minimizing the cost function may result in the selected sequence being an accurate sequence of one or more phonemes used in speaking the text. The synthetic speech signal may include one or more spectral features generated from at least one FSM included in the selected sequence. The computing system may output the synthetic speech signal or cause to be output via an audio output device such as a speaker.

In some examples the methods devices and systems described herein can be implemented using client devices and or so called cloud based server devices. Under various aspects of this paradigm client devices such as mobile phones tablet computers and or desktop computers may offload some processing and storage functions to remote server devices. These client services may communicate with the server devices via a network such as the Internet. As a result applications that operate on the client devices may also have a persistent server based component. Nonetheless it should be noted that at least some of the methods processes and techniques disclosed herein may be able to operate entirely on a client device or a server device.

Furthermore the server devices described herein may not necessarily be associated with a client server architecture and therefore may also be referred to as computing systems. Similarly the client devices described herein also may not necessarily be associated with a client server architecture and therefore may be interchangeably referred to as user devices. In some contexts client devices may also be referred to as computing systems. 

Network may be for example the Internet or some other form of public or private Internet Protocol IP network. Thus client devices and may communicate using packet switching technologies. Nonetheless network may also incorporate at least some circuit switching technologies and client devices and may communicate via circuit switching alternatively or in addition to packet switching. Further network may take other forms as well.

Server device may also communicate via network . Particularly server device may communicate with client devices and according to one or more network protocols and or application level protocols to facilitate the use of network based or cloud based computing on these client devices. Server device may include integrated data storage e.g. memory disk drives etc. and may also be able to access separate server data storage . Communication between server device and server data storage may be direct via network or both direct and via network as illustrated in . Server data storage may store application data that is used to facilitate the operations of applications performed by client devices and and server device .

Although only three client devices one server device and one server data storage are shown in communication system may include any number of each of these components. For instance communication system may include millions of client devices thousands of server devices and or thousands of server data storages. Furthermore client devices may take on forms other than those shown in .

User interface may include user input devices such as a keyboard a keypad a touch screen a computer mouse a track ball a joystick and or other similar devices now known or later developed. User interface may also include user display devices such as one or more cathode ray tubes CRT liquid crystal displays LCD light emitting diodes LEDs displays using digital light processing DLP technology printers light bulbs and or other similar devices now known or later developed. Additionally user interface may be configured to generate audible output s via a speaker speaker jack audio output port audio output device earphones and or other similar devices now known or later developed. In some embodiments user interface may include software circuitry or another form of logic that can transmit data to and or receive data from external user input output devices.

Communication interface may include one or more wireless interfaces and or wireline interfaces that are configurable to communicate via a network such as network shown in . The wireless interfaces if present may include one or more wireless transceivers such as a BLUETOOTH transceiver a Wifi transceiver perhaps operating in accordance with an IEEE 802.11 standard e.g. 802.11b 802.11g 802.11n a WiMAX transceiver perhaps operating in accordance with an IEEE 802.16 standard a Long Term Evolution LTE transceiver perhaps operating in accordance with a 3rd Generation Partnership Project 3GPP standard and or other types of wireless transceivers configurable to communicate via local area or wide area wireless networks. The wireline interfaces if present may include one or more wireline transceivers such as an Ethernet transceiver a Universal Serial Bus USB transceiver or similar transceiver configurable to communicate via a twisted pair wire a coaxial cable a fiber optic link or other physical connection to a wireline device or network. Other examples of wireless and wireline interfaces may exist as well.

Processor may include one or more general purpose processors e.g. microprocessors and or one or more special purpose processors e.g. digital signal processors DSPs graphical processing units GPUs floating point processing units FPUs network processors or application specific integrated circuits ASICs . Processor may be configured to execute computer readable program instructions that are contained in data storage and or other instructions to carry out various functions described herein.

Thus data storage may include one or more non transitory computer readable storage media that can be read or accessed by processor . The one or more computer readable storage media may include volatile and or non volatile storage components such as optical magnetic organic or other memory or disc storage which can be integrated in whole or in part with processor . In some embodiments data storage may be implemented using a single physical device e.g. one optical magnetic organic or other memory or disc storage unit while in other embodiments data storage may be implemented using two or more physical devices.

Data storage may also include program data that can be used by processor to carry out functions described herein. In some embodiments data storage may include or have access to additional data storage components or devices e.g. cluster data storages described below .

Server device and server data storage device may store applications and application data at one or more places accessible via network . These places may be data centers containing numerous servers and storage devices. The exact physical location connectivity and configuration of server device and server data storage device may be unknown and or unimportant to client devices. Accordingly server device and server data storage device may be referred to as cloud based devices that are housed at various remote locations. One possible advantage of such cloud based computing is to offload processing and data storage from client devices thereby simplifying the design and requirements of these client devices.

In some embodiments server device and server data storage device may be a single computing system residing in a single data center. In other embodiments server device and server data storage device may include multiple computing systems in a data center or even multiple computing systems in multiple data centers where the data centers are located in diverse geographic locations. For example depicts each of server device and server data storage device potentially residing in a different physical location.

In some embodiments each of the server clusters A B and C may have an equal number of server devices an equal number of cluster data storages and an equal number of cluster routers. In other embodiments however some or all of the server clusters A B and C may have different numbers of server devices different numbers of cluster data storages and or different numbers of cluster routers. The number of server devices cluster data storages and cluster routers in each server cluster may depend on the computing task s and or applications assigned to each server cluster.

In the server cluster A for example server devices A can be configured to perform various computing tasks of server device . In one embodiment these computing tasks can be distributed among one or more of server devices A. Server devices B and C in server clusters B and C may be configured the same or similarly to server devices A in server cluster A. On the other hand in some embodiments server devices A B and C each may be configured to perform different functions. For example server devices A may be configured to perform one or more functions of server device and server devices B and server device C may be configured to perform functions of one or more other server devices. Similarly the functions of server data storage device can be dedicated to a single server cluster or spread across multiple server clusters.

Cluster data storages A B and C of the server clusters A B and C respectively may be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives. The disk array controllers alone or in conjunction with their respective server devices may also be configured to manage backup or redundant copies of the data stored in cluster data storages to protect against disk drive failures or other types of failures that prevent one or more server devices from accessing one or more cluster data storages.

Similar to the manner in which the functions of server device and server data storage device can be distributed across server clusters A B and C various active portions and or backup redundant portions of these components can be distributed across cluster data storages A B and C. For example some cluster data storages A B and C may be configured to store backup versions of data stored in other cluster data storages A B and C.

Cluster routers A B and C in server clusters A B and C respectively may include networking equipment configured to provide internal and external communications for the server clusters. For example cluster routers A in server cluster A may include one or more packet switching and or routing devices configured to provide i network communications between server devices A and cluster data storage A via cluster network A and or ii network communications between the server cluster A and other devices via communication link A to network . Cluster routers B and C may include network equipment similar to cluster routers A and cluster routers B and C may perform networking functions for server clusters B and C that cluster routers A perform for server cluster A.

Additionally the configuration of cluster routers A B and C can be based at least in part on the data communication requirements of the server devices and cluster storage arrays the data communications capabilities of the network equipment in the cluster routers A B and C the latency and throughput of the local cluster networks A B C the latency throughput and cost of the wide area network connections A B and C and or other factors that may contribute to the cost speed fault tolerance resiliency efficiency and or other design goals of the system architecture.

As shown in client device may include a communication interface a user interface a processor and data storage all of which may be communicatively linked together by a system bus network or other connection mechanism .

Communication interface functions to allow client device to communicate using analog or digital modulation with other devices access networks and or transport networks. Thus communication interface may facilitate circuit switched and or packet switched communication such as POTS communication and or IP or other packetized communication. For instance communication interface may include a chipset and antenna arranged for wireless communication with a radio access network or an access point. Also communication interface may take the form of a wireline interface such as an Ethernet Token Ring or USB port. Communication interface may also take the form of a wireless interface such as a Wifi BLUETOOTH global positioning system GPS or wide area wireless interface e.g. WiMAX or LTE . However other forms of physical layer interfaces and other types of standard or proprietary communication protocols may be used over communication interface . Furthermore communication interface may include multiple physical communication interfaces e.g. a Wifi interface a BLUETOOTH interface and a wide area wireless interface .

User interface may function to allow client device to interact with a human or non human user such as to receive input from a user and to provide output to the user. Thus user interface may include input components such as a keypad keyboard touch sensitive or presence sensitive panel computer mouse trackball joystick microphone still camera and or video camera. User interface may also include one or more output components such as a display screen which for example may be combined with a presence sensitive panel CRT LCD LED a display using DLP technology printer light bulb and or other similar devices now known or later developed. User interface may also be configured to generate audible output s via a speaker speaker jack audio output port audio output device earphones and or other similar devices now known or later developed. In some embodiments user interface may include software circuitry or another form of logic that can transmit data to and or receive data from external user input output devices. Additionally or alternatively client device may support remote access from another device via communication interface or via another physical interface not shown .

Processor may include one or more general purpose processors e.g. microprocessors and or one or more special purpose processors e.g. DSPs GPUs FPUs network processors or ASICs . Data storage may include one or more volatile and or non volatile storage components such as magnetic optical flash or organic storage and may be integrated in whole or in part with processor . Data storage may include removable and or non removable components.

Processor may be capable of executing program instructions e.g. compiled or non compiled program logic and or machine code stored in data storage to carry out the various functions described herein. Therefore data storage may include a non transitory computer readable medium having stored thereon program instructions that upon execution by client device cause client device to carry out any of the methods processes or functions disclosed in this specification and or the accompanying drawings. The execution of program instructions by processor may result in processor using data .

By way of example program instructions may include an operating system e.g. an operating system kernel device driver s and or other modules and one or more application programs e.g. address book email web browsing social networking and or gaming applications installed on client device . Similarly data may include operating system data and application data . Operating system data may be accessible primarily to operating system and application data may be accessible primarily to one or more of application programs . Application data may be arranged in a file system that is visible to or hidden from a user of client device .

Application programs may communicate with operating system through one or more application programming interfaces APIs . These APIs may facilitate for instance application programs reading and or writing application data transmitting or receiving information via communication interface receiving or displaying information on user interface and so on.

In some vernaculars application programs may be referred to as apps for short. Additionally application programs may be downloadable to client device through one or more online application stores or application markets. However application programs can also be installed on client device in other ways such as via a web browser or through a physical interface e.g. a USB port on client device .

The corpus of recorded speech may generally be any suitable corpus of recorded speech units and corresponding text transcriptions. Each recorded speech unit may include an audio file and a corresponding text transcription may include text of the words spoken in the audio file. The recorded speech units may be read speech speech samples that include for example book excerpts broadcast news list of words and or sequence of numbers among other examples. The recorded speech units may also include spontaneous speech speech samples that include for example dialogs between two or more people narratives such as a person telling a story map tasks such as one person explaining a route on a map to another and or appointment tasks such as two people trying to find a common meeting time based on individual schedules among other examples. Other types of recorded speech units may also be included in the speech database .

The spectral feature extraction module may be configured to identify one or more spectral features for each recorded speech unit included in the speech database . The spectral feature extraction module may determine a spectral envelope for each of a given recorded speech unit. The spectral feature extraction module may then determine one or more spectral features of the given recorded speech unit from the spectral envelope. In one example the one or more spectral features may include one or more Mel Cepstral Coefficients MCCs . The one or more MCCs may represent the short term power spectrum of a portion of the waveform to be synthesized from the given training time predicted feature vector and may be based on for example a linear Fourier transform of a log power spectrum on a nonlinear Mel scale of frequency. A Mel scale may be a scale of pitches subjectively perceived by listeners to be about equally distant from one another even though the actual frequencies of these pitches are not equally distant from one another. In another example the spectral feature extraction module may determine one or more other types of spectral features such as a fundamental frequency Line Spectral pairs Linear Predictive coefficients Mel Generalized Cepstral Coefficients aperiodic measures log power spectrum and or phase.

The spectral feature extraction module may send the one or more spectral features for each recorded unit to the HMM training module and the FSM generation module . The HMM training module may train a plurality of HMMs based on the one or more spectral features of the recorded speech units included in the speech database . The HMM training module may also generate one or more decision trees for determining an HMM that corresponds to a phonemic representation of text such as a linguistic target. A number of decision trees may depend on the number of states of the trained HMMs. That is the HMM training module may determine a decision tree for each state of the trained HMMs. The HMM training module may use the text transcriptions corresponding to the recorded speech units in order to generate the one or more decision trees. The HMM training module may store the decision tree in the model database .

The FSM generation module may generate a plurality of FSMs based on the one or more spectral received from the spectral feature extraction module for each recorded speech. The FSM generation module may map each FSM in the plurality of FSMs to a phonemic representation of text. In one example the FSM generation module may be configured to reduce the number of FSM included in the plurality of FSMs perhaps by removing similar FSMs corresponding to a same phonemic representation of text. Once a final plurality of FSMs is determined the FSM generation module may store the final plurality of FSM in the model database .

Thus the hybrid TTS training system may generate a model database that includes a plurality of HMMs each associated with a phonemic representation of text a plurality of FSMs each associated with a phonemic representation of text and a decision tree for mapping a phonemic representation of text to a given HMM.

The text identification module may receive an input signal that includes information indicative of text. The text identification module may then determine a phonemic representation of text based on the input signal and may send the phonemic representation of text to the parameter generation module in a text signal . The text identification module may receive the input signal which may include information indicative of text. The information indicative of the text may include a single word or the text may include a text string. In one example the text identification module receives the input signal from an input interface component such as a keyboard touchscreen or any other input device suitable for inputting text. In another example the text identification module may receive the input signal from a remote computing system perhaps via a network such as the network described with respect to .

The parameter generation module may determine one or more possible sequences of synthetic speech models based on the phonemic representation of text included in the text signal . The parameter generation module may then determine a selected sequence that substantially matches the phonemic representation of text. The selected sequence may include at least one FSMs selected from the plurality of FSMs that is stored in the model database . In one example the selected sequence may also include one or more HMMs selected from the plurality of HMMs that is stored in the model database . The parameter generation module may then determine one or more spectral features to include a parameter signal based on the synthetic speech models included in the selected sequence.

The parameter generation module may send the parameter signal to the speech synthesizer . The speech synthesizer may generate a synthetic speech signal based on the one or more parameters included in parameter signal . The synthetic speech signal may cause an audio output device to output synthetic speech of the text . Accordingly the speech synthesizer may then send the synthetic speech signal to an audio output device such as a speaker.

In one example the update module may be configured to update an HMM included in the selected sequence. The update module may use one or more FSMs included to update the HMM. The update module may receive the sequence of models from the parameter generation module and determine whether the sequence of models includes an HMM. Upon determining that the selected sequence includes one or more HMMs the update module may update the one or more HMMs using one or more similar FSMs. This may result in the one or more HMMs being capable of generating more one or more spectral features that result in synthetic speech that sounds more natural.

At block the method includes training a plurality of HMMs based on a corpus of recorded speech units. As previously described the spectral feature extraction module may send one or more spectral features for each recorded speech unit in the corpus of recorded speech units to the HMM training module . The HMM training module may train a plurality of HMMs based on the one or more spectral features received from the spectral feature extraction module .

Each HMM in the plurality of HMMs may include N states where N is an integer greater than zero. In one example N may be equal to five though in other examples N may be greater or less than five. Each of the N states may be based on a multi mixture Gaussian density function that estimates one or more spectral features of speech corresponding to a given phonemic representation of text. Each multi mixture Gaussian density function may be based on the one or more spectral features received from the spectral feature extraction module for M similar recorded speech units where M is a positive integer. In this example the multi mixture Gaussian density function b o may be given by the following equation 

At block the method includes generating N decision tree for determining an HMM based on a linguistic target. As previously described the HMM training module may generate the N decision trees for determining an HMM that corresponds to a phonemic representation of text such as a linguistic target. The HMM training module may receive the text transcriptions corresponding to each recorded speech unit from the speech database . In one example the HMM training module may generate the N decision trees using a forced Viterbi algorithm. In another example the HMM training module may generate the decision tree using any algorithm method and or process suitable for generating a decision tree for an HMM.

At block the method includes generating a plurality of FSMs based on the corpus of recorded speech units. The FSM generation module may also receive the one or more spectral features corresponding to each recorded unit included in the corpus of recorded speech units from the spectral feature extraction module . The FSM generation module may generate the plurality of FSMs based on the one or more spectral features for each recorded speech unit.

Once each of the vectors v vis determined the FSM generation module may align one or more vectors into one of the N states Sof the FSM where Sis the jstate of the FSM . The FSM generation module may determine a mean and variance for the jstate based on the one or more vector aligned to the istate. The means and variances of states S Scan then be used to estimate the multi mixture Gaussian density function of equation 1 for the FSM allowing the FSM to simulate an HMM.

Returning to the FSM generation module may associate each FSM in the plurality of FSMs with a phonemic representation of text. The FSM generation module may associate each FSM with a phonemic representation of text. To this end the FSM generation module may use any algorithm method and or process now known or later developed that is suitable for associating each FSM with a phonemic representation of text.

At block the method includes reducing a number of FSMs included in the plurality of FSMs. Because the spectral features of each FSM are averaged over N states the amount of space need to store the plurality of FSMs in an electronic database may be less than the amount of data needed to store the speech database . However depending on the amount of available space in the model database in which to store the plurality of FSMs the FSM generation module may reduce the number of FSMs included in the plurality of FSMs that is stored in the model database .

Since each FSM is based on a recorded speech unit some FSMs corresponding to a same phonemic representation of text may be similar. The FSM generation module may reduce the number of FSMs included in the plurality of FSMs by removing a number of similar FSMs from the plurality of FSMs. For instance the FSM generation module may determine a Kullback Leibler distance for one or more FSMs corresponding to a same phonemic representation of text. The Kullback Leibler distance Dfrom a first FSM to a second FSM may be given by the following equation 

The FSM generation module may remove one or more FSM having a Kullback Leibler distance that is less than a threshold. A relative value of the threshold may depend on the size of the data storage device in which the model database is to be stored. In general a number of FSMs included in the plurality of FSM may be inversely proportional to the threshold. That is as the threshold increases the FSM generation module may remove more similar FSMs from the plurality of FSMs. In this example the FSM generation module may reduce a number of FSMs included in the plurality of FSMs by a factor of X. In another example the FSM generation module may use any suitable procedure for reducing the number of FSMs included in the plurality of FSMs.

The threshold may depend on a type of computing system in which the model database is to be stored. Varying the threshold may allow the model database to be stored in a variety of computing systems. For instance if the model database is to be stored in a mobile device such as the client terminal depicted in the threshold may be greater as compared to an example in which the model database is to be stored in a device with greater data storage capacity such as the server device depicted in .

At block the method includes storing the plurality of HMMs the N decision trees in a database and the plurality of FSMs. In one example the plurality of HMMs the N decision trees in a database and the plurality of FSMs are stored in a single database such as the model database . In another example the plurality of HMMs and the plurality of FSMs may be stored in a first database and the N decision trees may stored in a second database. In yet another example the plurality of HMMs the N decision trees in a database and the plurality of FSMs may each be stored in a separate database. Upon completion of the steps of block the method may end.

At block the method includes determining a phonemic representation of text that includes one or more linguistic targets. The text identification module may determine the phonemic representation of text based on text included in the input signal . The phonemic representation of text may include a sequence of one or more linguistic targets. Each of the one or more linguistic targets may include a previous phoneme a current phoneme and a next phoneme. Each of the one or more linguistic targets may also include information indicative of one or more additional features such as phonology details e.g. the current phoneme is a vowel is stressed etc. syllable boundaries and the like. The text identification module may employ any algorithm method and or process now known or later developed to determine the phonemic representation of text.

At block the method includes determining one or more target HMMs. The parameter generation module may identify the phonemic representation of text from the text signal and may access the model database to acquire the N decision trees. The parameter generation module may parse each of the linguistic targets through the N decision trees in order to determine a target HMM.

The parameter generation module may receive the linguistic targets l lfrom the text generation module and may acquire the N decision trees from the model database . The parameter generation module may then parse each of the linguistic targets l lthrough the N decision trees to determine three target HMMs and .

Returning to the method may include identifying one or more FSMs included in the plurality of FSMs having a same current phoneme as one of the one or more linguistic targets at block . The parameter generation module may access the model database to identify one or more FSMs corresponding to a current phoneme of one of the one or more linguistic targets. For instance if a current phoneme of a linguistic target is au the parameter generation module may identify each FSM in the plurality of FSMs having au as a current phoneme.

At block the method may include determining one or more possible sequences of synthetic speech models based on the phonemic representation of text. The parameter generation module may determine the one or more possible sequences. Each sequence may include a synthetic speech model such as an FSM or an HMM corresponding to each linguistic target. For example if there are three linguistic targets in a phonemic representation of speech each possible sequence may include a synthetic speech model corresponding to the first linguistic target a synthetic speech model corresponding to the second linguistic target and a synthetic speech model corresponding to the third linguistic target.

At block the method may include determining a selected sequence that minimizes the value of a cost function. As previously described the selected sequence may substantially match the phonemic representation of text. In order to determine the sequence of models the parameter generation module may minimize a cost function. The cost of a possible sequence may be representative of for instance a likelihood that the possible sequence substantially matches the phonemic representation of text. In one example the cost function C i for the isequence of models be given by the following equation 3 where C i is a target cost of the FSMs included in the isequence of models and C i is a join cost for joining the FSMs included in the isequence of models.

The target cost may be based on a similarity between an identified FSM and an associated target HMM. That is the more closely a given FSM matches an associated target HMM the lower the target cost for the given FSM. In one example the parameter generation module may determine that the target cost for a given FSM is the Kullack Leibler distance from the associated target HMM to the given FSM. For instance a first target HMM may correspond to a first linguistic target and a possible sequence may include an FSM corresponding to the first linguistic target. The target cost for the including the FSM in the possible sequence may be given by the following equation 

The join cost Cfor a given pair of FSMs may be indicative of a likelihood that the pair of FSMs substantially matches a given segment of the phonemic representation of text . In one example the join cost may be determined using a lattice that includes the one or more possible sequences. illustrates an example lattice . The parameter generation module may generate the lattice based in order to determine the one or more possible sequences of models.

In the phonemic representation of text may include three linguistic targets. Each column in the lattice may correspond to one of the three linguistic targets arranged from the left to right. Within each column the parameter generation module may sort the FSMs from lowest target cost to highest target cost. In this example three FSMs and may correspond to the current phoneme of the first linguistic target one FSM A may correspond to the current phoneme of the second linguistic target and two FSMs and may correspond to the current phoneme of the third linguist target. The parameter generation module may also include target HMMs and determined from the linguistic targets. In one example the parameter generation model does not include the target HMMs in the lattice . Additionally the phonemic representation of text may include more or fewer linguistic targets and the lattice may include more or fewer synthetic speech models for each linguistic target.

Each connection in the lattice may represent a segment of one of the one or more possible sequences determined at block of the method . The parameter generation module may determine the join cost by determining a distance between the last state of the kFSM and the first state of the k 1FSM. In one example the parameter generation module may determine the join cost Cdetermining a Kullack Leibler distance from a last state Sof the kFSM in a sequence of models and the first state first Sof the k 1FSM. In this example the join cost may be given by the following equation 

The cost function may also include a penalty cost for including an HMM in the sequence of models. In this example the cost function C i may then be given by the following equation C 6 where Cis the penalty cost. The penalty cost may be included to minimize the incidence of including a target HMM in the sequence of models. Additionally the join cost may minimize the incidence in which successive target HMMs are included in the model sequence.

To determine the selected sequence the parameter generation module may determine a value for the cost function for each of the one or more possible sequences. In an example in which the parameter generation module includes the target HMMs in the lattice the selected sequence may correspond to the possible sequence that has a minimum value of the cost function 6 . In an example in which the target HMMs are not included in the lattice the selected sequence may correspond to the possible sequence that has a minimum value of the cost function 3 . The selected sequence may include at least one FSM.

At block the method may include generating a synthetic speech signal based on the selected sequence. After determining the selected sequence the parameter generation module may generate the parameter signal based on the selected sequence. The parameter signal may include information indicative of the selected sequence. The parameter generation module may send the parameter signal to the speech generation module .

In one example the speech generation module may concatenate the one or more synthetic speech models included in the selected sequence to form a concatenated sequence. The speech generation module may then generate the synthetic speech signal based on the concatenated sequence. The synthetic speech signal may include information indicative of one or more spectral features for each state of each synthetic speech model included in the selected sequence. For instance the synthetic speech signal may include information indicative of one or more spectral features generated from at least one FSM included in the selected sequence. The speech generation module may send the synthetic speech signal to an audio output device such as a speaker. Alternatively the speech generation module may send the synthetic speech signal to another computing system configured to output audio the synthetic speech signal as audio.

At block the method includes updating target HMMs included in the selected sequence of models. The update module may receive the selected sequence from the parameter generation module and determine whether the selected sequence includes an HMMs such as one of the target HMMs. Upon determining that the selected sequence of models includes a target HMM the update module may update one or more spectral features estimated by the target HMM. The update may be based on one or more spectral features of one or more FSMs having a same current phoneme as the target HMM.

In one example the update module updates the target HMM using a transformation matrix. The transformation matrix may include information for updating one or more states of the HMM. For instance consider an example in which the synthetic speech models have five states. States Sand Smay be considered boundary states and states S Smay be considered central states. The transformation matrix may include an update to one or more central states of the HMM based on one or more central states of one or more FSMs corresponding to the same central phoneme unit as the HMM. The transformation matrix may also include an update for one or more boundary states of the HMM based on a boundary state of one or more FSMs concatenated to the HMM in the selected sequence. For instance an update to the first state of the HMM may be based on the Nstate of an FSM that precedes the HMM in the selected sequence. Similarly an update to the Nstate of the HMM may be based on the first state of an FSM that follows the HMM in the selected sequence. In this manner the central states of the HMM may be updated with more data than the boundary states. This may result in HMMs that more closely model natural speech.

The above detailed description describes various features and functions of the disclosed systems devices and methods with reference to the accompanying figures. In the figures similar symbols typically identify similar components unless context indicates otherwise. The illustrative embodiments described in the detailed description figures and claims are not meant to be limiting. Other embodiments can be utilized and other changes can be made without departing from the spirit or scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure as generally described herein and illustrated in the figures can be arranged substituted combined separated and designed in a wide variety of different configurations all of which are explicitly contemplated herein.

With respect to any or all of the flow diagrams scenarios and flow charts in the figures and as discussed herein each step block and or communication may represent a processing of information and or a transmission of information in accordance with example embodiments. Alternative embodiments are included within the scope of these example embodiments. In these alternative embodiments for example functions described as steps blocks transmissions communications requests responses and or messages may be executed out of order from that shown or discussed including in substantially concurrent or in reverse order depending on the functionality involved. Further more or fewer steps blocks and or functions may be used with any of the message flow diagrams scenarios and flow charts discussed herein and these message flow diagrams scenarios and flow charts may be combined with one another in part or in whole.

A step or block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein described method or technique. Alternatively or additionally a step or block that represents a processing of information may correspond to a module a segment or a portion of program code including related data . The program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique. The program code and or related data may be stored on any type of computer readable medium such as a storage device including a disk drive a hard drive or other storage media.

The computer readable medium may also include non transitory computer readable media such as computer readable media that stores data for short periods of time like register memory processor cache and or random access memory RAM . The computer readable media may also include non transitory computer readable media that stores program code and or data for longer periods of time such as secondary or persistent long term storage like read only memory ROM optical or magnetic disks and or compact disc read only memory CD ROM for example. The computer readable media may also be any other volatile or non volatile storage systems. A computer readable medium may be considered a computer readable storage medium for example or a tangible storage device.

Moreover a step or block that represents one or more information transmissions may correspond to information transmissions between software and or hardware modules in the same physical device. However other information transmissions may be between software modules and or hardware modules in different physical devices.

While various example aspects and example embodiments have been disclosed herein other aspects and embodiments will be apparent to those skilled in the art. The various example aspects and example embodiments disclosed herein are for purposes of illustration and are not intended to be limiting with the true scope being indicated by the following claims.

