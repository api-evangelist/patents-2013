---

title: Method and device for performing caching of dynamically generated objects in a data communication network
abstract: A method for maintaining a cache of dynamically generated objects. The method includes storing in the cache dynamically generated objects previously served from an originating server to a client. A communication between the client and server is intercepted by the cache. The cache parses the communication to identify an object determinant and to determine whether the object determinant indicates whether a change has occurred or will occur in an object at the originating server. The cache marks the object stored in the cache as invalid if the object determinant so indicates. If the object has been marked as invalid, the cache retrieves the object from the originating server.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08788581&OS=08788581&RS=08788581
owner: Citrix Systems, Inc.
number: 08788581
owner_city: Fort Lauderdale
owner_country: unknown
publication_date: 20130118
---
The present application claims priority to and is a continuation of U.S. application Ser. No. 11 324 131 titled Method and Device for Performing Caching of Dynamically Generated Objects in a Data Communication Network filed on Dec. 30 2005 and claims priority to and is a continuation in part of U.S. application Ser. No. 11 169 002 titled Method and Device for Performing Integrated Caching in a Data Communication Network filed on Jun. 29 2005 and claims priority to and is a continuation in part of U.S. application Ser. No. 11 039 046 titled Barrier Crossover Device filed on Jan. 24 2005 all of which are incorporated herein by reference for all purposes.

The present invention relates generally to caching data in a network. In particular the present invention relates to a method and device for performing caching of dynamically generated objects in a network.

The growth rate of network traffic continues to strain the infrastructure that carries that traffic. Various solutions have arisen to permit network operators to handle this increasing problem including the development of caching technology. With traditional caching static content can be reused and served to multiple clients without burdening server infrastructure. Additionally cache memories permit static content to be stored closer to the end user thereby improving response time while at the same time reducing server infrastructure burden. Lowered response times and lowered server infrastructure load reduces bandwidth and the processing requirements of such infrastructure.

However an increasing amount of the content delivered across networks is dynamically generated including a large percentage of network traffic created by enterprise computing solutions and complex internet applications. Dynamically generated content is content generated by the server at the time an object is requested and is often based on inputs received from the client. Therefore it frequently changes both through time and with respect to inputs made to the generating system. Common examples of dynamic content include where a stock quotation request made by a client or database searches. In each instance the response object is generated in real time following receipt of a specific client request.

The challenges to caching dynamically generated content are manifold. For example there are no generally accepted standards or specifications for caching dynamically generated content. Since there exists no standard for designating whether a dynamically generated object may be cached such objects are typically treated as non cacheable. Another challenge is determining the validity of freshness of a dynamically generated object because changes to the underlying data used to generate such objects may be irregular and unpredictable.

In addition to the above difficulties requests for dynamically generated content are also typically more complex than requests for static content. Dynamic requests often contain a string of information that needs to be processed or parsed by the destination application to identify applicable parameters that will be used by the cache to identify the appropriate object related to such request. These parameters however are rarely placed in the request in a logical or consistent order by the client. To determine which of the multitude of dynamically generated objects is identified by the request each such request must be normalized i.e. place the parameters in non arbitrary order .

Furthermore matching a request to a dynamically generated object becomes a much more complex task with dynamically generated content because certain processing done by the application may need to be duplicated or otherwise anticipated by making an educated guess. This duplication or guessing is necessary to decide whether an object stored by the cache is appropriate for serving to a particular incoming request. The complexity arises as a result of the complexity of the applications themselves and also because the contents of the response can be a function of both the contents of the request as well as certain other external variables like the user identity which may or may not be present in the request the time of the day the current state of the user database and a myriad of other factors.

In summary caching originally developed around the caching of static objects. As the Internet and applications becomes more and more dependent upon delivering dynamically generated content the need has arisen for a solution that extends the benefits of caching to dynamic content and that solves the variety of challenges such content presents for traditional caching technology.

The solution of the present invention increases the ability of cache memories to store and serve dynamically generated data. The present invention also enables the cache to effectively deal with a variety of different application request types thereby increasing application performance and easing the administrative complexity of preserving freshness of data served from the cache. The present invention provides an effective approach to caching dynamic content by the use of heuristics to effectively predict the behavior of such applications servers in addition to incorporating the ability to understand and process data in a way that does not duplicate processing carried out by the application server that originally generates the object. These techniques of the present invention in turn increase the use of dynamic caching and thereby contribute to the improvement of the performance of both the network as well as underlying application infrastructure.

The present invention is directed towards a method and system for caching and maintaining dynamically generated objects in a cache. The techniques of the present invention include receiving an invalidation command at the cache to invalidate an object such as a dynamically generated object previously served from an originating server and stored in the cache. The dynamically generated object may not be identified as cacheable from the originating server. The invalidation command received by the cache identifies the cached dynamically generated object. In response to the invalidation command the cache marks the cached dynamically generated object as invalid and flushes the object from the cache.

The present invention also provides techniques for identifying cached dynamically generated objects using an object determinant. The cache may intercept a communication between the client and server and parse the communication to identify an object determinant. The object determinant may identify an object previously served and stored in the cache. The cache determines from the object determinant whether a change has occurred or will occur in the object identified by the object determinant at the originating server. If a change has occurred or will occur the cache marks as invalid in the cache the object identified by the object determinant. Once the object has been marked as invalid the cache may flush the invalid object and retrieve the object from the originating server.

Further embodiments of the present invention apply the above methods to groups of dynamically generated objects. For example a group of previously served dynamically generated objects are formed in the cache. The group of objects is associated with at least one object determinant. A record of the group is maintained in the cache and may be associated with the object determinant. The group of previously served objects is marked as invalid if the identified object determinant of a communication intercepted by the cache indicates a change has occurred or will occur in one or more objects of the group at the originating server.

In one aspect the present invention is related to a method for caching a dynamically generated object not identified as cacheable. The method includes storing in a cache a dynamically generated object served from an originating server and not identified as cacheable and receiving by the cache a request to invalidate the cached dynamically generated object. In response to the request the cache marks the cached dynamically generated object as invalid. The cache may be operated on any device such as an appliance a network device or a computing device in communications between the originating server and a client.

In some embodiments the method of the present invention includes requesting by the originating server to invalidate the cached dynamically generated object. In a further embodiment the originating server automatically requests to invalidate the cached dynamically generated object in response to a change to the dynamically generated object in the originating server. In another embodiment the client is in communication with the originating server to receive the dynamically generated object and the client requests to invalidate the cached dynamically generated object. In yet another embodiments an external administrative control requests invalidation of the cached dynamically generated object.

In one embodiment the method of the present invention includes flushing from the cache the cached dynamically generated object marked as invalid. In another embodiment the cache receives the request to invalidate within a very short time period for example ten milliseconds or less of caching the dynamically generated object. In some embodiments the cache invalidates the cached dynamically generated object responsive to an expiration of a very short time expiry such as an expiry of 10 milliseconds or less of the cached object.

In another aspect the present invention is related to a method for caching a group of dynamically generated objects. In some embodiments the group of objects has at least one object not identified as cacheable. The method includes identifying in a cache a group of dynamically generated objects previously served from an originating server. The cache associates the group with an object determinant. The method further includes intercepting by the cache a communication identifying the object determinant of the group and indicating a change is about to occur or has occurred on the originating server to one of the objects of the group. In one embodiment the method of the present invention includes marking by the cache the group of dynamically generated objects as invalid in response to intercepting the communication or identifying the object determinant.

Furthermore in some embodiments of the present invention the cache may flush from the cache the group of objects marked as invalid. In other embodiments the group of dynamically generated objects is pre designated. In other embodiments the method automatically identifies the group of dynamically generated objects and associates the object determinant with the group according to a rule.

In one aspect the present invention is related to another method for maintaining a cache of dynamically generated object. This method of the present invention includes intercepting by a cache a communication between a client and an originating server for example a client s request to an originating server for a dynamically generated object. The dynamically generated object may have been previously served from the originating server and stored in the cache. The method identifies by the cache an object determinant in the communication indicating one of a change has occurred or will occur in a dynamically generated object at the originating server and marks by the cache the cached dynamically generated object as invalid. The method then obtains the requested dynamically generated object from the originating server.

In one embodiment of the method the cache flushes the invalid dynamically generated object. In another embodiment the cache associates the dynamically generated object with a group of dynamically generated objects previously served from the originating server associates the group with the object determinant and marks the group of dynamically generated objects as invalid in response to the request. In some embodiments the dynamically generated object is not identified as cacheable. In further embodiments the cache flushes the group of dynamically generated objects marked as invalid. The group of dynamically generated objects may be pre designated or other automatically identified via an object determinant according to a rule.

In some embodiments of the method the client embeds in the communication the object determinant as a pre defined string. In other embodiments the cache identifies the object determinant based on a pre defined heuristic rule associated with the dynamically generated object. In one embodiment the cache selects the object determinant from one of the following elements of the communication 1 USERID 2 IP address 3 TCP port 4 HTTP header 5 custom HTTP header 6 client URL 7 cookie header 8 URL query string and 9 POST body. The cache may also extract from the communication the object determinant based on a user configured invalidation policy.

In another embodiment the method of the present invention includes maintaining a table in the cache to associate the object determinant with one or more objects or groups of objects stored in the cache. In some embodiments the method includes examining by an intelligent statistical engine communications from the client to identify a set of dynamically generated objects to associate as a group in the cache. The cache may associate objects stored in the cache into a content group. The content group may be represented by a hash table having an incarnation number as an index. In other embodiments the cache performs a hash algorithm on the dynamically generated object identified via the request to determine a change to the dynamically generated object.

In some aspects the present invention is related to a system for caching a dynamically generated object not identified as cacheable the system includes means for storing in a cache a dynamically generated object not identified as cacheable the dynamically generated object served from an originating server and a means for receiving by the cache a request to invalidate the cached dynamically generated object. The system also includes means for marking by the cache in response to the request the cached dynamically generated object as invalid.

In other aspects the presented is related to a system for caching a group of objects such as dynamically generated objects. In some embodiments the dynamically generated objects has at least one object not identified as cacheable. The system includes means for identifying in a cache a group of dynamically generated objects previously served from an originating server at least one of the dynamically generated objects not identified as cacheable and means for associating by the cache the group with an object determinant. The system also includes means for intercepting by the cache a communication identifying the object determinant of the group and indicating a change is about to occur or has occurred on the originating server to one of the objects of the group.

In one aspect the present invention is related to a system for maintaining a cache of dynamically generated objects. The system includes means for intercepting by a cache a communication between a client and an originating server. The system also includes means for identifying by the cache an object determinant in the communication that indicates a change has occurred or will occur in a dynamically generated object at the originating server and means for marking by the cache the cached dynamically generated object as invalid. In some embodiments the system also includes means for obtaining the dynamically generated object from the originating server.

The details of various embodiments of the invention are set forth in the accompanying drawings and the description below.

The features and advantages of the present invention will become more apparent from the detailed description set forth below when taken in conjunction with the drawings in which like reference characters identify corresponding elements throughout. In the drawings like reference numbers generally indicate identical functionally similar and or structurally similar elements.

Each of the clients are communicatively coupled to appliance via a public data communication network while appliance is communicatively coupled to servers via a private data communication network . In one embodiment public data communication network comprises the Internet and private data communication network comprises an enterprise network. The public data communication network and private data communication network can be any type and form of network public private or otherwise and in some cases may be the same network.

Although shows a network and a network between the clients and the servers the clients and the servers may be on the same network or . The networks and can be the same type of network or different types of networks. The network and or the network can be a local area network LAN such as a company Intranet a metropolitan area network MAN or a wide area network WAN such as the Internet or the World Wide Web. The network and or may be any type and or form of network and may include any of the following a point to point network a broadcast network a wide area network a local area network a telecommunications network a data communication network a computer network an ATM Asynchronous Transfer Mode network a SONET Synchronous Optical Network network a SDH Synchronous Digital Hierarchy network a wireless network and a wireline network. The topology of the network and or may be a bus star or ring network topology. The network and or and network topology may be of any such network or network topology as known to those ordinarily skilled in the art capable of supporting the operations of the present invention described herein.

As shown in the appliance is shown between the public data communication network and the private data communication network some In other embodiments the appliance may be located on the public data communication network or on the private data communication network . In other embodiments the appliance could be an integral part of any individual client or any individual server on the same or different network as the client . As such the appliance may be located at any point in the network or network communications path between a client and a server 

In accordance with an embodiment of the present invention the appliance includes cache management logic and also includes or has access to a storage medium which it utilizes to implement a cache memory. Using these features appliance monitors object requests made by clients to any of the servers . Objects returned from servers in response to these object requests are stored in the cache memory by appliance . Subsequent requests for the same object from any of clients are intercepted by appliance which attempts to deliver the object from the cache rather than passing the request on to servers . This provides the dual benefit of reducing both the time required to respond to requests from clients and the load on the infrastructure supporting servers 

In summary the network environment depicted in is presented by way of example only and is not intended to be limiting. Based on the teachings provided herein persons skilled in the relevant art s will readily appreciate that the present invention may be implemented in any network environment in which object requests and responses are transferred between nodes of one or more network s .

As will be described in more detail herein in an embodiment of the present invention the appliance integrates caching functionality at the kernel level of the operating system with one or more other processing tasks including but not limited to decryption decompression or authentication and or authorization. Such an implementation is illustrated in the commonly owned and co pending U.S. patent application Ser. No. 11 169 002 entitled Method and Device for Performing Integrated Caching in a Data Communications Network filed Jun. 29 2005 which is incorporated by reference herein. Such an example architecture is described herein in accordance with but the present invention is not so limited and other architectures may be used in practicing the operations of the present invention described herein.

Hardware layer provides the hardware elements upon which programs and services within kernel space and user space are executed. Hardware layer also provides the structures and elements which allow programs and services within kernel space and user space to communicate data both internally and externally with respect to appliance . As shown in the hardware layer includes a processing unit for executing software programs and services a memory for storing software and data network ports for transmitting and receiving data over a network and an encryption processor for performing functions related to Secure Sockets Layer processing of data transmitted and received over the network. In some embodiments the central processing unit may perform the functions of the encryption processor in a single processor. Additionally the hardware layer may comprise multiple processors for each of the processing unit and the encryption processor . Although the hardware layer of appliance is generally illustrated with an encryption processor processor may be a processor for performing functions related to any encryption protocol such as the Secure Socket Layer SSL or Transport Layer Security TLS protocol. In some embodiments the processor may be a general purpose processor GPP and in further embodiments may be have executable instructions for performing processing of any security related protocol.

Although the hardware layer of appliance is illustrated with certain elements in the hardware portions or components of appliance may comprise any type and form of elements hardware or software of a computing device such as the computing device illustrated and discussed in conjunction with further herein. In some embodiments the appliance may comprise a server gateway router switch bridge or other type of computing or network device and have any hardware and or software elements associated therewith.

The operating system of appliance allocates manages or otherwise segregates the available system memory into kernel space and user space . In example software architecture the operating system may be any type and or form of Unix operating system although the invention is not so limited. As such the appliance can be running any operating system such as any of the versions of the Microsoft Windows operating systems the different releases of the Unix and Linux operating systems any version of the Mac OS for Macintosh computers any embedded operating system any network operating system any real time operating system any open source operating system any proprietary operating system any operating systems for mobile computing devices or network devices or any other operating system capable of running on the appliance and performing the operations described herein.

The kernel space is reserved for running the kernel including any device drivers kernel extensions or other kernel related software. As known to those skilled in the art the kernel is the core of the operating system and provides access control and management of resources and hardware related elements of the application . In accordance with an embodiment of the present invention the kernel space also includes a number of network services or processes working in conjunction with a cache manager . sometimes also referred to as the integrated cache the benefits of which are described in detail further herein. Additionally the embodiment of the kernel will depend on the embodiment of the operating system installed configured or otherwise used by the device .

In one embodiment the device comprises one network stack such as a TCP IP based stack for communicating with the client and or the server . In one embodiment the network stack is used to communicate with a first network such as network and a second network . In some embodiments the device terminates a first transport layer connection such as a TCP connection of a client and establishes a second transport layer connection to a server for use by the client e.g. the second transport layer connection is terminated at the appliance and the server . The first and second transport layer connections may be established via a single network stack . In other embodiments the device may comprise multiple network stacks for example and and the first transport layer connection may be established or terminated at one network stack and the second transport layer connection on the second network stack . For example one network stack may be for receiving and transmitting network packet on a first network and another network stack for receiving and transmitting network packets on a second network. In one embodiment the network stack comprises a buffer for queuing one or more network packets for transmission by the appliance .

As shown in the kernel space includes the cache manager a high speed layer 2 7 integrated packet engine an encryption engine a policy engine and multi protocol compression logic . Running these components or processes and in kernel space or kernel mode instead of the user space improves the performance of each of these components alone and in combination. Kernel operation means that these components or processes and run in the core address space of the operating system of the device . For example running the encryption engine in kernel mode improves encryption performance by moving encryption and decryption operations to the kernel thereby reducing the number of transitions between the memory space or a kernel thread in kernel mode and the memory space or a thread in user mode. For example data obtained in kernel mode may not need to be passed or copied to a process or thread running in user mode such as from a kernel level data structure to a user level data structure. In another aspect the number of context switches between kernel mode and user mode are also reduced. Additionally synchronization of and communications between any of the components or processes and can be performed more efficiently in the kernel space .

In some embodiments any portion of the components and may run or operate in the kernel space while other portions of these components and may run or operate in user space . In one embodiment the present invention uses a kernel level data structure providing access to any portion of one or more network packets for example a network packet comprising a request from a client or a response from a server . In some embodiments the kernel level data structure may be obtained by the packet engine via a transport layer driver interface or filter to the network stack . The kernel level data structure may comprise any interface and or data accessible via the kernel space related to the network stack network traffic or packets received or transmitted by the network stack . In other embodiments the kernel level data structure may be used by any of the components or processes and to perform the desired operation of the component or process. In one embodiment a component and is running in kernel mode when using the kernel level data structure while in another embodiment the component and is running in user mode when using the kernel level data structure. In some embodiments the kernel level data structure may be copied or passed to a second kernel level data structure or any desired user level data structure.

The cache manager may comprise software hardware or any combination of software and hardware to provide cache access control and management of any type and form of content such as objects or dynamically generated objects served by the originating servers . The data objects or content processed and stored by the cache manager may comprise data in any format such as a markup language or communicated via any protocol. In some embodiments the cache manager duplicates original data stored elsewhere or data previously computed generated or transmitted in which the original data may require longer access time to fetch compute or otherwise obtain relative to reading a cache memory element. Once the data is stored in the cache memory element future use can be made by accessing the cached copy rather than refetching or recomputing the original data thereby reducing the access time. In some embodiments the cache memory element nat comprise a data object in memory of device . In other embodiments the cache memory element may comprise memory having a faster access time than memory . In another embodiment the cache memory element may comprise any type and form of storage element of the device such as a portion of a hard disk. In some embodiments the processing unit may provide cache memory for use by the cache manager of the present invention. In yet further embodiments the cache manager may use any portion and combination of memory storage or the processing unit for caching data objects and other content.

Furthermore the cache manager of the present invention includes any logic functions rules or operations to perform any embodiments of the techniques of the present invention described herein. For example the cache manager includes logic or functionality to invalidate objects based on the expiration of an invalidation time period or upon receipt of an invalidation command from a client or server . In some embodiments the cache manager may operate as a program service process or task executing in the kernel space and in other embodiments in the user space . In one embodiment a first portion of the cache manager executes in the user space while a second portion executes in the kernel space . In some embodiments the cache manager can comprise any type of general purpose processor GPP or any other type of integrated circuit such as a Field Programmable Gate Array FPGA Programmable Logic Device PLD or Application Specific Integrated Circuit ASIC .

The policy engine may include for example an intelligent statistical engine or other programmable application s . In one embodiment the policy engine provides a configuration mechanism to allow a user to identifying specify define or configure a caching policy. Policy engine in some embodiments also has access to memory to support data structures such as lookup tables or hash tables to enable user selected caching policy decisions. In other embodiments the policy engine may comprise any logic rules functions or operations to determine and provide access control and management of objects data or content being cached by the appliance in addition to access control and management of security network traffic network access compression or any other function or operation performed by the appliance . Further examples of specific caching policies are further described herein.

The encryption engine comprises any logic business rules functions or operations for handling the processing of any security related protocol such as SSL or TLS or any function related thereto. For example the encryption engine encrypts and decrypts network packets or any portion thereof communicated via the appliance . The encryption engine may also setup or establish SSL or TLS connections on behalf of the client server or appliance . As such the encryption engine provides offloading and acceleration of SSL processing. In one embodiment the encryption engine uses a tunneling protocol to provide a virtual private network between a client and a server . In some embodiments the encryption engine is in communication with the Encryption processor . In other embodiments the encryption engine comprises executable instructions running on the Encryption processor .

The multi protocol compression engine comprises any logic business rules function or operations for compressing one or more protocols of a network packet such as any of the protocols used by the network stack of the device . In one embodiment multi protocol compression engine compresses bi directionally between clients and servers any TCP IP based protocol including Messaging Application Programming Interface MAPI email File Transfer Protocol FTP HyperText Transfer Protocol HTTP Common Internet File System CIFS protocol file transfer Independent Computing Architecture ICA protocol Remote Desktop Protocol RDP Wireless Application Protocol WAP Mobile IP protocol and Voice Over IP VoIP protocol. In other embodiments multi protocol compression engine provides compression of Hypertext Markup Language HTML based protocols and in some embodiments provides compression of any markup languages such as the Extensible Markup Language XML . In one embodiment the multi protocol compression engine provides compression of any high performance protocol such as any protocol designed for appliance to appliance communications. In another embodiment the multi protocol compression engine compresses any payload of or any communication using a modified transport control protocol such as Transaction TCP T TCP TCP with selection acknowledgements TCP SACK TCP with large windows TCP LW a congestion prediction protocol such as the TCP Vegas protocol and a TCP spoofing protocol.

As such the multi protocol compression engine of the present invention accelerates performance for users accessing applications via desktop clients e.g. Microsoft Outlook and non Web thin clients such as any client launched by popular enterprise applications like Oracle SAP and Siebel and even mobile clients such as the Pocket PC. In some embodiments the multi protocol compression engine by executing in the kernel mode and integrating with packet processing engine accessing the network stack is able to compress any of the protocols carried by the TCP IP protocol such as any application layer protocol.

High speed layer 2 7 integrated packet engine also generally referred to as a packet processing engine or packet engine is responsible for managing the kernel level processing of packets received and transmitted by appliance via network ports . The high speed layer 2 7 integrated packet engine may comprise a buffer for queuing one or more network packets during processing such as for receipt of a network packet or transmission of a network packer. Additionally the high speed layer 2 7 integrated packet engine is in communication with one or more network stacks to send and receive network packets via network ports . The high speed layer 2 7 integrated packet engine works in conjunction with encryption engine cache manager policy engine and multi protocol compression logic . In particular encryption engine is configured to perform SSL processing of packets policy engine is configured to perform functions related to traffic management such as request level content switching and request level cache redirection and multi protocol compression logic is configured to perform functions related to compression and decompression of data.

The high speed layer 2 7 integrated packet engine includes a packet processing timer . In one embodiment the packet processing timer provides one or more time intervals to trigger the processing of incoming i.e. received or outgoing i.e. transmitted network packets. In some embodiments the high speed layer 2 7 integrated packet engine processes network packets responsive to the timer . The packet processing timer provides any type and form of signal to the packet engine to notify trigger or communicate a time related event interval or occurrence. In many embodiments the packet processing timer operates in the order of milliseconds such as for example 100 ms 50 ms or 25 ms. For example in some embodiments the packet processing timer provides time intervals or otherwise causes a network packet to be processed by the high speed layer 2 7 integrated packet engine at a 10 ms time interval while in other embodiments at a 5 ms time interval and still yet in further embodiments as short as a 3 2 or 1 ms time interval. The high speed layer 2 7 integrated packet engine may be interfaced integrated or in communication with the encryption engine cache manager policy engine and multi protocol compression engine during operation. As such any of the logic functions or operations of the encryption engine cache manager policy engine and multi protocol compression logic may be performed responsive to the packet processing timer and or the packet engine . Therefore any of the logic functions or operations of the encryption engine cache manager policy engine and multi protocol compression logic may be performed at the granularity of time intervals provided via the packet processing timer for example at a time interval of less than or equal to 10 ms. For example in one embodiment the cache manager may perform invalidation of any cached objects responsive to the high speed layer 2 7 integrated packet engine and or the packet processing timer . In another embodiment the expiry or invalidation time of a cached object can be set to the same order of granularity as the time interval of the packet processing timer such as at every 10 ms

In contrast to kernel space user space is the memory area or portion of the operating system used by user mode applications or programs otherwise running in user mode. A user mode application may not access kernel space directly and uses service calls in order to access kernel services. As shown in user space of appliance includes a graphical user interface GUI a command line interface CLI shell services health monitoring program and daemon services . GUI and CLI provide a means by which a system administrator or other user can interact with and control the operation of appliance such as via the operating system of the appliance and either is user space or kernel space . The GUI may be any type and form of graphical user interface and may be presented via text graphical or otherwise by any type of program or application such as a browser. The CLI may be any type and form of command line or text based interface such as a command line provided by the operating system. For example the CLI may comprise a shell which is a tool to enable users to interact with the operating system. In some embodiments the CLI may be provided via a bash csh tcsh or ksh type shell. The shell services comprises the programs services tasks processes or executable instructions to support interaction with the appliance or operating system by a user via the GUI and or CLI .

Health monitoring program is used to monitor check report and ensure that network systems are functioning properly and that users are receiving requested content over a network. Health monitoring program comprises one or more programs services tasks processes or executable instructions to provide logic rules functions or operations for monitoring any activity of the appliance . In some embodiments the health monitoring program intercepts and inspects any network traffic passed via the appliance . In other embodiments the health monitoring program interfaces by any suitable means and or mechanisms with one or more of the following the encryption engine cache manager policy engine multi protocol compression logic packet engine daemon services and shell services . As such the health monitoring program may call any application programming interface API to determine a state status or health of any portion of the appliance . For example the health monitoring program may ping or send a status inquiry on a periodic basis to check if a program process service or task is active and currently running. In another example the health monitoring program may check any status error or history logs provided by any program process service or task to determine any condition status or error with any portion of the appliance .

Daemon services are programs that run continuously or in the background and handle periodic service requests received by appliance . In some embodiments a daemon service may forward the requests to other programs or processes such as another daemon service as appropriate. As known to those skilled in the art a daemon service may run unattended to perform continuous or periodic system wide functions such as network control or to perform any desired task. In some embodiments one or more daemon services run in the user space while in other embodiments one or more daemon services run in the kernel space.

Dynamic content such as one or more dynamically generated objects may be generated by servers referred to as application or originating servers and or back end databases not shown that process object requests from one or more clients local or remote as depicted in . As those applications or databases process data including data related to inputs received from clients the response objects served by these databases and applications may change. Prior objects generated by those applications or databases in an originating server will no longer be fresh and therefore should no longer be stored by a cache. For example given the same set of inputs a dynamically generated object of a first instance may be different than a dynamically generated object of a second instance. In another example the same object may be dynamically generated with a different set of inputs such that a first instance of the object is generated differently from a second instance of the object.

In order to achieve improved network performance the appliance is designed and configured to addresses the problems that arise in caching dynamically generated content through a variety of methods as described in detail below. In some embodiments of the present invention described herein the appliance incorporates a set of one or more techniques for making the invalidation of dynamically generated content stored in the cache more efficient and effective. Furthermore the appliance may incorporate techniques for performing control and caching for flash crowds. Cache memories typically store every response to a request for an object as long as such response is not marked as non cacheable. As described herein efficient caching of dynamically generated contents requires techniques that enable the timely invalidation of objects in the cache memory that have undergone a change at the originating server. Timely invalidation allows the cache to avoid serving stale content a task of particular concern with dynamically generated content especially where changes to the content occur irregularly. Set forth below are a number of techniques to ensure timely invalidation of dynamically generated content.

In one aspect the present invention is related to techniques of integrating functions logic or operations of the cache manager policy engine encryption engine and or the multi protocol compression engine with packet processing operations of the high speed layer 2 7 integrated packet engine responsive to the packet processing timer . For example the operations of the cache manager can be performed within the time intervals of the packet processing timer used for packet processing operations such as on a receipt or transmit of a network packet. In one embodiment by integrating with the packet processing operations and or using the packet processing timer the cache manager of the present invention can cache objects with expiry times down to very small intervals of time as will be described in further detail below. In other embodiments the cache manager responsive to the packet processing timer can also receive an invalidation command to invalidate an object within a very short time period of caching the object.

The method depicted in illustrates one embodiment of a technique of the present invention for requesting the cache manager policy engine encryption engine and or the multi protocol compression engine to perform an operation during processing or in association with the time intervals for processing a network packet by the high speed layer 2 7 integrated packet engine or packet processing engine . In brief overview at step of method the device receives a network packet or is requested to transmit a network packet. At step the device requests the packet processing engine to process the network packet responsive to the packet processing timer . As part of or associated with packet processing operations at step the packet processing engine requests the cache manager policy engine encryption engine and or the multi protocol compression engine to perform an operation on a cached object. At step the cache manager policy engine encryption engine and or the multi protocol compression engine performs the requested operation which may include any one or combination of the techniques of the present invention described herein. In one embodiment the cache manager determines invalidation of a cached object and marks the cached object invalid. In some embodiments the cache manager flushes the invalid object in response to a request by the packet processing engine . As the cache manager is performing these operations responsive to the packet processing timer invalidation of objects can occur within time periods in the order of milliseconds and with objects having an expiry in the order of the time intervals provided by the packet processing timer such as 10 ms.

In further detail of method of the present invention at step the appliance receives one or more network packets and or transmits one or more network packets. In some embodiments the appliance requests to transmit one or more network packets over the network or network . In another embodiment the appliance receives a network packet on one port and transmits a network packet on the same port or a different port . In some embodiments the packet engine of the appliance transmits or requests to transmit one or more network packets. In one embodiment the appliance receives or transmits a packet on a first network while in another embodiment the appliance receives or transmits a packet on a second network . In other embodiments the appliance receives and transmits packets on the same network. In some embodiments the appliance receives and or transmits networks packets to one or more clients . In other embodiments the appliance receives and or transmits networks packets to one or more servers 

At step the device may request or trigger packet processing operations of the packet processing engine upon receipt of a network packet at the network port of the device or upon request to transmit a network packet from the device or upon any combination of receipt and or transmit of one or more network packets. In some embodiments the packet processing operations of the packet processing engine are triggered via a signal provided by a packet processing timer . In one embodiment the packet processing timer may provide interrupt driven or event driven timer functionality related to the receipt and or transmission of one or more network packets. In some embodiments the packet processing timer is driven by a rate of receipt and or transmit of network packets via the device or by the rate by which each packet or a batch of packets are processed. As such the packet processing timer may be triggered and reset after each set of one or more packet processing operations. In another embodiment the packet processing timer provides time intervals either equal or variable time intervals to trigger wake up or signal the packet processing engine to perform a function or operation such as handling a received packet or transmitting a submitted packet. As discussed above in connection with the device of the packet processing timer may operate in the order of milliseconds such as causing time intervals or triggering of packet processing operations at intervals of 10 ms or less. The granular timer functionality of the packet processing timer of the present invention may be provided in various ways and used in operations of the packet processing operations of the packet processing engine .

At step of method of the present invention the packet processing engine requests one or more of the cache manager policy engine encryption engine and or the multi protocol compression engine to perform an operation. In one embodiment the packet processing engine or packet processing timer generates a signal or signals to one or more of the cache manager policy engine encryption engine and or the multi protocol compression engine . The packet processing engine may request or signal the operation at any point before during or after a packet processing operation of a network packet or one or more packets. In one embodiment the packet processing engine makes the request upon trigger of the packet processing timer or expiration of a time interval provided by the packet processing timer and before performing a packet processing operation on a network packet. In another embodiment during the course of performing one or more packet processing operations the packet processing engine makes the request. For example during execution of an operation such as within a function call the packet processing engine may make an application programming interface API call to one of the cache manager policy engine encryption engine and or the multi protocol compression engine . In other embodiments the packet processing engine makes the request upon completion of a network packet processing operation.

At step the requested operation is performed by one or more of the cache manager policy engine encryption engine and or the multi protocol compression engine . In some embodiments any functionality or operation provided via the kernel may be requested to be executed such as via a kernel application programming interface API . As such any of the functions of the device may be performed in conjunction with the timing or timing intervals of packet processing via the packet processing timer . In some embodiments the requested operation is performed synchronously and in conjunction with the packet processing operations of the packet processing engine . For example the packet processing operations wait and continue upon a completion of or response from the requested operation. In other embodiments the requested operation is performed asynchronously with the packet processing operations. For example the packet processing engine sends a request to perform the operation but does not block or wait to receive a response from the operation. As will be discussed in further detail in conjunction with method of the present invention depicted in the packet processing engine may request the cache manager to perform any cache management function such as checking for expiry or invalidation of objects marking objects as invalid or flushing invalid or expired objects.

In some embodiments the packet processing engine at step sends multiple requests such as a first request to the cache manager and a second request to the encryption engine . In other embodiments the packet processing engine at step sends a single request comprising multiple requests to be distributed by the device such as via the kernel to the intended component of the device . In one embodiment the requests are communicated subsequent to each other. In another embodiment requests may be dependent on the status result success or completion of a previous request. For example a first request to the policy engine may be used to determine a policy for processing a network packet from another device or a user associated with the network packet. Based on a policy of the policy engine a second request to the cache may be made or not made depending on a result of the first request. With the cache manager policy engine encryption engine and or the multi protocol compression engine integrated in the kernel space of the device with the packet processing engine there are various operations of the device as described herein that may be triggered by and integrated with packet processing operations.

In another aspect the present invention is related to and incorporates the ability to configure the expiration time of objects stored by the cache to fine granular time intervals such as the granularity of time intervals provided by the packet processing timer. This characteristic is referred to as invalidation granularity. As such in one embodiment the present invention can cache objects with expiry times down to very small intervals of time. In other embodiments the cache manager responsive to a packet processing timer can also receive an invalidation command to invalidate an object within a very short time period of caching the object. By providing this fine granularity in expiry time the cache of the present invention can cache and serve objects that frequently change sometimes even many times within a second. One technique is to leverage the packet processing timer used by the device of the present invention that is able operate at time increments on the order of milliseconds to permit invalidation or expiry granularity down to 10 ms or less. Traditional caches by contrast to the present invention typically do not set expiry or invalidation granularity of less than one second.

Referring now to an embodiment of a method of the present invention is depicted for invalidating or expiring a cached object responsive to the packet processing timer and or packet processing engine . As such in some embodiments of the present invention cached objects can be invalidated or expired in the order of milliseconds such as 10 ms or less. In overview at step of method the cache manager receives a signal or request to perform an operation via the packet processing engine in response to the packet processing timer . At step the cache manager determines if a cached object such as a dynamically generated object is invalid or expired. At step if the object is invalid the cache manager marks the object as invalid and at step flushes the invalid object from the cache manager .

In further detail of step in some embodiments the cache manager may be signaled or requested to perform a cache related operation at any point of time during network packet processing. In one embodiment at step the cache manager receives an operation request prior to the processing of a network packet received or to be transmitted by the device . In another embodiment the cache manager receives an operation request upon the completion of processing of a network packet. For example the packet processing engine completes processing of a network packet and before either waiting for the next time interval of the timer or before processing the next packet requests the cache to perform an operation. In other embodiments during an operation of packet processing the packet processing engine communicates an operation request to the cache manager . In another embodiment the cache manager receives a signal such as from the packet processing engine or packet processing timer to trigger the cache manager to perform an operation. In some embodiments the signal indicates to invalidate a cached object or to expire an expiry of a cached object.

In some embodiments the cache manager may receive a request to perform a cache operation from an entity external to the cache manager such as a request to invalidate an object communicated by a server and processed by the packet processing engine . In one embodiment the cache manager may receive an invalidation request within 10 ms or less of caching the object while in another embodiment as short as 5 ms 2 ms or 1 ms. In other embodiments the cache manager may perform a cache operation responsive to the operations or functionality of the cache manager such as the expiration of a timer to cause an object to be invalidated or during the processing of any cache command. In other embodiments the cache manager uses the packet processing timer of the device to trigger cache operations. For example the timer may trigger or signal the cache to check for invalidation or expiry of a cached object at any time interval capable of being set by the timer . In one embodiment the timer may be set to trigger or signal the cache within 10 ms or less of being set or in another embodiment as short as 5 ms 2 ms or 1 ms of being set. In some embodiments the originating server may set the expiry time of the object. In other embodiments the appliance or client may set the expiry time of the object.

At step the cache manager determines the invalidation or expiry of an object stored in cache. In some embodiments an object in cache is invalidated based on the expiration of a timer. In one embodiment the cache manager may issue an invalidation command on an object based on the expiration of a timer. In another embodiment the object stored in cache is automatically invalidated by the cache manager responsive to the expiration of a timer such as a timer set with the packet processing timer . In some embodiments responsive to the packet processing timer the cache manager checks for the expiration of any timers for cached objects. In one embodiment the cache manager determines an object timer has expired while in another embodiment the cache manager determines the object timer has not expired. In a further embodiment the cache manager responsive to a second trigger or second timer interval of the packer processing timer will check a second time if a previously checked object timer has expired.

In some embodiments the cache manager parses interprets accesses reads or otherwise processes an invalidation command or request to identify the object to invalidate in the cache. In one embodiment an entity external to the cache manager issues an invalidation command to the cache manager to invalidate the object. In another embodiment the external entity may issue the invalidation command responsive to a packet processing timer . If the object is valid and or has not been invalidated the cache manager invalidates the object responsive to the request. In some embodiments the invalidation request processed by the cache manager is responsive to the packet processing operations of the packet processing engine processing the request which in turn may also be responsive to the packet processing timer .

At step the cache manager marks the object as invalid. The cache manager may mark each object as invalid in any suitable or desired manner. In one embodiment an object is marked as invalid by setting a flag attribute or property of the stored object. For example a flag may be set to any value identifying to the cache manager the object is invalid. In another embodiment an object may be marked as invalid by moving the object to an area or portion of the cache for storing invalid objects. In other embodiments the cache manager may identify or track the invalid and or valid state of a stored object by a database or a linked list or any type and form of data structure. In some embodiments the cache manager uses one or more objects to identify or track the validity or invalidity of one or more objects stored in cache. In another embodiment the object is marked as invalid by changing modifying or altering the stored object for example deleting or removing a portion of the object so that is may not be used or by changing or mangling the name of the object.

At step the cache manager in some embodiments flushes from the cache those objects marked as invalid. In another embodiment the cache manager flushes the invalid object from cache upon request for the object such as by a client . In some embodiments the cache manager overwrites the invalid object with an updated copy or version of the object received after invalidation or expiration of the object. In another embodiment the cache manager reuses the cache memory occupied by the invalid object by storing another to the same portion of cache memory. In yet another embodiment the cache manager does not flush the object marked as invalid but keeps the object stored in memory or storage of the cache.

Although method describes invalidation and flushing of cached objects responsive to a packet processing timer and or in conjunction with packet processing operations to provide invalidation granularity any operation of the cache and any techniques of cache management as well as any other operation of the device described herein may be executed at fine granular time intervals provided by the packet processing timer. In some embodiments the invalidation or expiration of cached objects can occur as short as a 100 ms time interval while in another embodiment as short as a 50 ms time interval. In some embodiments the invalidation or expiration of cached objects can occur as short as 25 ms time interval and in other embodiments as short as a 10 ms time interval. While in other embodiments the invalidation or expiration of cached objects can occur as short as a 5 ms time interval and still yet in further embodiments as short as a 3 2 or 1 ms time interval.

By incorporating the capacity to invalidate objects after the elapse of very small increments of time as described in methods and in conjunction with above improved caching of dynamically generated content is enabled. Some dynamic content is in fact amenable to being stored and served from a cache for very short periods of time. To successfully cache such content however an approach in accordance with an embodiment of the present invention provides caching objects for very short periods of time before the object is invalidated and flushed from the cache memory. For example certain dynamically generated objects may be cacheable for as long as 1 second but anything longer is frequently unacceptable for content that is constantly changing. In an embodiment the approach of the present invention included invalidating or expiring cached content after small fractions of a second. As an example if an application takes milliseconds to generate a dynamic response then the cache can store and serve that response for a duration of less than or equal to the period of 100 milliseconds without compromising the freshness of the data. There will not be a new object generated during that 100 millisecond period because it is shorter than the time it takes to generate a new object. The appliance can thus be set up to serve the prior object during that duration. The ability of the appliance to invalidate down to very small increments of time is frequently very useful for application environments where the database transaction isolation level is set to allow Repeatable Reads or Serialized Reads.

Traditional caching technology invalidates stored content based on a pre defined expiry time for the content which is typically configured either by the administrator or is received from the server that served the object. Described below is another technique of the present invention for invalidating content in order to more efficiently cache dynamically generated content. The technique of the present invention includes the ability to receive at the appliance an invalidation command that identifies one or more of the previously stored objects in the cache as invalid in real time. For example the invalidation command may be communicated via a network packet transmitted to the client or an application programming interface API call made by a server to the appliance. This differs from the traditional approach by which the server simply sets a cache expiry time that it includes in the object header at the time the object is served.

The technique of the present invention is more specifically illustrated in . is a flow chart illustrating a method for maintaining a cache such as a computer memory cache. In brief overview and according to step dynamically generated objects previously served from an originating server are stored in the cache. For example the dynamically generated object may not be identified as cacheable or otherwise include any cache or cache control information. At step an invalidation command is received at the cache or cache manager . The invalidation command identifies one or more previously served objects as invalid. As step in response to the invalidation command the cache or cache manager marks the identified object as invalid.

In further detail at step the cache manager stores in a cache memory element a dynamically generated object received obtained or communicate from any source. In some embodiments the dynamically generated object may be generated and served from a server . In other embodiments the dynamically generated object may be generated and communicated by a client . In some embodiments another portion component or process of the appliance generates the object and stores the object in the cache. In further embodiments the dynamically generated object may be generated by another appliance or another computing device on the network and transmitted or communicated to the appliance . In some embodiments the dynamically generated object is not identified as cacheable or identified as non cacheable. In other embodiments the dynamically generated object is identified as cacheable or is under cache control.

At step the cache manager receives an invalidation command identifying an object to invalidate such a dynamically generated object stored in the cache. In one embodiment the invalidation command may comprise any type of directive or instruction indicating to the cache that an object in invalid or otherwise may be stale. In some embodiments the invalidation command identifies the object and may also identify the time at which the object is invalid as well as what portions of the object may be invalid. In one embodiment the cache manager provides an application programming interface API that may be called remotely by an originating server . In some embodiments the cache manager may provide any type and form of protocol for receiving commands and replying to commands via one or more network packets. In one embodiment the cache manager or device provides an Extensible Markup Language XML API interface for receiving and processing invalidation commands. For example the cache manager may provide a web service interface. In some embodiments the cache manager replies to the invalidation command by sending an acknowledgement status or other response to the originating server . In other embodiments the cache manager does not reply to the invalidation command. In one embodiment an object is marked as invalid if an application running in an originating server performed an action that made the stored object stale such as by generated a new or updated version of the object. This could occur for example when news editors make changes to a fast developing news story and therefore want to be assured the most recent version of the story is being served to clients.

Invalidation commands may be issued from an originating server by the application that generated the object by another server or another appliance . In one embodiment the originating server issues or communicates an invalidation command to the cache automatically in response to a change to the dynamically generated object on the originating server . The invalidation command can also be generated by an administrative control outside or external to the server and the appliance . For example the administrative control may be any type and form of program or application running on the network and in communication with the appliance such as administrator console. Furthermore a client could issue or communicate an invalidation command to the appliance or cache manager . For example if the client were to take action that the client recognizes would cause a change to the requested objects at the originating server the client may communicate the invalidation command. Any object stored in the cache can be invalidated by the transmission to the cache of a user command executed locally at the cache or invoked remotely using the XML API infrastructure.

According to step an object stored in cache e.g. a previously served dynamically generated object that has been identified as invalid is marked as such in response to the invalidation command. An invalid object will not be provided to a requesting client from the cache but instead would be served directly from the originating server. The cache manager may mark each object as invalid in any suitable or desired manner. In one embodiment an object is marked as invalid by setting a flag attribute or property of the stored object. For example a flag may be set to any value identifying to the cache manager the object is invalid. In another embodiment an object may be marked as invalid by moving the object to an area or portion of the cache for storing invalid objects. In other embodiments the cache manager may identify or track the invalid and or valid state of a stored object by a database or a linked list or any type and form of data structure. In some embodiments the cache manager uses one or more objects to identify or track the validity or invalidity of one or more objects stored in cache. In another embodiment the object is marked as invalid by changing modifying or altering the stored object for example deleting or removing a portion of the object so that is may not be used or by changing or mangling the name of the object.

In some embodiments the appliance subsequently flushes from the cache those objects marked as invalid. In another embodiment the appliance flushes the invalid object from cache upon request for the object such as by a client . In some embodiments the appliance overwrites the invalid object with an updated copy or version of the object. In another embodiment the appliance reuses the cache memory occupied by the invalid object by storing another dynamically generated object to the same portion of cache memory.

With the command invalidation API of the cache manager of the present invention any computing device or user in communication with the appliance may request to invalidate an object such as a dynamically generated object stored in the cache. As such the invalidation of objects stored in cache can be controlled real time instead of using pre determined configuration expiry or invalidation time periods. Thus using these techniques the longevity of the cached objects can be controlled from external application processing nodes such as databases or originating application servers. For example the appliance can be configured to work with a database such that a change to the database automatically triggers an invalidation command from the database or application to the appliance to flush a particular object or objects.

In a further embodiment of the present invention the appliance identifies and invalidates at the same time a group of objects stored by the cache. Objects stored in a traditional cache memory are each treated individually and separately by the cache in determining whether the object is stale. As each object reaches its specified expiry time generally as set by the server and stored by the cache in a table that item is flushed from cache memory. This traditional approach is inefficient and ultimately insufficient however to successfully handle the challenges that arise in attempting to cache dynamically generated content.

Step is the same as in wherein an object is stored in the cache of the appliance such as dynamically generated objects previously served from an originating server . In some embodiments one or more of the objects may not be identified as cacheable or otherwise may not have any cache or cache control information. For example the server may assume the dynamically generated objects will not be cached.

According to step the appliance forms a group out of a set of the objects previously served from the originating server and stored in the cache. Any suitable or desired set of objects may be associated with each other to form a group. For example any dynamically generated objects generated for or associated with serving a web page may form a group. In some embodiments an object may be associated with multiple groups. In other embodiments one group of objects may form a subset of another groups of objects. In some embodiments the formed group of objects have objects served from the same server while in other embodiments the formed group of objects have objects served from different servers . In further embodiments the formed group of objects may comprise objects from a client objects from a server or objects generated by or served from both clients and servers . In one embodiment one object in the group is static while another object in the group is dynamically generated. In some cases one object in the group is not identified as cacheable while another object in the group is identified as cacheable. In other cases the objects in the group may be logically related in accordance with functionality or application provided by a server . In another case the objects in the group may be related as associated with the same client or the same user.

In step a record of the group of objects is maintained. Various techniques for recording and maintaining a record of a group of objects or otherwise associating objects may be used in practicing the operations of the present invention described herein. In one embodiment the record may be maintained directly in for example a look up table. In another embodiments the records could be represented in a hash table format. In some embodiments the cache manager maintains the association of objects in a database or a data structure or object in memory. In further embodiments a flag property or attribute of each object in the group is assigned or set to a value identifying the group such as a value equal to identifying or referencing the name or identifier of the group such as a group s object determinant that will be described in more detail below. In some embodiments a group of objects is arranged placed or located in a portion of cache memory identified as holding the group

In step an invalidation command is received at the appliance or cache manager . According to the embodiment described in the invalidation command identifies that one or more objects are invalid or otherwise are stale. In some embodiments the invalidation command references identifies or specifies a name or identifier of the group of objects. In one embodiment the invalidation command comprises a single invalidation request to invalidate all the objects in the group. In another embodiment the invalidation command identifies one object in the group to invalidate. In other embodiments the invalidation command comprises a plurality of invalidation request to invalidate a plurality of objects in the group

According to step the group of previously served objects is marked as invalid if the invalidation command references identifies or specifies an object of the group as invalid each object in the group as invalid or the group as invalid. In some embodiments if the invalidation command identifies an object in the group as invalid the cache manager marks the object as invalid. In other embodiments if the invalidation command identifies an object in the group as invalid the cache manager marks the group of objects as invalid or each object in the group as invalid. In yet further embodiments the cache manager may only invalidate the group of objects when a plurality of objects are identified as invalid via one or more invalidation commands. In another embodiment the invalidation command may specify a name or identifier of the group and the cache manager marks the group as invalid or each object in the group as invalid.

In one embodiment the appliance or cache manager flushes from the cache memory a group of objects that has been marked as invalid. In some embodiments the objects in the group may be flushed from cache memory only when each object in the group is marked as invalid. In other embodiments if one object of the group has been marked as invalid then the entire group is flushed. In another embodiment the group of objects or any object in the group marked as invalid may be flushed upon receipt of a request for the group of objects or any object in group by a client . In other embodiments the group of objects or any object in the group marked as invalid may be flushed upon receipt of a response from a server provide one or more new objects in the group.

An example of the above described embodiments follows. Customer resource management CRM applications are used by many businesses to track and evaluate all aspects of resource management. Often CRM applications are implemented and accessed over private and public networks including the Internet. These applications which provide access to large amounts of data that is frequently being accessed thus benefit from caching the data generated by such applications. For example sales reports are frequently generated and served to remotely connected users. These sales reports are built by the relevant application through compiling data from sales information that is posted to such application servers and or their underlying databases. As many users request the same document i.e. a certain sales report without caching the application server must re generate the object for each request. If however such objects can be stored in the cache then application and database processing is conserved including potentially valuable bandwidth as the cache is placed closer to the requesting clients.

The challenge for caching such objects arises because each time a new sale is posted to the application running at the originating server or to its underlying database the information in the sales report needs to be updated. As a result all sales reports that may have been stored in any caches supporting these application servers must be invalidated and the content flushed out of cache memory. The traditional approach to caching however has no way of accurately determining when the change to the underlying database or application is going to occur and therefore cannot reasonably evaluate the freshness of dynamic content. Every time a change occurs in database or application or originating server the cache has to be able to identify that the change has been made and which group of objects should be invalidated as a consequence of such change. Generation of invalidation commands that contain object determinants linked to groups of previously served objects as described above can meet this need.

Multiple groups of related objects may be formed at a single hierarchical level. Alternatively sub groups of objects may be formed to create multiple hierarchical levels. In an embodiment the groups or sub groups of objects may be pre designated by a user. In another embodiment a user may establish rules by which the appliance automatically forms groups of related objects and associates object determinants therewith.

An embodiment of the present invention also addresses the need to be able to identify all objects affected by a state change at the originating application server and or underlying database by generating groupings of objects and implementing parameterized invalidation. In this embodiment any object or pre defined group of objects can be invalidated by an intercepted HTTP request for example from a client that the cache parses in order to identify an object determinant. The term object determinant refers to any information data data structure parameter value data pattern request reply or command that references identifies or specifies one object or a set of objects uniquely or otherwise. In some embodiments an object determination is a pattern of bytes or characters in a communication that may be associated with an object or used to uniquely identify that the communication is associated with or referencing the object. In one embodiment an object determinant indicates whether change has occurred or will occur in the originating server to a group of previously served objects stored in the cache manager with which the object determinant is associated. In some embodiments the objects in a group of objects are related in that they are associated with at least one object determinant. Specific non limiting examples of object determinants and further illustrations of their use are described more fully below.

In some embodiments of the present embodiment object determinants are certain pre defined parameters or data structures included or embedded in a client request or response. In other embodiments the client server or appliance embeds in a communication one or more object determinants such as pre defined strings or sets of characters representing the object determinant. The object determinants indicate whether such request will have the effect of causing a change in the state of objects stored in the originating server or databases linked thereto. In one embodiment the existence of the object determinant in a request indicates a change has or will occur to an object. In another embodiment the syntax structure parameter or value of the object determinant indicates a change has or will occur to an object. In an embodiment the cache receives an object request from a client . The request may include certain parameters or values object determinants that the cache recognizes will change the state of the originating server or application server which will as a consequence make stale certain related objects stored by the cache manager that had been previously generated by such originating server or application server . Depending on the invalidation policy set by the user the parameters object determinants may require invalidation of one or more previously served objects or group of objects by the originating server that have been stored by the cache. The cache is configured to identify the relevant objects that will be effected by this state change i.e. those objects or groups of objects linked to the object determinant and invalidate these objects via the method marking each of the objects as invalid and or flushing such objects from the cache memory.

The above described technique is illustrated in . As with other embodiments described herein step comprises storing in the cache objects such as dynamically generated objects previously served from an originating server. The objects could be generated by an application running on the originating server or could be drawn for example from a database accessed by the originating server . In some embodiments the dynamically generated objects are identified as not cacheable or otherwise not identified as cacheable.

According to step the cache intercepts or otherwise receives a communication between the client and the server such as a request from a client or a response from a server. In some embodiment the request is for a specified object the object having been previously served and stored in the cache. In another embodiment the communication includes a response from a server having a requested object. In one embodiment such receipt or interception occurs according to established caching protocol and communications standards. Although the cache manager or appliance may be generally described as receiving a request response or communication in receiving such request response or communication the cache or appliance may intercept or obtain by any suitable means and or mechanisms the request response or communication even though not communicated directly or explicitly to the cache.

In step an object determinant is identified in the intercepted communication. The cache manager may extract interpret parse access read or otherwise process the intercepted communication to determine or identify one or more objects determinants in the communications. Any parameter value syntax data structure or set of one or more characters of the communication may be used to identify an object determinant. In one embodiment the cache manager may identify the name or identifier of an object in a request from the client to the server in which the client requests the object. In another embodiment the cache manager may identify the name or identifier of a first object in the request of the client or response from the server that indicates a change has occurred or will occur to a second object stored in the cache. In other embodiments the cache manager determines if any patterns of characters in the request match any object determinants associated with an object or group of objects in the cache. In some embodiments an object determinant may be determined for an object not currently stored in cache. In other embodiments an object determinant may be determined for an object currently marked as invalid. In other embodiments an object determinant for a requested object is determined to be associated with an object determinant of a cached object. In yet another embodiment upon the first reference request or response for an object in a communication the cache manager establishes the identified object determinant as the object determinant for the object.

By receiving and parsing the communication such as a client request or server response to identify an object determinant the cache manager or appliance may effectively determine whether to mark as invalid a cached object that has been associated with the identified object determinant. Thus according to step a determination is made as to whether the object determinant indicates a change to the cached object. In some embodiments the identified object determinant may be part of a communication that does not alter modify or generate an object. In other embodiments the identified object determinant is a part of a communication that indicates a change has occurred or will occur to the object associated with the object determinant. For example the communication may be a get request for a dynamically generated object or a submit request that will change the data used for one or more dynamically generated objects. In some embodiments the existence of the object determinant in the communication indicates a change has or will occur on one or more objects. In another embodiment the type or name of a command directive or instruction in the communication along with the object determinant indicates a change has or will occur on one or more objects. In yet a further embodiment the existence value or setting of a parameter or variable of a command directive or instruction indicates a change has or will occur on one or more objects associated with an object determinant.

In other embodiments the cache manager performs a hash function algorithm or operation on the intercepted communication or object determinant to determine if a change has occurred in the object. In some embodiments the hash value is compared with a previous stored hash value for the object and if different then the cache manager recognizes the object has changed. In yet another embodiment a hash value for the object may be included in the communication or object determinant. In one embodiment the communication indicates the object has changed by the value or setting of a parameter such as with a Boolean flag. In other embodiments an entity tag control and validation mechanism as will be described in more detail below may be used to identify the object and determine if the object has changed.

If a change is indicated then at step then the object associated with or identified by the object determinant is marked as invalid. In some embodiments an object requested by the intercepted communication is marked as invalid in accordance with step and retrieved from the originating server in accordance with step . Otherwise in other embodiments the requested object is retrieved from the cache in accordance with step . In one embodiment any object marked as invalid will be flushed from the cache.

The above embodiment of the present invention describes the case of invalidating a previously served object in the cache manager based on identification of an object determinant in the client request. This general concept may also be used in another embodiment to identify and invalidate a group of objects with which one or more object determinants have been associated. This embodiment is illustrated in .

The method described in begins in the same fashion as the method of . Step comprises storing in the cache objects such as dynamically generated objects previously served from an originating server. In some embodiments one or more of the objects are not identified as cacheable. According to step and similar to previously served objects are formed into groups. In one embodiment and in accordance with the object determinant technique of the present invention a group of objects is associated with or identified by at least one object determinant. As described more fully below in some embodiments the association of groups with object determinants depends on the nature and details of the users caching policy such as a policy defined controlled or used by the policy engine . In other embodiment the one or more object determinant of the group comprises the one or more object determinants of the objects in the group. In another embodiment the object determinant of the group comprises a combination of object determinants of objects in the group.

According to step a record is maintained of the group along with its associated object determinants if applicable. This step is similar to step illustrated in . In one embodiment the record and or any object determinants of the group is maintained in a look up table. In other embodiments the record and or any object determinants of the group may be maintained in a hash table format. The hash table may be designed to efficiently store non contiguous keys that may have wide gaps in their alphabetic and numeric sequences. In another embodiment an indexing system can be built on top of a hash table. In some embodiments the cache manager maintains the association of objects as a group with one or more object determinants in a database or a data structure or object in memory. In further embodiments a flag property or attribute of each object in the group is assigned or set to a value identifying the group such as a value equal to identifying or referencing the name or identifier of the group or a group s object determinant. In some embodiments a group of objects is arranged placed or located in a portion of cache memory identified as holding the group. In another embodiment the one or more object determinants are stored in association with the group of objects.

Steps and are similar to steps and as illustrated in . According to step the cache manager or appliance intercepts or otherwise receives a communication between the client and server such as a request from a client for an object previously served and stored in the cache. In one embodiment the cache manager intercepts a request from the client to the server . In some embodiments the request is for an object stored in cache. In other embodiments the request is an instruction command or directive to the server that will cause a change to an object stored in cache such as to cause an object to be dynamically generated. In another embodiment the cache manager intercepts a response from a server to the client comprising or identifying an object stored in cache.

In step an object determinant is identified in the intercepted communication. As noted above the object determinant indicates whether a change has occurred or will occur in the requested object at the originating server . However in the embodiment of the object determinant may be associated with a group of objects. This enables efficient invalidation of all objects stored in the cache that may be affected by a particular object determinant. In some embodiments an object determinant of an object in the group is identified. In other embodiments an object determinant for example a group object determinant for the group of objects is identified. In another embodiment a combination of object determinants of one or more objects in the group are identified.

Thus according to step a determination is made as to whether the object determinant indicates a change in the group of previously served objects. In some embodiments the existence of the object determinant of the group in the intercepted communication indicates a change has occurred or will occur to one or more or all of the objects in the group. In other embodiments the name and type of a command directive or instruction in the intercepted communication indicates such changes. In yet another embodiment the existence value or setting of any parameters or variables in the communication may also indicate such changes.

If at step the object determinant indicates a change in the group then the group of previously served objects is marked as invalid in the cache in accordance with step . In some embodiments one or more or all of the objects of the group are requested and retrieved from the originating server in accordance with step . If at step the object determinant does not indicate a change in the group then in some embodiments any objects requested as part of intercepted communication and previously served and stored in the cache is retrieved from the cache manager in accordance with step . In an embodiment any object or group of objects marked as invalid may be flushed by the cache manager from the cache.

The cache administrator may specifically designate which objects get included into a particular group. Whenever an object is stored in the cache the administrator may make that object a member of one of the configured or implicit groups depending on the configuration. The configured groups can be based on configurations that an administrator has previously established or alternatively based on application behavior and other data related to object invalidation. An object may also be part of an implicit group if its configured group is dynamic. Objects in the implicit group are grouped by the value of the significant invalidation parameters.

By permitting very flexible grouping of objects a cache can achieve a level of flexibility and coordination in invalidation that is necessary to effectively cache dynamically generated content. The cache can invalidate a very specific group of objects simultaneously thereby making the cache more responsive to the frequent need to invalidate dynamically generated content. At the time the cache assigns an object to a group the group determines a number of things relative to that object including the invalidation parameters and the hit determinants in order to associate one or more object determinants therewith.

In the customer resource management CRM example the cache administrator may pre designate each of the groupings. For example the administrator configures the cache to group each of the sales departments by name. Thus the administrator can designate an auto department a motorcycle department etc. and each time an object determinant is recognized in a request coming to the cache the cache can then invalidate all objects stored in a designated group linked to an appropriate department via the object determinant.

Alternatively the cache administrator may establish rules that allow the cache appliance to determine on the run which objects to include in a particular group or groups. Such rules based groupings may rely on the designation of groups by virtue of established rules that link the object to significant object determinants that the cache utilizes to create the relevant groups. An example of this approach may involve configuring the cache with rules that the cache uses to recognize what objects to put in each group.

Again turning to the CRM example a rule may state that each subdivision of the Sales Department that is set up on the application should be recognized by the cache as its own grouping. In this way the groupings can be created without the cache administrator having to specifically identify each grouping but allows the cache to determine based on the relevant rules. This technique creates a more flexible and often less work intensive way to designate groupings. The cache administrator could configure a rule that states that every subdivision department of Sales i.e. sales auto sales motorcycle etc. should generated a new grouping by the cache. As a request from the Auto Sales Department is processed and returned by the application via the cache the cache can recognize each subgrouping of sales and automatically create a grouping for it based on the pre configured rule.

The rule may be implemented by the cache each time it sees a new request for an object of the type report sales auto or report sales motorcycle etc. This process can then be repeated when a Motorcycle Sales Department request showing that it is a sub grouping of the Sales Department then the Bicycle Sales Department and so forth as the cache recognizes these subgroups and establishes an object grouping for each of them. When a known invalidation request comes to the cache linked to one of these groupings or if a relevant object determinant is identified in a client request for example a post of a sales report to the Motorcycle Sales Department sales motorcycle found in the parsing the request the cache knows to invalidate all the cached objects in the Motorcycle Sales Department Grouping.

In this way when a cache recognizes that a change has occurred or will occur to data served by the application either because the cache recognizes that contents of a request received by the cache will trigger a change at the application or because of the occurrence of some outside change the above technique enables the cache to quickly and simply identify which objects require invalidation through the process of grouping. In this way the cache is able to invalidate large numbers of dynamically generated objects that are no longer fresh because of changes in the application or database state.

The ability of the cache to successfully store and serve out of its cache memory dynamically generated content can also be enhanced with an intelligent statistical engine that examines the pattern of request and response traffic in order to determine over a period of time the set of objects that would provide the most caching benefit. The engine can either be integrated into the cache appliance itself or run in a separate computer as a heuristic to select some subset of objects for further investigation to determine suitability for dynamic caching.

As described above object determinants may be any data structure that indicates whether a change has occurred or will occur in the originating server to the group of previously served objects stored in the cache with which the object determinant is associated. Object determinants could be set up on the basis of predefined string values embedded in the request. For example when a request comes in with a certain USERID the USERID can be linked to a group of objects in the cache memory that should be invalidated each time a post or other request comes from that certain USERID. Potential candidates for object determinants could also include using service identifiers of the server that originally served the object. The service identifier contains service IP address TCP port and service identifier present in the HTTP request.

Another potential object determinant present in the request the request uniform resource locator URL . In the case of caching of static objects the request URL is typically sufficient to uniquely identify the object. For requests for dynamically generated content however the information present in the URL may not be sufficient to identify the cached object. The cache must therefore inspect other information in the request to find object determinants including in HTTP headers cookie header or in other custom HTTP headers. The cache can additionally look for a subset of relevant parameter information in a variety of other places in the client request including without limitation in the URL query string in the POST body in a cookie header or in any other request or response headers.

The problem in parsing a URL for object determinants is that the URL and other headers may contain a lot of information in addition to what is relevant for the cache s decision. The cache must therefore be able to parse through quite a lot of information to be able to identify the appropriate object determinants. In addition the data in the header is often arbitrarily ordered meaning there are no standardized ways that such data is placed into the HTTP header and therefore a simple comparison is often insufficient to locate the relevant object determinants in such string.

If there is no pre configured policy to match a particular object determinant to a relevant object or group of objects stored in cache memory the cache may still in another embodiment make such a determination. For example the cache may examine and parse various aspects of the request to discover whether any other object determinants may be found in such request and used to link such request to particular objects stored in the cache memory that should be invalidated. Alternatively one could also enable the cache to examine a request for certain object determinants that the cache determines based on certain pre defined heuristics may meaningfully linked to particular objects or group of objects. For example when the request comes into the cache for an update of a calendar associated with a particular USERID an embodiment of the present invention could be set up to recognize that all cached objects with USERID equal to the USERID of the request updating the calendar and that contains the user s calendar for any one particular day will need to be invalidated.

The cache may also assume that the object determinants are present as a group of name value or similar pairs in a non specified order in the URL Stem in the queries present in the URL in the POST body or in a Cookie header. In an embodiment it is assumed that the query is formatted as a list of name value pairs. The user can therefore configure which parameter names are significant. Every cached object is keyed using first its access URL. The URL may look like site application special file.ext p1 v1 p2 v2 p3 v3. The site application special file.ext part is the URL stem. The p1 v1 p2 v2 p3 v3 part is the URL query and contains parameter value pairs. These parameter value pairs may also be present in the POST body or in the Cookie headers.

In an embodiment the user or administrator establishes that p1 and p2 shall be the invalidation parameters or object determinants. The cache will thereafter automatically group objects that have matching p1 and p2 values. One way of implementing this grouping is to map p1 and p2 to primary keys in database tables i.e. to uniquely identifiable objects in the table that the cache will know how to reference in order to determine validation status. To update something in those database tables in order to reflect the fact that data stored in the cache is no longer valid the cache will specify new values for p1 and p2 and when the cache recognizes such new values the next time it goes to serve such content it will know to invalidate the linked objects stored in its memory. The cache when it encounters such a request on seeing the update request knows that it has to invalidate the group with matching p1 and p2 values because the cache understands that data in the origin will change thereby affecting all objects that are related to those p1 and p2 object determinants.

To address the more complex case where the administrator has not pre configured specific parameters embedded in the request as object determinants the cache can deploy user configured policies to extract the relevant object determinants from the request to assist in identifying when to invalidate groupings of objects. The determinant string is then used to locate the group of objects stored in the cache and invalidate such objects. These object determinants can be used to configure the cache to generate lists of significant parameter values. If an incoming write request has matching values for the significant parameters then the objects tied to those parameter names should be invalidated. Alternatively a user could specify the policy framework action that can extract the object determinant string from the request. The object determinant string is extracted from the write request and all objects with matching determinant strings are invalidated. In this alternative approach a request arrives at the cache the cache makes a determination whether the request string matches an invalidation policy. The invalidation policy specifies objects in which content group should be invalidated.

Alternatively the cache could use any other user information that may be present in the client request. As noted above the authentication and authorization integration allows the cache access to the user information. The USERID or the GROUPID could be one of the determinants in the event the relevant grouping of cached objects are linked to a user or a group of users. Although user information is often an important object determinant the user information often may not be present in the HTTP request. In a further embodiment of the present invention the dynamic caching aspects of the invention can be combined with another of Applicant s patent applications. To accomplish this reference is made to Applicant s above referenced copending patent application Ser. No. 11 169 002 the Integrated Caching patent . That application describes a system and method for integrating the cache with a variety of other networking elements including the ability to perform certain kinds of authentication access control and audit AAA infrastructure. Thus the level of security accorded to data that is generated by the applications is applied to data that is instead served from a cache. This technique allows the applications to cache sensitive access controlled information that could not otherwise be cached.

This approach allows the cache to identify users that do not include identifiable user information in the HTTP request but that may be identifiable via the AAA approach described in the Integrated Caching patent. Such an approach enables the cache to identify the relevant user to a particular request through examining the authorization state information that can be shared from the AAA processing. In a further embodiment the integration enables the application of security policies to information stored in the cache to prevent unauthorized users from accessing information stored at the cache.

This approach also address the challenge posed by the fact that a significant portion of dynamically generated data requires that the client requesting such data be authorized and authenticated before the cache can respond to the relevant request from the client. The cache must have the ability to authorize requests made by authenticated users so that applications can cache access controlled objects and by integrating such dynamic caching technology with authentication and authorization information this security can be achieved. The USERID or the GROUPID will be one of the object determinants if the objects are personalized to a user or a group of users. Thus the level of security accorded to data that is generated by the applications is applied to cached information as well. This technique allows the applications to cache sensitive access controlled information that could not otherwise be cached.

Finally other information like time of day state of the database at the origin etc. may be parsed from the request and used as object determinants to determine whether objects stored in the cache are still valid. The cache may take care of this situation by configuring appropriate expiration behavior in groupings of objects that are configured to be sensitive to such external variables.

To further address the challenge presented by the fact that requests for dynamic content must be parsed and interpreted by the cache the cache in accordance with an embodiment of the present invention can limit which parameters are deemed to be relevant object determinants for the cache. In this way the success rate for serving objects from the cache rather than forwarding such requests to the applicable application server can be enhanced. By way of example a request query from a client may contain both a city and a state parameter. However the cache may be configured to comply with the requirements of the application for which the cache is storing content to recognize that the response can be served to requests coming from clients that the query shows come from all clients in a given state without regard to the city value. For this purpose the city parameter is not relevant and the cache could recognize this fact. An alternate embodiment involves configuring the cache so that a response can be served from the cache if just the city parameter makes a match regardless of what is specified for the state parameter.

In summary the cache implements generalized parameterized object matching. In this approach the cache is configured to recognize the subset of information in the request that will be useful as object determinants and that are linked to a particular object so that when such object determinants are recognized the cache can utilize the presence or conversely the absence of such determinants in evaluating whether the object or group of objects remains fresh and capable of being served from the cache. The cache maintains a table that it consults each time a request comes in to check against the configured parameters to determine if the requested data remains fresh and which also allows the cache to match the relevant data to the proper object stored in the cache memory.

In yet another embodiment the cache can utilize incarnation numbers to invalidate a group of objects. Where a cache needs to change the state of each of a group of objects at one time because of a change in the state at the origin incarnation numbers provides a simple technique for effecting this invalidation. Whereas identifying each object and changing the state individually is an inefficient approach to assuring freshness of data stored in a cache use of incarnation numbers enables a much more simple and effective approach to invalidating groups of objects. The present embodiment describes how each object points to a data structure that represents the group and therefore the server need only send a command that changes the state in the data structure for the group. When a subsequent request for a cached object arrives from a client the cache must first figure out whether the state has changed. To do so it looks up the data structure to reference whether the state has changed for the group.

In order to implement the data structure effectively the cache must be able to determine whether to look up for a state change. Therefore the cache must be able to determine whether it has already looked at the state change in the group or not. This is where the incarnation numbers are helpful. The cache associates dynamically generated objects into content groups. Each of these content groups may be represented through a hash table look up process with a particular index value or incarnation number contained in a data structure. Thereafter whenever the cache receives a client request that the cache recognizes as causing a state change the client parses the client request for the relevant parameters performs the hash look up based on the recognized object determinants and increments the index or incarnation number in the data structure. Each time an object stored within a designated grouping is requested by a client the cache performs the hash algorithm on the object and compares it to the original stored value in the data structure for such content group. If the stored value is the same as the number calculated by the cache for such object then the cache knows the content remains fresh and can be served to the requestor. In the event the cache detects a discrepancy between the current incarnation number calculated for such object in and the number stored for such content group in the data structure the cache knows that the stored object is no longer fresh. The cache then invalidates the stored object and sends the request along to the application server. When the response comes back the cache appliance will store the new response in the cache memory and link such response again to the new data structure. Thereafter each time the cache receives a request for an object in that grouping the cache can make the comparison and assuming no further changes have been made to the data structure the cache can serve the newly stored object.

By utilizing invalidation of a group of objects in this fashion the cache is able to invalidate very quickly and the time taken is constant regardless of the number of objects invalidated. Through this faster and more efficient process of invalidation the techniques of the present invention enables the cache to more effectively handle dynamically generated objects. The approach allows cache appliances that sit in front of applications to more aggressively store and serve dynamically generated objects without serving invalid or stale content because of rapid changes in such data. The embodiment enables the cache to serve data that frequently or unpredictably changes thereby improving the performance of the cache. The cache is also able to invalidate objects and group of objects stored in the cache memory using user commands and also by examining and grouping various kinds of web traffic.

Another embodiment of the present invention may also include a technique that is able to increase cache hit rates for extremely fast changing dynamically generated objects that would not otherwise be cacheable. When a cache manager receives a first request from a client for a particular object the cache manager forwards that request to the originating server for processing because as the first such request the cache manager cannot yet have such object stored. As the response object is generated and then returned to the requesting client via the cache manager the cache manager automatically stores a copy of the object. With objects that change extremely quickly traditional caches in contrast to the present invention just pass all responses along to the originating server for processing as such content is always assumed to be not fresh and therefore not valid for cache storage.

In one aspect the present invention is directed towards a flash cache technique for handling additional requests for an object received by the cache manager during the time the cache manager or appliance is in the processing of transmitting or waiting to transmit a response for a first requestor of the object. Referring now to method of the present invention depicts the flash cache technique of the present invention. In brief overview and in view of at step the cache manager of the present invention receives a response from an originating server for an object requested by a first client for example client illustrated in . The object may comprise a dynamically generated object which is generated by one of the originating servers and the response from the originating server may include the dynamically generated object. At step the cache manager requests transmission of the response to the first client such as via a network stack for example a TCP IP stack of the device . At step the response may be stored or held in a buffer while waiting to be transmitted. For example the device may throttle communications to a slow connected or low bandwidth client and therefore the device queues network packets which represent the response.

At step while the response of the first client is waiting to be transmitted in the buffer or otherwise in the process of being transmitted and or prior to completing transmission of the response to the first client the cache manager receives a second request for the object from a second client B. At step the cache manager determines the object is currently in the buffer and provides the object from the buffer to response to the second request to the second client B. In one embodiment as the device uses a single TCP IP stack the same object can be provided for transmission to the first client and the second client B. At step the first response is transmitted to the first client and a response is transmitted to the second client B. At step the response of the first client B is removed from the buffer.

In further detail at step the device such as by the cache manager intercepts or otherwise receives a response from an originating server to a request from a first client for an object such as dynamically generated object. In one embodiment the dynamically generated object is identified as non cacheable such as by the originating server or is otherwise not identified as cacheable. In some embodiments the cache manager receives the request from the first client and forwards the request to the originating server . In one embodiment the cache manager checks the cache manager for the requested object prior to forwarding the request to the originating server . In some cases the cache manager determines the object is invalid or has expired. In other cases the cache manager determines the object is not stored or otherwise available in the cache manager . For example the object may have been flushed for example or this may be the first time the object has been requested from an originating server . In some embodiments the response comprises the object and in other embodiments the response indicates a status such as a failure or error message regarding the object.

At step the response for the request by the first client is provided for transmission to the first client . In some embodiments the device receives the response and requests the packet processing engine to transmit the response to the client . In other embodiments the cache manager receives the response and requests the packet processing engine to transmit the response to the client . In one embodiment the device comprises one network stack for receiving and transmitting network packets to the clients . In some embodiments the network stacks comprises a TCP IP stack. In other embodiments the device may include multiple network stacks each associated with or used by one or more clients . The network stacks or stacks are associated with the kernel network ports and or the packet processing engine as those ordinarily skilled in the art will recognize and appreciate.

At step during the course of transmitting the response to the first client requested at step the device queues holds or otherwise stores the response in a buffer. In some embodiments the buffer is a part of the network stack such as a TCP IP stack. For example the buffer may comprise a data structure used in or by the TCP IP stack or by a network driver filter or other network related software processing or handling any portion of the network stack. In other embodiments the buffer is part of the network port . In additional embodiments the buffer is part of the packet processing engine or yet in further embodiments the kernel comprises the buffer. The buffer may be any memory or storage element located in any portion of the device . In many embodiments the response is queued held or stored in the buffer as one or more network packets such as TCP IP protocol based packets. In one embodiment the response is held in one or more data structures providing the buffer for use by the present invention. As those ordinarily skilled in the art will recognize and appreciate the response may be queued held or stored in the buffer in any suitable form.

Furthermore the response may be stored or queued in the buffer for any duration of time period either arbitrary predetermined implicit explicit or otherwise and using any suitable means and or mechanisms. In one embodiment the response is stored in the buffer as the network packet awaits to be transmitted by the device based on a rate of transmission packet queuing rate network traffic and congestion or any other network characteristics and as may be determined by the device such as via the packet processing engine and or the network ports . In another embodiment the response is held in a buffer for a determined time period. In some embodiments the device queues manages and transmits network packets to a client based on characteristics such as bandwidth type of connection etc. of the client s network connection s or characteristics of the client . For example for a client with a slower connection to the device than the device s connection to an originating server the device may throttle or otherwise manage the transmission of the network packets to the client to provide a desired performance of or behavior for the client or the network connection of the client . As such the device may queue or hold the response in a buffer for a desired time period in accordance with any queuing or buffer management logic function rule or operation.

At step while the response of the first client is held in the buffer or is being transmitted or otherwise prior to completing transmission of the response to the first client such as a buffer of a TCP IP stack the device such as via the cache manager intercepts or otherwise receives a second request from a second client B for the object obtained by the cache manager for the first request. That is in one embodiment the object requested by the second client B is stored in the buffer waiting to be transmitted. In some embodiments the device receives multiple requests from multiple clients for the same object requested by the first client and is currently being held in the buffer for transmission to the first client or is other currently being transmitted to the first client . In some embodiments the cache manager determines the second request is requesting the same object as the first request using any suitable means and or mechanisms such as any of the techniques described herein for example by using object determinants. In some embodiments the appliance or the cache manager queues the second request and any additional requests in a queue using any type and form of queuing mechanism. As such in one embodiment the clients do not need to resubmit the requests as the appliance or the cache manager queues and responds to the requests on behalf of the servers . In another embodiment the appliance or cache manager queues and responds the requests transparently to the client without submitting the request to the server . The appliance or cache manager may use any mechanism such as a queue object or data structure in memory to queue the requests from the client to be responded to using the techniques of the present invention.

At step the device determines the object requested by the second client is in the response stored in the buffer or is in the response currently being transmitted to the first client . In some embodiments the cache manager determines the object is stored in the buffer. In some cases the cache manager first checks cache memory or storage for the object and then checks the buffer for the object. In other cases the buffer may be considered a storage location for cached objects to be searched or managed by the cache manager . In other embodiments the cache manager requests the packet processing engine to check if the object is in the buffer. In additional embodiments the cache manager determines if the object is in the buffer via any application programming interface API of the kernel . In some cases the cache manager interfaces to any drivers network handlers filters or other software handling or processing any portion of the network stack to determine if the object is in the buffer.

Further at step the present invention provides the object stored in the buffer or being transmitted as the response to the first client for use in the response to the second request from the second client B at step . In some embodiments the device such as via the packet processing engine forms a response to the second request by using any portion of the buffer and related data structures of the network stack. In one embodiment the object is stored in the buffer once such as in a single network stack configuration and used to form the network packets to be transmitted to the first client and second client B respectively. In other embodiments a copy of the object from the buffer is used to form the response to the second request. In further embodiments the object is provided from a first network stack to a second network stack to form the response to the second request for the second client B. The response of the first client may be maintained in the buffer and any portion or all of the response in the buffer may be used to provide the response for the second client B. In one embodiment the response of the first client stored in the buffer may be used without modification to respond to the second client B. In another embodiment the data representing the object stored in the response of the first client in the buffer is modified to provide the response for the second client B.

At step of method of the present invention the first response to the first client is transmitted from the device and the second response in response to the second request of the second client B is transmitted from the device . The response to the first client and second client B may be transmitted in any order from the device with any time interval between each other using the same network stack or different network stacks. In some embodiments the response for the second client B is also queued held or stored in a buffer waiting to be transmitted. As such any additional requests for the same object may also be provided via the first response or second response stored in the buffer and not yet transmitted flushed removed or otherwise unavailable.

At step the response of the first client is flushed or otherwise removed from the buffer. In some embodiments if the response of the second client B was stored to the buffer the response of the second client is also removed. In other embodiments the response of the second client may remain in the buffer and steps to of method are practiced again using the second client response while receiving a third request for the object from a third client.

In another aspect the present invention is directed towards a flash crowd technique for handling situations where the appliance or cache manager receives additional requests e.g. nearly simultaneous requests for the same object during the time the server is processing and returning the response object for a first requestor. Once all such nearly simultaneous requests are responded to by the cache the object is immediately flushed from the cache memory with no additional expiry time or invalidation action by application or administrator. This technique of the present invention enables data to be cached and served for very small amounts of time for objects that would otherwise be considered non cacheable. This approach yields a significant improvement in applications that serve fast changing data to a large volume of concurrent users such for example as real time stock quotes or a fast evolving news story.

In further details and in view of step of method of the present invention the cache manager of appliance in one embodiment may intercept or otherwise receive a request from a client for example client in for an object such as a dynamically generated object at any point in time during communications between the client and the originating server for example . In one embodiment the request from the first client may be the first time the first client has requested the object. In one example the client may request the object for the first time upon connection to the origination server . In another example the cache manager may have not previously cached or requested the object from the originating server on behalf of the client or any other client . In other embodiments the request from the client may be a subsequent request for a previously requested object. In yet other embodiments the cache manager may have previously cached the object requested by the client at step . As such the techniques of the present invention as illustrated in may be practiced at any point during the course of communications between the clients and the originating servers B regardless if the object is currently cached has been previously cached or has never been cached.

At step the cache manager forwards the request from the client to the originating server . In one embodiment the cache manager forwards the request to the originating server without checking if the object is stored in the cache manager . In some embodiments the cache manager first checks if the object requested by the client is available from the cache manager and if not then forwards the request to the originating server . In other embodiments the cache manager checks if the object is stored in the cache but is either invalid or otherwise needs to be updated or is otherwise about to be invalid or shortly need to be updated. In another embodiment the cache manager determines a dynamically generated object stored in the cache should be dynamically regenerated and served from the originating server N. As such if the cache manager determines the object should be requested from an originating server the cache manager forwards the request from the client to the originating server . In one embodiment the cache manager forwards the same or original request received from the first client to the originating server . In another embodiment the cache manager sends a request formed by the cache manager for the object requested or identified in the request by the first client

While waiting for a response to the request of the first client from the originating server N and or prior to responding to the request of the first client the cache manager at step may intercept or otherwise receive one or more additional requests for the object identified associated with or requested by the first request of the first client at step . For example in one embodiment the cache manager receives a second request from a second client B for the object which may be dynamically generated by the originating server in response to the first request. In one embodiment the appliance or the cache manager queues the second request and any additional requests in a queue using any type and form of queuing mechanism. In some embodiments the second and or additional requests for the object may occur or be received by the cache manager nearly simultaneously. In other embodiments the second and or additional requests may be intercepted or received by the cache manager at any point between receiving the first request and receiving a response to the first request from the originating server . In yet another embodiment the second and or additional requests may be received by the cache manager at any point between receiving the first request and providing a response to the first request to the client

In one embodiment prior to responding to the first request the cache manager does not forward the second request from the second client for the object and or any other additional requests for the objects from any other clients . In one embodiment the cache manager at step queues or otherwise holds the second and additional requests for example at the appliance until receiving the response to the first request. As such the cache manager transparently to the second client B does not forward the second client s request. However in some embodiments it appears to the second client B that the request is being processed for example by an originating server 

In some embodiments the second request from the second client may request the same object as the first request at step but also include an additional request for another object or other information static dynamic or otherwise. For example the request of the second client B may include multiple requests. In these embodiments the cache manager may identify the portion of the request related to the object already requested in connection with the first request of the first client and not process the identified portion until the response to the first request is received. For the other portions of the request from the second client B the cache manager may process these other requests in any suitable manner such as by any techniques of the present invention described herein. For example the second request may include a request for a second object for which the cache manager requests the second object from an originating server B or obtains the second object from the cache manager .

At step the method of the present invention receives a response to the first request forwarded to the originating server at step . In some embodiments the response includes the object such as an object dynamically generated by any of the servers . In other embodiments the response includes a pointer to a location of the object to be fetched or otherwise obtained by the cache manager . In further embodiments the response may indicate some error message or otherwise indicate the object requested cannot be provided. In one embodiment the cache manager compares the received response to the corresponding request to determine if the response is suitable or appropriate for the request. In further embodiments the cache manager obtains contents of the response such as the dynamically generated object and creates modifies or otherwise provides the response desired to be communicated to the first client

At step the cache manager of the present invention uses the response and or object from the response received for the first request to respond to the first request and the second or additional requests for the same object. In one embodiment the cache manager transmits the received response to the first client in response to the first request and transmits the received response to the second client B in response to the second request. In another embodiment the cache manager transmits the received response to the first client but modifies the received response and transmits the modified response to the second client B in response to the second request. For example the cache manager may modify the response to the second client B to include session information or other information corresponding to the second request. In further embodiments the cache manager may obtain the object requested from the received response and create modify or otherwise provide a response corresponding to the first request and the second request and including the object such as a first response and a second response and transmit the corresponding response to the first client and second client B. As such the cache manager may use all or any portion of the response received for the first request from an originating server in responding to the requests from the clients for the object.

In some embodiments the cache manager may intercept or otherwise receive at step additional requests for the object from other clients after receiving the response to the first request from the server and before completing transmission of a response to the first client second client or any other client requesting the object at step . For example the cache manager may receive a third request for the object from a third client C. In these embodiments the cache manager may use all or any portion of the response received for the first request from an originating server and communicated to clients and or at step to respond to these additional requests. In another embodiment the cache manager may have completed transmission of responses to the first client and second client but may still have any of the responses in memory or otherwise available to respond to the third request of the third client with the object provided via the first request. As such at step the cache manager of the present invention can respond to the third request or further additional requests without requesting the object again from the originating server and or without storing the requested object in cache.

In some embodiments the appliance or the cache manager queues the second request third request or any additional requests in a queue. As such in one embodiment the clients do not need to resubmit the requests as the appliance or the cache manager queues and responds to the requests on behalf of the servers . In another embodiment the appliance or cache manager queues and responds the requests transparently to the client without submitting the request to the server . The appliance or cache manager may use any mechanism such as a queue object or data structure in memory to queue the requests from the client to be responded to using the techniques of the present invention. Additionally the appliance or cache manager may respond to the client requests in any order and not necessarily in the order received or in a first in first out manner from the queue.

At step the cache manager flushes the received response to the first request from the originating server any responses to the client s requests and or the object requested by the requests from the cache manager . In one embodiment the cache manager immediately flushes the response and or object from cache upon completing the transmission of the last response such as at step in one embodiment or step in another embodiment. In some embodiments the cache manager maintains the object for a predetermined time period such as in accordance with an expiry time. In other embodiments the cache manager maintains the object in cache until the originating server N invalidates the object. In yet another embodiment the object remains in memory or storage until the memory or storage is written over or otherwise used by the cache manager for other purposes.

Although the techniques of methods and of the present invention are generally described above in regards to a request for a single object from multiple clients those ordinarily skilled in the art will recognize and appreciate that the flash cache and flash crowd handling techniques of the present invention may be used to handle multiple object requests either in a single request or a series of requests from multiple clients that may occur either sequentially concurrently or nearly concurrently or simultaneously. As such multiple instances of practicing methods and may be executing at various frequencies and at various times during the course of operation of the cache manager or appliance in a network environment . Furthermore each request or series of requests from each client may request different sets of objects with a common set of one or more objects requested among one or more clients.

For example in view of the flash crowd techniques of the present invention a first client may request a first object and a second object while a second client may request the second object and a third object and the third client may request the first object and the third object. In some embodiments the cache of the present invention uses multiplexing and or demultiplexing techniques to request an object once for each object of multiple requests outstanding between multiple clients and provide the corresponding response to each client accordingly. Further to the above example the cache may obtain the first object from the originating server once for the first client and the third client the second object once for the first client and second client and the third object once for the second client and third client. In some embodiments when all the objects for one or more requests for a client are received by the cache the cache transmits the corresponding response or responses to the client. In other embodiments the cache may respond to the client with each object upon receipt of the object by the cache.

Likewise in view of the flash cache technique of method of the present invention the buffer of the appliance may comprise multiple objects waiting to be transmitted while additional requests for those objects are received by the appliance. In one embodiment the appliance of the present invention uses a single TCP IP stack from which multiple buffers are associated with and temporarily store multiple objects during the course of transmission. As such a first object in the buffer may be used to respond to requests from a first client and a second client for the first object and a second object in the buffer may be used to respond to requests from a third client and fourth client for the second object or to respond to a request from either or both of the first client or second client for the second object.

Furthermore the flash cache and flash crowd techniques of the present invention may be practiced in conjunction or in any combination with each other as those ordinarily skilled in the art will recognize and appreciate. For example the appliance of the present invention may be providing responses to additional requests for an object from the buffer via the flash cache technique but determines the object in the buffer is either invalid or otherwise needs to be requested from the originating server. At that point the appliance may switch to the flash crowd technique by requesting the object from the originating server. The appliance then uses the object requested from the originating server to respond to additional requests for the object received and queued at the appliance while waiting for the originating server to respond.

In another aspect the present invention is directed towards using techniques to increase cache hit rates by inserting information such as entity tag and cache control information for an object in a response to a client to enable the cache to check for a hit in a subsequent request. The present invention provides a technique for identifying a dynamically generated object as cacheable to a client when the originating server did not identify the object as cacheable. In some embodiments such as an embodiment handling HTTP requests and responses for objects the techniques of the present invention insert an entity tag or etag into the response to provide cache control for objects provided without entity tags and or cache control information from an originating server. In brief reference to the HTTP protocol as known to those ordinarily skilled in the art an etag is the ETag header field of an HTTP based message which provides the current value of the entity tag for a requested variant such as an object and may be used for comparison with other entities from the same resource. The etag provides a mechanism for validation in the HTTP protocol. Also an HTTP based message may include a Cache Control header field to specify directives to caching mechanisms along the request response message or communication chain. The cache control directives specify desired caching behavior.

Referring now to and in view of steps are depicted for an embodiment of practicing the etag techniques of the present invention. In brief overview of method of the present invention step receives a response having an object requested by a client such as object served from an originating server and received by a cache manager of the appliance . At step the cache manager determines if the object from the response is under tag entity control or otherwise has an entity tag. If the object does not have an entity tag then at step the cache manager generates an entity tag for the object. At step the cache manager modifies the response to provide entity control information including the cache generated entity tag. At step the cache manager stores the object with the entity tag information to the cache manager . At step the cache manager responds to the client requesting the object with the modified response. As such the client receives an entity tag controlled object from the cache manager in a manner transparent to the client such that the client is not aware that the originating server did not provide the object with the entity tag information.

In further detail at step of the present invention the cache manager receives a response to a request of a client for the object from an originating server . In some embodiments the response may be received from a request sent from the client to the server or from the cache manager on behalf of the client to the server . In one embodiment the cache manager forwards the request from the client to the server after checking the cache manager for a matching and or valid object. In another embodiment the cache manager requests the object from the server during practicing the flashing crowd technique of the present invention described in conjunction with .

At step of method the cache manager determines if the object received in the response at step is under entity tag control or has an entity tag. In some embodiments the cache manager inspects examines checks or otherwise reads any field header data or other information of the one or more packets providing the response. As those ordinarily skilled in the art will recognize and appreciate the portion of the packet or packets to inspect or read depends on the type of one or more protocols used for the response. In one embodiment the cache manager determines the entity tag portion of the response packet is missing empty or otherwise not provided. In another embodiment the cache manager determines the entity tag portion of the response packet is corrupted invalid or otherwise not useable by the cache manager and or the client B. In some embodiments the cache manager determines the response has an entity tag or is under entity control but not in the type form or manner desired by the cache manager . In addition to or instead of the entity tag the cache manager in a similar manner to the entity tag determines if there exists desired cache control information in the response. In further embodiments the cache manager may determine via the policy engine if the object should be under entity tag and or cache control. As such in some embodiments the cache manager may not continue performing the etag techniques of the present invention such as steps to if the appliance determines via the policy engine the object should not be under entity tag or cache control.

At step the cache manager generates desired entity tag and cache control information for the object of the received response. The etag may comprise any type and or form of entity tag representation and may comprise any numeric alphanumeric letters in one or more characters. In the embodiment of HTTP the etag may comprise a string or quoted string and may further include a prefix for indicating the etag is weak or strong in accordance with the HTTP protocol. In some embodiments the cache manager uses an entity header or etag counter which increments an etag counter sequentially or in any desired increments. In some embodiments the etag counter is global to all objects for which the cache manager generates an etag. In other embodiments the cache manager may have multiple etag counters each associated with or specific to an object or groups of objects.

In one embodiment the etag may comprise a universal identifier UID or global universal identifier GUID associated with the object. In another embodiment the etag may comprise an incarnation number as previously described above. In some embodiments the cache manager obtains the etag from an application program or other form of executable instructions running on the appliance or in other embodiments from another system or device accessible by the appliance via the network. In further embodiments the cache manager may use any suitable algorithm business rule or logic to generate a desired unique etag for the object. In one embodiment the appliance comprises a database file or other organized storage element for generating and maintaining etags for objects for the cache manager .

At step of method the cache manager modifies the response from the originating server to include desired entity tag and or cache control information such as the entity tag generated by the cache at step . In one embodiment the cache manager inserts the entity tag information via an entity tag header of the protocol such as the HTTP protocol. In another embodiment the cache manager modifies any entity tag header fields in the response to include the desired entity tag. In yet another embodiment the cache manager replaces the entity tag fields in the response with the entity tag fields desired to be used by the cache manager . In further embodiments the entity tag information is inserted modified or placed into any portion of the network packet or packets for communicating the response to the client 

Additionally step the cache manager may insert modify or otherwise place any desired cache control information into any header fields of a protocol or any other portion of network packets for communicating the response. The cache control information may be generated by the cache manager by any suitable means and or mechanisms. In one embodiment the cache control information is configured into and obtained via the policy engine of the device . In another embodiment the cache manager determines the desired cache control information from any algorithms business rules or logic of the cache manager . In some embodiments the cache control information may be based on any history of the network and caching performance between the client and the appliance or cache manager .

At step of the method of the present invention the cache manager stores the generated entity tag with the object in any suitable or desired manner in the cache manager . In some embodiments the entity tag is included in or is made a part of the stored object. In other embodiments the entity tag is stored separately but associated with the object. In another embodiment the entity tag and object are associated via a database linked list lookup table or any other mechanism for associating one entity with another entity as known to those skilled in the art. Additionally the cache control information provided with the object and or entity tag in some embodiments may also be stored in association with the object in the cache manager .

At step the cache manager causes the appliance to transmit the modified response to the client . For example the cache manager requests the packet processing engine to transmit the modified response to the client . As such the client receives the object with entity tag and cache control information even though unbeknownst to the client the originating server did not generate such information in one embodiment or the cache manager changed such information generated by the server . The etag technique of the present invention can ensure that every object has a validator i.e. an etag or has an etag controlled or otherwise desired by the cache manager . If an object doesn t have a validator or a desired validator then the cache manager inserts its own etag. These etag validators can avoid serving full responses on repeat requests for an object as will be discussed in connection with .

In further detail at step the cache manager may receive a request from the client to check or validate the entity tag of an object by any type and or form of communication. In one embodiment the request comprises an HTTP message with an If Match or If None Match request header field which includes an entity tag string value provided by the client . For example the client may have a copy of the object in a client cache or otherwise available on the client . The client uses the entity tag of the client s copy of the object in the request to determine or validate if the object is current.

At step the cache manager compares the entity tag value received from the client B with the entity tag for the object to determine if the cache manager has a different version of the object than the client . The cache manager may use any suitable means and or mechanisms to compare the entity tag value from the client with the entity tag value for the cached object. In one embodiment the cache manager performs a byte compare on the etag value from the request with the etag stored with the object in the cache. In another embodiment the cache manager performs and uses hash algorithms on the entity tag and or the cache object to determine if the object has been modified. In further embodiments the cache manager requests the object from the server and compared the received object with the cached object to determine if the object has been modified.

If at step the entity tag from the client does not match the entity tag stored with the object in the cache manager then at step the cache manager sends a response to the client indicating the entity tags do not match or otherwise the object has been modified. In one embodiment the cache manager sends a response to the client providing a new entity tag value for the object. In another embodiment the cache manager sends a response to the client provide a new version of the object or the modified object such as a modified object with a new entity tag stored in the cache manager . If at step the entity tag from the client does match the entity tag stored with the object in the cache manager then at step the cache manager sends a response to the client indicating the entity tags do match or otherwise the object has not been modified.

Although the etag techniques of the present invention has generally been described in reference to the HTTP protocol those ordinarily skilled in the art will recognize and appreciate the present invention may be practiced with any type and or form of protocol to provide entity tag and cache information. In some embodiments the protocol may not support entity tag and cache information fields while in other embodiments the protocol may support entity tag and cache information fields. In any of these embodiments the techniques of the present invention provide entity tag and cache control information when the server does not provide this information in a response or when the cache decides to change or control the entity and cache control information provided by the server in a manner controlled or desired by the appliance .

In order to address the challenge to dynamically caching content dynamically generated by origin servers and applications the cache appliance of the present invention can incorporate pre packaged policies for certain commonly used applications such as Outlook Web Access Oracle Configurator and Serena TeamTrack . These packages are designed with an understanding of the way such applications process data thereby enabling the cache to make more intelligent decisions about which object and how long to keep objects generated by such applications stored in the cache memory.

The functions of the present invention may be implemented using hardware software or a combination thereof and may be implemented in one or more computing devices or other processing systems. For example depict an example computing device that may be utilized to implement any of the techniques methods and functions of the present invention. As such the appliance of the present invention may be a computing device or a specific purpose device designed and constructed to provide any of the techniques functions and operations of the present invention described herein.

The central processing unit is any logic circuitry that responds to and processes instructions fetched from the main memory unit . In many embodiments the central processing unit is provided by a microprocessor unit such as those manufactured by Intel Corporation of Mountain View Calif. those manufactured by Motorola Corporation of Schaumburg Ill. those manufactured by Transmeta Corporation of Santa Clara Calif. those manufactured by International Business Machines of White Plains N.Y. or those manufactured by Advanced Micro Devices of Sunnyvale Calif. The computing device may be based on any of these processors or any other processor capable of operating as described herein.

Main memory unit may be one or more memory chips capable of storing data and allowing any storage location to be directly accessed by the microprocessor such as Static random access memory SRAM Burst SRAM or SynchBurst SRAM BSRAM Dynamic random access memory DRAM Fast Page Mode DRAM FPM DRAM Enhanced DRAM EDRAM Extended Data Output RAM EDO RAM Extended Data Output DRAM EDO DRAM Burst Extended Data Output DRAM BEDO DRAM Enhanced DRAM EDRAM synchronous DRAM SDRAM JEDEC SRAM PC100 SDRAM Double Data Rate SDRAM DDR SDRAM Enhanced SDRAM ESDRAM SyncLink DRAM SLDRAM Direct Rambus DRAM DRDRAM or Ferroelectric RAM FRAM . The main memory may be based on any of the above described memory chips or any other available memory chips capable of operating as described herein. In the embodiment shown in the processor communicates with main memory via a system bus described in more detail below . depicts an embodiment of a computing device in which the processor communicates directly with main memory via a memory port . For example in the main memory may be DRDRAM.

In the embodiment shown in the processor communicates with various I O devices via a local system bus . Various busses may be used to connect the central processing unit to any of the I O devices including a VESA VL bus an ISA bus an EISA bus a MicroChannel Architecture MCA bus a PCI bus a PCI X bus a PCI Express bus or a NuBus. For embodiments in which the I O device is a video display the processor may use an Advanced Graphics Port AGP to communicate with the display . depicts an embodiment of a computer in which the main processor communicates directly with I O device via HyperTransport Rapid I O or InfiniBand. also depicts an embodiment in which local busses and direct communication are mixed the processor communicates with I O device using a local interconnected bus while communicating with I O device directly.

The computing device may support any suitable installation device such as a floppy disk drive for receiving floppy disks such as 3.5 inch 5.25 inch disks or ZIP disks a CD ROM drive a CD R RW drive a DVD ROM drive tape drives of various formats USB device hard drive or any other device suitable for installing software and programs such as any software or portion thereof related to the present invention or otherwise providing any of the techniques of the present invention. The computing device may further comprise a storage device such as one or more hard disk drives or redundant arrays of independent disks for storing an operating system and other related software and for storing application software programs such as any program related to the software of the present invention. Optionally any of the installation devices could also be used as the storage device .

Furthermore the computing device may include a network interface to interface to a Local Area Network LAN Wide Area Network WAN or the Internet through a variety of connections including but not limited to standard telephone lines LAN or WAN links e.g. 802.11 T1 T3 56 kb X.25 broadband connections e.g. ISDN Frame Relay ATM wireless connections or some combination of any or all of the above. The network interface may comprise a built in network adapter network interface card PCMCIA network card card bus network adapter wireless network adapter USB network adapter modem or any other device suitable for interfacing the computing device to any type of network capable of communication and performing the operations described herein.

A wide variety of I O devices may be present in the computing device . Input devices include keyboards mice trackpads trackballs microphones and drawing tablets. Output devices include video displays speakers inkjet printers laser printers and dye sublimation printers. The I O devices may be controlled by an I O controller as shown in . The I O controller may control one or more I O devices such as a keyboard and a pointing device e.g. a mouse or optical pen. Furthermore an I O device may also provide storage and or an installation medium for the computing device . In still other embodiments the computing device may provide USB connections to receive handheld USB storage devices such as the USB Flash Drive line of devices manufactured by Twintech Industry Inc. of Los Alamitos Calif.

In further embodiments an I O device may be a bridge between the system bus and an external communication bus such as a USB bus an Apple Desktop Bus an RS 232 serial connection a SCSI bus a FireWire bus a FireWire bus an Ethernet bus an AppleTalk bus a Gigabit Ethernet bus an Asynchronous Transfer Mode bus a HIPPI bus a Super HIPPI bus a SerialPlus bus a SCl LAMP bus a FibreChannel bus or a Serial Attached small computer system interface bus.

A computing device of the sort depicted in typically operate under the control of operating systems which control scheduling of tasks and access to system resources. The computing device can be running any operating system such as any of the versions of the Microsoft Windows operating systems the different releases of the Unix and Linux operating systems any version of the Mac OS for Macintosh computers any embedded operating system any network operating system any real time operating system any open source operating system any proprietary operating system any operating systems for mobile computing devices or network devices or any other operating system capable of running on the computing device and performing the operations described herein. Typical operating systems include WINDOWS 3.x WINDOWS 95 WINDOWS 98 WINDOWS 2000 WINDOWS NT 3.51 WINDOWS NT 4.0 WINDOWS CE and WINDOWS XP all of which are manufactured by Microsoft Corporation of Redmond Wash. MacOS manufactured by Apple Computer of Cupertino Calif. OS 2 manufactured by International Business Machines of Armonk N.Y. and Linux a freely available operating system distributed by Caldera Corp. of Salt Lake City Utah or any type and or form of a Unix operating system among others.

In other embodiments the computing device may have different processors operating systems and input devices consistent with the device. The computing device can be any workstation desktop computer laptop or notebook computer server handheld computer mobile telephone any other computer or other form of computing or telecommunications device that is capable of communication and that has sufficient processor power and memory capacity to perform the operations of the present invention described herein. Moreover the computing device can be any type and form of network device such as a remote access device a Virtual Private Network VPN device a Secure Socket Layer SSL VPN device router switch bridge or other network device in any form capable of performing the operations of the present invention described herein

Although the appliance of the present invention is generally discussing using or communicating via TCP IP the appliance may use or communicate with any modified transport control protocol such as Transaction TCP T TCP TCP with selection acknowledgements TCP SACK TCP with large windows TCP LW congestion prediction such as in the TCP Vegas protocol and TCP spoofing. Additionally the appliance of the present invention may use any internode or high performance protocol. Moreover the appliance of the present invention may use or operate with any other transport and network protocols such as Sequenced packet exchange SPX protocol over the Internetwork Packet Exchange IPX protocol.

While various embodiments of the present invention have been described above it should be understood that they have been presented by way of example only and not limitation. Thus it will be understood by those skilled in the relevant art s that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined in the appended claims. Accordingly the breadth and scope of the present invention should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

