---

title: Low power voice trigger for finding mobile devices
abstract: Systems and methods may provide for monitoring an input audio signal from an onboard microphone of a mobile device while a host processor of the mobile device is in a standby mode. Additionally, a predetermined audio pattern may be identified in the input audio signal and a device location session may be triggered with respect to the mobile device based on the predetermined audio pattern. In one example, an output audio signal is generated during the device location session.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09582983&OS=09582983&RS=09582983
owner: Intel Corporation
number: 09582983
owner_city: Santa Clara
owner_country: US
publication_date: 20130911
---
Embodiments generally relate to the use of low power voice triggers to operate computing devices. More particularly embodiments relate to the use of low power voice triggers to find mobile devices and automatically adapt the routing of computing device audio streams in the presence of wireless audio accessories.

Misplacing a mobile device such as a wireless smart phone or tablet may be a common occurrence particularly given the increased popularity of these devices in modern society. In order to locate a misplaced mobile device conventional solutions may involve the use of another device such as a separate phone or computer to call and or text the misplaced mobile device a radio frequency identifier RFID based device to detect an RFID tag installed on the misplaced mobile device and so forth. The use of another device may be inconvenient and or impractical depending upon the circumstances. Other solutions may involve installing a location based application on the mobile device in advance of the device being misplaced. Such an approach may require the misplaced mobile device to be in an active mode in order for the application to function. In most cases however the mobile device enters a standby state once it has been misplaced for a certain amount of time rendering the location based application inoperable.

Turning now to a scenario is shown in which a mobile device such as for example a wireless smart phone tablet personal digital assistant PDA mobile Internet device MID media player etc. is misplaced by a user of the mobile device . In the illustrated example the user speaks a predetermined phrase and or audio pattern such as for example the phrase Hello phone the phrase Help me locate you a whistle etc. in order to initiate a device location session with respect to the mobile device . Other audio patterns such as for example a door closing may also be used to trigger the device location session.

The device location session may involve the mobile device generating an output audio signal that is audible to the user even though the mobile device is occluded by other objects e.g. pillows located in another room behind furniture and so forth. As will be discussed in greater detail the output audio signal may include a tone pre recorded message speech dialog prompt etc. wherein the user may follow the sound of the output audio signal to the mobile device speak additional phrases in order to audibly interact with the mobile device or any combination thereof. In addition to the audio output signal the mobile device may conduct other activities such as for example vibrating generating one or more light effects and other programmable feedback to assist the user during the device location session.

The mobile device may have a host processor that is in a standby mode e.g. host processor is powered off and the operating system OS is not running . In such a case the mobile device may use a low power audio processor to monitor the surroundings for the predetermined audio pattern while the host processor is in the standby mode. Moreover the low power audio processor may either generate the output audio signal during the device location session while the host processor remains in the standby mode or initiate an activation of the host processor and use software e.g. speech dialog application voice trigger service driver etc. running on the host processor to generate the output audio signal during the device location session. In one example the standby mode may be a low power state such as the S0i2 state of the Advanced Configuration and Power Interface e.g. ACPI Specification Rev. 5.0a Dec. 6 2011 standard although other standby modes may be used.

Turning now to a method of locating mobile devices is shown. The method may be implemented as a set of logic instructions stored in a machine or computer readable storage medium such as random access memory RAM read only memory ROM programmable ROM PROM firmware flash memory etc. in configurable logic such as for example programmable logic arrays PLAs field programmable gate arrays FPGAs complex programmable logic devices CPLDs in fixed functionality hardware logic using circuit technology such as for example application specific integrated circuit ASIC complementary metal oxide semiconductor CMOS or transistor transistor logic TTL technology or any combination thereof. For example computer program code to carry out operations shown in method may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages.

Illustrated processing block provides for monitoring an input audio signal from an onboard microphone of a mobile device while a host processor of the mobile device is in a standby mode. Monitoring the input audio signal may involve implementing a low power solution that minimizes the potential impact on battery life. For example a low power processor e.g. digital signal processor DSP operating at a relatively low frequency might sample the input audio signal on an intermittent basis and reduce the power consumption of one or more audio front end components in between samples in order to reduce power consumption. A predetermined audio pattern may be identified in the input audio signal at block . The predetermined audio pattern might include a key phrase such as for example Hello phone and or a command such as for example. Help me locate you .

Block may trigger a device location session with respect to the mobile device based on the predetermined audio pattern. For example if the predetermined audio pattern is a command such as Help me locate you the device location session might involve generating an output audio signal e.g. tone beacon that may be audibly followed by the originator source e.g. user of the predetermined audio pattern in order to locate the mobile device. Such an approach may be conducted without activating the host processor or OS. In this regard a low power audio processor of the mobile device might be configured to recognize a relatively small number of predetermined audio patterns e.g. five without negatively impacting power consumption or battery life. As will be discussed in greater detail a verification of the source of the predetermined audio pattern may also be conducted prior to generating the output audio signal in order to ensure that only authorized individuals may trigger device location sessions. In such a case the voice of the user may be recorded and analyzed in advance and compared to the voice of the originator of the predetermined audio pattern at the time of triggering the device location session.

The predetermined audio pattern may also be a key phrase such as Hello phone that enables the low power audio processor to determine that a device location session is being requested with respect to the particular mobile device in question e.g. and not another device . In such a case the low power audio processor might be configured to recognize only a single predetermined audio pattern and achieve even lower power consumption and longer battery life. In response to identifying the key phrase the low power audio processor may initiate an activation of the host processor e.g. via inter processor communication IPC interrupt etc. and use software running on the host processor to generate the output audio signal during the device location session.

Triggering the device location session at block may also be contingent upon context data from one or more sensors e.g. ambient light sensors accelerometers and so forth of the mobile device. For example block might be conducted only if the context data indicates that the mobile device has been unused and or stationary for a predetermined period of time. Otherwise it may be inferred that the predetermined audio pattern is a false positive and a device location session is not being requested.

For example the host processor may include a low power audio driver that receives an IPC from the trigger module once the host processor has been taken out of the standby mode. On receiving the IPC the low power audio driver may send a notification e.g. voice trigger VT event to a speech dialog application . The speech dialog application may in turn open an audio capture pipeline via the audio module using an OS audio application programming interface API . The speech dialog application may also start a speech interaction with a user such as the user via an audio stream . The audio stream may include one or more speech commands and or responses that are transferred between the speech dialog application and the user. The output audio signal containing the responses may be made audible to the user via an onboard speaker e.g. hands free loudspeaker HFL embedded earpiece etc. . As will be discussed in greater detail the output audio signal may be routed to the onboard speaker even if a wireless audio accessory such as a Bluetooth e.g. Institute of Electrical and Electronics Engineers IEEE 802.15.1 2005 Wireless Personal Area Networks headset is connected to the mobile device.

The determination of whether to trigger the device location session may also take into consideration context data from one or more sensors e.g. ambient light sensors accelerometers and so forth on the mobile device as already discussed. In such a case the device location session may be triggered in response to detecting a predetermined audio pattern only if the context data indicates that the mobile device has been either unused or stationary for a predetermined period of time.

A challenge may arise when a user tries to use predetermined audio pattern detection to trigger speech interaction with a computing device but a wireless audio accessory such as a Bluetooth headset is still connected to the computing device. To illustrate the challenge imagine that a user leaves work is driving in his or her car and has a Bluetooth headset connected to a wireless phone. The user might arrive at home place the phone on the kitchen counter or living room table and place the Bluetooth headset at some other location still in range of the phone without switching the headset off. If the user now speaks the predetermined audio pattern e.g. Hello phone and a speech dialog application on the phone begins responding back to the user over the Bluetooth headset as per convention the user may not know why he or she is not hearing the response. Moreover there may be no visual indication to the user that the phone is connected to the Bluetooth headset since user is located relatively far from the phone and cannot see the display screen of the phone. Such an occurrence may have a negative impact on user experience.

As will be discussed in greater detail the onboard microphone may be kept on and listening for predetermined audio patterns even though a wireless audio accessory is connected to the computing device. Note that the user may also trigger speech interaction by pressing a button on the wireless audio accessory in which case the audio capture may occur via the microphone of the wireless audio accessory and the audio response of the speech dialog application may occur through the speakers of the wireless audio accessory. Simply put since the initial trigger originated from the wireless audio accessory in such a scenario it may be a reasonable assumption that user wishes to use the wireless audio accessory.

If on the other hand the trigger occurs due to the detection of a predetermined audio pattern via an onboard microphone the assumption may be that user wishes not to use the wireless audio accessory for speech interaction. Accordingly the computing device may use the onboard microphone and onboard speakers for audio capture and audio responses respectively. During this time if an incoming phone call is received or there is a system alert the onboard microphone and onboard speakers may be used for these applications as well. As will be discussed in greater detail a voice trigger activity flag may be used to facilitate the audio routing determination.

Illustrated processing block provides for receiving a request to open an audio stream. The request may be associated with the placement of an outgoing call the receipt of an incoming call the playing of media content the initiation of a device location session and so forth. A determination may be made at block as to whether a wireless audio accessory such as for example a Bluetooth headset is connected to the mobile device. If so illustrated block determines whether a voice trigger VT activity flag is true. The flag may indicate whether the request at block resulted from a predetermined audio pattern being detected via an onboard microphone of the computing device. If not it may be inferred that the request resulted from the user pressing a button on the wireless audio accessory and block may route input and output audio over the wireless audio accessory.

If either no wireless audio accessory is connected or the VT activity flag is true e.g. the audio stream request resulted from a predetermined audio pattern being detected audio may generally be routed to either an onboard speaker or a wired speaker of the computing device. In the illustrated example block determines whether a wired headset is connected to the computing device. If so audio may be routed over the wired headset at block .

In this regard if a wired headset is connected to a phone then instead of using the onboard microphone and onboard speakers the wired headset speakers and wired headset microphone e.g. if present to capture audio in conjunction with low power voice triggering may be used. There may be enough visual indication from a distance that the headset is connected to the phone and so the user may expect that audio is directed to the headset. To elaborate when a user speaks a predetermined audio pattern with a wired headset connected and does not hear the response back the user may likely look towards the phone and see that it is connected to the wired headset. A speech command may also be added so that user can utter a phrase such as Use loudspeaker and the speech dialog application will then use the hands free loudspeaker instead of the wired headset speakers. Thus the illustrated solution may make it easier for the user to use voice triggers and speech interaction without having to touch the device or look at the screen.

If no wired headset is connected block may determine whether proximity is detected between the user and the computing device e.g. a device sensor determines whether the user s face is nearby . If so audio may be routed over the onboard microphone and earpiece of the computing device at block . If proximity is not detected illustrated block routes audio over the onboard microphone and hands free loudspeaker of the computing device. The user may also configure whether voice triggering is to be used when the wireless audio accessory is connected. Such an approach may provide even greater flexibility to configure the above behavior.

A low power audio driver may receive the IPC and send a voice trigger VT notification to a VT service as well as an operating system OS audio stack e.g. audio hardware abstraction layer HAL . An audio route manager in the OS audio stack may receive the VT notification and set a VT activity flag to true to indicate that the trigger occurred via the onboard microphone e.g. a voice trigger has initiated a request to open an audio stream on the computing device rather than a Bluetooth interface .

When the VT service issues a corresponding VT event to a speech dialog application the application may open an audio stream via the OS audio stack to communicate with the user. The illustrated audio route manager checks the VT activity flag in order to determine whether the activation of the host processor was based on a predetermined audio pattern. If the VT activity flag is true then the audio route manager may generate routing instructions that cause an audio switch matrix to route the outgoing portion of the audio stream from the speech dialog application to the onboard speaker e.g. HFL earpiece and route the incoming portion of the audio stream from the onboard microphone to the speech dialog application . The audio route manager may also query a proximity sensor not shown to determine if the HFL or earpiece should be activated. Such a routing solution may be provided even though a Bluetooth connection state provided by a Bluetooth software SW stack has caused a Bluetooth connection flag to indicate that a Bluetooth headset is connected to the Bluetooth interface and a Bluetooth driver

The illustrated VT activity flag stays true until there is an ongoing audio connection e.g. voice and or video call via for example a modem or the user changes the routing via a settings application . In such a case one or more VT settings may be sent to the VT module and a new routing policy may be sent to the audio route manager in order to route the audio stream through the Bluetooth interface . In one example the Bluetooth interface an audio codec and the modem are coupled to the computing device architecture via synchronous serial ports SSPs which may be accessible by the low power processor although other approaches may be used.

The processor is shown including execution logic having a set of execution units through N. Some embodiments may include a number of execution units dedicated to specific functions or sets of functions. Other embodiments may include only one execution unit or one execution unit that can perform a particular function. The illustrated execution logic performs the operations specified by code instructions.

After completion of execution of the operations specified by the code instructions back end logic retires the instructions of the code . In one embodiment the processor allows out of order execution but requires in order retirement of instructions. Retirement logic may take a variety of forms as known to those of skill in the art e.g. re order buffers or the like . In this manner the processor core is transformed during execution of the code at least in terms of the output generated by the decoder the hardware registers and tables utilized by the register renaming logic and any registers not shown modified by the execution logic .

Although not illustrated in a processing element may include other elements on chip with the processor core . For example a processing element may include memory control logic along with the processor core . The processing element may include I O control logic and or may include I O control logic integrated with memory control logic. The processing element may also include one or more caches.

Referring now to shown is a block diagram of a system embodiment in accordance with an embodiment. Shown in is a multiprocessor system that includes a first processing element and a second processing element . While two processing elements and are shown it is to be understood that an embodiment of the system may also include only one such processing element.

The system is illustrated as a point to point interconnect system wherein the first processing element and the second processing element are coupled via a point to point interconnect . It should be understood that any or all of the interconnects illustrated in may be implemented as a multi drop bus rather than point to point interconnect.

As shown in each of processing elements and may be multicore processors including first and second processor cores i.e. processor cores and and processor cores and . Such cores may be configured to execute instruction code in a manner similar to that discussed above in connection with .

Each processing element may include at least one shared cache . The shared cache may store data e.g. instructions that are utilized by one or more components of the processor such as the cores and respectively. For example the shared cache may locally cache data stored in a memory for faster access by components of the processor. In one or more embodiments the shared cache may include one or more mid level caches such as level 2 L2 level 3 L3 level 4 L4 or other levels of cache a last level cache LLC and or combinations thereof.

While shown with only two processing elements it is to be understood that the scope of the embodiments are not so limited. In other embodiments one or more additional processing elements may be present in a given processor. Alternatively one or more of processing elements may be an element other than a processor such as an accelerator or a field programmable gate array. For example additional processing element s may include additional processors s that are the same as a first processor additional processor s that are heterogeneous or asymmetric to processor a first processor accelerators such as e.g. graphics accelerators or digital signal processing DSP units field programmable gate arrays or any other processing element. There can be a variety of differences between the processing elements in terms of a spectrum of metrics of merit including architectural micro architectural thermal power consumption characteristics and the like. These differences may effectively manifest themselves as asymmetry and heterogeneity amongst the processing elements . For at least one embodiment the various processing elements may reside in the same die package.

The first processing element may further include memory controller logic MC and point to point P P interfaces and . Similarly the second processing element may include a MC and P P interfaces and . As shown in MC s and couple the processors to respective memories namely a memory and a memory which may be portions of main memory locally attached to the respective processors. While the MC and is illustrated as integrated into the processing elements for alternative embodiments the MC logic may be discrete logic outside the processing elements rather than integrated therein.

The first processing element and the second processing element may be coupled to an I O subsystem via P P interconnects respectively. As shown in the I O subsystem includes P P interfaces and . Furthermore I O subsystem includes an interface to couple I O subsystem with a high performance graphics engine . In one embodiment bus may be used to couple the graphics engine to the I O subsystem . Alternately a point to point interconnect may couple these components.

In turn I O subsystem may be coupled to a first bus via an interface . In one embodiment the first bus may be a Peripheral Component Interconnect PCI bus or a bus such as a PCI Express bus or another third generation I O interconnect bus although the scope of the embodiments are not so limited.

As shown in various I O devices e.g. cameras may be coupled to the first bus along with a bus bridge which may couple the first bus to a second bus . In one embodiment the second bus may be a low pin count LPC bus. Various devices may be coupled to the second bus including for example a keyboard mouse network controllers communication device s which may in turn be in communication with a computer network and a data storage unit such as a disk drive or other mass storage device which may include code in one embodiment. The code may include instructions for performing embodiments of one or more of the methods described above. Thus the illustrated code may implement the method and or the method already discussed and may be similar to the code already discussed. Further an audio I O may be coupled to second bus .

Note that other embodiments are contemplated. For example instead of the point to point architecture of a system may implement a multi drop bus or another such communication topology. Also the elements of may alternatively be partitioned using more or fewer integrated chips than shown in .

Example 1 may include an apparatus to locate a wireless device comprising a monitor module to monitor an input audio signal from an onboard microphone of the mobile device while a host processor of the mobile device is in a standby mode a language module to identify a predetermined audio pattern in the input audio signal and a trigger module to trigger a device location session with respect to the mobile device based on the predetermined audio pattern.

Example 2 may include the apparatus of Example 1 further including an audio module to generate an output audio signal during the device location session.

Example 3 may include the apparatus of Example 2 further including a verification module to verify a source of the predetermined audio pattern prior to generation of the output audio signal.

Example 4 may include the apparatus of Example 1 wherein the trigger module is to initiate an activation of the host processor and use software running on the host processor to generate an output audio signal during the device location session.

Example 5 may include the apparatus of Example 4 further including an audio route manager to determine that a wireless audio accessory is connected to the mobile device and route the output audio signal to an onboard speaker of the mobile device if the activation of the host processor was based on the predetermined audio pattern.

Example 6 may include the apparatus of Example 5 wherein the audio route manager is to check a voice trigger activity flag to determine whether the activation of the host processor was based on the predetermined audio pattern.

Example 7 may include the apparatus of any one of Examples 1 to 6 wherein the trigger module is to trigger the device location session further based on context data from one or more sensors on the mobile device.

Example 8 may include the apparatus of Example 7 wherein the trigger module is to trigger the device location session in response to the predetermined audio pattern if the context data indicates that the mobile device has been either unused or stationary for a predetermined period of time.

Example 9 may include a method of locating a mobile device comprising monitoring an input audio signal from an onboard microphone of the mobile device while a host processor of the mobile device is in a standby mode identifying a predetermined audio pattern in the input audio signal and triggering a device location session with respect to the mobile device based on the predetermined audio pattern.

Example 10 may include the method of Example 9 further including generating an output audio signal during the device location session.

Example 11 may include the method of Example 10 further including verifying a source of the predetermined audio pattern prior to generation of the output audio signal.

Example 12 may include the method of Example 9 further including initiating an activation of the host processor and using software running on the host processor to generate an output audio signal during the device location session.

Example 13 may include the method of Example 12 further including determining that a wireless audio accessory is connected to the mobile device and routing the output audio signal to an onboard speaker of the mobile device if the activation of the host processor was based on the predetermined audio pattern.

Example 14 may include the method of Example 13 further including checking a voice trigger activity flag to determine whether the activation of the host processor was based on the predetermined audio pattern.

Example 15 may include the method of any one of Examples 9 to 14 wherein the device location session is triggered further based on context data from one or more sensors on the mobile device.

Example 16 may include the method of Example 15 wherein the device location session is triggered in response to the predetermined audio pattern if the context data indicates that the mobile device has been either unused or stationary for a predetermined period of time.

Example 17 may include at least one computer readable storage medium comprising a set of instructions which if executed by a mobile device cause the mobile device to monitor an input audio signal from an onboard microphone of the mobile device while a host processor of the mobile device is in a standby mode identify a predetermined audio pattern in the input audio signal and trigger a device location session with respect to the mobile device based on the predetermined audio pattern.

Example 18 may include the at least one computer readable storage medium of Example 17 wherein the instructions if executed cause the mobile device to generate an output audio signal during the device location session.

Example 19 may include the at least one computer readable storage medium of Example 18 wherein the instructions if executed cause the mobile device to verify a source of the predetermined audio pattern prior to generation of the output audio signal.

Example 20 may include the at least one computer readable storage medium of Example 17 wherein the instructions if executed cause the mobile device to initiate an activation of the host processor and use software running on the host processor to generate an output audio signal during the device location session.

Example 21 may include the at least one computer readable storage medium of Example 20 wherein the instructions if executed cause the mobile device to determine that a wireless audio accessory is connected to the mobile device and route the output audio signal to an onboard speaker of the mobile device if the activation of the host processor was based on the predetermined audio pattern.

Example 22 may include the at least one computer readable storage medium of Example 21 wherein the instructions if executed cause the mobile device to check a voice trigger activity flag to determine whether the activation of the host processor was based on the predetermined audio pattern.

Example 23 may include the at least one computer readable storage medium of any one of Examples 17 to 22 wherein the device location session is to be triggered further based on context data from one or more sensors on the mobile device.

Example 24 may include the at least one computer readable storage medium of Example 23 wherein the device location session is to be triggered in response to the predetermined audio pattern if the context data indicates that the mobile device has been either unused or stationary for a predetermined period of time.

Example 25 may include an apparatus to route audio comprising an audio route manager to receive a request to open an audio stream on a mobile device determine that a wireless audio accessory is connected to the mobile device and route the audio stream over one of a wired headset or an onboard speaker of the mobile device if the request to open the audio stream corresponds to a voice trigger containing a predetermined audio pattern.

Example 26 may include the apparatus of Example 25 wherein the audio route manager is to check a voice trigger activity flag to determine whether the request to open the audio stream corresponds to the voice trigger.

Example 27 may include a method of routing audio comprising receiving a request to open an audio stream on a mobile device determining that a wireless audio accessory is connected to the mobile device and routing the audio stream over one of a wired headset or an onboard speaker of the mobile device if the request to open the audio stream corresponds to a voice trigger containing a predetermined audio pattern.

Example 28 may include the method of Example 27 further including checking a voice trigger activity flag to determine whether the request to open the audio stream corresponds to the voice trigger.

Example 29 may include at least one computer readable storage medium comprising a set of instructions which if executed by a mobile device cause the mobile device to receive a request to open an audio stream on a mobile device determine that a wireless audio accessory is connected to the mobile device and route the audio stream over one of a wired headset or an onboard speaker of the mobile device if the request to open the audio stream corresponds to a voice trigger containing a predetermined audio pattern.

Example 30 may include the at least one computer readable storage medium of Example 29 wherein the instructions if executed cause the mobile device to check a voice trigger activity flag to determine whether the request to open the audio stream corresponds to the voice trigger.

Example 31 may include an apparatus to locate a wireless device comprising means for performing the method of any one of Examples 9 to 16.

Example 32 may include an apparatus to route audio comprising means for performing the method of any one of Examples 27 or 28.

Thus techniques described herein may enable devices to listen in low power mode for specific phrases using a relatively small speech recognition module running in a low power audio DSP. If the device detects that the phrase has been spoken by the user the device may assume that the user is addressing the device in question. The device may then use the phrase or a speech command following the phrase to respond back to the user by either generating a tone or other type of audio response. Accordingly the user may follow the audible sound to locate the device. Such an approach may obviate any need for a separate phone or computer to call and or text the misplaced device. Additionally additional technology such as RFID technology may also be eliminated. Moreover the techniques may provide for device location even when the device is in standby mode.

Embodiments are applicable for use with all types of semiconductor integrated circuit IC chips. Examples of these IC chips include but are not limited to processors controllers chipset components programmable logic arrays PLAs memory chips network chips systems on chip SoCs SSD NAND controller ASICs and the like. In addition in some of the drawings signal conductor lines are represented with lines. Some may be different to indicate more constituent signal paths have a number label to indicate a number of constituent signal paths and or have arrows at one or more ends to indicate primary information flow direction. This however should not be construed in a limiting manner. Rather such added detail may be used in connection with one or more exemplary embodiments to facilitate easier understanding of a circuit. Any represented signal lines whether or not having additional information may actually comprise one or more signals that may travel in multiple directions and may be implemented with any suitable type of signal scheme e.g. digital or analog lines implemented with differential pairs optical fiber lines and or single ended lines.

Example sizes models values ranges may have been given although embodiments are not limited to the same. As manufacturing techniques e.g. photolithography mature over time it is expected that devices of smaller size could be manufactured. In addition well known power ground connections to IC chips and other components may or may not be shown within the figures for simplicity of illustration and discussion and so as not to obscure certain aspects of the embodiments. Further arrangements may be shown in block diagram form in order to avoid obscuring embodiments and also in view of the fact that specifics with respect to implementation of such block diagram arrangements are highly dependent upon the platform within which the embodiment is to be implemented i.e. such specifics should be well within purview of one skilled in the art. Where specific details e.g. circuits are set forth in order to describe example embodiments it should be apparent to one skilled in the art that embodiments can be practiced without or with variation of these specific details. The description is thus to be regarded as illustrative instead of limiting.

The term coupled may be used herein to refer to any type of relationship direct or indirect between the components in question and may apply to electrical mechanical fluid optical electromagnetic electromechanical or other connections. In addition the terms first second etc. may be used herein only to facilitate discussion and carry no particular temporal or chronological significance unless otherwise indicated.

As used in this application and in the claims a list of items joined by the term one or more of may mean any combination of the listed terms. For example the phrases one or more of A B or C may mean A B C A and B A and C B and C or A B and C.

Those skilled in the art will appreciate from the foregoing description that the broad techniques of the embodiments can be implemented in a variety of forms. Therefore while the embodiments have been described in connection with particular examples thereof the true scope of the embodiments should not be so limited since other modifications will become apparent to the skilled practitioner upon a study of the drawings specification and following claims.

