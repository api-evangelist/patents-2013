---

title: Communication traffic processing architectures and methods
abstract: Communication traffic processing architectures and methods are disclosed. Processing load on main Central Processing Units (CPUs) can be alleviated by offloading data processing tasks to separate hardware.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09654406&OS=09654406&RS=09654406
owner: REALTEK SINGAPORE PTE LTD
number: 09654406
owner_city: Singapore
owner_country: SG
publication_date: 20131219
---
This application is related to and claims the benefit of U.S. Provisional Patent Application Ser. No. 61 745 951 filed on Dec. 26 2012 the contents of which are incorporated in their entirety herein by reference.

This invention relates generally to communications and in particular to communication traffic processing.

The emergence of such technologies as Internet Protocol Television IPTV technology and the convergence of Digital Video Broadcast DVB router gateways and Digital Video Recorder DVR Set Top Boxes STBs place ever increasing demands on processing platforms.

Multi service processing is provided in a single delivery platform that can deliver simultaneous line rate bandwidth for secured data voice video and mobile services without service degradation.

Data networking and application processing are integrated together into a single chip or integrated circuit package. Features may include flexible hardware design multiple data interfaces one or more general purpose main processors in combination with offload hardware and efficient inter processor communications.

A special purpose processor multiple processors and or specialized hardware could be provided to enable hardware offload or acceleration for processing intensive functions. This approach offloads functions from primary general purpose processors also referred to as application processors or main CPUs reserving CPU processing power for additional value added services for example.

General purpose main Central Processing Units CPUs in a processing platform may be loaded to such a degree in performing networking or data communication tasks that remaining capacity for performing other tasks such as application or service related tasks suffers. Maintaining performance in respect of networking may come at a cost of limited or degraded application or service performance. For example networking tasks could occupy 75 80 of main CPU processing cycles leaving limited resources available for application or service processing.

Such high utilization of main CPU resources could also have an impact on power consumption and or operating temperature. The main CPU in an STB for example would be one of the higher power components and likely the component with the highest potential power consumption in such a device. Actual power consumption by a CPU depends on its utilization and accordingly a high utilization would have a high associated power consumption. High utilization also increases heat generation placing additional demand on heat sinks or other temperature control measures. Significant efficiencies can be gained through the use of special purpose reconfigurable engines as disclosed herein.

The example architecture can also include a 256 kB L2 cache an 8 kB secured boot Read Only Memory ROM a Cache Coherency Port a network engine a security engine a packet engine a traffic manager a Direct Memory Access DMA controller a 256 kB packet buffer and a 16 bit or 32 bit Double Data Rate DDR memory controller . Other sizes and or types of memory could be provided in other embodiments in addition to or instead of the example memory sizes and types shown in .

It should be appreciated that the example architecture of as well as the contents of the other drawings are intended solely for illustrative purposes and that the present disclosure is in no way limited to the particular example embodiments explicitly shown in the drawings and described herein.

All of the components in the example architecture may be integrated into the same chip or integrated circuit package or across multiple integrated circuits. A single chip or package then includes both networking and data processing components. For example specific processing tasks can be assigned to less powerful and more power efficient processors in the network engine the security engine and or the packet engine thereby making processing cycles in the more powerful general purpose main CPUs available for performing other tasks such as application or service related tasks.

This type of architecture can be more power efficient by reducing main CPU utilization for tasks that can be performed in less powerful processors that are optimized for their specific tasks. Performance gains can also be realized by making more main CPU processing cycles available for performing other tasks.

For instance supposing that security tasks are offloaded from the main CPUs to the security engine the main CPUs then have more processing cycles available for application or service related tasks. While a device with a main CPU architecture might provide for similar or even the same data rates as a device with an architecture based on the example architecture a device with an architecture based on the example architecture might support more feature rich applications or services and or better application service response times as a result of better main CPU availability due to task offloading to one or more engines .

This is illustrative of hardware acceleration features for higher performance in service provider networks. In an embodiment hardware acceleration features are accessed through customized software device drivers which make the hardware transparent to upper layer software components and applications. Under a Linux environment for instance open source drivers and a slightly modified kernel could be used. This allows users to further customize the kernel and run software applications on top of a Linux environment. Other operating systems can be supported using this type of hardware abstraction approach.

The example architecture integrates acceleration hardware for networking operations in the network engine security in the security engine and packet handling operations such as transport stream frame aggregation in the packet engine . Networking operations could include for example one or more of classification and Access Control List ACL handling Virtual Local Area Network VLAN operations Quality of Service QoS illustratively through the Linux QDisc model forwarding Network Address Translation NAT Netfilter operations multicasting and or queuing scheduling. Features and related processing that could be offloaded from the main CPUs to the security engine in the example architecture could include one or more of Internet Protocol Security IPSec Digital Transmission Content Protection DTCP Secure Real time Transport Protocol SRTP and or Secure Sockets Layer SSL .

The foregoing provides a general description of an example architecture as shown in . Further details are discussed by way of example below.

In an embodiment each of the main processors is a commercially available general purpose processor. Illustrative processor speeds are 600 MHz to 750 MHz. 32 kB Layer 1 or L1 Instruction I and Data D caches and are shown in . The main CPUs could support other features such as software acceleration for reduced code size and application acceleration Asymmetric Multi Processing AMP and Symmetric Multi Processing SMP for single or multi Operating System O S applications Single Instruction Multiple Data SIMD Instruction Set for graphics computation processing JTAG Program Trace Interface PTM performance monitoring and or buffering to accelerate virtual address translation for example. The present disclosure is not limited to any specific main CPU or type of main CPU. Also although the example architecture is a dual CPU architecture aspects of the present disclosure could be applied in single CPU architectures and or in architectures with more than two main CPUs.

Configuration of the main CPUs in one embodiment involves setting configuration parameters in configuration registers. When each main CPU boots up after reset it will read its configuration parameters. These parameters may also provide a default configuration of the L2 cache in addition to the default configuration for the main CPU cores . To change configuration parameters the appropriate registers are modified and a restart or reset is issued to one or both of the main CPUs . In an embodiment registers in the system are memory mapped. Configuration parameters could then be modified by writing to an address that each register has been assigned in the memory space.

As shown in the main CPUs are coupled to the various interfaces and any peripherals which are connected to those interfaces through the flexible interconnect . The network engine the security engine and the packet engine are also coupled to the interfaces and peripherals through the flexible interconnect and may communicate with and control those peripherals directly. Through the flexible interconnect any processor in the system including the main CPUs and separate offload processors or hardware in an offload subsystem implementing the network engine the security engine and or the packet engine for example can control any resource in the system. This allows system software to allocate which processors will control which inputs outputs I Os at run time. This in turn enables the separate offload processors or hardware to take control of high bandwidth SerDes I Os such as PCIe interfaces when associated processing is offloaded from the main CPUs .

The network engine shown in may provide such features as high speed packet forwarding editing queuing shaping and policing. The network engine can switch route and perform packet services such as Point to Point Protocol over Ethernet PPPoE tunnelling and Transmission Control Protocol TCP segmentation without main CPU intervention thereby offloading these networking tasks from the main CPUs .

For ease of reference other components with which the example network engine interacts are also shown. These other components include memory one or more offload acceleration engine processors the DMA controller and the main CPUs . The memory includes one or more memory devices. In an embodiment the memory includes DDR memory.

In an embodiment the example network engine may use multiple forwarding tables to accomplish packet forwarding schemes in the Linux IP stack. Both Linux rule and flow tables may be implemented in hardware. The rule tables are based on information found in a current packet. Some rule based entries such as firewall entries could be configured by system software before traffic begins to flow. Adaptation to other operating systems or custom forwarding stacks can be accommodated.

Flow tables may be programmed by system software when the first packet in a flow is received and every following packet for that flow can then be handled by the example network engine with no intervention by the main CPUs . An unmatched packet could be sent to the main CPUs to drop or initiate a learning process based on a filtering option. Packets in selective flows could be forwarded to the main CPUs for example if the payload associated with flows requires deeper packet inspection if the total number of hardware flows for acceleration using the example network engine exceeds a certain number of hardware flows and or if the number of hardware lookups based on any packet fields in any combination exceeds a certain number of lookups. In an embodiment the example network engine supports up to 8192 hardware flows and 12000 hardware lookups before selective flows are forwarded to the main CPUs . Hardware acceleration using the example network engine could also be turned on or off on a per flow rule basis.

Linux based flow connections can be established by the kernel and then programmed into hardware tables. This network engine model allows the Linux kernel and networking applications to make all decisions for new flows.

Data flows or flows as referenced herein may be associated with data that shares some sort of common characteristic. For example certain data processing tasks might be performed on a certain type of data. A data flow for that type of data could then be configured when that data type is first encountered and identified by the main CPUs as disclosed herein so that subsequently received data of that type can be identified as being associated with a known data flow and processed accordingly in an offload subsystem without involvement of the main CPUs. Data type is one illustrative example of a characteristic or pattern that could differentiate different data flows. Other examples include sending or source address es and or destination address es .

Suppose that a packet arrives into the ingress network interface illustratively through one of the Ethernet GMAC interfaces but is not part of a known traffic flow. An unknown flow could be dropped or forwarded to the main CPUs to be inspected. If the packet is dropped then nothing further would happen. For the purposes of illustration this example considers a scenario in which the received packet is forwarded to the main CPUs for inspection.

In an embodiment the packet has arrived on what is called a Physical Source Port ID PSPID and the packet some early L2 parse information and timestamp are passed to the forwarding engine . The forwarding engine may perform several stages of lookups 

Results of the lookups are decided based on their hits and priority mapping between those results. Based on results of the forwarding lookups the forwarding engine may modify the packet for transmission. Even if the packet header is not modified the aspects of the packet getting forwarded to a main CPU queue for example policing indices etc. could be determined and taken into account.

Forwarding results could be varied or over ridden based on ACLs. As an example an ACL could be set up to observe packet type and override any forwarding engine action that is different from a default action in the ACL. ACL entries could also be logically chained together. For example several ACL entries could be written for different actions with their results AND ed together to form a superset of those ACL rules.

Returning to the example of a packet from an unknown flow and presuming for the purposes of illustration that there is no ACL that specifies a different action since this particular packet misses a normal forwarding to a forwarding engine port it is not part of a known flow in this example it is placed into a Virtual Output Queue VOQ that is meant for the main CPUs . This enqueuing is through the queue manager and into the memory in the example shown in . The packet will reside in the VOQ until it is dequeued as ordered by the scheduler for scheduling the packet out of the main CPU queue.

Once the scheduler dequeues the packet the main CPUs dequeue the packet from the queue in the memory either through an interface to the memory or the DMA controller . The packet is then analyzed by the main CPUs . For the purposes of this example suppose that inspection of the packet identifies a new flow and the main CPUs decide that the packet should be forwarded on to a forwarding engine port with some transformation. The forwarding engine allows the transformed packet to pass through on that port. The main CPUs could instead forward the transformed packet out at this point so that it is not lost or wait until the next frame if frame loss is not a concern. As noted above the flexible interconnect enables any processor in the system including a main CPU and an offload subsystem to communicate with and assume control of any resource and thus the main CPUs could forward the transformed packet. The main CPUs would also update the flow table in this example.

The next time the same type of packet is received on the ingress network interface the forwarding engine now has a hit in the forwarding table after classification the previously determined packet transformation takes place and the packet is modified and the outbound VOQ is marked to an egress network interface port illustratively an Ethernet port.

The packet is now enqueued into a queue manager hardware VOQ which will be dequeued by the scheduler in due time. The upstream or downstream VOQ as configured in the scheduler dequeues the packet destined for an Ethernet port. The queue manager passes the packet on to the egress network interface . As the packet is dequeued an error check could be performed illustratively by checking a Cyclic Redundancy Check CRC code to make sure that an error of memory soft error has not taken place on the packet. The error check could be performed by the queue manager or another element. If the error check does not pass the packet could optionally have its CRC code stamped as being invalid as it is sent out to ensure the other side will receive an error and drop the frame. The packet is then queued on a transmit port and sent out.

As noted above packets may be transformed during the forwarding process. Packet transformation or editing functions could include for example 

Consider an example of PPPoE PPP Encapsulation Decapsulation. This example illustrates not only packet transformation but also interactions between the forwarding engine and the offload acceleration engine processor s .

When software running on the main CPUs receives the first PPPoE packet in a flow it configures a flow in flow tables of the forwarding engine to remove the PPPoE PPP header from a Wide Area Network WAN interface. It then configures another flow in the forwarding engine flow tables to add a PPPoE PPP header for traffic destined for the WAN and henceforth every packet in this flow is handled solely by hardware.

To decapsulate PPPoE PPP packets the forwarding engine sets a bit in the packet header to inform the packet engine supported by the offload acceleration engine processor s in this example to convert the packet from PPPoE PPP to IPv4 IPv6. The packet must have an Ethertype of 0x8864 or a PPP type of either 0x0021 or 0x0057 before it can be converted to an IPv4 or IPv6 packet. During the conversion the Ethertype is replaced with either 0x0800 for IPv4 or 0x86DD for IPv6. The next 6 bytes the PPPoE Header V T Code Session ID and Length and PPP type are all stripped.

Packet decapsulation works with VLAN tagged packets. The packet engine may also be able to parse the IP portion of the packet beyond the encapsulated PPP type. This allows IP VLAN MAC operations for PPPoE PPP packets.

IP VLAN and MAC operations are available under the packet engine which is responsible for encapsulating packets into PPPoE PPP in this example. The forwarding engine can identify which packet to encapsulate based on its flow result. The packet engine can then use the session ID from the flow which is also supplied with the inner packet s IP version to encapsulate the packet. The Ethertype and PPPoE fields including version type and code are configured in the forwarding engine in this example.

The PPPoE Version Type and Code fields make up a 16 bit header that is inserted into the original packet by the Packet Engine for encapsulation. The session ID length and PPP Type are also inserted. The length field is the length of the packet including the PPPoE header and the rest of the packet.

In this example the main CPUs are involved in initial flow identification and configuration of the forwarding engine flow tables. Once the flow tables have been configured encapsulation decapsulation tasks and security tasks if any are performed by the offload acceleration processor s . The encapsulation decapsulation and security tasks are examples of data processing tasks as disclosed herein and can occupy many processing cycles on the main CPUs leaving fewer processing cycles available for other tasks. Offloading these tasks to the offload acceleration processor s reduces the processing load on the main CPUs for performing data processing tasks.

Interaction of the offload acceleration engine processor s with the forwarding engine may be through VOQs as described above in the context of packets being forwarded to the main CPUs for inspection. In an embodiment there is one port for the packet engine and one port for the security engine and each of these ports has eight queues controlled by the scheduler and settable as destination VOQs. Once a packet arrives in the packet engine or similarly the security engine the packet is processed and may have its header modified by the packet engine be encrypted or decrypted by the security engine etc. Ultimately a processed packet can be moved out the packet engine port or security engine port or back out to the memory illustratively through an onboard local DMA controller of the offload acceleration engine processor s . This type of port and queue arrangement provides for efficient inter processor communications between the main CPUs and the offload acceleration engine processor s in this example.

Considering queuing in more detail the example network engine uses VOQs as noted above to identify which packet queue stores a packet while awaiting transmission. In an embodiment there are VOQs. When packets are received by any source such as the GMACs the main CPUs or other sources they are passed to the forwarding engine which ultimately decides if the packet is to be dropped or forwarded modified if appropriate . If a packet is to be forwarded then the forwarding engine identifies the queue which is to hold the packet until it is scheduled to leave by the scheduler . For operating systems such as Linux this could be controlled by the Traffic Control module which allows for the scheduling of packets.

There could be multiple queues per port to provide QoS for priority traffic such as voice video and controlled messages for example. In an embodiment queues are provided for all gigabit ports packet engines for tasks such as IP fragmentation reassembly IPSec etc. packet replication root scheduler and the main CPUs . The main CPUs could also have a large number of queues to support various priorities for different types of traffic. User types could be classified to support higher end enterprise type applications for example.

The queue manager in the example network engine accepts packets from the forwarding engine and stores them into queues in the memory . The queue manager could be configured to maintain priority and class of service as it manages memory buffers.

Any of various scheduling types and possibly multiple scheduling types could be provided by the scheduler . In an embodiment the scheduler implements hierarchical scheduling. For example a root queue scheduler a main CPU scheduler and per port schedulers could all schedule traffic queues to a top level scheduler. The lower level schedulers could each schedule SP queues and DRR queues. A DRR scheduler could schedule traffic from DRR queues with SP queues and DRR scheduled queues subsequently being scheduled in a next level SP or DRR scheduler which feeds into the top level scheduler. Per port schedulers could feed into a further next level scheduler for all of the ports illustratively a Round Robin RR scheduler which feeds into the top level scheduler.

SP scheduling services all queues according to their priority. Higher priority queues are serviced before lower priorities. Voice and video applications can be serviced with low jitter latency and packet loss in the high priority queues. While SP scheduling serves high priority applications well lower priority packets might be starved. To overcome this problem packet policers and or shapers could be used for the highest priority services with DRR scheduling for the rest. Using DRR allows bandwidth to be shared across all services while maintaining QoS. Weights can be applied to different priorities according to user requirements.

Although not specifically shown in a traffic manager could be used to control policing of packets and queuing parameters. It could also provide the ability to decide when to send pause frames on a link based on queue depth and or other traffic management functions.

In an embodiment congestion avoidance features are also provided. A Weighted Random Early Discard WRED function for example could determine packet drop probabilities for traffic queues based on Average Queue Depth AQD . AQD could be calculated with a software configurable weight and linear drop profiles could be defined by a minimum AQD maximum AQD and a maximum drop probability intercept point for instance. Backpressure is another example of a feature that could be used to reduce or avoid congestion and or packet dropping due to congestion. This type of functionality could be implemented in the queue manager or possibly elsewhere.

Other features could also or instead be provided by a network engine. The foregoing is intended solely for the purposes of illustration.

The packet interface enables the example subsystem to exchange at least data packets in this example with other components. Through the packet interface packets might be received from traffic queues for processing and returned to the queues or other components after processing. The packet interface or possibly another interface could support exchange of other types of signals such as backpressure signals to the scheduler which as noted above schedules packets from VOQs to the offload acceleration engine processor s shown in as the packet engine processor s . In an embodiment the packet interface provides for multiple virtual internal ports to connect to the packet engine processor s and the security engines s . This internal interface using ports and VOQs in one embodiment as described above enables extremely fast turnaround for packets with multiple passes such as IPSec Generic Routing Encapsulation GRE or other tunneled or bridged frames.

The non packet interface similarly enables the example subsystem to exchange at least data with other components. although in the case of the non packet interface this data would not be in the form of packets. In an embodiment the packet interface is an Ethernet interface and the non packet interface could include PCIe SATA and or USB interfaces for example.

A packet engine processor or more generally any offload processor could be the same type of processor as the main CPUs or a different type of processor. Unlike the main CPUs however an offload processor such as a packet engine processor is configured as a special purpose or dedicated processor for carrying out certain types of functions. In the example subsystem these functions include packet processing functions of a packet engine. A packet engine in this example is implemented in software stored in the memory or another memory that is executed by the packet engine processor s . The type of packet engine processor s or other offload processor s could be dependent upon the specific functions that are to be offloaded from the main CPUs . In general main CPUs would be more powerful than an offload processor so that offloading of the main CPUs does not rely on additional hardware that is nearly as complex as the hardware the main CPUs that is being offloaded. This also results in power savings when transferring tasks from the main CPUs to offload processor s or other offload hardware.

A security engine in the example subsystem represents a hardware implementation of security functions. In an embodiment a security engine is a configurable but hard coded encryption core. The example subsystem thus illustrates two types of offload engines including one or more offload processors executing a software engine in this example the packet engine processor s executing packet engine software and one or more hardware engines namely the security engine .

The memory in the example subsystem can include one or more solid state memories in an embodiment. For example the memory could include multiple blocks of Static Random Access Memory SRAM . The SA database would also be stored in memory but is shown separately from the memory in . In an embodiment only the security engine s and possibly only one security engine even if multiple security engines are implemented has full direct access the SA database . Other components of the example subsystem and or components of a system in which the example subsystem is implemented might have write only access to a memory device or area in which the SA database is stored.

The DMA controller represents an onboard DMA controller which provides the example subsystem with access to external memory such as memory shown in at SRAM and or one or more on chip memories. The DMA controller is also shared with Linux drivers in an embodiment for moving security keys and data to reduce latency and processing overhead.

A packet engine is a powerful and reconfigurable block that can be customized to accelerate proprietary and or new encapsulation protocols. In an embodiment a packet engine bridges different protocols. For example in an embodiment the example network engine is hard coded to handle Ethernet switching and a packet engine bridges traffic between the network engine and other non Ethernet interfaces. In this case packets are received by the packet engine processor s through the non packet interface for initial processing or translation conversion to Ethernet and then provided to the network engine.

Data processing tasks such as the examples provided above can thus be offloaded from the main CPUs to the example subsystem thereby reducing the load on the main CPUs for performing data processing tasks. More main CPU processing cycles are then available for performing other tasks such as higher layer application or service related tasks. Offload engines or more generally an offload subsystem that supports such engines can also be optimized for the particular data processing tasks that are to be offloaded thereby enabling those tasks to be performed more efficiently and faster than if they were to remain on the main CPUs .

In an embodiment a packet engine can have two types of users including the main CPUs for encryption support in conjunction with the security engine and the network engine for encapsulation encryption bridging and reassembly support. These users can use the security engine s simultaneously in some embodiments to preconfigure a number of security associations on chip for each user.

The security engine s may support any of various algorithms ciphers and hashes and security functions such as IPSec encryption decryption disk block encryption decryption base station encryption decryption etc.

The security engine s are used to offload cryptographic tasks from the main CPUs . Such tasks would be expensive in terms of processing load if implemented purely in software. There are two possible models that could be implemented including one in which the main CPUs control the security engine s directly and one in which an offload processor such as a packet engine processor controls the security engine s .

In the direct control case software executing on the main CPUs would program the security engine s to perform one or more security functions such as encryption decryption illustratively by using memory mapped registers that control the security engine s . Then the main CPUs could provide a memory pointer which indicates the location of the one or more packets to be processed by the security engine s . The security engine s would encrypt decrypt or otherwise process the packet s and then provide the pointer back to the main CPUs . In this example data is shared between the main CPUs and the security engine s through the exchange of memory pointers. Other data sharing or exchange mechanisms could also or instead be provided to enable offloading of security tasks to the security engine s .

For an indirect control embodiment where an offload processor and not the main CPUs controls the security engine s the main CPUs would indicate or otherwise provide one or more packets to be processed to the offload processor. Memory pointers could be provided to the packet engine processor s for example. The offload processor s would then program the security engine s and coordinate encryption decryption or other security processing of the packets by the security engine s . This could involve providing memory pointers to the security engine s and receiving memory pointers from the security engine s when security processing is completed. Then the offload processor s would indicate completion back to the main CPUs by providing memory pointers back to the main CPUs for example.

It should be appreciated that the packet engine processor s and the security engine s are illustrative examples of offload or acceleration engines. Other embodiments could include additional and or different engines.

For example the packet engine processor s could be shared processors which are also used to execute software for other engines. Similar to the security engine s other offload or acceleration engines could be implemented in dedicated hardware.

A linked list walker engine a buffer allocator engine and a SAMBA offload engine are illustrative examples of other offload or acceleration engines that could be implemented in an offload or acceleration subsystem to further enhance its functionality. These additional example engines are not shown in but could be interconnected with the other components of in the same manner as the packet engine processor s and the security engine s with the exception of the direct full access to the SA database as shown for the security engine s .

A linked list walker engine could be implemented for example as a hardware module that offloads the task of walking linked lists. Software which processes packets may spend a lot of time storing and retrieving packets which are placed in linked list data structures. These structures become quite convoluted and it can take many memory reads to track down a leaf node where a packet is stored. A linked list walker engine could be used to offload this processing from software executing on the main CPUs . Instead of doing many memory reads on a linked list structure the main CPUs may then provide the head of the linked list structure to the linked list walker engine which will follow the linked list structure down to the leaf node level. Once this is done the packet can be easily read written by the software.

In an embodiment a linked list walker engine could be programmed with the format of the list such as where to find the bytes that indicate the address of the next pointer and other format information about the structure of the list. The linked list walker engine could have a number of different formats programmed with each format being identified by an index for example. When software running on a main CPU is to walk a list it could provide to the linked list walker engine the address of the head of the list the index number which describes the format of the list and an indicator of what action to perform. The actions that can be performed could include for example inserting one or more new items to the end of the list in which case a main CPU could provide a pointer to an array in memory which contains the items to insert removing the last N items from the list in which case a main CPU could provide a pointer to an empty array in memory which the linked list walker engine can fill and or other actions. The linked list walker engine signals completion to the main CPUs by setting interrupts in an embodiment.

A buffer allocator engine could be implemented for example as a hardware implementation of a memory allocation call. When software running on the main CPUs wants to store something into memory it might request for the kernel to allocate memory by using a memory allocation call. This call could take many main CPU cycles and happen many times per second. In an offload engine architecture when the software needs memory it can instead request memory from the buffer allocator engine. The buffer allocator engine could be a special hardware offload engine that tracks available memory in the system and returns the requested buffer to the software. In an embodiment what is returned to the main CPUs by the buffer allocator engine is a pointer to e.g. the memory address of the buffer which has been allocated.

A SAMBA offload engine is an implementation which accelerates the SAMBA protocol. The SAMBA protocol allows storage such as hard disk drives to be accessed over networks. The protocol requires that networking traffic be received and processed into a format suitable for storing onto a disk. Since each received packet on a networking interface must be processed in SAMBA it can take many CPU cycles. A SAMBA offload engine would allow the main CPUs to simply forward network traffic which is destined for the disk to the SAMBA offload engine. The SAMBA offload engine then processes the traffic according to the SAMBA protocol and handles all the resulting file system management thereby reducing the processing load on the main CPUs by performing data processing tasks that would otherwise be performed by the main CPUs.

Components of a processing architecture are described above by way of example with reference to . Detailed examples of embodiments which provide offload in the context of WiFi applications are described below with reference to which are block diagrams of further example processing architectures.

The example architecture in includes a 5 GHz IEEE 802.11ac WiFi module . Other embodiments may include other types of WiFi modules. An Ethernet Network Interface Card NIC is also shown. Both of these modules are coupled to PCIe interfaces in this example. PCIe interfaces are not separately shown in but are shown at in .

A dual main CPU architecture is shown in . In order to avoid congestion the main CPUs are shown in a single block . Each main CPU supports a Linux networking protocol stack although other operating systems may be supported in other embodiments. A WiFi driver includes lower layer driver and an upper layer driver . An Ethernet driver is shown at and the main CPUs also execute a network interface driver . A CPU port enables communications between the main CPUs and the network engine .

The network engine includes a forwarding engine and other hard coded functions of the network engine are represented at . In the example architecture there are 8 priority queues per port shown at . One or more network interfaces in the network engine enable communications over Ethernet connections shown as Gigabit Ethernet GE 0 GE 1 GE 2. These connections are through the GMAC interfaces in an embodiment.

The example architecture in includes a hardware offload engine or accelerator in the form of the network engine . Further offload acceleration hardware is shown in the example architecture in . The security engine 0 security engine 1 packet engine 0 and packet engine 1 enable additional offloading and acceleration. The security engines handle security related functions and the packet engines handle data plane functions as described herein. The security engines are hard coded but configurable by system software running on the main CPUs and the packet engines include respective packet engine processors packet memories and DMA controllers .

The main CPUs as noted above support a Linux networking protocol stack and provide a CPU port for communicating with the network engine and a network interface driver . The network engine kernel module controls forwarding functions and implements an interface between the Linux networking protocol stack interface and the network engine hardware shown at . The network engine kernel module also provides kernel hooks to enable the offload and flow management capability in the network engine and controls and manages operation configuration and monitoring of the network engine.

In the example architecture there are two WiFi modules including a 2.4 GHz IEEE 802.11n module and a 5 GHz IEEE 802.11ac module which connect to the packet engines through PCIe interfaces. The packet engine 0 and the packet engine 1 are represented in primarily with functional blocks which illustrate the functions carried out by those engines in this embodiment. As shown the packet engine 0 executes a lower layer WiFi transmit Tx driver and the packet engine 1 executes a lower layer WiFi receive Rx driver. Each packet engine includes an Inter Processor Communication IPC mail box which would be stored in memory and a WiFi driver tunnel module for handling tunneling creation and termination for example. One or more security modules could also be provided and used by the packet engines and or the main CPUs but are not shown in in order to avoid congestion in the drawing.

The main CPUs support the Linux networking protocol stack and include the network interface driver and the network engine kernel module . Each main CPU also includes the CPU port for communicating with the network engine an IPC mail box a WiFi driver which includes an upper layer driver and a WiFi Offload Adaptation Layer WOAL and WiFi driver tunnel modules .

The WiFi driver tunnels provided by the WiFi driver tunnel modules at the main CPUs and the packet engines encapsulates 802.11 WiFi frames into 802.3 Ethernet frames which can be delivered to the main CPU via the network engine . In an embodiment the network engine is based on standard Ethernet and can understand and forward 802.3 frames. The frames sent and received via the WiFi modules could be in the form of 802.11 frames which is very different from 802.3 frames.

The IPC mail box operates in conjunction with the IPC mail boxes of the packet engines to provide an efficient communication mechanism between the main CPUs and the packet engines. This is described in further detail below. The IPC mechanism between the main CPUs and the packet engines is used for configuration control and management functions in an embodiment. In the present example of WiFi offload it is used to directly control and update the 802.11 frame to 802.3 frame conversion and vice versa on a per station basis. It could also used for the management such as diagnostics and performance monitoring.

A station in WiFi technology refers to any client device connected to an access point AP . Processor architectures as disclosed herein could be implemented in an AP such as a home gateway for example. Station to station communication will typically go through the AP. For each station the 802.11 frame header may be different and in an embodiment the packet engines maintain a translation table for each station or for each destination MAC address.

Regarding the WiFi driver a reason why main CPU utilization is high when handling WiFi user data frames in for example is high context switch and long memory access latency. An objective of WiFi offloading as shown in is to remove this bottleneck by relocating the user data traffic forwarding to the packet engines and the network engine . As a result those data frames no longer go through the main CPU path. In the example offload design shown in the packet engines handle the data interface and move the user data frames into and out of the WiFi modules . Thus the packet engines implement lower layer driver functions as represented at and upper layer driver functions relating to protocol management and control remain in the WiFi driver on the main CPUs as shown at . The WOAL enables this offloading and is described in further detail below.

The network engine continues to provide such features as forwarding frame buffering and QoS functions. The lower layer drivers are primarily involved in the data frame movement between the WiFi modules and the packet engines in the offload case or the main CPUs in the non offload case . In addition the lower layer drivers optionally process other data processing tasks such as 802.11 format conversion to 802.3 frame format for an Ethernet based network engine frame aggregation rate control and power savings. If frame conversion is provided the packet engines maintain a conversion table for each station since the 802.11 header information varies from one station to another. The table is dynamically updated via the IPC mail boxes by the main CPUs which are responsible for the association of each table with a station using control and management frames.

In operation a WiFi module supports either of two user data frame formats across the PCIe or host interface namely 802.11 frame format or 802.3 frame format. For illustrative purposes consider an embodiment in which the Linux networking protocol stack is configured to be in a bridging mode in which frames are forwarded based on the destination MAC address.

The WiFi driver tunnels provided by the WiFi driver tunnel modules are an internal path to transmit frames between the packet engines and the upper layer driver of the WiFi device driver on the main CPUs . These tunnels are established as dedicated flows in the network engine in an embodiment and they have the capability to encapsulate 802.11 frames inside 802.3 frames which can be recognized by the network engine. The encapsulation is provided by the WiFi driver tunnel modules in an embodiment. The WiFi driver tunnels and could be separate logical interfaces on the CPU port each with 8 virtual priority queues. In this example implementation the CPU port supports 8 logical interfaces or 64 virtual priority queues. Each GE interface connected to the network engine could also have 8 virtual priority queues on the network interface driver .

Considering receive Rx operation when a management frame identified by frame type is received by the packet engine 1 from one of the WiFi modules the packet engine will send this frame directly to the main CPUs through the WiFi driver tunnel between the WiFi driver tunnel modules . The frame will be delivered to the upper layer driver in a transparent fashion. The WOAL enables offloading of data processing tasks and provides an interface between the upper layer driver and the lower layer drivers such that the offloading is transparent to the upper layer driver.

When a data frame identified by a different frame type is received by the packet engine 1 from one of the WiFi modules the lower layer driver in the packet engine will first check a transmit or forwarding table to determine whether there is already an entry in the table for the destination MAC address. If it exists this frame is not the first data frame in a data flow for the destination MAC address and it will be delivered to the network engine for forwarding and processing. If it does not exist then it is the first data frame for the destination MAC address and it will be forwarded to the main CPUs through the WiFi driver tunnel. The upper layer driver will process the frame in the same way as the upper layer driver in including conversion of the frame format from 802.11 to 802.3. Then the frame is passed to the Linux networking protocol stack where a forwarding decision will be made. This decision will provide the egress port to which the frame will be forwarded. The network engine kernel module will create a flow entry in the network engine for the source MAC address. The frame will be passed onto the network interface driver which will in turn send it to the network engine for forwarding.

Turning to transmit Tx operation when a frame is received on one of the Ethernet interfaces in the network engine and no flow entry match is found for its destination MAC address it will be then forwarded to the network interface driver on the main CPUs . The network interface driver will pass the frame to the Linux networking protocol stack for a forwarding decision. If the egress port for this frame is a WiFi interface then the frame in 802.3 format will be passed on to the upper layer driver in the WiFi device driver for processing. A flow entry is then or substantially simultaneously created in the network engine by the network engine kernel module so that subsequent frames carrying the same destination MAC address will be directly forwarded from the network engine to the packet engine 0 without involving the main CPUs thereby providing the offload effect. The basic operation at the WiFi lower layer device driver when a frame is forwarded to it directly by the network engine is to convert the 802.3 frame into an 802.11 frame among other processing functions. The frame will be sent to the packet engine 0 through the WiFi driver tunnel. Then or substantially simultaneously the WOAL will send a configuration message to the packet engine 0 so an entry will be created in the transmit table indexed by the destination MAC address. This entry will allow the 802.3 frame carrying the destination MAC address to be converted to an 802.11 frame so it can be directly transmitted to the appropriate WiFi module .

The example architecture in is substantially similar to the example architecture in except that both packet engine 0 and packet engine 1 handle transmit and receive operations. The lower layer drivers the IPC mail boxes and the WiFi driver tunnel modules thus support bidirectional communications. Interaction between the IPC mail boxes is also slightly different in the example architecture in that the IPC mail boxes in this example need not interact with each other directly where each packet engine handles both transmit and receive operations. One difference between the example architectures in in is that the former allows load balancing if processing power requirements of the WiFi modules are asymmetric. However it would be possible to interconnect both WiFi modules to both packet engines 0 and 1 in the example architecture in as well.

The example processing architecture in relates to web filtering. In this embodiment data processing tasks related to web filtering are offloaded from the main CPUs to the network engine which includes a hash classifier a traffic manager and a forwarding engine The network engine could be implemented in substantially the same manner as in other embodiments but is labeled differently in to illustrate that it provides offloading of web filtering tasks in addition to forwarding tasks in some embodiments. The network engine communicates with the internet . Protocol management or control tasks remain on the main CPUs and are shown in as Uniform Resource Locator URL processing . The URL processing is in the form of software executed by the main CPUs in this example. The local URL database stores filtering control information specifying how data traffic is to be filtered. In an embodiment the local URL database could store white list or permitted flow information specifying data traffic that is permitted in which case non permitted flows are to be dropped or otherwise filtered. The local URL database is populated by URL database updates from a could security server in the example shown. These updates could be on a daily basis some other automatic schedule and or request driven. A network engine kernel module is also shown in .

The hash classifier the forwarding engine and the traffic manager are hardware based in an embodiment and implemented in configurable but hard coded hardware for example. The hash classifier identifies HTTP flows in the example processing architecture based on a white list configuration by the network engine driver . If a HyperText Transfer Protocol HTTP flow 1 is not recognized by the hash classifier which would be the case for a new packet in a flow for example the flow is forwarded 2 to the main CPUs for identification. As part of the URL processing at the local URL database and or the could service security server would be consulted 3 4 . If the flow is a permitted flow 5 then a hash table of the hash classifier is configured 6 for the permitted flow by the network engine kernel module or the URL processing sends 5 Deny an HTTP reply with TCP session reset for a denied flow or alternatively a URL redirect message not shown in the figure . This HTTP reply or redirect is returned to the requesting user system through the network engine .

A flow that is recognized by the hash classifier is handled by the network engine without involvement by the main CPUs thereby offloading data processing after the initial identification from the main CPUs.

The WiFi and web filtering examples in illustrate a form of first packet processing that enables offloading of substantial data processing tasks from the main CPUs . Although the main CPUs are involved when a flow is not recognized by an offload engine data processing for a flow after it has been initially identified software executing on the main CPUs can be offloaded. Management or control tasks remain on the main CPUs and data processing tasks are offloaded to offload engines. In the WiFi examples of the main CPUs still handle upper layer WiFi protocol management or control tasks and thus the offloading does not change how the protocol operates or require any changes in the WiFi modules . Similarly in the web filtering example in the URL processing resides on the main CPUs and offloading of filtering to the hash classifier in the network engine does not affect HTTP and TCP operation. Protocol management or control tasks for HTTP and TCP are handled by the main CPUs and data processing is offloaded to the network engine .

Processing architectures as disclosed herein enable tasks to be offloaded from one or more main CPUs to one or more offload or acceleration engines. For example software such as peripheral device drivers might involve protocol management or control tasks and data processing tasks. In an embodiment management or control tasks remain on the main CPU s so that offloading does not change the way in which protocols or interface devices such as WiFi modules operate and lower layer data processing tasks are offloaded. Such software partitioning or splitting entails identifying which pieces of software or which tasks make sense to relocate to an offload engine and which pieces or tasks should reside on the main CPU s . In an embodiment pieces of the software drivers that handle the most data traffic and therefore are least efficient on a general purpose application processor could be rewritten revised or otherwise ported to an offload engine and carved out of the software that will remain for execution by the main CPU s .

In an embodiment the upper layer driver performs 802.11 protocol management tasks and provides a device driver interface to the Linux networking stack and the lower layer drivers handle actual data movement to and from peripheral devices namely WiFi modules through PCIe interfaces and the PCIe controller driver in the example shown. Tasks such as 802.11 802.3 frame conversion by the frame converter at frame aggregation by the frame aggregator at rate control by the rate controller at and power management for power saving features by the power controller at are offloaded in the lower layer drivers in this example.

The movement of data between the WiFi modules and the lower layer drivers is performed by a DMA operation through a packet ring structure in an embodiment. The packet ring structure contains packet descriptors which describe the packets stored in a packet memory with a read pointer and a write pointer. Each packet descriptor has packet information such as the memory location for the packet and packet length. When a packet is ready to be transmitted from a WiFi module to a packet engine an interrupt signal is sent to the packet engine. The packet engine then initiates the transmission from the read pointer in the receive packet ring. There is a similar packet ring for the transmission from the packet engine to a WiFi module .

Between the upper layer driver and lower layer drivers the WOAL provides a shim or interface layer to enable the offload capability in a manner that is transparent to the upper layer driver. The WOAL controls and communicates with offload engines namely packet engines in this example via the IPC mail box and also provides the WiFi driver tunnel for transparent data delivery. The lower layer drivers can be rewritten or otherwise revised for compatibility with the offload API provided by the WOAL which in turn interfaces with the upper layer driver . Offloading could be entirely transparent to the upper layer driver by having the WOAL provide an interface to the upper layer driver that is consistent with an interface definition or specification through which routines or functions that are to remain on the main CPUs interact with routines or functions that are to be offloaded. For instance the WOAL could be adapted to accept function or routine calls from the upper layer driver in the driver native formats and to return results to the upper layer driver in native formats as well. Translation between the native format and other format s used to implement the offloaded tasks or functions can then be handled by the WOAL . The WiFi driver tunnel modules represent an example of this type of feature which allows WiFi frames to be transported between packet engines and the main CPUs through the network engine .

For instance in the example processing architecture in web filtering software is split between the main CPUs and the network engine . URL processing which handles protocol management or control tasks remains on the main CPUs. Data processing tasks in this case filtering are offloaded to the network engine .

Considering software splitting more generally one objective of offloading tasks from main CPUs might be to relocate tasks that are not efficient on general purpose processors to less powerful but specially configured processors or other offload hardware. This type of approach could be driven by main CPU processing bottlenecks and or high main CPU utilization for example.

In developing an offload strategy it could also be desirable not to change protocols as doing so would create additional processing load and or changes in devices that connect to a processing architecture. Considering the WiFi offload as an example it might be possible to change WiFi modules so that some tasks are performed at the front end before data arrives on a PCIe interface. This approach however significantly impacts WiFi device design. Traditionally WiFi devices are not intelligent in that processing intelligence resides elsewhere in a processing system. Relocating that intelligence onto WiFi devices themselves requires a significant shift in device design and also significantly impacts WiFi protocols.

Analysis of device driver software and or other types of software could be undertaken in an embodiment to identify lower layer e.g. layer 1 or layer 2 data processing bottlenecks which involve data processing at only a single layer in an embodiment. Protocol management or control tasks tend to be less processor intensive and are generally performed less often than data processing tasks and therefore protocol management or control tasks could be good candidates to remain on main CPUs. Once data processing tasks are identified for offloading software for performing those tasks can be rewritten or otherwise revised to run on offload hardware. In some embodiments such tasks could be hard coded into hardware which mimics the software tasks. Hard coding of offload tasks can provide further benefits in terms of speed.

Device drivers for example might perform specific tasks on specific types of data. Thus for a certain type or pattern of input generally called a flow herein a certain task or set of tasks would always be performed. This type of action could be soft or hard coded into an offload engine. In an embodiment the first packet for a new data flow is provided to main CPUs for identification based on header processing or other protocol management processing. Software executing on the main CPU can then update offload engine tables or otherwise provide identification information to an offload engine which can then identify other packets in the same flow and perform the same data processing tasks without involving the main CPUs. Such first packet processing by the main CPUs in this example provides for centralized protocol management processing while still enabling data processing tasks to be offloaded. The first packet may be extended in an embodiment to include multiple packets until a flow for offloading can be identified on the main CPUs.

Splitting or partitioning software functionality incurs communication overhead between the main CPU s and offload processor s . Cache coherency hardware is provided in some embodiments and allows transactions that go across the system bus between processors to be coherent from the perspective of each processor s memory subsystem. This reduces the amount of overhead spent locking and unlocking resources and therefore allows the processors to communicate more quickly. Cache coherency implementations could be provided for homogenous main CPU offload processor architectures i.e. the main CPU s and the offload processor s are of the same type or heterogeneous processor architectures.

Cache coherency allows main CPUs to communicate with offload engines using memories and caches without incurring the overhead of having to wait for message passing mechanisms such as spin locks or mailboxes. This results in fewer wasted main CPU clock cycles and therefore minimizes power dissipation and maximizes performance.

In an embodiment cache coherency is implemented by giving offload engines access to main CPU L1 and L2 caches through a processor cache coherency port. When the offload engines are configured to use cache coherent accesses they read from and write to DDR or SRAM memory locations by going through the main processor L1 or L2 caches.

For example a main CPU might pass an offload engine a memory pointer which indicates the location of a stored packet. In a non cache coherent configuration the offload engine would then read the packet directly from memory and process it. Then it would write the packet back to memory which can take a long time due to the slow speed of memory relative to the speed of on chip processors. If the main CPU tried to read the same packet data during the time while the offload engine is working then it would get incorrect data. To avoid this the main CPU must instead use software cycles to poll or otherwise wait for the offload engine to indicate completion of the writes to memory and then proceed to read the packet data back from memory.

In a system with coherence enabled the offload engine would read the packet through the L1 L2 cache structure of the main CPU. This would cause the main CPU to read the packet data from memory and expose the packet data to its cache. When the offload engine is done modifying the packet data it writes it back to the L1 L2 cache structure of the main CPU. This allows the CPU to immediately have access to the modified data without waiting for it to be written back to memory.

Processing architectures as disclosed herein could work in a cache coherent mode or a non cache coherent mode. For non cache coherent mode IPC mail boxes could be provided to facilitate the communication between the offload engine s and the main CPU s . Mail boxes such as those shown in allow for reliable message passing with relatively low CPU overhead. When an offload engine has completed a task it can place a message indicating completion into the mailbox for the main CPU s . In an embodiment this will cause an interrupt to be generated to the main CPU s . The main CPU s as part of an interrupt handling routine can then read the message and be notified of task completion. This keeps the main CPU s and the offload engine s in synchronization with each other.

In an embodiment a flexible and dynamically controllable interconnect such as shown at in enables any processor or offload acceleration engine in the a processing system to control any resource in the system. This allows software to allocate which processors or hardware will control which I Os at run time. For example offload processors could take control of high bandwidth SERDES I Os such as PCIe when it makes sense to do so such as when a particular PCIe interface is connected to a WiFi module and data processing tasks for WiFi are to be offloaded.

Some embodiments might also or instead provide for multiplexing of interfaces over the same pin s or port s . This type of flexibility in I O is shown by way of example in which is a block diagram illustrating low speed interfaces. As shown in low speed interfaces such as the PCM interface the Flash interface and the LCD interface may be multiplexed with GPIO functions for the GPIO interface . This allows software to dynamically allocate I O pins to functions.

In a service provider video gateway for example the PCIe integrated interfaces could be used to provide two independent WiFi connections and additional high speed multi channel transcoding decoding to facilitate a full video solution. One of the USB ports could be used for access to the processing architecture leaving the other available for host or device user connectivity for printers and disk attached storage in an embodiment. The integrated SATA port and or one or more PCIe SATA interfaces could be used in this type of application for Personal Video Recorder PVR and or Network Attached Storage NAS functionality.

Scalable interfaces and performance in a processor architecture could support wide range of cost and performance media server models. The example architecture in supports up to four SATA ports at for example any or all of which could be used to implement a wide range of NAS solutions. The LCD interface directly supports picture frame functionality in an embodiment and could also connect to a panel through a High Definition Multimedia Interface HDMI converter for example to provide for medium resolution display output at low cost.

In implementing a router VPN concentrator one of the dual USB ports could be configured in device mode to allow USB storage and other USB device connectivity. Under the USB device mode the USB port is seen as a USB mass storage device by a PC or other connected systems. SATA ports at could also be used for external storage. VPN applications would also utilize the encryption capabilities provided by the security engine .

The example architecture could also be useful to provide a low cost solution for security premises equipment through its PCIe interfaces for high camera count video converters. The onboard encryption capability in the security engine allows secure storage of encoded video. The processing power of the main CPUs could support multiple camera transcoding without additional hardware support. If a video capture device supports coding then the example architecture could provide just encryption and decryption of the storage data by the security engine .

Regarding the DPI module instead of just looking at L2 L3 or L4 headers to decide whether to admit drop route a packet this module could look very deeply into for example L7 content of the packet and then decide what to do. The DPI module could employ rules which specify what to look for and what action to take and could be used for example to look into packets and find viruses. Infected packets could then be identified and dropped. This could be of interest in cloud environments to prevent malicious activity before entry into the cloud network at any edge .

In an embodiment the pico cloud is provided by a gateway which includes a processing architecture and multiple interfaces. is a block diagram illustrating an example gateway.

The example gateway includes powering components such as the regulators coupled to a 110V supply in this example and a battery . The battery could be implemented to provide for life line protection for telephones that require power to operate for example. If the example gateway is used for home phone service then the battery could maintain telephone service at least temporarily in the event of a power failure.

A processing architecture in accordance with teachings provided herein is coupled through its various interfaces to memory in the form of DRAM and flash memory in this example. WiFi radios connect to the processing architecture through integrated PCIe interfaces. USB ports are shown at for connection to external USB devices. A gateway might also include disk storage such as a hard drive connected to a SATA interface of the processing architecture . Telephone interfaces such as phone jacks could connect to one or more integrated PCM interfaces and or other interfaces in the case of Voice over IP VoIP phones for example in the processing architecture .

A video enabled gateway could include one or more TV tuners connected to transport stream interfaces in the processing architecture . Ethernet ports are shown at and could be used to provide internet connectivity for one or more standalone computers and or networked computers.

What has been described is merely illustrative of the application of principles of embodiments of the invention. Other arrangements and methods can be implemented by those skilled in the art without departing from the scope of the present invention.

For example the drawings are intended solely for illustrative purposes. Other embodiments may include additional fewer and or additional components interconnected in similar or different arrangements. Each of the main CPUs could include a Digital Signal Processor DSP with its own data cache and instruction cache for instance. In an embodiment these caches are each 32 kB although different numbers and or sizes of caches are also contemplated.

In addition although described primarily in the context of methods and systems other implementations of the invention are also contemplated as instructions stored on a computer readable medium for example.

Features herein in singular or plural form are not intended to limit embodiments to any number of instances or components. For example the processing architectures disclosed herein need not be implemented in conjunction with multiple main CPUs.

It is also noted that packets are an illustrative and non limiting example of blocks of data that could be processed as disclosed herein. Cells frames and or other data blocks could be handled in the same or a similar manner as packets.

