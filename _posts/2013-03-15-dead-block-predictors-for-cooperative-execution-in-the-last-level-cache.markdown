---

title: Dead block predictors for cooperative execution in the last level cache
abstract: A cache memory eviction method includes maintaining thread-aware cache access data per cache block in a cache memory, wherein the cache access data is indicative of a number of times a cache block is accessed by a first thread, associating a cache block with one of a plurality of bins based on cache access data values of the cache block, and selecting a cache block to evict from a plurality of cache block candidates based, at least in part, upon the bins with which the cache block candidates are associated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09195606&OS=09195606&RS=09195606
owner: Intel Corporation
number: 09195606
owner_city: Santa Clara
owner_country: US
publication_date: 20130315
---
Embodiments described herein generally relate to the field of microprocessors and more particularly microprocessor cache memory policies for evicting cache lines.

Multicore processors and multi threaded cooperative workloads may exhibit cache access patterns that differ from cache access characteristics of single threaded execution environments. Methods for identifying dead blocks in a single core processor however may not extend well to a multicore environment.

Some embodiments pertain to the use of a thread aware dead block predictor TA DBP in a shared cache memory. In at least one embodiment a method for predicting dead blocks recognizes distinctions between shared data and unshared data. In at least one embodiment a replacement policy emphasizes the replacement of cache lines referred to herein simply as blocks representing unshared data and preferentially preserves shared data blocks.

In some embodiments a TA DBP algorithm classifies cache blocks in a core cache or private cache using a pair of parameters referred to herein as use count UC and trip count TC values. In at least one embodiment the UC parameter for a cache block indicates the number of hits seen by the cache block while it resides in the core cache. In at least one embodiment the TC parameter indicates the number of times the block was recalled from a higher level cache referred to herein as the shared cache to the core cache.

In at least one embodiment a multi core processor is operated according to a not recently used NRU replacement policy pseudo NRU policy a not recently filled NRF policy or another suitable replacement policy and dead block counts and live block counts are maintained for each UC TC pair. Some embodiments increment a dead block count associated with a UC TC pair when a UC TC cache block i.e. a cache having the applicable values of UC and TC is evicted from the shared cache. In these embodiments all other instances of blocks possessing the UC TC values represent live blocks. After sufficient dead block and live block data per tuple has been gathered embodiments may access review or analyze the dead block data and live block data to identify any correlation or other relationship between dead blocks and live blocks. In at least one embodiment dead block predictions may be based at least in part on any correlation or relationship identified. These dead block predictions may be used in some embodiments to prioritize eviction policies. In at least one embodiment dead block predictions influence eviction policy by determining an age attribute when a block is allocated in the shared cache and leveraging an existing recency based protocol to make the final eviction determination.

In at least one embodiment access patterns associated with any data identified as being shared data i.e. data accessed by two or more threads of the multicore processor during the cache memory life cycle of the data are excluded or segregated from access patterns associated with unshared data. These embodiments are referred to herein as thread aware embodiments. The cache memory lifecycle also sometimes referred to herein as a cache memory tenure refers to the interval when the cache block is first filled in one of the caches from memory until the time the block is evicted from the shared cache. During the cache memory lifecycle a block may move between the shared cache and one or more core data caches. Death predictions may be made each time a block is allocated in the shared cache by examining the dead and live block counters of the TC UC bin to which an unshared block is categorized. In at least one embodiment dead and live block counters for various TC UC tuples may be maintained for each core separately.

In at least one embodiment a processor includes a plurality of processing cores and a cache memory subsystem. In at least one embodiment the cache memory subsystem includes a plurality of core caches where each of the core caches corresponds to one of the processing cores. The cache memory subsystem may further include a shared cache common to each of the processing cores. The cache memory subsystem in at least one embodiment includes TC logic and UC logic. The TC logic may increment a thread aware TC for a block recalled from the shared cache to the first core cache. The UC logic may increment a UC for a block responsive to each access of the block in the first core cache. In at least one embodiment the cache memory subsystem includes shared block logic to set a shared block bit for a block accessed by multiple core caches. Bin logic within the cache memory subsystem may maintain live block counts and dead block counts for each bin. In at least one embodiment a bin corresponds to a UC TC pair. The live block count may indicate the number of valid unshared blocks associated with the corresponding bin. The dead block count may indicate the number of blocks evicted from a corresponding bin. In at least one embodiment the cache memory subsystem may include eviction logic to select a shared cache block to evict from a plurality of eviction candidates based on at least one of TC values live block counts and dead block counts.

In some embodiments the bin logic increments a live block count for a first bin and decrements a live block count for a second bin when it detects a UC TC pair changing from a first combination to a second combination. The bin logic may further increment a dead block count for a third bin when an eviction of a cache block associated with the third bin is detected.

In some embodiments the cache memory subsystem may include last to allocate LTA logic to set LTA bits for a block to indicate the last thread to allocate the block in the shared cache. In at least one embodiment the LTA field identifies the first thread as the last thread to allocate the block. The shared block logic may set a shared block bit responsive to detecting that the LTA field identifies a second thread when the first thread has most recently allocated the block. In at least one embodiment the cache memory subsystem includes age logic to set the maximum value for an age field of a block in response to setting the shared block bit for that block so that shared blocks are preferentially maintained in the shared cache with respect to unshared or private blocks. In some embodiments the selection of a block to evict may include consideration of the age field.

In at least one embodiment a cache memory eviction method includes maintaining thread aware cache access data per cache block in a cache memory. The cache access data may be indicative of a number of times the cache block is accessed by a first thread. In some embodiments access by a first thread corresponds to access by a first core cache. In at least one embodiment the cache memory eviction method includes associating a cache block with one of many cache access bins referred to herein as bins based on cache access data values applicable to the cache block. In some embodiments the cache memory eviction method includes selecting a cache block to evict from multiple candidates based at least in part upon the cache access bins in which a cache block candidate resides.

In some embodiments the cache memory includes a shared cache within a cache memory hierarchy. The cache access data may include TC data indicating the number of times a block is forwarded from a shared cache to a first core cache. In some embodiments the cache access data may also include UC data indicating the number of times a block is accessed by a first thread while residing in the first core cache. In some embodiments the cache access bins that are maintained include a bin for each unique UC TC pair. In some embodiments the method includes maintaining a database of live block counts and dead block counts for each bin. In these embodiments selecting the cache block to evict may be based at least in part upon a live block count and a dead block count for the particular cache access. In some embodiments the method may further include maintaining a shared block status per cache block where the shared block status indicates whether a second thread has access to the cache block. In these embodiments the method may further include excluding shared cache blocks from the cache access to bins once a block is identified as a shared cache block. In these embodiments the method may include excluding from the live and dead block counts shared cache blocks. In these embodiments the selection of a cache block to evict may exclude from consideration any cache block candidate that is a shared block. In at least one embodiment maintaining shared blocks status includes maintaining LTA data indicating the last thread to allocate a block. By maintaining LTA data embodiments of the method may include as shared blocks blocks that are accessed by multiple threads but which are never valid in multiple cores at any point in time. In addition to these one at a time shared blocks the shared blocks may include any blocks that have a shared block status under the MESI protocol or any other shared coherency status. In some embodiments detection of a shared block is translated into an eviction policy preference by assigning the shared block a maximum age.

In these embodiments the age field may be used to arbitrate between two candidates sharing common thread aware block prediction data characteristics.

In at least one embodiment a computer system includes a processor with a core region that has a plurality of processing cores where each processing core includes a core cache and the processor as a whole includes a shared cache that is shared among the core cache. A memory controller is integrated into a non core region of the processor and an I O hub is connected to the processor in some embodiments. In at least one embodiment the processor may include dead block prediction logic to maintain shared block data indicating when a block is shared maintain cache access data indicating how many times an unshared block is accessed associate unshared blocks with cache access bins based on the cache access data applicable to a block maintain live block counts for each cache accessibility and select blocks to evict based on the cache access data the live block counts and the dead block counts. In at least one embodiment the shared cache is a set associative multiple way cache and the selection of any blocks to evict includes selecting the block to evict from the group of blocks that share the same set.

In the following description details are set forth in conjunction with embodiments to facilitate discussion of the disclosed subject matter. It should be apparent to a person of ordinary skill in the field however that the disclosed embodiments are exemplary and not exhaustive of all possible embodiments.

Throughout this disclosure a hyphenated form of a reference numeral refers to a specific instance of an element and the un hyphenated form of the reference numeral refers to the element generically or collectively. Thus widget refers to an instance of a widget class which may be referred to collectively as widgets and any one of which may be referred to generically as a widget .

Embodiments may be implemented in many different system types and platforms. illustrate a multi processor system used in conjunction with at least one embodiment. In at least one embodiment system is a multi processor system that include a first processor and a second processor . Although some embodiments include two processors other embodiments may include more or fewer processors. In at least one embodiment processors include a core region and an integration region . In some embodiments core region includes one or more processing cores . In some embodiments integration region includes a memory controller hub MCH a shared cache sometimes referred to as a last level cache LLC a processor hub point to point interface and a processor processor point to point interface .

In at least one embodiment processing cores may each include hardware and firmware resources not depicted to support an execution pipeline. In some embodiments these resources may include a cache memory hierarchy which may include a dedicated level one L1 instruction cache a dedicated L1 data cache a level 2 L2 data instruction cache or a combination thereof prefetch logic and buffers branch prediction logic decode logic a register file various parallel execution resources including arithmetic logic units floating point units load store units address generation units a data cache and so forth.

In at least one embodiment MCH supports bidirectional transfer of data between a processor and a system memory via a memory interconnect . In some embodiments system memory may be a double data rate DDR type dynamic random access memory DRAM while memory interconnect and MCH may comply with a DDR interface specification. In some embodiments system memory may represent a bank of memory interfaces or slots that may be populated with corresponding memory circuits for a desired DRAM capacity.

In some embodiments each processor includes an MCH to communicate with a portion of system memory that is local to processor . In at least one embodiment system memory is local to processor and represents a portion of the system memory as a whole. In at least one embodiment system is a distributed memory multi processor system in which each processor can access each portion of system memory whether local or not. While local accesses may have lower latency accesses to non local portions of system memory are permitted in some embodiments.

In at least one embodiment each processor also includes a point to point interface that supports communication of information with a point to point interface of one of the other processors via an inter processor point to point interconnection . In some embodiments processor hub point to point interconnections and processor processor point to point interconnections are distinct instances of a common set of interconnections. In other embodiments point to point interconnections may differ from point to point interconnections .

In some embodiments processors include point to point interfaces to communicate via point to point interconnect with a point to point interface of an I O hub . In at least one embodiment I O hub includes a graphics interface to support bidirectional communication of data with a graphics adapter via a graphics interconnection which may be implemented as a high speed serial bus e.g. a peripheral components interface express PCIe bus or another suitable bus.

In some embodiments I O hub also communicates via an interface and a corresponding interconnection with a bus bridge hub that supports various bus protocols for different types of I O devices or peripheral devices. In at least one embodiment bus bridge hub supports a network interface controller NIC that implements a packet switched network communication protocol e.g. Gigabit Ethernet a sound card or audio adapter and a low bandwidth bus e.g. low pin count LPC I2C Industry Standard Architecture ISA to support legacy interfaces referred to herein as desktop interfaces that might include interfaces for a keyboard mouse serial port parallel port and a removable media drive. In some embodiments low bandwidth bus further includes an interface for a nonvolatile memory NVM device such as flash read only memory ROM and other low bandwidth I O devices e.g. keyboard mouse and a storage protocol bus e.g. serial AT attachment SATA small computer system interface SCSI to support persistent storage devices including conventional magnetic core hard disk drive HDD . In some embodiments HDD is illustrated as including store code which may represent processor executable instructions including operating system instructions application program instructions and so forth that when executed by the processor cause the processor to perform operations illustrated herein.

In at least one embodiment system also includes a non volatile random access memory NVRAM which may include a solid state drive a phase change RAM or another suitable device and a peripheral bus e.g. USB I2C PCI PCIe Bluetooth to support various peripheral devices including a sensor and a touchscreen controller . Although specific instances of communication busses and bus targets have been illustrated and described other embodiments may employ different communication busses and different target devices.

In some embodiments system includes an operating system that may be entirely or partially stored in HDD . In some embodiments operating system may include various modules application programming interfaces and the like that expose to varying degrees various hardware and software features of system . In at least one embodiment system includes a sensor application programming interface API a resume module a connect module and a touchscreen user interface .

In some embodiments sensor API provides application program access to one or more sensors not depicted that may be included in system . In some embodiments sensors that system might have include an accelerometer a global positioning system GPS device a gyro meter an inclinometer and an ambient light sensor. In at least one embodiment resume module may be implemented as software that when executed performs operations for reducing latency when transitioning system from a power conservation state to an operating state. Resume module may in some embodiments work in conjunction with the solid state drive SSD to reduce the amount of SSD storage required when system enters a power conservation mode. Resume module may in some embodiments flush standby and temporary memory pages before transitioning to a sleep mode. In some embodiments by reducing the amount of system memory space that system is required to preserve upon entering a low power state resume module beneficially reduces the amount of time required to perform the transition from the low power state to an operating state. In some embodiments connect module may include software instructions that when executed perform complementary functions for conserving power while reducing the amount of latency or delay associated with traditional wake up sequences. In some embodiments connect module may periodically update certain dynamic applications including email and social network applications so that when system wakes from a low power mode the applications that are often most likely to require refreshing are up to date. In at least one embodiment touchscreen user interface supports a touchscreen controller that enables user input via touchscreens traditionally reserved for handheld applications. In at least one embodiment the inclusion of touchscreen support in conjunction with support for communication devices enable system to provide features traditionally found in dedicated tablet devices as well as features found in dedicated laptop and desktop type systems.

In at least one embodiment processing cores include a core instruction cache a front end execution pipes and a core data cache . In some embodiments front end monitors an instruction pointer and based on predictions regarding program flow fetches or prefetches instructions from core instruction cache and issues instructions to execution pipes . In some embodiments execution pipes include multiple parallel pipelines including one or more floating point pipelines one or more integer arithmetic logic unit pipelines one or more branch pipelines and one or more memory access pipelines also referred to herein as load store pipelines. In some embodiments execution pipes decode instructions retrieve operands required to perform instructions and may generate micro code to process the instructions from core instruction cache may route the instructions through the appropriate execution pipeline and may store any results. In at least one embodiment execution pipes include a register file that may support register renaming speculative execution and out of order execution of instructions.

In some embodiments integration region includes an LLC and cache control logic . In this embodiment LLC is a shared cache that is shared among all of processing cores of processor . In some embodiments as suggested by its name LLC represents from the perspective of processor the last available hierarchical tier of cache memory. In at least one embodiment if a memory access instruction that is presented to LLC generates a cache miss the requested data must be retrieved from system memory .

In at least one embodiment processing core and or integration region may include one or more levels of a cache hierarchy between core caches and LLC . In at least one embodiment processing core includes a cache memory intermediate between core caches and LLC . Processing core may include in some embodiments an intermediate tier cache memory not shown hierarchically located between core caches and LLC . In at least one embodiment each of the cache memories of processing core may have a unique architectural configuration. In at least one embodiment core data cache and LLC are both multiple way set associative caches. In some embodiments LLC is inclusive with respect to core data cache while in other embodiments LLC may be non inclusive with respect to core data cache .

In some embodiments cache control logic controls access to the cache memories enforces a coherency policy implements a replacement policy and monitors memory access requests from external agents e.g. other processors or I O devices. In at least one embodiment LLC and core caches comply with an MESI protocol or a modified MESI protocol. The four states of the MESI protocol are described in Table 1.

A modified MESI protocol could include in some embodiments an additional state the F state identifying one of a plurality of S state lines where the F state block is designated as the block to forward the applicable data should an additional request for the data be received e.g. from a processor that does not have the data.

In at least one embodiment integration region of processor also includes power management unit to control power provided to the various resources of processor . In some embodiments power management unit provides unique power supply levels to core region and integration region . In other embodiments power management unit may be further operable to provide unique power supply levels to each processing core and or provide clock signals at unique frequencies to processing cores . In addition in some embodiments power management unit may implement various power states for processor and define events that produce power state transitions.

In some embodiments integration region includes graphics accelerator to support low latency high bandwidth communication with a display device not depicted . In at least one embodiment graphics accelerator may be integrated into processor which represents an alternative to embodiments in which communication with graphics adapter is implemented in the I O hub .

In at least one embodiment integration region includes an I O interface to support communication with one or more chipset devices discreet bus interfaces and or individual I O devices. In some embodiments I O interface provides one or more point to point interfaces such as interfaces and . In other embodiments I O interface may provide an interface to a shared bus to which one or more other processors may also connect.

In at least one embodiment a cache block from core data cache and a cache block from shared cache are included. In some embodiments core data cache block includes cache data an associated cache tag and block status indicators . In an analogous manner in some embodiments cache block in shared cache includes cache data a corresponding cache tag and block status indicators .

In at least one embodiment block status indicators include an LTA bit a UC field a TC field a coherency state CS field a recently used field referred to herein simply as age field and a shared bit . Similarly in some embodiments the block status indicators of cache block include an LTA bit a UC field a TC field a CS field a recently used field referred to herein simply as age field and a shared bit . Although in some embodiments core cache blocks and shared cache blocks having substantially the same set of block status indicators are included the core cache block status indicators may differ from the shared cache block status indicators in other embodiments.

In some embodiments LTA bit indicates the last thread to allocate the block in the shared cache. In at least one embodiment although LTA bit may be included in the block status indicators of core cache block the value of LTA bit is determined at the time a block is evicted from core data cache and allocated in a non inclusive cache or updated in an inclusive cache. In at least one embodiment in which the LTA bit is retained when the block is recalled to core data cache accommodates embodiments in which shared cache is exclusive of or noninclusive of core data cache . If shared cache LLC is inclusive of core data cache it may be permissible to omit LTA bit from the block status indicators of core data cache .

In at least one embodiment the LTA information whether referring to LTA bit in core cache block or LTA bit in shared cache is useful in conjunction with shared bit to indicate whether the corresponding block has been accessed by more than one thread during its cache memory lifecycle. In some embodiments during its cache memory lifecycle a block may move between shared cache and core data cache . If a core data cache block is recalled from shared cache by a first thread and then gets evicted from core data cache back to shared cache the block may in some embodiments be recalled by a second thread later. In this situation the TC may in at least one embodiment indicate two distinct trips from shared cache to core data cache but the information would be inaccurate with respect to any single thread. In at least one of the thread aware embodiments illustrated herein the detection of the block as a shared block through the use of LTA bit and or LTA bit would result.

In at least one embodiment when the block was evicted from core data cache of a first processing core LTA bit in shared cache was written with a value indicating the first processing core as the core that allocated the block in shared cache i.e. the core that last evicted the block. In some embodiments as illustrated in share logic receives thread ID signal from an execution pipe or other source indicating the thread that is currently executing. In some embodiments thread ID information may be used to record last to allocate information in LTA bit . In embodiments where there is a 1 1 correspondence between executing threads and processing cores share logic may simply record an identifying number of the core data cache in lieu of a thread indicator.

In at least one embodiment share logic also receives CS . In some embodiments when an eviction occurs CS transitions to invalid and share logic may record thread ID signal or processor core information not depicted in UC or provide the information to shared cache for storage in LTA bit . In addition in some embodiments share logic receives LTA bit from shared cache . From this information share logic may in some embodiments determine that the previous value of LTA bit differs from the thread ID signal of the currently executing thread or the core data cache allocating the block in shared cache and asserts the shared bit in core data cache as well as the shared bit in shared cache . In these embodiments a block is permanently identified as a shared block via CS bit in cache block or CS in shared block when it is determined that two different core caches i.e. two or more different threads have accessed the block in their respective cores. Thus some embodiments may identify a block as shared even if that block was never valid in more than one core cache at any given moment in its cache memory lifecycle.

In at least one embodiment cache memory subsystem gives shared blocks preferential treatment with respect to eviction from shared cache . In some embodiments this preference is achieved at least in part by assigning shared blocks an age field that reduces the probability of the cache block being evicted with respect to other blocks. In at least one embodiment share logic sets age field with the maximum age used by the cache memory subsystem for incrementing recency based selection and eviction policies.

In at least one embodiment the dead block predictor logic further includes UC logic . In at least one embodiment UC logic receives a hit miss signal from core data cache indicating a tag access that hits or misses in core data cache . In addition UC logic receives CS bit in some embodiments. In some embodiments if UC logic detects a cache hit for a block that is valid as indicated by CS bit UC logic increments UC field in core data cache .

In some embodiments dead block predictor logic further includes TC logic . In at least one embodiment TC logic receives hit miss signal from core data cache and a hit miss signal from shared cache . In some embodiments when a cache access generates a miss in core data cache and a hit in shared cache the block will be recalled from shared cache to core data cache constituting a new trip and TC logic will increment the TC field in shared cache .

In some embodiments bin logic is included to control information stored in a cache access bin table . In at least one embodiment bin logic performs or maintains cache access bin table by updating live and dead block count fields and for each cache access bin entry . Dead block predictor logic updates in some embodiments information in cache access bin table when a change in a block s UC TC pair is detected and when a block is evicted from shared cache .

In at least one embodiment when UC logic or TC logic increments the UC or TC bits of a cache block the block moves from one cache access bin to another. For purposes of this discussion a cache access bin corresponds to a UC TC pair. If a 0 1 block i.e. a block that has a UC TC pair of 0 1 is accessed while valid in core data cache the block s UC value will in some embodiments increment and the block will effectively relocate from cache access bin 0 1 to cache access bin 1 1 . Cache access block table may then be updated by bin logic to decrement the live count for cache access bin 0 1 and increment the live block count for cache access bin 1 1 . Similarly when a cache block is evicted from a shared cache the eviction is reported to bin logic via an eviction signal and bin logic will increment the dead block count field for the applicable cache access bin entry and decrement the applicable live block count field for the same cache access bin entry . To illustrate if any 1 1 cache block is evicted from shared cache bin logic updates bin table by decrementing the live block count field for cache access block 1 1 and incrementing the dead block count field for the same cache access bin 1 1 in some embodiments.

In at least one embodiment a second table includes the cumulative live block count and dead block count for each bin. In at least one embodiment eviction policy for cache memory subsystem is influenced by the cache access bin live block count and the account values. Qualitatively a cache access bin that has a dead block count significantly larger than its live block count but which also has a substantial live block count may be the best candidate for a dead block prediction. However while predicting dead blocks based solely on live block count and dead counts may be desirable for its relative simplicity other embodiments may store and analyze more detailed profiling of the cache access bin status when they block is predicted. Thus although and illustrate cache accessible information that may be used in conjunction with at least one embodiment and that may be preserved and employed to improve the selection or prediction of a dead block it will be appreciated that other embodiments may include more less or different cache access data.

In at least one embodiment method is initiated by the cache controller of a processor in a multi processor system to detect a cache block event operation . In some embodiments a predetermined shared cache eviction policy is utilized to determine how many times a cache block is accessed in a private cache and how many times the cache block is recalled from a shared cache to the private cache during a tenure of the cache block. In at least one embodiment operation determines if the cache block is a shared block. In some embodiments if a determination is made that the cache block is a shared block the block is identified operation as a shared block and is excluded from the eviction policy and from cache block access data gathering and returns to operation to monitor the cache for cache block events. Otherwise operation continues on in some embodiments to operation where a determination is made of an unshared access event such as a TC or UC. In some embodiments if a determination is made that an event occurred the thread aware access data is updated operation for this cache block with the UC TC value and then continues to operation to monitor the caches for cache block events. In some embodiments if a determination is made that no access event occurred operation continues onto operation where a determination is made if a transaction requiring a cache block eviction from the shared cache will occur. In at least one embodiment if an eviction occurs the eviction database is updated operation with the UC TC value and continues to operation to monitor the cache for cache block events. If an eviction does not occur in operation then in some embodiments a determination is made between the relationship between UC TC values and evictions operation and then proceeds to modify the shared cache eviction policy operation based at least in part on the relationship determined in operation .

In some embodiments method is initiated by the cache controller of a processor in a multi processor system to maintain LLC eviction policies operation based at least in part on a relationship determined by the UC TC values and the eviction occurrence. In at least one embodiment method continues to assign a higher priority to shared block over non shared or private blocks so that the shared blocks are less likely to be evicted operation . In some embodiments prioritizing a shared block includes assigning a maximum predetermined age to the block before insertion in the LLC. In at least one embodiment the method continues where the LLC eviction victims are chosen using the LLC eviction policy operation . In some embodiments in operation the thread aware dead block predictors continue to make decisions for private blocks.

Additionally a circuit level model with logic and or transistor gates may be produced at some stages of the design process. This model may be similarly simulated sometimes by dedicated hardware simulators that form the model using programmable logic. This type of simulation taken a degree further may be an emulation technique. In any case re configurable hardware is another embodiment that may involve a tangible machine readable medium storing a model employing the disclosed techniques.

Furthermore most designs at some stage reach a level of data representing the physical placement of various devices in the hardware model. In the case where conventional semiconductor fabrication techniques are used the data representing the hardware model may be the data specifying the presence or absence of various features on different mask layers for masks used to produce the integrated circuit. Again this data representing the integrated circuit embodies the techniques disclosed in that the circuitry or logic in the data can be simulated or fabricated to perform these techniques.

In any representation of the design the data may be stored in any form of a tangible machine readable medium. In some embodiments an optical or electrical wave modulated or otherwise generated to transmit such information a memory or a magnetic or optical storage such as a disc may be the tangible machine readable medium. Any of these mediums may carry the design information. The term carry e.g. a tangible machine readable medium carrying information thus covers information stored on a storage device or information encoded or modulated into or on to a carrier wave. The set of bits describing the design or the particular part of the design are when embodied in a machine readable medium such as a carrier or storage medium an article that may be sold in and of itself or used by others for further design or fabrication.

Embodiment 1 is a processor comprising a plurality of processing cores a cache memory subsystem comprising a plurality of core caches each of the core caches corresponding to one of the processing cores a shared cache common to each of the processing cores trip count TC logic to increment a thread aware trip count for a block recalled from the shared cache to a first core cache use count UC logic to increment a UC for a block responsive to each access of the block in the first core cache shared block logic to set a shared block bit for a block accessed by multiple core caches bin logic to maintain live block counts and dead block counts for each bin wherein a bin corresponds to a UC TC pair a live block count indicates a number of valid unshared blocks associated with a bin and a dead block count indicates a number of blocks evicted from a corresponding bin and eviction logic to select a shared cache block to evict from a plurality of eviction candidates based on at least one of trip count values UC values live bin counts and dead bin counts.

In embodiment 2 the bin logic included in the subject matter of embodiment 1 is optionally operable to increment a live block count for a first bin and decrement a live block count for a second bin responsive to detecting a change in UC TC pair for a cache block and increment a dead block count for a third bin responsive to detecting an eviction of a cache block associated with the third bin.

In embodiment 3 the subject matter of embodiment 1 can optionally include last to allocate LTA logic to set an LTA field for a block responsive to a first thread allocating the block in the shared cache wherein the LTA field identifies the first thread.

In embodiment 4 the shared block logic included in the subject matter of embodiment 3 can optionally include sets the shared block bit responsive to detecting the LTA field identifying a second thread.

In embodiment 5 the subject matter of embodiment 4 can optionally include age logic to set maximum value in an age field of the block responsive to detecting the setting of the shared block bit.

In embodiment 6 the eviction logic included in the subject matter of embodiment 5 is optionally operable to select the block to evict based on a value in the age field of a block responsive to identifying multiple eviction candidates from the same bin.

Embodiment 7 is a cache memory eviction method comprising maintaining thread aware cache access data per cache block in a cache memory wherein the cache access data is indicative of a number of times a cache block is accessed by a first thread and associating a cache block with one of a plurality of bins based on cache access data values of the cache block selecting a cache block to evict from a plurality of cache block candidates based at least in part upon the bins with which the cache block candidates are associated.

In embodiment 8 the cache memory included in the subject matter of embodiment 7 can optionally include a shared cache in a cache memory hierarchy and the cache access data included in the subject matter of embodiment 7 can optionally include trip count TC data indicative of a number of times a cache block is forwarded from the shared cache to a core cache.

In embodiment 9 the cache access data included in the subject matter of embodiment 8 can optionally include use count UC data indicative of a number of times the cache block was accessed by the first thread while residing in the core cache.

In embodiment 10 the plurality of cache access bins included in the subject matter of embodiment 9 can optionally include bins corresponding each unique UC TC pair.

In embodiment 11 the subject matter of embodiment 10 can optionally include maintaining a database of live block counts and dead block counts for each bin and the selecting of the cache block to evict included in the subject matter of embodiment 10 is optionally based at least in part upon the live block counts and dead block counts for the cache access.

In embodiment 12 the subject matter of embodiment 7 can optionally include maintaining a shared block status per cache block and the shared block status included in the subject matter of embodiment 8 is optionally indicative of whether a second thread accessed the cache block.

In embodiment 13 the subject matter of embodiment 12 can optionally include excluding shared cache blocks from the bins responsive to identifying the cache block as a shared cache block.

In embodiment 14 the selecting the cache block to evict included in the subject matter of embodiment 12 can optionally include excluding from the plurality of cache block candidates shared cache blocks.

In embodiment 15 the maintaining the shared block status included in the subject matter of embodiment 12 can optionally include maintaining last thread data indicative of a last thread to allocate the cache block.

In embodiment 16 the subject matter of embodiment 12 can optionally include assigning a maximum value to an age attribute of a shared cache block allocated in the shared cache responsive to evicting the shared cache block from the core cache.

Embodiment 17 is a computer system comprising a processor including a core region including a plurality of processing cores each processing core including a core cache and a shared cache shared among the core caches a memory controller integrated in an uncore region of the processor and an I O hub connected to the processor wherein the processor includes dead block prediction logic to maintain shared block bits indicative of when a block is shared maintain cache access data indicative of a number of times an unshared block is accessed associate unshared blocks with bins based on their cache access data maintain live block counts and dead block counts for each bin and select blocks to evict based on the cache access data the live block counts and the dead block counts.

In embodiment 18 the dead block prediction logic included in the subject matter of embodiment 17 is optionally operable to select a block to evict from a set of candidate blocks comprising all valid blocks associated with a first set in the shared cache.

In embodiment 19 the cache access data included in the subject matter of embodiment 17 can optionally include use count data indicative of a number of times an unshared block is accessed in the core cache and trip count data indicative of a number of times an unshared block is recalled from the shared cache to a first core cache.

In embodiment 20 the dead block prediction logic included in the subject matter of embodiment 17 is optionally operable to evict unshared blocks preferentially to shared blocks.

In embodiment 21 the subject matter of embodiment can optionally include first storage to store an operating system an I O hub to interface to the processor and an I O device to interface to the I O hub wherein the I O device is selected from a touchscreen controller a solid state drive and a sensor.

In embodiment 22 the operating system included in the subject matter of embodiment 21 can optionally include processor executable resume module instructions to reduce latency associated with transitioning out of a power conservation state and processor executable connect module instructions to maintain a currency of a dynamic application during the power conservation state.

In embodiment 23 the subject matter of any one of embodiments 1 or 2 can optionally include last to allocate LTA logic to set an LTA field for a block responsive to a first thread allocating the block in the shared cache wherein the LTA field identifies the first thread.

In embodiment 24 the cache memory eviction method included in the subject matter of any one of embodiments 8 9 10 or 11 can optionally include maintaining a shared block status per cache block wherein the shared block status is indicative of whether a second thread accessed the cache block.

In embodiment 25 the subject matter of any one of embodiments 17 18 19 or 20 can optionally include first storage to store an operating system an I O hub to interface to the processor and an I O device to interface to the I O hub wherein the I O device is selected from a touchscreen controller a solid state drive and a sensor.

To the maximum extent allowed by law the scope of the present disclosure is to be determined by the broadest permissible interpretation of the following claims and their equivalents and shall not be restricted or limited to the specific embodiments described in the foregoing detailed description.

