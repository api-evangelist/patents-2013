---

title: Exposing method related data calls during testing in an event driven, multichannel architecture
abstract: Approaches for application testing are provided. An approach includes transmitting a command to an agent associated with an application installed on a mobile device. The approach includes receiving from the agent information about an executed step of the application on the mobile device. The approach includes comparing the information to an expected value of the step. The approach includes display an output based on the comparing.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09336127&OS=09336127&RS=09336127
owner: KONY, INC.
number: 09336127
owner_city: Orlando
owner_country: US
publication_date: 20131206
---
The present invention relates generally to mobile applications and more particularly to systems and method of testing mobile applications.

Mobile application development is the process by which application software is developed for handheld devices such as personal digital assistants enterprise digital assistants mobile phones e.g. smart phones tablet computers etc. Mobile applications e.g. apps can be pre installed on devices during manufacturing downloaded by customers from various mobile software distribution platforms or delivered as web applications using server side or client side processing to provide an application like experience within a web browser.

It is common for an application developer to test an application before releasing the application to the end user. Testing may be used to find and fix errors in an application so that once released the end user has a positive user experience when using the application. Mobile applications may be initially tested within the development environment using emulators and later subjected to field testing. Emulators provide an inexpensive way to test applications on mobile devices to which developers may not have physical access. Many aspects of application testing are performed manually which leads to the application testing process as a whole being time consuming and expensive.

In accordance with aspects of the invention there is a method of application testing that includes transmitting a command to an agent associated with an application installed on a mobile device. The method also includes receiving from the agent information about an executed step of the application on the mobile device. The method additionally includes comparing the information to an expected value of the step. The method further includes display an output based on the comparing. The transmitting the receiving the comparing and the displaying are performed by a computing device.

In accordance with additional aspects of the invention there is a system for testing applications. The system includes a test server comprising a test control is adapted to record a test case of a mobile application remotely control running the test case on a mobile device receive information about execution of the mobile application during the test case on the mobile device compare the information to an expected result and display an output based on the comparing.

In accordance with further aspects of the invention there is a computer system for testing an application the system including a CPU a computer readable memory and a computer readable storage media. The system includes program instructions to communicate with an agent loaded on a mobile device to remotely control running a test case of the application on the mobile device. The system also includes program instructions to receive from the agent loaded on the mobile device information associated with a function called by the application during the test case on the mobile device wherein the information comprises a name of the function an input to the function and a return of the function based on the input. The system further includes program instructions to display the information and a screenshot of the mobile device associated with the function. The program instructions are stored on the computer readable storage media for execution by the CPU via the computer readable memory. The receiving and the displaying are performed at a test server that is separate from the mobile device.

The present invention relates generally to mobile applications and more particularly to systems and methods of testing mobile applications. More particularly the invention facilitates testing an application running on a mobile device by leveraging the use of an agent installed on the mobile device or the use of a javascript library for web based applications. In embodiments the agent or script referred to herein simply as the agent provides a mechanism for exposing methods and data within the application that are associated with events the application performs while running on the mobile device. In this manner implementations of the invention provide a concise view of the testing steps that occur along with the resulting view created by the event when the application is being tested.

In accordance with aspects of the invention the agent resides on the mobile device and acts as a reporter for the application that is running on the mobile device. In embodiments the agent reports information associated with the application to a test server which may be part of a test management system. The information may include for example but is not limited to application performance memory usage visual layout called methods inputs and outputs from the screen and any variables the developer marks for introspection e.g. monitoring . In embodiments the server receives the information from the agent and performs any desired comparisons and testing logic e.g. validation for testing the application in accordance with aspects of the present invention. According to aspects of the invention the output of each step highlights the screen shot of the resulting event the inputs and outputs from the screen of the mobile device the functions called by the application and any variables set out by the developer to watch e.g. monitor .

More specifically implementations of the invention provide a developer a comprehensive view of what happens when an event occurs in an application. This is accomplished by not only providing a screen shot and high level inputs and outputs of the event but by also allowing the developer to see what underlying methods were called by the event and the inputs and returns of these methods. By way of example in embodiments a library is injected to the application code and is configured to inspect what methods are called when an event is fired in the application. Also in aspects of the invention variables can be marked by the developer to watch e.g. monitor in the testing process and then these variables can be correlated to a method being fired in the application. In addition constraints can be set on an introspected e.g. monitored variable such as for example regular expressions determine if the value is undefined determine if the value is defined determine if a value is null determine if the resulting value is true false equal greater than less than greater than or equal to and less than or equal to. Aspects of the invention may also be adapted to support close to comparisons for comparing floating point values e.g. a value of 3.14159 is close to 3.14. Also aspects of the invention allow a developer to determine when exceptions have been thrown as expected.

It should be understood that browser based testing can have multiple challenges in a mobile and desktop environment since there is a wide range of browsers to support. For example code introspection and remote controlling of a multiple browsers is a difficult task with ongoing maintenance concerns as well as noted differences in which browsers can report an object that is in the same location. For example where pixel comparison may show the pages are equal layout comparison may show a multipixel difference in location height or width. Also there is an inherent complexity of taking screenshots on each platform. Although the functions may overall stay the same there may be presentation nuances that are presented when tests are run across channels. In view of these and other issues the present invention provides a testing system which provides many advantages over current manual processes as described herein.

Implementations of the invention may be directed to automating aspects of application testing. By implementing the automated aspects of the present invention testing throughout the software lifecycle is greatly improved. In addition it is now possible to improve visibility into backlog through defect tracking system integration as well as reduce the number of testers needed to perform testing and reduce errors introduced by manual testing. It is also now possible to automate repeatable processes for consistency allowing testers to use test cases as building blocks for larger test scenarios provide deep coverage in all areas of the application under test and produce automated reports based on testing process.

According to aspects of the invention a multichannel automated testing solution focuses on one or more of the following building blocks of testing unit testing functional testing data testing and user experience testing. These different testing techniques in accordance with aspects of the present invention are described below in detail.

Unit testing provides developers with a mechanism for testing a unit of code of an application to detect problems before the application is passed to the tester. In embodiments unit testing tests individual functions in isolation with the ability to set preconditions and post conditions. Unit testing also allows grouping smaller tests into larger scenarios. Unit testing also includes tolerance testing regular expression and range function testing. According to aspects of the invention application developers can unit test locally and store the unit test within a test server. In embodiments aspects of unit testing may be automated by integrating unit testing into the application build process e.g. build management system with application programming interfaces API s that can be called from software.

Functional testing provides testers with the ability to validate expected navigation and behaviors of the application. In embodiments functional testing monitors the behavior of the application to ensure the application displays the correct outputs navigates correctly and correctly responds to swipes taps inputs and other gestures for example. In this manner functional testing tests the inputs and outputs of the application from the glass or screen perspective of the mobile device on which the application is run.

Data testing ensures that the information used for testing is correct. In embodiments data testing includes testing of services and data throughout the application lifecycle. Initial data testing provides a mechanism for ensuring data quality with back end systems. Other aspects of data testing called introspection focus on what methods are called by the application being tested what data is passed between methods and the order in which these methods are called. In this manner data testing ensures that data from external services is correct and gives developers insight into key data points within their application in relation to the testing steps and process.

User experience UX testing provides a mechanism for testing the overall user experience through the use of layout validation in conjunction with pixel comparison. In embodiments UX testing ensures that objects display correctly on the mobile device screen including ensuring that text boxes labels and other user interface components are properly displayed in a correct format. In this manner implementations of the invention create a robust automated testing mechanism that can compare dynamic repeating content as well as understand specific areas of both layout and pixel comparison.

For example UX testing compares objects on the view and their properties such as height width location on the page and other widget level properties and can create repeating patterns that recognize when visual objects repeat on the page so the testing automation system can test complex repeating user interfaces. The UX testing can also test static areas of a view using pixel comparison which allows a tester to select a specific area for image comparison or detect text overruns as examples.

An exemplary application testing scenario in accordance with aspects of the invention is as follows. First a developer creates an application sets the application to test mode and publishes the application to a testing application store. Next a tester e.g. one or more users in a test team downloads the application from the testing application store to one or more mobile devices. In embodiments the tester records one or more new test cases by e.g. logging on to a test workbench e.g. a software interface of a test server adapted for among other things software based control of test applications installed on mobile devices opening a recording dialog selecting the mobile device with the application installed beginning recording the new script and manipulating the application using the device while the test system is recording. At each page e.g. different screen of the application on the mobile device the test server saves a screenshot of the view on the mobile device saves any objects on the page and their layout properties and saves any inputs to and outputs from methods of the application.

Still referring to the exemplary application testing scenario in accordance with aspects of the invention the tester may edit the recorded test case prior to performing functional and or UX testing. For example the tester may open the recorded test case using a test workbench look at each view that was produced in the test case and mark objects from one or more views for validation. The test workbench may be programmed with default values for some validation fields. In other validation fields the tester may define or import one or more values e.g. values such as username and password which may change with each test run. For example the tester may import a spreadsheet from another device and assign columns in the spreadsheet to a field of the test case for dynamic validation. The tester may also change the timeout of one or more back end calls e.g. to account for back end server performance. The tester may also change the time to think that was recorded by the test server when the test case was recorded e.g. to make the test run faster between screens and or applications selections that do not require back end service calls. The tester may save the edited test case at the test server for later running tests of the application on one or more mobile devices. When a test case is recorded it captures the values of that were associated with the object. For instance if an object s test.properties Andrew was here the default value after the test is recorded is Andrew was here . The tester may override this text with new text using the text workbench.

With continued reference to the exemplary application testing scenario in accordance with aspects of the invention the tester may perform functional testing and or UX testing using the saved test case including automated scripts. In embodiments functional testing and UX testing may each include selecting a saved test case from a testing menu of the test workbench using the test workbench to select a device on which to run the test running the test on the mobile device from the test server recording the results of the test at the test server and checking the results of the test. Functional testing may specifically include the test server recording at each step of the test for each mobile device a screen shot of the application at the mobile device any functions called by the application including the order in which the functions were called and any variables the tester set for watches in the application code. UX testing may specifically include the test server receiving layout information from the mobile device running the application comparing the received layout information to layout information that was stored when recording the test case and marking as possible defects those filed that do not match.

The present invention may be embodied as a system method or computer program product. The present invention may take the form of a hardware embodiment a software embodiment or a combination of software and hardware. Furthermore the present invention may take the form of a computer program product embodied in a computer readable storage medium of expression having computer usable program code embodied in the medium. The computer usable or computer readable medium may be any medium apparatus or device that can contain store and communicate the program for use by or in connection with an instruction execution system apparatus or device. The computer readable storage medium may be for example an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device.

The server includes a computing device which can be resident on a network infrastructure or computing device. The computing device includes a processor e.g. a CPU a memory A an I O interface and a bus . The bus provides a communications link between each of the components in the computing device . In addition computing device includes a random access memory RAM a read only memory ROM and an operating system O S . The computing device is in communication with external I O device resource and a storage system B. The I O device can comprise any device that enables an individual to interact with the computing device e.g. user interface or any device that enables the computing device to communicate with one or more other computing devices e.g. devices etc. using any type of communications link. The storage system B can store data such as one or more databases which can be maintained for example by an enterprise administrator.

The processor executes computer program code e.g. program control which can be stored in memory A and or storage system B. In embodiments the program control controls a test control which comprises an application that is adapted to perform one or more of the processes described herein. The test control can be implemented as one or more program code in program control stored in memory A as separate or combined modules. Additionally test control may be implemented as separate dedicated processors or a single or several processors to provide the functions described herein. While executing the computer program code processor can read and or write data to from memory A storage system B and or I O interface . In this manner the program code executes the processes of the invention.

According to aspects of the invention test control communicates with one or more mobile devices . . . each of which has a copy of an application to be tested. As used herein a mobile device refers to a smart phone tablet computer personal digital assistant or similar device that includes a mobile operating system wireless communication antenna processor user interface memory etc. In embodiments an agent resides on each mobile device and reports information about application to test control when application is running. More specifically in embodiments agent is configured to remote control mobile device and by adding an agent to the device it is now possible to capture performance data of the device e.g. application and send this application information to the computing device e.g. test control. In embodiments test control and more specifically computing device may be implemented as or on a testing integrated development environment IDE . In this way the agent can be added to an application that sends native object information to the testing IDE including the dimensions and location of the object the native ID the class and the text associated with the object amongst other functionality and features.

In aspects of the invention test control may use agent to remotely control the mobile devices for testing purposes. The agent may be one or more scripts or other suitable programming and may be added to application when the application code is compiled. While application is running on one of mobile devices agent sends native object information to test control such as for example an identification of methods called by application data inputs and data outputs associated with each called method user inputs e.g. taps clicks swipes etc. performance data from the device dimensions and locations of objects on the mobile device screen and the native ID the class and the text associated with each object amongst other features and combinations thereof.

The program code can include computer program instructions which are stored in a computer readable storage medium. The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer. Moreover any methods provided herein in the form of flowcharts block diagrams or otherwise may be implemented using the computer program instructions implemented on the computer readable storage medium.

The computer readable storage medium may be for example an electronic magnetic optical electromagnetic infrared and or semiconductor system. More specific examples a non exhaustive list would include a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any combination thereof. Accordingly the computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device of the present invention.

Still referring to the various devices of the system may communicate via a network which may be one or more conventional communication network types including but not limited to Internet local area network LAN WiFi etc. In one exemplary non limiting environment test app store is a storage node in a cloud environment and server communicates with devices locally e.g. through LAN or WiFi.

More specifically in embodiments UI includes a play button record button stop button and rewind button that control the recording and playback of a test case . In aspects of the invention the UI has an Objects library that displays the present step in the test case the view generated e.g. at the mobile device running the application during the present step and objects reported by the mobile device for the view . In embodiments the UI includes a validation area that shows components validated in the test case a validation type for each component an action associated with each component arguments if any associated with each component and a think time and timeout time if any associated with each component.

With continued reference to one or more of the components may be defined by the developer e.g. as variables that should be watched during testing. For such developer defined components at least one of the validation type action arguments think time and time out may be pre defined with default values e.g. defined in the test libraries of the application code. In embodiments the UI is configured to permit a user to change one or more of the default values e.g. by altering the data shown in a particular field e.g. field e.g. by placing a cursor in a field and entering a new value e.g. by typing pasting etc. . In aspects of the invention the UI is adapted to permit a user to add an object from the Objects library to the component validation area in order to add validations to the test case .

In embodiments the UI may also display a list of mobile devices that are currently connected to the test server e.g. server via the test agent in the particular application. Each mobile device in the list may be associated with a selection field that permits a user to select the particular mobile device for either recording or running the test case. In this manner a test case may be recorded from a first mobile device and run on a second different mobile device. In aspects of the invention the application and the test case are programmed with logic that permits use on different native platforms e.g. different mobile operating systems and devices such that a test case may be recorded from a first device having a first operating system and then run on a second device having a second operating system different than the first operating system.

In embodiments after a test case is recorded and saved using the test workbench e.g. using the UI of the test case may be run against an application on a mobile device. When a tester runs a test case e.g. by selecting a device from list and hitting the play button the test control e.g. test control sends execution commands to the application e.g. application via the agent e.g. agent at the mobile device e.g. mobile device . The execution commands cause the application to step through the test case e.g. test case in which case the test server e.g. server is remotely controlling the application during testing.

In embodiments while the application is stepping through the test case the agent e.g. agent at the mobile device acts as a reporter for the application by communicating back to the test control information such as application performance memory usage visual layout methods being called and inputs and outputs from the screen of the mobile device in conjunction with any variables included in the test case amongst other test control information. The test control receives the information from the agent and performs comparisons and or testing logic as defined in the validations of the test case e.g. at validation area . The test control then displays an output of the test case based on the information from the agent and the results of any comparisons and or testing logic defined in the validations of the test case.

In embodiments the screenshot provides the tester with a screen shot of the resulting function. In embodiments the failures displays a component name e.g. as defined in field of test case the expected value of the component e.g. as defined in field of test case and the actual value of the component when the application was run on the mobile device e.g. as reported by the agent . In this manner the output permits a tester to quickly find failures on the screen by validating objects in the view such as input boxes and text.

In embodiments of when a tester wants to record a test case the tester can select the device they want to record it on from a testing workbench. After selecting the connected device the tester can select a record button on the testing workbench. The agent sends the properties of each native object displayed on the view. When a user performs an event such as a button tap the agent sends the information about what event was performed and the object it was performed against to the TMS. When a new view is displayed as the result of an event the objects within the view are sent to the TMS. The TMS records all the objects for each view for later editing. The agent also sends a screenshot of the view to the TMS. When the tester completes the recording process a stop button is selected on the testing workbench. The TMS can then send a stop request to the agent which stops sending recording data to the Test Management Server. In this way the test recorded to the workbench is provided in a step by step format with the test recorded in for example javascript format allowing the user to manipulate the actual scripting. Also advantageously the object data from each page as well as each action performed is sent to the TMS. Thus all inputs recorded are sent to the TMS.

More specifically at step the tester selects a device with which to work e.g. via list of UI . At step the tester presses the record button e.g. button of UI and the TMS begins recording the test case. During recording the agent sends to the TMS the functional data of the application e.g. methods called method inputs method returns etc. . For example when a user performs an action at step such as a button tap swipe textbox input etc. then at step the agent sends to the TMS information about what application event was performed and the application object it was performed against. At step the agent sends to the TMS a screenshot of each application view when the view changes. This gives the tester a reference point to see when an event or input was received and what effect it had on the view. At step the agent sends to the TMS each object within the view. The TMS records all the functional data and objects for each view for later editing. At step the tester presses the stop button e.g. button of UI and the TMS stops recording the test case. At step the TMS sends a stop request to the agent telling the agent to stop sending data to the TMS.

In embodiments using the processes of the tester retrieves a test case from the test repository into the TMS. The tester pulls an object from the object store that was recorded with the test case and places it on the desired step. The tester then selects to validate the object from a drop down menu for example. The object loads the recorded properties into a view on the screen the tester selects multiple runs in the properties and selects a CSV file from their local device assigning a column of the spreadsheet to the input field for multi run validation. In embodiments the tester can edit a test case they just finished recording and or edit a validator to check for a single response.

More specifically at step the tester loads a stored test case from a test repository e.g. storage system B into the test workbench e.g. UI . At step the tester pulls an object from the object library that was recorded with the test case e.g. object library and places the object at a desired location in the list of validations e.g. validations . Step may be performed in a manner similar to that described with respect to . In the event the object is associated with static data then at step the tester indicates confirms static data e.g. at field of UI and enters any other desired data in fields associated with the object e.g. fields . In the event the object is associated with dynamic data then step the tester indicates dynamic data e.g. at field indicates multiple validations e.g. at field and selects a spreadsheet file from their local device and assigns a column of the spreadsheet to the input field e.g. field for a multi run validation. At step the tester saves the test case using the UI and the TMS saves the test case at the test repository.

In embodiments the tester can use the test workbench to test a range of tolerances search for a specific word or other validation by editing the underlying script that is created by the recording process. By way of example the tester can open the test case up and select to edit a script. Once the script is opened the test can add a validation to compare an object to a range regular expression or other validation summarized herein. When a test case is saved after editing the test script is checked to insure the syntax is correct. For example 

More specifically at step the tester opens a saved test case e.g. similar to step and chooses to edit the script e.g. via UI . Selecting to edit the script in the UI automatically changes the UI window to a JavaScript editor instead of the step drive view shown in . As such at step the TMS shows the script in a script editor. At step the tester manually edits the script by adding a validation to for example compare an object to a range determine an exact equality determine if a value is undefined determine if a value is defined determine if a value is null determine if a value is true or false determine if a value greater than less than greater than or equal to less than or equal to another value determine if a value is close to another value e.g. a result is close to 3.14 determine when exceptions have been thrown as expected etc. At step the tester saves the script. At step the TMS validates the stored script to determine whether syntax is correct. When the syntax is correct then at step the TMS saves the edited script to the test repository. When the syntax has an error then at step the TMS displays the error to the tester via the UI.

In embodiments the tester selects a test or group of test to run and the devices that they want to execute functional testing on. The tester selects play for the test cases and the TSM executes the test cases in the order they are selected. The TSM sends the testing step to the agent and the Agent performs the event and sends back the view of the page after the event is completed. The agent also sends any introspected data and UI inputs and outputs for comparison by the TSM. The TSM can then compare the expected results of the event with the actual results sent by the agent. If an expected result does not match an actual result then the step fails. When the test case is complete the results of the test case are sent to the reporting engine along with the time the test case ran the name of the test case and the name of the test scenario if declared for example.

In embodiments the processes and systems of the present invention are also configured for the following additional alternative actions 

More specifically at step the TMS retrieves a list of one or more saved test cases from the test repository e.g. based on tester log in credentials. At step the TMS shows the one or more test cases to the tester via the UI e.g. UI . At step the tester selects a test case from the one or more test cases e.g. using the UI . At step the TMS loads the selected test case from the test repository. At step the TMS displays the loaded test case to the tester via the UI. At step the tester uses the UI to play the test case on one or more devices. In embodiments step includes the tester selecting one or more devices e.g. via list and then pressing the play button e.g. button .

Still referring to at step the TMS communicates the next test step e.g. as defined by the loaded test case to the agent at the device. At step the agent causes the application to perform the received test step and the agent communicates the result of the test step to the TSM. Step may include for example the agent performing an event and sending to the TSM a view of the page after the event was completed. Step may also include the agent sending to the TSM any introspected data e.g. data marked for watching validation etc. and any user inputs and outputs at the device. At step the TSM compares the result e.g. from step to the expected values e.g. as defined in the test case e.g. at field . In the event the test step did not fail the comparison at step then at step the TMS shows the result of the test step e.g. via output and the process returns to step where the TMS communicates the next test step to the agent. At step the TSM may also save the results of the test step to a reporting component e.g. storage system B that stores results of a test run.

In the event the test step failed the comparison at step then at step the TSM marks the test step as failed. If the TMS is set to stop the test at the first failure then at step the TMS ends the test case and shows the result at step and saves the results at step . On the other hand if the TMS is not set to stop on failure then the TMS shows the result at step saves the results at step and the process returns to step where the TMS communicates the next test step to the agent.

In embodiments a service provider such as a Solution Integrator could offer to perform the processes described herein. In this case the service provider can create maintain deploy support etc. the computer infrastructure that performs the process steps of the invention for one or more customers. These customers may be for example any business that uses technology. In return the service provider can receive payment from the customer s under a subscription and or fee agreement and or the service provider can receive payment from the sale of advertising content to one or more third parties.

The foregoing examples have been provided for the purpose of explanation and should not be construed as limiting the present invention. While the present invention has been described with reference to an exemplary embodiment Changes may be made within the purview of the appended claims without departing from the scope and spirit of the present invention in its aspects. Also although the present invention has been described herein with reference to particular materials and embodiments the present invention is not intended to be limited to the particulars disclosed herein rather the present invention extends to all functionally equivalent structures methods and uses such as are within the scope of the appended claims.

