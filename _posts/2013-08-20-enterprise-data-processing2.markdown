---

title: Enterprise data processing
abstract: An enterprise data processing module and method are described herein. The enterprise data processing module comprises at least one collector and at least one analyzer. The collectors may be operable to collect data pieces from a plurality of data sources. The analyzers may be operable to analyze the collected data pieces to determine cross-source relationships that exist between the data pieces collected from the plurality of sources. The analyzed data pieces may be stored in one or more big-data databases as blocks of data according to the cross-source relationships.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09547682&OS=09547682&RS=09547682
owner: BITVORE CORP.
number: 09547682
owner_city: Los Angeles
owner_country: US
publication_date: 20130820
---
This application claims priority to U.S. provisional patent application No. 61 691 911 filed Aug. 22 2012. The above referenced United States patent application is hereby incorporated herein by reference in its entirety.

The invention relates to the field of large scale data processing. In particular but not exclusively it relates to automated analysis of data that may be stored in a decentralized manner.

With the growth of the use of the Internet the growth of data usage in private networks and the growth of data used by companies and other entities both internal and external data the need for massive data storage and massive computing power has risen. Therefore many entities are turning to cloud computing. The terms the cloud or cloud computing may refer generally to large scale data centers that are maintained by a third party or a company or entity for example one that maintains systems and or software that work with the data center s where the storage and computing capabilities of the numerous servers within the data center are offered to internal or external customers through one or more network connections. Because relatively small entities may have access to the large scale storage and computing power of many servers the entities can have access to large scale computing power that is flexible and available while lowering or eliminating the costs needed to maintain the data centers. Various databases such as communications databases and or databases in a cloud computing data center may be useful for storing massive amounts of data but in various database approaches the data is stored in a decentralized manner across several servers or nodes and information regarding the relationships or correlations between the data may not be stored. In various databases for example communications databases and or other forms of databases large scale data is formatted or structured to be most easily used for task specific computations. In other words data may be analyzed at the outset for example a particular relationship may be analyzed and then the data and the resulting conclusion are stored in a specific format. For various databases this is called a schema. Once the initial analysis is done it may be very time consuming and difficult to re structure and or re analyze data to find a new value short of retrieving harvesting and or archiving locally all the data the user is interested and then organizing and or performing computations or routines on the data to analyze relationships.

Data has become a key asset for most modern day enterprises. Managing this data has become a major problem for the IT departments of these companies and organizations. For many years the changes in business requirements have made it more and more difficult and expensive for enterprises to keep abreast of the changes in data firstly because of continuous changes in the tools and standards and secondly because of the exponential increase in the amount of data that is being made available.

Enterprises may find it difficult to detect business value in the relationships between data points where many different types of data exist. Trying to convert data to a heterogeneous but flexible format may likely result in incomplete information that is collected from limited points.

Further limitations and disadvantages of conventional and traditional approaches will become apparent to one of skill in the art through comparison of such systems with the present invention as set forth in the remainder of the present application with reference to the drawings.

Aspects of the present invention are aimed at reducing or eliminating the above problems with the prior art. In particular but not exclusively aspects of the invention are directed to large scale data processing modules and methods for automated analysis of data that may be stored in a decentralized manner.

One example embodiment of this invention comprises an enterprise data processing module. The enterprise data processing module comprises at least one collector and at least one analyzer. The collectors may be operable to collect data pieces from a plurality of data sources. The analyzers may be operable to analyze the collected data pieces to determine cross source relationships that exist between the data pieces collected from the plurality of sources. The analyzed data pieces may be stored in one or more big data databases as blocks of data according to the cross source relationships.

In another example embodiment of this invention the enterprise data processing module comprises a user interface operable to receive a request from a user to interact with a data group stored in the one or more big data databases. The request may be attempting to utilize information from the cross source relationship. The information from the cross source relationship may comprise conclusion data that supports a schema. If the user has permission to access information from the cross source relationship the request may be processed to return the conclusion data without extracting all underlying data required to compute the requested conclusion data.

In another example embodiment of this invention the cross source relationship may comprise a degree of correlation that is determined by a correlation intensity algorithm. The correlation intensity algorithm may determine a level of similarity with respect to factors such as but not limited to the number of unique concepts in each data piece a complexity of the data pieces a size of the data pieces a spam score of each data piece and or a readability score of each data piece.

Entities or enterprises for example large companies may either store or be in control of massive amounts of internal and external data for example communications data e.g. email social media bulletin board forums etc. and or other form s of data. Some of this information may be stored on one or more internal enterprise servers. Some of this information may be stored on one or more external servers for example in one or more servers used to run a social networking or bulletin board forum service. In various enterprise data management systems this massive amount of internal and external data may be valuable to the enterprise however the data may be difficult to manage and or analyze for example because the data is unstructured and or continuously growing and changing. In various enterprise data management systems this massive amount of internal and external data for example communications data is not captured or analyzed across the whole data spectrum of different data types and formats.

The present disclosure describes one or more systems methods routines techniques and or tools for enterprise data processing EDP . The EDP may be designed and or adapted to capture and analyze data for example communications data from various sources e.g. email social networks bulletin boards forums content management systems phone logs other corporate databases etc. . . . and or other form s of data such that this massive amount of data may be used in a valuable way by an enterprise. The EDP may be designed and or adapted to compare analyze annotate and or correlate this data across the whole data set i.e. data from all of the various sources . The EDP may be designed and or adapted to store data for example communications data from various sources as well as information about various correlations or relationships between the data under a single umbrella in a manner such that the data is readily accessible and modifiable. The EDP may be designed and or adapted to integrate various types of internal and external enterprise data and or systems into a single flexible data store.

The EDP may utilize cloud computing and or other systems that offer massive data storage and or massive computing power. The terms the cloud or cloud computing may refer generally to large scale data centers that are maintained by a third party or a company or entity for example one that maintains systems and or software that work with the data center s where the storage and computing capabilities of the numerous servers within the data center are offered to internal or external customers through one or more network connections. The EDP may utilize massive storage capabilities of cloud computing to store data for example communications data and or other form s of data from various sources e.g. email social networks bulletin boards forums content management systems phone logs other corporate databases etc. . Various databases for example communications databases other form s of databases and or databases in a cloud computing data center may be useful for storing massive amounts of data but in various database approaches the data is stored in a decentralized manner across several servers or nodes and information regarding the relationships or correlations between the data may not be stored.

The EDP may be designed and adapted to provide all the benefits of cloud computing while preserving valuable correlation information and or other information or metadata. The EDP may be designed and adapted to determine and or analyze relationships or correlations between data and may save this correlation information or other information when the data is stored to a database for example a communications database and or other form s of database. In various databases for example communications databases and or other form s of databases large scale data is formatted or structured to be most easily used for task specific computations. In other words data may be analyzed at the outset for example a particular relationship may be analyzed and then the data and the resulting conclusion are stored in a specific format. For various databases this is called a schema. Once the initial analysis is done it may be very time consuming and difficult to re structure and or re analyze data to find a new value short of retrieving harvesting and or archiving locally all the data the user is interested and then organizing and or performing computations or routines on the data to analyze relationships. The EDP may be designed and adapted to restructure data in various ways that are useful to an entity for example an enterprise and various relationships between the data that are useful to the entity may be determined for example according to strengths weaknesses opportunities and threats of and to the entity within and across the data. Saving the correlation information may also be referred to as mapping the correlation information to the database for example a communications database and or other database for other form s of data. Correlation information may refer generally to information that indicates how one or more pieces of data relate to each other. One example of correlation information using an example of data within a communications database may be the relationship between individual messages within an email thread or a social media thread.

The EDP may be designed and adapted to allow users to quickly explore analyze manipulate and interact with stored data and correlation information for example in real time and receiving quick responses. One benefit of the EDP may be that because relationships between data are stored in addition to the data and because new relationships can be discovered or generated at a later time users may explore relationships between data on the fly for example relationships that the user may not have intended to explore when they saved the data. The EDP may create an Agile style data model that allows dynamic analysis of data where the data and relationships may be kept live and readily accessible by users. The EDP may offer benefits over other various database approaches that may require a user to retrieve harvest and or archive all the data the user is interested in from the database for example a communications database and or other database for other form s of data and then organize and or perform computations and or routines on the data to analyze relationships. The EDP may be designed and adapted to allow users to re use relationship and other information about the data in various ways without having to re create data relationship sets. The EDP may be designed and adapted to provide data management and analysis capabilities similar to those offered by various relational databases while offering the power and flexibility of non relational databases and other forms of databases often found in cloud computing. The EDP may be designed and adapted to allow applications that are built around relational databases to continue running like normal i.e. as the application may normally run on a relational database while still taking advantage of cloud computing. The EDP may be designed and adapted to integrate with various sorts of internal and external enterprise data and systems and those data and systems of other entities.

Source devices may be examples of various devices within an entity or enterprise that create and manage large amounts of various forms of data for example both structured and unstructured data e.g. unstructured data such as email social media documents web content. and structured data such as server logs phone logs customer service logs etc. . In some embodiments source devices may be examples of various internal and or external sources that an entity or enterprise is interested in for example various email and messaging databases and or various social networking and or bulletin board forum services. Source devices may be in communication with each other and or an EDP via a network . The EDP may be in communication with one or more cloud data centers via a network . Network may be the same general collection of network connections as network or it may be separate. It should be understood that although shows the EDP as being separated from the source devices and cloud data centers via networks other configurations are contemplated. For example the EDP may be integrated with one or more cloud data centers and or the EDP may be part of a local network with one or more source devices. In some embodiments the EDP may be configured in a distributed fashion to perform data correlation with respect to one or more source devices.

In operation source devices source devices create and or store large amounts of data. Various source devices may initiate storage of data to one or more cloud data centers or connections databases. Additionally various source devices for example social networking services may store data that is accessed by a separate device that initiates storage of the data to one or more cloud data centers or connections databases. The EDP may collect and analyze the data from the source devices for example before the data is stored to the cloud data centers or connections databases. The EDP may be designed and adapted to determine and or analyze relationships or correlations between data and may save this correlation information when the data is stored to a database for example a communications database and or other form s of database s .

The collector s accept data from multiple sources for example source devices similar to source devices shown in . Collector s may implement a number of crawlers for example social network crawlers. A crawler may refer to a computer program or routine that browses a network for example the internet in a methodical automated manner or in an orderly fashion. Crawlers may maintain a list of network locations to visit and browse or scan for relevant data. For example one crawler or portion of a crawler may be programmed or designed to scan a particular social networking service for data that is relevant to an enterprise. The data may be relevant because it was sent to or posted to a profile associated with the enterprise or it may be data that mentions the enterprise but is sent to or posted to other profiles. Collector s may organize raw data from multiple sources and present data to one or more analyzers in a useful format.

The analyzer s may accept data from the collector s and may analyze the data and or compare the data to other data to determine relationships between data and or within data. The analyzer s may execute and or use one or more correlation intensity algorithms to analyze the data. A correlation intensity algorithm may determine the degree of correlation between pieces of data. For example data may be strongly correlated loosely correlated or not correlated. A correlation intensity algorithm may determine the degree of correlation between pieces of data with or without referencing the content of the data.

The analyzer s may use one or more algorithms to assemble subsets or new groupings of the data in various ways and or to associate data according to different relationships or correlations. The analyzer s may be designed adapted and or tuned to slice and or correlate data in ways that are important to an entity for example to deal with strengths weaknesses opportunities and threats across the enterprise. In various embodiments of the present disclosure the analyzer s may analyze data for relationships including but not limited to the following time content keywords associated users or profiles and sentiment. Non textual data may be similarly stored and analyzed with the system. In some embodiments the analyzer s may analyze data to determine for example which employees in an enterprise are talking to which customers and optionally what subjects they are discussing. In some embodiments the analyzer s may analyze data to look for and correlate different types of business data for example server logs and sales data could be correlated with engineer and or marketing data. As another example public data i.e. data from external sources such as social networking services may be correlated with private data i.e. a company s server logs and internal email messages . As another example data from a particular type of source e.g. company email may be correlated same source correlation . As another example data from multiple sources e.g. company email social networking services news feeds and server logs may be correlated cross source correlation .

A correlation intensity algorithm may utilize one or more characteristics and or metrics to determine the intensity of correlation between data. The algorithm may assign weights for example based on user input to characteristics and or metrics where the weights may indicate how important a particular characteristic and or metrics is to the correlation determination. Characteristics or metrics may allow a correlation intensity algorithm to flexibly evaluate correlations across many different dimensions of the data. Characteristics or metrics may be normalized for example to a range between 0 and 1 inclusively or between 1 and 1. Normalization may allow characteristics or metrics to be combined with user provided weightings for example to generate a single composite weight on the fly. The following may be example characteristics or metrics used by correlation intensity algorithms data length e.g. all messages in a thread or a parallel metric in the case of structured data such as server logs number of data pieces in a data chunk e.g. number of messages in a thread or similar metric in the case of structured data such as server logs number of users associated with a data chunk e.g. number of participants in a message thread average data chunk length e.g. average message length of message in a thread entropy score a measure of how complex or unique the contents of the data chunk are e.g. the combined entropy score of all messages in thread spam score e.g. a spam score for each message in a thread readability score that indicates the approximate grade level of the text in the data chunk readability score indicating the number of years of education needed for comprehension of the text in the data chunk the number of unique concepts e.g. the number of unique concepts in a thread for example as determined by a search and or analysis engine average sentiment score e.g. the average sentiment score of messages for example as calculated by a search and or analysis engine and or concept density e.g. unique concepts divided by length of a thread .

The analyzer s may build up one or more data models from the data from one or more source devices for example using information from one or more correlation intensity algorithms and optionally one or more other tools such as filters. The analyzer s may include other information in a data model for example access controls and or other rules associated with the data. The analyzer s may communicate one or more data models and or other data or information to one or more datastores for example one or more cloud data centers.

A data model may include one or more data globs. A data glob may refer to a cluster or block of data and various other related pieces of information or attributes related to the data. A data glob may contain data in various forms for example structured data unstructured data both structured and unstructured data communications data or other form of data. A data glob may include multiple pieces of data content related users relevant access rules and the like. As one example a data glob may include a block of messages including the content of the messages related profiles times access rules and the like. Each data glob may store information about the data glob information similar to metadata for example a name a description a unique identifier and the like. Data globs may reside in a database for example a communications database or some other form of database and or a cloud data center either in a relatively centralized manner or data globs may be stored in a distributed fashion for example with different parts of the data glob being stored on different servers. Data globs may be designed adapted or programmed such that they can be easily interacted with. For example data globs as a single unit may be capable of being moved copied filtered searched processed synchronized split merged modified analyzed and the like.

Data globs may be reusable in a manner similar to the way objects are reused in object oriented programming. For example multiple data globs may be assembled into a larger piece of data which may constitute a new data glob. One efficient approach to using data globs may be to reuse as much of an existing data glob as possible changing only details that differ from the data glob being used. For example when a new data glob is created using an existing data glob much of the existing data glob may be useable and extra and or different data may be added to the data glob to create a new data glob. In this respect the EDP may analyze the content inside of a data glob as opposed to just treating a data glob like a black box. This manner of reusing portions of existing data globs to create new data globs optionally with some additional or different information may be referred to as integration. In some embodiments of the present disclosure the EDP may include an integration tool that may manage existing data globs and utilize existing data globs to create new data globs. The integration tool may be designed to use as much data from existing data globs as possible for example to make data storage and access as efficient as possible.

Referring to the analyzer s and or the manager may build and or manage data globs. The analyzer the manager or some other component may run statistical analysis on data globs for example to determine counts of types of data attributes and the like or to create lists or other statistics. Data globs may be designed or programmed in an Agile style data model that allows dynamic analysis of data. Each data glob may go through a data lifecycle meaning it may go through an iterative incremental adaptive and or evolutionary development process for example where contents relationships and other information about or in the data glob evolve through user interaction development of related information and the like. Thus data globs may be flexible and quickly responsive to change.

One or more datastores for example one or more cloud data centers may accept data from the analyzer s for example in the form of one or more data models. Datastore s may be large and scalable for example consisting of one or more servers and or data storage devices. Datastore s may be designed and or adapted optionally in conjunction with manager and or other components of the enterprise data processing EDP to store data correlations and or other information in a manner that the data is kept alive meaning the data is readily accessible and manageable. Datastore s may be implemented in a big table manner. For example datastore and or one or more components of the enterprise data processing EDP may utilize a database architecture that is designed for distributed scalable big data store for example a Hadoop database program such as HBase .

Referring to manager may communicate with one or more datastores or cloud data centers to manage the data stored therein. Manager may analyze data in the datastore s to ensure it is current live and or accessible. Manager may analyze data in the datastore s to determine new or changed relationships between the data for example comparing existing data to newly added data. Manager may run one or more algorithms that are similar to the algorithms explained above with regard to the analyzer for example a correlation intensity algorithm to determine whether relationships exist and or the intensity of correlations between data. In some embodiments one or more algorithms used by the manager to analyze the data in the datastore may be the same algorithms as used by the analyzer . Manager may implement one or more version control algorithms and or routines that analyze data in the datastore s to determine whether data is current. Version control algorithms and or routines may replace old data or may designate new data as the most relevant data while archiving old data.

The user interface visualizer may provide an interface between users and or source devices that interface with the EDP . The user interface visualizer may provide visual feedback for example via various visualizations ie. screens or displays to a user such that the user can see the data and correlation information stored in datastore . The user may then interact with the user interface visualizer to explore analyze and manipulate data and correlations or relationships between data as well as other information like statistics. The user interface visualizer may allow a user to interact with the data and relationships in real time and receive quick responses. Various configurations of components communication links code routines and the like may exist in the EDP in order to implement analysis and management commands communicated between a user and the datastore . One example configuration is shown in where the user interface visualizer is in communication with the manager and the manager then communicates with the datastore .

The EDP may implement resource access controls and or role based access controls. The EDP may utilize one or more routines algorithms components or the like to implement resource access controls. As described above the EDP may collect analyze and store data from various sources and this data may be sensitive or private in a variety of ways. For example an email thread may be sensitive or private to a particular group of users and or profiles for example users that create send or receive messages in the thread. As another example content from a social networking service may be sensitive or private to a particular group of users and or profiles. Each of these users and or profiles may have associated data such as accounts passwords and the like. The EDP may create and or enforce resource access controls with respect to data collected from various sources for example to control which users and or profiles can interact with the particular chunks of data. The EDP may keep track of users and or profiles that may have certain rights rules and or privileges with respect to particular pieces or types of data. The EDP may determine users profiles and or role information from one or more user databases associated with the enterprise for example LDAP servers. The EDP may modify update or create new roles. The resource access controls used by the EDP may include rights rules and or privileges with respect to various commands that a user may attempt to execute for example attempts to create read write update or modify data. One benefit of EDP over various types of resource access control schemes may be that the EDP is designed and or adapted to create and or enforce access controls and or role based access controls with respect to big data or large quantities of data stored in a distributed manner in one or more databases for example communications databases cloud data centers and or other form s of databases.

The EDP may implement resource access controls that are based on various roles of users who may attempt to create read write update or modify data. Examples of user roles may be supervisors managers high level user accounts and the like. Another example of a user role may be a user that manages only data from a particular type of source for example one social networking service or one message board. It should be understood that these are just examples of roles and many other types of roles may be implemented by the EDP to manage which users may interact with particular types of data. The EDP may determine users profiles and or role information from one or more user databases associated with the enterprise for example LDAP servers. The EDP may modify update or create new roles.

Referring to the EDP may implement resource access controls and or role based access controls in one or more components of the EDP or at one or more stages of the data ingest and or management process. For example the analyzer s may implement resource access controls when it builds up one or more data models from the data from one or more source devices. For example the analyzer may assign each data glob a set of resource access rules for example before each data glob is stored in a data store . In this respect resource access controls may be part of the data stored with each data glob and resource access controls may be portable like the data globs are. Manager may also implement resource access controls for example in a resource access control module as shown in . The resource access control module may perform runtime enforcement of resource access controls and or role based access controls for example as users attempt to interact with data stored in a datastore .

Certain embodiments of the present disclosure may be found in one or more methods of enterprise data processing. depicts an illustration of a flow chart showing example steps in a method of enterprise data processing according to one or more example embodiments of the present disclosure. It should be understood that in some embodiments one or more of the steps depicted in may be performed in a different order than depicted. Additionally in some embodiments a method of enterprise data processing may include more or less steps than are depicted in . Specifically depicts an example import process for enterprise data processing. At step the enterprise data processing may collect raw data from a variety of sources for example using one or more collectors as explained above. At step the enterprise data processing may analyze data to determine whether relationships or correlations exist in or between the data and optionally the intensity of the relationships. Step may utilize one or more analyzers as explained above. At step the enterprise data processing method may form one or more data globs and may assign each data glob resource access rules. At step the enterprise data processing may store or map one or more data models including data correlations to one or more datastores or cloud data centers. At step the enterprise data processing may continually execute one or more routines to manage data in the datastore s for example to determine new relationships. Step may use a manager and or a database management program similar to the ones described above. As one example step may include executing one or more management routines at regular time intervals.

Components of one or more embodiments may include for example and not limited to the following one or more analyzers collectors and web services that allow the management and action of these collectors and analyzers. Analyzers may come in many different forms and examples include but are not limited to adding modifying and replacing tags adding modifying and removing records adding modifying or removing other analyzers adding modifying and removing collectors and importing and exporting records in different forms although the import function may be shared by collectors . Collectors may come in many different forms and examples include but are not limited to collecting from HTTP sources such as web sites and RSS feeds collecting from web service APIs such as Twitter Facebook Dropbox LinkedIn Salesforce and similar and collecting from other APIs such as Java Database Connectivity SQL databases Internet Mail protocol servers IMAP and FTP servers. Also collectors may import other file formats such as comma separated files CSV Microsoft Outlook data files PST and others.

The Analyzer mechanism may schedule and analyze jobs and use job service to maintain schedules. The Analyzer Service may schedule jobs with the job service pass trigger information to an analyzer to actually run turn around and make call backs to invoke an analyzer and handle mapping between collectors and analyzers. Some analyzers may be triggered to run when a collector finishes e.g. a PST collector may be configured such that a thread analyzer and profile analyzer will run. A cross message type threader may launch a whole series of analyzers that are run. Jobs may be concurrent map reduce jobs or centralized jobs. Analyzers may run asynchronously and can be dynamically loadable and or role restricted. A reporter may be used to retrieve data. The reporter may run on demand or may be scheduled using the job service. The reporter may be implemented in a pre processor for programmatic or user interface access. The reporter may collect and assemble pre calculated results from analyzer runs. For example to compare one complex query from last week s data to this week s each search may take 20 minutes and then you would still need to merge and compare the results. The reporter may condense known queries for any time granularity to take a few tenths of seconds.

An Analyzer Service REST may be the Web based API for controlling analyzer services. A Configuration Service is a data storage that holds configuration information for the rest of the components. The Configuration Service may be used to share configuration information such as the list of analyzers or collectors that are available and configured into system. The Configuration Service may also be used to share information on how Hbase system may be setup and what systems may be involved. A Data API may be the underlying Java based API implementation for accessing key value pairs and doing queries and managing what s going in and out of the data store. A Data Service may be the actual mapping onto the storage system resources and the management of transfer of data between the users applications and machines. A Data Service REST may be the Web based API for data service. An HBase Client may be the entry point for access to the underlying HBase store which is a point where storage can be swapped from in memory to cloud stored file formats. A Job Service may be a set of functions to maintain schedule and invoke services like analyzers and collectors.

An Admin Analyzer may backup and restore records to and from ZIP files. A Validation Administrator may check and repair record integrity. A De Duplication Analyzer may identify records that return similar information and should be hidden from particular view. A Discovery Analyzer may build new collectors from existing record content. An Analyzer Builder Analyzer may build new analyzers from existing record content. An Entity Analyzer may perform Entity Name Recognition and tag records with additional metadata. An Index Analyzer may re index records following format changes. A Profiles Analyzer may identify references to individuals and build profiles for them. An Email Thread Analyzer may identify email message threads and link to the profiles that created them. A Record Administrator may delete selected records identified by tags. A Sentiment Analyzer may be a 3 value sentiment analyzer which labels individual blobs or messages as negative neutral or positive or some other quality attribute. A Stats Analyzer may perform statistical operations on the values of tags or the number and or types of records. A Table Definition Analyzer may perform bulk tagging operations defined by a database table that is entered and modified by a user. A Tagger Analyzer may add remove or update tags on records that match a specific query. A Web Link Analyzer may be employed to find links to web pages in record tags and collect these as new records.

A Bing Search News Collector may be used to collect news feed data from the Microsoft Bing Service. The Bing Search Web Collector may also be used to collect records identified by a web search using the Microsoft Bing Service. A Bridge Collector may allow the import of sets of records filtered by queries from one Bitvore system to another. A CSV Collector may be used to import content from comma separated value files CSV . A Twitter Collector is a Twitter structure smart component that imports filters and incrementally downloads data from a specific Twitter account in a compliant manner. A Facebook Collector is a Facebook structure smart component that imports filters and incrementally downloads data from a specific Facebook account in a compliant manner. A Wget Collector may be a generic download manager for grabbing unstructured data and may pull down unstructured content using HTTP HTTPs FTP content. An RSS Collector may be the component that goes out understands and incrementally grabs RSS feed data from a specific Web address. An IMAP Collector may connect to an Internet Mail Protocol server and retrieve records for one or more email accounts. A Web Content Collector may connect to a remote HTTP site and retrieve the contents of this site. The Web Content Collector may also generate multiple records and may perform authentication authorization and content filtering and transformation if required. An SQL Collector may connect to a standard SQL database execute queries and save the resulting data to the datastore as records.

There may be additional services and support components libraries to provide common services and capabilities to the analyzers collectors and web services. These may include a Common Library that may contain general code shared by many projects an Email Common that may contain code specific to components dealing with email messages a Search Common that may contain code used to parse build and execute content searches described by VQL Vore Query Language queries and a Web Collect Common that may provide code used in components that access web services such as HTTP and HTTPS servers.

The present disclosure may be embedded in a program which comprises all the features enabling the implementation of the embodiments described herein and which when loaded in a computer system is able to carry out these embodiments. Computer program in the present context means any expression in any language code or notation of a set of instructions intended to cause a system having an information processing capability to perform a particular function either directly or after either or both of the following a conversion to another language code or notation b reproduction in a different material form.

While the present disclosure has been described with reference to certain embodiments it will be understood by those skilled in the art that various changes may be made and equivalents may be substituted without departing from the scope of the present disclosure. In addition many modifications may be made to adapt a particular situation or material to the teachings of the present disclosure without departing from its scope. Therefore it is intended that the present disclosure not be limited to the particular embodiment disclosed but that the present disclosure will include all embodiments falling within the scope of the appended claims.

