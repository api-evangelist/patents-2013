---

title: Techniques for sharing priorities between streams of work and dynamic parallelism
abstract: One embodiment sets forth a method for assigning priorities to kernels launched by a software application and executed within a stream of work on a parallel processing subsystem that supports dynamic parallelism. First, the software application assigns a maximum nesting depth for dynamic parallelism. The software application then assigns a stream priority to a stream. These assignments cause a driver to map the stream priority to a device priority and, subsequently, associate the device priority with the stream. As part of the mapping, the driver ensures that each device priority is at least the maximum nesting depth higher than the device priorities associated with any lower priority streams. Subsequently, the driver launches any kernel included in the stream with the device priority associated with the stream. Advantageously, by strategically assigning the maximum nesting depth and prioritizing streams, an application developer may increase the overall processing efficiency of the software application.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09575760&OS=09575760&RS=09575760
owner: NVIDIA Corporation
number: 09575760
owner_city: Santa Clara
owner_country: US
publication_date: 20130517
---
Embodiments of the present invention generally relate to general purpose computing and more specifically to techniques for sharing priorities between streams of work and dynamic parallelism.

A typical parallel processing subsystem that may include one or more graphics processing units GPUs is capable of very high performance using a relatively large number of small parallel execution threads on dedicated programmable hardware processing units. The specialized design of such parallel processing subsystems usually allows these subsystems to efficiently perform certain tasks such as rendering 3 D scenes or computing the product of two matrices using a high volume of concurrent computational and memory operations.

To fully realize the processing capabilities of advanced parallel processing subsystems subsystem functionality may be exposed to application developers through one or more application programming interfaces APIs of calls and libraries. Among other things doing so enables application developers to tailor a software application executing on a central processing unit CPU to optimize the way parallel processing subsystems function. In one approach to developing a software application the software application developer may divide work included in the software application into streams of work components e.g. computational and memory operations . Each stream may be executed concurrently on the parallel processing subsystem. More specifically work components included in different streams may run concurrently and may be interleaved. In contrast within each stream a sequence of work components executes in issue order on the parallel processing subsystem.

The parallel processing subsystem may schedule the execution of the work components using a variety of techniques depending on the functionality included in the parallel processing subsystem. Two features that may be included in advanced parallel processing systems are support for prioritizing work components and preemption of currently executing computation work components. For example a parallel processing subsystem that supports prioritization may be configured to schedule work components in priority order. And a preemption capable parallel processing subsystem may be configured to preempt a lower priority computational work component executing on a parallel processing subsystem resource in favor of a higher priority computational work component.

Typically a parallel processing subsystem that includes prioritization functionality supports a limited set of priorities referred to herein as a set of valid device priorities. In one approach to exposing prioritization capabilities an API included in a software stack enables the software application to assign a desired stream priority to a stream. An API driver also included in the software stack then maps the desired stream priority to a device priority included in the set of valid device priorities. Further the API driver may store the device priority in a memory resource associated with the stream. Subsequently if the software application requests the launch of a work component within the stream then the API driver may request that the parallel processing subsystem launch the work component with the device priority associated with the stream.

Advanced parallel processing subsystems may also support dynamic parallelism. Dynamic parallelism allows a parent work component executing on the parallel processing subsystem to launch a child work component on the parallel processing subsystem. The parallel processing subsystem may also enable to parent work component to optionally synchronize on the completion of the child work component. Further the parallel processing subsystem may enable the parent work component to consume the output produced from the child work component. In some implementations the parallel processing subsystem performs the launching synchronization and consumption of the results of a child work component without involving the CPU.

Some parallel processing subsystems support multiple levels of nested child launches where each subordinate launch executes at a new level. In other words the parent work component executing at a first level may launch a first child work component. The first child work component executing at a second level may then launch a second child work component. The second child work component executing at a third level may then launch a third child work component and so on. Because of resource limitations such as the memory required by the parallel processing system to support each new level the parallel processing subsystem will typically define a max nesting depth N . Notably the max nesting depth N is the maximum number of work components in the chain and therefore the maximum number of levels . For example if a parent work component launches a first child work component and then the first child work component launches a second work component then the nesting depth would be three N 3 . Any launch of a work component which would result in a child kernel executing at a deeper level than the maximum nesting depth will fail.

Dynamic parallelism usually requires that a parent work component is able to synchronize on any child work components that the parent work component launches. However if executing the parent work component were to completely occupy the parallel processing subsystem resources then the child work component would be unable to execute. Consequently the parent work component would be unable to synchronize on the child work component. To avoid synchronization problems the parallel processing subsystem is typically configured to ensure that the child work component receives enough resources to fully execute. In particular the parallel processing subsystem is typically configured to give preference to the child work component whenever there is a resource contention between the child work component and the parent work component.

To ensure preferential treatment for the child work component the parallel processing subsystem typically uses on or more of the valid device priorities mentioned above. More specifically the parallel processing subsystem assigns a child work component a device priority that is one higher than the parent work component. Because each child work component may also launch nested child work components to support a max nesting depth of N the parallel processing subsystem requires N 1 valid device priorities for child work components.

In one approach to accommodating dynamic parallelism in conjunction with prioritizing streams the API driver reserves N 1 valid device priorities to support a fixed maximum nesting depth of N. Further the API driver is configured to disregard the device priority associated with a stream when launching a parent work component within the stream. In particular upon receiving a request to launch a parent work component the API driver launches the parent work component at the lowest valid device priority.

One drawback to this approach to prioritizing work is that by reserving N 1 valid device priorities the number of valid device priorities available for prioritizing streams is reduced by N 1 . And reducing the number of device priorities available for prioritizing streams may reduce the ability of application developers to optimize the performance of software applications. For example to tune a software algorithm that performs video decoding and encoding using a pipelined workflow with M stages an application developer could strategically allocate the work components for each stage into M prioritized streams. More specifically to reduce the likelihood that a particular frame is starved for resources by subsequent frames the second stage could be prioritized higher than the first stage the third stage could be prioritized higher than the second stage and so on. If a parallel processing subsystem were to support less than M device priorities for prioritizing streams then the parallel processing subsystem would be unable to fully support the stream prioritization requests included in the software application. Consequently the overall latency of each frame could be increased and therefore could be more likely to cause jitter in frame rates than in a parallel processing subsystem that supported M device priorities for streams.

Another drawback to the above approach to prioritizing work is that indiscriminately launching all parent work components at the default device priority could adversely affect latency sensitive work components included in a high priority stream. For instance suppose that a stream StreamA associated with a high device priority were to include a work component ChildlessA and a work component ParentA. Further suppose that the work component ParentA were configured to launch a child work component ChildA. Finally suppose that the work component ChildlessA were not configured to launch any child work components. The API driver would launch the work component ChildlessA at the high device priority associated with the stream and the work component ParentA at the lowest device priority. Subsequently the parallel processing subsystem would launch the ChildA work component at a device priority reserved for child priorities i.e. one higher than the lowest device priority . Consequently if ParentA were a highly latency sensitive work component then executing ParentA at the lowest device priority could increase latency and reduce the execution speed of the software application.

As the foregoing illustrates what is needed in the art is a more effective way to prioritize work submitted to parallel processing subsystems that support dynamic parallelism.

One embodiment of the present invention sets forth a method for mapping a plurality of stream priorities associated with a software application to a plurality of device priorities supported by a parallel processing subsystem. The method includes identifying a maximum nesting depth receiving a first request from the software application to associate a first stream with a first stream priority mapping the first stream priority to a first device priority receiving a second request from the software application to associate a second stream with a second stream priority where the second stream priority is higher in priority than the first stream priority and mapping the second stream priority to a second device priority where the second device priority is at least the maximum nesting depth higher in priority than the first device priority.

Other embodiments of the present invention include without limitation a computer readable storage medium including instructions that when executed by a processing unit cause the processing unit to implement aspects of the techniques described herein as well as a system that includes different elements configured to implement aspects of the techniques described herein.

By implementing the disclosed techniques together the API and the driver enable software applications to affect the number of priority levels available for streams. Strategically setting the number of priority levels allows the driver to more effective prioritize work components submitted to parallel processing subsystems that support dynamic parallelism. Consequently the performance of the software applications may be improved by decreasing latency and increasing throughput.

In the following description numerous specific details are set forth to provide a more thorough understanding of the present invention. However it will be apparent to one of skill in the art that the present invention may be practiced without one or more of these specific details.

In one embodiment the parallel processing subsystem incorporates circuitry optimized for graphics and video processing including for example video output circuitry and constitutes a graphics processing unit GPU . In another embodiment the parallel processing subsystem incorporates circuitry optimized for general purpose processing while preserving the underlying computational architecture described in greater detail herein. In yet another embodiment the parallel processing subsystem may be integrated with one or more other system elements in a single subsystem such as joining the memory bridge CPU and I O bridge to form a system on chip SoC .

In operation the CPU is the master processor of the computer system controlling and coordinating operations of other system components. In particular the CPU issues commands that control the operation of the parallel processing subsystem . Those commands may originate within a software application resident in the system memory and executing on the CPU . A compute unified device architecture CUDA software stack is also resident in the system memory . CUDA is a general purpose computing environment which uses the parallel processing subsystem to perform various computing tasks. The CUDA software stack is a set of programs included in the CUDA that issue and manage general purpose computations that operate on components in the parallel processing subsystem . The software application may generate requests i.e. calls for processing by the CUDA software stack to produce a desired set of results. In alternate embodiments the CUDA software stack may be replaced with any set of software programs that expose and manage parallel processing subsystem functionality. For i the CUDA software stack may be replaced with a different general purpose compute software stack or a graphics software stack. Further the CUDA software stack may be configured to inter operate with one or more additional software stacks.

It will be appreciated that the system shown herein is illustrative and that variations and modifications are possible. The connection topology including the number and arrangement of bridges the number of CPUs and the number of parallel processing subsystems may be modified as desired. For instance in some embodiments system memory is connected to CPU directly rather than through a bridge and other devices communicate with system memory via memory bridge and CPU . In other alternative topologies parallel processing subsystem is connected to I O bridge or directly to CPU rather than to memory bridge . In still other embodiments I O bridge and memory bridge might be integrated into a single chip instead of existing as one or more discrete devices. Large embodiments may include two or more CPUs and two or more parallel processing subsystems . The particular components shown herein are optional for instance any number of add in cards or peripheral devices might be supported. In some embodiments switch is eliminated and network adapter and add in cards connect directly to I O bridge .

To efficiently achieve a set of results using the parallel processing subsystem the software application passes application CUDA requests to the CUDA software stack . As shown the CUDA software stack includes a CUDA runtime application programming interface API and a CUDA driver . The CUDA runtime API includes calls and libraries that expose the functionality of the parallel processing subsystem to application developers. And the CUDA driver is configured to translate the application CUDA requests received by the CUDA runtime API to lower level commands that execute on components within the parallel processing subsystem .

More specifically the CUDA driver may submit one or more streams not shown to the parallel processing subsystem for execution within the parallel processing subsystem . Each stream may include any number and combination of work components. In particular a CUDA stream may include one or more CPU launched kernels . In general a kernel is a function that has a defined entrance and exit and typically performs a computation on each element of an input list. Each CPU launched kernel is invoked by code that is executed by the CPU such as the software application . In contrast a GPU launched kernel not shown is invoked by code that is executed by the parallel processing subsystem such as the CPU launched kernel . Within each stream the components including the CPU launched kernels execute in issue order on the parallel processing subsystem . However work components such as CPU launched kernels included in different streams may run concurrently and may be interleaved.

The parallel processing subsystem includes advanced prioritization functionality that enables prioritization of kernels and preemption of currently executing kernels. Thus the parallel processing subsystem may schedule kernels in priority order. And the parallel processing subsystem may preempt a lower priority kernel executing on a parallel processing subsystem resource in favor of one or more higher priority kernels. For example suppose that the parallel processing subsystem were to receive a high priority kernel KHIGH. Further suppose that a low priority kernel KLOW were executing on a resource included in the parallel processing subsystem . The parallel processing subsystem could interrupt the kernel KLOW execute the kernel KHIGH on the resource and then resume executing the kernel KLOW on the resource.

Further the parallel processing subsystem is configured to support dynamic parallelism. As previously noted herein dynamic parallelism enables a parent work component executing on the parallel processing subsystem to launch a child work component on the parallel processing subsystem . More specifically any number including zero of CPU launched kernels may be configured to launch one or more child CPU launched kernels. In other words any number of CPU launched kernels may be parent kernels. And any number of CPU launched kernels may be configured not to launch any kernels and therefore would not be parent kernels. Further since CPU launched kernels are initiated by the software application not by a kernel executing on the parallel processing subsystem no CPU launched kernel is a child kernel.

As part of dynamic parallelism the parallel processing subsystem supports multiple levels of nested child launches where each subordinate launch executes at a new level and the total number of execution levels is the nesting depth. For example suppose that the parent CPU launched kernel were to execute at a first level and launch a first child GPU launched kernel. The first child GPU launched kernel could execute at a second level and launch a second child GPU launched kernel and so on. The parallel processing subsystem is configured to ensure that a child GPU launched kernel is not starved for resources by the parent kernel either the CPU launched kernel or another child GPU launched kernel . When a parent kernel launches a subordinate child GPU launched kernel the parallel processing subsystem assigns a higher priority to the child GPU launched kernel than to the parent kernel. In particular in the embodiment of the parallel processing subsystem is configured to assign a priority to the child kernel that is exactly 1 priority level higher than the priority of the parent kernel. In alternate embodiments the parallel processing subsystem may be configured to ensure higher priorities are assigned to child kernels in any technically feasible manner.

Advantageously the CUDA software stack is configured to flexibly expose both the available prioritization capability and the dynamic parallelism functionality of the parallel processing subsystem to application developers. Notably the CUDA runtime API exposes these features by supporting application CUDA requests to assign priorities to streams and to adjust dynamic parallelism limits. To facilitate the processing of these application CUDA requests the CUDA runtime API defines a set of valid stream priorities . And the CUDA driver includes valid device priorities max nesting depth priority mappings and stream data .

The valid stream priorities are defined by the CUDA runtime API and may be included in application CUDA requests to prioritize streams. The valid stream priorities may use higher numbers to represent higher priorities or lower numbers to represent higher priorities and may specify any number as a default priority. For instance the valid device stream priorities could include three numbers with 1 representing the highest priority and 1 representing the lowest priority. Similarly the valid device priorities represent priorities defined and supported by the parallel processing subsystem . The valid device priorities may include any number including zero of priorities that are supported by the parallel processing subsystem . The valid device priorities may use higher numbers to represent higher priorities or lower numbers to represent higher priorities and may specify any number as a default priority. For example the valid device priorities could include sixty four numbers with 63 representing the highest priority and 0 representing the lowest priority. The number of valid stream priorities may be greater than less than or equal to the number of valid device priorities . Further the number of valid device priorities available for prioritizing streams is typically limited by support for dynamic parallelism. More specifically as outlined previously herein one or more of the valid device priorities may be used by the parallel processing subsystem to support dynamic parallelism.

Each execution level supported by dynamic parallelism requires additional resources such as memory resources and valid device priorities . Consequently to efficiently allocate resources the parallel processing subsystem is configured to limit nested launches to the max nesting depth . Notably the parallel processing system will not initiate the launch of a GPU launched kernel that would execute at a level lower than the max nesting depth . And as persons skilled in the art will understand as the max nesting depth increases the number of valid device priorities available for prioritizing streams decreases. In particular for each valid device priority that is allocated for prioritizing streams an additional higher priority valid device priority is also allocated for each supported nesting level. In other words suppose that the max nesting depth were set to N and the number of available valid device priorities were M. The number of valid device priorities available for prioritizing streams would be M divided by N rounded down to the nearest integer. Advantageously the CUDA runtime API supports application CUDA requests that enable the software application to set the max nesting depth to reflect the requirements of the software application .

To enable comprehensive support for prioritizing streams in conjunction with dynamic parallelism the CUDA driver is configured map the valid stream priorities to the valid device priorities . To facilitate this mapping the CUDA driver stores the mappings as the priority mappings and data associated with streams as stream data . The CUDA driver may store the priority mappings and the stream data in any available memory resource such as the system memory . As part of the mapping process the CUDA driver is configured to assign priorities to different streams in a manner that optimally reflects the intention of the software application . Consequently the CUDA driver uses the max nesting depth to guide the mapping of the valid stream priorities to the valid device priorities .

More specifically the CUDA driver calculates the number of valid device priorities available for prioritizing streams by dividing the number of valid device priorities by the max nesting depth and subsequently rounding down to the nearest integer. The CUDA driver then splits the valid device priorities between valid device priorities for prioritizing streams and valid device priorities for child GPU launched kernels. For example suppose that the max nesting depth were N. The CUDA driver would ensure that for each of the valid device priorities allocated for prioritizing streams the next N 1 higher priority valid device priorities were reserved for child GPU launched kernels. In general each valid device priority allocated for prioritizing streams will be at least the max nesting depth higher than any lower priority valid device priority allocated for prioritizing streams. If the number of valid stream priorities exceeds the number of valid device priorities available for prioritizing streams then the CUDA driver may assign multiple valid stream priorities to the same valid device priority .

Embodiments of the invention disclosed herein may employ any technically feasible mapping algorithm. For instance suppose that the valid device priorities were to range from a lowest valid device priority of 0 to a highest valid device priority of 9. Further suppose that the max nesting depth were to be 4. Finally suppose that the valid stream priorities were to range from a lowest valid stream priority of 100 to a highest valid stream priority of 199. The number of device priorities allocated for stream prioritization would be 2. In one embodiment the CUDA driver could map the valid stream priority of 100 to the valid device priority of 0 and the valid stream priorities of 101 199 to the valid device priority of 4. In another embodiment the CUDA driver could map the valid stream priority of 199 to the valid device priority of 6 and the valid stream priorities of 100 198 to the valid device priority of 2.

Advantageously by allocating the valid device priorities based on an application specific max nesting depth the CUDA driver provides a flexible framework for supporting both prioritization and dynamic parallelism. In particular this flexible approach enables application developers to make trade offs between prioritization and dynamic parallelism based on the needs of the software application . By assigning a low max nesting depth the software application may use more unique valid stream priorities to strategically prioritize CPU launched kernels . Alternatively by assigning a high max nesting depth the software application may use more levels of parallelism. In this fashion application developers may structure their applications to increase performance. In contrast in prior art approaches to supporting both prioritization and dynamic parallelism the allocation of the valid device priorities is fixed. Consequently prior art approaches may limit the ability of application developers to tune the performance of their software applications.

As disclosed previously herein the CUDA runtime API supports application CUDA requests that enable the software application to assign valid stream priorities to streams. Upon receiving an application CUDA request to assign a particular valid stream priority to a stream the CUDA driver accesses the priority mappings . The CUDA driver uses data included in the priority mappings to determine the valid device priority corresponding to the particular valid stream priority . Subsequently the CUDA driver stores the determined valid device priority along with stream specific identifying information such as a stream name or a stream ID as stream data . In alternate embodiments the CUDA driver may be configured to perform this priority mapping on the fly. In other words the CUDA driver may not be configured to pre determine and store the priority mappings upon receiving the max nesting depth. Instead the CUDA driver may be configured to calculate a specific priority mapping upon receiving an application CUDA request to assign a particular valid stream priority to a stream.

Upon receiving a specific application CUDA request to launch a particular CPU launched kernel within a stream the CUDA driver accesses the stream data to determine whether the stream is associated with one of the valid device priorities . If the steam is associated with one of the valid device priorities then the CUDA driver includes the valid device priority when submitting the CPU launched kernel to the parallel processing system for launch within the stream. If the stream is not associated with one of the valid device priorities then the CUDA driver includes a default device priority when submitting the CPU launched kernel to the parallel processing system for launch within the stream. In alternate embodiments the CUDA driver may submit the CPU launched kernel to the parallel processing system without including any priority information.

Advantageously if a stream is associated with a particular valid device priority then the CUDA driver is configured to submit all CPU launched kernels within the stream at the particular valid device priority . Thus the CUDA driver may submit CPU launched kernels that initiate dynamic parallelism i.e. are configured to launch child GPU launched kernels at valid device priorities other than the lowest valid device priority . In contrast in prior art approaches to prioritizing streams CPU launched kernels that initiate dynamic parallelism are submitted at the lowest priority included in the valid device priorities thereby limiting the effectiveness of assigning priorities to streams.

As shown the two priority mappings are depicted by arrows organized vertically from a highest priority to a lowest priority . As also shown the valid device priorities defined by the parallel processing subsystem include a lowest valid device priority of 0 to a highest valid device priority of 11. Thus the total number of valid device priorities is 12.

The first software application includes the max nesting depth of 4 and the valid stream priorities . To determine the number of valid device priorities available for prioritizing streams the CUDA driver divides the number of valid device priorities by the max nesting depth and then rounds down to the nearest integer. Consequently the number of valid device priorities available for prioritizing streams included in the first software application is 3. Because the max nesting depth is 4 the CUDA driver ensures that the valid device priorities included in the priority mappings corresponding to the valid stream priorities are separated by at least 4 valid device priorities . This separation enables the parallel processing subsystem to support dynamic parallelism to a nesting depth of 4 i.e. the max nesting depth .

As shown the valid stream priorities include a lowest valid stream priority of 0 to a highest valid stream priority of 2. The priority mapping corresponding to the first software application is depicted by the arrows linking the valid stream priorities to the valid device priorities . As also shown the CUDA driver maps the valid stream priority of 0 to the valid device priority of 0. The CUDA driver then maps the valid stream priority of 1 to the valid device priority of 4. Finally the CUDA driver maps the valid stream priority of 2 to the valid device priority of 8.

The second software application includes the max nesting depth of 2 and the valid stream priorities . As previously noted herein the number of device priorities included in the valid device priorities is 12. Consequently the CUDA driver determines that the number of valid device priorities available for prioritizing streams included in the second software application is 6 12 divided by 2 . And to support dynamic parallelism the CUDA driver ensures that the valid device priorities included in the priority mappings corresponding to the valid stream priorities are separated by at least 2 valid device priorities .

As shown the valid stream priorities include a lowest valid stream priority of 0 to a highest valid stream priority of 5. The priority mapping corresponding to the second software application is depicted by the arrows linking the valid stream priorities to the valid device priorities . More specifically as shown the CUDA driver maps the valid stream priority of 0 to the valid device priority of 0. The CUDA driver then maps the valid stream priority of 1 to the valid device priority of 2 and so on.

As shown the CPU launched kernel directs the parallel processing subsystem to launch a GPU launched kernel . Because the parallel processing subsystem launches the GPU launched kernel as a child of the CPU launched kernel the parallel processing subsystem prioritizes the GPU launched kernel . In particular the parallel processing subsystem sets the device priority included in the GPU launched kernel to one higher than the device priority included in the CPU launched kernel . In other words the parallel processing subsystem sets the device priority included in the GPU launched kernel to . This prioritization ensures that the parallel processing subsystem allocates resources towards the execution of the child GPU launched kernel in preference to the parent CPU launched kernel .

While executing the GPU launched kernel launches a nested GPU launched kernel . Because the parallel processing subsystem launches the GPU launched kernel as a child of the GPU launched kernel the parallel processing subsystem prioritizes the GPU launched kernel . More specifically the parallel processing subsystem sets the device priority included in the GPU launched kernel to one higher than the device priority included in the GPU launched kernel . In other words the parallel processing subsystem sets the device priority included in the GPU launched kernel to . This prioritization ensures that the parallel processing subsystem prioritizes the child GPU launched kernel over the parent GPU launched kernel .

Similarly the GPU launched kernel launches a nested GPU launched kernel . And the parallel processing subsystem sets the device priority included in the GPU launched kernel to . After the GPU launched kernel completes executing the GPU launched kernel completes executing. And after the GPU launched kernel completes executing the GPU launched kernel completes executing. Finally after the GPU launched kernel finishes executing the CPU launched kernel complete executing and the parallel processing subsystem executes the next CPU launched kernel within the stream.

As shown a method begins at step where the software application sends an application CUDA request to the CUDA runtime API assigning a max nesting depth . At step the CUDA runtime API passes the application CUDA request to the CUDA driver . At step the CUDA driver calculates a number of available stream priorities by dividing the number of valid device priorities by the max nesting depth and subsequently rounding down to the nearest integer. At step the CUDA driver sets a current stream priority to the lowest priority included in the valid stream priorities and a current device priority to the lowest priority included in the valid device priorities . At step the CUDA driver includes i.e. stores the current stream priority with the current device priority in the priority mappings and reduces the number of available stream priorities by 1.

At step if the CUDA driver determines that the number of available stream priorities is not equal to 0 or that the current stream priority is not equal to the highest priority included in the valid stream priorities then the method proceeds to step . At step the CUDA driver increases the current stream priority by 1 and increases the current device priority by the max nesting depth Subsequently the method returns to step . Advantageously by performing the mapping in this manner the CUDA driver reserves enough and only enough valid device priorities for the parallel processing subsystem to support dynamic parallelism to the max nesting depth .

The method continues to execute steps through mapping valid stream priorities to valid device priorities until the CUDA driver has mapped all of the valid stream priorities or has performed a number of mappings equal to number of valid device priorities available for prioritizing streams. If at step the CUDA driver determines that the number of available stream priorities is equal to 0 or that the current stream priority is equal to the highest priority included in the valid stream priorities then the method proceeds to step .

At step the CUDA driver processes each unmapped priority included in the valid stream priorities by including each of the unmapped valid stream priorities with the current device priority in the priority mappings . For example suppose that the number of valid device priorities available for prioritizing steams were to be S less than the number of valid stream priorities . The CUDA driver would map the S 1 highest priorities included in the valid stream priorities to the current device priority. In alternate embodiments the CUDA driver may be configured to perform the mapping in any technically feasible manner. For example in some alternate embodiments the CUDA driver could be configured to map the S 1 lowest priorities included in the valid stream priorities to the lowest valid device priority.

As shown a method begins at step where the parallel processing subsystem receives a command from a current kernel to launch a child GPU launched kernel . The current kernel may be either a CPU launched kernel or a CPU launched kernel . If at step the parallel processing subsystem determines that the depth of the current kernel equals the max nesting depth then the method proceeds to step . The parallel processing system may determine the depth of the current kernel in any technically feasible manner. At step the parallel processing system issues an error and does not launch the child CPU launched kernel and the method terminates. In alternate embodiments the parallel processing subsystem may respond to the attempted launch of a kernel beyond the max nesting depth in any technically feasible manner.

At step if the parallel processing subsystem determines that the depth of the current kernel does not equal the max nesting depth then the method proceeds to step . At step the parallel processing subsystem sets the device priority included in the child CPU launched kernel to one higher than the device priority included in the parent kernel. This prioritization ensures that the child CPU launched kernel receives parallel processing subsystem resources in preference to the parent kernel. Advantageously in contrast to prior art techniques the current invention enables a parent CPU launched kernel to include a device priority that is not equal to the lowest priority included in the valid device priorities .

At step the parallel processing subsystem launches the child CPU launched kernel according to scheduling algorithms that incorporate the device priority included in the child CPU launched kernel . For example suppose that the parallel processing subsystem were to be executing a CPU launched kernel Klow. Further suppose that the CPU launched kernel Klow included a device priority that were lower than the device priority included in the child GPU launched . The parallel processing subsystem could preempt the CPU launched kernel Klow to execute the child GPU launched kernel .

It will be appreciated that the system and techniques described in are illustrative and that variations and modifications are possible. For example in alternate embodiments the CUDA software stack including the CUDA runtime API and the CUDA driver CUDA programming model and CUDA language may be replaced with may be replaced with any set of software programs that expose and manage parallel processing subsystem functionality e.g. OpenCL . Further the kernels may be replaced with any computational operation that may be launched on the parallel processing subsystem .

In sum together a CUDA API and a CUDA driver enable software applications to effectively exploit both the prioritization and dynamic parallelism capabilities of advanced parallel processing subsystems. More specifically the CUDA API allows software applications to assign a max nesting depth for dynamic parallelism. And the CUDA driver is configured to use the max nesting depth to determine the number of device priorities included in a set of valid device priorities supported by the parallel processing system that are available for prioritizing streams. The CUDA driver calculates the number of valid device priorities available for prioritizing streams by dividing the number of device priorities included in the set of valid device priorities by the max nesting depth and subsequently rounding down to the nearest integer. The CUDA driver then maps a set of valid stream priorities to the set of valid device priorities ensuring that each valid device priority assigned to a valid stream priority is separated by the max nesting depth. In this fashion for each valid stream priority the CUDA driver reserves enough valid device priorities to support the nested launch of child work components to the max nesting depth.

Advantageously the CUDA API and the CUDA driver enable software applications to influence the number of priority levels available for CUDA streams. Thus the CUDA driver may more effectively prioritize work components submitted to parallel processing subsystems that support dynamic parallelism compared to prior art more restrictive approaches. For example in order to reduce latency and increase throughput the application developer may configure a software application with a lower max nesting depth thereby freeing more priority levels for strategic prioritization of latency sensitive work components. Consequently the overall performance of the software application may be increased. Yet another advantage is that this approach enables the CUDA driver to productively direct the parallel processing subsystem to launch all work components included in a stream at the device priority associated with the stream. In particular the CUDA driver may launch parent work components i.e. work components that initiate dynamic parallelism at the device priority associated with the stream. Thus application developers may more comprehensively use priorities to increase the speed at which software applications execute.

While the foregoing is directed to embodiments of the present invention other and further embodiments of the invention may be devised without departing from the basic scope thereof. For example aspects of the present invention may be implemented in hardware or software or in a combination of hardware and software. One embodiment of the invention may be implemented as a program product for use with a computer system. The program s of the program product define functions of the embodiments including the methods described herein and can be contained on a variety of computer readable storage media. Illustrative computer readable storage media include but are not limited to i non writable storage media e.g. read only memory devices within a computer such as CD ROM disks readable by a CD ROM drive flash memory ROM chips or any type of solid state non volatile semiconductor memory on which information is permanently stored and ii writable storage media e.g. floppy disks within a diskette drive or hard disk drive or any type of solid state random access semiconductor memory on which alterable information is stored.

The invention has been described above with reference to specific embodiments. Persons of ordinary skill in the art however will understand that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. The foregoing description and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense.

