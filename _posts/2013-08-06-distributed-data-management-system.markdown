---

title: Distributed data management system
abstract: A distributed data management system has multiple virtual machine nodes operating on multiple computers that are in communication with each other over a computer network. Each virtual machine node includes at least one data store or “bucket” for receiving data. A digital hash map data structure is stored in a computer readable medium of at least one of the multiple computers to configure the multiple virtual machine nodes and buckets to provide concurrent, non-blocking access to data in the buckets, the digital hash map data structure including a mapping between the virtual machine nodes and the buckets. The distributed data management system employing dynamic scalability in which one or more buckets from a virtual machine node reaching a memory capacity threshold are transferred to another virtual machine node that is below its memory capacity threshold.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09122713&OS=09122713&RS=09122713
owner: Pivotal Software, Inc.
number: 09122713
owner_city: Palo Alto
owner_country: US
publication_date: 20130806
---
This application is a continuation of U.S. patent application Ser. No. 11 431 500 entitled Distributed Data Management System and filed May 9 2006 which claims priority to U.S. provisional application 60 703 678 filed Jul. 28 2005 and entitled Distributed Data Management System both of which are hereby incorporated by reference.

The present invention to data management and in particular to managing data in a multicomputer environment to provide dynamic scalability while simultaneously providing concurrent non blocking access to data.

In the field of data management maximizing both scalability and performance at the same time is a challenging problem. Scalability and performance are often conflicting objectives because improvements in one typically come at the expense of the other. In a multicomputer environment where processes are physically distributed the problem is exacerbated by network latency and communication bandwidth between nodes. The problem is even more daunting if nodes must join the distributed system in an ad hoc fashion without service interruption.

Prior solutions provide high scalability at the expense of low performance or vice versa. Prior distributed data management systems for example though highly scalable are designed to maintain invariance over several data objects at once and are thus encumbered by the need for transactional scope management and by the need for distributed global locking.

In particular Database Management Systems DBMs guarantee that data modifications are strictly serializable and thus require expensive transaction management overhead and distributed locking to insure data correctness. The need for such transaction overhead and locking greatly reduces concurrent data access and limits performance

Prior Scalable Distributed Data Structures SDDS solutions offer dynamic data scalability in a multicomputer environment but encounter vexing performance problems that limit operational utility. Like distributed DBMS solutions existing SDDS solutions inevitably encounter performance bottlenecks when accessed by a plurality of concurrent users. Though SDDS solutions can load balance data uniformly across multiple computer nodes access to data on a particular node can be blocked undesirably by concurrent requests.

SDDS solutions also encounter performance limitations when managing data with complex shapes or of large size. Data sets composed of complex relationships form deep object graphs that incur expensive serialization costs. Compared to primitive data types the computational cost of serializing and deserializing a complex object graph is significant. As a step in the process of data transfer the impact of slow serialization on overall performance can be profound.

Moreover prior SDDS solutions virtualize data access by resolving client requests from server nodes that contain actual data. If a requested object is managed by a server node that is different from the client node a network data transfer must occur to move the object from the server node to the client node. Because large objects consume significant network bandwidth and result in undesirable transfer latency SDDS solutions inevitably encounter performance bottlenecks because they must repeatedly drag large objects across the network for every remote request.

The present invention provides equivalent representations of complex data types that result in compressed byte arrays. These compressed data representations are stored and only reified back to their original format as needed. Accordingly the invention provides data translation and passivation that not only reduce resource storage footprint but also speeds data transfer. The invention provides caching and synchronization of data sets without the expensive node to node data transfers that are commonly used. The invention provides scalable data structures concurrency efficient serialization and passivation and data caching that enable applications to store and retrieve data in a manner that is optimal for use in a distributed environment where high speed service delivery and graceful scalability are critical.

In one implementation the invention includes a distributed data management system with multiple virtual machine nodes operating on multiple computers that are in communication with each other over a computer network. Each virtual machine node includes at least one data store or bucket for receiving data. A digital hash map data structure is stored in a computer readable medium of at least one of the multiple computers to configure the multiple virtual machine nodes and buckets to provide concurrent non blocking access to data in the buckets the digital hash map data structure including a mapping between the virtual machine nodes and the buckets. The distributed data management system employing dynamic scalability in which one or more buckets from a virtual machine node reaching a memory capacity threshold are transferred to another virtual machine node that is below its memory capacity threshold.

The present invention eliminates the need for the transaction management of conventional systems and maximizes concurrency through a distributed data structure that enables concurrent access to all data at all times even as the data structure is growing. The disclosed invention overcomes this concurrency limitation with its non blocking approach to data management while offering the dynamic scalability benefits of SDDS.

The challenge of creating a data management system that scales automatically to any size enables concurrent user access and guarantees high performance in a multicomputer environment is a daunting task. The method and system implemented herein solve the ubiquitous data management problem of high performance concurrent access to data in a distributed environment wherein the amount of data may grow dynamically and be modified at anytime.

Additional objects and advantages of the present invention will be apparent from the detailed description of the preferred embodiment thereof which proceeds with reference to the accompanying drawings.

In step a distributed hash map DHM data structure is defined for a class or type of data objects or elements sometimes referred to as a class of data objects with for example DHM.create stocks properties . DHM data structure includes a metadata configuration region with a spacename field e.g. Stocks in the illustration of and a configuration definition field . DHM distributed data structure also includes a metadata bucket to node mapping field .

Configuration definition field has a nodes subfield a total size subfield and a num buckets subfield . Nodes subfield defines or sets the number of nodes included for the class of data objects. Total size subfield sets the memory size of each logical bucket at each node and num buckets subfield sets the number of logical buckets at each node . Metadata bucket to node mapping field maps specific logical buckets to specific nodes .

In step virtual machine nodes are started with their corresponding logical buckets for data objects. is an illustration of for example virtual machine nodes and which are started for the Stocks class of data objects. Node has bucket node has buckets and and node has bucket .

In one implementation distributed hash map data system uses the java.util.Map application programming interface which is available in the JAVA 2 Standard Edition development package. The java.util.Map application programming interface accepts two type variables one for a key type and one for a value type and maps keys to values such that a map cannot contain duplicate keys. Each key can map to at most one value.

Map entries or keys are distributed across one or more distributed nodes and therefore provide a transparent way to partition data across the nodes in a dynamic fashion that maximizes the physical memory available for caching at any given moment. Individual processes participate as nodes in the management of a single logical DHM data structure by providing real physical memory for storing a subset of the overall content of the data structure. Such storage is referred to as a partition. All nodes providing partitions for the same DHM data structure are members of the same underlying distributed data management system .

Only one node is responsible for each map entry or key . In one implementation for example each data object physically resides on just a single host or home node in a compressed byte array format thereby maximizing the usage of available system wide resources and the volume of data that can be cached in memory to improve performance. On lookup a key s hash code determines the bucket identifier where the key s value is stored. Node location of the bucket is readily accessed via system metadata structures guaranteeing a cache entry is resolved in at most a single network hop.

Accordingly DHM data structure is a non blocking distributed data structure in which all entries key value pairs are distributed throughout a cluster of network connected physical machine nodes . DHM data structure provides a logical naming unit or a user namespace. Because DHM entries live only within a single memory node of the multicomputer environment consistency management overhead can be dramatically reduced. Multiple DHM data structures may be created within a single computer network . For example DHM data structure for Stocks Currencies Sellers and all their entries may be physically distributed across available machine resources in computer network . Thus any system node can contain entries for different DHM data structures . To add search and remove data the system provides a simple familiar interface that implements the java.util.Map API put key value get key delete key etc. The name of the DHM data structure is scoped to the underlying distributed system specified by hostname and port number.

A logical DHM data structure is identified by a unique name and accessed using a static factory method. The implementation internally synchronizes runtime metadata across all member nodes. A connection to the system is automatically established when a virtual machine process loads the class implementing the DHM data structure . Connections are gracefully shutdown during normal virtual machine shutdown. The implementation uses point to point communication between nodes based on sockets. Applications can fine tune the transport layer through a variety of configuration options such as number of retries retry wait time TCP buffer sizes etc. Each system node monitors socket connections and does automatic cleanup when connections go away with minimal impact on application clients.

Each node may be configured with a maximum memory threshold. Node memory management process manages node memory and migrates data as node capacity approaches the threshold maximum. This process of dynamic data migration is referred to as rebalancing which is managed by a virtual machine wide rebalancing thread that is started lazily when a DHM instance is created.

The rebalancing thread periodically wakes up based on a configured sleep time and checks available memory step . If any DHM instance in the virtual machine has exceeded the maximum memory step then a rebalancing operation is initiated. A rebalancing operation iterates over all distributed map instances looking for the fullest bucket in all DHM instances step and then sends a request to other member nodes to accept data for rebalancing step . Requests are first sent to nodes managing the current DHM data structure and then are sent to other nodes participating in separate DHM data structures .

The implementation provides a high level of concurrency by storing node level entries in concurrent data structures. During rebalancing step get operations are non blocking but put and replace operations on the bucket being migrated are blocked until migration successfully completes. To minimize latency application clients can optionally turn on local Least Recently Used LRU caching of entries in the DHM data structure . The local LRU caching provides very fast access rates when a small set of objects are more frequently accessed. The LRU cache is always kept synchronized with the underlying DHM.

Node memory management process supports dynamic scalability of the DHM system. In order to manage dynamic growth the system gracefully handle changes to the amount of system wide free memory. For example as more and more entries are hashed to a particular bucket the free memory of the node designated to host entries of the bucket will eventually reach a fullness threshold. In this case the system attempts to rebalance its load distribution among available nodes . Similarly when a new node joins the system the system handles the new addition by offloading bucket entries lazily to this fresh node . Rebalancing logic is triggered when node allocation thresholds are encountered and when new nodes join the system.

Actual rebalancing of occurs through bucket remapping of the BucketNodeRegion  table whereby one node relinquishes management of one or more buckets to another node . As a simple way of rebalancing bucket remapping creates high load factor as follows. When a node s allocation threshold configured by the rebalance threshold property is reached the node requests a remap operation to initiate system wide load balancing. The remap operation begins with the node first calculating the size s of its fullest bucket or buckets . The node then engages its peers with a Bucket Remapping Protocol dramatized as follows . . .

Node requester Hey I m filling up does anybody have space to manage one of my buckets I d like to do this remap now to be proactive and asynchronous . 

Node Great I m done allocating them . . . and I updated the globally accessible BucketNodeRegion  Table that maps buckets to nodes. 

Node0 Thanks . . . now I ll remove my entries for the new bucket and forward any concurrent requests to you immediately. 

Once a helper node is found buckets are moved en masse to the rebalancing node and the BucketNodeRegione  table is updated accordingly. To maintain high concurrency get operations are non blocking throughout the duration of a bucket remapping operation. Modifications put and remove however are postponed until the bucket remapping is complete.

As a common step in the process of sending an object graph from one node to another serialization is often a dominant factor in system performance. Therefore to minimize performance bottlenecks when managing data with complex shape or large size a serialization step is implemented that compresses and canonicalizes all data type instances. Compared to stock Java serialization this serialization step significantly reduces the memory storage footprint as well as the transfer latency of complex data types. Even simple data types benefit from this serialization process . For example a large two dimensional byte array consisting of java.lang.Integer objects can be reduced to roughly half the size of the original format.

Server nodes of the DHM store all data in byte array representations that are the result of serialization process rather than as fully reified objects. This maximizes the available storage space. When a client makes a key request for a given object the object s byte array is transported to the client node and then transformed back into its connected graph of related objects.

When data is put into the DHM data structure the java.lang.Object hashCode for the entry s key is fed into a hashing algorithm resulting in a unique value that is based on a simple modulus function as illustrated in . The hash result is then mapped onto a virtual address space known as Bucket Space that is accordingly partitioned into buckets . Bucket space starts from the lowest hash value and ranges to the maximum hash value. Demarcations in bucket space define individual buckets . Every key is thus mapped to a unique bucket range. Because the hashing function creates a uniform distribution population of bucket space similarly occurs in a uniform manner. Each bucket is then mapped to a physical host node through a BucketNodeRegion .

If a bucket is free that is if it is not previously mapped the DHM system will automatically map the current node i.e. the node where the put is invoked to the free bucket so long as the node is configured to manage buckets . If the current node cannot accommodate the new bucket with the entry value peer nodes will be sent a request to manage the bucket along with the size of the entry value. If a peer node can accommodate the new bucket it will respond affirmatively to the current node and the current node will initiate a remote put operation to the helper node . If no helper node is available for bucket management then a DistributedHashMapException RuntimeException is thrown alerting the put invoker that the DHM system has reached its total capacity.

Assuming a bucket and node can be resolved to complete the put operation the value of the submitted key is ultimately sent to the mapped hosting node found via the BucketNodeRegion  which binds the key into the node s private memory space. This memory space resides solely in the heap of a Java virtual machine in the form of a Map instance.

To optimize lookup performance Least Recently Used LRU caches are created on client nodes . A LRU cache holds a configurable number of entries or is based on a configurable memory size. When the entry limit or memory limit is exceeded the least recently used entry is removed to accommodate any new entries. Contents in the LRU cache are kept synchronized with the underlying values stored in remote nodes through a simple modification count mechanism e.g. a vector timestamp . With each get operation the modification count of the cached value is sent to the remote node where the real value of the key is managed. If the cached modification count is out of date the remote node sends the requester the updated value. If the cached modification count is still current the remote node only sends back a Boolean saying that the cached value is still valid and can thus be utilized. The design of this synchronization mechanism is suited for highly concurrent data where modifications to remote data are frequent and thus exploit lazy detection of invalid data. This is in contrast to conventional mechanisms that eagerly push invalidations and updates to remote clients.

Enterprise operators express the use case where memory bandwidth must be utilized as efficiently as possible. In particular businesses cite the use case where they would like to cache huge volumes of data in distributed shared memory to avoid latency of database lookups ultimately improving application scalability and performance while minimizing database licensing costs. By storing unique data on a single physical host the method and system herein optimize the use of system wide memory allowing multicomputer environment to cache enormous volumes of data for complete database offload.

Furthermore configurability of the system enables processes to be explicitly tuned. When a virtual machine node must preserve much of its working set memory for application server duties the node can be configured as a read only client to the system only accessing data from peer server nodes while forgoing hosting of any DHM entries. In this way the application server node can effectively preserve its heap for servicing client requests while delegating distributed caching responsibilities to a plethora of overflow nodes.

In the use case involving round robin web servers or other load balancing schemes the system can store stateful data in a distributed fashion eliminating the constraints of session affinity and enabling randomized client access patterns.

Address space limitations on 32 bit operating systems limit the amount of addressable data in practice to less than 4 GB and sometimes to less than 2 GB. With the disclosed system a loaded machine with far more than 4 GB of RAM can be readily configured to host several DHM nodes that collectively cache well beyond 4 GBs of unique data.

Caching by nature targets read mostly data and the disclosed system can significantly augment performance scalability and transparency in such scenarios. However because the system is highly concurrent and deliberately unencumbered by consistency management duties the system can also address use cases where data sets are rapidly changing. In stark contrast to the use case where data is read only the system can be used for managing real time data. In other words the system addresses the use case where separate lines of business must share volumes of in motion data . Such business use cases inevitably lead to the requirement for an ad hoc enterprise wide operational data store. This is precisely the role that the disclosed system can play.

Having described and illustrated the principles of our invention with reference to an illustrated embodiment it will be recognized that the illustrated embodiment can be modified in arrangement and detail without departing from such principles. It should be understood that the programs processes or methods described herein are not related or limited to any particular type of computer apparatus unless indicated otherwise. Various types of general purpose or specialized computer apparatus may be used with or perform operations in accordance with the teachings described herein. Elements of the illustrated embodiment shown in software may be implemented in hardware and vice versa.

In view of the many possible embodiments to which the principles of our invention may be applied it should be recognized that the detailed embodiments are illustrative only and should not be taken as limiting the scope of our invention. Rather we claim as our invention all such embodiments as may come within the scope and spirit of the following claims and thereto.

