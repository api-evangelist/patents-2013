---

title: Physical path determination for virtual network packet flows
abstract: In general, techniques are described for determining a physical network path taken by packets of a network packet flow. The techniques may be applied to determine, or “trace,” a physical network path in the virtualized network domain. In some examples, a network device includes one or more processors and a switch executed by the processors to forward packets of a packet flow to a physical network path. The network device also includes a flow trace module to generate one or more flow trace packets having incrementally increasing respective time-to-live (TTL) values, wherein the switch module forwards the flow trace packets on an outbound interface of the network device for the physical network path, and wherein the flow trace module receives corresponding time exceeded messages for the flow trace packets, wherein each of the time exceeded message includes a source network address of a network element on the physical network path.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08750288&OS=08750288&RS=08750288
owner: Juniper Networks, Inc.
number: 08750288
owner_city: Sunnyvale
owner_country: US
publication_date: 20130315
---
This application claims the benefit of U.S. Provisional Application No. 61 729 474 filed Nov. 23 2012 U.S. Provisional Application No. 61 723 684 filed Nov. 7 2012 U.S. Provisional Application No. 61 723 685 filed Nov. 7 2012 U.S. Provisional Application No. 61 722 696 filed Nov. 5 2012 U.S. Provisional Application No. 61 721 979 filed Nov. 2 2012 U.S. Provisional Application No. 61 721 994 filed Nov. 2 2012 U.S. Provisional Application No. 61 718 633 filed Oct. 25 2012 U.S. Provisional Application No. 61 656 468 filed Jun. 6 2012 U.S. Provisional Application No. 61 656 469 filed Jun. 6 2012 and U.S. Provisional Application No. 61 656 471 filed Jun. 6 2012 the entire content of each of which being incorporated herein by reference.

In a typical cloud data center environment there is a large collection of interconnected servers that provide computing and or storage capacity to run various applications. For example a data center may comprise a facility that hosts applications and services for subscribers i.e. customers of data center. The data center may for example host all of the infrastructure equipment such as networking and storage systems redundant power supplies and environmental controls. In a typical data center clusters of storage systems and application servers are interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers. More sophisticated data centers provide infrastructure spread throughout the world with subscriber support equipment located in various physical hosting facilities.

In general techniques are described for determining a physical network path taken by packets of a network packet flow. The techniques may be applied to determine or trace a physical network path in the virtualized network domain. In a virtualized or overlay network environment the edge of the network extends from a physical network element e.g. a switch or a router to software switches i.e. virtual switches each executed by a hypervisor or a host operating system of a physical server. In such an environment physical servers may execute application instances that communicate by exchanging layer three L3 or network packets using a virtual network that is implemented by one or more software switches and that is orthogonal from the physical network and the corresponding physical addressing scheme. Virtual network elements which include both the virtual switches and physical network elements encapsulate packets generated or consumed by instances of the application in the virtual network domain in a tunnel header that includes addresses that conform to the physical network addressing scheme. Accordingly and hereinafter a packet generated or consumed by application instances may be referred to as an in inner packet while the physical network packet that includes the inner packet encapsulated within the added tunnel header may be referred to as an outer packet. The tunnel header allows the physical network to tunnel the inner packet toward a destination virtual switch for delivery to a destination application instance. In some cases the tunnel header may include sub headers for multiple layers such as a transport layer e.g. Transmission Control Protocol TCP or User Datagram Protocol UDP header network layer and or tunneling layer.

In some examples a virtual network element receives a request to determine a physical network path taken by packets of a network packet flow. For a packet that corresponds to a particular packet flow the virtual network element generates a first flow trace packet that has one or more tunnel header fields identical to packets of the packet flow. As a result the first flow trace packet will traverse a physical network path identical to packets of the packet flow. To determine a first next hop along the physical network path taken by the packet flow the virtual network element sets a time to live TTL value for the flow trace packet to 1 then forwards the first flow trace packet to the first next hop according to the virtual network element network forwarding table. Because the TTL value for the first flow trace packet is set to 1 the first next hop discards the first flow trace packet and returns a Time Exceeded message for the first flow trace packet such as an Internet Control Message Protocol ICMP Time Exceeded message. The time exceeded message includes a network address of the first next hop. The virtual network element iterates TTL values on successive flow trace packets otherwise similar to the first flow trace packet and forwards the successive flow trace packets according to the virtual network element forwarding table. Successive next hops along the physical path for the packet flow therefore each receives a flow trace packet with TTL value set to 1 and each returns a time exceeded message. The virtual network element can use the returned time exceeded messages returned from physical next hops along the physical path to generate a list of the physical next hops which the virtual network element can return to a device that has requested the physical network path.

Because the flow trace packet is a trace packet and may not include application data for an application the virtual network element may in some instances add a special flow trace packet indicator to a field of the tunnel header of flow trace packets. A tunnel termination virtual network element or tunnel endpoint ordinarily decapsulates received outer packets of the tunneled packet flow to remove the outer header and forwards the resulting inner packet toward an application. The flow trace packet indicator indicates to the tunnel endpoint that a received packet is a flow trace packet and should be discarded rather than forwarded. The tunnel endpoint therefore identifies packet flow packets that include the flow trace packet indicator and discards the packet. In some instances the tunnel endpoint may have previously received an antecedent flow trace packet for the packet flow for which the tunnel endpoint returned a time exceeded message to the issuing virtual network element. In some examples the tunnel endpoint may return an ICMP Echo Reply message or other confirmation message upon receiving a flow trace packet to the issuing virtual network element in order to confirm receipt of the flow trace packet at the termination of the tunnel.

The techniques described herein may provide one or more advantages. For example the techniques may allow for determination of a physical network path for a packet flow traversing a virtualized network domain. Determining a physical network path for a packet flow using the described techniques may also overcome certain limitations of the network trace route utility conventionally used to determine a physical network path. In some cases multiple paths of equal cost exist between a virtual network element and a destination. The virtual network element may allocate packet flows having the same source and destination to different equal cost paths according to the particularities of packet flow packet headers. In contrast to the trace route utility which may produce ICMP echo request messages that form a packet flow allocated by a virtual network element to a different one of the multiple paths the techniques may ensure that the path taken by the flow trace packets matches the physical network path taken by a corresponding packet flow in a multi path environment.

In one aspect a method for determining a physical network path of a packet flow includes generating with a network device one or more flow trace packets having incrementally increasing respective time to live TTL values. The method also includes sending with the network device the flow trace packets on an outbound interface of the network device for the physical network path. The method further includes receiving with the network device corresponding time exceeded messages for the flow trace packets wherein each of the time exceeded message includes a source network address of a network element on the physical network path.

In another aspect a network device includes one or more processors and a switch executed by the processors to forward packets of a packet flow to a physical network path. The network device also includes a flow trace module to generate one or more flow trace packets having incrementally increasing respective time to live TTL values wherein the switch module forwards the flow trace packets on an outbound interface of the network device for the physical network path wherein the flow trace module receives corresponding time exceeded messages for the flow trace packets and wherein each of the time exceeded message includes a source network address of a network element on the physical network path.

In another aspect a non transitory computer readable medium contains instructions. The instructions cause one or more programmable processors to generate with a network device one or more flow trace packets having incrementally increasing respective time to live TTL values. The instructions further cause the programmable processors to send with the network device the flow trace packets on an outbound interface of the network device for the physical network path. The instructions further cause the programmable processors to receive with the network device corresponding time exceeded messages for the flow trace packets wherein each of the time exceeded message includes a source network address of a network element on a physical network path. The instructions also cause the programmable processors to determine with the network device the physical network path using the source network addresses of the time exceeded message.

The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features objects and advantages of the invention will be apparent from the description and drawings and from the claims.

In some examples data center may represent one of many geographically distributed network data centers. As illustrated in the example of data center may be a facility that provides network services for customers . Customers may be collective entities such as enterprises and governments or individuals. For example a network data center may host web services for several enterprises and end users. Other exemplary services may include data storage virtual private networks traffic engineering file service data mining scientific or super computing and so on. In some embodiments data center may be individual network servers network peers or otherwise.

In this example data center includes a set of storage systems and application servers A X herein servers interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers. Switch fabric is provided by a set of interconnected top of rack TOR switches A BN collectively TOR switches coupled to a distribution layer of chassis switches A M collectively chassis switches . Although not shown data center may also include for example one or more non edge switches routers hubs gateways security devices such as firewalls intrusion detection and or intrusion prevention devices servers computer terminals laptops printers databases wireless mobile devices such as cellular phones or personal digital assistants wireless access points bridges cable modems application accelerators or other network devices.

In this example TOR switches and chassis switches provide servers with redundant multi homed connectivity to IP fabric and service provider network . Chassis switches aggregate traffic flows and provides high speed connectivity between TOR switches . TOR switches may be network devices that provide layer 2 MAC and or layer 3 e.g. IP routing and or switching functionality. TOR switches and chassis switches may each include one or more processors and a memory and that are capable of executing one or more software processes. Chassis switches are coupled to IP fabric which performs layer 3 routing to route network traffic between data center and customers by service provider network .

Virtual network controller VNC provides a logically and in some cases physically centralized controller for facilitating operation of one or more virtual networks within data center in accordance with one or more embodiments of this disclosure. In some examples virtual network controller may operate in response to configuration input received from network administrator .

Typically the traffic between any two network devices such as between network devices within IP fabric not shown or between servers and customers or between servers for example can traverse the physical network using many different paths. For example there may be several different paths of equal cost between two network devices. In some cases packets belonging to network traffic from one network device to the other may be distributed among the various possible paths using a routing strategy called multi path routing at each network switch node. For example the Internet Engineering Task Force IETF RFC 2992 Analysis of an Equal Cost Multi Path Algorithm describes a routing technique for routing packets along multiple paths of equal cost. The techniques of RFC 2992 analyzes one particular multipath routing strategy involving the assignment of flows to bins by hashing packet header fields that sends all packets from a particular network flow over a single deterministic path.

For example a flow can be defined by the five values used in a header to a packet or five tuple i.e. the protocol Source IP address Destination IP address Source port and Destination port that are used to route packets through the physical network. For example the protocol specifies the communications protocol such as TCP or UDP and Source port and Destination port refer to source and destination ports of the connection. A set of one or more packet data units PDUs that match a particular flow entry represent a flow. Flows may be broadly classified using any parameter of a PDU such as source and destination MAC and IP addresses a Virtual Local Area Network VLAN tag transport layer information a Multiprotocol Label Switching MPLS or Generalized MPLS GMPLS label and an ingress port of a network device receiving the flow. For example a flow may be all PDUs transmitted in a Transmission Control Protocol TCP connection all PDUs sourced by a particular MAC address or IP address all PDUs having the same VLAN tag or all PDUs received at the same switch port.

Each individual switch router in the network may perform its own independent hashing computation to determine the path that will be used by a particular flow. The ECMP paths between the first and second network devices may be viewed by the virtual network as one physical connection as their packet inner packet is encapsulated by an outer IP header.

In the example of multiple paths A B collectively path of equal routing cost exist from server A to server X. Path B traverses a physical network path proceeding from server A and consisting of TOR switch A chassis switch A TOR switch N and server X. Path A by contrast traverses a physical network path proceeding from server A and consisting of TOR switch A chassis switch M TOR switch N and server X. Server A may allocate packet flows generated by applications executing on server A not shown in to any of paths according to an allocation scheme. The allocation scheme may allocate packets according to an invariant selection of the packet header fields including source IP address destination IP address IP protocol IPv4 or next header IPv6 transport layer source port and or transport layer destination port for example. Invariant is used to refer to packet fields that do not change among packets belonging to a packet flow. Packets belonging to a packet flow allocated to path A for example traverse path A to reach server X.

In accordance with techniques described in this disclosure server A may receive a request A to determine a physical network path traversed by packets of a particular packet flow that server A has allocated to path A. Server A generates a first flow trace packet that has at least the packet header fields identical to packets of the requested packet flow sufficient to cause server A to match the first flow trace packet to the packet flow according to the allocation scheme and thereby cause the first flow trace packet to be forwarded according to path A. As a result the first flow trace packet will traverse a physical network path identical to packets of the packet flow.

To determine a first next hop along the physical network path taken by the packet flow server A sets a time to live TTL value for the flow trace packet to 1 then forwards the first flow trace packet to the first next hop of path A i.e. TOR switch A. Because the TTL value for the first flow trace packet is set to 1 TOR switch A decrements the TTL value to zero discards the first flow trace packet and returns an Internet Control Message Protocol ICMP Time Exceeded message for the first flow trace packet which includes a network address of TOR switch A. Although described as an ICMP Time Exceeded Message the time exceeded message may include another type of message that indicates TOR switch A has received an IP packet having TTL value 1.

Server A generates a second flow trace packet otherwise similar to the first flow trace packet but increments the TTL value to set the TTL value on the second flow trace packet to 2. Server A forwards the second flow trace packet along path A. Chassis switch M receives the second flow trace packet discards the packet and responsively returns an ICMP Time Exceeded message to server A. Server A iteratively generates additional flow trace packets for the packet flow incrementing the TTL value with each successive flow trace packet and forwarding the additional flow trace packets along path A. As a result successive next hop along the path A each receives for the packet flow a flow trace packet with zero TTL and each returns an ICMP Time Exceeded message to server A. In contrast to the trace route utility which if executed by server A would produce ICMP echo request messages that form a packet flow that could be allocated by server A to path B the techniques may ensure that the path taken by the flow trace packets matches the physical network path taken by a corresponding packet flow i.e. path A in a multi path environment. Server A can use the returned ICMP Time Exceeded messages returned from physical next hops along path A to generate a list of the physical next hops which server A returns to VNC in response B. In some instances the techniques described above as being performed by server A may instead or additionally be performed by elements of switch fabric such as TOR switches and chassis switches .

Each virtual switch may execute within a hypervisor a host operating system or other component of each of servers . In the example of virtual switch A executes within hypervisor also often referred to as a virtual machine manager VMM which provides a virtualization platform that allows multiple operating systems to concurrently run on one of host servers . In the example of virtual switch A manages virtual networks each of which provides a network environment for execution of one or more virtual machines VMs on top of the virtualization platform provided by hypervisor . Each VM is associated with one of the virtual subnets VN VN managed by the hypervisor .

In general each VM may be any type of software application and may be assigned a virtual address for use within a corresponding virtual network where each of the virtual networks may be a different virtual subnet provided by virtual switch A. A VM may be assigned its own virtual layer three L3 IP address for example for sending and receiving communications but may be unaware of an IP address of the physical server A on which the virtual machine is executing. In this way a virtual address is an address for an application that differs from the logical address for the underlying physical computer system e.g. server A in the example of .

In one implementation each of servers includes a corresponding one of virtual network VN agents A X collectively VN agents that controls the overlay of virtual networks and that coordinates the routing of data packets within server . In general each VN agent communicates with virtual network controller which generates commands to control routing of packets through data center . VN agents may operate as a proxy for control plane messages between virtual machines and virtual network controller . For example a VM may request to send a message using its virtual address via the VN agent A and VN agent A may in turn send the message and request that a response to the message be received for the virtual address of the VM that originated the first message. In some cases a VM may invoke a procedure or function call presented by an application programming interface of VN agent A and the VN agent A may handle encapsulation of the message as well including addressing.

In one example network packets e.g. layer three L3 IP packets or layer two L2 Ethernet packets generated or consumed by the instances of applications executed by virtual machines within the virtual network domain may be encapsulated in another packet e.g. another IP or Ethernet packet that is transported by the physical network. The packet transported in a virtual network may be referred to herein as an inner packet while the physical network packet may be referred to herein as an outer packet. Encapsulation and or de capsulation of virtual network packets within physical network packets may be performed within virtual switches e.g. within the hypervisor or the host operating system running on each of servers . As another example encapsulation and de capsulation functions may be performed at the edge of switch fabric at a first hop TOR switch that is one hop removed from the application instance that originated the packet. This functionality is referred to herein as tunneling and may be used within data center to create one or more overlay networks. Besides IPinIP other example tunneling protocols that may be used include IP over GRE VxLAN MPLS over GRE etc.

As noted above virtual network controller provides a logically centralized controller for facilitating operation of one or more virtual networks within data center . Virtual network controller may for example maintain a routing information base e.g. one or more routing tables that store routing information for the physical network as well as one or more overlay networks of data center . Similarly switches and virtual switches maintain routing information such as one or more routing and or forwarding tables. In one example implementation virtual switch A of hypervisor implements a network forwarding table NFT for each virtual network . In general each NFT stores forwarding information for the corresponding virtual network and identifies where data packets are to be forwarded and whether the packets are to be encapsulated in a tunneling protocol such as with a tunnel header that may include one or more headers for different layers of the virtual network protocol stack.

For example virtual machine VM sends a packet an inner packet virtual switch A by an internal link. Virtual switch A uses NFTto look up a virtual network destination network address for packet . NFTspecifies an outbound interface for virtual switch A and encapsulation for packet . Virtual switch A applies the encapsulation to add a tunnel header to generate outer packet and outputs outer packet on the outbound interface in this case toward TOR switch A.

The routing information may for example map packet key information e.g. destination IP information and other select information from packet headers to one or more specific next hops within the networks provided by virtual switches and switch fabric . In some case the next hops may be chained next hop that specify a set of operations to be performed on each packet when forwarding the packet such as may be used for flooding next hops and multicast replication. In some cases virtual network controller maintains the routing information in the form of a radix tree having leaf nodes that represent destinations within the network. U.S. Pat. No. 7 184 437 provides details on an exemplary embodiment of a router that utilizes a radix tree for route resolution the contents of U.S. Pat. No. 7 184 437 being incorporated herein by reference in its entirety.

As shown in each virtual network provides a communication framework for encapsulated packet communications for the overlay network established through switch fabric . In this way network packets associated with any of virtual machines may be transported as encapsulated packet communications via the overlay network. In addition in the example of each virtual switch includes a default network forwarding table NFTand provides a default route that allows a packet to be forwarded to virtual subnet VN without encapsulation i.e. non encapsulated packet communications per the routing rules of the physical network of data center . In this way subnet VN and virtual default network forwarding table NFTprovide a mechanism for bypassing the overlay network and sending non encapsulated packet communications to switch fabric .

Moreover virtual network controller and virtual switches may communicate using virtual subnet VN in accordance with default network forwarding table NFTduring discovery and initialization of the overlay network and during conditions where a failed link has temporarily halted communication via the overlay network. Once connectivity with the virtual network controller is established the virtual network controller updates its local routing table to take into account new information about any failed links and directs virtual switches to update their local network forwarding tables . For example virtual network controller may output commands to virtual network agents to update one or more NFTs to direct virtual switches to change the tunneling encapsulation so as to re route communications within the overlay network for example to avoid a failed link.

When link failure is detected a virtual network agent local to the failed link e.g. VN Agent A may immediately change the encapsulation of network packet to redirect traffic within the overlay network and notifies virtual network controller of the routing change. In turn virtual network controller updates its routing information any may issues messages to other virtual network agents to update local routing information stored by the virtual network agents within network forwarding tables .

Virtual switch A of server A includes flow trace module FTM to determine physical network paths traversed by packet flows switched by virtual switch A. Flow trace module may be executed by hypervisor by a host operating system of server A or by VM agent A of server . To trace a physical network path traversed by outer packet flow trace module generates a flow trace packet A that includes a tunnel header similar to that of outer packet . However flow trace module initially sets a TTL value of first flow trace packet A to 1. In addition flow trace module may set a special flow trace packet indicator in a field of the tunnel header of flow trace packet A and subsequent flow trace packets e.g. flow trace packet B corresponding to flow trace packet A to direct a receiving virtual switch of data center to discard the inner packet of the first flow trace packet if received with a TTL value set greater than 1. Virtual switch A outputs flow trace packet A on the output interface shared by flow trace packet A and outer packet . TOR switch A that is a first next hop on a physical network path for flow trace packet A and outer packet receives flow trace packet A decrements the TTL value to 0 and because the TTL value is 0 returns an ICMP Time Exceeded message A to virtual switch A.

ICMP Time Exceeded message A may include a Time Exceeded message Code to indicate that TOR switch A discarded flow trace packet A due to an expired TTL field. ICMP Time Exceeded message A is an IP packet that includes an IMCP Time Exceeded message ICMP Type 11 . The IP packet header has a source IP address of TOR switch A and a destination IP address that is the source IP address of flow trace packet A e.g. the IP address of virtual machine VM . The IMCP Time Exceeded message includes the IP header and the first eight bytes of the encapsulated data of flow trace packet A.

Flow trace module additionally generates flow trace packet B which is similar to flow trace packet A but has a TTL value of 2. Virtual switch A outputs flow trace packet B on the output interface shared by flow trace packet B and outer packet . Chassis switch A receives flow trace packet B with TTL value set to 1 having been TTL value decremented and forwarded by TOR switch A. Chassis switch A like TOR switch A with respect to flow trace packet A decrements the TTL value of flow trace packet B to 0 and therefore return an ICMP Time Exceeded message B to virtual switch A. ICMP Time Exceeded message B is similar to ICMP Time Exceeded message A but has a source IP address that is an IP address of chassis switch A.

Flow trace module continues generating flow trace packets in this manner until switch A receives a confirmation message that one of the subsequent flow trace packets has arrived at another of virtual switches in this case server X. A confirmation message may include e.g. an ICMP Echo Reply message. In this way FTM of switch A may receive messages including ICMP Time Exceeded messages A B from each of the physical network elements on a physical network path traversed by outer packet . Flow trace module may aggregate IP addresses for each of the physical network elements from the respective received messages into a list which FTM may send to e.g. a virtual network controller of data center . Flow trace module may in some instances append a virtual IP address for server X received in confirmation message to the list. Flow trace module returns the list of IP addresses for the physical network elements to a requesting device or may provide the list to another component of virtual switch A or a host operating system of server A for example.

In this example chassis switch CH which may be any of chassis switches of is coupled to Top of Rack TOR switches A B TORs by chassis link A and chassis link B respectively chassis links . TORs may in some examples be any of TORs of . In the example of TORs are also coupled to servers A B servers by TOR links A D TOR links . Servers may be any of servers . Here servers communicate with both TORs and can physically reside in either associated rack. TORs each communicate with a number of network switches including chassis switch .

Chassis switch has a processor A in communication with an interface for communication with a network as shown as well as a bus that connects a memory not shown to processor A. The memory may store a number of software modules. These modules include software that controls network routing such as an OSPF module not shown containing instructions for operating the chassis switch in compliance with the OSPF protocol. Chassis switch maintains routing table RT table A containing routing information for packets which describes a topology of a network. Routing table A may be for example a table of packet destination Internet protocol IP addresses and the corresponding next hop e.g. expressed as a link to a network component. Reference herein to IP may refer to IPv4 or IPv6.

TORs each have a respective processor B C an interface in communication with chassis switch and a memory not shown . Each memory contains software modules including an OSPF module and routing table B C as described above.

TORs and chassis switch may exchange routing information specifying available routes such as by using a link state routing protocol such as Open Shortest Path First OSPF or IS IS. TORs may be configured as owners of different routing subnets. For example TOR A is configured as the owner of Subnet which is the subnet 10.10.10.0 24 in the example of and TOR A is configured as the owner of Subnet which is the subnet 10.10.11.0 24 in the example of . As owners of their respective Subnets TORs locally store the individual routes for their subnets and need not broadcast all route advertisements up to chassis switch . Instead in general TORs will only advertise their subnet addresses to chassis switch .

Chassis switch maintains a routing table RT table A which includes routes expressed as subnets reachable by TORs based on route advertisements received from TORs . In the example of RT table A stores routes indicating that traffic destined for addresses within the subnet 10.10.11.0 24 can be forwarded on link B to TOR B and traffic destined for addresses within the subnet 10.10.10.0 24 can be forwarded on link A to TOR A.

In typical operation chassis switch receives Internet Protocol IP packets through its network interface reads the packets destination IP address looks up these addresses on routing table A to determine the corresponding destination component and forwards the packets accordingly. For example if the destination IP address of a received packet is 10.10.10.0 i.e. the address of the subnet of TOR A the routing table of chassis switch indicates that the packet is to be sent to TOR A via link A and chassis switch transmits the packet accordingly ultimately for forwarding to a specific one of the servers .

Similarly each of TORs receives Internet Protocol IP packets through its network interface reads the packets destination IP address looks up these addresses on its routing table to determine the corresponding destination component and forwards the packets according to the result of the lookup. In some cases a network element e.g. one of TORs or chassis switch may receive an IP packet having a TTL value of 1. As a result the network element returns an ICMP Time Exceeded message to the source IP address of the packet. In accordance with techniques described herein servers may walk a physical network path of system by issuing successive flow trace packets with iterated TTL values and receive in response ICMP Time Exceeded messages from successive physical network elements along the path.

In one example iteration server A sends flow trace packet an IP packet having a TTL value set to 1 to TOR A. Flow trace packet may represent any of flow trace packets of . TOR A receives flow trace packet decrements the TTL value and because the TTL value is now 0 returns an ICMP Time Exceeded message to server A.

Virtual network controller VNC of illustrates a distributed implementation of a VNC that includes multiple VNC nodes A N collectively VNC nodes to execute the functionality of a data center VNC including managing the operation of virtual switches for one or more virtual networks implemented within the data center. Each of VNC nodes may represent a different server of the data center e.g. any of servers of or alternatively on a server or controller coupled to the IP fabric by e.g. an edge router of a service provider network or a customer edge device of the data center network. In some instances some of VNC nodes may execute as separate virtual machines on the same server.

Each of VNC nodes may control a different non overlapping set of data center elements such as servers individual virtual switches executing within servers individual interfaces associated with virtual switches chassis switches TOR switches and or communication links. VNC nodes peer with one another using peering links to exchange information for distributed databases including distributed databases A K collectively distributed databases and routing information e.g. routes for routing information bases A N collectively RIBs . Peering links may represent peering links for a routing protocol such as a Border Gateway Protocol BGP implementation or another peering protocol by which VNC nodes may coordinate to share information according to a peering relationship.

VNC nodes of VNC include respective RIBs each having e.g. one or more routing tables that store routing information for the physical network and or one or more overlay networks of the data center controlled by VNC . In some instances one of RIBs e.g. RIB A may store the complete routing table for any of the virtual networks operating within the data center and controlled by the corresponding VNC node e.g. VNC node A .

In general distributed databases define the configuration or describe the operation of virtual networks by the data center controlled by distributed VNC . For instance distributes databases may include databases that describe a configuration of one or more virtual networks the hardware software configurations and capabilities of data center servers performance or diagnostic information for one or more virtual networks and or the underlying physical network the topology of the underlying physical network including server chassis switch TOR switch interfaces and interconnecting links and so on. Distributed databases may each be implemented using e.g. a distributed hash table DHT to provide a lookup service for key value pairs of the distributed database stored by different VNC nodes . VNC nodes may request the servers return a physical path through a virtual network for a network flow. VNC nodes may then store the physical path to one of distributed databases In some instances any of VNC nodes may determine a physical path through virtual network for a network flow using techniques described herein as being performed by a server .

As illustrated in the example of distributed virtual network controller VNC includes one or more virtual network controller VNC nodes A N collectively VNC nodes . Each of VNC nodes may represent any of VNC nodes of virtual network controller of . VNC nodes that peer with one another according to a peering protocol operating over network . Network may represent an example instance of switch fabric and or IP fabric of . In the illustrated example VNC nodes peer with one another using a Border Gateway Protocol BGP implementation an example of a peering protocol. VNC nodes provide to one another using the peering protocol information related to respective elements of the virtual network managed at least in part by the VNC nodes . For example VNC node A may manage a first set of one or more servers operating as virtual network switches for the virtual network. VNC node A may send information relating to the management or operation of the first set of servers to VNC node N by BGP A. Other elements managed by VNC nodes may include network controllers and or appliances network infrastructure devices e.g. L2 or L3 switches communication links firewalls and VNC nodes for example. Because VNC nodes have a peer relationship rather than a master slave relationship information may be sufficiently easily shared between the VNC nodes . In addition hardware and or software of VNC nodes may be sufficiently easily replaced providing satisfactory resource fungibility.

Each of VNC nodes may include substantially similar components for performing substantially similar functionality said functionality being described hereinafter primarily with respect to VNC node A. VNC node A may include an analytics database A for storing diagnostic information related to a first set of elements managed by VNC node A. VNC node A may share at least some diagnostic information related to one or more of the first set of elements managed by VNC node A and stored in analytics database as well as to receive at least some diagnostic information related to any of the elements managed by others of VNC nodes . Analytics database A may represent a distributed hash table DHT for instance or any suitable data structure for storing diagnostic information for network elements in a distributed manner in cooperation with others of VNC nodes . Analytics databases A N collectively analytics databases may represent at least in part one of distributed databases of distributed virtual network controller of .

VNC node A may include a configuration database A for storing configuration information related to a first set of elements managed by VNC node A. Control plane components of VNC node A may store configuration information to configuration database A using interface A which may represent an Interface for Metadata Access Points IF MAP protocol implementation. VNC node A may share at least some configuration information related to one or more of the first set of elements managed by VNC node A and stored in configuration database A as well as to receive at least some configuration information related to any of the elements managed by others of VNC nodes . Configuration database A may represent a distributed hash table DHT for instance or any suitable data structure for storing configuration information for network elements in a distributed manner in cooperation with others of VNC nodes . Configuration databases A N collectively configuration databases may represent at least in part one of distributed databases of distributed virtual network controller of .

Virtual network controller may perform any one or more of the illustrated virtual network controller operations represented by modules which may include orchestration user interface VNC global load balancing and one or more applications . VNC executes orchestration module to facilitate the operation of one or more virtual networks in response to a dynamic demand environment by e.g. spawning removing virtual machines in data center servers adjusting computing capabilities allocating network storage resources and modifying a virtual topology connecting virtual switches of a virtual network. VNC global load balancing executed by VNC supports load balancing of analytics configuration communication tasks e.g. among VNC nodes . Applications may represent one or more network applications executed by VNC nodes to e.g. change topology of physical and or virtual networks add services or affect packet forwarding.

User interface includes an interface usable to an administrator or software agent to control the operation of VNC nodes . For instance user interface may include methods by which an administrator may modify e.g. configuration database A of VNC node A. Administration of the one or more virtual networks operated by VNC may proceed by uniform user interface that provides a single point of administration which may reduce an administration cost of the one or more virtual networks.

VNC node A may include a control plane virtual machine VM A that executes control plane protocols to facilitate the distributed VNC techniques described herein. Control plane VM A may in some instances represent a native process. In the illustrated example control VM A executes BGP A to provide information related to the first set of elements managed by VNC node A to e.g. control plane virtual machine N of VNC node N. Control plane VM A may use an open standards based protocol e.g. BGP based L3VPN to distribute information about its virtual network s with other control plane instances and or other third party networking equipment s . Given the peering based model according to one or more aspects described herein different control plane instances e.g. different instances of control plane VMs A N may execute different software versions. In one or more aspects e.g. control plane VM A may include a type of software of a particular version and the control plane VM N may include a different version of the same type of software. The peering configuration of the control node devices may enable use of different software versions for the control plane VMs A N. The execution of multiple control plane VMs by respective VNC nodes may prevent the emergence of a single point of failure.

Control plane VM A communicates with virtual network switches e.g. illustrated VM switch executed by server using a communication protocol operating over network . Virtual network switches facilitate overlay networks in the one or more virtual networks. In the illustrated example control plane VM A uses Extensible Messaging and Presence Protocol XMPP A to communicate with at least virtual network switch by XMPP interface A. Virtual network route data statistics collection logs and configuration information may in accordance with XMPP A be sent as XML documents for communication between control plane VM A and the virtual network switches. Control plane VM A may in turn route data to other XMPP servers such as an analytics collector or may retrieve configuration information on behalf of one or more virtual network switches. Control plane VM A may further execute a communication interface A for communicating with configuration virtual machine VM A associated with configuration database A. Communication interface A may represent an IF MAP interface.

VNC node A may further include configuration VM A to store configuration information for the first set of element to and manage configuration database A. Configuration VM A although described as a virtual machine may in some aspects represent a native process executing on an operating system of VNC node A. Configuration VM A and control plane VM A may communicate using IF MAP by communication interface A and using XMPP by communication interface A. In some aspects configuration VM A may include a horizontally scalable multi tenant IF MAP server and a distributed hash table DHT based IF MAP database that represents configuration database A. In some aspects configuration VM A may include a configuration translator which may translate a user friendly higher level virtual network configuration to a standards based protocol configuration e.g. a BGP L3VPN configuration which may be stored using configuration database A. Communication interface may include an IF MAP interface for communicating with other network elements. The use of the IF MAP may make the storage and management of virtual network configurations very flexible and extensible given that the IF MAP schema can be dynamically updated. Advantageously aspects of virtual network controller may be flexible for new applications .

VNC node A may further include an analytics virtual machine VM A to store diagnostic information and or visibility information related to at least the first set of elements managed by VNC node A. Control plane VM and analytics VM may communicate using an XMPP implementation by communication interface A. Analytics VM A although described as a virtual machine may in some aspects represent a native process executing on an operating system of VNC node A.

Analytics VM A may include analytics database A which may represent an instance of a distributed database that stores visibility data for virtual networks such as one of distributed database of distributed virtual network controller of . Visibility information may describe visibility of both distributed VNC itself and of customer networks. The distributed database may include an XMPP interface on a first side and a REST JASON XMPP interface on a second side.

Virtual network switch may implement the layer 3 forwarding and policy enforcement point for one or more end points and or one or more hosts. The one or more end points or one and or one or more hosts may be classified into a virtual network due to configuration from control plane VM A. Control plane VM A may also distribute virtual to physical mapping for each end point to all other end points as routes. These routes may give the next hop mapping virtual IP to physical IP and encapsulation technique used e.g. one of IPinIP NVGRE VXLAN etc. . Virtual network switch may be agnostic to actual tunneling encapsulation used. Virtual network switch may also trap interesting layer 2 L2 packets broadcast packets and or implement proxy for the packets e.g. using one of Address Resolution Protocol ARP Dynamic Host Configuration Protocol DHCP Domain Name Service DNS etc.

In some cases different VNC nodes may be provided by different suppliers. However the peering configuration of VNC nodes may enable use of different hardware and or software provided by different suppliers for implementing the VNC nodes of distributed VNC . A system operating according to the techniques described above may provide logical view of network topology to end host irrespective of physical network topology access type and or location. Distributed VNC provides programmatic ways for network operators and or applications to change topology to affect packet forwarding and or to add services as well as horizontal scaling of network services e.g. firewall without changing the end host view of the network.

Control plane VMs may request the servers return a physical path through a virtual network for a network flow. Upon control plane VMs receiving a physical path corresponding analytics VMs may store the physical path to corresponding analytics databases . In some instances any of VNC nodes may determine a physical path through virtual network for a network flow using techniques described herein as being performed by a server .

Initially flow trace module FTM of server A receives a request to determine or trace a physical network path traversed by packets of a packet flow . The request may include e.g. a complete IP packet that conforms to the packet flow an IP header that matches IP headers of packets of the packet flow or one or more fields for such an IP header a source IP address and destination IP address for example . The packet flow may represent a packet flow that traverses an overlay virtual network. In some examples VN agent A receives the request from VNC and sends the request to FTM . FTM may use a destination IP address for the packet flow to query a network forwarding table corresponding to the virtual network of the packet flow such as NFT to obtain a tunnel header and in some cases other tunneling information such as an outbound interface for packets of the packet flow . To initialize the flow trace FTM sets a variable TTL VAR to 0 .

Flow trace module generates an outer flow trace packet for the virtual network by generating an inner packet that includes trace packet information such as a trace packet identifier and sequence number and appending the obtained tunnel header to the inner packet and adding a flow trace packet indicator to a field of the outer packet . For subsequently generated flow trace packets for this request FTM may increment the sequence number. The flow trace packet indicator is described in further detail with respect to . The inner packet of the flow trace packet is a data payload of the flow trace packet.

Flow trace module increments TTL VAR and sets the TTL field of the flow trace packet to TTL VAR . For the initial flow trace packet this means the TTL field has a value of 1. Switch A forwards the flow trace packet module by the outbound interface according to the network forwarding table corresponding to the virtual network of the packet flow .

If switch A does not receive a confirmation message NO branch of and instead receives a time exceeded message e.g. an ICMP Time Exceeded message responsive to the latest flow trace packet for the request FTM appends the source address of the time exceeded message to a list of physical network element network addresses . The source address is an address of the physical network element that sent the time exceeded message. Flow trace module then performs again at least steps to generate and forward another flow trace packet with an incremented value for the TTL field. In this way flow trace module generates one or more flow trace packets having incrementally increasing respective time to live values.

If switch A receives a confirmation message YES branch of responsive to a flow trace packet such as an ICMP Echo Reply this indicates that a tunnel endpoint for the virtual network has received the flow trace packet and that the physical network path has been fully traced. FPM module therefore replies to the request by sending the list of physical network element network addresses to the requesting device. In some examples VM agent A sends the list to VNC .

Switch A of server A is a tunnel endpoint for a virtual network such as the virtual network associated with network forwarding table NFT and receives a packet by a tunnel of the virtual network . The packet is an outer packet that includes a tunnel header and may represent one of the packets described with respect . If the tunnel header does not include a flow trace packet indicator NO branch of then switch A decapsulates the inner packet of the tunnel header and forwards the inner packet to one of VMs that has a network address that is a destination network address of the inner packet . If however the tunnel includes a flow trace packet indicator YES branch of the packet is a flow trace packet and switch A sends a confirmation message to another of servers having a network address that is a source network address of the tunnel header to confirm receipt at server A . The confirmation message may transport at least a portion of the inner packet including the IP header of the inner packet in some cases. The confirmation message may be e.g. an ICMP Echo Reply message. Because the flow trace packet received by server A does not carry application data for any of VMs switch A discards the flow trace packet.

A MPLS in GRE tunnel endpoint generates MPLS in GRE packet to include tunnel header which includes an outer IP header composed of source IP address SRC IP destination IP address DST IP Time to Live field having a value incrementally set according to techniques described above with respect to e.g. and IP protocol field PROTO that defines the protocol used in the data portion of the IP datagram here GRE a GRE tunnel header composed of GRE protocol field GRE PROTO that identifies the protocol used in the data portion of the GRE datagram here MPLS and an MPLS label stack that includes MPLS label MPLS . MPLS label is an MPLS label value used to designate the individual MPLS in GRE overlay network on which communication VMs are situated. MPLS in GRE packet also includes an inner packet which may include flow trace information about MPLS in GRE packet such as an identifier and or sequence number.

In some cases the tunnel endpoint may allocate a packet flow to any one of a plurality of equal cost multipaths to reach a packet flow destination. The tunnel endpoint may apply a hashing function to one or more of the header fields of a packet for a packet flow and the output of the hashing function determines the path in the equal cost multipath that is selected for the packet flow. In the example MPLS in GRE packet the tunnel endpoint applies a hashing function to SRC IP DST IP IP protocol field and GRE protocol field . However the tunnel endpoint does not apply the hashing function to MPLS label . As a result MPLS in GRE packet may be hashed to the path of the equal cost multipath that is the same path as that selected for the packet flow whose physical network path is being traced. In other words for a packet flow being traced the tunnel endpoint generates MPLS in GRE packet to include hashed fields identical to packets of the packet. Other fields may vary. Because ICMP is identified differently in other instances of IP protocol field for e.g. ICMP Echo Requests and because IP protocol field is a hash field ICMP packets will necessarily be hashed by a tunnel endpoint differently that MPLS in GRE packet and may result in ICMP Echo Request being transported on a different path in an equal cost multipath proceeding from the tunnel endpoint.

Accordingly to identify MPLS in GRE packet as a flow trace packet MPLS in GRE packet includes a flow trace packet indicator in the form of a flow trace packet indicator value for MPLS label . The flow trace packet indicator value like MPLS label may be a 20 bit value that is a specially designated value to identify a packet as a flow trace packet rather than to designate an individual MPLS in GRE based overlay network. Because a tunnel endpoint does not hash MPLS label in a hashing function MPLS in GRE packet may follow the same physical network path as a packet of the packet flow being traced.

A VxLAN Tunnel End Point is a tunnel endpoint that generates VxLAN packet to include tunnel header which includes an outer IP header composed of source IP address SRC IP destination IP address DST IP Time to Live field having a value for multiple instances of VxLAN packet incrementally increasing according to techniques described above with respect to e.g. and IP protocol field that defines the protocol used in the data portion of the IP datagram here UDP an outer UDP header composed of source UDP port SRC PORT and destination UDP port DST PORT and a VxLAN header that includes VxLAN network identifier VNI VNI alternatively referred to as a VxLAN segment identifier . VNI is a 24 bit value used to designate the individual VxLAN overlay network on which communication VMs are situated. VxLAN packet also includes an inner packet which may include flow trace information about VxLAN packet such as an identifier and or sequence number.

In some cases the tunnel endpoint may allocate a packet flow to any one of a plurality of equal cost multipaths to reach a packet flow destination. The tunnel endpoint may apply a hashing function to one or more of the header fields of a packet for a packet flow and the output of the hashing function determines the path in the equal cost multipath that is selected for the packet flow. In the example VxLAN packet the tunnel endpoint applies a hashing function to SRC IP DST IP IP protocol field SRC PORT and DST PORT . However the tunnel endpoint does not apply the hashing function to VNI . As a result VxLAN packet may be hashed to the path of the equal cost multipath that is the same path as that selected for the packet flow whose physical network path is being traced. In other words for a packet flow being traced the tunnel endpoint generates VxLAN packet to include hashed fields identical to packets of the packet. Other fields may vary. Because ICMP is identified differently in other instances of IP protocol field for e.g. ICMP Echo Requests and because IP protocol field is a hash field ICMP packets will necessarily be hashed by a tunnel endpoint differently that VxLAN packet and may result in ICMP Echo Request being transported on a different path in an equal cost multipath proceeding from the tunnel endpoint.

Accordingly to identify VxLAN packet as a flow trace packet VxLAN packet includes a flow trace packet indicator in the form of a flow trace packet indicator value for VNI . The flow trace packet indicator value like VNI may be a 24 bit value that is a specially designated value to identify a packet as a flow trace packet rather than to designate an individual VxLAN overlay network. Because a tunnel endpoint does not hash VNI in a hashing function VxLAN packet may follow the same physical network path as a packet of the packet flow being traced. Although described above with respect to MPLS in GRE and VxLAN based network virtualization the techniques of this disclosure may applicable to other network virtualization encapsulation types including MPLS in IP Network Virtualization using Generic Routing Encapsulation NVGRE and others.

As shown in the specific example of computing device includes one or more processors one or more communication units one or more input devices one or more output devices and one or more storage devices . Computing device further includes operating system virtualization module and one or more applications A N collectively applications . Virtualization module may represent hypervisor of server A for instance and applications may represent different VMs . Each of components and may be interconnected physically communicatively and or operatively for inter component communications. As one example in components and may be coupled by one or more communication channels . In some examples communication channels may include a system bus network connection interprocess communication data structure or any other channel for communicating data. Virtualization module and applications as well as operating system may also communicate information with one another as well as with other components in computing device . Virtualization module includes software switch to switch packets on one or more virtual networks. Virtualization module also includes flow trace module to determine physical network paths of network flows switched by computing device by generating flow trace packets and incrementally setting respective TTL values to cause downstream switching device to return time exceeded messages to computing device. Flow trace module may represent an example instance of FTM of .

Processors in one example are configured to implement functionality and or process instructions for execution within computing device . For example processors may be capable of processing instructions stored in storage devices . Examples of processors may include any one or more of a microprocessor a controller a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or equivalent discrete or integrated logic circuitry.

One or more storage devices may be configured to store information within computing device during operation. Storage devices in some examples are described as a computer readable storage medium. In some examples storage devices are a temporary memory meaning that a primary purpose of storage devices is not long term storage. Storage devices in some examples are described as a volatile memory meaning that storage devices do not maintain stored contents when the computer is turned off. Examples of volatile memories include random access memories RAM dynamic random access memories DRAM static random access memories SRAM and other forms of volatile memories known in the art. In some examples storage devices are used to store program instructions for execution by processors . Storage devices in one example are used by software or applications running on computing device e.g. operating system virtualization module and the like to temporarily store information during program execution.

Storage devices in some examples also include one or more computer readable storage media. Storage devices may be configured to store larger amounts of information than volatile memory. Storage devices may further be configured for long term storage of information. In some examples storage devices include non volatile storage elements. Examples of such non volatile storage elements include magnetic hard discs tape cartridges or cassettes optical discs floppy discs flash memories or forms of electrically programmable memories EPROM or electrically erasable and programmable memories EEPROM .

Computing device in some examples also includes one or more communication units . Computing device in one example utilizes communication units to communicate with external devices. Communication units may communicate in some examples by sending data packets over one or more networks such as one or more wireless networks via inbound and outbound links. Communication units may include one or more network interface cards IFCs such as an Ethernet card an optical transceiver a radio frequency transceiver or any other type of device that can send and receive information. Other examples of such network interfaces may include Bluetooth 3G and Wi Fi radio components. In some examples computing device utilizes communication units to exchange tunneled packets with other computing devices in a virtualized network domain of a data center.

Computing device in one example also includes one or more input devices . Input devices in some examples are configured to receive input from a user through tactile audio or video feedback. Examples of input devices include a presence sensitive display a mouse a keyboard a voice responsive system video camera microphone or any other type of device for detecting a command from a user. In some examples a presence sensitive display includes a touch sensitive screen.

One or more output devices may also be included in computing device . Output devices in some examples are configured to provide output to a user using tactile audio or video stimuli. Output devices in one example include a presence sensitive display a sound card a video graphics adapter card or any other type of device for converting a signal into an appropriate form understandable to humans or machines. Additional examples of output devices include a speaker a cathode ray tube CRT monitor a liquid crystal display LCD or any other type of device that can generate intelligible output to a user.

Computing device may include operating system . Operating system in some examples controls the operation of components of computing device . For example operating system in one example facilitates the communication of modules applications with processors communication units input devices output devices and storage devices . Applications may each include program instructions and or data that are executable by computing device . As one example application A may include instructions that cause computing device to perform one or more of the operations and actions described in the present disclosure.

The techniques described herein may be implemented in hardware software firmware or any combination thereof. Various features described as modules units or components may be implemented together in an integrated logic device or separately as discrete but interoperable logic devices or other hardware devices. In some cases various features of electronic circuitry may be implemented as one or more integrated circuit devices such as an integrated circuit chip or chipset.

If implemented in hardware this disclosure may be directed to an apparatus such a processor or an integrated circuit device such as an integrated circuit chip or chipset. Alternatively or additionally if implemented in software or firmware the techniques may be realized at least in part by a computer readable data storage medium comprising instructions that when executed cause a processor to perform one or more of the methods described above. For example the computer readable data storage medium may store such instructions for execution by a processor.

A computer readable medium may form part of a computer program product which may include packaging materials. A computer readable medium may comprise a computer data storage medium such as random access memory RAM read only memory ROM non volatile random access memory NVRAM electrically erasable programmable read only memory EEPROM Flash memory magnetic or optical data storage media and the like. In some examples an article of manufacture may comprise one or more computer readable storage media.

In some examples the computer readable storage media may comprise non transitory media. The term non transitory may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM or cache .

The code or instructions may be software and or firmware executed by processing circuitry including one or more processors such as one or more digital signal processors DSPs general purpose microprocessors application specific integrated circuits ASICs field programmable gate arrays FPGAs or other equivalent integrated or discrete logic circuitry. Accordingly the term processor as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition in some aspects functionality described in this disclosure may be provided within software modules or hardware modules.

Various embodiments have been described. These and other embodiments are within the scope of the following examples.

