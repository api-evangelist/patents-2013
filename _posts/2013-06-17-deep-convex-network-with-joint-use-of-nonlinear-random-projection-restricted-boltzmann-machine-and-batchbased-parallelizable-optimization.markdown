---

title: Deep convex network with joint use of nonlinear random projection, restricted boltzmann machine and batch-based parallelizable optimization
abstract: A method is disclosed herein that includes an act of causing a processor to access a deep-structured, layered or hierarchical model, called a deep convex network, retained in a computer-readable medium, wherein the deep-structured model comprises a plurality of layers with weights assigned thereto. This layered model can produce the output serving as the scores to combine with transition probabilities between states in a hidden Markov model and language model scores to form a full speech recognizer. Batch-based, convex optimization is performed to learn a portion of the deep convex network's weights, rendering it appropriate for parallel computation to accomplish the training. The method can further include the act of jointly substantially optimizing the weights, the transition probabilities, and the language model scores of the deep-structured model using the optimization criterion based on a sequence rather than a set of unrelated frames.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09390371&OS=09390371&RS=09390371
owner: Microsoft Technology Licensing, LLC
number: 09390371
owner_city: Redmond
owner_country: US
publication_date: 20130617
---
This application is a continuation of U.S. patent application Ser. No. 13 077 978 filed on Mar. 31 2011 and entitled DEEP CONVEX NETWORK WITH JOINT USE OF NONLINEAR RANDOM PROJECTION RESTRICTED BOLTZMANN MACHINE AND BATCH BASED PARALLELIZABLE OPTIMIZATION the entirety of which is incorporated herein by reference.

Speech recognition has been the subject of a significant amount of research and commercial development. For example speech recognition systems have been incorporated into mobile telephones desktop computers automobiles and the like in order to provide a particular response to speech input provided by a user. For instance in a mobile telephone equipped with speech recognition technology a user can speak a name of a contact listed in the mobile telephone and the mobile telephone can initiate a call to the contact. Furthermore many companies are currently using speech recognition technology to aid customers in connection with identifying employees of a company identifying problems with a product or service etc.

Research in automatic speech recognition ASR has explored layered architectures to perform speech recognition motivated partly by the desire to capitalize on some analogous properties in the human speech generation and perception systems. In these studies learning of model parameters has been one of the most prominent and difficult problems. In parallel with the development in ASR research recent progresses made in learning methods from neural network research has ignited interest in exploration of deep structured models. One particular advance is the development of effective learning techniques for deep belief networks DBNs which are densely connected directed belief networks with many hidden layers. In general DBNs can be considered as a highly complex nonlinear feature extractor with a plurality of layers of hidden units and at least one layer of visible units where each layer of hidden units learns to represent features that capture higher order correlations in original input data.

While DBNs have been shown to be powerful in connection with performing recognition classification tasks training DBNs has proven to be somewhat difficult. In particular conventional techniques for training DBNs involve the utilization of a stochastic gradient descent learning algorithm. While this learning algorithm has been shown to be powerful in connection with fine tuning weights assigned to a DBN such learning algorithm is extremely difficult to parallelize across machines causing learning to be somewhat tedious.

The following is a brief summary of subject matter that is described in greater detail herein. This summary is not intended to be limiting as to the scope of the claims.

Described herein are various technologies pertaining to automatic classification. With more specificity described herein are various technologies pertaining to automatic speech recognition ASR and automatic character recognition. With still more specificity described herein are various technologies pertaining to training a deep convex network through utilization of convex optimization.

A deep convex network includes a plurality of layered modules wherein each module includes a specialized neural network that includes a single hidden layer. More particularly a lowest module in the deep convex network comprises a first linear layer that includes a plurality of linear input units a non linear layer that comprises a plurality of non linear hidden units and a second linear layer that includes a plurality of linear output units produced by feeding raw training data into the module. For instance if the deep convex network is utilized in connection with analyzing an image the plurality of input units can correspond to a number of pixels or the extracted features in the image and can be assigned values based at least in part upon intensity values RGB values or the like corresponding to the respective pixels. In another example if the deep convex network is utilized in connection with ASR the plurality of input units may correspond to samples of an audio file wherein values assigned to the input units are based upon characteristics of the respective samples or correspond to the extracted features from speech waveforms such as power spectra or cepstral coefficients.

The hidden layer of the lowest module comprises a plurality of non linear units that are mapped to the input units by way of a first weight matrix. For instance the weight matrix may comprise a plurality of randomly generated values between 0 and 1. The non linear units may be sigmoidal units that are configured to perform non linear operations on weighted outputs from the input units weighted in accordance with the first weight matrix .

The second linear layer includes the plurality of output units that are representative of targets of the classification task. For instance if the deep convex network is configured to perform digit recognition in either a form of an image or a form of speech e.g. the digits 1 10 then the plurality of output units may be representative of the values 1 2 3 and so forth up to 10. Similarly if the deep convex network is configured to perform phone recognition or more generally large vocabulary speech recognition then the plurality of output units may be representative of mono phones context dependent phones or phone states. The plurality of non linear units may be mapped to the plurality of output units by way of a second weight matrix. This second weight matrix can be learned by way of a batch based learning process such that learning can be undertaken in parallel. In particular convex optimization can be employed in connection with learning the second weight matrix. As an example the second weight matrix can be learned based at least in part upon the first weight matrix the target values of the classification and values of the input units.

As indicated above the deep convex network includes a plurality of layered modules wherein each module includes the aforementioned three layers a first linear layer that includes a plurality of linear input units a hidden layer that comprises a plurality of non linear units and a second linear layer that comprises a plurality of linear output units . The modules are referred to herein as being layered as output units of a lower module are a subset of the input units of an adjacent higher module in the deep convex network. More specifically in a second module that is directly above the lowest module in the deep convex network the input units can include the output units of the lowest module. The input units can additionally include the input units that correspond to the raw training data in other words the output units of the lowest module can be appended to the input units in the second module such that the input units of the second module also include the output units of the lowest module.

The input units in the second module corresponding to the raw training data can be mapped to the plurality of hidden units by the first weight matrix as described above. The input units in the second module that are the output units of the lowest module can be mapped to the plurality of hidden units by a third weight matrix wherein such weights can be learned in a pre training phase. Thereafter the aforementioned second weight matrix that describes weights of connections between the hidden units and the linear output units of the second module can be again learned by way of convex optimization. This pattern of including output units in a lower module as a portion of the input units in an adjacently higher module in the deep convex network and thereafter learning a weight matrix that describes connection weights between hidden units and linear output units via convex optimization can continue for many modules e.g. tens to hundreds of modules . A resultant learned deep convex network may then be deployed in connection with an automatic classification identification task.

Other aspects will be appreciated upon reading and understanding the attached figures and description.

Various technologies pertaining to deep convex networks DCNs will now be described with reference to the drawings where like reference numerals represent like elements throughout. In addition several functional block diagrams of example systems are illustrated and described herein for purposes of explanation however it is to be understood that functionality that is described as being carried out by certain system components may be performed by multiple components. Similarly for instance a component may be configured to perform functionality that is described as being carried out by multiple components and some steps in methodologies described herein may be omitted re ordered or combined.

With reference to an exemplary DCN is illustrated wherein the DCN subsequent to being subjected to training can be utilized in connection with performing automatic classification recognition. Pursuant to an example the DCN can be employed to perform automatic speech recognition ASR . In another example the DCN can be employed to perform character recognition handwriting recognition . In still yet another example the DCN can be employed to perform facial recognition. In another example the DCN can be employed to perform classification of text into one or more topics. Other applications for utilization of the DCN will be readily understood by one skilled in the art of automatic classification recognition.

The DCN comprises a plurality of layered modules wherein a number of layered modules in the DCN can vary depending upon application available computing resources e.g. processing and memory resources in a computing apparatus that is utilized to train the DCN and or utilize the DCN for automatic classification recognition. For instance the number of modules may be in the tens or hundreds or thousands.

Each of the modules can be a neural network with a single hidden layer wherein a weight matrix in the module as will be described in greater detail below can be learned by way of convex optimization. This facilitates training the DCN in a batch based manner such that training of the DCN learning weight matrices for the modules can be parallelized across multiple computing devices.

Additionally each of the modules can include a set of linear layers that surround the single hidden layer. The linear layers can include a plurality of input units and a plurality of output units respectively. The hidden layer comprises a plurality of non linear units. The input units are mapped to the hidden units with weights defined in one or more weight matrices and the hidden units are mapped to the output units with weights defined by the desirably learned weight matrix. Learning of the weight matrix that defines weights of connections between the hidden units and output units of a module in the DCN will be described in greater detail below.

With more detail the first module the lowest module in the DCN comprises a bottom linear layer that includes a plurality of linear input units a hidden layer that comprises a plurality of non linear units and a top linear layer that comprises a plurality of linear output units. The plurality of linear input units in the first linear layer can correspond to parameters of raw data. For instance if the DCN is configured to analyze a digital image to recognize characters therein the raw data may include approximately 800 pixels. Each of the linear input units in the linear layer may correspond to a single pixel in the image such that the linear layer comprises 800 linear input units. Values of such input units may be intensity values corresponding to the pixels gradients corresponding to the pixels or the like. In another example if the DCN is configured to analyze an audio signal to recognize one or more phones the raw data may be the audio signal that is sampled into a plurality of separate samples. Accordingly the number of linear input units in the bottom linear layer may correspond to the number of samples and values of such input may correspond to certain features of the samples.

The linear input units of the bottom linear layer can be fully connected to the non linear units in the hidden layer of the first module where a weight matrix Wrepresents weights assigned to connections between the linear input units and the non linear units. With respect to the first module such weight matrix Wcan be generated through utilization of a random number generator wherein values of Ware randomly distributed between 0 and 1 e.g. with uniform distribution or Gaussian distribution. Other mechanisms for assigning weights between the input units corresponding to raw training data and non linear units are contemplated and are intended to fall under the scope of the hereto appended claims.

The hidden layer as mentioned comprises the plurality of non linear units that are configured to perform a non linear mathematical computation on the weighted values corresponding to the input units. Pursuant to an example the non linear units can be sigmoidal units which can be of the form x 1 1 exp x where x is the weighted summation of input units.

The plurality of output units in the top linear layer can be representative of targets for learning. For instance if the DCN is configured to perform recognition of digits e.g. 1 10 then the output units in the top linear layer can be representative of the digits 1 2 3 and so forth up to 10 e.g. there are ten output units . In another example if the DCN is configured to perform recognition of phones then the output units in the top linear layer can be representative of possible phones.

The plurality of non linear units in the hidden layer can be fully connected to the plurality of output units in the top linear layer wherein weights of the connections are defined by another weight matrix U wherein U is desirably learned. For the first module U can be learned based at least in part W. More particularly convex optimization can be utilized in connection with learning U. For instance a pseudo inverse operation can be employed to learn U wherein U pinv H T where pinv is the pseudo inverse operation T represents all the targets in the training set for learning the supervised output values H WX where X represents all the input values in the training set and the sigmoid function described above is applied element wise. Values assigned to the output units in the top linear layer can be based at least in part upon the weight matrix U.

As mentioned above the DCN comprises numerous layered modules in other words the output units of the first module are included in a bottom linear layer of the second module as input units. The second module also comprises a hidden layer and a top linear layer which include substantially similar identical units as the first module . Input units in the bottom linear layer of the second module also include the same input units that correspond to the raw training data that were included in the bottom linear layer of the first module . Accordingly the output units in the top linear layer of the first module are appended to the input units corresponding to the raw data to form the bottom linear layer of the second module and both sets of units can be referred to collectively as input units of the second module .

The second module further comprises a single hidden layer that includes a plurality of non linear units that are fully connected to the input units of the bottom layer of the second module . Weights of connections between input units in the bottom linear layer and non linear units in the hidden layer can be defined be a set of weight matrices Wand W where RBM denotes Restricted Boltzmann Machine. Wcan serve as the weights of connections between the input units that correspond to the raw data and the plurality of non linear units in the hidden layer and Ware the weights associated with an RBM and can serve as the weights of connections between the input units received from the first module the output units of the first module and the plurality of non linear units in the hidden layer . Learning of Wis described below.

Again it is desirable to learn the weight matrix U that defines weights of connections between the plurality of non linear units in the hidden layer of the second module and the plurality of output units in the top linear layer . In the case of a non lowest module in the DCN any module other than the first module U can be computed based at least in part upon Wand W. Pursuant to an example U can be computed as follows U pinv H t where H WX where T and X have been described above here X is all input units in the bottom linear layer of the second module W is the concatenation of Wand W and WX 1 1 exp WX applied element wise. Values for output units in the top linear layer in the second module can then be computed based at least in part upon the learned U for the second module. These output units may then be included as input units in a bottom linear layer of yet another module. Thus numerous modules can be layered in this fashion and U can be learned for each layered module.

Referring briefly to a system that facilitates learning U for the first module is illustrated. As described above the first module comprises the bottom linear layer the hidden layer and the top linear layer . The bottom linear layer comprises input units which correspond to raw training data as described above. The hidden layer comprises a plurality of non linear units which may be sigmoidal units. The input units are fully connected to the non linear units . Weights assigned to connections between the input units and the non linear units are defined by W. Pursuant to an example a random number generator can be configured to generate W wherein values of Wcan be randomly distributed over a pre defined range such as zero to one. The non linear units perform non linear computations on values of the input units weighted by the weights in W. Wmay be replaced here partially or fully by W.

The top linear layer of the first module comprises a plurality of output units that are fully connected to the non linear units in the hidden layer . As discussed above it is desirable to learn a weight matrix U for connections between the non linear units and the output units . A learning component can receive W from the random number generator or from a data store can receive output computations from the non linear units the values of the input units the raw training data identities of the targets for learning t identities of the output units and can compute U based at least in part thereon through utilization of a convex optimization algorithm. An exemplary convex optimization algorithm has been described above. Values may then be assigned to the output units based at least in part upon the weight matrix U . Additionally while the learning component has been shown as receiving certain data directly from the first module it is to be understood that W x and t can be known a priori and the learning component can retrieve such data when needed immediately subsequent to h being computed by the non linear units .

Referring now to an exemplary system that facilitates learning U for the second module is illustrated. While the second module is illustrated in the system it is to be understood that the system can be utilized to learn U for any module in the DCN that is not the lowest module e.g. the first module . The system comprises the second module which includes the bottom linear layer the hidden layer and the top linear layer . The bottom linear layer comprises two sets of input nodes the input units that were also the input units to the first module and a second set of input units that are the output units from the first module .

The hidden layer of the second module comprises a plurality of non linear units which may be substantially similar e.g. identical to the non linear units in the hidden layer of the first module . The first set of input units in the bottom linear layer is fully connected to the non linear units in the hidden layer wherein weights of connections are defined by W. Additionally the second set of input units in the bottom linear layer is also fully connected to the non linear units in the hidden layer wherein weights of connections are defined by W which can be pre computed in a pretraining phase. Computation of W for the second set of input units will now be described.

A pair of layers in a module of the DCN can be treated as a Restricted Boltzmann Machine RBM . With more detail an RBM is a particular type of Markov random field MRF that has one layer of typically Bernoulli stochastic hidden units and one layer of typically Bernoulli or Gaussian stochastic visible units. RBMs can be represented as bipartite graphs since all visible units are connected to all hidden units but there are no visible visible or hidden hidden connections.

In the RBMs the joint distribution p v q over the visible units v and hidden units q given the model parameters can be defined in terms of an energy function E v q of the following algorithm 

For a Bernoulli visible Bernoulli hidden RBM the energy is as follows 3 where wrepresents the symmetric interaction term between visible unit and hidden unit q band arepresent the bias terms and V and Q are the numbers of visible and hidden units. The conditional probabilities can be calculated as follows 1 4 1 5 where x 1 1 exp x .

Similarly for a Gaussian Bernoulli RBM the energy is as follows after assuming that the variance is unity 

Following the gradient of the log likelihood log p the update rule for the weights can be obtained as follows 9 where qis the expectation observed in the training data and qis that same expectation under a defined distribution for the DCN . Unfortunately qcan be extremely expensive to compute exactly so the contrastive divergence CD approximation to the gradient may be used where qis replaced by running a Gibbs sampler initialized at the data for one full step.

From a decoding point of view the DCN can be treated as a multi layer perceptron with many layers. The input signal from the training data can be processed layer by layer through utilization of equation 4 until the final layer. The final layer can be transformed into a multinomial distribution using the following softmax operation 

Pursuant to an example frame level data can be used train a stack of RBMs in a generative manner resulting in output of W. W may then be employed as a weight matrix for each full connection of input units that are obtained from a lower module in the DCN .

Accordingly the non linear units can receive values from the second set of input units that are weighted by W. Based at least in part upon the first set of input units and the second set of input units as weighted by W and W respectively the non linear units in the hidden layer can compute H. As has been described above the non linear units are fully connected to a plurality of output units in the top linear layer of the second module wherein such output units are representative of targets for learning. It is desirable to learn the weight matrix U that defines weights of connections between the plurality of non linear units and the plurality of output units .

The learning component is configured to compute U for the second module based at least in part upon W W X the values of the first set of input units and the second set of input units T and H. Pursuant to an example the system may comprise a data store that includes W W and T as these values can be pre computed or known. A portion of X the first set of input units can also be retained in the data store as such values are static while the remainder of x can be received from the immediately lower module in the DCN . Based at least in part upon these values the learning component can compute U by way of convex optimization as described above.

Now referring to an exemplary system that facilitates learning U for various modules in the DCN through utilization of parallel computing is illustrated. The system comprises a plurality of computing devices . Each of the computing devices can have an instance of the DCN loaded thereon. A first computing device can include a first data store that comprises a first training batch . The first training batch can include a significant amount of training data. A data receiver component can receive data from the first training batch and provides the training to a first instance of the DCN . The learning component can learn U for modules in the DCN layer by layer until U for all modules have been obtained.

The Nth computing device comprises an Nth data store that includes an Nth training batch . The data receiver component receives data from the Nth training batch in the data store and provides such training data to the instance of the DCN on the Nth computing device . The learning component can learn U for all modules in the Nth instance of the DCN . Accordingly batch mode processing can be undertaken in parallel across numerous computing devices since the learning component utilizes a convex optimization function to learn U. Final values for U may be set later in time as a function of values of U learned by the learning component for the instances of the DCN across the computing devices .

With reference now to various exemplary methodologies are illustrated and described. While the methodologies are described as being a series of acts that are performed in a sequence it is to be understood that the methodologies are not limited by the order of the sequence. For instance some acts may occur in a different order than what is described herein. In addition an act may occur concurrently with another act. Furthermore in some instances not all acts may be required to implement a methodology described herein.

Moreover the acts described herein may be computer executable instructions that can be implemented by one or more processors and or stored on a computer readable medium or media. The computer executable instructions may include a routine a sub routine programs a thread of execution and or the like. Still further results of acts of the methodologies may be stored in a computer readable medium displayed on a display device and or the like. The computer readable medium may be any suitable computer readable storage device such as memory hard drive CD DVD flash drive or the like. As used herein the term computer readable medium is not intended to encompass a propagated signal.

With reference now to an exemplary methodology that facilitates training a DCN in a batch based manner is illustrated. The methodology starts at and at training data for training a deep convex network is received. As described above the deep convex network comprises a plurality of interconnected modules and each module includes at least one linear layer and at least one non linear hidden layer. At the deep convex network is trained in a batch based manner based at least in part upon the training data wherein training the deep convex network comprises learning a weight matrix corresponding to output of the non linear layer of at least one module in the plurality of interconnected modules. The methodology completes at .

Now turning to an exemplary methodology that facilitates learning a plurality of weight matrices by way of convex optimization is illustrated. The methodology starts at and at speech training data is received. At a deep convex network is trained in a batch manner through utilization of the training data wherein the deep convex network comprises a plurality of layered modules that each include a one hidden layer neural network. A hidden layer in a first module includes a plurality of non linear units that are interconnected to a plurality of linear input units in a linear layer of a second module. Training the deep convex network includes learning a plurality of weight matrices corresponding to the plurality of layered modules such that a weight matrix is learned for each layered module and wherein the weight matrix is learned by way of convex optimization. The methodology completes at .

Now referring to a high level illustration of an exemplary computing device that can be used in accordance with the systems and methodologies disclosed herein is illustrated. For instance the computing device may be used in a system that supports ASR. In another example at least a portion of the computing device may be used in a system that supports learning weight matrices in a DCN by way of convex optimization. The computing device includes at least one processor that executes instructions that are stored in a memory . The memory may be or include RAM ROM EEPROM Flash memory or other suitable memory. The instructions may be for instance instructions for implementing functionality described as being carried out by one or more components discussed above or instructions for implementing one or more of the methods described above. The processor may access the memory by way of a system bus . In addition to storing executable instructions the memory may also store a training data set a validation data set a DCN etc.

The computing device additionally includes a data store that is accessible by the processor by way of the system bus . The data store may be or include any suitable computer readable storage including a hard disk memory etc. The data store may include executable instructions a DCN a training data set a validation data set weight matrices etc. The computing device also includes an input interface that allows external devices to communicate with the computing device . For instance the input interface may be used to receive instructions from an external computer device from a user etc. The computing device also includes an output interface that interfaces the computing device with one or more external devices. For example the computing device may display text images etc. by way of the output interface .

Additionally while illustrated as a single system it is to be understood that the computing device may be a distributed system. Thus for instance several devices may be in communication by way of a network connection and may collectively perform tasks described as being performed by the computing device .

As used herein the terms component and system are intended to encompass hardware software or a combination of hardware and software. Thus for example a system or component may be a process a process executing on a processor or a processor. Additionally a component or system may be localized on a single device or distributed across several devices. Furthermore a component or system may refer to a portion of memory and or a series of transistors.

It is noted that several examples have been provided for purposes of explanation. These examples are not to be construed as limiting the hereto appended claims. Additionally it may be recognized that the examples provided herein may be permutated while still falling under the scope of the claims.

