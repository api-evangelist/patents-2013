---

title: Automatic removal of inappropriate content
abstract: The disclosure generally describes computer-implemented methods, software, and systems for automatically removing inappropriate content. One example method includes: identifying a report of inappropriate content received from a user, the report identifying a content item the user has identified as inappropriate and an identification of the user, determining whether to automatically remove the content item based at least in part on the identity of the user, and removing the content item upon determining that the content should be removed. In some instances, the user is associated with a report weight. The report weight can be based, at least in part, on a business role of the user. Determining whether to automatically remove the content item may include determining that the user or a business role of the user is associated with an automatic removal rule, and removing the content item upon determining that the report is associated the user.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09558287&OS=09558287&RS=09558287
owner: SAP Portals Israel Ltd.
number: 09558287
owner_city: Ra'anana
owner_country: IL
publication_date: 20130924
---
Inappropriate content including but not limited to abusive content unlawful statements and information and other content that violates the terms of service of websites and message boards consistently find their way onto the Internet. In most cases sites provide hyperlinks and defined processes for notifying site administrators of the content and manually initiating the removal process.

The disclosure generally describes computer implemented methods software and systems for automatically removing inappropriate content. One example method includes identifying a report of inappropriate content received from a user the report identifying a content item the user has identified as inappropriate and an identification of the user determining whether to automatically remove the content item based at least in part on the identity of the user and removing the content item upon determining that the content should be removed. In some instances the user is associated with a report weight. The report weight can be based at least in part on a business role of the user. Determining whether to automatically remove the content item may include determining that the user or a business role of the user is associated with an automatic removal rule and removing the content item upon determining that the report is associated the user.

While generally described as computer implemented software embodied on tangible media that processes and transforms the respective data some or all of the aspects may be computer implemented methods or further included in respective systems or other devices for performing this described functionality. The details of these and other aspects and embodiments of the present disclosure are set forth in the accompanying drawings and the description below. Other features objects and advantages of the disclosure will be apparent from the description and drawings and from the claims.

The present disclosure provides tools and systems for automatically removing inappropriate content from websites message boards frequently asked question FAQ lists wiki pages blog posts user comments and other suitable online locations. As usage of the Internet increases inappropriate and sometimes abusive content has become more and more common in online communities. The present disclosure describes new mechanisms and processes for identifying and reporting inappropriate content by users and based on an evaluation of the inappropriateness of the content and the identity of the reporting user can automatically remove the identifying and reported inappropriate content. Inappropriate content can include any number of abusive insensitive or illegal content as well as content violating a particular location s terms of service or usage. The present disclosure allows for fast removal of such inappropriate content and can remove the need for a time consuming manual review process prior to removal. In this way the tools of the present disclosure can ensure that the inappropriate content is timely removed and not available for additional views while a manual review process is initiated and occurs.

The tools and systems perform the automatic removal by analyzing one or more reports of inappropriate content as well as the particular users who submitted those reports and automatically remove inappropriate content based on an analysis related to one or more trust rules. Various factors and weighting may be applied to the received reports. One or more evaluation metrics may be considered in determining whether or not to remove the content including but not limited to 

Automatic removal algorithms may differ between implementations and may weigh different evaluation aspects of a particular report and reporting user differently. In some algorithms a role of a reporting user may be heavily weighted in a determination of whether to remove a particular content item. In some instances the fact that a user associated with a particular role or set of roles e.g. CEO CFO COO site moderator blog owner etc. has reported a content item as inappropriate may be alone enough to initiate an automatic removal process. Additionally certain reporting users may be identified as final arbiters of inappropriateness if such a user submits an inappropriateness report the identified content item may be removed automatically. In some instances certain roles may be associated with a weighing factor such that reports received from those users are considered more heavily or less heavily than a generic report according to their weighting factor. For example a vice president or brand manager of a company may be weighted much higher than a regular employee or outside user. In some instances the weight may be so significant that a single request from the CEO may be sufficient to automatically initiate the automatic removal process. In other instances the weighted score of the report may greatly affect the ultimate decision of whether to remove the identified content when considered with one or more additional reports of the same content.

In some instances these weighting factors may be fluid. For example a user may be provided an initial weighting of 1.0 that is the user is not weighted more heavily or less heavily than a hypothetical generic user. If the user reports content as inappropriate and upon evaluation such content is removed as inappropriate additional weight may be provided to that user s reports and the weight may be increased by an appropriate amount. Similarly if a user reports content as inappropriate and it is later determined not to be the user s weighting may be decreased by an appropriate amount. This may be continued over time with some user s weighting fluctuating accordingly. Additionally users may receive an initial increased or decreased weighting based on their role with a business or entity as described above. Those weightings can be similarly adjusted based on the history of the user s reports as described above.

In some instances the determination to remove reported content may be based on a plurality of reports where each reported content item is associated with a report score. The report score can be based on a number of time a particular content item has been reported as inappropriate where each reporting incident may be associated with a particular report weight based on one or more of the user reporting the content the role of the user reporting the content the category of the content and other suitable factors. Adding those individual report scores may result in a cumulative report score which can then be compared to a corresponding removal threshold. In some instances the removal threshold may vary based on the corresponding content category. For example if the reported content is based on foul or derogatory language a first removal threshold may be used. Alternative category types including sensitive company information defamatory language abusive language and others may be associated with different removal thresholds. If the report score meets or exceeds the removal threshold then the content item may be automatically removed. In some instances should the report score not meet the removal threshold a system admin or other person associated with the online location associated with the report can manually review the content to determine if removal is proper.

In general the content management system is a server or set of servers that provide content management services in association with one or more applications websites and other online locations including the one or more content servers . The content management system can respond to one or more inappropriate content reports received from various users including user associated with client . In particular the content management system can perform operations associated with an automatic content removal process in response to requests received from users in environment and can further respond accordingly. In some implementations the content management system may store and execute one or more additional processes not shown in addition to the operations related to the automatic content removal process via the content report manager as well as other types of processes and or applications. In other implementations the content management system may be a dedicated system or server meant to store and execute only operations associated with the automatic content removal processes. In some implementations the content management system may comprise a Web server where the content report manager represents one or more Web based services and or applications accessed and executed by clients and their applications via the network or directly at the content management system to perform the programmed tasks or operations of the content report manager .

At a high level the content management system comprises an electronic computing device operable to receive transmit process store or manage data and information associated with the environment . Specifically the content management system illustrated in is responsible for receiving user reports identifying potentially inappropriate content and subsequently determining whether or not the potentially inappropriate content should be removed. In addition to requests from the external clients requests associated with the automatic content removal process may also be sent from internal or local users external or third party customers automated applications or processes employees as well as any other appropriate entities individuals systems or computers. Users submitting reports may use a button hyperlink or other suitable component within a UI associated with the online location of the potentially inappropriate content to submit said reports. Further users may be able to annotate portions of the online location e.g. a particular post comment section page etc. which they believe to be inappropriate such that the report is specific to the particular portion of the online location.

As used in the present disclosure the term computer is intended to encompass any suitable processing device. For example although illustrates a single content management system environment can be implemented using two or more such systems a plurality of servers as well as computers other than servers including a server pool. Indeed content management system may be any computer or processing device such as for example a blade server general purpose personal computer PC Mac workstation UNIX based workstation or any other suitable device. In other words the present disclosure contemplates computers other than general purpose computers as well as computers without conventional operating systems. Further illustrated content management system may be adapted to execute any operating system including Linux UNIX Windows Mac OS Java Android iOS or any other suitable operating system. According to one implementation the content management system may also include or be communicably coupled with an e mail server a Web server a caching server and or other suitable server or computer.

In the present illustration the content management system hosts and or supports the content report manager and its associated engines and processes which include a report processing engine a content score engine a user weight engine a content category engine and an automatic removal engine . As described the content report manager receives one or more reports from users via clients regarding content items included in content servers which those reporting users believe to be inappropriate. Reports can be generated or prepared after activating a hyperlink button or other UI element associated with the identified content item and the report can be sent to the content report manager for analysis and evaluation. In some instances the hyperlink or button may generate reports or messages regarding the identified content item and the user initiating the report where those reports or messages are provided to the content report manager via one or more suitable application programming interfaces APIs not shown . The report processing engine may be the initial recipient of the report and can parse the report to determine the identified content and the reporting user.

Using this information the user weight engine can determine a current weight associated with the reporting user. As discussed above users of different positions in or associated with a business or entity may have different weights in effect making their reports more or less valued by the system in determining whether the associated content should be automatically removed. The user s associated weight can fluctuate over time based on changes in positions or roles. Additionally the user s associated weight can change based on the relative correctness of prior reports requesting removal of potentially inappropriate content. As illustrated database can store information associated with each user which can be used to derive a particular report weight associated with that user. In some instances a user s business role may be used to further determine a report weight to be associated with the particular user s reports. As described above the user s business role as well as alternative types of defined roles e.g. unpaid site moderator valued contributor etc. can determine an initial weighting of the user s report defined as the report weight for a particular user. Those initial weights may remain as a constant in some instances while in others the weights may fluctuate as additional feedback on one or more other reports are received. Where a user submits a report on an item that is ultimately removed that user may be considered more trustworthy and therefore provided with a higher report weight . Similarly should a user submit a report on an item that is not ultimately removed that user may be considered less trustworthy and therefore provided with a lower report weight . These values may fluctuate as content is identified and evaluated. The user weight engine can perform the operations that modify a particular user s report weight .

The content category engine determines the type or category of content associated with the received report. In some instances the report may include an identification of the category of content either provided manually by the reporting user or generated automatically based on an automated review of the identified content or based on a pre defined content type associated with the portion of the website or other online location associated with the report. Different types and categories of content may be associated with different levels of removal thresholds where the removal thresholds represent thresholds of report scores needed to automatically remove potentially inappropriate content. For example different types of content may include abusive language i.e. foul language bigoted language etc. sensitive company information or other suitable types. Sensitive company information such as financial information may be associated with a relatively low removal threshold such that almost any user i.e. depending on their respective report weights identifying such information may result in the information being removed. Abusive language depending on the type of online location may be associated with a higher removal threshold which may require two or more users again based on their respective report weights to report such content before invoking the removal process.

The content score engine can perform the evaluations related to calculating a report score for reporting instance as well as for each identified content in the case of multiple reports. Further the content score engine can compare the report score of the individual report or plurality of reports related to a common content item to the corresponding removal threshold of the content item and or its associated category or type. In some instances in order to determine an individual report score the content score engine can determine a particular report weight associated with the reporting user and use that report weight to calculate the individual report score. In instances where multiple reports are received for a single content item the content score engine can determine the individual report scores and combine them to identify a cumulative report score. Should the cumulative report score exceed the corresponding removal threshold the associated content item can be automatically removed. This algorithm represents an example algorithm. Any other suitable algorithms can also be used to determine when to trigger the automatic removal process.

The automatic removal engine can perform the operations necessary to remove or trigger the removal of a particular content item. In some instances the automatic removal engine can access the source code or databases associated with the corresponding content server to remove the identified content item while in other instances the automatic removal engine can request or remotely trigger removal of the content in those servers or systems via a remote function call request or other instruction. In some instances the automatic removal engine can notify some or all of the reporting users of the removal of the identified content items. Additionally the automatic removal engine may notify the user weight engine when a content item is removed allowing the user weight engine to update the report weights associated with particular reporting users. Additionally the automatic removal engine may notify the user weight engine when a content item is not removed allowing the user weight engine to negatively adjust the associated reporting users report weights .

As illustrated the content management system includes an interface a processor and a memory . The interface is used by the content management system for communicating with other systems in a distributed environment including within the environment connected to the network for example one of the clients or content servers as well as other systems communicably coupled to the network including those not illustrated . Generally the interface comprises logic encoded in software and or hardware in a suitable combination and operable to communicate with the network . More specifically the interface may comprise software supporting one or more communication protocols associated with communications such that the network or the interface s hardware is operable to communicate physical signals within and outside of the illustrated environment .

As illustrated in the content management system includes a processor . Although illustrated as a single processor in two or more processors may be used according to particular needs desires or particular implementations of the environment . Each processor may be a central processing unit CPU a blade an application specific integrated circuit ASIC a field programmable gate array FPGA or another suitable component. Generally the processor executes instructions and manipulates data to perform the operations of the content management system . Specifically the processor executes the functionality required to receive and respond to requests from the various devices including the execution of the content report manager .

Regardless of the particular implementation software may include computer readable instructions firmware wired and or programmed hardware or any combination thereof on a tangible medium transitory or non transitory as appropriate operable when executed to perform at least the processes and operations described herein. Indeed each software component may be fully or partially written or described in any appropriate computer language including C C Java Visual Basic assembler Perl any suitable version of 4 GL as well as others. While portions of the software illustrated in are shown as individual modules that implement the various features and functionality through various objects methods or other processes the software may instead include a number of sub modules third party services components libraries and such as appropriate. Conversely the features and functionality of various components can be combined into single components as appropriate.

The content management system includes a memory or multiple memories . The memory may include any type of memory or database module and may take the form of volatile and or non volatile memory including without limitation magnetic media optical media random access memory RAM read only memory ROM removable media or any other suitable local or remote memory component. The memory may store various objects or data including caches classes frameworks applications backup data business objects jobs web pages web page templates database tables repositories storing business and or dynamic information and any other appropriate information including any parameters variables algorithms instructions rules constraints or references thereto associated with the purposes of the content management system such as database . Additionally the memory may include any other appropriate data such as VPN applications firmware logs and policies firewall policies a security or access log print or other reporting files as well as others.

As described database includes information on one or more users one or more roles including business roles associated with at least some of the users and report weights associated with the users . Database may further include information defining one or more reported content items and any calculated report scores associated with those reported content items . In some instances one or more of the reported content items may be associated with a particular content category or type . Additionally various levels of removal thresholds may be stored in database .

Network facilitates wireless or wireline communications between the components of the environment i.e. between the content management system and the one or more clients devices or content servers as well as with any other local or remote computer such as additional clients servers or other devices communicably coupled to network including those not illustrated in . In the illustrated environment the network is depicted as a single network but may be comprised of more than one network without departing from the scope of this disclosure so long as at least a portion of the network may facilitate communications between senders and recipients. In some instances one or more of the components associated with the content management system may be included within network as one or more cloud based services or operations. For example at least a portion of the content management system may be within the network and operated at least partially within or as a cloud based system including in some instances multiple remote processors performing the operations described herein.

The network may be all or a portion of an enterprise or secured network while in another instance at least a portion of the network may represent a connection to the Internet. In some instances a portion of the network may be a virtual private network VPN . Further all or a portion of the network can comprise either a wireline or wireless link. Example wireless links may include 802.11a b g n 802.20 WiMax LTE and or any other appropriate wireless link. In other words the network encompasses any internal or external network networks sub network or combination thereof operable to facilitate communications between various computing components inside and outside the illustrated environment . The network may communicate for example Internet Protocol IP packets Frame Relay frames Asynchronous Transfer Mode ATM cells voice video data and other suitable information between network addresses. The network may also include one or more local area networks LANs radio access networks RANs metropolitan area networks MANs wide area networks WANs all or a portion of the Internet and or any other communication system or systems at one or more locations.

The illustrated environment of also includes the client and one or more content servers . Each of these devices may be any computing device operable to connect to or communicate with at least the content management system via the network using a wireline or wireless connection. In general the content servers and the client comprise electronic computer devices operable to receive transmit process and store any appropriate data associated with the environment of . These devices systems can connect to the content management system either directly or with the help of a client based wrapper. Connections between the clients servers and the content management system can be established for instance using HTTP or RFC protocols depending on client technologies and implementation preferences.

The content servers may be associated with one or more online locations in which the automatic removal of potentially inappropriate content is performed. In some instances the content servers may be Web servers supporting one or more types of online locations including web pages wikis blogs company websites review websites message boards and others. These content servers may include a plurality of content items and may provide or make available elements capable of initiating a report of potentially inappropriate content. In some instances at least a portion of the content servers may be included in the same system of the content management system such that the reported content items are available on the content management system .

Each client may be any device suitable to view and execute web pages and operations associated with the content servers . Further the client may include a graphical user interface GUI not shown . The GUI interfaces with at least a portion of the environment for any suitable purpose including generating a visual representation of content items associated with one or more of the content servers . In some instances the GUI may be associated with a Web browser or other client application. The GUI may be used to view and navigate various Web pages located both internally and externally to the content servers . The GUI may comprise a graphical user interface operable to for example allow the user of the client to interface with at least a portion of the content server and in some cases the content management system as well as other applications. Generally the GUI provides the particular associated user with an efficient and user friendly presentation of data provided by or communicated within the system. The GUI may comprise a plurality of customizable frames or views having interactive fields pull down lists and buttons operated by the user. Generally the GUI may also provide general interactive elements that allow a user to access and utilize various services and functions of one or more applications such as links or buttons associated with submitting particular content items as potentially inappropriate. The GUI may present information associated with the respective applications for viewing and interaction. In general the GUI is often configurable supports a combination of tables and graphs bar line pie status dials etc. and is able to build real time portals where tabs are delineated by key characteristics e.g. site or micro site . Therefore the GUI contemplates any suitable graphical user interface such as a combination of a generic web browser intelligence engine and command line interface CLI that processes information in the platform and efficiently presents the results to the user visually.

There may be any number of client or mobile devices associated with or external to the environment . For example while the illustrated environment includes one client alternative implementations of the environment may include multiple client devices communicably coupled to the network or any other number suitable to the purposes of the environment . Additionally there may also be one or more additional clients external to the illustrated portion of the environment that are capable of interacting with the environment via the network . Further the term user and administrator may be used interchangeably as appropriate without departing from the scope of this disclosure. Moreover while the client is described in terms of being used by a single user this disclosure contemplates that many users may use one computer or that one user may use multiple computers. Based on the respective user s identity role and history the content report manager can determine how to evaluate received reports about specific content items received from or associated with a particular user .

While is described as containing or being associated with a plurality of elements not all elements illustrated within environment of may be utilized in each alternative implementation of the present disclosure. For example although depicts a particular environment any suitable alternative environments and implementations are considered. Additionally one or more of the elements described herein may be located external to environment while in other instances certain elements may be included within or as a portion of one or more of the other described elements as well as other elements not described in the illustrated implementation. Further certain elements illustrated in may be combined with other components as well as used for alternative or additional purposes in addition to those purposes described herein.

At client sends a report of inappropriate content to the content report manager . The report is initiated by a user associated with the client and can be submitted via one or more suitable UI elements within a particular web page portal message board or other appropriate context. In some instances the tool may provide significant advantages in a portal environment. A portal can provide a site or location where information from a plurality of diverse sources is put together in a uniform way. Each information source e.g. associated with one or more particular content servers may be provided its own dedicated area for displaying information. In many situations the role of the user in an organization may determine which content can be added to deleted from or modified within the portal. In some aspects business information and data for an organization can be presented and or available to users of various roles within the portal. The present solution can allow users to submit issues or inappropriate content via the portal or other type of page or context with an associated report i.e. the report initiated by the user being submitted. The report can identify and or include the particular content item identified by the user which may include a portion of the online location i.e. a table post message etc. in which the potentially inappropriate material is located as well as the entire webpage in which the potentially inappropriate material exists. The report can further include an identity of the user associated with the report. By doing so information associated with the user can be determined and applied to the evaluation of the potentially inappropriate content.

At the content report manager performs a lookup operation at database to identify the appropriate user report weight associated with the user submitting or associated with the report. At the content report manager performs a lookup operation at database to identify a current content item report score for the content item. In some instances the current content item report score may comprise an aggregated score of each individual report for a particular content item. Records for each individual report may be maintained as well.

At the content report manager adds the user report weight associated with user to the aggregated content item report score and subsequently sends the updated content item report score to the database for saving at . At the content report manager can perform a lookup operation at database to determine the removal threshold e.g. removal threshold . The removal threshold may be based on a default removal threshold value or may be determined based on a category or type of identified content item.

Upon identifying the removal threshold the content report manager determines whether the aggregated content report score exceeds the removal threshold. If so then the content report manager sends a request to remove the content item to the content server where the content item can be removed. In some instances the content report manager may have sufficient authorization and editing capabilities to effect the removal of the content item itself while in other instances a component or process at the content server may perform the removal in response to the request . Should the content report manager determine that the content report score does not exceed the removal threshold no immediate action may be taken. In some instances potentially inappropriate content identified in one or more reports may be sent to a manual review queue after a pre determined period of time. This can ensure that such reports can be given consideration even where the number of reports is small.

At a report of inappropriate content submitted by a user is identified. The report can identify a content item the user identified as inappropriate. Additionally the report can include the identity of the user submitting the report. In some instances an identifier associated with the user may be included instead of the user s specific name where the identifier allows identification of the user at a backend system.

At a determination of whether to automatically remove the identified content item is performed based at least in part on the identity of the user associated with the report. In some instances some users may be weighted such that any of their reporting of inappropriate content immediately results in automatic removal of the identified content item. In other instances multiple factors including prior reports related to the same subject matter may also be considered. The identified report may be weighted based on a user s identity role within the organization and a trust value associated with the user based on prior reports. Using the user specific weighting a weighted report score associated with the identified report can be generated and used at least in part to determine whether to remove the content item.

If the determination results in automatic removal the content item can be removed at . If the determination does not result in automatic removal at the report may be moved into a queue for manual review to ensure that the complaint is considered.

At a report weight associated with a received report is identified where the report weight is based on the reporting user. The report weight may be defined in a table or database storing such information where the particular report weight is associated with the user. In some instances the report weight may be generally associated with a class of users such as users in a particular role. As previously described the report weight for a particular user can be based on one or more of the following the user the user s role in or association with the organization and the user s reporting history as well as others. The user s role may be defined as a particular position within an organization e.g. CEO CFO systems administrator blog author etc. an assigned position e.g. webmaster message board moderator etc. a role associated with a particular department within the organization as well as others.

In some instances a base weight may be initially provided to all reporting users. That weighting can be modified when the user is defined as having a particular role such that any reports received from or associated with that user are weighted accordingly. Additionally weighting may strengthen or weaken a particular user s report weight. For example a salesperson associated with the organization may have a report weight lower than the base weight while an online marketing employee may have a report weight higher than the base weight. Additionally a feedback mechanism may strengthen or weaken a particular user s weighting over time. Using the online marketing employee as an example such an employee may be provided with an initial report weight of 1.5. However for each incorrect reporting of inappropriate content that user s report weight may be decreased by 0.2. For each correct reporting the user s report weight may be increased by 0.1. The changes may increase or decrease in a geometric or exponential series or any other modification. In some instances users associated with a certain role or position may have an upper or lower limit on their report weighting so that changes based on the history of the user s reporting can only affect the report weight a pre defined amount. In some instances the particular user associated with the report may be associated with a heavy weight or an immediate determination of removal such that any report from that particular user may immediately trigger an automatic removal. In any event the report score of the identified report can be equal to the identified report weight.

At a current report score associated with the content item is identified. The current report score can represent the aggregation of report scores from prior reports associated with the content item. The current report score can be stored in a suitable database or other location and can be readily accessible when a new report for a particular content item is received.

At the report score based on the identified report weight from the current report is added to the current report score associated with the content to produce an updated report score. The updated report score represents the aggregation of the prior reports and the newly received report associated with the identified report weight.

At a removal threshold associated with the content item is identified. In some instances a default removal threshold may be identified. For example a default removal threshold may be 10. In other instances additional information associated with or based on the content item or the reporting user may modify the removal threshold. The additional information associated with the content item may include the category of the reason for the inappropriate content report. For example the inappropriate content report may be based on foul or derogatory language abusive statements sensitive or confidential information and others. In some instances the category of the content item may be adding by an annotation in the report while in others a text or context based analysis may identify the appropriate category. In some instances the online location associated with the content item may be used to determine or derive the appropriate removal threshold. For example a website or message board associated with a child related material may have a lower removal threshold while a movie review site for adults may have a higher removal threshold. Business sites may be associated with still another removal threshold.

At a determination is made as to whether the updated report score associated with the content item exceeds the identified removal threshold. If it does method continues at where a process to automatically remove the identified content is initiated. The process may include notifying a content server associated with the content item of the requested removal or may include directly removing the content item from the content server. If not method continues at . At if no additional user reports associated with the content item are received after a pre determined period of time then the report may be added to a manual review queue.

In some instances an automatic removal may be reversible. For example while a particular content item may be automatically removed based on one of the processes described herein an administrator or other reviewer may receive a notice of same or the removed content item may be placed into a removed items queue. The administrator may review removed content and provide a second level determination as to whether the content item is truly inappropriate. If the administrator disagrees with the automatic removal he or she can reverse the removal. Additionally administrators may be able to view received reports. If one or more of the reports are determined to be erroneous then the administrator can manually reduce the aggregated report score or can increase the removal threshold for the content item.

The preceding figures and accompanying description illustrate example processes and computer implementable techniques. But environment or its software or other components contemplates using implementing or executing any suitable technique for performing these and other tasks. It will be understood that these processes are for illustration purposes only and that the described or similar techniques may be performed at any appropriate time including concurrently individually or in combination. In addition many of the steps in these processes may take place simultaneously concurrently and or in different orders than as shown. Moreover environment may use processes with additional steps fewer steps and or different steps so long as the methods remain appropriate.

In other words although this disclosure has been described in terms of certain embodiments and generally associated methods alterations and permutations of these embodiments and methods will be apparent to those skilled in the art. Accordingly the above description of example embodiments does not define or constrain this disclosure. Other changes substitutions and alterations are also possible without departing from the spirit and scope of this disclosure.

