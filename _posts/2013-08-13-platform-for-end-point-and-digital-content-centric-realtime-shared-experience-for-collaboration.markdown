---

title: Platform for end point and digital content centric real-time shared experience for collaboration
abstract: A platform for end point and digital content centric real-time shared experience for collaboration across endpoints is disclosed. In one embodiment, media centric collaboration terminals (MCCTs) are communicatively connected to one or more terminals selected from the group consisting of video communication terminals (VCTs), voice over Internet protocol (IP) communication terminals (VoCTs), remote servers, remote streaming clients and/or appliances, machines and gadgets (AMGs) via a communication network. Further, audio and/or video bridging of one or more audio and/or video streams originating from one of the MCCTs designated as a current host along with incoming audio and/or video streams from at least one of any remaining MCCTs designated as clients or the one or more terminals is enabled on the current host, via the communication network.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09137489&OS=09137489&RS=09137489
owner: ITTIAM SYSTEMS PTE. LTD.
number: 09137489
owner_city: Singapore
owner_country: SG
publication_date: 20130813
---
Benefit is claimed under 35 U.S.C. 120 to Indian Non Provisional Application Ser. No. 1425 CHE 2013 entitled PLATFORM FOR END POINT AND DIGITAL CONTENT CENTRIC REAL TIME SHARED EXPERIENCE FOR COLLABORATION by ITTIAM SYSTEMS PTE. LTD. filed on Mar. 28 2013 to Indian Non Provisional Application Ser. No. 5495 CHE 2012 entitled SYSTEM METHOD AND ARCHITECTURE FOR IN BUILT MEDIA ENABLED PERSONAL COLLABORATION ON ENDPOINTS CAPABLE OF IP VOICE VIDEO COMMUNICATION by ITTIAM SYSTEMS PTE. LTD. filed on Dec. 28 2012 and to U.S. patent application Ser. No. 13 763 739 entitled SYSTEM METHOD AND ARCHITECTURE FOR IN BUILT MEDIA ENABLED PERSONAL COLLABORATION ON ENDPOINTS CAPABLE OF IP VOICE VIDEO COMMUNICATION by ITTIAM SYSTEMS PTE. LTD. filed on Feb. 11 2013.

Embodiments of the present subject matter relate to collaboration across end points. More particularly embodiments of the present invention relate to end point and digital content centric collaboration across the end points that are capable of Internet protocol IP audio and or video communication.

With continued globalization the need for personal and business interaction between people in real time over long distances is steadily increasing and also demanding change in the nature of interaction especially with respect to content. Voice communication remains the primary mode for achieving communication. However increasingly the voice communication is being supplemented by chat sessions including text audio and video. Lately there has also been an increasing usage of video conferencing for an improved quality of interaction using software applications like Skype to high end dedicated conferencing setups like in tele presence. Such software also complements the voice and video communication with information sharing bringing elements of collaboration into the experience.

Despite such advances the interactions between people in real time are primarily limited to increased effectiveness in communication with a degree of sharing content of interest between people located at different places. Such interactions have not been able to achieve a shared experience people get in real time when they are located at the same place. In recent years new products and services such as Hangout from Google allow sharing of audio video and image content with video conferencing to enhance the collaboration experience. However existing products and or services offer centralized server based services where conferencing and collaboration functions are done on servers. This also requires the users to upload the content to the servers. U.S. patent application Ser. No. 13 763 739 describes a technique that does the conferencing and collaboration functions on the end point exploiting their increasing processing capacity and hence gives the user an option of not having to upload the contents to the servers. Thus reducing overall latency and improving user experience. However this technique offers limited flexibility and control both on the host and clients for collaboration given the pre defined nature of content sharing as full or partial screen sharing.

Other features of the present embodiments will be apparent from the accompanying drawings and from the detailed description that follows.

A platform for end point and digital content centric real time shared experience for collaboration is disclosed. In the following detailed description of the embodiments of the invention reference is made to the accompanying drawings that form a part hereof and in which are shown by way of illustration specific embodiments in which the invention may be practiced. These embodiments are described in sufficient detail to enable those skilled in the art to practice the invention and it is to be understood that other embodiments may be utilized and that changes may be made without departing from the scope of the present invention. The following detailed description is therefore not to be taken in a limiting sense and the scope of the present invention is defined by the appended claims.

The term end points refers to video communication terminals VCTs voice over Internet protocol IP communication terminals VoCTs and media centric collaboration terminals MCCTs which can be standalone or part of any appliances machines or gadgets AMGs . Exemplary VCTs include terminals capable of video communication over IP including desktop video phones mobile or cell phones tablets stand alone or in built video conferencing units and the like. The VoCTs include stand alone or in built terminals capable of audio communication over IP. The term bridge refers to conferencing more than two end points capable of communication over IP.

The terms signal and stream are used interchangeably throughout the document. Further the terms end points and participants are used interchangeably throughout the document. Also the terms audio and voice are used interchangeably throughout the document.

In operation MCCTs are communicatively connected to the VCTs the VoCTs the AMGs the remote servers and or the remote streaming clients via a communication network . For example the communication network includes an IP network and the like. Further one of the MCCTs designated as a current host enables audio and or video bridging of one or more audio and or video streams originated from the current host along with incoming audio and or video streams from remaining MCCTs designated as clients the VCTs the VoCTs the one or more AMGs and or the remote servers via the communication network to achieve end point and digital content centric real time shared experience for collaboration amongst two or more participants. In one example any one of the MCCTs standalone or embedded in the AMGs can designate as the current host. In an example embodiment the content coming from the remote servers is a streamed audio video signal or a remote session being achieved using a custom application or a commercially available application like a remote desktop protocol RDP or JoinMe that enables the remote session.

In one embodiment the MCAVBM residing in the current host enables audio and or video bridging on the current host of the one or more audio and or video streams of its stored or captured digital content and or application s along with the incoming audio and or video streams from the remaining MCCTs designated as the clients the VCTs the VoCTs the AMGs and or the remote server via the communication network to achieve the end point and digital content centric real time shared experience for collaboration amongst two or more participants. This is explained in more detail with reference to . Further the MCAVBM residing in the MCCTs designated as the clients enables the multi stream encoding decoding and composition of the audio and or video streams for display as per user defined and or predefined configuration settings which can be defined locally and or in a centralized place.

Referring now to which is a block diagram that illustrates major functional sub components of the WMM and MCAVBM residing in each of the MCCTs such as those shown in according to one embodiment. As shown in the MCCT includes the WMM MCAVBM the API for application developers and the API for ISPs enterprise customers or any centralized controller. For example the MCCT is the one of the MCCTs designated as the current host.

Furthermore the WMM includes a window creation module a window accessing module a window host control module a window client control module and an audio stream accessing module . Further the MCAVBM includes an audio receive module ARM an audio render module AREM an audio decode module ADM an audio processing and mixing module APMM an audio encode module AEM an audio send module ASM to receive render decode process and send the audio streams. Furthermore the MCAVBM includes a video receive module VRM a video render module VREM a video processing and composing module VPCM a video decode module VDM a video encode module VEM and a video send module VSM to receive render process compose decode encode and send the video streams. In addition the MCAVBM includes an audio and or video synchronization module AVSM for synchronizing each of decoded audio and or video streams of each participant connected to the current host before local play out. Moreover the MCAVBM includes an audio acquisition module AAM and a video acquisition module VAM for acquiring audio from one or more audio channels or video from one or more video channels respectively on the current host.

In one embodiment the WMM creates and assigns windows to a plurality of applications running on the current host accesses input output buffer s of the applications and controls the windows. In one example in case of an operating system OS that already offer the creation assignment and access like Windows the WMM function is limited to accessing them. In an OS like Android where the windowing functionality is not natively available till version 4.2 the WMM provides window creation assignment and access to buffers including the input output buffer s .

In one exemplary embodiment the window creation module creates and assigns the window to each of the plurality of applications running on the current host. Further the window accessing module accesses one or more input output buffers associated with each of the plurality of applications to obtain a plurality of video streams associated with the plurality of applications. Furthermore the audio stream accessing module accesses one or more input output buffers associated with each of the plurality of applications to obtain a plurality of audio streams being played out by the plurality of applications. Moreover the window host control module controls the windows associated with the one or more audio and or video streams of the plurality of audio and or video streams of the current host and the incoming audio and or video streams. In addition the window client control module enables the remaining MCCTs designated as the clients to control the windows of the one or more audio and or video streams and the incoming audio and or video streams sent by the current host. In this context the window client control module enables each of the remaining MCCTs designated as the clients to locally control the windows of the audio and or video streams and incoming audio and or video streams sent by the current host. For example controlling the windows include control over the number of windows layout of the windows size of each window and the like. This is explained in more detail with reference to .

In another embodiment the WMM allows the remaining MCCTs designated as the clients to control the windows on the current host. Exemplary control of the windows on the current host includes touch event transfer from the remaining MCCTs designated as the clients or control of window size position and the like. In this case the window host control module captures user events such as touch events before translating transferring it to the current host or remaining MCCTs designated as the clients. In one example embodiment the current host streams its entire display to the remaining MCCTs designated as the clients so that the remaining MCCTs designated as the clients can see a complete screen of the current host control the actions on the screen completely and effectively control the application s running on the current host from the remaining MCCTs designated as the clients.

Further in this embodiment the MCAVBM enables audio and or video bridging on the current host of the one or more audio and or video streams originated from the current host along with incoming audio and or video streams from other end points designated as clients such as the remaining MCCTs designated as the clients one or more VCTs one or more VoCTs one or more AMGs and or one or more remote servers via an communication network e.g. the remaining MCCTs of the one or more MCCTs the VCTs the VoCTs the one or more AMGs or the remote servers and the communication network of to achieve end point and digital content real time shared experience for collaboration amongst two or more participants. For example the one or more audio and or video streams are obtained by converting digital content such as animation video image text graphics audio and the like originated from the current host.

In one example embodiment one or more of the remote servers enable deployment of audio and or video bridging of the current host with the remaining MCCTs designated as the clients the VCTs the VoCTs remote streaming clients e.g. the remote streaming clients of and or the AMGs. In this example embodiment the one or more of the remote servers enable deployment of the audio and or video bridging of the remaining MCCTs designated as the clients the VCTs the VoCTs the remote streaming clients and or the AMGs by registration and maintaining a database of the remaining MCCTs designated as the clients the VCTs the VoCTs the remote streaming clients and or the AMGs. For example the remote server includes a session initiation protocol SIP server and the like. In a typical audio and or video conferencing scenario where there exists the remote server to facilitate the deployment of the end points designated as clients. The remote server s primary functionality is to enable registration and identification of each of the end points designated as the clients. The remote server can in addition have a back end database to maintain the individual user profiles and map them to registered elements in the network. The remote server also enables connectivity of the clients by enabling network address translation NAT traversal by using one of different methods such as simple traversal of user datagram protocol UDP through NAT or more commonly known as STUN media relay traversal using relay NAT TURN etc. in conjunction with appropriate blocks on the end points designated as the clients. In one embodiment the remote server is a SIP registrar performing the above mentioned functionalities. In another embodiment one or more of the remote servers are used to store content that can be shared with multiple end points.

In one exemplary embodiment the ARM enables the current host to receive multiple audio streams in different formats from the remaining MCCTs designated as the clients the VCTs the VoCTs the AMGs and or the remote servers and de jitter each audio stream independently. Further the ADM decodes fully or partially each de jittered audio stream. Furthermore the VRM enables the current host to receive multiple video streams in different formats and resolutions from the remaining MCCTs designated as the clients the VCTs the VoCTs the one or more AMGs and or the remote servers and de jitter each video stream independently if required. In addition the VDM enables decoding fully or partially each de jittered video stream. Moreover the AREM and VREM render the audio streams and video streams respectively on the current host.

Also the AVSM synchronizes each of the decoded audio and or video streams of each participant connected to the current host before local play out. The AVSM further synchronizes the audio and or video streams before encoding and streaming out to each of the remaining MCCTs designated as the clients VCTs remote servers remote streaming clients and or VoCTs. In one example the AVSM works across all the other sub components of the MCAVBM to track and re timestamp the audio and or video streams as required in order to achieve audio and or video synchronization of the transmitted audio and or video streams.

Further in this exemplary embodiment the APMM post processes the audio stream coming from each of the remaining MCCTs designated as the clients VCTs AMGs or VoCTs before playback and or re encoding. Exemplary post processing includes mixing the incoming audio streams based on a weighted averaging for adjusting the loudness of the audio stream coming from each of the remaining MCCTs designated as the clients VCTs AMGs or VoCTs. Furthermore the APMM produces a separate audio stream specific to each of the remaining MCCTs designated as the clients VCTs AMGs or VoCTs by removing an audio stream originating from a selected set of the remaining MCCTs designated as the clients VCTs AMGs or VoCTs and mixing the audio streams coming from one or more other remaining MCCTs designated as the clients VCTs AMGs and or VoCTs. For example the set of the remaining MCCTs designated as the clients VCTs AMGs or VoCTs is selected based on user configuration. In one example the user selects a sub set of the remaining MCCTs designated as the clients the VCTs AMGs or the VoCTs to group them in a conference.

In addition the VPCM processes and composes the RAW video streams received from the VDM OR VAM . For example processing the decoded video streams includes resizing the video streams. In one example the VPCM composes the RAW video streams selectively to each of the remaining MCCTs designated as the clients VCTs VoCTs and AMGs based on a user defined criteria. This is explained in more detail with reference to . For example composing the RAW video streams includes selectively tiling the video streams. In one embodiment the VPCM composes the stream selectively for each end point with the user defined criteria such as a subset of end points as a group .

The VPCM further composes one or more RAW streams from the VAM and or RAW video streams from the VDM selectively to each of the remaining MCCTs designated as the clients VCTs VoCTs and AMGs by combining one or more of the decoded video streams along with region information. For example the region information includes a number of windows and or resolution. In one embodiment on the current host each application s output buffer or a displayable element can be encoded as a separate stream with its own codec format resolution frame rate bit rate and the like. Further in this embodiment multiple of such output buffers or displayable elements could be combined to form a single stream with its own codec format resolution frame rate and bit rate and like. Furthermore each of such buffers can be encoded along with the region information and sent to the remaining MCCTs designated as the clients. For example the current host is capable of carrying out collaboration on end points with or without asymmetric audio and or video streams based on the processing capability of the current host. The term asymmetric audio and or video streams refers to the audio and or video streams coming from each end point being different from each other in format frame rate resolution bit rate and the like as well as the audio and or video streams going to each of the end points being different from each other in format frame rate resolution bit rate composition and the like as well as audio and or video streams either coming into the end point only receive only without any transmit or audio and or video streams going out of the end point only transmit only without any receive .

In one embodiment the VPCM performs a differential frame rate composition of different windows within a larger tiled window. For example the VPCM combines one video at 15 fps repeating every alternate frame while the other video is at 30 fps. Effectively the composed signal is at 30 fps but the region that is composed at 15 fps is encoded using lesser number of bits as every alternate frame is identical to the previous frame thus reducing the overall bit rate without needing any explicit selective coding in the encoder OR needing any selective coding of regions as separate streams. Further the remaining MCCTs designated as the clients have the flexibility to display different audio and or video streams coming to them in a position and size as defined by the user OR an external program. Particularly the remaining MCCTs designated as the clients further have flexibility to be able to extract the region from the audio and or video stream which has the region information to display the region in the position and size as configured by the user or an external program. This is explained in more detail with reference to .

Moreover the AEM encodes each of the audio streams coming from the APMM separately in a format required by each of the associated remaining MCCTs designated as the clients VCTs AMGs and VoCTs. Also the ASM receives each of the audio streams from the AEM and sends the encoded audio streams to each of the associated remaining MCCTs designated as the clients VCTs AMGs and VoCTs. In one embodiment the ASM sends a selected set of audio streams to one or more of the remote servers and the remote streaming clients on the communication network.

Furthermore the VEM encodes each of the composed video streams coming from the VPCM in a format and resolution supported by each of the associated remaining MCCTs designated as the clients and VCTs. In one embodiment the VEM may encode only one stream that will be sent to all the remaining MCCTs VCTs VoCTs AMGs remote streaming clients and remote servers. In addition the VSM receives each of the encoded video streams from the VEM and sends them to the associated remaining MCCTs designated as the clients VCTs VoCTs AMGs remote streaming clients remote servers via the communication network. For example the remote streaming clients primarily receive the content from the current host to enable users to view them like personal computers on the network . The VSM further sends encoded video streams to one or more of the remote servers via the communication network for broadcasting real time view by authorized people storing for archival non real time play out by the authorized people re distributing to the end points in the network and the like.

In an exemplary embodiment the processing modules such as AEM VEM ADM VDM and the like can be accelerated outside the current host on which the collaboration functionality is implemented using a co processing unit. For example the acceleration is done by sending the RAW signals over a peripheral interface such as a universal serial bus USB interface which then gets processed and is returned back in a compressed format as required over the same or another peripheral interface. In one example the signal sent over to the co processing is compressed in a format such as joint photographic experts group JPEG and the like that is of low complexity and is implemented on the end point and the co processing unit further de compresses and re compresses in a more complex format such as H.264 or high efficiency video coding HEVC .

Moreover the AAM acquires for acquiring one or more channels of the audio streams from the current host. In one embodiment the AAM acquires uncompressed RAW audio of the local participant from the audio capture as well as the decompressed RAW audio frames coming from the audio stream accessing module which are being played out by an application on the current host. The AAM then delivers these one or more channels of uncompressed audio to the APMM . In one exemplary embodiment the AAM even acquire a compressed audio stream and deliver it directly to the ASM for packetization and streaming out bypassing the APMM and AEM .

Also the VAM acquires the one or more video streams from the current host. In one embodiment the VAM acquires uncompressed RAW video of the local participant from a camera capture as well as one or more decompressed RAW video frames coming from the window accessing module which are being rendered and or generated by one or more applications running on the current host from the local storage of the current host or coming from the remote server. The VAM then delivers the one or more RAW video streams to the VPCM which then uses this as one of the decoded or RAW streams for composition. In another embodiment the VAM may acquire the input buffers to an application in a RAW format. In yet another embodiment the VAM captures just one RAW video stream.

In one exemplary implementation the VAM captures the entire display area or selected sections of the display area or selected output windows of selected applications running on the current host or buffers of the display area or output or input buffers of the application running on the current host in the background. In one exemplary embodiment the VAM even acquire a compressed video stream and deliver it directly to the VSM for packetization and streaming out bypassing the VPCM and VEM . In one example where the VAM acquires different buffers as separate video signals the VPCM can be used to compose the viewing layout to each of the remaining MCCTs designated as the clients VCTs VoCTs and AMGs that are in communication with the current host to be different from the viewing layout on the local display of the current host itself. Whereas in the embodiment where only one RAW stream of uncompressed signal of the video on the current host is captured the viewing layout of the remaining MCCTs designated as the clients VCTs VoCTs and AMGs that are in communication with the current host may be the same as the viewing layout on the local display of the current host itself.

Further the AREM is responsible for rendering the audio on the local end point host or client via various audio play out interfaces such as a speaker a headset a high definition multimedia interface HDMI and the like. In one exemplary implementation the VREM is responsible for rendering the video on the local end point host or client via various video rendering interfaces such as a liquid crystal display LCD a HDMI and the like.

In one embodiment the current host can also be configured to send out the compressed audio and or video streams to the remote streaming clients to perform decode and render only functionality. In another embodiment the current host can be configured to send out the compressed audio and or video streams to the one or more of the remote servers for re distribution recording and the like.

Referring now to which is a block diagram that illustrates a MCCT designated as a client with local control over a number of windows layout of the windows and size of each window of the application s during the end point and digital content centric collaboration across the end points according to one embodiment. As shown in the the block diagram includes a MCCT designated as a current host and the MCCT and an end point designated as clients. For example the end point includes one of the VCTs VoCTs and AMGs embedded with the VCTs.

In one embodiment the MCCT is communicatively connected to the MCCT and the end point via the communication network . Further the MCCT enables audio and or video bridging of an audio and or video stream of its stored or captured digital content and or application s and incoming audio and or video streams from the MCCT and end point to achieve end point and digital content centric real time shared experience for collaboration across the end points. Particularly the MCCT enables streaming of the audio and or video stream and incoming audio and or video streams to the end point by controlling the windows of the audio and or video stream and incoming audio and or video streams during the end point and digital content centric collaboration. The MCCT also enables streaming of the audio and or video stream and incoming audio and or video streams to the MCCT by enabling it to locally control the windows of the audio and or video stream and incoming audio and or video streams during the end point and digital content centric collaboration. For example controlling the windows include control over the number of windows layout of the windows size of each window and the like. In one example the MCCT has the flexibility to display different audio and or video streams coming to it in the position and size as defined by the user OR an external program.

Referring now to which is a block diagram that illustrates audio and or video bridging on a MCCT designated as a current host of multiple audio and or video streams of its stored or captured digital content and or application s and incoming audio and or video streams from a MCCT and an end point designated as clients according to one embodiment. As shown in the the block diagram includes the MCCT the MCCT and the end point . For example the end point includes one of the VCTs VoCTs and AMGs embedded with the VCTs. In one embodiment the MCCT is communicatively connected to the MCCT and the end point via the communication network . Further the MCCT enables audio and or video bridging of the multiple audio and or video streams of its stored or captured digital content and or application s of the MCCT and the incoming audio and or video streams from the MCCT and end point via the communication network .

Referring now to which is a block diagram that illustrates distributed collaboration with each of a MCCT and an end point designated as clients receiving and or displaying different audio and or video streams relevant to the role of its user in the ongoing collaboration in the end point and digital content centric collaboration across the end points according to one embodiment. For example the end point includes one of the VCTs VoCTs and AMGs embedded with the VCTs. As shown in the MCCT designated as the current host selectively sends different audio and or video streams of its stored or captured digital content and or application s relevant to the role of its user in the ongoing collaboration to the MCCT and the end point during the end point and digital content centric collaboration across the end points. In one embodiment the MCCT sends the same audio and or video stream with region information based on which the MCCT and the end point display only the information that is relevant as per their region settings. For example the MCCT sends only one stream consisting of all the content being displayed on the MCCT but mark out only the relevant regions that gets displayed on each of the according to the information carried in the stream or by any out of band means.

Referring now to which is a block diagram that illustrates collaborative interaction through simultaneous operation on same content of the MCCT designated as the current host on MCCT designated as the client during the end point and digital content centric collaboration according to one embodiment. As shown in the MCCT selectively sends the audio and or video streams of its stored or captured digital content and or application s to the MCCT via the communication network and the MCCT and the MCCT performs simultaneous operation on the same content during the end point and digital content centric collaboration.

Referring now to which is a block diagram illustrating handover of control of collaboration from a MCCT designated as a current host to a MCCT designated as a client during the end point and digital content centric collaboration according to one embodiment. As shown in the MCCT streams its entire display to the MCCT so that the MCCT can see the complete screen of the MCCT and also control the actions on the screen completely effectively controlling the application s running on the MCCT from the MCCT .

Referring now to which is a block diagram illustrating temporary audio and or visual content lending by converting the application into the audio visual content and transmitting or mirroring between a MCCT designated as a client and a MCCT designated as a current host during the end point and digital content centric collaboration according to one embodiment. As shown in during the end point and digital content centric collaboration the MCCT requests the MCCT to send the application which is not there in the MCCT . Further the MCCT temporarily lends the audio and or visual content by converting the application into the audio and or visual content and transmitting the audio and or visual content to the MCCT as well as giving complete control over the screen area of that particular application so that the MCCT can use the application as if it were running on the MCCT and perform all the controls necessary on the application. The MCCT further streams the audio visual content along with the audio and or video streams of the MCCT to an end point designated as a client during the end point and digital content centric collaboration.

Referring now to which is a block diagram illustrating streaming of one or more audio and or video streams by a MCCT designated as a current host to a MCCT designated as a client during the end point and digital content centric collaboration according to one embodiment. As shown in the MCCT sends each of output buffers or displayable elements as separate encoded audio and or video streams or as a separate stream and and combined as a separate stream or and as one combined stream to the MCCT along with region information. Further the MCCT have the flexibility to decode audio and or video streams separately and extracts the regions that are part of a single audio and or video stream and render it separately with the flexibility that the user or a program wants.

Referring now to which is a block diagram illustrating a close integration of an example application using APIs provided by a MCCT designated as a current host and a MCCT designated as a client. In this embodiment a gaming application is taken as an example where the application is a multi party gaming application. As shown in the a game is programmed such a way that the game runs on both the MCCT and the MCCT in a split window mode. The local game runs on the left hand half e.g. a window of the MCCT to ensure the fastest response for the user on the MCCT while the video of the game running on the MCCT is transmitted to the MCCT and displayed on the left hand half of the MCCT e.g. a window . The game running on the MCCT runs on the right hand half of the MCCT e.g. a window giving the best response to the user while the video of the game from the MCCT is transmitted to the MCCT and displayed on the right hand half of the MCCT e.g. a window . Effectively both the users players in this case get to see the other party side by side. However the latency of their own game window which requires quick response times from the user is much shorter as it is locally run and displayed while the game window of their partner is a bit delayed as it is the processed captured encoded transmitted received decoded and displayed version. This gives an effect of a multi party game without compromising on the response times needed for games. In another example embodiment even the control of the games can be transferred using the APIs provided by the MCCT host and client.

Referring now to which is a process flow for the end point and digital content centric collaboration across end points that are capable of IP video communication according to one embodiment. At block one or more MCCTs are communicatively connected to one or more terminals selected from the group consisting of VCTs VoCTs remote servers remote streaming clients and or AMGs embedded with the VCTs or MCCTs via a communication network. At block audio and or video bridging of one or more audio and or video streams originating from one of the MCCTs designated as a current host along with any incoming audio and or video streams from at least one of any remaining MCCTs designated as clients or the one or more terminals is enabled on the current host via the communication network. In one embodiment audio and or video bridging of the one or more audio and or video streams originating from the current host along with any incoming audio and or video streams from at least two or more of any remaining MCCTs designated as the clients or the terminals is enabled on the current host via the communication network. In one exemplary embodiment the audio and or video bridging is enabled by the current host with the at least one of any remaining MCCTs designated as the clients or the one or more terminals via one or more of the remote servers maintaining user profiles. In one example the one or more audio and or video streams are obtained by converting digital content originating from the current host. For example the digital content includes animation audio video image graphics text and the like.

In one embodiment each of the MCCTs includes a WMM a MCAVBM an API for application developers and an API for ISPs enterprise customers or any centralized controller. In one exemplary embodiment the WMM creates and assigns windows to a plurality of applications running on the MCCT designated as the current host accesses input output I O buffer s of the applications and controls the windows. In one example the WMM includes a window creation module for creating and assigning a window to each of a plurality of applications running on the current host. The WMM further includes a window accessing module for accessing one or more input output buffers associated with each of the plurality of applications to obtain a plurality of video streams associated with the plurality of applications. The WMM further includes an audio stream accessing module for accessing one or more input output buffers associated with each of the plurality of applications to obtain a plurality of audio streams being played out by the plurality of applications. The WMM further includes a window host control module for controlling the windows associated with the one or more audio and or video streams of the plurality of audio and or video streams of the current host and the incoming audio and or video streams. The WMM further includes a window client control module for enabling each of the MCCTs to control the windows of the audio and or video streams and the incoming audio and or video streams received by the one or more MCCTs. For example the current host streams its entire display to the remaining MCCTs designated as the clients so that the remaining MCCTs designated as the clients can see the complete screen of the current host and one of the remaining MCCTs designated as the clients controls the actions on the screen completely and effectively control the application s running on the current host.

Further in this exemplary embodiment the MCAVBM enables the audio and or video bridging of the one or more audio and or video streams originating from the current host along with the one or more incoming audio and or video streams from the at least one of any remaining MCCTs designated as the clients or the one or more terminals via the communication network. In one example the MCAVBM includes an ARM for enabling the current host to receive multiple audio streams and de jitter each audio stream independently. The MCAVBM further includes an ADM for decoding fully or partially each de jittered audio stream. The MCAVBM further includes an AREM to render the audio streams on the current host.

Further in this example the MCAVBM includes a VRM for enabling the current host to receive multiple video streams and de jitter each video stream independently. The MCAVBM further includes a VDM for decoding fully or partially each de jittered video stream. The MCAVBM further includes VREM to render the video streams on the current host. The MCAVBM further includes an AVSM for synchronizing the decoded audio and corresponding video streams of corresponding terminals connected to the current host before local play out and for synchronizing the audio and or video streams before encoding and streaming out to the remaining MCCTs designated as the clients and the one or more terminals.

Furthermore in this example the MCAVBM includes an APMM for post processing the audio streams coming from the at least one of the remaining MCCTs designated as the clients or the one or more terminals before playback and or re encoding. The APMM further produces a separate audio stream specific to each of the remaining MCCTs designated as the clients and the one or more terminals by removing an audio stream originating from a selected set of the remaining MCCTs designated as the clients and the one or more terminals and mixing the audio streams coming from one or more other remaining MCCTs designated as the clients VCTs and or VoCTs. For example the set of the remaining MCCTs designated as the clients and the one or more terminals is selected based on user configuration.

In addition the MCAVBM includes a VPCM for processing and composing the decoded video streams received from the VDM. For example processing the decoded video streams includes resizing the video streams. The VPCM further composes the decoded video streams selectively to each of the remaining MCCTs designated as the clients and the one or more terminals based on a user defined criteria. For example composing the decoded video streams includes selectively tiling the video streams. The VPCM further composes decoded video streams selectively to each of the remaining MCCTs designated as the clients and the one or more terminals by combining output buffers of one or more of a plurality of applications along with region information and the VPCM further performs a differential frame rate composition of different windows of the plurality of applications within a larger tiled window.

Moreover the MCAVBM includes an AEM for encoding each of the audio streams coming from the APMM separately in a format required by each of the associated remaining MCCTs designated as the clients and the one or more terminals. The MCAVBM further includes an ASM for receiving each of the audio streams from the AEM and sending the encoded audio streams to each of the associated remaining MCCTs designated as the clients and the one or more terminals. The ASM further sends a selected set of audio streams to the remote server via the communication network. The MCAVBM further includes a VEM for encoding each of the composed video streams coming from the VPCM in a format supported by each of the associated remaining MCCTs designated as the clients and the one or more terminals. The MCAVBM further includes a VSM for receiving each of the encoded video streams from the VEM and sending them to the associated remaining MCCTs designated as the clients and the one or more terminals via the communication network and wherein the VSM further sends encoded video streams to the remote servers via the communication network for broadcasting real time view by authorized people storing for archival and or non real time play out by the authorized people. The MCAVBM further includes an AAM for acquiring the one or more audio streams from the current host. The MCAVBM further includes a VAM for acquiring the one or more video streams from the current host. The details of the operation of end point and digital content centric real time shared experience for collaboration are explained in more detail with reference to .

A platform for end point and digital content centric real time shared experience for collaboration across end points having electronics capable of carrying out video communication and capture and or store and or receive and or process audio visual and or graphical content. Collaboration capability of the above technique allows the user of an end point to share the captured stored received processed media content in real time with other users at different locations around the world having IP VCTs or end points and substantially simultaneously converse with each other having a shared real time experience similar to the experience of all the users being at the same location without requiring any external hardware and or service to enable the same. For example the user of the PC capable tablet can watch a movie on his tablet and simultaneously share it with his friends at different locations who can watch it together on their IP video communication capable tablets mobile TV and the like and at the same time converse with each other just like they would do if they were sitting next to each other. The connection of users for collaboration will use a combination of private connection like user phone book OR public searchable connection like Linkedin OR Face book. The choice will be user configurable for complete or subset of users.

For example the user of the PC capable tablet who is operating on a document can simultaneously share it with his colleagues at different locations who can also operate it simultaneously on their IP video communication capable tablets mobile TV and the like and at the same time converse with each other just like they would do if they were sitting next to each other. For example a teacher can conduct lessons to students across geography. In one example a real time online examination can be conducted across geography. Doctors can collaborate on a medical procedure or practice across geography. Amateur or professionals performing to an interactive audience across geography. A geographically distant artist performing together to virtually create a single event across geography and real time reporting with interactive audience across geography.

In another example a user while taking vacation snaps on his PC capable camera can simultaneously share it with their friends or family members at different locations who can watch it together on their IP video communication capable mobiles tablets and or personal computers and the like and see each other and converse on the shared pictures at the same time. In another example a student while studying his electronic version of the text book on a PC capable e book reader can simultaneously share it with his fellow students at different locations who can read it together on their IP video communication capable tablets or personal computers etc. and simultaneously discuss with each other about the subject just like they would if they were next to each other. In one embodiment of deployment where there could be more than one MCCT in the network the above technique also provides a method to not only view but also control the applications and screen of the other participants. For example in one embodiment two or more people having MCCTs with each other can view collaborate and annotate on a picture or a map about a location to get a truly being together experience. In another embodiment the MCCT can provide appropriate application programmable interfaces APIs to enable users to write and or install applications on MCCTs to have a tight integration of a shared experience. For example users can write and or install games made specifically for MCCT to transfer the signals between one or more participants having MCCTs to experience a multi player game as if they are at the same location.

The above technique also enables the selective control of the MCCT via a centralized place on the deployment network. In an example embodiment the features available for the end user OR the features available on the end point can be selectively enabled or disabled via a user key provided by the end point user to the centralized server thus enabling control of enabling disabling selective features on the end point. Also as the content being shared remains on the end point and is not residing on any server there is privacy of content by default. In another embodiment the selective control of features as well as applications on the end device can be selectively enabled by the user of the end device. In an exemplary embodiment the user can enable the control of either a select region OR only a select set of applications from either another user end point in the deployed network OR from a centralized server in the deployed network. The user using the above technique can record the entire session and store broadcast it at any point in time.

In one embodiment an article comprising a non transitory computer readable storage medium having instructions thereon which when executed by a computing platform result in execution of the above mentioned method. The method described in the foregoing may be in a form of a machine readable medium embodying a set of instructions that when executed by a machine causes the machine to perform any method disclosed herein. It will be appreciated that the various embodiments discussed herein may not be the same embodiment and may be grouped into various other embodiments not explicitly disclosed herein.

In addition it will be appreciated that the various operations processes and methods disclosed herein may be embodied in a machine readable medium and or a machine accessible medium compatible with a data processing system e.g. a computer system and may be performed in any order e.g. including using means for achieving the various operations . Accordingly the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.

