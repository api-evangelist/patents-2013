---

title: Mobile application testing systems and methods
abstract: Approaches for application testing are provided. An approach includes recording a test case of an application running on a mobile device. The approach also includes generating an expected layout based on object data received from the mobile device during the recording. The approach additionally includes receiving playback object data based on the test case being played back on the mobile device or a different mobile device. The approach further includes comparing the playback object data to the expected layout data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09459994&OS=09459994&RS=09459994
owner: KONY, INC.
number: 09459994
owner_city: Orlando
owner_country: US
publication_date: 20131206
---
The present invention relates generally to mobile applications and more particularly to systems and methods of testing mobile applications.

Mobile application development is the process by which application software is developed for handheld devices such as personal digital assistants enterprise digital assistants mobile phones e.g. smartphones tablet computers etc. Mobile applications e.g. apps can be pre installed on devices during manufacturing downloaded by customers from various mobile software distribution platforms or delivered as web applications using server side or client side processing to provide an application like experience within a web browser.

It is common for an application developer to test an application before releasing the application to the end user. Testing may be used to find and fix errors in an application so that once released the end user has a positive user experience when using the application. Mobile applications may be initially tested within the development environment using emulators and later subjected to field testing. Emulators provide an inexpensive way to test applications on mobile devices to which developers may not have physical access. Many aspects of application testing are performed manually which leads to the application testing process as a whole being time consuming and expensive.

In accordance with aspects of the invention there is a method of application testing that includes recording a test case of an application running on a mobile device. The method also includes generating an expected layout based on object data received from the mobile device during the recording. The method additionally includes receiving playback object data based on the test case being played back on the mobile device or a different mobile device. The method further includes comparing the playback object data to the expected layout data. At least one of the recording the generating the receiving and the comparing are performed by a computing device.

In accordance with additional aspects of the invention there is a system for testing applications. The system includes a test server including a test control is adapted to generate an expected layout based on object data received while recording a test case of an application receive playback object data during playback of the test case on a mobile device equipped with the application and compare the playback object data to the expected layout.

In accordance with further aspects of the invention there is a computer system for testing an application the system including a CPU a computer readable memory and a computer readable storage media. The system includes program instructions to program instructions to communicate with an agent loaded on a mobile device to remotely control running a test case of the application on the mobile device. The system includes program instructions to program instructions to receive from the agent loaded on the mobile device a screen shot and playback object data associated with a view of the application during the test case wherein the playback object includes at least one of object location and object size. The system includes program instructions to program instructions to compare the playback object data to an expected layout and to compare the screen shot to an expected screen shot. The program instructions are stored on the computer readable storage media for execution by the CPU via the computer readable memory. The receiving and the comparing are performed at a test server that is separate from the mobile device

The present invention relates generally to mobile applications and more particularly to systems and methods of testing mobile applications. More particularly the invention facilitates testing an application running on a mobile device by leveraging the use of an agent installed on the mobile device or the use of a javascript library for web based applications. In embodiments the agent or script referred to herein simply as the agent provides a mechanism for reporting to a test server data defining an actual layout of objects presented by the application on the mobile device. The test server may be used to compare the actual layout to a stored e.g. intended layout for testing the application. The layout may include repeating objects that extend beyond the display area of the mobile device. In this manner implementations of the invention may be used to facilitate user experience testing based on layouts instead of or in addition to pixel based comparisons.

A pixel based way of performing user experience testing involves taking a screen shot of a baseline of the screen and comparing future results e.g. a screen shot of a device during a test run against the baseline screen shot. Pixel based test engines are not suited for comparing user experiences that leave the page e.g. objects that extend beyond the display area of the mobile device or that have a dynamic area that repeats since the focus is on pixel comparison of the display area. Accordingly in pixel based comparisons small changes to the user experience can require recreating the tests which is time consuming. Additionally the focus is on pixel comparison even though layout is usually the first problem especially for repeating areas.

In contrast to pixel based systems implementations of the invention focus on the ability to test the layout of a mobile user interface UI instead of testing the UI only from a pixel perspective. In embodiments the agent at the mobile device reports the properties of each layout and the objects in the layout to a test server. Instead of focusing on the pixels an object consumes on the screen the test server stores the layout object location height width etc. which facilitates comparing test run layouts to intended layouts e.g. for error detection.

In accordance with aspects of the invention the agent resides on the mobile device and acts as a reporter for the application that is running on the mobile device. In embodiments the agent reports information associated with the application to a test server which may be part of a test management system. The information may include for example but is not limited to application performance memory usage visual layout called methods inputs and outputs from the screen and any variables the developer marks for introspection e.g. monitoring . In embodiments the server receives object data from the agent and generates a layout based on the object data e.g. when recording a test case or compares the object data to a stored layout e.g. when playing back a test case .

Layout based UX testing in accordance with aspects of the invention focuses on the layout of the page and ensuring the layout meets a design specification whereas pixel based testing focuses on the exact location of an image or object on the screen. Additionally layout based UX testing in accordance with aspects of the invention tests layout components that are off the screen whereas pixel based testing is limited to what is seen on the screen. Further layout based UX testing in accordance with aspects of the invention focuses on the layout which allows the tester to determine repeating widgets within the layout ensures integration quality of testing data and promotes testing early and often. Pixel based testing on the other hand cannot test objects that are not on the screen uses image recognition to determine GUI objects and often requires reproducing a master image for comparison.

Aspects of the invention may include combining layout based testing and pixel based testing. For example implementations of the invention may compare playback layouts to previous layouts to ensure the screen is properly formatted accommodate dynamic areas using layout testing ensure properties such as word wrap are enabled combine edge detection with text pixel detection to find text overruns without need for storing golden images and perform conventional pixel testing as an added layer of comparison.

It should be understood that browser based testing can have multiple challenges in a mobile and desktop environment since there is a wide range of browsers to support. For example code introspection and remote controlling of multiple browsers is a difficult task with ongoing maintenance concerns as well as noted differences in which browsers can report an object that is in the same location. For example where pixel comparison may show the pages are equal layout comparison may show a multipixel difference in location height or width. Also there is an inherent complexity of taking screenshots on each platform. Although the functions may overall stay the same there may be presentation nuances that are presented when tests are run across channels. In view of these and other issues the present invention provides a testing system which provides many advantages over current manual processes as described herein.

Implementations of the invention may be directed to automating aspects of application testing. By implementing the automated aspects of the present invention testing throughout the software lifecycle is greatly improved. In addition it is now possible to improve visibility into backlog through defect tracking system integration as well as reduce the number of testers needed to perform testing and reduce errors introduced by manual testing. It is also now possible to automate repeatable processes for consistency allowing testers to use test cases as building blocks for larger test scenarios provide deep coverage in all areas of the application under test and produce automated reports based on testing processes.

According to aspects of the invention a multichannel automated testing solution focuses on one or more of the following building blocks of testing unit testing functional testing data testing and user experience testing. These different testing techniques in accordance with aspects of the present invention are described below in detail.

Unit testing provides developers with a mechanism for testing a unit of code of an application to detect problems before the application is passed to the tester. In embodiments unit testing tests individual functions in isolation with the ability to set preconditions and post conditions. Unit testing also allows grouping smaller tests into larger scenarios. Unit testing also includes tolerance testing regular expression and range function testing. According to aspects of the invention application developers can unit test locally and store the unit test within a test server. In embodiments aspects of unit testing may be automated by integrating unit testing into the application build process e.g. build management system with application programming interfaces API s that can be called from software.

Functional testing provides testers with the ability to validate expected navigation and behaviors of the application. In embodiments functional testing monitors the behavior of the application to ensure the application displays the correct outputs navigates correctly and correctly responds to swipes taps inputs and other gestures for example. In this manner functional testing tests the inputs and outputs of the application from the glass or screen perspective of the mobile device on which the application is run.

Data testing ensures that the information used for testing is correct. In embodiments data testing includes testing of services and data throughout the application lifecycle. Initial data testing provides a mechanism for ensuring data quality with back end systems. Other aspects of data testing called introspection focus on what methods are called by the application being tested what data is passed between methods and the order in which these methods are called. In this manner data testing ensures that data from external services is correct and gives developers insight into key data points within their application in relation to the testing steps and processes.

User experience UX testing in aspects of the invention provides a mechanism for testing the overall user experience through the use of layout validation in conjunction with pixel comparison. In embodiments UX testing ensures that objects display correctly on the mobile device screen including ensuring that text boxes labels and other user interface components are properly displayed in a correct format. In this manner implementations of the invention create a robust automated testing mechanism that can compare dynamic repeating content as well as understand specific areas of both layout and pixel comparison.

For example UX testing compares objects on the view and their properties such as height width location on the page and other widget level properties and can create repeating patterns that recognize when visual objects repeat on the page so the testing automation system can test complex repeating user interfaces. The UX testing can also test static areas of a view using pixel comparison which allows a tester to select a specific area for image comparison or detect text overruns as examples.

An exemplary application testing scenario in accordance with aspects of the invention is as follows. First a developer creates an application sets the application to test mode and publishes the application to a testing application store. Next a tester e.g. one or more users in a test team downloads the application from the testing application store to one or more mobile devices. In embodiments the tester records one or more new test cases by for example logging onto a test workbench e.g. a software interface of a test server adapted for among other things software based control of test applications installed on mobile devices opening a recording dialog selecting the mobile device with the application installed beginning recording the new script and manipulating the application using the device e.g. gestures such as finger taps swipes button inputs etc. while the test system is recording. At each page e.g. different screen of the application on the mobile device the test server saves a screenshot of the view on the mobile device saves any objects on the page and their layout properties and saves any inputs to and outputs from methods of the application.

Still referring to the exemplary application testing scenario in accordance with aspects of the invention the tester may edit the recorded test case prior to performing functional and or UX testing. For example the tester may open the recorded test case using a test workbench look at each view that was produced in the test case and mark objects from one or more views for validation. The test workbench may be programmed with default values for some validation fields. In other validation fields the tester may define or import one or more values e.g. values such as username and password which may change with each test run. For example the tester may import a spreadsheet from another device and assign columns in the spreadsheet to a field of the test case for dynamic validation. The tester may also change the timeout of one or more back end calls e.g. to account for back end server performance. The tester may also change the time to think that was recorded by the test server when the test case was recorded e.g. to make the test run faster between screens and or applications selections that do not require back end service calls. The tester may save the edited test case at the test server for later running tests of the application on one or more mobile devices.

With continued reference to the exemplary application testing scenario in accordance with aspects of the invention the tester may perform functional testing and or UX testing using the saved test case including automated scripts. In embodiments functional testing and UX testing may each include for example selecting a saved test case from a testing menu of the test workbench using the test workbench to select a device on which to run the test running the test on the mobile device from the test server recording the results of the test at the test server and checking the results of the test. Functional testing may specifically include the test server recording at each step of the test for each mobile device a screen shot of the application at the mobile device any functions called by the application including the order in which the functions were called and any variables the tester set for watches in the application code. UX testing may specifically include the test server receiving layout information from the mobile device running the application comparing the received layout information to layout information that was stored when recording the test case and marking as possible defects those fields that do not match.

The present invention may be embodied as a system method or computer program product. The present invention may take the form of a hardware embodiment a software embodiment or a combination of software and hardware. Furthermore the present invention may take the form of a computer program product embodied in a computer readable storage medium of expression having computer usable program code embodied in the medium. The computer usable or computer readable medium may be any medium apparatus or device that can contain store and communicate the program for use by or in connection with an instruction execution system apparatus or device. The computer readable storage medium may be for example an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device.

The server includes a computing device which can be resident on a network infrastructure or computing device. The computing device includes a processor e.g. a CPU a memory A an I O interface and a bus . The bus provides a communications link between each of the components in the computing device . In addition computing device includes a random access memory RAM a read only memory ROM and an operating system O S . The computing device is in communication with external I O device resource and a storage system B. The I O device can comprise any device that enables an individual to interact with the computing device e.g. user interface or any device that enables the computing device to communicate with one or more other computing devices e.g. devices etc. using any type of communications link. The storage system B can store data such as one or more databases which can be maintained for example by an enterprise administrator.

The processor executes computer program code e.g. program control which can be stored in memory A and or storage system B. In embodiments the program control controls a test control which comprises an application that is adapted to perform one or more of the processes described herein. The test control can be implemented as one or more program code in program control stored in memory A as separate or combined modules. Additionally test control may be implemented as separate dedicated processors or a single or several processors to provide the functions described herein. While executing the computer program code processor can read and or write data to from memory A storage system B and or I O interface . In this manner the program code executes the processes of the invention.

According to aspects of the invention test control communicates with one or more mobile devices . . . each of which has a copy of an application to be tested. As used herein a mobile device refers to a smartphone tablet computer personal digital assistant or similar device that includes a mobile operating system wireless communication antenna processor user interface memory etc. In embodiments an agent resides on each mobile device and reports information about application to test control when application is running. More specifically in embodiments agent is configured to remote control the mobile device and by adding an agent to the device it is now possible to capture performance data of the device e.g. application and send this application information to the computing device e.g. test control . In embodiments test control and more specifically computing device may be implemented as or on a testing integrated development environment IDE . In this way agent can be added to an application that sends native object information to the testing IDE including the dimensions and location of objects the native ID the class and the text associated with the object amongst other functionality and features.

In aspects of the invention test control may use agent to remotely control the mobile devices for testing purposes. The agent may be one or more scripts or other suitable programming and may be added to application when the application code is compiled. While application is running on one of mobile devices agent sends native object information to test control such as for example an identification of methods called by application data inputs and data outputs associated with each called method user inputs e.g. taps clicks swipes etc. performance data from the device dimensions and locations of objects on the mobile device screen and the native ID the class and the text associated with each object amongst other features and combinations thereof.

The program code can include computer program instructions which are stored in a computer readable storage medium. The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer. Moreover any methods provided herein in the form of flowcharts block diagrams or otherwise may be implemented using the computer program instructions implemented on the computer readable storage medium.

The computer readable storage medium may be for example an electronic magnetic optical electromagnetic infrared and or semiconductor system. More specific examples a non exhaustive list would include a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any combination thereof. Accordingly the computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device of the present invention.

Still referring to the various devices of the system may communicate via a network which may be one or more conventional communication network types including but not limited to Internet local area network LAN WiFi etc. In one exemplary non limiting environment test app store is a storage node in a cloud environment and server communicates with devices locally e.g. through LAN or WiFi.

More specifically in aspects of the invention UI includes a play button record button stop button and rewind button that control the recording and playback of a test case. In embodiments the UI may also display a list of mobile devices that are currently connected to the test server e.g. server via the test agent in the particular application. In aspects of the invention the list is automatically populated by the test server which scans for devices on which the application and agent are installed e.g. via communication between the agent and the test server . Each mobile device in the list may be associated with a selection field that permits a user to select the particular mobile device for either recording or running the test case. In this manner a test case may be recorded from a first mobile device and run on a second different mobile device. In aspects of the invention the application and the test case are programmed with logic that permits use on different native platforms e.g. different mobile operating systems and devices such that a test case may be recorded from a first device having a first operating system and then run on a second device having a second operating system different than the first operating system.

In aspects of the invention the test server e.g. test control is configured to use the object data received from the agent to create and display e.g. in UI a layout of the view and a screen shot corresponding to the layout . More specifically screen shot is a copy of what is displayed on the mobile device in a single step of the application when recording the test and layout shows a representation of the objects included in the screen shot . For example screen shot includes objects and and layout includes corresponding objects and . In embodiments the location size spacing etc. of the objects and in layout is determined based on object data received from the agent e.g. object height object width object location word wrap font font size password border padding margin etc. . In this manner the test server e.g. test control is adapted to create layout based on data received from the agent.

The screen shot in is useful for conventional pixel based comparisons of this application running on the mobile device. However the screen shot does not provide information about repeating objects that are associated with this view that are off the screen e.g. not currently shown on the mobile device but accessible by scrolling the display on the mobile device.

In the example shown in there are two additional objects and that are not shown in screen shot but nonetheless are presentable in the view of the application. These additional objects and are depicted outside of screen shot to represent that they are associated with the view of screen shot but not actually shown in the screen shot . If a user were to employ a gesture at the mobile device to scroll the objects the screen display of the mobile device would change from showing objects to showing objects or objects while static objects and remain stationary on the screen as is understood by those of skill in the art.

With continued reference to and in accordance with aspects of the invention the agent at the mobile device sends information to the test server about all of the objects associated with the view including objects that are off the screen but that are accessible by scrolling. In this manner the layout at the UI of the test workbench may be configured to show all of the objects of a repeating area e.g. of repeating area of the mobile device not just those that are shown in the screen shot . This permits implementations of the invention to be used in comparing a layout of objects rather than just pixel based comparison of individual screen shots. The flexibility provided by comparing layouts instead of pixels permits a tester to change an aspect of the test e.g. change a number of objects in a repeating area without having to re create the entire test. The flexibility provided by comparing layouts instead of pixels further permits recording a test on one platform e.g. a first operating system running a first type of mobile device and running the recorded test on different platforms e.g. other operating systems running on other types of devices .

In aspects of the invention the test server e.g. running test control may be configured to automatically determine whether each object in layout is in a repeating area or a non repeating area of the display on the mobile device. For example the application may be coded to provide each object with an attribute e.g. non repeating vertical repeating horizontal repeating etc. and these attributes may be passed from the agent to the test server during recording of the test. Additionally or alternatively the test control may detect repeating objects using pattern recognition techniques based on the object data received from the agent e.g. object location size shape proximity etc. . In embodiments the test server marks objects as being in a repeating area based on such automatic determination.

In embodiments the test control may also be configured to permit a user to manually designate any object in the layout as repeating or non repeating e.g. by selecting an object in the layout in the UI and marking the object as desired e.g. from a programmed menu function. For example a tester can mark a set of objects as repeating by viewing the test objects recorded in a layout editor e.g. UI that highlights the layout objects for the view. The tester highlights the area that repeats and the test server asks the tester whether the highlighted area repeats horizontally or vertically. In this manner a tester may manually mark a repeating area .

In embodiments the test control may also be configured to permit a user to manually change the layout of objects in a recorded test case. For instance a repeating area may contain two objects when the test case is recorded. A tester may use a layout editor e.g. UI to change the test data to return three objects in the repeating area. In embodiments the test server adjusts the layout by changing the location of objects that follow e.g. below the repeating area to compensate for the additional object.

In embodiments after a test case is recorded and saved using the test workbench e.g. using UI and or UI the test case may be run against an application on a mobile device. When a tester runs a test case e.g. by selecting a device from list and hitting the play button the test control e.g. test control sends execution commands to the application e.g. application via the agent e.g. agent at the mobile device e.g. mobile device . The execution commands cause the application to step through the test case in which case the test server e.g. server is remotely controlling the application during testing.

In embodiments while the application is stepping through the test case the agent e.g. agent at the mobile device acts as a reporter for the application by communicating back to the test control information such as application performance memory usage visual layout methods being called and inputs and outputs from the screen of the mobile device in conjunction with any variables included in the test case amongst other test control information.

More specifically in aspects of the invention the agent sends to the test server object information that is generated by the application running the test case. The test server then compares the object information from the playback to the stored layout to mark possible errors in the application. For objects that are marked as repeating in the stored layout e.g. manually marked by the tester and or automatically marked by the test server as being in a repeating area as described above the test server compares the corresponding objects in the playback to determine whether these objects all have the correct size and or location. For example in non limiting implementations such as the illustrative example of the test server determines whether the playback objects in the repeating area all have the same size e.g. length and width and the same location e.g. vertical or horizontal alignment on the screen . In this example when playback objects in the repeating area do not have the same size and location the test server automatically indicates a possible layout error for a tester to further examine. The invention is not limited to comparing object size and location in error detection during playback and any object attributes can be used such as but not limited to font style font size word wrap on or off 

In further aspects of the invention error detection in the playback includes examining which objects immediately follow a repeating area on the screen. For example in the illustrative case shown in static objects and follow e.g. are immediately adjacent to the end of the repeating area of repeating objects in the layout in the stored test case. A tester may change the test case to add more objects to the repeating area or remove objects from the repeating area . In this example during playback the test server determines whether static objects and immediately follow the last object of the repeating area on the screen. In the event static objects and do not follow the last object of the repeating area in the playback the test server indicates a possible layout error for a tester to review for possible correction. In some cases the last object of the repeating area is not visible on the screen e.g. as in however aspects of the invention accommodate this situation by showing all objects in the repeating area in the layout . In this manner implementations of the invention provide for automatic error detection in the layout of applications during playback.

In embodiments during playback the test server e.g. running test control determines patterns of objects based on their location width and height. The test server can be programmed to determine when a repeating area ends back looking for the start of a non repeating area following the repeating area. When a repeating object is not detected the test server looks for the adjusted area below it to ensure the area has stopped repeating. If the area below the repeating section does not match the adjusted area then it can mark it as a possible error.

In the case of two separate repeating areas that occur sequentially the test control is configured to look for the first repeating area and if it does not exist e.g. 0 rows of returned objects the test control then looks for the second repeating area. Still referring to the case of two separate repeating areas that occur sequentially in the event the second row returns zero the test server considers a layout that returns data from the first repetition and the adjusted area as valid. Still further there may be occurrences when no rows are returned from an area. In this case the anchor area and the adjusted area would still be present with no repeats in between.

Accordingly implementations of the invention as described herein provide the ability to detect repeating areas using coded attributes that indicate a repeating area detect sub areas of a repeating area using code attributes determine the layout pattern of objects within a repeating section based on the properties of the repeating section e.g. height width relative position etc. manually mark an area as repeating manually mark sub objects in a repeating section as repeating the layout type manually mark an area as anchored e.g. the area does not move when the rest of the screen does determine objects are anchored based on attributes of the object store the location of objects within a repository in conjunction with the properties of the object e.g. height word wrap and width etc. and or determine if objects off the screen are laid out properly.

At step the application from step is installed on a device. This may be performed for example by downloading the application from the testing app store to a mobile device e.g. device .

At step a test case is recorded using the application and device of step . In embodiments a tester uses a test workbench e.g. UI of test server to select the mobile device and the application and selects e.g. presses a record button to begin recording the test case e.g. as described with respect to . While recording the test case the test server e.g. running test control receives from the agent at the device object data and screen shots of the application as the application is running on the device. As described with respect to the object data may include but is not limited to object height object width object location word wrap font font size password border padding and margin. Step may include the test server automatically generating an expected layout e.g. layout based on object data received from the mobile device during the recording e.g. in a manner similar to that described with respect to .

At step repeating and non repeating areas of the layout are determined. As described with respect to the repeating and non repeating areas may be determined automatically by the test server and or manually by the tester. For example the test server may automatically determine repeating and non repeating areas based on built in attributes of the objects e.g. received with the object data and or using pattern recognition. The tester may mark areas as repeating or non repeating using a layout editor e.g. UI . At step the test server stores the recorded test case including the layout s with an area marked as repeating or non repeating e.g. at a test case repository e.g. at storage system B .

At step the recorded test case from step is played back on a test device for testing the application. In embodiments a tester uses UI to select a test device on which the application e.g. from step is installed. The test device may be the same device from step or a different device e.g. as indicated at list of UI . The tester also uses the test server to retrieve the recorded test case e.g. from a test case repository. In aspects of the invention the test server sends instructions to the agent at the test device to remotely control playback of the application on the test device according to the recorded test case. During playback the agent sends to the test server object data and screen shots of the application running on the test device. Step may thus include the test server receiving playback object data from an agent on a test device based on the test case being played back on the test device.

At step the test server compares the playback object data e.g. received at step to the layout s of the stored test case e.g. the expected layout data for detecting errors in the layout s of the playback. In embodiments based on the repeating areas defined at step the test server performs error detection by comparing the size and location of playback objects within the repeating areas e.g. as described above with respect to FIG. B . In additional embodiments the test server performs error detection by analyzing the static objects that follow repeating areas e.g. as described above with respect to . Step may include the test server flagging potential errors for further review by the tester.

At step the test server performs pixel based comparison by comparing the screen shots of the recorded test case to the screen shots of the playback. The pixel based comparison may be performed in a conventional manner e.g. by comparing an entire screen shot obtained during playback to a stored screen shot recorded with the test case. Additionally or alternatively the pixel based comparison may be performed based on user selected sub areas of a screen shot e.g. as described in greater detail with respect to .

In embodiments of when a tester wants to record a test case the tester can select the device they want to record it on from a testing workbench. After selecting the connected device the tester can select a record button on the testing workbench. The agent sends the properties of each native object displayed on the view. When a user performs an event such as a button tap the agent sends the information about what event was performed and the object it was performed against to the TMS. When a new view is displayed as the result of an event the objects within the view are sent to the TMS. The TMS records all the objects for each view for later editing. The agent also sends a screenshot of the view to the TMS. When the tester completes the recording process a stop button is selected on the testing workbench. The TMS can then send a stop request to the agent which stops sending recording data to the Test Management Server. In this way the test recorded to the workbench is provided in a step by step format with the test recorded in for example javascript format allowing the user to manipulate the actual scripting. Also advantageously the object data from each page as well as each action performed is sent to the TMS. Thus all inputs recorded are sent to the TMS.

More specifically at step the tester selects a device with which to work e.g. via list of UI . At step the tester selects e.g. presses the record button e.g. button of UI and the TMS begins recording the test case. During recording the agent sends to the TMS the data associated with execution of the application e.g. methods called method inputs method returns screen shots and or object data as described herein etc. . For example when a user performs an action at step such as a button tap swipe textbox input etc. then at step the agent sends to the TMS information about what application event was performed and the application object it was performed against. At step the agent sends to the TMS a screenshot of each application view when the view changes. This gives the tester a reference point to see when an event or input was received and what effect it had on the view. At step the agent sends to the TMS object data for each object within the view. The TMS records all the object data and for each view for later editing. At step the tester presses the stop button e.g. button of UI and the TMS stops recording the test case. At step the TMS sends a stop request to the agent telling the agent to stop sending data to the TMS.

At step the tester uses the UI of the test server to stop recording the test e.g. button or the like . At step the test server instructs the agent to stop recording. At step the agent confirms to the test server that recording is stopped. At step the test server informs the tester that testing is stopped e.g. via UI . In this manner the process depicted in may be used to record a visual test case in accordance with aspects of the invention.

In the flow of the actors include the tester the TMS and the agent instrumented in the application and communicating with the TMS. In this flow preconditions are the tester has recorded a test case on the TMS and the application is instrumented with the agent. In this flow the post conditions are the test case is played back on the device the agent sends performance data to the TMS server for each step of the test case the agent sends introspection data functions called and variables being instrumented to the TMS and the agent sends layout data back to the TMS.

More specifically at step the tester selects the test case they want to run on the device. Step may include for example loading a stored test case from a test repository e.g. storage system B into the test workbench e.g. UI . At step the tester selects a device for play back e.g. via list of UI . At step the tester selects a type of play back data to obtain from the device e.g. UX data performance data or functional data. The selection at step may be made using an interface e.g. UI of the test server. At step the tester initiates play back of the test case on the application at the device e.g. by pressing a play button of an interface e.g. button of UI . At step the TMS sends instructions to the agent to perform a step of the test case which causes the agent to cause the application to perform the step of the test case. At step the agent reports data e.g. object data as described herein about the performed step of the test case to the TMS. At step the TMS records the data received from the agent e.g. in the test repository. Steps and repeat for all steps of the test case or until the tester stops the play back.

In the flow of the actors include the tester a UX component e.g. test control that validates layouts properties such as word wrap height password field etc. the TMS and the test repository that stores saved test cases. In this flow preconditions are a test case has been recorded and the UX objects are recorded. In this flow the post conditions are objects within the view are marked as repeating allowing the UX component to validate repeating objects during test replay. Variations to this flow may include single objects can be marked for UX validation as individual objects.

More specifically at step the tester selects a test case for marking using a user interface e.g. UI or UI of the test server e.g. TMS . At step the test server retrieves the test case selected at step from the test repository. At step the test case is returned from the test repository to the test server. At step the test server instructs the UX component to create an overlay of objects. At step the UX component creates the overlay e.g. screen shot and layout and provides the overlay to the test server. At step the test server displays the overlay to the tester via layout editor e.g. UI . At step the tester uses the layout editor to select and mark one or more objects of the layout as repeating objects e.g. as described above with respect to . At step the test server requests the UX component to validate the selected repeating objects. At step the UX component informs the test server that the selected repeating objects are validated. At step the test server informs the tester that the selected repeating objects are validated. At step the tester saves the test case via the layout editor. At step the test server saves the test case to the test repository. At step the test repository informs the test server that the save of the test case is complete. At step the test server informs the tester that the save of the test case is complete.

In the flow of the actors include the agent e.g. sends visual objects as part of the playback sequence the TMS e.g. controls the user interface and connectivity to the test repository and the UX component e.g. test control that functions as the validator . In this flow preconditions are repeating objects are previously marked for sectional validation and the tester has selected the tests for playback. In this flow post conditions are each repeating section is validated and individual objects are validated. In a variation of this flow a section may fail validation and get marked as a possible error. In the event the section is marked as an error but the tester accepts it anyway then the possible error is stored as a tolerance.

More specifically at step the tester initiates UX testing e.g. by initiating playback of a stored test case on a device having the application and the agent. In embodiments the tester selects a stored test case for playback selects a device e.g. from list and provides a particular input e.g. presses play button or the like via a user interface of the test server. At step the test server instructs the agent to perform the next step of the recorded test case. At step the agent passes the step the application the application performs the step and the agent returns the object data associated with the view to the test server. At step the test server compares the object data of step to the stored layout and object data of the stored test case of step . In embodiments step may include but is not limited to comparing object sizes and locations in repeating areas and or analyzing non repeating objects that follow repeating areas e.g. as described above with respect to . In the event the objects are determined as valid then at step the UX component returns a message indicating the objects are valid. In the event the objects are determined as a possible error then at step the UX component returns a message indicating a possibly invalid object. At step the test server sends the results of the validation to the reporting component. At step the test server informs the tester that the test is complete.

In the flow of the actors include the tester the test repository and the TMS. In this flow preconditions are the test case has been recorded with screen shots of each step and the tester is logged into the TMS. In this flow post conditions are Area to be compared as part of a UX test case is saved in the test repository. In a variation of this flow multiple areas of a view can be saved as part of a test case.

More specifically at step the tester chooses a view for which they want to perform static testing e.g. pixel based comparison . Step may be performed via a user interface e.g. UI or UI of the test server. At step the test server displays the selected view to the tester e.g. via the user interface. In embodiments the view displayed at step is an editable view that allows the tester to select areas for static comparison. At step the tester selects an area of the displayed view for static comparison. Step is an optional step in which the tester may alter the properties of the selected area. In embodiments the tester may request view and change one or more properties e.g. buffer etc. of the selected area. In this manner implementations of the invention allow the image to move a small amount on the device without affecting the test case. At step the tester initiates saving the captured area s . At step the test server saves the capture area s to the test repository. At step the test repository confirms the capture area s are saved. At step the test server informs the tester the capture area s are saved.

In embodiments a service provider such as a Solution Integrator could offer to perform the processes described herein. In this case the service provider can create maintain deploy support etc. the computer infrastructure that performs the process steps of the invention for one or more customers. These customers may be for example any business that uses technology. In return the service provider can receive payment from the customer s under a subscription and or fee agreement and or the service provider can receive payment from the sale of advertising content to one or more third parties.

The foregoing examples have been provided for the purpose of explanation and should not be construed as limiting the present invention. While the present invention has been described with reference to an exemplary embodiment Changes may be made within the purview of the appended claims without departing from the scope and spirit of the present invention in its aspects. Also although the present invention has been described herein with reference to particular materials and embodiments the present invention is not intended to be limited to the particulars disclosed herein rather the present invention extends to all functionally equivalent structures methods and uses such as are within the scope of the appended claims.

