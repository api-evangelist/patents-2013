---

title: Cloning and recovery of data volumes
abstract: Aspects of a data environment, such as the cloning, hibernation, and recovery of databases, are managed using a separate control environment. A monitoring component of the control environment can periodically communicate with the data environment to determine any necessary actions to be performed, such as to recover from faults or events for a data instance in the data environment. A workflow can be instantiated that includes tasks necessary to perform actions such as recovery, hibernation, resumption from hibernation, or backup or cloning. Tasks of the workflow can cause certain jobs to be performed by host managers in the data environment to affect calls made to the control environment.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09218245&OS=09218245&RS=09218245
owner: Amazon Technologies, Inc.
number: 09218245
owner_city: Reno
owner_country: US
publication_date: 20131025
---
This application is a continuation of allowed U.S. patent application Ser. No. 13 620 962 filed Sep. 15 2012 which is a continuation of U.S. patent application Ser. No. 12 415 968 filed Mar. 31 2009 and issued as U.S. Pat. No. 8 332 365 on Dec. 11 2012 

A portion of the disclosure of this patent document contains material that is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever.

As an increasing number of applications and services are being made available over networks such as the Internet an increasing number of content application and or service providers are turning to technologies such as cloud computing. Cloud computing in general is an approach to providing access to electronic resources through services such as Web services where the hardware and or software used to support those services is dynamically scalable to meet the needs of the services at any given time. A user or customer typically will rent lease or otherwise pay for access to resources through the cloud and thus does not have to purchase and maintain the hardware and or software to provide access to these resources.

A customer can occasionally wish to create a copy of a data store for purposes such as testing or development. Using conventional approaches the source data store must be taken offline and the data exported out of the data store to create a new copy. Such an approach is inefficient requires manual intervention and results in the unavailability of the source data store for a period of time.

Further a data store or data instance might experience an error of a type that requires a complete recovery action. In conventional systems this typically requires a database administrator DBA or similar operator to physically visit the location of the machines hosting the data store locate the physical backups often on tape and execute and monitor the recovery process. Such an approach is inefficient at best and can result in substantial downtime of the data store. Further such an approach is limited to the data on the backup tapes which often are archived once a day at most.

Systems and methods in accordance with various embodiments of the present disclosure may overcome one or more of the aforementioned and other deficiencies experienced in conventional approaches to managing aspects of data storage in an electronic environment. In particular various embodiments provide a separate control environment or control plane that can be used to monitor and or control aspects of a data environment or data plane. The functionality of a control plane can be provided as a set of Web services for example enabling the control plane to act as a virtual database administrator DBA . A user or customer can submit a request to the control plane through an externally visible application programming interface API for example which can be analyzed to determine actions to be performed in the data plane such as actions that create delete modify expand or otherwise modify a data store or data storage instance. State information can be passed to a component of the data plane for each task necessary to perform the action such that the control plane can manage the performance of the tasks without having direct access into the data stores or other such components of the data plane. Once provisioned a user can native access to the data instance s in the data plane and can simply point existing applications such as MySQL applications to the domain name system DNS name or other location information for the particular data instance. There is no restriction or modification of query models or other such functionality as a user can continue to use applications built on MySQL Oracle or other such database technology.

Systems and methods in accordance with various embodiments take advantage of a monitoring component in the control plane to continually monitor performance aspects of the data environment such as by monitoring host machines or data instances for a relational database or other such data storage system. The monitoring component can analyze information from the data environment and determine any data stores or data instances that potentially need recovery or a similar action. The control environment can generate and execute a workflow to perform the recovery action. In various embodiments the workflow is able to access snapshots of the data store or instance at some point in the past. Snapshots of a data store or data instance can be captured at any appropriate time such as every fifteen minutes and stored to persistent storage. The persistent storage can be in the control environment and or the data environment in various embodiments. Log data can also be captured and stored to persistent storage. As part of the recovery workflow a new data instance can be instantiated and data from the snapshots can be used to populate the new data instance. Information from the log files can be applied to the new data instance to bring the data instance to a more recent state. If available log data that was not persisted can also be applied to bring the data instance up to the most recent state of the source data instance. A data store or data instance alternatively can be restored to a previous point in time instead of the most recent state where the data for that point in time is available.

Systems and methods in accordance with various embodiments also can allow a customer to clone a data store or data instance using snapshot and or log information. A customer can call into the control environment with an identifier of a source data store or data instance to be cloned. To generate a copy of a data instance for example a workflow is generated in the control environment that causes a new data instance to be instantiated in the data environment and data can be pulled from the appropriate snapshots to populate the data instance. If available and useful for the request the stored log data also can be applied to bring the new instance to a particular state that is more recent than the snapshot used to populate the data instance. Other log information also can be used if available. Such an approach does not require the original data instance to be taken down for a period of time and does not require a tedious manual exporting and importing process.

The illustrative environment includes at least one application server and a data store . It should be understood that there can be several application servers layers or other elements processes or components which may be chained or otherwise configured which can interact to perform tasks such as obtaining data from an appropriate data store. As used herein the term data store refers to any device or combination of devices capable of storing accessing and retrieving data which may include any combination and number of data servers databases data storage devices and data storage media in any standard distributed or clustered environment. The application server can include any appropriate hardware and software for integrating with the data store as needed to execute aspects of one or more applications for the client device handling a majority of the data access and business logic for an application. The application server provides access control services in cooperation with the data store and is able to generate content such as text graphics audio and or video to be transferred to the user which may be served to the user by the Web server in the form of HTML XML or another appropriate structured language in this example. The handling of all requests and responses as well as the delivery of content between the client device and the application server can be handled by the Web server. It should be understood that the Web and application servers are not required and are merely example components as structured code discussed herein can be executed on any appropriate device or host machine as discussed elsewhere herein. Further the environment can be architected in such a way that a test automation framework can be provided as a service to which a user or application can subscribe. A test automation framework can be provided as an implementation of any of the various testing patterns discussed herein although various other implementations can be used as well as discussed or suggested herein.

The environment also includes a development and or testing side which includes a user device allowing a user such as a developer data administrator or tester to access the system. The user device can be any appropriate device or machine such as is described above with respect to the client device . The environment also includes a development server which functions similar to the application server but typically runs code during development and testing before the code is deployed and executed on the production side and is accessible to outside users for example. In some embodiments an application server can function as a development server and separate production and testing storage may not be used.

The data store can include several separate data tables databases or other data storage mechanisms and media for storing data relating to a particular aspect. For example the data store illustrated includes mechanisms for storing production data and user information which can be used to serve content for the production side. The data store also is shown to include a mechanism for storing testing data which can be used with the user information for the testing side. It should be understood that there can be many other aspects that may need to be stored in the data store such as for page image information and access right information which can be stored in any of the above listed mechanisms as appropriate or in additional mechanisms in the data store . The data store is operable through logic associated therewith to receive instructions from the application server or development server and obtain update or otherwise process data in response thereto. In one example a user might submit a search request for a certain type of item. In this case the data store might access the user information to verify the identity of the user and can access the catalog detail information to obtain information about items of that type. The information then can be returned to the user such as in a results listing on a Web page that the user is able to view via a browser on the user device . Information for a particular item of interest can be viewed in a dedicated page or window of the browser.

Each server typically will include an operating system that provides executable program instructions for the general administration and operation of that server and typically will include a computer readable medium storing instructions that when executed by a processor of the server allow the server to perform its intended functions. Suitable implementations for the operating system and general functionality of the servers are known or commercially available and are readily implemented by persons having ordinary skill in the art particularly in light of the disclosure herein.

The environment in one embodiment is a distributed computing environment utilizing several computer systems and components that are interconnected via communication links using one or more computer networks or direct connections. However it will be appreciated by those of ordinary skill in the art that such a system could operate equally well in a system having fewer or a greater number of components than are illustrated in . Thus the depiction of the system in should be taken as being illustrative in nature and not limiting to the scope of the disclosure.

An environment such as that illustrated in can be useful for a provider such as an electronic marketplace wherein multiple hosts might be used to perform tasks such as serving content authenticating users performing payment transactions or performing any of a number of other such tasks. Some of these hosts may be configured to offer the same functionality while other servers might be configured to perform at least some different functions. The electronic environment in such cases might include additional components and or other arrangements such as those illustrated in the configuration of discussed in detail below.

Systems and methods in accordance with one embodiment provide a relational database service RDS that enables developers customers or other authorized users to easily and cost effectively obtain and configure relational databases so that users can perform tasks such as storing processing and querying relational data sets in a cloud. While this example is discussed with respect to the Internet Web services and Internet based technology it should be understood that aspects of the various embodiments can be used with any appropriate services available or offered over a network in an electronic environment. Further while the service is referred to herein as a relational database service it should be understood that such a service can be used with any appropriate type of data repository or data storage in an electronic environment. An RDS in this example includes at least one Web service that enables users or customers to easily manage relational data sets without worrying about the administrative complexities of deployment upgrades patch management backups replication failover capacity management scaling and other such aspects of data management. Developers are thus freed to develop sophisticated cloud applications without worrying about the complexities of managing the database infrastructure.

An RDS in one embodiment provides a separate control plane that includes components e.g. hardware and software useful for managing aspects of the data storage. In one embodiment a set of data management application programming interfaces APIs or other such interfaces are provided that allow a user or customer to make calls into the RDS to perform certain tasks relating to the data storage. The user still can use the direct interfaces or APIs to communicate with the data repositories however and can use the RDS specific APIs of the control plane only when necessary to manage the data storage or perform a similar task.

The control plane in this example is essentially a virtual layer of hardware and software components that handles control and management actions such as provisioning scaling replication etc. The control plane in this embodiment includes a Web services layer or tier which can include at least one Web server for example along with computer executable software application servers or other such components. The Web services layer also can include a set of APIs or other such interfaces for receiving Web services calls or requests from across the network which the Web services layer can parse or otherwise analyze to determine the steps or actions needed to act on or process the call. For example a Web service call might be received that includes a request to create a data repository. In this example the Web services layer can parse the request to determine the type of data repository to be created the storage volume requested the type of hardware requested if any or other such aspects. Information for the request can be written to an administration Admin data store or other appropriate storage location or job queue for subsequent processing.

A Web service layer in one embodiment includes a scalable set of customer facing servers that can provide the various control plane APIs and return the appropriate responses based on the API specifications. The Web service layer also can include at least one API service layer that in one embodiment consists of stateless replicated servers which process the customer APIs. The Web service layer can be responsible for Web service front end features such as authenticating customers based on credentials authorizing the customer throttling customer requests to the API servers and marshalling or unmarshalling requests and responses. The API layer also can be responsible for reading and writing database configuration data to from the administration data store in response to the API calls. In many embodiments the Web services layer will be the only externally visible component or the only component that is visible to and accessible by customers of the control service. The servers of the Web services layer can be stateless and scaled horizontally as known in the art. API servers as well as the persistent data store can be spread across multiple data centers in a region for example such that the servers are resilient to single data center failures.

The control plane in this embodiment includes what is referred to herein as a sweeper component . A sweeper component can be any appropriate component operable to poll various components of the control plane or otherwise determine any tasks to be executed in response to an outstanding request. In this example the Web services layer might place instructions or information for the create database request in the admin data store or a similar job queue and the sweeper can periodically check the admin data store for outstanding jobs. Various other approaches can be used as would be apparent to one of ordinary skill in the art such as the Web services layer sending a notification to a sweeper that a job exists. The sweeper component can pick up the create database request and using information for the request can send a request call or other such command to a workflow component operable to instantiate at least one workflow for the request. The workflow in one embodiment is generated and maintained using a workflow service as is discussed elsewhere herein. A workflow in general is a sequence of tasks that should be executed to perform a specific job. The workflow is not the actual work but an abstraction of the work that controls the flow of information and execution of the work. A workflow also can be thought of as a state machine which can manage and return the state of a process at any time during execution. A workflow component or system of components in one embodiment is operable to manage and or perform the hosting and executing of workflows for tasks such as repository creation modification and deletion recovery and backup security group creation deletion and modification user credentials management and key rotation and credential management. Such workflows can be implemented on top of a workflow service as discussed elsewhere herein. The workflow component also can manage differences between workflow steps used for different database engines such as MySQL as the underlying workflow service does not necessarily change.

In this example a workflow can be instantiated using a workflow template for creating a database and applying information extracted from the original request. For example if the request is for a MySQL Relational Database Management System RDBMS instance as opposed to an Oracle RDBMS or other such instance then a specific task will be added to the workflow that is directed toward MySQL instances. The workflow component also can select specific tasks related to the amount of storage requested any specific hardware requirements or other such tasks. These tasks can be added to the workflow in an order of execution useful for the overall job. While some tasks can be performed in parallel other tasks rely on previous tasks to be completed first. The workflow component or service can include this information in the workflow and the tasks can be executed and information passed as needed.

An example create database workflow for a customer might include tasks such as provisioning a data store instance allocating a volume of off instance persistent storage attaching the persistent storage volume to the data store instance then allocating and attaching a DNS domain name service address or other address port interface or identifier which the customer can use to access or otherwise connect to the data instance. In this example a user is provided with the DNS address and port to be used to access the instance. The workflow also can include tasks to download and install any binaries or other information used for the specific data storage technology e.g. MySQL . The workflow component can manage the execution of these and any related tasks or any other appropriate combination of such tasks and can generate a response to the request indicating the creation of a database in response to the create database request which actually corresponds to a data store instance in the data plane and provide the DNS address to be used to access the instance. A user then can access the data store instance directly using the DNS address and port without having to access or go through the control plane . Various other workflow templates can be used to perform similar jobs such as deleting creating or modifying one of more data store instances such as to increase storage. In some embodiments the workflow information is written to storage and at least one separate execution component not shown pulls or otherwise accesses or receives tasks to be executed based upon the workflow information. For example there might be a dedicated provisioning component that executes provisioning tasks and this component might not be called by the workflow component but can monitor a task queue or can receive information for a provisioning task in any of a number of related ways as should be apparent.

As mentioned various embodiments can take advantage of a workflow service that can receive requests or calls for a current state of a process or task such as the provisioning of a repository and can return the current state of the process. The workflow component and or workflow service do not make the actual calls or requests to perform each task but instead manage the state and configuration information for the workflow that enables the components of the control plane to determine the next task to be performed and any information needed for that task then generate the appropriate call s into the data plane including that state information whereby a component of the data plane can make the call to perform the task. Workflows and tasks can be scheduled in parallel in order to increase throughput and maximize processing resources. As discussed the actual performing of the tasks will occur in the data plane but the tasks will originate from the control plane. For example the workflow component can communicate with a host manager which can make calls into the data store. Thus for a given task a call could be made to the workflow service passing certain parameters whereby the workflow service generates the sequence of tasks for the workflow and provides the current state such that a task for the present state can be performed. After the task is performed or otherwise resolved or concluded a component such as the host manager can reply to the service which can then provide information about the next state in the workflow such that the next task can be performed. Each time one of the tasks for the workflow is performed the service can provide a new task to be performed until the workflow is completed. Further multiple threads can be running in parallel for different workflows to accelerate the processing of the workflow.

The control plane in this embodiment also includes at least one monitoring component . When a data instance is created in the data plane information for the instance can be written to a data store in the control plane such as a monitoring data store . It should be understood that the monitoring data store can be a separate data store or can be a portion of another data store such as a distinct set of tables in an Admin data store or other appropriate repository. A monitoring component can access the information in the monitoring data store to determine active instances in the data plane . A monitoring component also can perform other tasks such as collecting log and or event information from multiple components of the control plane and or data plane such as the Web service layer workflow component sweeper component and various host managers. Using such event information the monitoring component can expose customer visible events for purposes such as implementing customer facing APIs. A monitoring component can constantly monitor the health of all the running repositories and or instances for the control plane detect the failure of any of these instances and initiate the appropriate recovery process es .

Each instance in the data plane can include at least one data store and a host manager component for the machine providing access to the data store. A host manager in one embodiment is an application or software agent executing on an instance and or application server such as a Tomcat or Java application server programmed to manage tasks such as software deployment and data store operations as well as monitoring a state of the data store and or the respective instance. A host manager in one embodiment listens on a port that can only be reached from the internal system components and is not available to customers or other outside entities. In some embodiments the host manager cannot initiate any calls into the control plane layer. A host manager can be responsible for managing and or performing tasks such as setting up the instances for a new repository including setting up logical volumes and file systems installing database binaries and seeds and starting or stopping the repository. A host manager can monitor the health of the data store as well as monitoring the data store for error conditions such as I O errors or data storage errors and can restart the data store if necessary. A host manager also perform and or mange the installation of software patches and upgrades for the data store and or operating system. A host manger also can collect relevant metrics such as may relate to CPU memory and I O usage.

The monitoring component can communicate periodically with each host manager for monitored instances such as by sending a specific request or by monitoring heartbeats from the host managers to determine a status of each host. In one embodiment the monitoring component includes a set of event processors or monitoring servers configured to issue commands to each host manager such as to get the status of a particular host and or instance. If a response is not received after a specified number of retries then the monitoring component can determine that there is a problem and can store information in the Admin data store or another such job queue to perform an action for the instance such as to verify the problem and re provision the instance if necessary. The sweeper can access this information and kick off a recovery workflow for the instance to attempt to automatically recover from the failure. The host manager can act as a proxy for the monitoring and other components of the control plane performing tasks for the instances on behalf of the control plane components. Occasionally a problem will occur with one of the instances such as the corresponding host instance or volume crashing rebooting restarting etc. which cannot be solved automatically. In one embodiment there is a logging component not shown that can log these and other customer visibility events. The logging component can include an API or other such interface such that if an instance is unavailable for a period of time a customer can call an appropriate events or similar API to get the information regarding the event. In some cases a request may be left pending when an instance fails. Since the control plane in this embodiment is separate from the data plane the control plane never receives the data request and thus cannot queue the request for subsequent submission although in some embodiments this information could be forwarded to the control plane . Thus the control plane in this embodiment provides information to the user regarding the failure so the user can handle the request as necessary.

As discussed once an instance is provisioned and a user is provided with a DNS address or other address or location the user can send requests directly to the data plane through the network using a Java Database Connectivity JDBC or other such client to directly interact with that instance . In one embodiment the data plane takes the form of or at least includes or is part of a computing cloud environment or a set of Web services and resources that provides data storage and access across a cloud or dynamic network of hardware and or software components. A DNS address is beneficial in such a dynamic cloud environment as instance or availability failures for example can be masked by programmatically remapping a DNS address to any appropriate replacement instance for a use. A request received from a user or application for example can be directed to a network address translation NAT router or other appropriate component which can direct the request to the actual instance or host corresponding to the DNS of the request. As discussed such an approach allows for instances to be dynamically moved updated replicated etc. without requiring the user or application to change the DNS or other address used to access the instance. As discussed each instance can include a host manager and a data store and can have at least one backup instance or copy in persistent storage . Using such an approach once the instance has been configured through the control plane a user application service or component can interact with the instance directly through requests to the data plane without having to access the control plane . For example the user can directly issue structured query language SQL or other such commands relating to the data in the instance through the DNS address. The user would only have to access the control plane if the user wants to perform a task such as expanding the storage capacity of an instance. In at least one embodiment the functionality of the control plane can be offered as at least one service by a provider that may or may not be related to a provider of the data plane but may simply be a third party service that can be used to provision and manage data instances in the data plane and can also monitor and ensure availability of those instances in a separate data plane .

Upon receiving the job information the information is analyzed to determine and or assemble an appropriate workflow for the requested action . As discussed different tasks can be selected for the workflow based upon factors such as the type of action requested and the type of database engine being used. Beginning with the first task of the workflow state information is sent to a host manager in the data environment operable to use the state information to determine a task to be performed perform the task with respect to a data repository and or data instance and return a response upon completion of the task . Upon receiving the response the workflow component determines whether there is another task to be performed . If so state information for the next task is sent to the host manager and upon completion of that task the host manager sends a response to the workflow component. After the final task has been completed a message is sent to the requesting customer or another appropriate user application or location that the requested action has been completed . After the action has been performed the customer is able to directly access the data instance upon which the action was performed using a data interface of the data environment without accessing or passing through the control plane . As mentioned the user can provided with a DNS name and port number for example such that if the action resulted in movement of data or another similar action the customer or an application can continue to use the same DNS name which will be directed to the appropriate location in the data plane.

As discussed one advantage to use of a control plane is that the control plane can function as a virtual database administrator DBA and avoid the need for a human DBA to perform tasks such as monitoring performance data and performing trending or other such analysis. A control plane can also perform functions such as automatically performing scaling recovery or other such actions in the event of an actual or predicted need for action. Conventional approaches relying on a DBA to perform actions such as monitoring analysis cloning and recovery are expensive and time consuming and can result in significant unavailability of customer data during the recovery and or cloning processes.

As discussed above a control plane can be used to perform tasks such as cloning a data store or recovering a data store to a specified point in time. illustrates an example of a process that can be used to clone a data store in accordance with one embodiment. In this example a request is received to clone a data store . This request can be received from a customer or application such as by receiving a Web services call through an externally facing API of the Web services layer or can be generated by a monitoring component or other component of the control plane. A data source identifier can be determined from the request . The data source identifier can be any appropriate alphanumeric or other identifier used to uniquely identify a data store data instance or other data storage mechanism to serve as a source for generating a new data store. Any criteria for the cloned data store also can be extracted from the request or from another appropriate source such as user preference or configuration information. The data source identifier can point to a data source having a database engine query language hardware class storage requirements availability zone or any of a number of other such aspects that are different from the criteria specified for the cloned data store.

Information for the cloning request can be written to a job queue . As discussed above this can involve components of the Web service layer parsing the request analyzing the parsed information and writing information for the determined cloning action to an Admin data store. A component such as a sweeper component can pull the information from the job queue and pass the information to a workflow component which can generate a clone data store or similar workflow . The workflow can include any appropriate tasks for performing the cloning such as may be specific to the criteria specified for the cloning and or the determined aspects of the data source. For example if the source is a MySQL data store but the clone is to be generated using Oracle RDBMS then at least one specific task can be selected for the workflow that handles at least one of these database engines.

As discussed above state information for each task of the workflow can be sent to a host manager in the data environment . As part of one task a new data store is provisioned using the criteria specified by the request . As part of another task a determination is made as to the logical volume group that corresponds to the data source and each data volume that is a member of that group. For each data volume which may correspond to a separate physical or logical device a snapshot can be stored to a persistent data storage an any particular time interval etc. A snapshot as used herein refers to the state of the data volume at a particular point in time where the snapshot can include information relating to the data data structure data schema or any other appropriate aspect of the data volume. In some embodiments snapshots are taken every fifteen minutes although any other appropriate timing or interval can be used as appropriate. A DBA or other operator can configure the timing for snapshots in accordance with various embodiments.

As part of one of the tasks a host manager can pull information from each identified relevant snapshot and store the information into the new data store. In some embodiments the new data store will be created with the appropriate storage capacity and data from the snapshots will be written to and potentially apportioned across the new data store independent of the volume distribution of the source data store. In other embodiments the new data store can use the same number of data volumes of the same size such that each individual data volume can be cloned using the corresponding snapshot.

As should be apparent copying data from the latest set of snapshots to the new data store will generally cause the state of the new data store to represent the time at which the snapshots were taken. If a user wants the data store cloned as of the time of the request or another specified time more recent than the snapshot s another task of the workflow can cause log information for each data volume or the logical volume group to be accessed and the updates in the log executed against the new data store in order to cause the data store to be current as of a specific time. In some cases the updating can be done iteratively in order to bring the data store as close as possible to the current state of the data store. If the user wants the clone to be an exact replica of the current data store then the customer can request that no updates be made to the source data store between the time of the request and the creation of the new data store such that applying the log data to the snapshot data will result in what should be a completely replicated state of the data store. Once the source data is able to be updated however the source and cloned data stores will not be guaranteed to be in sync unless another mechanism is used to cause each update of the source data store to also be executed against the cloned data store. The new data store can be assigned a new data store identifier and possibly other information such as a DNS name and port number that allows the customer or other such user to access the new data instance .

Such an approach is advantageous for at least the reason that the cloning procedure does not result in an outage or otherwise significantly affect the availability of the source data store. Conventional cloning approaches require manual execution and can be very difficult to accomplish. For example there is generally no native call to clone a data store such that the data has to be exported using a complex export process. Further a DBA or other operator typically has to configure the new data store re instantiate the entire data instance and import back the data. Such an approach is complex and time consuming and provides a substantial opportunity for error that may not be detected until it is too late. An approach in accordance with various embodiments discussed herein however can allow a user to cause a data store to be cloned by submitting a single clone data store or similar request or call whereby components of a control plane can cause a clone to be created that can have different aspects or criteria than the source data store. Further such an approach does not result in any unavailability of the source data store.

A data store cloned as of a particular point in time can be advantageous in many situations. For example a new instance of a data store can be generated for use in testing an application or service that allows actual data to be used without risk of losing or corrupting the source data. In other cases a customer might want periodic snapshots of the data without affecting the availability of the data. For example a customer might run quarterly reports and might want a snapshot captured and stored at the end of each quarter such that information in the reports can be verified for compliance or other such reasons. A customer also might want to test a particular data storage configuration before making changes to the source data store. In such a case the customer can clone the data store and adjust the configuration for purposes of running regression test cases or otherwise verifying the configuration. Such an approach also would allow a customer to modify a schema of a data store during a development phase enabling the customer to apply and test the schema without having to risk losing data or having to undo the changes. The source data sets will not be disrupted.

Cloning as of a particular point in time also can be beneficial for customers who will not be accessing a data store for an extended period of time. If for example a company does not access a particular data store on weekends when development is not currently in process or at other such times the company might prefer not to incur costs for keeping the data store running while it is idle. illustrates an example process in accordance with one embodiment wherein a request is received from a company to hibernate a data instance . This request might be received at the end of the day on Friday or after the last access of the week for example. A backup such as a snapshot is performed for the data instance and stored for subsequent retrieval. The data instance is then hibernated with the hibernated instance being released from the active cloud such that all the company has to pay for is the storage of the snapshot data. The company then may not be charged for power processing capacity memory capacity allocated bandwidth or any other such fee that might be incurred if the data store is active and available over that period. Such an approach can be cost effective as the customer only is charged for an active data store during those periods when the customer requires the data store to be active. The company can be notified as to the success of the hibernation .

When it is desirable for a hibernated data instance to again be active and available a resume or similar request can be received from the company wherein a resume or similar workflow can be kicked off in the control plane . In other embodiments the resume workflow can be the result of a resume call generated from within the control plane. In some embodiments a job can be stored to the job queue with a particular time for resuming which will be picked up by the sweeper at or around the scheduled time. In other embodiments information can be written to a data store that can be picked up by the monitoring component around the scheduled time and written to the job queue. Various other such approaches to scheduling the resume action can be used as should be apparent. As part of the tasks of the resume workflow a new data instance can be provisioned in the data environment and a resume task can be passed to the appropriate host manager with at least a snapshot identifier wherein the host manger can obtain the snapshot data for the data instance and copy the data to the new data instance . An identifier for the new data instance can be tied to the DNS and port name for example to allow a user to directly access the data instance via the data plane.

Another use for cloned data instances involves the repartitioning of a data store. When repartitioning a clone of the data instance can be made for each partition. The data that is not needed for each individual partition can be selectively deleted. Such an approach can be a relatively easy and efficient way to ensure that the appropriate data is received to each partition as it is not necessary to determine up front where to send each block of data but instead each partition can receive a full copy and then delete the data that does not belong to the partition. Such an approach can be more efficient as the work can be distributed to the machines for each partition. Such an approach also can be relatively fast as the information does not need to be extracted out of the data store but can instead utilize snapshots or archived data volumes. Such an approach can be beneficial because unlike normal import and export operations it is not necessary to take the source data store offline or otherwise make the data store unavailable. At most approaches in some embodiments temporarily shut down the individual volumes but there is no significant downtime for the data store. Another advantage is that any amount of data can be copied over using the snapshots. Standard import and export routines only are practical for data stores up to a certain size.

Approaches in accordance with various embodiments also can provide an improved way to recover a data store or data instance. As discussed snapshot data can be captured and persistently stored at appropriate intervals such as every fifteen minutes. Log volumes also can be persistently stored at regular intervals which can be the same as or shorter than the intervals for the snapshots. In the event of a failure of a data store for example the data store can be recovered to a particular time in the recent past using the snapshots and where appropriate and available the stored log data.

In some instances the monitoring component can receive information from the host manager as to the available snapshots and log files to be used for the recovery. If the monitoring component has information indicating that an error started occurring twenty five minutes ago which might have lead to the problem with the data instance then the monitoring component or other component of the control plane can decide to restore to an earlier available time such as an hour ago. Such an approach enables the data instance to be restored to a point which should not include any of the errors. The log information then can be examined and applied as desired in order to ensure that the problems with the data instance do not recur. In other instances the workflow can simply request to restore to the most recent snapshot and log information. If snapshot and log information are alternatively and or additionally stored in the control environment then the monitoring component can determine the available information for the data instance and select the copy or copies from which to pull information.

Based on the workflow the monitoring component can pass information to at least one host manager of the control plane to provision a new data instance in the data environment and a recovery task can request information from the snapshot for the data instance identifier be copied into the new data instance . As discussed the snapshot data can come from the control and or data environments depending upon the embodiment. After the data is copied into the data instance state information for another recovery task can be passed to at least one host manager of the control plane to apply the information from a persistently stored log volume to the new data instance . In certain situations the customer or operator might configure or decide to simply restore to the last snapshot while in other situations where the error was not due to corrupt data but instead to a hardware or similar glitch the customer might want the most recent state of the data instance any might apply as much log information as possible. If available state information for another recovery task can be passed to at least one host manager of the control plane to apply the information from any cached or otherwise available log information to the new data instance . In cases where the log volumes are snapshotted or otherwise stored to persistent storage at intervals such as every five minutes for example there can be information in memory or another storage location that can be available that corresponds to the log information since the last log snapshot. This information then can be applied to the new data instance to bring the new data instance back to as recent a state as possible. Once all log information has been applied the data instance can be made available to users . In some cases a DNS name and port can be associated with the new data instance identifier such that customers calling into the data environment for the original data instance can instead have their calls routed to the new data instance.

For each of the tasks in such a workflow at least one test for success or failure can be executed. For example it can be desirable to ensure that a the snapshot data was stored into the new data instance properly before applying log information as well as ensuring that log information was properly extracted before applying the information to the new data instance. If a test is run for a task and it is determined that the task was not successful the task can be retried at least one time possibly up to a determined or selected number of times before generating an error message or other such notification. The testing and retry can be performed automatically via the data environment or as managed by the control environment. If a task fails a specified number of times the entire process can be failed in order to avoid errors data loss or other such issues. Further the control plane can manage the reversal of previous tasks such as undoing of the application of a log volume to the data of a new data instance. Various other approaches can be used as well within the scope of the various embodiments.

Such an approach can provide for automatic point in time recovery that does not rely upon backup tapes or other such manual processes. Such a process also can be relatively fast as the recovery operation can begin as soon as the problem is discovered and does not rely upon a human to get to the location which sometimes can take an hour or more locate the appropriate tapes or other backups and then perform each step of the recovery process. Even in situations where a customer does not want an automatic recovery to occur but instead wishes to authorize the recovery the customer can be notified of the problem via the control plane and can call into the control plane via a recovery or similar API which can recover from the failure by creating a new data store or data instance based at least upon the source identifier for the data store or instance and potentially upon a point in time specified by the customer.

As mentioned the control plane layer can take advantage or sit on top of various basic software frameworks for performing tasks such as implementing workflows establishing secure communication channels between the host managers of the data plane and the components of the control plane installing software on the instances of the data plane and performing various database backup and recovery procedures.

One such aspect that can rely upon an underlying framework relates to repository and data backup. It can be desirable for the control plane to backup customer repositories and instances for various reasons such as user initiated backups which can be performed during the backup time windows and system initiated backups during database restore etc. A single framework can be implemented to handle both instances. To backup a repository a framework can handle backing up both the data files and any associated log files. While various steps and processes will be described it should be understood that various steps and approaches can differ from various database engines such as MySQL and others.

An approach for backing up data in accordance with one embodiment uses two separate backup processes a first procedure that backs up data at relatively short intervals in the data environment and a second procedure that backs up data at relatively longer intervals in the control environment. illustrates an example of a configuration for performing such operations. As discussed above snapshots can be stored for a source data store or volume group at intervals such as every fifteen minutes as may be controlled in the data environment using the respective host manager . These snapshots are stored to a snapshot data store in the data environment without having to suspend operations of the source data store . Log data for the source data store also can be written to a log data store of the data environment. As discussed such storage can allow for point in time recovery cloning or other such operations.

As part of a second process snapshot and log data also can be written to a snapshot data store and a log data store of the control environment. It should be understood that although these data stores are described as separate data stores they can be part of the same data store such as separate tables in the Admin or monitoring data stores discussed above. Such storage to the control environment can be desirable as the data in the data environment can be restored to a particular point in time regardless of the availability of the data environment. Such storage may not be done as frequently as in the data environment however as the copying to the control plane can cause data environment operations to be suspended until shapshots are taken of the appropriate data volumes. The log files can similarly be copied over to the control environment. When it is desired to create a new data store a new host manager can be tasked with pulling and or applying the appropriate information from the snapshot and log data stores. If a recovery action is performed using information from the control environment then the data can be passed from the snapshot and log data stores to the new data store while if the action is a cloning action from the data environment the data can be passed from the local snapshot and log data stores to the new data store . The flow of data can be determined in some embodiments by the task of the particular workflow for the determined action. It should be understood however that in various embodiments all of the snapshot and log data can be stored to either the control environment or data environment or even to a separate environment accessible by at least one of the control and data environments. For example the snapshot and log data can be stored by the customer.

In some embodiments components of an Admin tier of the control plane can wait for the backup window before initiating a backup procedure. Since a backup window might be specified for once a day in some embodiments a snapshot can be captured to the control plane approximately once a day which can make it advantageous in some instances to also store more frequent snapshots to the data environment. Once inside the backup window the Admin tier can create a workflow that will create a workflow instance for repository backups. In one example the workflow invokes a supendDatabaseForBackup or similar API for the host manager. This API can manage tasks to for example flush and lock the tables suspend I O to the data volume create and mount an LVM snapshot for the log volume create a log position file with the last log position and start a timer to resume the database. This timer can be used to resume the repository in case the Admin tier hangs up while performing a task such as taking snapshots preventing the repository from being accidentally suspended for indefinite period of time. The workflow can poll the host manager for completion of these and or other such tasks. Once the workflow has confirmed that the host manager has suspended the repository the workflow can attempt to backup the data volumes using a set of ordered tasks. For example the workflow can indicate to create snapshots of each data volume and verify that the snapshots have been successfully created. A row can be inserted for each snapshot volume in a location such as a backup data volumes table. Subsequently the workflow can invoke a host manager s resumeDatabaseFromBackup or similar API. This process can copy the repository logs and log position information to an appropriate storage location can unmount the log snapshot remove the log snapshot log volume and unlock all tables. The Admin tier then can create a customer event that indicates the backup has been completed and the repository is again available.

As discussed the log files also can be backed up in a similar fashion. The logs can be used to perform tasks such as replaying various transactions in case the data files have to be restored. The engine logs can be copied to an appropriate storage location such that previously backed up log files can be obtained using a simple list command. A host manager will use this result to determine whether there are logs that need to be copied. For example the host manager can request a bucket list to obtain the list of log files written such that the last sequence can be backed up. If new logs have been created it can first be determined that the logs are not actively being written to by a database engine and then the logs can be copied and the copying verified to have been performed successfully.

As discussed above users of the control plane can perform various tasks relating to data repositories and data instances using a set of APIs or other such interfaces. While the selection and names of the example APIs are used for purposes of explanation it should be apparent that other selections combinations names and other aspects can vary between the various embodiments. As discussed in one of the examples above customers can create a data store using a CreateDatabase or similar API. The user can call a Web service to specify any desired values for an instance type which describes the CPU and memory capacity storage size repository name port and other such values. The customer could also utilize a DescribeDatabase or similar API to poll on the status of the repository to determine the state of the repository such as whether the repository state is provisioned. When the status of database is AVAILABLE for example the customer can retrieve an endpoint which is returned as part of a response to the DescribeDatabase call. Customers can delete a repository or instance using a DeleteDatabase or similar API. Customers also can have the ability to hibernate a repository or instance placing an instance in a sleep state for example using a HibernateDatabase or similar API. During such a sleep state the data typically will not be accessible but the data will be backed up durably. Customers can wake a hibernated data repository or instance using a ResumeDatabase or similar API.

As mentioned earlier a control plane or service can handle the complexity of not just database provisioning but also tasks such as upgrades patch management backups and failover. A customer can control the times for backups and maintenance activities by enabling customers to specify or modify the backup window and maintenance window times while invoking a CreateDatabase or ModifyDatabase or similar API. Using a ModifyDatabase API customers can increase the storage size change the instance type or modify various other fields.

At some point the customer may wish to implement improved or updated processes for various development needs and may wish to set up a test instance of a particular data store. The customer may also want to take a snapshot of the production instance so that the test instance is fully populated and comparable with the production date. The customer decides that for the particular needs of the testing procedure the customer can utilize a SMALL instance for purposes of processing capacity and can provision the same storage capacity as is used for production. The customer thus can submit a request to clone the database using a command line tool such as by submitting 

As discussed previously the use of a control plane or service in accordance with various embodiments does not restrict the type of SQL queries that a customer can run and does not impose any restrictions relating to construction of a schema such as to be partition ready and not allow queries spanning partitions. Instead a repository such as a relational database can be provisioned in a computing cloud without restricting the users schema or queries. As commonly known even though there is a theoretical SQL standard the SQL quirks syntaxes and their behaviors e.g. NULL handling vary across different relational database engines e.g. MySQL Oracle or Postgres . For at least these reasons users may wish to choose a relational database engine that is familiar for purposes of programming and operations. Such an approach allows customers to use the same set of database tools that the customers have used previously for tasks such as data modeling development and debugging even when the customers migrate their data stores to the cloud or elsewhere via the control plane. Using such an approach customers are not required to rewrite their application or any operational tools which lowers the barrier of entry significantly for customers to move data to the cloud.

A customer s data repositories can be moved to the cloud in one embodiment by running the repositories on compute nodes of a cloud computing environment. Block level storage volumes such as off instance storage volumes that persist independently from the life of an instance can be used with these instances for storing the repository binary logs and volumes for example. Such an approach can be advantageous as the virtualization provides flexibility to quickly and easily scale a compute and storage resources for a repository. Further such an approach can provide for persistent storage in the cloud.

As known in the art relational databases can be run in different modes such as may include stand alone non replicated replicated or replicated and partitioned. A customer typically makes the choice of which mode to run for a repository based on the availability and scalability needs of the repository and the incurred total cost of ownership TCO . Some applications and services to not require a repository to be highly available and durable and may instead utilize a stand alone repository that is able to tolerate outages on the order of minutes. Other applications and servers can require a repository to be always available and require the repository to never lose data even in the event of a failure. In this case the applications and services typically require a replicated database offering. Some users applications or services require a massively scalable repository that can partition data across multiple repositories such that scaling can occur beyond the compute and storage capacity of a single database. To address these different use cases an approach in accordance with one embodiment offers at least two modes such as stand alone and high availability for each database engine. Some embodiments also allow customers build their own partitioning layer on top of either stand alone or high availability repositories.

As discussed above the various embodiments can be implemented in a wide variety of operating environments which in some cases can include one or more user computers computing devices or processing devices which can be used to operate any of a number of applications. User or client devices can include any of a number of general purpose personal computers such as desktop or laptop computers running a standard operating system as well as cellular wireless and handheld devices running mobile software and capable of supporting a number of networking and messaging protocols. Such a system also can include a number of workstations running any of a variety of commercially available operating systems and other known applications for purposes such as development and database management. These devices also can include other electronic devices such as dummy terminals thin clients gaming systems and other devices capable of communicating via a network.

Various aspects also can be implemented as part of at least one service or Web service such as may be part of a service oriented architecture. Services such as Web services can communicate using any appropriate type of messaging such as by using messages in extensible markup language XML format and exchanged using an appropriate protocol such as SOAP derived from the Simple Object Access Protocol . Processes provided or executed by such services can be written in any appropriate language such as the Web Services Description Language WSDL . Using a language such as WSDL allows for functionality such as the automated generation of client side code in various SOAP frameworks.

Most embodiments utilize at least one network that would be familiar to those skilled in the art for supporting communications using any of a variety of commercially available protocols such as TCP IP OSI FTP UPnP NFS CIFS and AppleTalk. The network can be for example a local area network a wide area network a virtual private network the Internet an intranet an extranet a public switched telephone network an infrared network a wireless network and any combination thereof

In embodiments utilizing a Web server the Web server can run any of a variety of server or mid tier applications including HTTP servers FTP servers CGI servers data servers Java servers and business application servers. The server s also may be capable of executing programs or scripts in response requests from user devices such as by executing one or more Web applications that may be implemented as one or more scripts or programs written in any programming language such as Java C C or C or any scripting language such as Perl Python or TCL as well as combinations thereof. The server s may also include database servers including without limitation those commercially available from Oracle Microsoft Sybase and IBM .

The environment can include a variety of data stores and other memory and storage media as discussed above. These can reside in a variety of locations such as on a storage medium local to and or resident in one or more of the computers or remote from any or all of the computers across the network. In a particular set of embodiments the information may reside in a storage area network SAN familiar to those skilled in the art. Similarly any necessary files for performing the functions attributed to the computers servers or other network devices may be stored locally and or remotely as appropriate. Where a system includes computerized devices each such device can include hardware elements that may be electrically coupled via a bus the elements including for example at least one central processing unit CPU at least one input device e.g. a mouse keyboard controller touch screen or keypad and at least one output device e.g. a display device printer or speaker . Such a system may also include one or more storage devices such as disk drives optical storage devices and solid state storage devices such as random access memory RAM or read only memory ROM as well as removable media devices memory cards flash cards etc.

Such devices also can include a computer readable storage media reader a communications device e.g. a modem a network card wireless or wired an infrared communication device etc. and working memory as described above. The computer readable storage media reader can be connected with or configured to receive a computer readable storage medium representing remote local fixed and or removable storage devices as well as storage media for temporarily and or more permanently containing storing transmitting and retrieving computer readable information. The system and various devices also typically will include a number of software applications modules services or other elements located within at least one working memory device including an operating system and application programs such as a client application or Web browser. It should be appreciated that alternate embodiments may have numerous variations from that described above. For example customized hardware might also be used and or particular elements might be implemented in hardware software including portable software such as applets or both. Further connection to other computing devices such as network input output devices may be employed.

Storage media and computer readable media for containing code or portions of code can include any appropriate media known or used in the art including storage media and communication media such as but not limited to volatile and non volatile removable and non removable media implemented in any method or technology for storage and or transmission of information such as computer readable instructions data structures program modules or other data including RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disk DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the a system device. Based on the disclosure and teachings provided herein a person of ordinary skill in the art will appreciate other ways and or methods to implement the various embodiments.

The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. It will however be evident that various modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims.

