---

title: Method of entropy randomization on a parallel computer
abstract: Method, system, and computer program product for randomizing entropy on a parallel computing system using network arithmetic logic units (ALUs). In one embodiment, network ALUs on nodes of the parallel computing system pseudorandomly modify entropy data during broadcast operations through application of arithmetic and/or logic operations. That is, each compute node's ALU may modify the entropy data during broadcasts, thereby mixing, and thus improving, the entropy data with every hop of entropy data packets from one node to another. At each compute node, the respective ALUs may further deposit modified entropy data in, e.g., local entropy pools such that software running on the compute nodes and needing entropy data may fetch it from the entropy pools. In some embodiments, entropy data may be broadcast via dedicated packets or included in unused portions of existing broadcast packets.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09335970&OS=09335970&RS=09335970
owner: INTERNATIONAL BUSINESS MACHINES CORPORATION
number: 09335970
owner_city: Armonk
owner_country: US
publication_date: 20130305
---
This application is a continuation of co pending U.S. patent application Ser. No. 13 760 510 filed Feb. 6 2013. The aforementioned related patent application is herein incorporated by reference in its entirety.

High quality random numbers are essential for many aspects of computer systems most notably in the area of security. However computer systems have a difficult time generating high quality random numbers i.e. numeric sequences that are close to being truly random. There are many algorithms that generate random numbers but they typically generate the same sequence of numbers thus suffering from predictability.

Further some computing systems may lack or have limited access to a convenient source of entropy. Entropy data from the seemingly random behavior of a human typing on a keyboard for example may be unavailable to computers without keyboard inputs. The computer may instead rely on time intervals between interrupts as a source of entropy but this is often unhelpful due to the regularity of such intervals and the ability of outside forces to manipulate such intervals. As a result computing systems with limited external or user inputs such as massively parallel computers or embedded systems may lack entropy data.

Embodiments disclosed herein provide a method system and computer program product for performing an operation for randomizing entropy data on a parallel computing system the operation including broadcasting from a first node of the parallel computing system first entropy data to one or more nodes of the parallel computing system. The operation further includes modifying by arithmetic logic units ALUs on the first node and the one or more nodes the first entropy data during the broadcast. In addition the operation includes storing the first entropy data as modified during the broadcast at each of the one or more nodes.

Embodiments of the invention randomize entropy on a parallel computing system using network arithmetic logic units ALUs . In one embodiment network ALUs on nodes of the parallel computing system pseudo randomly modify entropy data during broadcast operations through application of arithmetic and or logic operations. That is each compute node s ALU may modify the entropy data during broadcasts thereby mixing and thus improving the entropy data with every hop of entropy data packets from one node to another. Here the modification of entropy data may be transparent to kernels and programs on the nodes of the parallel computing system and minimize the involvement of such kernels and programs in managing entropy data so as to reduce noise jitter which would otherwise result from such management. At each compute node the ALUs may deposit modified entropy data in e.g. local entropy pools such that software running on the compute nodes may fetch entropy data from the entropy pools. In some embodiments entropy data may be broadcast via dedicated packets. Alternatively entropy data may be included in unused portions of existing broadcast packets.

In the following reference is made to embodiments of the disclosure. However it should be understood that the disclosure is not limited to specific described embodiments. Instead any combination of the following features and elements whether related to different embodiments or not is contemplated to implement and practice the disclosure. Furthermore although embodiments of the disclosure may achieve advantages over other possible solutions and or over the prior art whether or not a particular advantage is achieved by a given embodiment is not limiting of the disclosure. Thus the following aspects features embodiments and advantages are merely illustrative and are not considered elements or limitations of the appended claims except where explicitly recited in a claim s . Likewise reference to the invention shall not be construed as a generalization of any inventive subject matter disclosed herein and shall not be considered to be an element or limitation of the appended claims except where explicitly recited in a claim s .

As will be appreciated by one skilled in the art aspects of the present disclosure may be embodied as a system method or computer program product. Accordingly aspects of the present disclosure may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore aspects of the present disclosure may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon.

Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable RF etc. or any suitable combination of the foregoing.

Computer program code for carrying out operations for aspects of the present disclosure may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects of the present disclosure are described below with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the disclosure. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

An entropy property as used herein refers to the ability of a random number generator to provide an equal probability of outputting any given value from a set of predefined values that the random number generator is configured to output. To improve the entropy property of a random number generator an I O node or a compute node may add entropy data to its entropy pool which may include a set of one or more entropy pool elements. Each of the entropy pool elements may include an arbitrary length sequence of bits and may be used as input to a random number generator in order to compute a random number.

As shown the I O node includes a random number generator an entropy pool and an arithmetic logic unit ALU . In one embodiment the random number generator may be configured to use entropy pool elements as initial values in the generation of pseudorandom numbers. That is the random number generator may generate pseudorandom numbers with the elements of the entropy pool as seed values. To improve the entropy property of the random number generator new entropy data may be added as elements to the entropy pool such that more initial values are available for the random number generator to use. In particular entropy data from the high quality entropy source as modified by the ALU may be added to the entropy pool . As discussed in greater detail below modification of entropy data during broadcast operations allows the high quality entropy data to undergo pseudorandom variation from node to node such that each node acquires different entropy data.

In one embodiment the ALU may be a hardware component supporting an internal network of the parallel computing system which may facilitate message passing. The ALU may be configured to perform arithmetic and logic operations to e.g. generate broadcast addresses manipulate packet data and the like. More specifically the ALU may be configured to modify during a broadcast operation entropy data being broadcasted. For example during the broadcast the ALU may perform an exclusive or XOR of the packet s payload i.e. the entropy data against some other value to modify the payload pseudo randomly. Here the other value may be e.g. a unique rank or location of the I O node a direction from which the entropy data arrived other entropy data received from the entropy source etc. In one embodiment the arithmetic or logic operation which is applied may also be determined based on e.g. the node rank or another value. In some embodiments entropy data may be broadcast via dedicated packets but entropy data may also piggyback in unused portions of packets broadcast as part of ongoing operations on the computing core.

As shown packets broadcast by the I O node may be transmitted to compute nodes and further forwarded to compute nodes and by each of compute nodes and respectively. Illustratively compute node includes a random number generator an entropy pool and an ALU which may be similar to the random number generator the entropy pool and the ALU respectively. Compute node and each of compute nodes may also include similar components. In one embodiment the ALUs and ALUs not shown of the compute nodes may modify packets having entropy data payloads via arithmetic and or logic operation upon e.g. receipt of the packets transmittal of the packets and the like. Doing so mixes and thus improves the entropy data with every hop of the packet from one node to another. As a result relatively small inputs of entropy data from the internal or external source may provide abundant entropy data for all nodes in the system. Further the ALUs may deposit the mixed entropy data in a corresponding entropy pool from where software on the respective node may retrieve and use the entropy data. As a result programs and kernels of the compute nodes may be freed from having to manage entropy data thereby eliminating noise and or jitter which may occur during such management.

As shown computer system includes a compute core having a number of compute nodes arranged in a regular array or matrix which perform the useful work performed by system . The operation of computer system including compute core may be controlled by control subsystem . Various additional processors in front end nodes may perform auxiliary data processing functions and file servers provide an interface to data storage devices such as disk based storage A B or other I O not shown . Functional network provides the primary data communication path among compute core and other system components. For example data stored in storage devices attached to file servers is loaded and stored to other system components through functional network .

Also as shown compute core includes I O nodes A C and compute nodes A I. Compute nodes provide the processing capacity of parallel system and are configured to execute applications written for parallel processing. I O nodes handle I O operations on behalf of compute nodes . Each I O node may include a processor and interface hardware that handles I O operations for a set of N compute nodes the I O node and its respective set of N compute nodes are referred to as a Pset. Compute core contains M Psets A C each including a single I O node and N compute nodes for a total of M N compute nodes . The product M N can be very large. For example in one implementation M 1024 1 K and N 64 for a total of 64 K compute nodes. In some embodiments each of I O nodes and compute nodes may include an ALU similar to the ALUs of which improves entropy data during broadcast operations by applying arithmetic and logic operations as broadcasted packets hop from one node to another.

In general application programming code and other data input required by compute core to execute user applications as well as data output produced by the compute core is communicated over functional network . The compute nodes within a Pset communicate with the corresponding I O node over a corresponding local I O collective network A C. The I O nodes in turn are connected to functional network over which they communicate with I O devices attached to file servers or with other system components. Thus the local I O collective networks may be viewed logically as extensions of functional network and like functional network are used for data I O although they are physically separated from functional network . One example of the collective network is a tree network. As discussed in greater detail below an I O node may broadcast entropy data over such a tree network and network ALUs of each compute node on the tree network may pseudorandomly modify the entropy data during the broadcast operation. That is during each hop of entropy data packets from one compute node to another in the tree network the ALUs of the compute nodes may e.g. upon receipt or transmission of the packets modify the entropy data. Entropy data as modified during the broadcast operations may then be stored at each of the compute nodes and used by applications running thereon e.g. random number generators .

Control subsystem directs the operation of the compute nodes in compute core . Control subsystem is a computer that includes a processor or processors internal memory and local storage . An attached console may be used by a system administrator or similar person. Control subsystem may also include an internal database which maintains state information for the compute nodes in core and an application which may be configured to among other things control the allocation of hardware in compute core direct the loading of data on compute nodes and perform diagnostic and maintenance functions.

Control subsystem communicates control and state information with the nodes of compute core over control system network . Network is coupled to a set of hardware controllers A C. Each hardware controller communicates with the nodes of a respective Pset over a corresponding local hardware control network A C. The hardware controllers and local hardware control networks are logically an extension of control system network although physically separate.

In addition to control subsystem front end nodes provide computer systems used to perform auxiliary functions which for efficiency or otherwise are best performed outside compute core . Functions which involve substantial I O operations are generally performed in the front end nodes. For example interactive data input application code editing or other user interface functions are generally handled by front end nodes as is application code compilation. Front end nodes are connected to functional network and may communicate with file servers .

In one embodiment the computer system determines from among a plurality of class route identifiers for each of the compute nodes along a communications path from a source compute node to a target compute node in the network a class route identifier available for all of the compute nodes along the communications path. The computer system configures network hardware of each compute node along the communications path with routing instructions in dependence upon the available class route identifier and a network topology for the network. The routing instructions for each compute node associate the available class route identifier with the network links between that compute node and each compute node adjacent to that compute node along the communications path. The source compute node transmits a network packet to the target compute node along the communications path which includes encoding the available class route identifier in a network packet. The network hardware of each compute node along the communications path routes the network packet to the target compute node in dependence upon the routing instructions for the network hardware of each compute node and the available class route identifier encoded in the network packet. As used herein the source compute node is a compute node attempting to transmit a network packet while the target compute node is a compute node intended as a final recipient of the network packet.

In one embodiment a class route identifier is an identifier that specifies a set of routing instructions for use by a compute node in routing a particular network packet in the network. When a compute node receives a network packet the network hardware of the compute node identifies the class route identifier from the header of the packet and then routes the packet according to the routing instructions associated with that particular class route identifier. Accordingly by using different class route identifiers a compute node may route network packets using different sets of routing instructions. The number of class route identifiers that each compute node is capable of utilizing may be finite and may typically depend on the number of bits allocated for storing the class route identifier. An available class route identifier is a class route identifier that is not actively utilized by the network hardware of a compute node to route network packets. For example a compute node may be capable of utilizing sixteen class route identifiers labeled 0 15 but only actively utilize class route identifiers 0 and 1. To deactivate the remaining class route identifiers the compute node may disassociate each of the available class route identifiers with any routing instructions or maintain a list of the available class route identifiers in memory.

Routing instructions specify the manner in which a compute node routes packets for a particular class route identifier. Using different routing instructions for different class route identifiers a compute node may route different packets according to different routing instructions. For example for one class route identifier a compute node may route packets specifying that class route identifier to a particular adjacent compute node. For another class route identifier the compute node may route packets specifying that class route identifier to different adjacent compute node. In such a manner two different routing configurations may exist among the same compute nodes on the same physical network.

In one embodiment compute nodes are arranged logically in a three dimensional torus where each compute node may be identified using an x y and z coordinate. is a conceptual illustration of a three dimensional torus network of system according to one embodiment disclosed herein. More specifically illustrates a 4 4 4 torus of compute nodes in which the interior nodes are omitted for clarity. Although shows a 4 4 4 torus having 64 nodes it will be understood that the actual number of compute nodes in a parallel computing system is typically much larger. For example a complete Blue Gene L system includes 65 536 compute nodes. Each compute node in torus includes a set of six node to node communication links A F which allows each compute nodes in torus to communicate with its six immediate neighbors two nodes in each of the x y and z coordinate dimensions.

As used herein the term torus includes any regular pattern of nodes and inter nodal data communications paths in more than one dimension such that each node has a defined set of neighbors and for any given node it is possible to determine the set of neighbors of that node. A neighbor of a given node is any node which is linked to the given node by a direct inter nodal data communications path. That is a path which does not have to traverse another node. The compute nodes may be linked in a three dimensional torus as shown in but may also be configured to have more or fewer dimensions. Also it is not necessarily the case that a given node s neighbors are the physically closest nodes to the given node although it is generally desirable to arrange the nodes in such a manner insofar as possible.

In one embodiment the compute nodes in any one of the x y or z dimensions form a torus in that dimension because the point to point communication links logically wrap around. For example this is represented in by links D E and F which wrap around from a last node in the x y and z dimensions to a first node. Thus although node appears to be at a corner of the torus node to node links A F link node to nodes D E and F in the x y and z dimensions of torus .

As shown I O node includes processor cores A and B and also includes memory used by both processor cores an external control interface which is coupled to local hardware control network an external data communications interface which is coupled to the corresponding local I O collective network and the corresponding six node to node links of the torus network and monitoring and control logic which receives and responds to control commands received through external control interface . Monitoring and control logic may access processor cores and locations in memory on behalf of control subsystem to read or in some cases alter the operational state of I O node .

In a Blue Gene system the external data interface may transmit message passing interface MPI messages by encapsulating such messages within packets and transmitting the packets of over a network e.g. a tree or torus network . For example the MPI call of MPI Send may be used to transmit a message and the call of MPI Bcast may be used to broadcast the message. Other parallel systems and other parallel computing libraries may include similar mechanisms. Illustratively the external data interface includes an ALU which may be similar to the ALUs discussed above with respect to . That is ALU may be configured to modify during a broadcast operation entropy data being broadcasted. ALU may generally be capable of performing arithmetic and logic operations on packets traversing the external data interface . More specifically ALU may perform operations e.g. an XOR operation against some value to pseudorandomly modify entropy data in packets being broadcast to compute nodes not shown thereby mixing and improving the entropy data.

As shown in memory stores an operating system image an application code image and user application data structures as required. Some portion of memory may be allocated as a file cache i.e. a cache of data read from or to be written to an I O file. Operating system image provides a copy of a simplified function operating system running on I O node . Operating system image may include a minimal set of functions required to support operation of the I O node . As shown the memory also contains entropy pool . In some embodiments the entropy pool may be similar to the entropy pools of . As discussed with reference to the entropy pool is generally configured to store entropy pool elements received from an internal or external entropy source which as discussed may be a service node a hardware random number generator correctible error events and the like. The pool elements may then be distributed to a random number generator for use in creating pseudorandom numbers and distributed via broadcast operations to compute nodes. As discussed the ALU may perform arithmetic and or logic operations to modify entropy data during such broadcast operations so that relatively small inputs of entropy data from the internal or external source may provide abundant entropy data for all nodes in the system.

Although discussed in with respect to an I O node compute nodes of the parallel computing system may also have processors memories external data interfaces etc. Further the external data interfaces of the compute nodes may each include an ALU which similar to ALU performs arithmetic and or logic operations to modify entropy data during broadcast operations. Doing so mixes and thus improves the entropy data with every hop of the packet from one node to another.

At step the I O node broadcasts entropy pool elements to compute nodes in a tree network. For example in a Blue Gene system the MPI call of MPI Beast may be used to broadcast messages including such entropy pool elements over the tree network. Other parallel systems and other parallel computing libraries may use similar mechanisms for broadcasting messages. In one embodiment the I O node may periodically broadcast entropy pool elements such that each of the compute nodes has sufficient entropy data. As discussed dedicated packets may be used for such broadcasts. Alternatively unused space within normal network packets may be used.

At step network ALUs pseudorandomly modify the entropy data as it traverses the tree network. As discussed an ALU of the I O node may initially perform arithmetic and or logic operation s to pseudorandomly modify entropy data leaving the I O node. ALUs of compute nodes may further modify the entropy data during each hop of the entropy data packets from one node to another thereby mixing and thus improving the data during the broadcast. For example the network ALUs may modify received entropy data packets before depositing those packets in a local entropy pool and also forward those modified packets to other compute nodes. The network ALUs may use any feasible arithmetic and or logic operation s to modify the broadcast entropy data. For example each network ALU may perform an XOR of the entropy data against another value such as a rank or location of the node of the ALU a direction from which the entropy data arrived other entropy data and the like. Of course other arithmetic and or logic operations e.g. addition AND OR etc. against other values may be performed to modify the entropy data. In one embodiment the particular arithmetic or logic operation which is applied may also be determined based on e.g. the node rank or another value.

At step the entropy data as modified by network ALUs during the broadcast operation is stored at the compute nodes. For example after modification by an ALU of a compute node received entropy data which may also have been modified during previous hops may be stored as elements of an entropy pool located in a memory of the compute node. Of course the entropy data may also be stored elsewhere such as in registers in the ALU in a storage device etc. Applications which need entropy data may then fetch e.g. by invoking an application programming interface API the stored entropy data for use. For example a random number generator may fetch entropy data to use as initial values in producing pseudorandom numbers.

Although discussed above primarily with respect to broadcasts over a tree network by an I O node similar techniques may be used to modify entropy data packets broadcast over networks having other topologies and other nodes may perform the broadcast. Further in some embodiments network ALUs may modify entropy during operations other than broadcast operations. In one embodiment entropy data may bounce around between nodes endlessly being modified by network ALUs during the bounces. In another embodiment applications on compute nodes may invoke an API to request entropy data from the I O node. In such a case network ALUs may pseudorandomly modify entropy data transmitted by the I O node in response to the request as the entropy data traverses the network. In addition although discussed above primarily with respect to network ALUs ALUs present elsewhere may also modify entropy data according to techniques disclosed herein.

Advantageously embodiments disclosed herein modify entropy data broadcast over a parallel computer network to provide distinct entropy data to computing node within the computer network. Doing so improves the quality of the entropy data traversing nodes of the parallel computer and also ensures that the entropy data on each of the nodes is different. Further relatively small inputs of original entropy data from an internal or external source may provide abundant entropy data for all nodes in the system. In one embodiment network ALUs perform the pseudorandom modifications thereby freeing the kernels and programs of the parallel computer nodes from noise jitter due to having to manage entropy. Using entropy data as improved by techniques disclosed herein random number generators of the compute nodes may have higher entropy properties such that the random number generators have a more equal probability of outputting any given value from a set of predefined values.

While the foregoing is directed to embodiments of the present disclosure other and further embodiments of the disclosure may be devised without departing from the basic scope thereof and the scope thereof is determined by the claims that follow.

