---

title: Graphics processing unit with a texture return buffer and a texture queue
abstract: A processor and a system are provided for performing texturing operations loaded from a texture queue that provides temporary storage of texture coordinates and texture values. The processor includes a texture queue implemented in a memory of the processor, a crossbar coupled to the texture queue, and one or more texture units coupled to the texture queue via the crossbar. The crossbar is configured to reorder texture coordinates for consumption by the one or more texture units and to reorder texture values received from the one or more texture units.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09171525&OS=09171525&RS=09171525
owner: NVIDIA Corporation
number: 09171525
owner_city: Santa Clara
owner_country: US
publication_date: 20130226
---
The present invention relates to computer graphics and more particularly to texture operations in graphics processing.

One of the fundamental operations of graphics processing units GPUs is texturing. A texture map is a source array of color values i.e. texels that may be mapped to a surface of a graphics object. For each pixel in a digital image one or more texels in the texture map are sampled and filtered to produce a color value for the pixel. Texturing may be used to generate more realistic computer generated images of a three dimensional model.

Sampling the texture map typically requires texel values to be fetched from memory. The memory operations may introduce latency into the texture operation slowing down the graphics processing pipeline. Thus there is a need for addressing this issue and or other issues associated with the prior art.

A processor and a system are provided for performing texturing operations loaded from a texture queue that provides temporary storage of texture coordinates and texture values. The processor includes a texture queue implemented in a memory of the processor a crossbar coupled to the texture queue and one or more texture units coupled to the texture queue via the crossbar. The crossbar is configured to reorder texture coordinates for consumption by the one or more texture units and to reorder texture values received from the one or more texture units.

In one embodiment the PPU includes an input output I O unit configured to transmit and receive communications i.e. commands data etc. from a central processing unit CPU not shown over the system bus . The I O unit may implement a Peripheral Component Interconnect Express PCIe interface for communications over a PCIe bus. In alternative embodiments the I O unit may implement other types of well known bus interfaces.

The PPU also includes a host interface unit that decodes the commands and transmits the commands to the grid management unit or other units of the PPU e.g. memory interface as the commands may specify. The host interface unit is configured to route communications between and among the various logical units of the PPU .

In one embodiment a program encoded as a command stream is written to a buffer by the CPU. The buffer is a region in memory e.g. memory or system memory that is accessible i.e. read write by both the CPU and the PPU . The CPU writes the command stream to the buffer and then transmits a pointer to the start of the command stream to the PPU . The host interface unit provides the grid management unit GMU with pointers to one or more streams. The GMU selects one or more streams and is configured to organize the selected streams as a pool of pending grids. The pool of pending grids may include new grids that have not yet been selected for execution and grids that have been partially executed and have been suspended.

A work distribution unit that is coupled between the GMU and the SMs manages a pool of active grids selecting and dispatching active grids for execution by the SMs . Pending grids are transferred to the active grid pool by the GMU when a pending grid is eligible to execute i.e. has no unresolved data dependencies. An active grid is transferred to the pending pool when execution of the active grid is blocked by a dependency. When execution of a grid is completed the grid is removed from the active grid pool by the work distribution unit . In addition to receiving grids from the host interface unit and the work distribution unit the GMU also receives grids that are dynamically generated by the SMs during execution of a grid. These dynamically generated grids join the other pending grids in the pending grid pool.

In one embodiment the CPU executes a driver kernel that implements an application programming interface API that enables one or more applications executing on the CPU to schedule operations for execution on the PPU . An application may include instructions i.e. API calls that cause the driver kernel to generate one or more grids for execution. In one embodiment the PPU implements a SIMD Single Instruction Multiple Data architecture where each thread block i.e. warp in a grid is concurrently executed on a different data set by different threads in the thread block. The driver kernel defines thread blocks that are comprised of k related threads such that threads in the same thread block may exchange data through shared memory. In one embodiment a thread block comprises 32 related threads and a grid is an array of one or more thread blocks that execute the same stream and the different thread blocks may exchange data through global memory.

In one embodiment the PPU comprises X SMs X . For example the PPU may include 15 distinct SMs . Each SM is multi threaded and configured to execute a plurality of threads e.g. 32 threads from a particular thread block concurrently. Each of the SMs is connected to a level two L2 cache via a crossbar or other type of interconnect network . The L2 cache is connected to one or more memory interfaces . Memory interfaces implement 16 32 64 128 bit data buses or the like for high speed data transfer. In one embodiment the PPU comprises U memory interfaces U where each memory interface U is connected to a corresponding memory device U . For example PPU may be connected to up to 6 memory devices such as graphics double data rate version 5 synchronous dynamic random access memory GDDR5 SDRAM .

In one embodiment the PPU implements a multi level memory hierarchy. The memory is located off chip in SDRAM coupled to the PPU . Data from the memory may be fetched and stored in the L2 cache which is located on chip and is shared between the various SMs . In one embodiment each of the SMs also implements an L1 cache. The L1 cache is private memory that is dedicated to a particular SM . Each of the L1 caches is coupled to the shared L2 cache . Data from the L2 cache may be fetched and stored in each of the L1 caches for processing in the functional units of the SMs .

In one embodiment the PPU comprises a graphics processing unit GPU . The PPU is configured to receive commands that specify shader programs for processing graphics data. Graphics data may be defined as a set of primitives such as points lines triangles quads triangle strips and the like. Typically a primitive includes data that specifies a number of vertices for the primitive e.g. in a model space coordinate system as well as attributes associated with each vertex of the primitive. The PPU can be configured to process the graphics primitives to generate a frame buffer i.e. pixel data for each of the pixels of the display . The driver kernel implements a graphics processing pipeline such as the graphics processing pipeline defined by the OpenGL API.

An application writes model data for a scene i.e. a collection of vertices and attributes to memory. The model data defines each of the objects that may be visible on a display. The application then makes an API call to the driver kernel that requests the model data to be rendered and displayed. The driver kernel reads the model data and writes commands to the buffer to perform one or more operations to process the model data. The commands may encode different shader programs including one or more of a vertex shader hull shader geometry shader pixel shader etc. For example the GMU may configure one or more SMs to execute a vertex shader program that processes a number of vertices defined by the model data. In one embodiment the GMU may configure different SMs to execute different shader programs concurrently. For example a first subset of SMs may be configured to execute a vertex shader program while a second subset of SMs may be configured to execute a pixel shader program. The first subset of SMs processes vertex data to produce processed vertex data and writes the processed vertex data to the L2 cache and or the memory . After the processed vertex data is rasterized i.e. transformed from three dimensional data into two dimensional data in screen space to produce fragment data the second subset of SMs executes a pixel shader to produce processed fragment data which is then blended with other processed fragment data and written to the frame buffer in memory . The vertex shader program and pixel shader program may execute concurrently processing different data from the same scene in a pipelined fashion until all of the model data for the scene has been rendered to the frame buffer. Then the contents of the frame buffer are transmitted to a display controller for display on a display device.

The PPU may be included in a desktop computer a laptop computer a tablet computer a smart phone e.g. a wireless hand held device personal digital assistant PDA a digital camera a hand held electronic device and the like. In one embodiment the PPU is embodied on a single semiconductor substrate. In another embodiment the PPU is included in a system on a chip SoC along with one or more other logic units such as a reduced instruction set computer RISC CPU a memory management unit MMU a digital to analog converter DAC and the like.

In one embodiment the PPU may be included on a graphics card that includes one or more memory devices such as GDDR5 SDRAM. The graphics card may be configured to interface with a PCIe slot on a motherboard of a desktop computer that includes e.g. a northbridge chipset and a southbridge chipset. In yet another embodiment the PPU may be an integrated graphics processing unit iGPU included in the chipset i.e. Northbridge of the motherboard.

As described above the work distribution unit dispatches active grids for execution on one or more SMs of the PPU . The scheduler unit receives the grids from the work distribution unit and manages instruction scheduling for one or more thread blocks of each active grid. The scheduler unit schedules threads for execution in groups of parallel threads where each group is called a warp. In one embodiment each warp includes 32 threads. The scheduler unit may manage a plurality of different thread blocks allocating the thread blocks to warps for execution and then scheduling instructions from the plurality of different warps on the various functional units i.e. cores DPUs SFUs and LSUs during each clock cycle.

In one embodiment each scheduler unit includes one or more instruction dispatch units . Each dispatch unit is configured to transmit instructions to one or more of the functional units. In the embodiment shown in the scheduler unit includes two dispatch units that enable two different instructions from the same warp to be dispatched during each clock cycle. In alternative embodiments each scheduler unit may include a single dispatch unit or additional dispatch units .

Each SM includes a register file that provides a set of registers for the functional units of the SM . In one embodiment the register file is divided between each of the functional units such that each functional unit is allocated a dedicated portion of the register file . In another embodiment the register file is divided between the different warps being executed by the SM . The register file provides temporary storage for operands connected to the data paths of the functional units.

Each SM comprises L processing cores . In one embodiment the SM includes a large number e.g. 192 etc. of distinct processing cores . Each core is a fully pipelined single precision processing unit that includes a floating point arithmetic logic unit and an integer arithmetic logic unit. In one embodiment the floating point arithmetic logic units implement the IEEE 754 2008 standard for floating point arithmetic. Each SM also comprises M DPUs that implement double precision floating point arithmetic N SFUs that perform special functions e.g. copy rectangle pixel blending operations and the like and P LSUs that implement load and store operations between the shared memory L1 cache and the register file . In one embodiment the SM includes 64 DPUs 32 SFUs and 32 LSUs .

Each SM includes an interconnect network that connects each of the functional units to the register file and the shared memory L1 cache . In one embodiment the interconnect network is a crossbar that can be configured to connect any of the functional units to any of the registers in the register file or the memory locations in shared memory L1 cache .

In one embodiment the SM is implemented within a GPU. In such an embodiment the SM comprises J texture units . The texture units are configured to load texture maps i.e. a 2D array of texels from the memory and sample the texture maps to produce sampled texture values for use in shader programs. The texture units implement texture operations such as anti aliasing operations using mip maps i.e. texture maps of varying levels of detail . In one embodiment the SM includes 16 texture units .

The PPU described above may be configured to perform highly parallel computations much faster than conventional CPUs. Parallel computing has advantages in graphics processing data compression biometrics stream processing algorithms and the like.

More illustrative information will now be set forth regarding various optional architectures and features with which the foregoing framework may or may not be implemented per the desires of the user. It should be strongly noted that the following information is set forth for illustrative purposes and should not be construed as limiting in any manner. Any of the following features may be optionally incorporated with or without the exclusion of other features described.

Modern GPUs support programmable shading which allows various shader programs to be configured to run on a large number of functional units i.e. cores DPUs SFUs and LSUs . GPUs typically have large register files to support a large number of hardware contexts. A hardware context comprises a set of registers for the shader program to read and write values related to the shader program as well as other registers and or memory locations to hold information about the primitive which the instance of the shader program is acting upon.

Shader programs can contain texture operations. A texture operation typically samples a texture map using texture coordinates e.g. s t etc. to generate a final texture value for a fragment. Texture operations typically generate many accesses to off chip memory which are associated with significant latency. A texture map is an array of values that may be mapped to a fragment. For example a texture map may contain a 2D array of color values that can be used to map a 2D image to a 3D surface of the primitive. The texture coordinates specify a point within the array from which a sample may be generated. Each texture operation writes a final texture value into one or more registers for the hardware context associated with the thread that generated the texture operation. The number of registers consumed by a single texture operation varies according to which type of texture operation the shader program implements and what type of texture map was accessed by the texture operation. Because shader programs are dependent on the values returned by the texture operations to continue executing the shader programs are often stalled while waiting on long latency memory access operations to complete.

Two techniques are used to reduce the time during which the execution units are idle. First a compiler implemented by the driver kernel performs an optimization similar to load hoisting which moves the texture operations as early in the shader program as possible. In addition the compiler attempts to arrange texture operations in a parallel manner. It will be appreciated that both of these optimizations increase the number of registers needed by the shader program because each of the parallel texture operations requires a set of registers to store return values and performing texture operations earlier in the shader program requires the registers to be allocated earlier in time such that additional registers are required for intervening operations unrelated to the texture operation. Second the number of hardware contexts per execution unit is increased to enable context switching between several different hardware contexts. When a first hardware context is idled while waiting for a texture operation to complete a different hardware context may be executed. Both of these techniques require additional registers for each execution unit which increases the size of the GPU or reduces the number of execution units that can be placed on a die of a particular size.

As described above instances of a shader program are instantiated as groups of threads called thread blocks or warps. The warp comprises a number of parallel threads executing on different functional units of the SM . Each thread in a warp executes the instructions in the shader program on different input data such as the vertices of a number of primitives. For example a shader program may include a load LD instruction followed by a multiply MUL instruction. The scheduler unit dispatches the LD instruction for a warp to a number of the LSUs which load a value from the shared memory L1 cache into the register file . Once the value is loaded into the register file the scheduler unit dispatches the MUL instruction to a number of cores . For example if the size of a warp is 32 threads then the scheduler unit may dispatch the LD instruction to 32 LSUs during a first clock cycle and then dispatch the MUL instruction to 32 cores during a subsequent clock cycle. The 32 LSUs will load 32 values into 32 different registers of the register file . The 32 cores then consume the 32 values to produce 32 results that are stored into another 32 registers of the register file .

Texture operations are processed by one or more of the functional units of the SM . For example a shader program may include one or more LD instructions that load texture coordinates into registers of the register file one or more arithmetic instructions e.g. MUL ADD etc. that may transform the texture coordinates and a texture TEX instruction that samples a texture map to generate a final textured value based on the texture coordinates. The scheduler unit dispatches the one or more LD instructions to a set of LSUs to retrieve the texture coordinates from shared memory L1 cache dispatches the one or more arithmetic instructions to a set of cores to generate transformed texture coordinates and dispatches the TEX instruction to a set of texture units to generate final texture values. The cores read the texture coordinates from the register file and optionally may transform the texture coordinates to generate transformed texture coordinates which are stored in the register file . Then the texture units read the texture coordinates or transformed texture coordinates from the register file and generate one or more physical addresses that identify locations within the texture map to sample to generate one or more sampled values of the texture map. The one or more sampled values may then be processed by the TFU to generate a final texture value.

The TAU reads the texture coordinates from registers in the register file associated with the hardware context that originated the TEX instruction. As shown in a first texture operation received by the texture unit is originated by a warp associated with a first hardware context i.e. Context1 and a second texture operation received by the texture unit is originated by a warp associated with a second hardware context i.e. Context7 . Texture unit receives the first texture operation and reads the texture coordinates from registers associated with the first hardware context i.e. Context1 . The TAU generates the one or more physical addresses for the first texture operation transmits one or more memory read requests to the memory subsystem and adds the first texture operation to the texture latency FIFO . The texture unit subsequently receives the second texture operation and reads the texture coordinates from registers associated with the second hardware context i.e. Context7 . The TAU generates the one or more physical addresses for the second texture operation transmits one or more memory read requests to the memory subsystem and adds the second texture operation to the texture latency FIFO . Once the sampled values for the first texture operation have been returned by the memory subsystem the TFU pops the first texture operation from the texture latency FIFO and generates a final texture value which is stored in registers in the register file associated with the first hardware context i.e. Context1 . Once the sampled values for the second texture operation have been returned by the memory subsystem the TFU pops the second texture operation from the texture latency FIFO and generates a final texture value which is stored in registers in the register file associated with the second hardware context i.e. Context7 .

Because the compiler cannot know when the final texture value will be generated by the texture unit one or more registers are allocated to store the final texture value when the TEX instruction is transmitted to the texture unit . The addresses for these registers are then passed to the texture unit or a texture interface unit so that the TFU knows where to store the final values when the texture operation is complete. It will be appreciated that the number of registers that are allocated for an instance of the shader program may become quite large especially when the shader program implements a number of texture operations in parallel.

One hardware organization utilizes a different number of cores configured to process instructions from a warp than the number of texture units configured to process instructions from a warp. For example 16 cores may be configured to process a MUL instruction from a particular warp with half of the threads of the warp executing in parallel during a first clock cycle and the other half of the threads of the warp executing in parallel during a second clock cycle. However 8 texture units may be configured to process a TEX instruction from a warp with each texture unit generating texture values for four threads of the warp. Because a warp may include a different number of threads than texture units configured to process the TEX instruction for a warp the texture operation may be broken up into a set of texture operations with each texture operation from the set of texture operations configured to generate final texture values for a different subset of threads in the warp.

As shown in an input buffer and an output buffer may be coupled to one or more texture units to perform swizzling operations. A swizzling operation is an operation that reorders the components of an array. For example a warp may include a TEX instruction that is executed for 32 parallel threads. In this example the texture coordinates are stored in groups of 32 values for each texture coordinate which corresponds to the size of the warp. In other words the set of texture units configured to process a texture operation would receive 32 s coordinates followed by 32 t coordinates and so forth. However the number of texture units configured to perform a texture operation for a warp may be different than 32. Thus the input buffer I Buf receives the texture coordinates and reorders the texture coordinates grouping a first subset of the s coordinates with a corresponding first subset of the t coordinates for a first texture operation grouping a second subset of the s coordinates with a corresponding second subset of the t coordinates for a second texture operation and so forth. The output buffer O Buf performs a similar operation in reverse i.e. unswizzling which buffers a first subset of final texture values a second subset of final texture values and so forth to generate a set of final texture values that corresponds to the width of a warp e.g. 32 final texture values so that the final texture values can be consumed in parallel by the set of cores in a subsequent instruction of the warp. The input buffer and the output buffer decouple the number of texture units which perform a parallel texture operation from the number of cores that generate the texture coordinates or consume the final texture values.

The benefit of the TRB is that entries are allocated and deallocated when the final texture values are produced and consumed. This hardware organization enables a smaller register file to provide the same performance as larger register files associated with the hardware organization set forth in . Furthermore decoupling the TRB from the texture unit enables the TFU to continue to generate additional final texture values for subsequent texture operations while the preceding final texture values are being consumed.

In one embodiment an instruction set of the SM is expanded to include a new type of identifier for texture values. Texture identifiers are handles i.e. an unsigned integer that are associated with the output of a texture operation. With respect to the instructions texture identifiers are similar to normal registers but texture identifiers can only be used as input operands for all instructions except texture instructions and can only be used as output operands for texture instructions. However texture identifiers are different from normal registers in that only texture operations can use the texture identifiers as output operands. When a texture operation is initiated by a hardware context the texture identifier is transmitted to the texture unit and passed to the TFU in the texture latency FIFO . When the TFU generates a final texture value the value is stored in a slot of the TRB and the address of the slot is associated with the texture identifier.

In one embodiment the TRB is implemented in a portion of the register file . For example a 1 KB portion of registers in the register file may be allocated to store entries in the TRB . In one embodiment the size of the TRB may be changed dynamically. Between different shader programs the driver kernel can adjust the allocation of the register file to change the capacity of the TRB . For example some shader programs may generate a large number of texture operations that may benefit from a larger TRB while other shader programs may generate fewer texture operations that benefit from a larger number of registers allocated to each hardware context. Allocating registers from the register file to implement the TRB does not require an explicit buffer to be designed into the SM and takes advantage of storage resources that are already available in a conventional processor design. In another embodiment the TRB may be allocated as a part of shared memory L1 cache .

Storing final texture values in the TRB may be more efficient than storing texture values directly to the hardware contexts of the register files. However care should be taken that the TRB is efficiently drained by the active warps executing within the SMs . In one embodiment a wake up signal may be sent to a scheduler such as scheduler unit when a texture value is generated and stored in the TRB that indicates that the warp that sent the texture request associated with that texture value should be woken up as soon as possible to consume the texture value. Efficient scheduling can alleviate the problem of the TRB filling up and causing the texture unit to idle.

In one embodiment the texture identifier is passed to the texture unit as a part of the texture operation. The texture unit tracks the texture identifier throughout the texture operation and when the final texture value is written to the TRB an entry is added to the TIM table which indicates that the final texture value is ready to be consumed by the thread that generated the texture operation. In another embodiment the texture unit may transmit a signal to the scheduler unit to indicate that the final texture value is ready to be consumed.

In one embodiment an instruction that reads a value in the TRB includes a last use bit that is set in the instruction to indicate that the shader program will no longer access the final texture value in the TRB . When the last use bit is set the entry in the TIM table will be invalidated i.e. removed indicating that the slot in the TRB can be deallocated and used for the next texture operation. Another table not shown may be used to track the free i.e. deallocated entries of the TRB . A TRB free list table is a queue which holds all of the entry identifiers for the slots of the TRB which are not currently associated with a texture value. In other words when the TFU generates a new final texture value an entry identifier may be removed from the TRB free list table and allocated to that texture operation. If the TRB free list table is empty then the TFU stalls until an entry has been deallocated due to consumption of a final texture value by a currently executing shader program.

In one embodiment a spill buffer may be allocated in memory to avoid deadlock conditions when the TRB is full. In such an embodiment additional slots of the TRB may be allocated in the spill buffer in memory and loaded to the TRB as the texture identifiers associated with texture values stored in the spill buffer are accessed. The implementation of the spill buffer prevents the TRB from stalling the texture unit because there are no free entries available in the TRB .

A pixel tile is a two dimensional array of pixels associated with an image such as a 16 pixel by 16 pixel array. In different embodiments pixel tiles may be different sizes e.g. 8 8 16 16 8 16 32 32 etc. per the desires of the user. A pixel tile may be covered fully or partially by some number of graphics primitives i.e. triangles triangle strips etc. . The one or more texture operations may be implemented for each of the graphics primitives that covers a particular pixel tile. In other words a batch of texture operations is executed for the covered quads in each pixel tile of an image. One or more warps may be generated that correspond to the covered quads of a pixel tile. The warps are executed by the PPU .

A batch of texture operations includes one or more texture instructions with each texture instruction including one or more texture coordinates as operands. For example a batch of texture operations may comprise a first texture instruction i.e. TEX s t u v having four texture coordinates as operands and a second texture instruction i.e. TEX s t u v having four texture coordinates as operands. In order to execute the batch of texture operations the texture coordinates associated with the batch of texture operations are stored in the texture queue before being transmitted to the texture units for processing. As shown in in one embodiment texture coordinates for a plurality of quads are stored in the texture queue . The particular arrangement of texture coordinates within the texture queue does not necessarily match the order that texture coordinates are transmitted to the texture units as will be discussed more fully below. The number of quads stored in the texture queue is dependent on the size of a pixel tile for a particular batch of texture operations.

A write crossbar and a read crossbar which are included in the interconnect network of SM are coupled to the shared memory L1 cache and may be configured to connect the texture queue to other units within the SM . The write crossbar and the read crossbar may have a width of arbitrary size and the number of texture coordinates that may be written to or read from the texture queue in a single clock cycle is dependent on the widths of the write crossbar and the read crossbar . Although shown as separate and distinct units in the write crossbar and the read crossbar may be considered as a single unit having separate circuitry that functions as the separate and distinct units described herein. In yet another embodiment a single crossbar may be implemented that may be configured to perform the functions of either the write crossbar or the read crossbar as required.

It will be appreciated that only one texture coordinate may be written to or read from each memory bank during a given clock cycle. In one embodiment the write crossbar and the read crossbar have a width of 1024 bits such that one value from each of the 32 memory banks may be written or read during a given clock cycle. In other embodiments the widths of the write crossbar and the read crossbar may be some other value including but not limited to 128 256 or 512 bits in width. It will be appreciated that in some embodiments multiple values may be stored in one slot of a memory bank e.g. two 16 bit values may be stored in one 32 bit slot . In such embodiments more than one value may be read from each memory bank per clock cycle. In yet other embodiments the width of a memory bank may be greater than or less than 32 bits such as 16 bits or 64 bits and one or more values may be read from each memory bank per clock cycle.

In one embodiment a texture interface buffer may be included within the SM as an interface between the texture units and the texture queue . The texture interface buffer provides a small buffer e.g. 512 bytes for properly ordering texture coordinates for transmission to the texture units . A portion of the texture coordinates may be loaded from the texture queue into the slots of the texture interface buffer via the read crossbar . The texture interface buffer enables all of the data for a texture operation to be loaded from memory into the texture units in a single operation. Alternatively the texture units could receive the data for a texture operation over multiple cycles using multiple memory operations. However scheduling multiple memory operations may be more complicated and tie up the memory unit over multiple clock cycles thereby preventing the memory unit from processing other memory requests. For example if the transfer of texture coordinates from the memory to the texture interface buffer uses only some of the memory banks and other types of memory access requests are being interleaved between memory access requests for the texture coordinates then scheduling memory requests transmitted to the memory is more complicated. In other embodiments the texture interface buffer may include memory sufficient to store texture coordinates for two or more texture operations. Thus one set of texture coordinates may be transmitted to the texture units while one or more additional sets of texture coordinates are stored in and possibly being drained from the texture interface buffer .

In one embodiment the texture units may have an input interface that is 512 bits wide which routes up to 16 texture coordinates for one quad to the texture pipeline i.e. the TAU the texture latency FIFO and the TFU in the texture units to generate four texture values for the quad. The texture interface buffer enables a subset of the texture coordinates within the texture queue to be grouped and ordered according to the configuration of the input interface of the texture unit . The texture queue in conjunction with the texture interface buffer eliminates the need for the input buffer of for performing swizzling operations. Even if the input buffer is not eliminated completely the texture queue enables the input buffer to be greatly reduced in size and circuit complexity.

In some embodiments the texture interface buffer is not included within an SM and the texture units are configured to drain texture coordinates directly from the texture queue via the read crossbar . In such embodiments care should be taken that each of the texture coordinates for a given texture operation are stored in different memory banks of the texture queue . If two texture coordinates for a single texture operation are stored in the same memory bank then it could be impossible to read out those texture values in a minimum number of clock cycles decreasing the efficiency of the texture operation.

In one embodiment a flag is set when each of the texture coordinates for a batch of texture operations has been stored in the texture queue . The flag indicates when the texture coordinates are ready to be drained to the texture units and processed to generate texture values. Because texture coordinates are not drained from the texture queue until the entire batch has been stored the order that texture coordinates are stored in the texture queue is irrelevant. However the order that texture coordinates are drained from the texture queue is important because the texture values written back to the texture queue in order corresponds to the order of the texture coordinates drained from the texture queue . In another embodiment additional state information may track which texture coordinates from the batch of texture operations have been loaded into the texture queue . The state information enables partial draining of the texture coordinates to the texture units to generate texture values while the remaining texture coordinates are stored in the texture queue . Texture values generated by the texture units are stored in locations in the texture queue that correspond to but are not necessarily the same as the storage locations for the texture coordinates drained from the texture queue to produce the texture values.

The operation of the texture queue is described as follows. The texture queue stores texture coordinates for a batch of texture operations for a pixel tile. In order to process a batch of texture operations for a particular pixel tile the scheduler unit reserves a space in the texture queue to store the texture coordinates associated with the batch. The space comprises one or more slots of memory within the texture queue that store the texture coordinates for the batch of texture operations. As used herein a slot of memory may be a plurality of bits spread across a number of memory banks e.g. 1024 bits spread across 32 memory banks . As shown in a first s coordinate s may be stored in a first slot of the texture queue a first t coordinate t may be stored in a second slot of the texture queue and so forth.

In one embodiment the scheduler unit transmits commands to the LSUs that cause the LSUs to store the texture coordinates e.g. s t u v s t u and v for a plurality of quads in the space reserved in the texture queue . Once all of the texture coordinates for the batch of texture operations for a pixel tile have been stored in the texture queue the batch of texture operations may be flagged as ready. In one embodiment a register for a hardware context associated with the batch of texture operations may include one or more bits that indicate that the batch of texture operations is ready to be transmitted to the texture units . The scheduler unit then transmits commands to the texture units to drain the texture coordinates from the texture queue . Once all of the texture coordinates have been drained from the texture queue for processing by the texture units the space reserved for the texture coordinates may be released by the scheduler unit and used for another batch of texture operations.

The texture units drain the texture coordinates from the texture queue and process the texture coordinates to generate a plurality of texture values. The scheduler unit may reserve another space in the texture queue for storing the plurality of texture values. The output of the texture units is then stored in the other reserved space described more fully below in conjunction with . In some embodiments two distinct texture queues may be implemented in an SM a first texture queue dedicated to storing texture coordinates for consumption by the texture units and a second texture queue dedicated to storing texture values generated by the texture units . Descriptions for the structure and operation of a single texture queue are equally applicable to a dual texture queue implementation with the operations and structure relating to texture coordinates associated with the first texture queue and the operations and structure relating to texture values associated with the second texture queue. It will be appreciated that implementations with two separate and distinct texture queues are technically equivalent to implementations having a single texture queue with enough memory to store both texture coordinates and texture values simultaneously i.e. a first portion of memory for storing texture coordinates for one batch of texture operations and a second portion of memory for storing texture values for the batch of texture operations .

When all of the texture values for the batch of texture operations have been stored in the texture queue the texture values for the batch of texture operations may be flagged as ready to be consumed by the threads of the warps for the pixel tile. The scheduler unit may transmit commands included in the shader program that originated the texture operations to the LSUs to load the texture values from the texture queue as needed. Once all of the texture values for the batch of texture operations have been consumed the space reserved for the texture values may be released and used for another batch of texture operations.

It will be appreciated that more than one space may be reserved within the texture queue for texture coordinates associated with two or more batches of texture operations for one or more pixel tiles at any one time. The number of texture operations in a batch may be specified within instructions in a shader program. The scheduler unit tracks how many warps are allocated to a particular pixel tile and can schedule texture operations for each batch of texture operations based on the information in the instructions of the shader program. For example the scheduler unit may reserve a first space within the texture queue for a first batch of texture operations. Before all of the texture coordinates have been stored in the first space the scheduler unit may reserve a second space within the texture queue for a second batch of texture operations. Similarly more than one space within the texture queue may be reserved to store texture values associated with two or more batches of texture operations for one or more pixel tiles. Storing texture coordinates into and consuming texture values from the texture queue may be performed in order i.e. in first in first out order or out of order per the desires of the user.

In another embodiment as shown in texture coordinates may be drained from the texture queue according to a QuadTex priority mode. In the QuadTex priority mode the texture units are configured to drain texture coordinates for each of the texture operations in the batch of texture operations in order for a first quad. Then the texture units are configured to drain texture coordinates for each of the texture operations in order for a second quad and so forth until all of the texture coordinates associated with the batch of texture operations have been drained from the texture queue . In other words the texture coordinates for each of the quads of the pixel tile i.e. Q Q Q Q and so forth are loaded into the texture interface buffer and transmitted to the texture units in order to generate texture values. It will be appreciated that as many quads as will fit in the texture interface buffer may be loaded into the texture interface buffer in parallel and then the quads in the texture interface buffer may be loaded serially into the texture units . The QuadTex priority mode increases the efficiency of texture operations by maximizing texture cache locality for each quad when multiple texture operations reference the same texture map. The QuadTex priority mode may increase efficiency in certain operations such as calculating soft shadows.

Texture coordinates for the multiple batches of texture operations may be drained in order from the texture queue according to the TexTile priority mode. First texture coordinates for the first batch of texture operations may be drained from the texture queue . The texture coordinates for a first texture operation i.e. s t for a plurality of quads e.g. Q Q Q and Q are loaded into the texture interface buffer and transmitted to the texture units to generate texture values. Then the texture coordinates for the first texture operation for other quads of the pixel tile e.g. Q Q Q and Q are loaded into the texture interface buffer and transmitted to the texture units to generate texture values. Once all of the texture coordinates for the first texture operation have been transmitted to the texture units the texture coordinates for the second texture operation for each of the quads of the pixel tile are loaded into the texture interface buffer and transmitted to the texture units . Once texture coordinates from the first batch of texture operations have been processed by the texture units texture coordinates from the second batch of texture operation may be drained from the texture queue . Note that in one embodiment the first batch and the second batch may be associated with different pixel tiles i.e. the first batch may be associated with a first pixel tile and the second batch may be associated with a second pixel tile . In one embodiment texture coordinates from the first batch and the second batch of texture operations may be drained from the texture queue out of order i.e. the second batch may be drained before the first batch or in parallel i.e. a portion of the texture coordinates from the first batch is drained and then a portion of the texture coordinates from the second batch is drained or texture coordinates from both the first batch and the second batch are drained simultaneously and transmitted to different texture units .

In another embodiment as shown in texture coordinates may be drained from the texture queue according to the QuadTex priority mode. In the QuadTex priority mode the texture coordinates for the texture operations in the first batch of texture operations for a first quad Q are loaded into the texture interface buffer and transmitted to the texture units . Then the texture coordinates for the texture operations in the first batch of texture operations for a second quad Q are loaded into the texture interface buffer and transmitted to the texture units and so forth until all of the texture coordinates associated with the first batch of texture operations have been transmitted to the texture units . Again it will be appreciated that as many quads as will fit in the texture interface buffer may be loaded into the texture interface buffer in parallel and then drained to the texture units in order. Then texture coordinates associated with a second batch of texture operations are loaded into the texture interface buffer and transmitted to the texture units in order. Again the embodiments illustrated by assume that the texture operations are associated with two texture coordinates.

In one embodiment as shown in texture coordinates may be drained from the texture queue according to the TexTile priority mode. The texture coordinates for a first texture operation i.e. TEX s for a plurality of quads e.g. Q Q Q Q Q Q Q and Q are loaded into the texture interface buffer and transmitted to the texture units to generate texture values. Then the texture coordinates for a second texture operation i.e. TEX s for the plurality of quads are loaded into the texture interface buffer and transmitted to the texture units and so forth for each of the texture operations in the batch of texture operations.

In another embodiment as shown in texture coordinates may be drained from the texture queue according to the QuadTex priority mode. The texture coordinates for the first batch of texture operations for a first quad Q are loaded into the texture interface buffer and transmitted to the texture units . Texture coordinates for the first batch of texture operations for a second quad Q are loaded into the texture interface buffer and transmitted to the texture units and so forth until all of the texture coordinates associated with the first batch of texture operations have been drained from the texture queue . Again it will be appreciated that as many quads as will fit in the texture interface buffer may be loaded into the texture interface buffer in parallel and then drained to the texture units in order.

It will be appreciated that in each of the embodiments illustrated in TexTile priority mode corresponds to loading the texture coordinates for each of the quads in a pixel tile in order for one texture operation at a time in the batch of texture operations. In contrast QuadTex priority mode corresponds to loading the texture coordinates for each of the texture operations in the batch of texture operations in order for one quad at a time in a pixel tile.

As shown in each of the batches of texture operations includes texture operations of uniform size. In other words a batch of texture operations may contain texture operations of one two three four or more coordinates as operands and each of the texture operations in the batch of texture operations contains the same number of texture coordinates as operands. In some implementations a batch of texture operations may contain texture operations of non uniform size. For example a first texture operation in the batch of texture operations may include two texture coordinates as operands while a second texture operation in the batch of texture operations may include three texture coordinates as operands.

In one embodiment padding bits may be added to data stored in the texture queue to cause each of the texture operations to have the same amount of data that is transmitted to the texture units . In such embodiments the padding bits may not affect the output of the texture units . It will be appreciated in some embodiments that padding bits may not be stored in the texture queue and that some bits or banks in a slot of the texture queue may simply remain unused based on the alignment of texture operations that include a particular number of texture coordinates as operands. These unused bits do not need to be transferred to the texture units . In another embodiment texture operations of multiple sizes may be transmitted to the texture units . However care should be taken when scheduling texture operations of different sizes due to possible bank conflicts when loading texture coordinates in the texture queue or storing texture values in the texture queue . In yet another embodiment the batch of texture operations could be split into multiple batches of texture operations where each batch of texture operations includes texture operations having a uniform size. Then each of the batches of texture operations of uniform size may be processed independently.

In one embodiment as shown in texture coordinates may be drained from the texture queue according to the TexTile priority mode. In the TexTile priority mode the texture units generate texture values associated with the first texture operation i.e. r g b a for each of the quads in a pixel tile in order before generating texture values associated with the second texture operation for each of the quads in the pixel tile and so forth. In other words the texture units generate texture values for a first texture operation before texture values are generated for subsequent texture operations in the batch of texture operations. Although the texture values generated by the texture units are transmitted to the texture queue in order the texture interface buffer in conjunction with the write crossbar may rearrange the order of the texture values stored in the texture queue . In one embodiment the texture interface buffer of configured to store texture values is the same unit as the texture interface buffer of configured to store texture coordinates. In another embodiment separate and distinct texture interface buffers are provided a first texture interface buffer configured to store texture coordinates drained to the texture units and a second texture interface buffer configured to store texture values generated by the texture units .

In another embodiment as shown in texture coordinates may be drained from the texture queue according to the QuadTex priority mode. In the QuadTex priority mode the texture units generate texture values associated with the first quad Q for each of the texture operations in the batch of texture operations. Then the texture units generate texture values associated with the second quad Q for each of the texture operations in the batch of texture operations and so forth for each of the quads in the pixel tile. The texture interface buffer in conjunction with the write crossbar stores the texture values in the correct location within the texture queue .

In one embodiment the functionality of the TRB and the texture queue may be combined in one portion of memory in the shared memory L1 cache . For example the TIM table may associate locations in the texture queue with texture identifiers such that slots in the texture queue function as slots of the TRB . Merging the functionality of the TRB and the texture queue has some benefits such as reducing the need for double buffering while implementing the TRB in the register file and the texture queue in the shared memory L1 cache has other benefits such as making it easier for threads to consume final texture values directly from the TRB . In another embodiment a portion of the shared memory L1 cache may be allocated as the TIM table and another portion of the shared memory L1 cache may be allocated as the TRB free list table.

The system also includes input devices a graphics processor and a display i.e. a conventional CRT cathode ray tube LCD liquid crystal display LED light emitting diode plasma display or the like. User input may be received from the input devices e.g. keyboard mouse touchpad microphone and the like. In one embodiment the graphics processor may include a plurality of shader modules a rasterization module etc. Each of the foregoing modules may even be situated on a single semiconductor platform to form a graphics processing unit GPU .

In the present description a single semiconductor platform may refer to a sole unitary semiconductor based integrated circuit or chip. It should be noted that the term single semiconductor platform may also refer to multi chip modules with increased connectivity which simulate on chip operation and make substantial improvements over utilizing a conventional central processing unit CPU and bus implementation. Of course the various modules may also be situated separately or in various combinations of semiconductor platforms per the desires of the user.

The system may also include a secondary storage . The secondary storage includes for example a hard disk drive and or a removable storage drive representing a floppy disk drive a magnetic tape drive a compact disk drive digital versatile disk DVD drive recording device universal serial bus USB flash memory. The removable storage drive reads from and or writes to a removable storage unit in a well known manner.

Computer programs or computer control logic algorithms may be stored in the main memory and or the secondary storage . Such computer programs when executed enable the system to perform various functions. The memory the storage and or any other storage are possible examples of computer readable media.

In one embodiment the architecture and or functionality of the various previous figures may be implemented in the context of the central processor the graphics processor an integrated circuit not shown that is capable of at least a portion of the capabilities of both the central processor and the graphics processor a chipset i.e. a group of integrated circuits designed to work and sold as a unit for performing related functions etc. and or any other integrated circuit for that matter.

Still yet the architecture and or functionality of the various previous figures may be implemented in the context of a general computer system a circuit board system a game console system dedicated for entertainment purposes an application specific system and or any other desired system. For example the system may take the form of a desktop computer laptop computer server workstation game consoles embedded system and or any other type of logic. Still yet the system may take the form of various other devices including but not limited to a personal digital assistant PDA device a mobile phone device a television etc.

Further while not shown the system may be coupled to a network e.g. a telecommunications network local area network LAN wireless network wide area network WAN such as the Internet peer to peer network cable network or the like for communication purposes.

While various embodiments have been described above it should be understood that they have been presented by way of example only and not limitation. Thus the breadth and scope of a preferred embodiment should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

