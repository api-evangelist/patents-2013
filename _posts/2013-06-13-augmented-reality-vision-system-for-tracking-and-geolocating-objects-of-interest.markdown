---

title: Augmented reality vision system for tracking and geolocating objects of interest
abstract: Methods and apparatuses for tracking objects comprise one or more optical sensors for capturing one or more images of a scene, wherein the one or more optical sensors capture a wide field of view and corresponding narrow field of view for the one or more images of a scene, a localization module, coupled to the one or more optical sensors for determining the location of the apparatus, and determining the location of one more objects in the one or more images based on the location of the apparatus and an augmented reality module, coupled to the localization module, for enhancing a view of the scene on a display based on the determined location of the one or more objects.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09495783&OS=09495783&RS=09495783
owner: SRI INTERNATIONAL
number: 09495783
owner_city: Menlo Park
owner_country: US
publication_date: 20130613
---
This application claims the benefit of U.S. provisional patent application No. 61 675 734 filed Jul. 25 2012 and U.S. provisional patent application No. 61 790 715 filed on Mar. 15 2013 the disclosures of which are herein incorporated by reference in their entirety.

This invention was made with U.S. government support under contract number N00014 11 C 0433. The U.S. government has certain rights in this invention.

Embodiments of the present invention generally relate to augmented reality and more particularly to a method and apparatus for tracking and geolocating objects of interest.

Currently binoculars are entirely optical devices allowing users to zoom in on a particular real world area from a long distance. If a user is attempting to view precise movements of an object at a distance such as a car truck or person the user is able to use a binocular lens switching mechanism to change to a different magnification. In other binoculars a zooming function is provided which can vary magnification ranges using a switch or lever. However once the user increases magnification level the user may experience difficulty in finding the object of interest within the zoomed scene.

Further in conventional binocular system if there are several binocular users in communication and one user has identified one or more objects of interest such as interesting wildlife people or the like difficulty arises in signaling the location of the object of interest to other binocular users. The user who sighted the object of interest may use landmarks but this method is imprecise and landmarks may not be in view of other user s binocular systems. In addition there may be several similar landmarks making it more difficult to identify the precise location of objects of interest.

Therefore there is a need in the art for a method and apparatus for precisely determining the geolocation of distant objects in addition to tracking and sharing of the location for those objects.

An apparatus and or method for tracking and geolocating objects of interest comprising one or more optical sensors for capturing one or more images of a scene wherein the one or more optical sensors capture a wide field of view and corresponding narrow field of view for the one or more images of a scene a localization module coupled to the one or more optical sensors for determining the location of the apparatus and determining the location of one more objects in the one or more images based on the location of the apparatus and an augmented reality module coupled to the localization module for enhancing a view of the scene on a display based on the determined location of the one or more objects.

Various advantages aspects and features of the present disclosure as well as details of an illustrated embodiment thereof are more fully understood from the following description and drawings.

Embodiments of the present invention generally relate to a method and apparatus for tracking and geolocating objects of interest. According to one embodiment an apparatus for tracking objects of interest comprises an optical sensor used in conjunction with positional sensors such as inertial measurement units IMU and global navigation satellite systems such as geographical positioning satellite GPS systems Glonass and Galileo to locate the apparatus and a laser rangefinder to geographically localize objects in reference to the location of the optical sensor. Objects of interest are tracked as a user zooms the optical sensors or the user switches from a wide field of view to a narrow field of view. Embodiments of the present invention preserve a location of an object of interest within the user s view throughout a change in zoom or field of view. Additionally if the object of interest escapes the user s view during a zooming function an augmented reality system may provide the user with indicia of the location of the object within the user s view as well as other pertinent information. An observed scene is augmented with information labels of real world objects and guidance information on moving a camera unit or binocular unit to view objects of interest in a user s field of view. The augmentation appears stable with respect to the observed scene.

According to one embodiment the sensors to are optical sensors and the sensors to are positional sensors. In some embodiments sensors to may comprise infrared IR sensors visible sensors night vision sensors radiation signature sensors radio wave sensors or other types of optical sensors. Sensors to simultaneously capture one or more images of a scene while the sensors to capture data about the geographic location and orientation of the apparatus . According to an exemplary embodiment all of the sensors are physically coupled to the apparatus . In some embodiments the one or more sensors to may be housed in a telescope a binocular unit a headset bifocals or the like. In some instances the sensors may be remotely located from the apparatus.

One or more of the sensors to comprise a sensor for capturing wide field of view images e.g. 50 degrees to 80 degrees horizontal field of view and one or more of the sensors to comprise a sensor for capturing a narrow field of view images e.g. 1 degree to 10 degrees. A wide angle image of the scene provides context for narrower field of view images. The images captured by sensors to are coupled to the object recognition module . The object recognition module performs object recognition on the wide field of view images and the narrow field of view images to recognize objects in all of the images. According to exemplary embodiments invariant image features are used to recognize objects in the images as described in commonly assigned and issued U.S. Pat. No. 8 330 819 filed on Apr. 12 2012 commonly assigned U.S. Pat. No. 8 345 988 B2 filed on Jun. 22 2005 and U.S. Pat. No. 8 243 991 B2 filed on Aug. 14 2012 herein incorporated by reference in their entirety.

Simultaneously capturing narrow field of view images with wide field of view images of the scene allow the vision algorithms to have additional context in recognizing objects of interest in a scene. The narrow and wide FOV images in conjunction with various positional sensors aid in high fidelity localization i.e. highly accurate geolocalization of objects of interest can be achieved. Further the object recognition module is coupled to database to receive invariant feature records and the like to assist in object recognition. The object recognition module may also receive scene and language data from the database to localize wide angle images as described in commonly assigned co pending U.S. patent application Ser. No. 13 493 654 filed Jun. 11 2012 herein incorporated by reference in its entirety.

For example a GNSS received provides an initial bearing of apparatus and an IMU can provide relative movements of the apparatus such as rotation and acceleration. Together six degrees of freedom 6DOF can be obtained using the various sensors. In some instances a GPS signal may only be sporadically available. According to some embodiment the apparatus can still provide tracking of objects of interest during periods of sporadic GNSS reception. The signals previously received from the GNSS may be stored in a memory of the apparatus . The localization module calculates a projected geographical location of the apparatus based on the previously stored GNSS location and trajectory of the apparatus in conjunction with the trajectory of the objects of interest.

The localization module couples the localized object and apparatus information to the tracking module . The tracking module correlates the objects in the wide field of view images and the narrow field of view images. For example in wide field of view images a dog and cat are recognized at a distance. In narrow field of view images the dog and cat are recognized and correlated with the dog and cat in the wide field of view images because the location of the dog and cat are known from the localization module . The tracking module tracks the location of the dog and cat when a user switches from viewing a wide field of view of the scene to a narrow field of view of the scene . This process is referred to as visual odometry as described in commonly assigned co pending U.S. patent application Ser. No. 13 217 014 filed on Aug. 24 2011 herein incorporated by reference in its entirety. Accordingly the visual odometry is performed on both the wide field of view and the narrow field of view simultaneously or in parallel. The wide field of view provides robustness while the narrow field of view provides accuracy. Users may be provided with preset field of view angles for example a lens with ten steps of fields of view may be provided to the user. According to some embodiments to support multiple fields of view prior geometric calibration may be performed between images taken with different fields of view.

According to other embodiments a high resolution camera e.g. a camera that can capture greater than 50 MegaPixels MP may be used instead of two views of a scene with differing field of view. According to this embodiment the camera enables a user to capture the wide field of view with a very high resolution for example 50 MP or more and as the user zooms into a particular area of the wide field of view of the scene there is enough pixel data to represent the narrow field of view of the scene also. The object recognition module uses the wide field of view image to detect objects of interest and the localization module may use a laser rangefinder to calculate precise geographic coordinates of the objects of interest. The tracking module enables a user of the apparatus to track the objects of interest as the zoom on the camera is changed.

According to exemplary embodiments the sensors to may comprise navigation sensors such as a GNSS receiver an IMU unit a magnetometer pressure sensors a laser range finder and the like. The localization module localizes the apparatus using the sensors to and the narrow wide field of view images captured by sensors to . A three dimensional location and orientation is established for the apparatus with more accuracy than with the positional sensors to alone. Refer to commonly assigned co pending U.S. patent application Ser. No. 13 217 014 filed on Aug. 24 2011 and commonly assigned U.S. Pat. No. 8 174 568 filed on Dec. 3 2007 herein incorporated by reference in their entireties for more detail regarding calculating 3D coordinates based on an image and navigational sensors.

Once the location of the apparatus is calculated the object recognition module may recognize objects within the wide field of view and narrow field of view of the scene . A user of the apparatus may select objects of interest by manually inputting selections in the field of view using a laser rangefinder or the like. In other instances the object recognition module scans both fields of views and detects objects automatically based on training data stored in database . While a user pans the optical sensors the object recognition module detects objects of interest for example facial recognition movement recognition bird recognition or the like. According to exemplary embodiments the localization module further receives object recognition information from the object recognition module to aid in localization. For example if the object recognition module recognizes an oak tree at a distance d from a park bench at a known location in scene in a wide angle image and a distance d from the apparatus the localization module uses the location of the park bench to determine the accuracy of the estimated distance d of the oak tree from the apparatus .

The localization module also locates objects of interest in the user s fields of view and generates guides to locate objects of interest. For example the apparatus may automatically detect different types of birds in a scene and automatically guide a user towards the birds as well as augmented a user display with information regarding the birds. The user may mark a target bird stationary or moving while in a zoomed out field of view and be guided by the registration of images on how to move the camera while zooming in without losing the target object ensuring the target is still in view.

The reasoning module uses the localization knowledge and the object recognition knowledge from the database to generate relevant content to be presented to the user. A word model consisting of geo spatial information such as digital terrain geographical tags and 3D models can be used in the reasoning module . Similarly dynamic geographic data can be transmitted to the reasoning module from an external source to augment the reasoning generated by the reasoning module . Based on geo location and context overlay content can be customized in the reasoning module . The overlay is also used in aiding in navigation of a user of the apparatus .

The overlay content is then projected using the augmented reality module in the accurate coordinate system of the narrow field of view optical sensors as derived by the localization module . Additionally the world model may be used by the reasoning module to determine occlusion information during the rendering process. According to one embodiment the apparatus is a video see through system i.e. a system with an optical sensor as well as an optical viewfinder where content is rendered on the optical viewfinder as the user of apparatus views a scene through the optical viewfinder. In this embodiment the content is fused with the captured images from the sensor to for display on the viewfinder allowing the user to view the augmented content overlaid on the scene .

Overlaid content may include dynamic information such as weather information social networking information flight information traffic information star map information and other external geographically located information fed into the system. Another example of geographically located information may include overlaying real estate house and commercial sales information tourist attractions and the like on a view of the apparatus .

According to one embodiment a visually impaired person wearing a single head or body mounted camera may be aided using the apparatus . As a visually impaired individual pans his or her head an implicit image mosaic or landmark images are generated by the AR tracking module and stored in database . An object of interest can be designated in a certain view by image processing by the object recognition module or manually by the user. The visually impaired user is then able to move around but be guided back to the designated view point or location based on the geolocated apparatus as well as through alignment of images from the camera with landmark and mosaic images stored in the database .

According to other embodiments the database may store images or videos of people of interest such as celebrities athletes news worthy figures or the like. The object recognition module may then perform object recognition while a user is scanning a scene and match against the people of interest stored in database to identify people of interest in the scene.

According to some embodiments the computer system may be housed internally in the binocular unit . In other embodiments the computer system is remote from the binocular unit and the binocular unit transmits and receives data from the computer system through wired transmission lines or wirelessly.

The wide FOV optic is coupled to the camera . The narrow FOV optic is coupled to the camera . The camera and camera are coupled to a computer system . The wide FOV optic and the narrow FOV optic are both coupled to the eyepiece . The eyepiece comprises according to one embodiment a first lens A and a second lens B. In other embodiments the eyepiece may comprise only a single viewing lens or more than two viewing lenses.

In some embodiments a user of the binocular unit is able to switch between viewing the wide FOV optic view and he narrow FOV optic view in the eyepiece . The laser rangefinder outputs a laser towards a direction aligned with the orientation of the binocular unit . The laser rangefinder establishes distances to objects within the field of view of the binocular unit . In addition the laser rangefinder may be used to map the terrain into three dimensional space and store the 3D terrain map in the database .

The computer system includes a processor various support circuits and memory . The processor may include one or more microprocessors known in the art. The support circuits for the processor include conventional cache power supplies clock circuits data registers I O interface and the like. The I O interface may be directly coupled to the memory or coupled through the supporting circuits . The I O interface may also be configured for communication with input devices and or output devices such as network devices various storage devices mouse keyboard display video and audio sensors and the like.

The memory or computer readable medium stores non transient processor executable instructions and or data that may be executed by and or used by the processor . These processor executable instructions may comprise firmware software and the like or some combination thereof. Modules having processor executable instructions that are stored in the memory comprise an augmented reality AR tracking module . The AR tracking module further comprises an object recognition module a localization module a tracking module an augmented reality module a reasoning module and a communication module .

The computer may be programmed with one or more operating systems generally referred to as operating system OS which may include OS 2 Java Virtual Machine Linux SOLARIS UNIX HPUX AIX WINDOWS WINDOWS95 WINDOWS98 WINDOWS NT AND WINDOWS2000 WINDOWS ME WINDOWS XP WINDOWS SERVER WINDOWS 8 Mac OS X IOS ANDROID among other known platforms. At least a portion of the operating system may be disposed in the memory .

The memory may include one or more of the following random access memory read only memory magneto resistive read write memory optical read write memory cache memory magnetic read write memory and the like as well as signal bearing media as described below.

The camera and the camera are coupled to the support circuits and the I O interface of the computer system . The I O interface is further coupled to an overlay of the eyepiece . The object recognition module recognizes objects within the wide FOV optic and the narrow FOV optic as the images are received by the cameras and i.e. in real time as the user of the binocular unit scans a viewing area. According to one embodiment the user can scan a larger area than that visible in the wide FOV optic and have the object recognition module scan for objects in the larger area. Additionally the user can identify using the laser rangefinder or other visual means an object of interest to track or recognize and the object recognition module will perform object recognition on that particular object or the user designated area.

The objects recognized by the object recognition module are localized using the localization module and tracked by the tracking module . The tracking module may also notify the augmented reality module that a visual indication should be rendered to indicate the location of a particular recognized object even if the object moves out of view of the binocular unit . The augmented reality module then creates an overlay for the narrow field of view based on the recognized object requested content by the user markers indicating the location of recognized objects amongst additional augmented reality content known to those of ordinary skill in the art. The content is then coupled from the augmented reality module to the binocular unit and in particular to the eyepiece as overlay . According to this embodiment the overlay overlays the real time view through the lenses A and B i.e. as the user surveys a scene through the lenses A and B the overlay is also visible through the lenses A and B. The augmented reality module generates the overlay in geometrical and geographical correspondence with the terrain visible to the user through the binocular unit .

For example a binocular unit may observe with the wide FOV optic a scene depicted by the image shown in and with the narrow FOV optic observe a scene depicted by the image shown in . In image a user has marked a landmark as a landmark of interest after the object recognition module has recognized the objects in the image and the localization module localized the landmark of interest. When the user zooms or alternates views to the narrow field of view from the eyepiece the augmented reality module places the marker appropriately to indicate the landmark of interest according to the modified view because the localization module has precisely located the landmark in geographic space. According to other embodiments the object may be tracked so that when a user of the unit moves the tracked objects of interest remain centered in the near field of view.

According to another embodiment the augmented reality module can overlay the view from a binocular unit to create a simulation. For example depicts a wide FOV image where a user of a binocular unit indicates that a helicopter should be placed at a particular location. The augmented reality module places the helicopter in the preferred location and overlay the helicopter in real time on overlay where the position of the helicopter in the user s view changes as the user adjusts the binocular unit . The AR module scales rotates and translates the helicopter simulation based on the geographic localization data of the binocular unit and the rotational movement of the IMU . depicts another example of inserting simulated objects into a scene for aiding in simulated operations. Image depicts a narrow field of view scene with a tank object inserted into the scene. According to exemplary embodiments the tank may be animated by the augmented reality module with a size location and orientation corresponding to the other objects recognized by the object recognition module and the orientation of the device according to the localization module to create a mixed reality and simulated environment to aid in user training such as procedural training strategic training or the like.

Additionally the reasoning module may occlude the tank as it is animated to conform to the terrain depicted in image . According to another embodiment a user of the binocular unit may scan a scene by spatially moving the binocular unit to geographically locate the entire scanned contents of the scene. The reasoning module can then generate a three dimensional terrain based on the geographically located objects terrain and the like. The wide field of view and narrow field of view images can be used to construct a 3D map of an observed area by imaging the area based on multiple images and then estimating the 3D by a motion stereo process accurately using the geographic location provided by the various sensors as described in U.S. patent application Ser. No. 13 493 654 filed on Jun. 11 2012 herein incorporated by reference in its entirety.

The scene comprises one or more objects the one or more objects comprising a first person a vehicle and a second person . The scene is being observed by three users a first user operating BU a second user operating BU and a third user operating BU each oriented in a different direction. Though not shown in each of the BU and may be oriented in three dimensional space as opposed to the 2D plane depicted. For example BU may be positioned at a height above BU and BU may be looking at an area in scene lower in height than the area in scene being viewed by BU .

The BU has person and vehicle in view. The operator of BU identifies person and vehicle as objects of interest using a laser rangefinder or in some other manner well known in the art such as delimiting a particular area wherein the person and the vehicle are located and directing the unit to perform object recognition.

The BUs is all coupled to the network . According to some embodiments the BUs and create an ad hoc network and communicate directly with each other.

The scene is an example view of a field containing several objects a person a vehicle a second person and several trees and bushes . According to one embodiment of the present invention the user of binocular unit may mark the location using a laser rangefinder or other means of the person and the vehicle . A marker identifies the location of the first person and the marker identifies the location of the vehicle . The marker and location are shared across the network with the other Bus and . Similarly the BU marks the location of the second person with a marker .

The first person and the vehicle are in the view of BU however they are out of the view of BU . If the operator of BU directs the attention of the user of the BU towards the vehicle and the first person the user will have indications in their view showing where to turn their BU to view the person and the vehicle . For example indicator is shown as pointing in the direction that the BU unit must turn in order to see person . Indicator indicates the direction the BU must turn to view vehicle . According to this embodiment since the localization module of BU unit has localized vehicle and first person the location is available to all units on the network . The AR module of each of the BU units generates the indicators and .

Similarly the AR module of the BU unit generates an indicator pointing in the direction of the person and the AR module of the BU unit generates a marker pointing in the direction of the location of person .

The BU also has first person in its view . The user of BU may also mark the location of first person using the laser rangefinder. Since both BU unit and BU unit have marked the location of person the localization module may localize person more accurately given two perspectives and two distances from the BUs. In addition when a user of any of the Bus pan their own view they do not lose sight of the objects in the scene because of the indicators and as well as the markers and .

According to other embodiments the AR modules of each BU may augment the BU view of the scene based on the determined location of the objects with many other types of data such as distance motion estimation threat level or the like. In other instances facial and object recognition may be applied in an urban setting for recognizing faces and well known structures to aid in guidance towards a particular landmark. A user of the BU or even a mobile phone with a camera may designate and identify real world objects in wide and narrow field of views while panning the phone or keeping the device still. The object is then located according to the discussion above geographically in world space and in local view. The AR module then provides audio and or visual guidance to another user on how to move their device to locate the target object. In addition users performing the object tagging may attach messages to their tagged objects or other tags such as images video audio or the like.

Other users of mobile devices may then have objects of interest highlighted on their displays or have directions towards the highlighted objects on their displays. In some embodiments one of the viewers of a scene may be viewing the scene aerially while other viewers of a scene are on the ground and either party may tag highlight objects of interest for the users to locate or observe.

Those of ordinary skill in the art would recognize that the Binocular units and may be replaced with for example a head mounted display vision unit a mobile camera or the like. A head mounted display or vision unit may be wearable by a user who can view the surrounding scene through the visor or glasses in the head mounted unit and also view the AR enhancements in the visor or glasses overlaid on top of the user s view of the scene.

For example several users of head mounted units and may be engaged in a session of gaming i.e. playing a video game where objects are rendered onto each user s field of view with regard to each unit s frame of reference view and angle. In this embodiment the scene can become a virtual gaming arena where users can view and use real world objects with simulated objects to achieve particular goals such as capturing a flag or scoring a point in a sport. In these embodiments as well as other embodiments virtual advertisements may be displayed on each user s view in a geographically sensible position as in game advertisements.

According to some instances when users create Geo tags they may be stored in a local ad hoc network created by similar viewing units throughout a particular area or units manually synced together. Geo tags and other messages may then be shared directly from one binocular unit another binocular unit. In other embodiments the geo tags and other landmark objects of interest tags and markings are stored on external servers cloud servers for example and each unit may access the cloud to access geo tags associated with them i.e. tags within the user s group or global geo tags depending on their preference. In this embodiment a second user s location is determined to be in the same vicinity as one of the geo tags and the cloud server transmits the tags within the user s area. Geo tags objects of interest and guided directions towards objects of interest may then be downloaded from the external cloud servers and used to overlay an AR display on an AR viewing unit. In other embodiments users can share geo tags objects of interest landmarks and the like with other users by giving an input command such as a speech command or at the press of a menu item.

According to exemplary embodiments the apparatus may have a application programming interface API which allows developers to plug in particular applications of object detection such as in recreational sports bird watching tourism hunting hiking law enforcement and the like.

At step one or more optical sensors capture one or more images of a scene containing wide field of view and narrow field of view images. In some embodiments the images are captured and processed in real time i.e. as a user views a scene in the narrow field of view a larger field of view image containing the scene is also being captured by the optical sensors.

At step the localization module determines the location of the optical sensors. In this embodiment the user is directly holding or otherwise coupled with the location of the optical sensors so that the location of the optical sensors is essentially equivalent to the location of the user of BU for example. The object recognition module recognizes objects in the images and using the laser rangefinder of the BU the distance between the optical sensor and the objects can be determined. Since the location of the BU has been determined and the distance to the objects has been determined the location of the objects can also be accurately determined. In addition if several binocular units observe the same objects another degree of accuracy is added to the determined location of the objects.

At step the AR module enhances the view through the BU based upon the determined locations of the objects. As described above the AR module overlays content on the user s view. The content may include markers of objects of interest indicators directing the user of BU to objects of interest outside of the current field of view weather information flight map information and other information.

At step the tracking module tracks the one or more objects from the wide field of view to the narrow field of view as a user switches between the two fields of view. For example the user of BU may view the wide FOV scene and sight a bird of interest. However in order to see the bird in more detail the user of BU switches to a narrow field of view.

Under ordinary circumstances the user would have difficulty in finding the bird again in the narrow field of view. However the tracking module tracks the objects from the wide FOV to the narrow FOV so the user will be presented with a narrow FOV already containing the bird. According to another embodiment when a user switches from a wide FOV to a narrow FOV the AR module overlays an indication of the direction in which to rotate the BU to locate the bird of interest.

At step the BU may use the communication module to broadcast the determined locations of the BU in addition to the objects of interest found to other people in the area with mobile devices binocular units mobile computers or the like. In addition the other people in the area may send the location of objects of interest outside of the user s field of view to the communication module . At step accordingly the AR module updates the overlay on the enhanced view indicating the direction in which to look to find those objects of interest outside the user s field of view.

Subsequently the method moves to step . At step the enhanced view is updated in real time based on the determined location of the one or more detected objects as the user relocates the BU . At step the view is further enhanced to incorporate external sources of geographically located information sources. For example traffic may be overlaid on the BU view weather information social media information or the like. According to some embodiments based on the determined location of the BU and selected objects of interest the AR module may overlay virtual advertisements relevant to the user location. The method terminates at step .

Various elements devices modules and circuits are described above in association with their respective functions. These elements devices modules and circuits are considered means for performing their respective functions as described herein. While the foregoing is directed to embodiments of the present invention other and further embodiments of the invention may be devised without departing from the basic scope thereof and the scope thereof is determined by the claims that follow.

