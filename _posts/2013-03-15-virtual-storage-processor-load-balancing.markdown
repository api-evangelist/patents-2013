---

title: Virtual storage processor load balancing
abstract: A technique performs virtual storage processor (VSP) load balancing. The technique involves receiving a VSP move command to load balance a particular VSP from a source physical storage processor to a destination physical storage processor. The technique further involves relinquishing, by the source physical storage processor, access to a set of VSP definitions that define the particular VSP. The technique further involves obtaining, by the destination physical storage processor, access to the set of VSP definitions that define the particular VSP, the particular VSP being load balanced from the source physical storage processor to the destination physical storage processor upon the destination physical storage processor obtaining access to the set of VSP definitions that define the particular VSP.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09304999&OS=09304999&RS=09304999
owner: EMC Corporation
number: 09304999
owner_city: Hopkinton
owner_country: US
publication_date: 20130315
---
Data storage systems typically include one or more physical storage processors SPs accessing an array of disk drives and or electronic flash drives. Each SP is connected to a network such as the Internet and or a storage area network SAN and receives transmissions over the network from host computing devices hosts . The transmissions from the hosts include IO requests also called host IOs. Some IO requests direct the SP to read data from an array whereas other IO requests direct the SP to write data to the array. Also some IO requests perform block based data requests where data are specified by LUN Logical Unit Number and offset values whereas others perform file based requests where data are specified using file names and paths. Block based IO requests typically conform to a block based protocol such as Fibre Channel or iSCSI Internet SCSI where SCSI is an acronym for Small Computer System Interface for example. File based IO requests typically conform to a file based protocol such as NFS Network File System CIFS Common Internet File System or SMB Server Message Block for example.

In some data storage systems an SP may operate one or more virtual data movers. As is known a virtual data mover is a logical grouping of file systems and servers that is managed by the SP and provides a separate context for managing host data stored on the array. A single SP may provide multiple virtual data movers for different users or groups. For example a first virtual data mover may organize data for users in a first department of a company whereas a second virtual data mover may organize data for users in a second department of the company. Each virtual data mover may include any number of host file systems for storing user data.

In a typical virtual data mover arrangement the SP has a root file system with mount points to which the host file systems of each virtual data mover are mounted. Thus the SP and all its virtual data movers form a single large directory and all share a common namespace. Hosts can access their virtual data mover managed data by connecting to the SP over the network logging on and specifying paths relative to the SP s root where their data are kept. The typical arrangement thus requires hosts to access data of a virtual data mover using paths that are referenced to and dependent upon the root of the SP.

In addition settings for prescribing virtual data mover operations are conventionally stored in the root file system of the SP. Many of these settings are global to all virtual data movers operating on the SP others may be specific to particular virtual data movers.

Unfortunately the intermingling of virtual data mover content within an SP s root file system impairs the ease of mobility and management of virtual data movers. For example administrators wishing to move a virtual data mover e.g. its file systems settings and servers from one SP to another SP must typically perform many steps on a variety different data objects. File systems server configurations and other settings may need to be moved one at a time. Also as the contents of different virtual data movers are often co located care must be taken to ensure that changes affecting one virtual data mover do not disrupt the operation of other virtual data movers. Moreover there may be situations in which it may make sense to move a conventional virtual data mover from one physical SP to another for load balancing purposes but the above described complexity of conventional virtual data movers makes it prohibitively difficult to move a conventional virtual data mover among physical SPs.

In contrast to the above described conventional virtual data movers which are not moved among physical SPs due to their complexity improved techniques are directed to performing virtual storage processor VSP load balancing in which a VSP moves from one physical SP to another on a data storage apparatus. Such a VSP is capable of being used by any physical SP of the data storage apparatus to create an appropriate operating environment within which to access a particular host file system. Load balancing of the VSP enables precise positioning of the VSP on a physical SP based on a variety of load balancing criteria such as CPU utilization network traffic input output I O statistics number of connections and so on. Accordingly the performance and operation of the data storage apparatus may be finely tuned optimized for improved quality of service.

One embodiment is directed to a method of performing VSP load balancing. The method includes receiving a VSP move command to load balance a particular VSP from a source physical storage processor to a destination physical storage processor. The method further includes relinquishing by the source physical storage processor access to a set of VSP definitions that define the particular VSP. The method further includes obtaining by the destination physical storage processor access to the set of VSP definitions that define the particular VSP the particular VSP being load balanced from the source physical storage processor to the destination physical storage processor upon the destination physical storage processor obtaining access to the set of VSP definitions that define the particular VSP.

It should be understood that in some arrangements the set of VSP definitions includes a VSP configuration file system which is used by the source physical storage processor to create an operating environment within which to access a host file system. Additionally the VSP configuration file system is later used by the destination physical storage processor to recreate the operating environment within which to access the host file system.

In some arrangements a configuration database stores a set of records indicating that the source physical storage processor initially owns the particular VSP. In these arrangements the method further includes when the destination physical storage processor obtains access to the set of VSP definitions updating the set of records of the configuration database to indicate that the destination physical storage processor owns the particular VSP.

It should be understood that the VSP move command may be manually provided by a user e.g. through a user interface . Alternatively the VSP move command may be automatically provided in response to electronic detection of a load balancing event such as detecting when a level of central processing unit CPU utilization exceeds a predetermined CPU utilization threshold on the source physical storage processor i.e. a trigger .

In some arrangements the method includes generating based on a set of heuristics a ranking of existing VSPs which are accessed by the source physical storage processor and selecting the particular VSP from the generated ranking of the existing VSPs which are accessed by the source physical storage processor e.g. select the highest lowest ranked VSP for movement etc. . Such heuristics may based on measured CPU utilization measured network traffic to each of the existing VSPs measured I O statistics to each of the existing VSPs the measured number of connections to each of the existing VSPs combinations thereof and so on.

In some arrangements the method further includes in response the destination physical storage processor obtaining access to the set of VSP definitions starting a counter to measure an amount of time that the destination physical storage processor has access to the set of VSP definitions. With the amount of time being monitored the destination physical storage processor can be configured to maintain access to the particular VSP for at least a set period of time to prevent the particular VSP from bouncing among physical storage processors. Such time based operation prevents the possibility of starving out access to the particular VSP since the VSP will remain on the destination physical storage processor for at least a controlled amount of time e.g. one hour half a day one day etc. .

It should be understood that in the cloud context electronic circuitry is formed by remote computer resources distributed over a network. Such a computing environment is capable of providing certain advantages such as enhanced fault tolerance processing flexibility and so on.

Other embodiments are directed to electronic systems and apparatus processing circuits computer program products and so on. Some embodiments are directed to various methods electronic components and circuitry which are involved in performing VSP load balancing.

Embodiments of the invention will now be described. It is understood that such embodiments are provided by way of example to illustrate various features and principles of the invention and that the invention hereof is broader than the specific example embodiments disclosed.

An improved technique is directed to performing virtual storage processor VSP load balancing in which a VSP moves from one physical storage processor SP to another on a data storage apparatus. Such a VSP is capable of being used by any physical SP of the data storage apparatus to create an appropriate operating environment within which to access one or more host file systems. Load balancing of the VSP enables precise positioning of the VSP on a physical SP based on a variety of load balancing criteria such as CPU utilization network traffic input output I O statistics number of connections combinations thereof and so on. As a result the performance and operation of the data storage apparatus may be finely tuned optimized for improved quality of service.

In contrast with the above described conventional virtual data movers an improved technique for managing host data in a data storage apparatus provides virtualized storage processors VSPs as substantially self describing and independent entities. Each VSP has its own namespace which is independent of the namespace of any other VSP. Each VSP also has its own network address. Hosts may thus access VSPs directly without having to include path information relative to the SP on which the VSPs are operated. VSPs can thus be moved from one physical SP to another with little or no disruption to hosts which may in many cases continue to access the VSPs on the new SPs using the same paths as were used to access the VSPs on the original SPs.

In some examples each VSP includes within its namespace a configuration file system storing configuration settings for operating the VSP. These configuration settings include for example network interface settings and internal settings that describe the VSPs personality i.e. the manner in which the VSP interacts on the network. By providing these settings as part of the VSP itself e.g. within the file systems of the VSP the VSP can be moved from one physical SP to another substantially as a unit. The increased independence of the VSP from its hosting SP promotes many aspects of VSP management including for example migration replication failover trespass multi tenancy load balancing and gateway support.

In some examples the independence of VSPs is further promoted by storing data objects of VSPs in the form of respective files. These data objects may include for example file systems LUNs virtual storage volumes vVols and virtual machine disks VMDKs . Each such file is part of a set of internal file systems of the data storage apparatus. Providing data objects in the form of files of a set of internal file systems promotes independence of VSPs and unifies management of file based objects and block based objects.

In accordance with improvements hereof certain embodiments are directed to a method of managing host data on a data storage apparatus connected to a network. The method includes storing a network address and a set of host data objects accessible within a namespace of a virtualized storage processor VSP operated by a physical storage processor of the data storage apparatus. The namespace includes only names of objects that are specific to the VSP. The method further includes receiving by the physical storage processor a transmission over the network from a host computing device. The transmission is directed to a network address and includes an IO request designating a pathname to a host data object to be written or read. The method still further includes identifying the host data object designated by the IO request by i matching the network address to which the transmission is directed with the network address stored for the VSP to identify the VSP as the recipient of the IO request and ii locating the host data object within the namespace of the VSP using the pathname. The IO request is then processed processed to complete the requested read or write operation on the identified host data object.

Other embodiments are directed to computerized apparatus and computer program products. Some embodiments involve activity that is performed at a single location while other embodiments involve activity that is distributed over a computerized environment e.g. over a network .

An improved technique for managing host data in a data storage apparatus provides virtualized storage processors VSPs as substantially self describing and independent constructs.

The network can be any type of network such as for example a storage area network SAN local area network LAN wide area network WAN the Internet some other type of network and or any combination thereof. In an example the hosts N connect to the SP using various technologies such as Fibre Channel iSCSI NFS SMB 3.0 and CIFS for example. Any number of hosts N may be provided using any of the above protocols some subset thereof or other protocols besides those shown. As is known Fibre Channel and iSCSI are block based protocols whereas NFS SMB 3.0 and CIFS are file based protocols. The SP is configured to receive IO requests N in transmissions from the hosts N according to both block based and file based protocols and to respond to such IO requests N by reading or writing the storage .

The SP is seen to include one or more communication interfaces control circuitry e.g. a set of processors and memory . The communication interfaces include for example adapters such as SCSI target adapters and network interface adapters for converting electronic and or optical signals received from the network to electronic form for use by the SP . The set of processors includes one or more processing chips and or assemblies. In a particular example the set of processors includes numerous multi core CPUs. The memory includes both volatile memory e.g. RAM and non volatile memory such as one or more ROMs disk drives solid state drives SSDs and the like. The set of processors and the memory are constructed and arranged to carry out various methods and functions as described herein. Also the memory includes a variety of software constructs realized in the form of executable instructions. When the executable instructions are run by the set of processors the set of processors are caused to carry out the operations of the software constructs. Although certain software constructs are specifically shown and described it is understood that the memory typically includes many other software constructs which are not shown such as various applications processes and daemons.

As shown the memory includes an operating system such as Unix Linux or Windows for example. The operating system includes a kernel . The memory is further seen to include a container . In an example the container is a software process that provides an isolated userspace execution context within the operating system . In various examples the memory may include multiple containers like the container with each container providing its own isolated userspace instance. Although containers provide isolated environments that do not directly interact and thus promote fault containment different containers can be run on the same kernel and can communicate with one another using inter process communication IPC mediated by the kernel . Containers are well known features of Unix Linux and other operating systems.

In the example of only a single container is shown. Running within the container is an IO stack and multiple virtualized storage processors VSPs . The IO stack provides an execution path for host IOs e.g. N and includes a front end and a back end . The VSPs each run within the container and provide a separate context for managing host data. In an example each VSP manages a respective set of host file systems and or other data objects and uses servers and settings for communicating over the network with its own individual network identity. Although three VSPs are shown it is understood that the SP may include as few as one VSP or as many VSPs as the computing resources of the SP and storage resources of the storage allow.

Although the VSPs each present an independent and distinct identity it is evident that the VSPs are not in this example implemented as independent virtual machines. Rather all VSPs operate in userspace and employ the same kernel of the SP . Although it is possible to implement the VSPs as independent virtual machines each including a virtualized kernel it has been observed that VSPs perform faster when the kernel is not virtualized.

Also it is observed that the VSPs all run within the container i.e. within a single userspace instance. Again the arrangement shown reflects a deliberate design choice aimed at optimizing VSP performance. It is understood though that alternative implementations could provide different VSPs in different containers or could be provided without containers at all.

The memory is further seen to store a configuration database . The configuration database stores system configuration information including settings related to the VSPs and their data objects. In other implementations the configuration database is stored elsewhere in the data storage apparatus such as on a disk drive separate from the SP but accessible to the SP e.g. over a backplane or network.

In operation the hosts N issue IO requests N to the data storage apparatus . The IO requests N may include both block based requests and file based requests. The SP receives the IO requests N at the communication interfaces and passes the IO requests to the IO stack for further processing.

At the front end of the IO stack processing includes associating each of the IO requests N with a particular one of the VSPs . In an example each VSP stores a network address e.g. an IP address in a designated location within its file systems. The front end identifies the network address to which each IO request is directed and matches that address with one of the network addresses stored with the VSPs . The front end thus uses the network address to which each IO request is sent to identify the VSP to which the IO request is directed. Further processing of the IO request is then associated e.g. tagged with an identifier of the matching VSP such that the IO request is processed within a particular VSP context. Any data logging metrics collection fault reporting or messages generated while the IO request is being processed are stored with the associated VSP e.g. in a file system dedicated to the VSP . Also any path information provided with the IO request e.g. to a particular directory and file name is interpreted within the namespace of the identified VSP.

Processing within the front end may further include caching data provided with any write IOs and mapping host data objects e.g. host file systems LUNs vVols VMDKs etc. to underlying files stored in a set of internal file systems. Host IO requests received for reading and writing both file systems and LUNs are thus converted to reads and writes of respective files. The IO requests then propagate to the back end where commands are executed for reading and or writing the physical storage .

In an example processing through the IO stack is performed by a set of threads maintained by the SP in a set of thread pools. When an IO request is received a thread is selected from the set of thread pools. The IO request is tagged with a VSP identifier and the selected thread runs with the context of the identified VSP. Typically multiple threads from different thread pools contribute to the processing of each IO request there are many processing layers . Multiple threads from the thread pools can process multiple IO requests simultaneously i.e. in parallel on the data objects of any one VSP or multiple VSPs.

Although shows the front end and the back end together in an integrated form the front end and back end may alternatively be provided on separate SPs. For example the IO stack may be implemented in a modular arrangement with the front end on one SP and the back end on another SP. The IO stack may further be implemented in a gateway arrangement with multiple SPs running respective front ends and with a back end provided within a separate storage array. The back end performs processing that is similar to processing natively included in many block based storage arrays. Multiple front ends can thus connect to such arrays without the need for providing separate back ends. In all arrangements processing through both the front end and back end is preferably tagged with the particular VSP context such that the processing remains VSP aware.

The storage pool organizes elements of the storage in the form of slices. A slice is an increment of storage space such as 256 MB in size which is obtained from the storage . The pool may allocate slices to lower deck file systems for use in storing their files. The pool may also deallocate slices from lower deck file systems if the storage provided by the slices is no longer required. In an example the storage pool creates slices by accessing RAID groups formed from the storage dividing the RAID groups into FLUs Flare LUNs and further dividing the FLU s into slices.

Continuing with reference to the example shown in a user object layer includes a representation of a LUN and of an HFS host file system and a mapping layer includes a LUN to file mapping and an HFS to file mapping . The LUN to file mapping maps the LUN to a first file F and the HFS to file mapping maps the HFS to a second file F . Through the LUN to file mapping any set of blocks identified in the LUN by a host IO request is mapped to a corresponding set of blocks within the first file . Similarly through the HFS to file mapping any file or directory of the HFS is mapped to a corresponding set of blocks within the second file . The HFS is also referred to herein as an upper deck file system which is distinguished from the lower deck file systems which are for internal use.

In this example a first lower deck file system includes the first file and a second lower deck file system includes the second file . Each of the lower deck file systems and includes an inode table and respectively . The inode tables and provide information about files in respective lower deck file systems in the form of inodes. For example the inode table of the first lower deck file system includes an inode which provides file specific information about the first file . Similarly the inode table of the second lower deck file system includes an inode which provides file specific information about the second file . The information stored in each inode includes location information e.g. block locations where the respective file is stored and may thus be accessed as metadata to identify the locations of the files and in the storage .

Although a single file is shown for each of the lower deck file systems and it is understood that each of the lower deck file systems and may include any number of files each with its own entry in the respective inode table. In one example each lower deck file system stores not only the file F or F for the LUN or HFS but also snaps of those objects. For instance the first lower deck file system stores the first file along with a different file for every snap of the LUN . Similarly the second lower deck file system stores the second file along with a different file for every snap of the HFS .

As shown a set of slices is allocated by the storage pool for storing the first file and the second file . In the example shown slices S through S are used for storing the first file and slices S through S are used for storing the second file . The data that make up the LUN are thus stored in the slices S through S whereas the data that make up the HFS are stored in the slices S through S.

In some examples each of the lower deck file systems and is associated with a respective volume such as a sparse LUN. Sparse LUNs provide an additional layer of mapping between the lower deck file systems and the pool and allow the lower deck file systems to operate as file systems normally do by accessing underlying volumes. Additional details about sparse LUNs and their relation to lower deck file systems may be found in U.S. Pat. No. 7 631 155 which is hereby incorporated by reference in its entirety. The incorporated patent uses the term container file system to refer to a construct similar to the lower deck file system disclosed herein.

Although the example of shows storage of a LUN and a host file system in respective lower deck file systems and it is understood that other data objects may be stored in one or more lower deck file systems in a similar manner. These may include for example file based vVols block based vVols and VMDKs.

For example the VSP includes a first lower deck file system and a second lower deck file system . The first lower deck file system includes a file FA which provides a file representation of a first host file system . Similarly the second lower deck file system includes a file FB which provides a file representation of a second host file system . The host file systems and are upper deck file systems which may be made available to hosts N for storing file based host data. HFS to file mappings like the HFS to file mapping are understood to be present although not shown in for expressing the files FA and FB in the form of upper deck file systems. Although only two host file systems and are shown it is understood that the VSP may include any number of host file systems. In an example a different lower deck file system is provided for each host file system. The lower deck file system stores the file representation of the host file system and if snaps are turned on any snaps of the host file system. In a similar manner to that described in connection with each of the lower deck file systems and includes a respective inode table allowing the files FA and FB and their snaps to be indexed within the respective lower deck file systems and accessed within the storage .

In some examples the VSP also includes one or more lower deck file systems for storing file representations of LUNs. For example a lower deck file system stores a file FC which provides a file representation of a LUN . A LUN to file mapping not shown but similar to the mapping expresses the file FC in the form of a LUN which may be made available to hosts N for storing block based host data. In an example the lower deck file system stores not only the file FC but also snaps thereof and includes an inode table in essentially the manner described above.

The VSP further also includes a lower deck file system . In an example the lower deck file system stores file representations FD and FE of two internal file systems of the VSP a root file system and a configuration file system . In an alternative arrangement the files FD and FE are provided in different lower deck file systems. In an example the lower deck file system also stores snaps of the files FD and FE and files are accessed within the lower deck file system via file system to file mappings and using an inode table substantially as described above.

In an example the root file system has a root directory designated with the slash and sub directories as indicated. Any number of sub directories may be provided within the root file system in any suitable arrangement with any suitable file structure the example shown is merely illustrative. As indicated one sub directory Local stores for example within constituent files information about the local environment of the SP such as local IP sub net information geographical location and so forth. Another sub directory Rep stores replication information such as information related to any ongoing replication sessions. Another sub directory Cmd Svc stores command service information and yet another sub directory MPs stores mount points.

In the example shown the directory MPs of the root file system provides mount points e.g. directories on which file systems are mounted. For example the host file systems and are respectively mounted on mount points MP and MP and the configuration file system is mounted on the mount point MP. In an example establishment of the mount points MP MP and execution of the mounting operations for mounting the file systems onto the mount points MP MP are provided in a batch file stored in the configuration file system e.g. in Host Objects . It is understood that additional mount points may be provided for accommodating additional file systems.

The root file system has a namespace which includes the names of the root directory sub directories and files that belong to the root file system . The file systems and also each have respective namespaces. The act of mounting the file systems and onto the mount points MP MP and MP of the root file system serves to join the namespace of each of the file systems and with the namespace of the root file system to form a single namespace that encompasses all the file systems and . This namespace is specific to the VSP and is independent of namespaces of any other VSPs.

Also it is understood that the LUN is also made available to hosts through the VSP . For example hosts can send read and write IO requests to the LUN e.g. via Fibre Channel and or iSCSI commands and the SP services the requests for the VSP e.g. by operating threads tagged with the context of the VSP . Although shows both the LUN and the host file systems and together in a single VSP other examples may provide separate VSPs for LUNs and for file systems.

Although the VSP is seen to include file systems and LUNs other host objects may be included as well. These include for example file based vVols block based vVols and VMDKs. Such host objects may be provided as file representations in lower deck file systems and made available to hosts 

As its name suggests the configuration file system stores configuration settings for the VSP . These settings include settings for establishing the personality of the VSP i.e. the manner in which the VSP interacts over the network . Although the configuration file system is shown with a particular directory structure it is understood that any suitable directory structure can be used. In an example the configuration file system stores the following elements 

Although has been shown and described with reference to a particular VSP it is understood that all of the VSPs may include a root file system a configuration file system and at least one host file system or LUN substantially as shown. Particular host objects and configuration settings differ however from one VSP to another.

By storing the configuration settings of VSPs within the file systems of the VSPs themselves and providing a unique namespace for each VSP VSPs are made to be highly independent both of other VSPs and of the particular SPs on which they are provided. For example migrating a VSP from a first data storage system to a second data storage system involves copying its lower deck file systems or some subset thereof from a source SP on the first data storage system to a target SP on the second starting the VSP s servers on the target SP in accordance with the configuration settings and resuming operation on the target SP. As the paths for accessing data objects on VSPs are not rooted to the SPs on which they are run hosts may often continue to access migrated VSPs using the same instructions as were used prior to moving the VSPs. Similar benefits can be enjoyed when moving a VSP from one SP to another SP in the same data storage system. To move a VSP from a first SP to a second SP The VSP need merely be shut down i.e. have its servers stopped on the first SP and resumed i.e. have its servers started up again on the second SP.

Although shows only a single record for a single VSP it is understood that the configuration database may store records like the record for any number of VSPs including all VSPs of the data storage apparatus . During start up of the data storage apparatus or at some other time a computing device of the data storage apparatus reads the configuration database and launches a particular VSP or a group of VSPs on the identified SPs. As a VSP is starting the SP that owns the VSP reads the configuration settings of the configuration file system to configure the various servers of the VSP and to initialize its communication protocols. The VSP may then be operated on the identified SP i.e. the SP may then be operated with the particular VSP s context.

It is understood that VSPs operate in connection with the front end of the IO stack . The VSPs thus remain co located with their respective front ends in modular and gateway arrangements.

At step a network address and a set of host data objects are stored in a data storage apparatus. The set of host data objects are accessible within a namespace of a virtualized storage processor VSP operated by a physical storage processor of the data storage apparatus. The namespace includes only names of objects that are specific to the VSP. For example an IP address of the VSP is stored in a file of a directory of the configuration file system . The VSP runs on the SP of the data storage apparatus . A set of host objects including host file systems and and LUN are also stored in the data storage apparatus . These host objects are made accessible within the namespace of the VSP by mounting these data objects to mount points MP MP within the root file system and thus merging their namespaces with that of the root file system . The resulting merged namespace includes only names of objects that are specific to the VSP .

At step a transmission is received by the physical storage processor over the network from a host computing device. The transmission is directed to a network address and includes an IO request designating a pathname to a host data object to be written or read. For example the SP receives a transmission over the network from one of the hosts N . The transmission is directed to a particular IP address and includes an IO request e.g. one of N . The IO request designates a location of a host data object to be written or read e.g. a pathname for a file based object or a block designation for a block based object . The location may point to any of the host file systems or to the LUN or to any file or offset range accessible through the host file systems or or the LUN respectively. The location may also point to a vVol or VMDK for example or to any other object which is part of the namespace of the VSP .

At step the host data object designated by the IO request is identified by i matching the network address to which the transmission is directed with the network address stored for the VSP to identify the VSP as the recipient of the IO request and ii locating the host data object within the namespace of the VSP using the pathname. For example each of the VSPs stores an IP address in its configuration file system . When an IO request is received an interface running within the front end of the IO stack checks the IP address to which the IO request is directed and matches that IP address with one of the IP addresses stored for the VSPs . The VSP whose IP address matches the IP address to which the IO request is directed is identified as the recipient of the IO request. The IO request arrives to the SP with a pathname to the host data object to be accessed. The front end looks up the designated pathname within the identified VSP to identify the particular data object to which the IO request is directed.

At step the IO request is processed to complete the requested read or write operation on the identified host data object. For example the front end and the back end process the IO request to perform an actual read or write to the designated host data object on the storage .

An improved technique has been described for managing host data in a data storage apparatus. The technique provides virtualized storage processors VSPs as substantially self describing and independent entities. Each VSP has its own namespace which is independent of the namespace of any other VSP. Each VSP also has its own network address. Hosts may thus access VSPs directly without having to include path information relative to the SP on which the VSP is operated. VSPs can thus be moved from one physical SP to another with little or no disruption to hosts which may continue to access the VSPs on the new SPs using the same paths as were used when the VSPs were running on the original SPs.

As used throughout this document the words comprising including and having are intended to set forth certain items steps elements or aspects in an open ended fashion. Also and unless explicitly indicated to the contrary the word set as used herein indicates one or more of something. Although certain embodiments are disclosed herein it is understood that these are provided by way of example only and the invention is not limited to these particular embodiments.

Having described certain embodiments numerous alternative embodiments or variations can be made. For example embodiments have been shown and described in which host file systems LUNs vVols VMDKs and the like are provided in the form of files of underlying lower deck file systems. Although this arrangement provides advantages for simplifying management of VSPs and for unifying block based and file based operations the use of lower deck file systems is merely an example. Indeed host file systems LUNs vVols VMDKs and the like may be provided for VSPs in any suitable way.

Also although the VSPs are shown and described as userspace constructs that run within the container this is also merely an example. Alternatively different VSPs may be provided in separate virtual machines running on the SP . For example the SP is equipped with a hypervisor and a virtual memory manager and each VSP runs in a virtual machine having a virtualized operating system.

Also the improvements or portions thereof may be embodied as a non transient computer readable storage medium such as a magnetic disk magnetic tape compact disk DVD optical disk flash memory Application Specific Integrated Circuit ASIC Field Programmable Gate Array FPGA and the like shown by way of example as medium in . Multiple computer readable media may be used. The medium or media may be encoded with instructions which when executed on one or more computers or other processors implement the various methods described herein. Such medium or media may be considered an article of manufacture or a machine and may be transportable from one machine to another.

By way of example only there is a storage pool a user interface and a configuration database . The storage pool is formed from a set of storage units and as mentioned earlier contains a set of lower deck file systems also see . The user interface takes input from and provides output to a user e.g. an administrator and may take the form of a user workstation or terminal in communication with the processing circuitry i.e. the physical SPs of the data storage apparatus to provide the user with a command line interface or GUI.

As shown in the storage pool provides storage for VSPs A A which are owned by the physical SP A . In particular a lower deck file A contains a VSP configuration file system A which defines a personality for the VSP A also see . Similarly another lower deck file A contains another VSP configuration file system A which defines a personality for the VSP A .

Additionally and as shown in a lower deck file A contains a host file system A for use by a host. Similarly another lower deck file A contains another host file system A for use by a host. The personalities or operating environments in which the host file systems A A reside are defined by the VSP configuration file systems A A respectively. Recall that the VSP configuration file systems and host file systems are mounted to the respective root file systems or root structures of the VSPs see dashed lines in and also see .

Furthermore and as shown in the storage pool further provides storage for a VSP B which is owned by the physical SP B . In particular a lower deck file B contains a VSP configuration file system B which defines a personality for the VSP B and a lower deck file B contains a host file system B for use by a host. Again the VSP configuration file system B and the host file system B are mounted to the root file system of the VSP B .

It should be understood that the configuration database includes a set of records also see which is used to manage and track ownership of various constructs objects of the data storage apparatus . Along these lines the configuration database indicates for each VSP a particular physical SP that owns that VSP . Likewise the configuration database indicates for each lower deck file a particular VSP that owns that that lower deck file i.e. the particular VSP to which that lower deck file is mounted and so on.

During operating time T it should be understood that the physical SP A processes host input output I O requests directed to the host file systems A A which are mounted to the VSPs A A respectively. Similarly the physical SP B processes host I O requests directed to the host file system B which is mounted to the VSP B .

Now suppose that a user e.g. a data storage administrator wishes to manually move the VSP A from the physical SP A to the physical SP B for load balancing purposes. For example the user may wish to improve the user experience e.g. response time quality of service etc. relating to host data access. Along these lines it may be that traffic to each of the host file systems A A continues to be substantially heavy while traffic to the host file system B continues to be relatively light.

To move the VSP A from the physical SP A to the physical SP B the user issues a move command . In particular the user enters the move command into the user interface . A suitable syntax may be as follows 

where nas move VSP to newSP is the specific instruction name source VSP A is an option identifying the source VSP VSP A and destination SPB is an option identifying the destination SP.

In response to the move command the processing circuitry of the data storage apparatus effectively moves the VSP A from the physical SP A to the physical SP B . In particular the processing circuitry accesses the records of the configuration database also see to identify the VSP root file system of the VSP A and to identify which file systems are mounted to the VSP root file system. The processing circuitry also confirms that the physical SP A currently owns the VSP A .

The processing circuitry then effectuates VSP load balancing illustrated by the arrow in . Along these lines the physical SP A may need to un mount the upper deck file systems that are mounted to the root of the VSP A namely VSP configuration file system A and host file system A . Additionally the physical SP A may need to release control over the lower deck files A A and perhaps others such as a file containing the VSP root file system i.e. the physical SP A may need to close unlock etc. these files. Any queued or uncompleted I O requests should be attended to e.g. drained completed etc. prior to relinquishing access.

Next and as illustrated in the physical SP B obtains control of the VSP root file system of the VSP A and the lower deck files A A . To illustrate that this transferred VSP is now the second VSP on the physical SP B this VSP is now referred to by reference number VSP B in . The physical SP B mounts the VSP configuration file system A and host file system A to the VSP root file system of the VSP B also see . Additionally the processing circuitry of the data storage apparatus updates the records of the configuration database to indicate that the VSP B resides on the physical SP B i.e. physical SP B is the owner of the VSP B and that the VSP configuration file system A and the host file system A are mounted to the VSP root file system see dashed lines from the file systems A A to the VSP B in .

It should be understood that movement of the VSP configuration file system A and the host file system A were not required during load balancing. Rather these upper deck file systems were simply re mounted to the VSP following movement of only VSP root objects from the physical SP A to the physical SP B .

At this point it should be understood that load balancing may be effectuated automatically. In particular the VSP move command is capable of being issued automatically in response to electronic detection of a load balancing event i.e. one or more changes in the operation of the data storage apparatus can trigger the move command . Along these lines a running script or application may call a procedure i.e. the VSP move command via an application programming interface API .

In some arrangements a simple event triggers load balancing. For example load balancing may be initiated in response to a level of central processing unit CPU utilization on a particular physical SP exceeding a predetermined CPU utilization threshold e.g. 60 75 80 etc. . As another example load balancing may be initiated in response to the difference in CPU utilization between physical SPs reaching a particular amount and so on.

In some arrangements a load balancing evaluation operation is performed before moving any VSP among the physical SPs . Such a load balancing evaluation operation may be performing in response to an event e.g. based on CPU utilization or periodically e.g. nightly weekly etc. .

An example load balancing evaluation operation involves generating a ranking i.e. a list of existing VSPs which are accessed by each physical SP . The ranking or sorting is based on a set of heuristics such as measured network traffic to each of the existing VSPs measured I O statistics to each of the existing VSPs measured number of connections to each of the existing VSPs combinations thereof and so on. The particular VSP to move is then based on selecting a particular VSP from the ranking e.g. the highest ranked VSP the lowest ranked VSP etc. .

Additionally a mechanism exists to prevent load balanced VSPs from bouncing among the physical SPs . Such a mechanism prevents VSPs from moving back and forth unnecessarily and further prevents starving out access to the VSPs if the VSPs are moved among the physical SPs too frequently.

In some arrangements in response to a VSP being moved to a new physical SP the processing circuitry of the data storage apparatus starts a counter to measure the amount of time that the new physical SP has had access to the VSP . The new physical SP maintains ownership of the VSP while the counter indicates an amount of time that is less than a predefined time threshold. Such operation prevents the particular VSP from moving from the new physical SP within an amount of time that is less than the predefined time threshold i.e. prevents bounce .

In some arrangements load balancing of VSPs occurs in tandem with trespassing block based lower deck objects among physical SPs i.e. moving block based processing from a source physical SP to a destination physical SP . Along these lines load balancing is capable of moving only VSPs only block based processing or both. Moreover load balancing decisions are capable of being based solely on file based statistics solely on block based statistics or a combination of both e.g. CPU utilization based on both file based and block based operations . Accordingly there is maximum load balancing flexibility available.

At the source physical SP of the data storage apparatus relinquishes access to a set of VSP definitions that define the particular VSP. In the example of the VSP A relinquishes access to the configuration file system A . Recall that the VSP configuration file system stores VSP data that provides a personality of the VSP i.e. that creates an operating environment for one or more host file systems also see . Such operation may involve accessing the the configuration database to determine which physical SP currently owns the particular VSP also see .

At the destination physical SP obtains access to the set of VSP definitions that define the particular VSP. In the example of the VSP B obtains access to the configuration file system A . The particular VSP is load balanced from the source physical SP to the destination physical SP upon the destination physical SP obtaining access to the set of VSP definitions that define the particular VSP.

As described above improved techniques are directed to performing VSP load balancing in which a VSP moves from one physical SP to another physical SP on a data storage apparatus . Such a VSP is capable of being used by any physical SP of the data storage apparatus to create an appropriate operating environment within which to access a particular host file system . Load balancing of the VSP enables precise positioning of the VSP on a physical SP based on a variety of load balancing criteria such as CPU utilization network traffic input output I O statistics number of connections and so on. Accordingly the performance and operation of the data storage apparatus may be finely tuned optimized for improved quality of service.

While various embodiments of the present disclosure have been particularly shown and described it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the present disclosure as defined by the appended claims.

For example it should be understood that the above described load balancing techniques are capable of being extended to move VSPs among physical storage processors on different data storage apparatus . In these situations the underlying file systems are transported between the different data storage apparatus as well e.g. via replication.

Additionally it should be understood that various components described above are capable of being implemented in or moved to the cloud i.e. to remote computer resources distributed over a network. Here the various computer resources may be distributed tightly e.g. a server blade farm in a single facility or over relatively large distances e.g. over a campus in different cities coast to coast etc. . In these situations the network connecting the resources is capable of having a variety of different topologies including backbone hub and spoke loop irregular combinations thereof and so on. Additionally the network may include copper based data communications devices and cabling fiber optic devices and cabling wireless devices combinations thereof etc. Furthermore the network is capable of supporting LAN based communications SAN based communications combinations thereof and so on.

Further although features are shown and described with reference to particular embodiments hereof such features may be included in any of the disclosed embodiments and their variants. Thus it is understood that features disclosed in connection with any embodiment can be included as variants of any other embodiment whether such inclusion is made explicit herein or not. Those skilled in the art will therefore understand that various changes in form and detail may be made to the embodiments disclosed herein without departing from the scope of the invention. Such modifications and enhancements are intended to belong to various embodiments of the disclosure.

