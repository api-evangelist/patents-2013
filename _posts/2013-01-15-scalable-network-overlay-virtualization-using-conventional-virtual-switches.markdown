---

title: Scalable network overlay virtualization using conventional virtual switches
abstract: In one embodiment, a system includes a server running a virtualization platform, the virtualization platform including logic adapted for creating one or more virtual machines (VMs) and logic adapted for managing a virtual switch (vSwitch), a controller in communication with the server, the controller including logic adapted for assigning a media access control (MAC) address and a virtual local area network (VLAN) identifier (ID) to each of the one or more VMs, wherein a specific tenant to which the one or more VMs belongs is indicated using a tenant ID derived from the VLAN ID, the MAC address, or a combination thereof. Other systems, methods, and computer program products are also described according to more embodiments.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09116727&OS=09116727&RS=09116727
owner: Lenovo Enterprise Solutions (Singapore) Pte. Ltd.
number: 09116727
owner_city: Singapore
owner_country: SG
publication_date: 20130115
---
The present invention relates to data center infrastructure and more particularly this invention relates to providing overlay functionality using conventional virtual switches vSwitches .

Network virtualization is an emerging data center and cloud computing trend which aims to virtualize a network as seen by end stations in a way that greatly simplifies network provisioning in multi tenant environments as well as traditional environments. One of the more common techniques of achieving network virtualization is to use network overlays where tunnels are established between edge network switches to which end stations e.g. Virtual Machines VMs connect. The tunnel is typically implemented by encapsulating packets transmitted by a source end station into an overlay header that transports the packet from the source switch to a target switch via an internet protocol IP based network media access control MAC in IP or an Ethernet network MAC in MAC . The overlay header includes an identifier ID that uniquely identifies the virtual network. The target switch tunnel end point strips off the overlay header encapsulation and delivers the original packet to the destination end station via conventional network connections. In addition to this tunneling mechanism the edge switches participate in an address discovery protocol which may be learning flooding based or lookup based. Internet Engineering Task Force IETF proposals such as Virtual eXtensible Local Area Network VXLAN and Network Virtualization using Generic Routing Encapsulation NVGRE propose some network virtualization frame formats and protocols. All current proposals use a 24 bit virtual network identifier VNID that is double the size of the 12 bit Virtual Local Area Network VLAN ID used in traditional networks. This effectively removes the 4K limit that is imposed if VLANs are used for rudimentary network virtualization as has been attempted by several solution providers recently .

An overlay network may be implemented using a number of VMs with a virtualization platform controlling the processing of networking packets in and out of each VM and one or more VMs may be associated with the overlay network. A virtualization platform processes the networking traffic associated with each overlay network and or VM using predefined properties and policies for the corresponding overlay network and or VM. As the number of overlay networks increases so does the processing load requirements of the virtualization platform.

Existing network virtualization solutions in the industry are based on modifying the vSwitches within the virtualization platform. These solutions have several disadvantages such as cost e.g. this functionality often demands the most expensive license interoperability issues due to virtualization platform dependency e.g. different virtualization platforms deploy different techniques to achieve network virtualization and this means these solutions do not interoperate with each other unless some sort of translation gateways are used and this introduces inefficiencies in the data path and resource requirements if implemented within virtualization platforms these mechanisms take up valuable processor cycles especially on older hardware that does not support offloads for tunneled packets . Some solutions to this problem either require changes to the virtualization platforms e.g. implementing the tunnel end point operation in software or are limited in scale e.g. only allow approximately 4000 VMs instead of approximately 16 million supported by current overlay solutions implemented on virtualization platforms .

Accordingly it would be beneficial to have a solution which supports more than 4000 VMs while not requiring changes to existing virtual switches.

In one embodiment a system includes a server running a virtualization platform the virtualization platform including logic adapted for creating one or more virtual machines VMs and logic adapted for managing a virtual switch vSwitch a controller in communication with the server the controller including logic adapted for assigning a media access control MAC address and a virtual local area network VLAN identifier ID to each of the one or more VMs wherein a specific tenant to which the one or more VMs belongs is indicated using a tenant ID derived from the VLAN ID the MAC address or a combination thereof.

In another embodiment a method for providing overlay functionality to a server includes creating one or more VMs on a server running a virtualization platform the server being in communication with a network and a controller assigning a MAC address and a VLAN ID to each of the one or more VMs wherein a specific tenant to which the one or more VMs belongs is indicated using a tenant ID derived from the VLAN ID the MAC address or a combination thereof.

In yet another embodiment a computer program product for providing overlay functionality to a server includes a computer readable storage medium having computer readable program code embodied therewith the computer readable program code including computer readable program code configured for creating one or more VMs on a server running a virtualization platform the server being in communication with a network and a controller and computer readable program code configured for assigning a MAC address and a VLAN ID to each of the one or more VMs wherein a specific tenant to which the one or more VMs belongs is indicated using a tenant ID derived from the VLAN ID the MAC address or a combination thereof.

Other aspects and embodiments of the present invention will become apparent from the following detailed description which when taken in conjunction with the drawings illustrate by way of example the principles of the invention.

The following description is made for the purpose of illustrating the general principles of the present invention and is not meant to limit the inventive concepts claimed herein. Further particular features described herein can be used in combination with other described features in each of the various possible combinations and permutations.

Unless otherwise specifically defined herein all terms are to be given their broadest possible interpretation including meanings implied from the specification as well as meanings understood by those skilled in the art and or as defined in dictionaries treatises etc.

It must also be noted that as used in the specification and the appended claims the singular forms a an and the include plural referents unless otherwise specified.

According to various embodiments a portion of a source media access control MAC address a virtual local area network VLAN identifier ID or a combination thereof such as an organizational unique identifier OUI field in an Ethernet header of a packet may be used as a tenant ID which may then be used by a physical switch located at an edge of a network such as a top of rack ToR switch an embedded blade switch etc. to implement tunnel end point TEP functionality to be used for network overlay. Although some OUIs may be reserved and unusable the number of usable OUIs is still in the order of a plurality of millions much larger than the approximately 4000 4096 limit imposed by VLANs and much closer to the 16 million tenant identifiers IDs achieved by VXLAN and NVGRE.

In one general embodiment a system includes a server running a virtualization platform the virtualization platform including logic adapted for creating one or more virtual machines VMs and logic adapted for managing a virtual switch vSwitch a controller in communication with the server the controller including logic adapted for assigning a MAC address and a VLAN ID to each of the one or more VMs wherein a specific tenant to which the one or more VMs belongs is indicated using a tenant ID derived from the VLAN ID the MAC address or a combination thereof.

In another general embodiment a method for providing overlay functionality to a server includes creating one or more VMs on a server running a virtualization platform the server being in communication with a network and a controller assigning a MAC address and a VLAN ID to each of the one or more VMs wherein a specific tenant to which the one or more VMs belongs is indicated using a tenant ID derived from the VLAN ID the MAC address or a combination thereof.

In yet another general embodiment a computer program product for providing overlay functionality to a server includes a computer readable storage medium having computer readable program code embodied therewith the computer readable program code including computer readable program code configured for creating one or more VMs on a server running a virtualization platform the server being in communication with a network and a controller and computer readable program code configured for assigning a MAC address and a VLAN ID to each of the one or more VMs wherein a specific tenant to which the one or more VMs belongs is indicated using a tenant ID derived from the VLAN ID the MAC address or a combination thereof.

As will be appreciated by one skilled in the art aspects of the present invention may be embodied as a system method or computer program product. Accordingly aspects of the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as logic a circuit module or system. Furthermore aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon.

Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a non transitory computer readable storage medium. A non transitory computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the non transitory computer readable storage medium include the following a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory a portable compact disc read only memory CD ROM a Blu Ray disc read only memory BD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a non transitory computer readable storage medium may be any tangible medium that is capable of containing or storing a program or application for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a non transitory computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device such as an electrical connection having one or more wires an optical fiber etc.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable radio frequency RF etc. or any suitable combination of the foregoing.

Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on a user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer or server may be connected to the user s computer through any type of network including a local area network LAN storage area network SAN and or a wide area network WAN any virtual networks or the connection may be made to an external computer for example through the Internet using an Internet Service Provider ISP .

Aspects of the present invention are described herein with reference to flowchart illustrations and or block diagrams of methods apparatuses systems and computer program products according to various embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams may be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that may direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

In use the gateway serves as an entrance point from the remote networks to the proximate network . As such the gateway may function as a router which is capable of directing a given packet of data that arrives at the gateway and a switch which furnishes the actual path in and out of the gateway for a given packet.

Further included is at least one data server coupled to the proximate network and which is accessible from the remote networks via the gateway . It should be noted that the data server s may include any type of computing device groupware. Coupled to each data server is a plurality of user devices . Such user devices may include a desktop computer laptop computer handheld computer printer and or any other type of logic containing device. It should be noted that a user device may also be directly coupled to any of the networks in some embodiments.

A peripheral or series of peripherals e.g. facsimile machines printers scanners hard disk drives networked and or local storage units or systems etc. may be coupled to one or more of the networks . It should be noted that databases and or additional components may be utilized with or integrated into any type of network element coupled to the networks . In the context of the present description a network element may refer to any component of a network.

According to some approaches methods and systems described herein may be implemented with and or on virtual systems and or systems which emulate one or more other systems such as a UNIX system which emulates an IBM z OS environment a UNIX system which virtually hosts a MICROSOFT WINDOWS environment a MICROSOFT WINDOWS system which emulates an IBM z OS environment etc. This virtualization and or emulation may be enhanced through the use of VMWARE software in some embodiments.

In more approaches one or more networks may represent a cluster of systems commonly referred to as a cloud. In cloud computing shared resources such as processing power peripherals software data servers etc. are provided to any system in the cloud in an on demand relationship thereby allowing access and distribution of services across many computing systems. Cloud computing typically involves an Internet connection between the systems operating in the cloud but other techniques of connecting the systems may also be used as known in the art.

The workstation shown in includes a Random Access Memory RAM Read Only Memory ROM an I O adapter for connecting peripheral devices such as disk storage units to the one or more buses a user interface adapter for connecting a keyboard a mouse a speaker a microphone and or other user interface devices such as a touch screen a digital camera not shown etc. to the one or more buses communication adapter for connecting the workstation to a communication network e.g. a data processing network and a display adapter for connecting the one or more buses to a display device .

The workstation may have resident thereon an operating system such as the MICROSOFT WINDOWS Operating System OS a MAC OS a UNIX OS etc. It will be appreciated that a preferred embodiment may also be implemented on platforms and operating systems other than those mentioned. A preferred embodiment may be written using JAVA XML C and or C language or other programming languages along with an object oriented programming methodology. Object oriented programming OOP which has become increasingly used to develop complex applications may be used.

Referring now to a conceptual view of an overlay network is shown according to one embodiment. In order to virtualize network services other than simply providing a fabric path connectivity between devices services may be rendered on packets as they move through the gateway which provides routing and forwarding for packets moving between the non virtual network s and the Virtual Network A and Virtual Network B . The one or more virtual networks exist within a physical real network infrastructure . The network infrastructure may include any components hardware software and or functionality typically associated with and or used in a network infrastructure including but not limited to switches connectors wires circuits cables servers hosts storage media operating systems applications ports I O etc. as would be known by one of skill in the art. This network infrastructure supports at least one non virtual network which may be a legacy network.

Each virtual network may use any number of virtual machines VMs . In one embodiment Virtual Network A includes one or more VMs and Virtual Network B includes one or more VMs . In other embodiments VMs or other physical devices may be connected to either Virtual Network A or Virtual Network B as would be understood by one of skill in the art and make use of the advantages and resources afforded to the devices by the virtual networks. As shown in the VMs are not shared by the virtual networks but instead are exclusively included in only one virtual network at any given time.

According to one embodiment the overlay network may include one or more cell switched domain scalable fabric components SFCs interconnected with one or more distributed line cards DLCs .

Components of an overlay network typically identify where to route packets based on a virtual network identifier referred to as a VNI or VNID. This is typically a 24 bit code or number which excludes 0x0 and 0xFFFFFF. The overlay network has the capability of tunneling Layer 2 L2 packets over the Layer 3 L3 network by encapsulating the L2 packets into an overlay header also referred to as virtual tunnel end point VTEP management. This may be performed using virtual extensible local area network VXLAN or some other overlay capable protocol such as locator ID separation protocol LISP overlay transport virtualization OTV Network Virtualization using Generic Routing Encapsulation NVGRE etc.

The packet may also be encapsulated in a user datagram protocol UDP and internet protocol IP UDP IP header. The overlay network may include one or more point to point tunnels and or point to multipoint tunnels. In addition any of these tunnels may be created removed altered and modified based on any number of factors such as new devices being added to the overlay network removal of devices from the overlay network startup of any end devices i.e. devices managing tunnel end points such as virtual overlay network gateways virtualization platforms switches capable of overlay functionality etc.

In order for a device to manage a tunnel there needs to be a mapping between an original packet s source address destination address and a tunnel identifier. In this way a physical server is capable of forwarding the encapsulated original packet to the proper destination device.

With reference to a server is shown according to one approach. As shown the server includes a virtualization platform which provides and manages a virtual switch . To provide overlay functionality to the server the virtualization platform also interacts with a plurality of discrete software engines such as a tunnel manager an ARP and forwarding information base FIB manager an engine for providing internet protocol multicast IPMC support a packet encapsulation and de capsulation engine and any other overlay enhancing software engines as known in the art. The virtualization platform also provides support for any number of VMs shown in as VM 0 VM 1 VM 2 . . . VM n. The VMs may be arranged in one or more virtual networks each virtual network may have a different virtual network identifier VNID indicated as VNID 1 VNID 2 VNID 3 etc. in . The number and arrangement of the VMs in the exemplary virtual networks is not meant to be limiting on what configurations are possible as the Figures only show exemplary arrangements of VMs and virtual networks.

In addition as shown in according to one approach the server includes a physical NIC which manages and provides for communications between a Layer 2 L2 network and the server in one approach. The NIC includes one or more networking ports adapted for communicating with the L2 network and the server . When a packet is encapsulated in an overlay header by the packet encap de cap engine the NIC simply passes the packet through to a destination specified in an outer header of the overlay encapsulated packet. According to one embodiment 

In order to bridge between virtualized and non virtualized networks the packet may be delivered to an overlay network gateway for further forwarding and or routing outside of the virtual network in which the server is located.

Referring now to a detailed view of an overlay network gateway is shown according to one approach. The overlay network gateway comprises a plurality of ports which may be used for packet ingress and or packet egress. Any number of ports may be present depending on the arrangement and capabilities of the overlay network gateway such as 16 ports 32 ports 64 ports 128 ports etc. The overlay network gateway also comprises logic adapted for performing look ups for L3 addresses and devices logic adapted for performing look ups for overlay addresses and devices logic adapted for performing look ups for L2 addresses and devices logic adapted for performing look ups for tunnel addresses and devices logic adapted for performing packet pre classification logic adapted for performing egress tunnel management actions VTEP logic adapted for performing egress overlay actions logic adapted for performing egress L2 actions logic adapted for performing egress forwarding actions along with possibly other packet functionality . Any or all of this logic may be implemented in hardware or software or a combination thereof. For example separate modules for each distinct logic block may be implemented in one or more processors in one embodiment. The processor s may include but are not limited to an application specific integrated circuit ASIC a field programmable gate array FPGA a central processing unit CPU a microcontroller MC a microprocessor or some other processor known in the art.

In order to increase performance of a virtual networking server using a virtualization platform overlay network traffic processing may be provided by utilizing a NIC having overlay gateway functionality. Specifically the NIC having overlay gateway functionality may be adapted for providing some or all functionality of an overlay network gateway such as managing virtual tunnel end points VTEPs address resolution protocol ARP cache handling ARP learning packet encapsulation and de capsulation for each overlay network various look up functionality for L2 L3 and or overlay networks egress packet processing and forwarding etc. This overlay network gateway functionality may be processed exclusively via the NIC or in combination with other overlay devices possibly using separate modules and or processors and the processed traffic may be delivered to the virtualization platform for final delivery to the appropriate VM s or the traffic may be sent down to the network for delivery to a destination VTEP or another IP based address.

Now referring to a server using a NIC having overlay network gateway functionality is shown according to another approach. The server also comprises a virtualization platform which provides and manages a virtual switch . The virtualization platform also provides support for any number of virtual machines VMs shown in as VM 0 VM 1 VM 2 . . . VM n. The VMs may be arranged in one or more virtual networks each virtual network may have a different virtual network identifier VNID indicated as VNID 1 VNID 2 VNID 3 etc. in . The number and arrangement of the VMs in the exemplary virtual networks is not meant to be limiting on what configurations are possible as the Figures only show exemplary arrangements of VMs and virtual networks.

The NIC makes use of one or more processors in order to provide overlay network gateway functionality such that all overlay network gateway functionality may be offloaded onto the NIC in one embodiment. This overlay network gateway functionality may be embedded as modules within the NIC and may include but is not limited to performing look ups for L3 addresses and devices performing look ups for overlay addresses and devices performing look ups for L2 addresses and devices performing look ups for tunnel addresses and devices performing packet pre classification performing egress tunnel management actions VTEP performing egress overlay actions performing egress L2 actions performing egress forwarding actions along with possibly other packet functionality as well as any other overlay network gateway functionality known in the art.

According to some embodiments the egress tunnel management actions module may be adapted for providing VTEP origination and termination operations the various look up modules and may be adapted for providing look up operations for packets having an unknown address and forwarding information for packets having a known address the various egress modules and may be adapted for handling the forwarding and or sending operations for packets.

In order to provide an interface between the NIC and the virtualization platform in the server a NIC driver may be provided in the server which understands and supports the overlay network gateway functionality that has been provided by the NIC and may manage the NIC accordingly.

In this way according to one embodiment in order to adapt a server for use with a different overlay protocol a NIC driver capable of interfacing with the overlay network protocol supported by an installed NIC having overlay network gateway functionality may be implemented in the server . In one non limiting example if a server is capable of VXLAN overlay functionality and this server is to be used in an environment using Microsoft s Hyper V an accelerated NIC capable of Hyper V may be installed in the server and a NIC driver supporting Hyper V may be implemented in the server thereby rendering the server capable of operating in an overlay environment which utilizes Hyper V as the overlay protocol. One of skill in the art would recognize that this procedure may be used regardless of the overlay protocol.

In one embodiment if the server already has a NIC driver installed that is capable of supporting a certain overlay protocol and a NIC having overlay network gateway functionality capable of providing the certain overlay protocol is installed in the server then the NIC may simply be inserted into an available PCIe slot of the server and may operate once installed in a plug and play type arrangement.

As shown in a system is shown according to one embodiment. As shown the system comprises a server running a virtualization platform and a controller in communication with the server . Either or both of the server and or the controller may be in communication with the network in some approaches. The virtualization platform comprises at least logic adapted for creating one or more VMs and logic adapted for managing a virtual switch vSwitch . The controller comprises logic adapted for assigning a MAC address and a VLAN ID to each of the one or more VMs . Accordingly a specific tenant to which the one or more VMs belongs may be indicated using a tenant ID derived from the VLAN ID the MAC address or a combination thereof.

In one approach an OUI field in the MAC address may be used as a tenant ID to indicate a specific tenant to which the one or more VMs belongs. That is to say the OUI is used to distinguish a MAC address as belonging to a particular tenant s VM . In other approaches any combination of the MAC address field and the VLAN ID of an Ethernet packet that is emitted by the virtualization platform on behalf of a VM may be used to derive a tenant ID by a physical switch which receives such a packet. The source OUI is a straight forward derivative of this broader concept but is not the only place in the Ethernet packet where the tenant ID may be stored and later derived.

The controller may be any type of controller which is capable of performing management operations on the server and VMs of the server such as cloud orchestration platform COP controller. Particularly the controller is capable of specifying MAC addresses and VLAN IDs for VM interfaces.

In one approach in order to provide overlay functionality to a server without modifying existing hardware in a system which prior to implementation has no overlay functionality or has overlays provided by a virtualization platform a NIC an overlay gateway or a combination thereof a high level controller such as an IBM developed Cloud Manager or some other suitable controller may create VMs on a server and assign the VMs a MAC address from a pool of available MAC addresses. In other words the controller may create and manage a pool of available MAC addresses with which to assign to VMs .

In this approach any portion of the MAC address VLAN ID or a combination thereof may be used to indicate a tenant ID. In particular the MAC OUI may be used as the tenant ID and therefore the controller manages a mapping between the tenant and its corresponding unique OUI field value unique VLAN ID or combination thereof which is being used as a tenant ID. That is to say that the controller manages a mapping between each tenant and corresponding identifying information that is used as the tenant ID such as a unique OUI field value unique VLAN ID combination thereof or some other values from the Ethernet packet.

The controller also controls which VM belongs to which tenant by virtue of having full control over each VM s source MAC address. In other words each MAC address assigned to a VM includes a OUI prefix which is unique to a specific tenant to which the MAC address belongs when the OUI field value or portion thereof is used to designate the tenant ID.

The controller may perform these tasks without substantial change. In one approach additional changes to the controller may be that the controller comprises logic adapted for maintaining the mapping between the tenants and portions of their OUIs VLAN IDs or combinations thereof and logic adapted for providing MAC addresses and VLAN IDs to VMs based on which tenant a given VM belongs to. No changes are required to be made to existing virtualization platforms switches NICs or other associated hardware. This solution makes use of support for basic features that are supported by all virtualization platforms in the industry today such as application programming interfaces APIs that provide an interface for the controller to create VMs assign a MAC address and a VLAN ID to a VM etc.

In another embodiment the system or any portion thereof such as the controller the server the virtualization platform etc. may include logic adapted for providing overlay functionality to packets logic adapted for using the VLAN ID a source MAC address or a combination thereof to derive a tenant ID from a packet for tunnel end point TEP operation during encapsulation processing. Furthermore the logic adapted for providing overlay functionality may be provided by any device capable of providing overlay functionality. According to various embodiments a NIC within the server may provide overlay functionality the virtualization platform may provide overlay functionality and or a physical switch may provide overlay functionality. The physical switch may be in communication with the server and may be located on an edge of the network thereby providing a VTEP for any overlay traffic directed to or received from the system . A device which is located at the edge of the network is a network device which is a closest link to the server e.g. it is a first hop network device from the server .

However in one preferred embodiment the overlay functionality may be implemented in the edge physical switch with the server virtualization platform NIC and other components being unaware of the network virtualization provided by the edge physical switch . Furthermore this allows the remaining components other than the edge physical switches to be substantially or completely unmodified while still providing network virtualization.

Accordingly this solution may be used to bring network virtualization technology to basic vSwitches from any vendor which can provide significant cost savings for each vSwitch and massive cost savings on a network wide basis.

Now referring to an edge physical switch is shown in more detail according to one embodiment. As shown the edge physical switch includes tunnel end point capability which allows the edge physical switch to terminate and originate tunnels for use in overlay network virtualization. Particularly some modules are shown all some or none of which may actually be implemented in an edge physical switch in various approaches. Any modules may be incorporated into an edge physical switch such as a tunnel manager an ARP and FIB manager an engine for providing IPMC support a packet encapsulation and de capsulation engine and any other overlay enhancing software engines as known in the art.

On the physical edge switches the following changes may be implemented to allow the network virtualization to be enacted. Instead of using the VLAN ID as a proxy for the Virtual Network Identifier VNID a feature that is being added to hardware by multiple merchant silicon vendors today the source MAC address or portion thereof such as the OUI field the VLAN ID or some combination thereof may be used for TEP operation at the encapsulation side.

Each edge physical switch also includes one or more ports such as Ethernet ports capable of connecting to a variety of devices networks servers controllers etc. One or more of these ports is connected to a server and through the server one or more VMs .

The TEP operates by originating the overlay tunnel for packets from end points e.g. VMs that are directly connected to a switch s server downlink ports. In the reverse direction the TEP terminates the tunnel and delivers the packet to the destination end point. This is standard TEP operation that is being included in hardware by merchant silicon vendors currently. In one approach since the OUI is used to identify the tenant the VLAN ID in the packet may be used for learning purposes so that packets sent to the VM from the TEP are tagged appropriately for final delivery.

In another aspect another mechanism to go along with the technique described above prevents local switching within the virtualization platform which could defeat any tenant tenant rules present since tenant information is no longer being stored as VLAN IDs. In this embodiment the system may include logic adapted for preventing local switching within the virtualization platform thereby reserving full control over network packet processing and policies to the physical switch that implements overlay functionality by assigning a unique VLAN ID to each VM interface connected to a given vSwitch such that the given vSwitch is forced to assume that all destinations that a VM attempts to interact with are outside the virtualization platform . Preventing local switching within the virtualization platform is advantageous in scenarios where there is a desire to perform more complex traffic engineering or policy enforcement in the network that requires all packet switching to be handled by the external physical switch .

In this embodiment the virtualization platform is unaware of network virtualization and therefore the virtualization platform is incapable of applying policies to traffic in the virtual network.

To this end a mechanism where the VLAN ID associated with a VM is not used to group VMs of a given tenant together but instead serves to prevent local switching within the virtualization platform may be provided. In this embodiment two VMs belonging to the same tenant may still be allocated different VLANs but the same MAC OUI. Packets sent from one VM to the another within the same tenant may be sent by the traditional vSwitch to the uplink which is connected to the edge physical switch . The edge physical switch then performs the TEP operations and sends the packet back to the virtualization platform with a change in the VLAN ID.

An interesting and useful side effect of this mechanism is that since the source MAC OUI includes the tenant ID there is no need for the encapsulation header to carry the tenant ID hence there is no need to use another header to carry this information. Eliminating the tenant ID field from the encapsulation header makes it possible to eliminate the UDP header required by current overlay solutions. For example the tunnel header may include only the outer Ethernet and IP headers at the minimum. In this embodiment an overlay encapsulation header for a packet may be created which does not comprise a UDP header when the packet is not locally switched within the virtualization platform .

Exiting overlay solutions require the UDP header as part of encapsulation. For an IPv6 case the UDP checksum is mandatory. Calculating the UDP checksum consumes many cycles or processing especially ASIC based TEPs which cannot compute the UDP checksum at line rate. According to one embodiment there is no need to have the UDP header in the encapsulation. Instead the MAC in IPv6 tunnel may be used to build the overlay network. By avoiding UDP implementing ASIC based TEP is much simpler and software based TEPs save processor cycles.

In another approach another mechanism to go along with the technique described above which prevents local switching within the virtualization platform creates and manages a pool of available VLAN IDs for each vSwitch with which to assign to VMs . The controller may maintain a pool of up to approximately 4000 VLAN IDs per vSwitch that the controller is adapted for assigning to VMs which connect to the virtualization platform in such a way that a given VLAN ID is assigned to at most one VM . That is to say a unique VLAN ID is assigned to each VM connected to a vSwitch. In this embodiment the VLAN ID no longer identifies the network to which a VM connects instead it is solely used to prevent local switching such that the external tunnel end point may switch based on policies specified therein.

In this embodiment each vSwitch is capable of hosting up to about 4000 VMs one for each available unique VLAN ID. The VLAN IDs are locally unique but are no longer globally unique which allows any given VLAN ID to be used on each vSwitch without any chance of conflict between their usage.

In a further embodiment when a VM is migrated from one a first vSwitch to a second vSwitch the first VM s VLAN ID is maintained and a VLAN ID for any VM on the second vSwitch which conflicts with the first VM s VLAN ID is reassigned with disrupting the VMs on the second vSwitch . This is a method of handling live migration. When a VM migrates there may be a need on some virtualization platforms to connect the VM to the same VLAN on the target vSwitch second vSwitch as on the source vSwitch first vSwitch since the virtualization platforms continues to believe that it is implementing a traditional VLAN scheme. To account for cases where the VLAN ID of a VM is assigned to another VM on the target virtualization platform the VLAN IDs are swapped in a non intrusive way such that the migrating VM is assigned the VLAN to which it needs to maintain the scheme and the second VM from which the VLAN was stolen is not disrupted in any way.

Solving the problem for multidestination packets multicast and broadcast requires additional processing. This is because the layer 2 multicast MAC address is derived based on the layer 3 IP multicast group address and if this is used unchanged the packet will not have an OUI in the inner Destination MAC DMAC address for the tunnel termination point to use as a tenant ID.

Two solutions are possible for multidestination packet processing either of which may be used based on the capabilities of the hardware. In a first approach the tunnel encapsulation point may be used to rewrite the original DMAC address with an address which is constructed from a target OUI and a layer 3 IP multicast destination address included in the packet with an individual group I G bit set to 1 or 0 if that is the proper identifying bit to indicate that this is a multicast packet. This preserves the OUI tenant ID lookup semantics at tunnel egress for unicast packets. In this embodiment the system may include logic adapted for constructing an address from a target OUI and a layer 3 internet protocol IP multicast destination address included in a packet logic adapted for rewriting a destination MAC address in the packet with the constructed address and logic adapted for setting an I G bit to indicate multicast packet status.

However if the network has IP multicast support then the forwarding entries at the tunnel edge are updated in order to use the new inner DMAC address. In addition if the packet is destined for more than one target tenant then the tunnel ingress will make a copy of the packet for each tenant using the respective OUI to derive the DMAC addresses.

In another approach the tunnel egress uses the inner source MAC SMAC address OUI to derive the target tenant ID. At this point communication is possible within a tenant group but not across tenant groups. For cases where communication between tenant groups across tenant IDs is requested a rule is applied to the egress port to replicate the packet to each target tenant ID. In a sense this solution shifts the packet replication responsibility to the tunnel egress instead of the tunnel ingress as described in the first approach. In this embodiment the system may include logic adapted for using an OUI from an inner source MAC address to derive a target tenant ID.

Now referring to a method for providing overlay functionality in a server is shown according to one embodiment. The method may be performed in accordance with the present invention in any of the environments depicted in among others in various embodiments. Of course more or less operations than those specifically described in may be included in method as would be understood by one of skill in the art upon reading the present descriptions.

Each of the steps of the method may be performed by any suitable component of the operating environment. For example in various non limiting embodiments the method may be partially or entirely performed by a controller a processor such as a CPU an ASIC a FPGA etc. which may be embedded in and or operate within a server a NIC or computer program code embedded in a computer readable storage medium within a controller or server etc.

As shown in method may initiate with operation where one or more VMs are created on a server running a virtualization platform. The server may be in communication with a network and a controller.

In operation a MAC address is assigned to each of the one or more VMs wherein a specific tenant to which the one or more VMs belongs is indicated using a tenant ID derived from portions of a VLAN ID a MAC address or a combination thereof.

In optional operation a pool of available MAC addresses with which to assign to VMs is created and managed.

In optional operation a mapping between each tenant and a corresponding tenant ID represented by portions of the VLAN ID the MAC address or a combination thereof is created and managed.

Any other embodiment described in relation to system may be included in FIG. s method as well as would be understood b one of skill in the art. Other embodiments described herein may be implemented in the method as well as would be understood by one of skill in the art upon reading the present descriptions.

According to another embodiment the method may be embodied as a computer program product for providing overlay functionality to a server. The computer program product may comprise a computer readable storage medium having computer readable program code embodied therewith the computer readable program code comprising computer readable program code configured for creating one or more VMs on a server running a virtualization platform the server being in communication with a network and a controller and computer readable program code configured for assigning a MAC address to each of the one or more VMs wherein a specific tenant to which the one or more VMs belongs is indicated using a tenant ID derived from a VLAN tag field an OUI field in the MAC address or a combination thereof. Other embodiments described herein may be implemented in the computer program product as well as would be understood by one of skill in the art upon reading the present descriptions.

While various embodiments have been described above it should be understood that they have been presented by way of example only and not limitation. Thus the breadth and scope of an embodiment of the present invention should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

