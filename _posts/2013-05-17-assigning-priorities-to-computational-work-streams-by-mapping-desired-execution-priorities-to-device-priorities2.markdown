---

title: Assigning priorities to computational work streams by mapping desired execution priorities to device priorities
abstract: One embodiment sets forth a method for assigning priorities to kernels launched by a software application and executed within a stream of work on a parallel processing subsystem. First, the software application assigns a desired priority to a stream using a call included in the API. The API receives this call and passes it to a driver. The driver maps the desired priority to an appropriate device priority associated with the parallel processing subsystem. Subsequently, if the software application launches a particular kernel within the stream, then the driver assigns the device priority associated with the stream to the kernel before adding the kernel to the stream for execution on the parallel processing subsystem. Advantageously, by assigning priorities to streams and, subsequently, strategically launching kernels within the prioritized streams, an application developer may fine-tune the software application to increase the overall processing efficiency of the software application.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09632834&OS=09632834&RS=09632834
owner: NVIDIA Corporation
number: 09632834
owner_city: Santa Clara
owner_country: US
publication_date: 20130517
---
The present invention generally relates to general purpose computing and more specifically to techniques for assigning priorities to streams of work.

A typical parallel processing subsystem that may include one or more graphics processing units GPUs is capable of very high performance using a relatively large number of small parallel execution threads on dedicated programmable hardware processing units. The specialized design of such parallel processing subsystems usually allows these subsystems to efficiently perform certain tasks such as rendering 3 D scenes or computing the product of two matrices using a high volume of concurrent computational and memory operations

To fully realize the processing capabilities of advanced parallel processing subsystems subsystem functionality may be exposed to application developers through one or more application programming interfaces APIs of calls and libraries. Among other things doing so enables application developers to tailor their software application to optimize the way parallel processing subsystems function. In one approach to developing a software application the software application developer may implement an algorithm by dividing the work included in the algorithm into streams of work components e.g. computational and memory operations that may be executed in parallel on the parallel processing subsystem. Within each stream a sequence of work components executes in issue order on the parallel processing subsystem. In contrast work components included in different streams may run concurrently and may be interleaved.

In one approach to scheduling work components a scheduler within the parallel processing subsystem allocates parallel processing subsystem resources in discrete time slices to work components included in concurrent streams. When allocating a particular parallel processing subsystem resource the scheduler typically selects the appropriate work component in issue order. In other words the scheduler selects the work component that was issued least recently from the set of work components that may be successfully performed using the resource. Further if more than one appropriate parallel processing subsystem is available the scheduler typically executes the work component using the appropriate parallel processing subsystem resource that has been least recently used.

One drawback to this approach is that some work components are more sensitive to latency than others. And the execution of work components in strict issue order on the least recently used parallel processing subsystem resources may cause software applications that include latency sensitive work components to execute with unacceptable latency and throughput. For example if a software application is performing video decoding and encoding using a pipelined workflow and the first few stages in the pipeline are occupying most of the parallel processing subsystem resources processing a fifth frame then the processing of a fourth frame by the last stage in the pipeline could be delayed. Consequently the overall latency of the fourth frame could cause jitter in frame rates.

Another drawback to the above approach is that some software applications may be sensitive to execution order because they include inter stream dependencies between work components requiring varying execution times. For example a software application performing high performance simulation of large molecular systems e.g. NAMD may use parallel molecular dynamics algorithms that include work components whose required execution times vary dramatically. Often such algorithms divide the work into multiple streams with inter dependencies. For example a first stream could include halo work components whose results are required by dependent work components included a second stream. And the first stream could also include internal work components whose results are not required by work components included in any other stream. Further the halo work components could require much shorter execution times than the internal work components. If the internal work components occupy most of the subsystem resources then the halo work components could get stalled i.e. blocked until any internal work components preceding the halo components complete . Because dependent work components included in the second stream require the results from halo work components included in the first stream the second stream could be blocked until the blocking internal work components included in the first stream complete execution. Consequently overall throughput of the software application could be adversely impacted.

As the foregoing illustrates what is needed in the art is a more effective technique to schedule work submitted to parallel processing subsystems.

One embodiment of the present invention sets forth a method for prioritizing computational work components included in a software application for execution within a parallel processing subsystem. The method includes receiving a current request from the software application to launch a computational work component within a stream identifying a device priority associated with the stream and submitting the computation work component to the parallel processing subsystem for execution within the stream at the device priority.

Other embodiments of the present invention include without limitation a computer readable storage medium including instructions that when executed by a processing unit cause the processing unit to implement aspects of the techniques described herein as well as a system that includes different elements configured to implement aspects of the techniques described herein.

By implementing the disclosed techniques together the API and the driver enable application developers to tailor their software applications to leverage advanced prioritization functionality included in parallel processing subsystems. By exploiting the prioritization functionality the performance of the software applications may be improved by decreasing latency and increasing throughput.

In the following description numerous specific details are set forth to provide a more thorough understanding of the present invention. However it will be apparent to one of skill in the art that the present invention may be practiced without one or more of these specific details.

In one embodiment the parallel processing subsystem incorporates circuitry optimized for graphics and video processing including for example video output circuitry and constitutes a graphics processing unit GPU . In another embodiment the parallel processing subsystem incorporates circuitry optimized for general purpose processing while preserving the underlying computational architecture described in greater detail herein. In yet another embodiment the parallel processing subsystem may be integrated with one or more other system elements in a single subsystem such as joining the memory bridge CPU and I O bridge to form a system on chip SoC .

In operation the CPU is the master processor of the computer system controlling and coordinating operations of other system components. In particular the CPU issues commands that control the operation of the parallel processing subsystem . Those commands may originate within a software application resident in the system memory and executing on the CPU . A compute unified device architecture CUDA software stack is also resident in the system memory . CUDA is a general purpose computing environment which uses the parallel processing subsystem to perform various computing tasks. The CUDA software stack is a set of programs included in the CUDA that issue and manage general purpose computations that operate on components in the parallel processing subsystem . The software application may generate requests i.e. calls for processing by the CUDA software stack to produce a desired set of results. In alternate embodiments the CUDA software stack may be replaced with any set of software programs that expose and manage parallel processing subsystem functionality. For example the CUDA software stack may be replaced with a different general purpose compute software stack or a graphics software stack. Further the CUDA software stack may be configured to inter operate with one or more additional software stacks.

It will be appreciated that the system shown herein is illustrative and that variations and modifications are possible. The connection topology including the number and arrangement of bridges the number of CPUs and the number of parallel processing subsystems may be modified as desired. For instance in some embodiments system memory is connected to CPU directly rather than through a bridge and other devices communicate with system memory via memory bridge and CPU . In other alternative topologies parallel processing subsystem is connected to I O bridge or directly to CPU rather than to memory bridge . In still other embodiments I O bridge and memory bridge might be integrated into a single chip instead of existing as one or more discrete devices. Large embodiments may include two or more CPUs and two or more parallel processing subsystems . The particular components shown herein are optional for instance any number of add in cards or peripheral devices might be supported. In some embodiments switch is eliminated and network adapter and add in cards connect directly to I O bridge .

To efficiently achieve a set of results using the parallel processing subsystem the software application may pass application CUDA requests to the CUDA software stack . As shown the CUDA software stack includes a CUDA runtime application programming interface API and a CUDA driver . The CUDA runtime API includes calls and libraries that expose the functionality of the parallel processing subsystem to application developers. And the CUDA driver is configured to translate the application CUDA requests received by the CUDA runtime API to lower level commands that execute on components within the parallel processing subsystem . In particular the CUDA driver may submit one or more CUDA streams to the parallel processing subsystem for execution within the parallel processing subsystem . Each CUDA stream may include any number including zero of kernels i.e. functions interleaved with any number including zero of other work components such as memory operations. Each kernel has a defined entrance and exit and typically performs a computation on each element of an input list. Within each CUDA stream the kernels execute in issue order on the parallel processing subsystem . However kernels included in different CUDA streams may run concurrently and may be interleaved.

As noted previously herein parallel processing subsystems are typically configured to allocate discrete time slices of parallel processing subsystem resources to streams using a particular scheduling technique. Again the prior art scheduling technique used by many parallel processing subsystems is to assign the next time slice of the least recently used appropriate resource to the stream in which the least recently issued un executed work component was issued first. In other words these parallel processing subsystems execute work components in a strict issue order on the least recently used appropriate parallel processing subsystem resource. For example suppose that a kernel KA were the least recently issued un executed work component included in stream A and kernel KB were the least recently issued un executed work component included in stream B. Further suppose that the kernel KB were to have been issued before the kernel KA. A prior art parallel processing subsystem would typically allocate the least recently used appropriate parallel processing subsystem resource to execute stream B work components before allocating any similar parallel processing subsystem resource to execute stream A work components.

In contrast parallel processing subsystem includes advanced prioritization functionality that enables more flexible scheduling. More specifically parallel processing subsystem includes functionality that enables prioritization of kernels and preemption of currently executing kernels. Thus the parallel processing subsystem may schedule kernels in priority order as opposed to strict issue order . And the parallel processing subsystem may preempt a lower priority kernel executing on a parallel processing subsystem resource in favor of one or more higher priority kernels as opposed to using the resource used least recently . For example the parallel processing subsystem may receive a high priority kernel KHIGH interrupt a low priority kernel KLOW executing on a parallel processing subsystem resource execute the high priority kernel KHIGH on the resource and then resume executing the lower priority kernel KLOW on the resource.

Advantageously to facilitate the development of software applications that efficiently utilize the advanced prioritization capabilities of the parallel processing subsystem the CUDA software stack is configured to support the available prioritization functionality of the parallel processing subsystem . More specifically the CUDA runtime API exposes prioritization functionality by including a set of valid CUDA priorities and by supporting application CUDA requests that enable the software application to request a specific desired CUDA priority to be associated with a specific CUDA stream . And the CUDA driver is configured to receive application CUDA requests from the CUDA runtime API process prioritization information included in the application CUDA requests and launch each kernel included in a specific CUDA stream with the priority associated with the specific CUDA stream

As shown to facilitate the process described above the CUDA driver includes valid device priorities and CUDA stream data . The valid device priorities represent priorities defined and supported by the parallel processing subsystem . The CUDA driver may store data associated with CUDA streams as CUDA stream data included in any available memory resource such as the system memory . Upon receiving a particular application CUDA request to associate a desired CUDA priority with a particular CUDA stream the CUDA driver maps the desired CUDA priority to a stream specific device priority. As part of the mapping process the CUDA driver analyzes the valid CUDA priorities in conjunction with the valid device priorities . The CUDA driver then stores the stream specific device priority along with stream specific identifying information such as a stream name or a stream ID as CUDA stream data .

Upon receiving a particular application CUDA request to launch a particular kernel within a particular CUDA stream the CUDA driver is configured to access the CUDA stream data to determine whether the CUDA stream is associated with a device priority. If the CUDA stream is associated with a device priority then the CUDA driver includes the device priority when submitting the kernel to the parallel processing system for launch within the CUDA stream . If the CUDA stream is not associated with a device priority then the CUDA driver may include a default device priority when submitting the kernel to the parallel processing system for launch within the CUDA stream . Alternatively in some embodiments the CUDA driver may submit the kernel to the parallel processing system without including any device priority.

The valid CUDA priorities represent any number including zero of priorities that the application developer may use in the software application to fine tune the performance of the software application . Further the valid CUDA priorities may use higher numbers to represent higher priorities or lower numbers to represent higher priorities and may specify any number as a default priority. For example if the CUDA priorities included three numbers with 1 representing a high priority 0 representing both a medium and a default priority and 1 representing a low priority then the application developer could incorporate three levels of priority into the software application . Similarly the valid device priorities may include any number including zero of priorities that are supported by the parallel processing subsystem . The valid device priorities may use higher numbers to represent higher priorities or lower numbers to represent higher priorities and may specify any number as a default priority. For example the valid device priorities could include sixty four numbers with 63 representing the highest priority and 0 representing the lowest priority.

Again to meaningfully convey the CUDA priorities included in the application CUDA requests to the parallel processing subsystem the CUDA driver is configured to map valid CUDA priorities included in the application CUDA requests to corresponding valid device priorities . More specifically the CUDA driver uses the valid CUDA priorities and the valid device priorities to determine a corresponding device priority for each requested CUDA priority. The CUDA driver may be configured to execute any technically feasible mapping algorithm. For example suppose that the valid CUDA priorities were to include 5 to represent a high CUDA priority 6 to represent a medium CUDA priority and 7 to represent a low CUDA priority. Further suppose that the valid device priorities were to include 20 to represent the highest device priority through 1 to represent the lowest device priority. The CUDA driver could be configured to map CUDA priority 5 to device priority 5 CUDA priority 6 to device priority 4 and CUDA priority 7 to device priority 3. Alternatively the CUDA driver could be configured to map CUDA priority 5 to device priority 15 CUDA priority 0 to device priority 10 and CUDA priority 1 to device priority 5.

As part of the mapping algorithm the CUDA driver may be configured to reserve any number of valid device priorities to support other functionality included in the parallel processing subsystem . For example CUDA dynamic parallelism allows one kernel to launch another kernel on the parallel processing subsystem without the involvement of the CPU . As persons skilled in the art will understand CUDA dynamic parallelism may be implemented using device priorities. More specifically device priorities may be used to ensure that a parent keArnel relinquishes enough parallel processing subsystem resources to allow the child kernel to execute on the parallel processing subsystem . For example support for CUDA dynamic parallelism may include assigning a child kernel a priority level higher than the default dynamic parallelism priority level that is assigned to the parent kernel. Consequently the CUDA driver may be configured to reserve one or more valid device priorities for child kernels and exclude those reserved device priorities when mapping a requested CUDA priority to a corresponding device priority. Further the CUDA runtime API may be configured to include a limited set of valid CUDA priorities to reflect the reserved valid device priorities or expected use cases.

Advantageously the CUDA software stack is configured to support a variety of parallel processing subsystems in combination with a variety of software applications . More specifically the CUDA software stack supports parallel processing subsystems that that provide various levels including none of support for prioritization. And the CUDA software stack supports software applications that request CUDA priorities to be associated with any number including none of CUDA streams . For example if an alternate embodiment included a parallel processing subsystem that provided no support for prioritization then the CUDA driver would not include device priorities when including kernels in CUDA streams . And if another alternate embodiment included a software application that did not specify any CUDA priorities then the CUDA driver could be configured to include a default device priority when including kernels in CUDA streams .

Using the techniques disclosed herein the CUDA stack enables the software developers to strategically use prioritization to reduce latency and increase throughput of the software application thereby increasing the speed at which the software application executes on the parallel processing subsystem . For example in one embodiment the software application could implement a video decoding and encoding algorithm using a three stage pipelined workflow. And if the application developer wanted to reduce the possibility of jitter caused by latency then the application developer could strategically assign priorities to kernels.

More specifically the application developer could specify a low priority CUDA stream a medium priority CUDA stream and a high priority CUDA stream . Subsequently the application developer could specify the launch of kernels associated with the first stage within the low priority CUDA stream . Further the application developer could specify the launch of kernels associated with the second stage within the medium priority CUDA stream . And the application developer could specify the launch of kernels associated with the third stage within the high priority CUDA stream . As persons skilled in the art will understand this prioritization in conjunction with the processing performed by the CUDA stack would direct a parallel processing subsystem that is capable of prioritizing kernels to prioritize the third stage in the pipeline before the second stage and the second stage before the first stage. This prioritization would ensure that a particular frame is not starved for processing resources by subsequent frames. Consequently the overall latency of each frame could be decreased compared to prior art scheduling approaches and could therefore be less likely to cause jitter in frame rates.

In the example shown in the computer system includes two parallel processing subsystems not shown GPUA and GPUB both of which include functionality that enable prioritization of kernels and preemption of currently executing kernels. And the valid CUDA priorities include three numbers with 1 representing a high priority 0 representing a medium and default priority and 1 representing a low priority. The CUDA requests are issued by the software application as part of a simulation of large molecular systems e.g. NAMD . The software application has divided the work to be executed on the parallel processing subsystems into six separate kernels based on the required execution time for each of the kernels and any interactions between the kernels.

More specifically the software application has divided the work into two internal kernels internalA and internalB and four halo kernels haloABpart haloABpart haloBApart and haloBApart . Each of the internal kernels requires a much longer execution time than any of the halo kernels. Further each of the internal kernels has no dependencies on any other kernel. In contrast each of the halo kernels has a dependency i.e. either the halo kernel requires results from another halo kernel or the results from the halo kernel are required by another halo kernel on one of the other halo kernels. More specifically haloABpart requires results from halo ABpart and therefore haloABpart must complete execution before haloABpart begins executing. Similarly haloBApart requires results from halo BApart and therefore haloBApart must complete execution before haloBApart begins executing. Further other algorithms not detailed herein included in the software application require the results from both the haloABpart and the haloBApart kernels.

As the application CUDA requests illustrate the software application has taken advantage of both available parallel processing subsystems i.e. GPUA and GPUB as well as two i.e. high 1 and low 1 of the three valid CUDA priorities . More specifically based on the execution length and dependencies between the kernels the software application has strategically assigned each of the six kernels to one of four CUDA streams . Ahigh is a high priority CUDA stream to be executed on GPUA. Alow is a low priority CUDA stream to be executed on GPUA. Bhigh is a high priority CUDA stream to be executed on GPUB. And Blow is a low priority CUDA stream to be executed on GPUB. 

The first application CUDA request set device GPUA informs the CUDA software stack that components within the parallel processing subsystem GPUA are to be used to execute application CUDA requests until a new set device application CUDA request is received.

The second application CUDA request create stream Ahigh priority 1 directs the CUDA driver to create a new CUDA steam Ahigh for execution within the parallel processing subsystem GPUA. The second application CUDA request also directs the CUDA driver to subsequently launch kernels within the CUDA stream Ahigh with a device priority corresponding to CUDA priority 1. As previously disclosed herein to determine the corresponding device priority the CUDA driver maps the CUDA priority 1 to a corresponding device priority included in the valid device priorities . In the example shown in the valid device priorities include 64 numbers with 0 representing the highest priority and 63 representing the lowest priority. Further the CUDA driver in this example is configured to map CUDA priority 1 to device priority 59 and CUDA priority 1 to device priority 63. Consequently the CUDA driver associates the CUDA stream Ahigh with device priority 59 and stores this association in the CUDA stream data .

The third application CUDA request launch kernel haloABpart stream Ahigh requests that kernel haloABpart be launched within CUDA stream Ahigh . As previously disclosed in conjunction with before including the kernel haloABpart in the CUDA stream Ahigh the CUDA driver accesses the CUDA stream data to determine whether the CUDA stream Ahigh is associated with a device priority. Because the CUDA stream data includes the association of CUDA stream Ahigh with the device priority 59 the CUDA driver adds the kernel haloABpart together with the device priority 59 to the CUDA stream Ahigh. 

The fourth application CUDA request wait on haloBApart stream Ahigh causes the CUDA driver to add one or more corresponding synchronizing operations to the CUDA stream Ahigh. These operations instruct the parallel processing subsystem GPUA to wait until haloBApart has completed execution before executing the remaining work components including launching kernels included in CUDA stream Ahigh. The fifth application CUDA request launch kernel haloBApart stream Ahigh requests that kernel haloBApart be launched within CUDA stream Ahigh . This request causes the CUDA driver to add the kernel haloBApart together with the device priority 59 to the CUDA stream Ahigh. Together the fourth and fifth application CUDA requests ensure that the results from kernel haloBApart are available before kernel haloBApart is executed.

The sixth application CUDA request create stream Alow priority 1 causes the CUDA driver to create a new CUDA steam Alow for execution within the parallel processing subsystem GPUA. The sixth application CUDA request also directs the CUDA driver to subsequently launch kernels within the CUDA stream Alow with a device priority 63. And the seventh application CUDA request launch kernel internalA stream Alow causes the CUDA driver to add the kernel internalA together with the device priority 63 to the CUDA stream Alow. 

In a similar fashion the next seven application CUDA requests create two prioritized CUDA streams a high priority CUDA stream Bhigh and a low priority CUDA stream Blow for execution by components included in the parallel processing system GPUB. The CUDA stream Bhigh includes three sets of one or more commands. The first set of commands directs the parallel processing system GPUB to launch the kernel haloBApart whose results are required by the kernel haloBApart that is included in stream Ahigh with a device priority of 59. The second set of commands directs the parallel processing system GPUB to suspend processing of work components included in the stream Bhigh until the kernel haloABpart has finished executing on the parallel processing subsystem GPUA. And the third set of commands directs the parallel processing system GPUB to launch a kernel haloABpart at a device priority of 59. The CUDA stream Blow includes commands that direct the parallel processing system GPUB to launch a kernel internalB at a device priority of 63. 

As also shown within each set of stream commands associated with a particular stream ID and therefore a particular CUDA stream the stream commands execute sequentially in an intra stream command execution order . Again within each CUDA stream the kernels execute in issue order on the associated parallel processing subsystem . Therefore the intra stream command execution order within each set of stream commands corresponds to the issue order of the corresponding application CUDA requests of . Again kernels included in different sets of stream commands i.e. different CUDA streams may run concurrently and may be interleaved.

Advantageously as previously disclosed both the CUDA stack and the parallel processing subsystems GPUA and GPUB include functionality that enable prioritization of kernels and preemption of currently executing kernels. As previously outlined the CUDA streams Ahigh and Alow are configured to run on the parallel processing subsystem GPUA and the CUDA streams Bhigh and Blow are configured to run on the parallel processing subsystem GPUB. Consequently two kernels included in two different CUDA streams may run concurrently. In other words a kernel included in the CUDA streams Ahigh or Alow may run concurrently with a kernel included in the CUDA stream Bhigh or Blow. 

As shown the first kernel HaloABpart included in the CUDA stream Ahigh is associated with a higher priority than the first kernel InternalA included in the CUDA stream Alow. Therefore the kernel HaloABpart is the first command to begin executing on the parallel processing subsystem GPUA. Concurrently because the first kernel HaloBApart included in the CUDA stream Bhigh is associated with a higher priority than the first kernel InternalB included in the CUDA stream Blow the kernel HaloBApart is the first command to begin executing on the parallel processing subsystem GPUB. 

The kernel haloBApart included in the CUDA stream Bhigh finishes executing before the kernel haloABpart finishes executing. And the next command included in the CUDA stream Bhigh instructs parallel processing subsystem GPUB to wait until the kernel haloABpart has completed executing before continuing. Consequently the parallel processing subsystem GPUB begins executing the kernel internalB included in the CUDA stream Blow. Concurrently the parallel processing subsystems GPUA and GPUB perform synchronizing operations that result in no interrupt for wait on haloBApart . In other words the commands included in the CUDA streams to implement wait on haloBApart within the CUDA stream Ahigh have been satisfied and are no longer relevant to the GPUA command execution order .

When the kernel haloABpart finishes executing on the parallel processing subsystem GPUA the parallel processing subsystems GPUA and GPUB perform synchronizing operations that result in a preemptive interrupt for wait on haloABpart . Because the CUDA stream Bhigh is no longer blocked waiting for the kernel haloABpart to complete the parallel processing subsystem GPUB reevaluates the GPUB command execution order . The next command included in the CUDA stream Bhigh is the launch of the kernel haloABpart. The kernel haloABpart is associated with a higher device priority than the currently executing kernel internal B included in the separate CUDA stream Blow . Consequently the parallel processing subsystem GPUB interrupts the lower priority kernel internal B. The parallel processing subsystem GPUB then executes the higher priority kernel haloABpart and subsequently resumes executing the lower priority kernel internal B. 

Concurrently when the kernel haloABpart finishes executing on the parallel processing subsystem GPUA the parallel processing subsystem GPUA evaluates the device priorities of the least recently issued commands included in each of the two CUDA streams Alow and Ahigh. More specifically the parallel processing subsystem GPUA evaluates the kernel internalA included in the CUDA stream Alow and the kernel haloBApart included in the CUDA stream Ahigh. Since the kernel haloBApart is associated with a higher device priority than the kernel internalA the parallel processing subsystem GPUA executes the kernel haloBApart before the kernel internalA. 

Advantageously by assigning CUDA priorities to the CUDA streams and subsequently strategically launching kernels within the prioritized CUDA streams the software application optimizes resources. More specifically the software application ensures that the parallel processing subsystems GPUA and GPUB are continuously executing kernels. And each kernel associated with a high device priority is executed as soon as any associated input dependencies are satisfied. Further this advanced scheduling ensures that the haloABpart and haloBApart kernels are executed as soon as possible. And as outlined in this expedites the execution of other algorithms not detailed herein included in the software application that require the results from the haloABpart and the haloBApart kernels as inputs. In contrast in prior art approaches to scheduling the haloABpart and haloBApart kernels might get blocked waiting for one or more of the haloABpart haloABpart internalA and internalB kernels to finish executing. Further in these prior art approaches other algorithms included in the software application might get blocked waiting for the haloABpart and haloBApart kernels to finish executing. As illustrated by this example by exposing the prioritization functionality of the parallel processing subsystem the disclosed techniques may enable application developers to tailor the software application to reduce stalls and blockages. And reducing stalls and blockages may decrease latency increase throughput and decrease the execution time of the software application .

As shown a method begins at step where the software application sends an application CUDA request to the CUDA runtime API assigning a specific CUDA priority for the launch of kernels within a specific CUDA stream . At step the CUDA runtime API passes the application CUDA request to the CUDA driver . At step the CUDA driver maps the CUDA priority to an appropriate device priority based on the valid CUDA priorities and the valid device priorities . The CUDA driver may perform this mapping in any technically feasible manner. Further as previously disclosed the CUDA driver may reserve one or more valid device priorities to support other functionality such as dynamic parallelism. At step the CUDA driver stores the mapped device priority in the CUDA stream data associated with the specified CUDA stream . The CUDA stream data may be stored in any available memory resource such as system memory and may be included in any applicable data structure.

As shown a method begins at step where the software application sends an application CUDA request to the CUDA runtime API requesting the launch of a specific kernel within a specific CUDA stream . At step the CUDA runtime API passes the application CUDA request to the CUDA driver . At step the CUDA driver accesses the CUDA stream data associated with the specified CUDA stream to determine whether the CUDA stream data includes a device priority. If at step the CUDA driver determines that the CUDA stream data includes a device priority then the method proceeds to step . At step the CUDA driver launches the specified kernel with the device priority included in the CUDA stream data within the specified CUDA stream and the method terminates.

If at step the CUDA driver determines that the CUDA stream data does not include a device priority then the method proceeds to step . At step the CUDA driver launches the specified kernel with a default device priority within the specified CUDA stream and the method terminates. In alternate embodiments the CUDA driver may launch the specified kernel without a device priority within the specified CUDA stream .

It will be appreciated that the system and techniques described in are illustrative and that variations and modifications are possible. For example in alternate embodiments the CUDA software stack including the CUDA runtime API and the CUDA driver CUDA programming model and CUDA language may be replaced with may be replaced with any set of software programs that expose and manage parallel processing subsystem functionality e.g. OpenCL . Further the CUDA streams may be replaced with any generalized or specialized streams and the kernels included in the CUDA streams or any other streams may be replaced with any computational operation that may be launched on the parallel processing subsystem

In sum a CUDA runtime API exposes advanced parallel processing subsystem prioritization capabilities e.g. scheduling kernels by priority and preempting currently executing lower priority kernels to launch higher priority kernels to application developers. More specifically the CUDA runtime API defines a set of valid CUDA priorities and provides calls that enable the software application to request a specific CUDA priority for the launch of kernels within a specific CUDA stream. The CUDA driver maps the CUDA priorities to the valid device priorities i.e. the priorities supported by the parallel processing subsystem . Subsequently the CUDA driver includes the device priority associated with a particular CUDA stream when submitting kernels to the parallel processing system for processing within the particular CUDA stream.

Because the CUDA driver maps the valid CUDA priorities to the valid device priorities the CUDA driver supports the same software application on parallel processing subsystems that include a variety of prioritization functionality. For example if the software application does not specify CUDA priorities then the CUDA driver launches each kernel with the default device priority. And if the software application specifies CUDA priorities but the parallel processing subsystem does not support multiple priorities then the CUDA driver launches each kernel without a priority.

While the foregoing is directed to embodiments of the present invention other and further embodiments of the invention may be devised without departing from the basic scope thereof. For example aspects of the present invention may be implemented in hardware or software or in a combination of hardware and software. One embodiment of the invention may be implemented as a program product for use with a computer system. The program s of the program product define functions of the embodiments including the methods described herein and can be contained on a variety of computer readable storage media. Illustrative computer readable storage media include but are not limited to i non writable storage media e.g. read only memory devices within a computer such as CD ROM disks readable by a CD ROM drive flash memory ROM chips or any type of solid state non volatile semiconductor memory on which information is permanently stored and ii writable storage media e.g. floppy disks within a diskette drive or hard disk drive or any type of solid state random access semiconductor memory on which alterable information is stored.

The invention has been described above with reference to specific embodiments. Persons of ordinary skill in the art however will understand that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. The foregoing description and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense.

