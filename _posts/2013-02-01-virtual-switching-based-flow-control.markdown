---

title: Virtual switching based flow control
abstract: Flow control of data packets in a network may be enabled to at least one side of a virtual switching interface to provide a lossless environment. In some embodiments, wherever two buffer queues are in communication with at least one buffer queue being connected to a virtual switching interface, flow control may be used to determine if a threshold has been exceeded in one of the buffer queues. When exceeded, the transmission of data packets may cease to one of the buffer queues to prevent packet dropping and loss of data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09485188&OS=09485188&RS=09485188
owner: INTERNATIONAL BUSINESS MACHINES CORPORATION
number: 09485188
owner_city: Armonk
owner_country: US
publication_date: 20130201
---
The present invention relates to data networking and more specifically to virtual switching based flow control.

Packet loss in data networks is a well known cause of poor performance in networking systems. Typically physical switches are incorporated with measures to minimize packet loss. Ideally a physical network seeks to operate in a lossless environment.

However lossless environments in virtual switch networks may be a more difficult task. Typically virtual components may operate entirely within a single switch or server and or in some cases may span over multiple physical machines. In some cases where the physical network machine has incorporated measures to provide a lossless environment the virtual machine s communicating with the physical machine may not understand the controlling software. Accordingly the physical machine may reject interfacing with a virtual machine that is attempting to transfer data packets through the physical machine.

Thus it may be seen that a virtual networking environment may benefit from the implementation of data flow control that may be compatible with a physical networking environment.

According to an exemplary embodiment of the present invention a computer program product for controlling data traffic in a network with a virtual switch the computer program product comprises a computer readable storage medium having program code embodied therewith. The program code may be readable executable by a processor to determine by the processor that a data packet is designated for queuing up in a buffer queue connected to a virtual switching entity determine by the processor whether a threshold in the buffer queue is exceeded and control by the processor the flow of the data packet to or from the virtual switching entity based on the determination of the threshold in the buffer queue being exceeded.

According to another exemplary embodiment of the present invention a system comprises a first switching interface. A first buffering area may be connected to the first switching interface. A second switching interface may be configured for virtual switching in a data network. A second buffering area may be connected to the first buffering area and to the second switching interface. A processor may be configured to control a flow of data packets between the first buffering area and the second buffering area based on whether a threshold in the second buffering area is exceeded.

According to yet another embodiment of the present invention a method of controlling data packet flow in a network with at least one virtual switching entity comprises receiving a data packet either with a source address from the virtual switching entity or designated with a destination address to the virtual switching entity. It may be determined whether the data packet is designated for queuing in a first buffer queue. The data packet may be queued in the first buffer queue. It may be determined whether a threshold of the first buffer queue has been exceeded. A flow of more data packets either with a source address from the virtual switching entity or with a destination address to the virtual switching entity to the first buffer queue may be controlled if the threshold has been exceeded.

According to yet another embodiment of the present invention a method for building a lossless data transfer environment in a virtualized network comprises generating flow control commands between hardware and software elements in the virtualized network and receiving the flow control commands without packet loss or packet drops at either the hardware or software elements in the virtualized network.

In general embodiments of the subject disclosure may provide flow control of data packets wherever two queues in a virtual network are in communication with one another. More particularly embodiments of the subject disclosure may control packet flow using software defined networking SDN between any input buffer and output buffer within a network using virtual switching to provide a lossless environment in data transfer. An exemplary embodiment may include a lossless Distributed Overlay Virtual Ethernet DOVE SDN program for CEE networks with a vPFC interface. In an alternate embodiment credit based flow control signals from an external source may be used to interface with hardware using a PCI express PCIe or an Infiniband interface.

As will be appreciated by one skilled in the art aspects of the present invention may be embodied as a system method or process or computer program product. Accordingly aspects of the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore aspects of the present invention may take the form of a computer program product embodied in one or more computer readable storage media having computer readable program code embodied thereon.

Any combination of one or more computer readable storage media may be utilized. A computer readable storage medium is an electronic magnetic optical or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a computer readable storage medium is any tangible medium that can store a program for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable RF etc. or any suitable combination of the foregoing.

Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects of the present invention are described below with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams may be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable storage medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable storage medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus may provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

Referring now to a schematic of a computing system is shown. The computing system illustrated is only one example of a suitable cloud computing node and is not intended to suggest any limitation as to the scope of use or functionality of embodiments of the invention described herein. Regardless the computing system is capable of being implemented and or performing any of the functionality set forth herein.

The computing system may be operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use as the computing system may include but are not limited to personal computer systems server computer systems thin clients thick clients handheld or laptop devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputer systems mainframe computer systems and distributed cloud computing environments that include any of the above systems or devices and the like.

The computing system may be described in the general context of computer system executable instructions such as program modules being executed by a computer system. Generally program modules may include routines programs objects components logic data structures and so on that perform particular tasks or implement particular abstract data types. The computing system may be practiced in distributed cloud computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed cloud computing environment program modules may be located in both local and remote computer system storage media including memory storage devices.

As shown in the computing system is shown in the form of a general purpose computing device. The components of the computing system may include but are not limited to one or more processors or processing units a system memory and a bus that couples various system components including the system memory to the processor .

The bus represents one or more of any of several types of bus structures including a memory bus or memory controller a peripheral bus an accelerated graphics port and a processor or local bus using any of a variety of bus architectures. By way of example and not limitation such architectures may include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnects PCI bus.

The computing system may typically include a variety of computer system readable media. Such media could be chosen from any available media that is accessible by computing system including volatile and non volatile media removable and non removable media.

The system memory could include one or more computer system readable media in the form of volatile memory such as a random access memory RAM and or a cache memory . The computing system may further include other removable non removable volatile non volatile computer system storage media. By way of example only a storage system may be provided for reading from and writing to a non removable non volatile magnetic media device typically called a hard drive not shown . Although not shown a magnetic disk drive for reading from and writing to a removable non volatile magnetic disk e.g. a floppy disk and an optical disk drive for reading from or writing to a removable non volatile optical disk such as a CD ROM DVD ROM or other optical media could be provided. The storage system may also include other forms of storage media such as flash memory for example. In such instances each computer system storage media may be connected to the bus by one or more data media interfaces. As will be further depicted and described below the system memory may include at least one program product having a set e.g. at least one of program modules that are configured to carry out the functions of embodiments of the invention.

A program utility having a set at least one of program modules may be stored in the system memory by way of example and not limitation as well as an operating system one or more application programs other program modules and program data. Each of the operating system one or more application programs other program modules and program data or some combination thereof may include an implementation of a networking environment. The program modules generally carry out the functions and or methodologies of embodiments of the invention as described herein.

The computing system may also communicate with one or more external devices such as a keyboard a pointing device a display etc. one or more devices that enable a user to interact with the computing system and or any devices e.g. network card modem etc. that enable the computing system to communicate with one or more other computing devices. Such communication can occur via Input Output I O interfaces . Alternatively the computing system can communicate with one or more networks such as a local area network LAN a general wide area network WAN and or a public network e.g. the Internet via a network adapter . As depicted the network adapter may communicate with the other components of the computing system via the bus . It should be understood that although not shown other hardware and or software components could be used in conjunction with the computing system . Examples include but are not limited to microcode device drivers redundant processing units external disk drive arrays RAID systems tape drives and data archival storage systems etc.

Referring now to a data communication system is shown according to an exemplary embodiment of the present invention. The data communication system may also be referred to as the system . In general the system may be configured to provide a lossless data packet transfer environment by reacting to responding to and generating standard flow control signals. The flow control signals may be for example priority based flow control PFC PCI Express PCIe and Infiniband IB credits. In some embodiments the flow control signals may be On Off grants rate increase decrease signals or S PFC signals. The system may use the flow control signals to transmit or receive packets without packet losses or drops between physical and virtual networks and any combination thereof.

In an exemplary embodiment the system is part of a virtual switching environment. The system may represent any switching interface between two buffering entities that are exchanging data packets. For example the system may include a data packet producer exchanging data packets with a data packet consumer . The data packet producer is the transmitting side of the system while the data packet consumer is the receiving side. In an exemplary embodiment at least one side either the data packet producer or the data packet consumer of the system is a virtual entity for example virtual switches virtual machines hypervisors virtual network interface cards vNICs etc. . The interface between the producer and the consumer may be between two entirely virtual entities. In some embodiments the producer or the consumer may be a virtual switching entity while the other may be a physical switching entity for example a physical network switch or NIC .

The system may be configured to provide flow control of data packets exchanged between the data packet producer and the data packet consumer . The flow control of input data packet streams . . . referred to collectively as input streams and output data packet streams . . . referred to collectively as output streams may be controlled for example by the program . The program may include SDN controller stored in either hardware or software elements of the system . The SDN controller may generate flow control commands and react to flow control between the hardware and software elements. The program may control for example the processor or any networking element for which the system is implemented into to implement any of the following functions or steps. The program may allocate an input buffer area to the data packet producer . The input buffer area may include an input buffer queue dedicated to each input stream . Each input buffer queue may be configured to store a plurality of data packets . While each input buffer queue shows five data packets it will be understood that typically there is room for more packets within a queue . For each input buffer queue the program may designate thresholds for the number of data packets in the input buffer queue that may trigger an action by the processor .

The program may allocate an output buffer area to the data packet consumer . The output area buffer may include an output buffer queue for each output stream . The output buffer queues may be configured to receive data packets provided from the input data streams as allocated by a forwarding engine . The forwarding engine may be configured to handle functions such as virtual switching allocation of data packets through the system and scheduling forwarding of data packets through the system . For each output buffer queue the program may designate thresholds for the number of data packets in the output buffer queue that may trigger an action by the processor .

In an exemplary embodiment the program may be configured to control the flow of data packets into the input buffer queues and or into the output buffer queues to prevent packet loss. In some embodiments the data packet may enter either the input buffer queue or the output buffer queue through a switching device for example a virtual switch or a hypervisor. In some embodiments the data packet may bypass the switching device en route to the output buffer queue . The program may determine if a threshold for a maximum number of data packets in the output buffer queue before receiving more packets has been reached. The threshold for the maximum number of data packets is not necessarily the entire room available in the output buffer queue . When the threshold for a maximum number of data packets is reached the processor may send a command to stop the input data streams from communicating more data packets into one or more of the input buffer queues . Internal flow control signals may be exchanged between the output buffer queues and the input buffer queues signaling when the output buffer queues release enough data packets reaching a threshold signaling room for receipt of new data packets . Also for example the program may stop the data packet producer from sending more data packets when the input buffer queues have reached a threshold for a maximum number of data packets in the input buffer queue .

Thus with flow control opportunities present at two buffering locations of a virtual switching interface the input and output sides a lossless environment may be achieved even in a virtual networking environment. This may provide compatibility with physical networking machines since the interfaces to the buffers may appear to resemble the buffering constraints of a physical buffer space. The compatibility may protect the internal networking interfaces between hardware and software components for example by offloading networking capacity from the physical machines to the virtual machines without fear of packet loss. For example point to point flow control in a virtualized network employing embodiments of the present invention may be realized in otherwise incompatible interfaces.

In some interfaces the data packet producer is software based and transmitting to software based consumer . The data packet producer and the consumer may be from different vendors or have incompatible application programming interfaces APIs . To bridge the incompatibility an On Off flow control loop synchronized via an externally visible packet or signal may be used by the system to control packet flow as described above. The signal types may include for example Converged Enhanced Ethernet CEE PFC frame vPFC incl. S PFC etc. in CEE networks PCIe flow control credit vCDT PCIe in PCIe interfaces including SR IOV Single Rooted I O Virtualization and MR IOV Multi Rooted I O Virtualization offload environments and Infiniband flow control credit vCDT IB for Infiniband interfaces.

In some interfaces the data packet producer is hardware based and transmitting to software based consumer . The data packet producer and the consumer may be from different vendors or have incompatible application programming interfaces APIs . To bridge the incompatibility an On Off flow control loop synchronized via an externally visible packet or signal may be used by the system to control packet flow as described above. The signal types may include for example Converged Enhanced Ethernet CEE PFC frame vPFC incl. S PFC etc. in CEE networks PCIe flow control credit vCDT PCIe in PCIe interfaces including SR IOV Single Rooted I O Virtualization and MR IOV Multi Rooted I O Virtualization offload environments and Infiniband flow control credit vCDT IB for Infiniband interfaces.

In some interfaces the data packet producer is software based and transmitting to hardware based consumer . The data packet producer and the consumer may be from different vendors or have incompatible application programming interfaces APIs . To bridge the incompatibility an On Off flow control loop synchronized via an externally visible packet or signal may be used by the system to control packet flow as described above. The signal types may include for example Converged Enhanced Ethernet CEE PFC frame vPFC incl. S PFC etc. in CEE networks PCIe flow control credit vCDT PCIe in PCIe interfaces including SR IOV Single Rooted I O Virtualization and MR IOV Multi Rooted I O Virtualization offload environments and Infiniband flow control credit vCDT IB for Infiniband interfaces.

Details of the input side buffer control and output side buffer control are described according to the following descriptions of flow charts.

Referring now to a method of controlling an input side of data traffic within a network with a virtual switch is shown according to an exemplary embodiment of the present invention. In some embodiments the processor may receive 201 a flow control signal from an external source. The flow control signal may be for example a buffering credit for example PCIe flow control credit or Infiniband flow control credit. In other embodiments the flow control signal may be internal. The flow control signal may be from either the producing or consuming side of a data packet interface. The receipt of a flow control signal may cause the processor to issue a start command to process data packets in a data exchange between an input buffer I queue and an output buffer O queue. In an exemplary embodiment a virtual switching entity may be connected to either the input buffer queue or the output buffer queue. Accordingly the data packet may be received with a source address from a virtual switching entity or may be designated with a destination address to a virtual switching entity. The processor may wait for receipt of a data packet from a virtual switching entity. The processor may determine the type of data packet received. The data packet may be one designated for queuing in an input buffer queue or may be one designated for forwarding to the output buffer queue. Data packets marked as incoming from a producer may be designated for queue in the input buffer queue. The processor may determine whether the storage in the input buffer queue is more than the threshold for a maximum number of data packets in the input buffer queue. If the threshold has been exceeded then the processor may issue a stop command to the producer to stop sending further data packets. If the threshold for a maximum number of data packets in the input buffer queue has not been exceeded or if the stop command has issued then the processor may check the status of the output buffer queue for room to receive data packets. Also if in step the data packet was marked for forwarding to the output buffer queue then the processor may proceed directly to checking the status of the output buffer queue. The processor may determine whether the storage in the output buffer queue is more than the threshold for a maximum number of data packets in the output buffer queue. If the threshold has been exceeded then the data packet may cycle through the method until the processor determines that the threshold is not exceeded or packets from that queue have been dequeued which creates an opportunity to enqueue more packets. When the output buffer queue storage is below the threshold the processor may remove the data packet from the input buffer queue and may queue up the data packet for the output buffer queue. The processor may determine whether the input buffer queue storage is below the threshold signaling room for receipt of new data packets. If the input buffer queue storage is above the threshold signaling room for receipt of new data packets then the processor may cycle through the method until the processor determines that the input buffer may have room to receive new packets. If the input buffer queue is receiving new data packets then the processor may return to step .

Referring now to a method of controlling an output side of data traffic in a network with a virtual switch is shown according to an exemplary embodiment of the present invention. The processor may wait for receipt of a data packet designated for output through the output buffer queue. The processor may de queue the data packet i.e. release the data packet from the output buffer queue. The processor may determine whether the output buffer queue storage is below the threshold signaling room for receipt of new data packets. If the output buffer queue storage is below the threshold then the processor may issue a command to wake up a forwarding function in the controller and restart sending of data packets from the input buffer queue. After the wake up command or if the output buffer storage is higher than the threshold signaling room for receipt of new data packets then the processor may direct the system to consume the data packet send the packet to its destination . In an exemplary embodiment the packet s destination may be a virtual switching entity or a virtual machine VM hosted on the same virtualization platform. The processor may then wait for receipt of the next data packet.

Referring now to the transmission and reception path scheme between queues in a virtual machine is shown. Assuming a collection of virtualized servers each running a set of virtual machines the servers may be interconnected through a flat L2 fabric. The physical network may have per priority flow control allowing the network administrator to configure one or more priorities as lossless. Without loss of generation a single lossless priority may be used. The physical per priority flow control may be continued into the virtual domain by hypervisor software implementing embodiments of the present invention as described above.

The data packets may travel between processes applications running inside the VMs . The packets may move from one queue to another queue within different software and hardware components. The details of this queueing system are described with emphasis on the flow control mechanisms between each interfacing queue pair. The packet transmission and reception paths are shown in .

After processing within the VM s guest kernel the packets may be transferred to the hypervisor through a virtual adapter vNIC . The hypervisor may send the packets to the virtual switch . The virtual switch assures the communication between VMs and the physical adapter . The packets that have as destination addresses remote VMs not shown may be taken over by a bridge that encapsulates them and moves them to the physical adapter queues. The packets may travel through the physical network represented as physical link and may be delivered to the destination server where they may be taken over by the bridge that decapsulates them and moves them into the destination s virtual switch . The virtual switch does the forwarding and the packets may be received by the hypervisor that forwards them to the guest kernel . After processing in the guest kernel the packets may be delivered to the application . The loss points where data packets are dropped or lost are labeled as wake up and pause resume reception .

On the transmit side the packets may be generated by the user space processes. The process may issue a send system call that copies the packet from user space to the guest kernel space . After the copy the packets may be stored in a sk buff data structure that is enqueued in the TX buffer of the socket opened by the application . The application may be aware if the TX queue is full through the return value of the system call making this operation lossless. The packets from the socket TX buffer are enqueued in the Qdisc associated with the virtual interface. The Qdisc may stores a list of pointer to the packets belonging to each socket. The pointers may be sorted according to the selected discipline i.e. FIFO by default. To avoid packet losses at this step the length of the Qdisc may be increased to match the sum of all socket TX queues. As some may appreciate this change may require only negligible amounts of memory. The Qdisc may try to send the packets by enqueueing them into the adapter TX queue. If the TX queue reaches a threshold typically one MTU below maximum the Qdisc may be stopped and the transmission may be paused thus avoiding losses on the TX path of the kernel. When the TX queue drops below the threshold the Qdisc may be restarted and new packets may be enqueued in the TX queue of the virtual adapter . Thus the entire transmission path in the guest kernel OS may become lossless. The architecture implemented may be based on for example Virtio technology. Hence the virtual adapter queues may be shared between the guest kernel and the underlying hypervisor software running in the user space of the host. The network adapter may inform the hypervisor when new packets are enqueued in the TX queue of the adapter . The hypervisor software may be based on for example Qemu. The hypervisor may be responsible for taking packets from the TX queue of the virtual adapter and copying them in the TX queue of the virtual switch . The packets may arrive at the virtual switch TX queue of the port where the VM is attached. The virtual switch may take the packets from the TX queues of the input ports and may add them to the RX queues of the output ports. The switching may be done based on a forwarding table. The forwarding table may contain the MAC addresses of the locally connected VMs. If the destination is found to be locally connected the packets may be moved to the corresponding RX queue . Otherwise they may be enqueued in the RX port corresponding to the physical interface . From the physical interface port the packets may be consumed by a bridge that does the encapsulation step and enqueues the packet in the TX queue of the physical adapter . Then the lossless physical network may take over the packet and deliver it to the destination server RX queue. Thus internal flow control may be achieved making the TX path fully lossless.

On the reception side the packets may be consumed by the bridge from the RX queue of the physical NIC and decapsulated. The packets may be enqueued in the TX queue of the virtual switch that forwards them to the RX queue corresponding to the destination VM . The forwarding done may be lossless. The packets may be consumed by the hypervisor that copies them into the virtual device e.g. vNIC . The virtual device RX queue may be shared between the hypervisor and the guest kernel . The hypervisor notices the guest kernel when a packet is received and the guest OS receives an interrupt. This interrupt may be handled according to the Linux NAPI framework. A softirq may be raised that consumes the packets from the RX queue. The packet may be transferred to the netif receive skb function that does the IP routing and filtering. If the packet is found to be destined to the local stack it is enqueued in the destination socket RX buffer based on the port number. If the destination socket if full the packet may be discarded. In embodiments using a TCP socket this should not happen since TCP has end to end flow control that limits the amount of injected packets to the advertised window of the receiver. In embodiments using UDP sockets a modified Linux Kernel may be used such that when the destination socket RX queue occupancy reaches a threshold e.g. one MTU below maximum the softirq may be canceled and the reception may be paused. Once the process consumes data from the socket the reception may be resumed. This ensures full lossless operation both for TCP and UDP sockets.

The virtual switch may have one port for each VM running on the server plus one additional port for the physical interface. Each port may have an input TX queue for the packets produced by the VMs or received from the physical link . Each port may have an output RX queue for the packets to be consumed by VMs or sent out over the physical link . To provide a software based switch with lossless characteristics the switch work may be implemented according to the pseudocode from Algorithm 1.

Each sender producer is connected to an input queue Ij and each receiver producer is connected to an output queue Ok. After a packet is produces the sender checks if the associated TX queue is full. If full it goes to sleep until a free buffer becomes available else the producer enqueues the packet in the TX queue and then starts a forwarding process to try to push some packets from the input queue to the output queues. The forwarder checks the output queues if there is space available in one of them. If yes it transfers the packets to the output queues and wakes up the corresponding consumers that might be waiting for new packets. On the receiver side the associated output queue is checked. If not empty a packet is consumed else the forwarding process is started to pull out some packet from the input queues to the output queue. If some data is pulled then it is consumed else the receiver sleeps until woken up by the sender. Observe that the switch is designed to operate in a dual push pull mode. When the sender is faster than the receiver the sender will sleep most of the time waiting for free buffers and the receiver will wake it up only when it consumes data. On the other hand when the receiver is faster than the sender the receiver will sleep most of the time and the sender will wake it up only when new data is available. The overhead of a lossless switch is thus reduced to a minimum.

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that in some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration may be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

The corresponding structures materials acts and equivalents of all means or step plus function elements in the claims below are intended to include any structure material or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present invention has been presented for purposes of illustration and description but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The embodiment was chosen and described in order to best explain the principles of the invention and the practical application and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.

