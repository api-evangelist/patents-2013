---

title: Achieving low grace period latencies despite energy efficiency
abstract: A technique for achieving low grace-period latencies in an energy efficient environment in which processors with Read-Copy Update (RCU) callbacks are allowed to enter low power states. In an example embodiment, for each processor that has RCU callbacks, different grace period numbers are assigned to different groups of the processor's RCU callbacks. New grace periods are periodically started and old grace periods are periodically ended. As old grace periods end, groups of RCU callbacks having corresponding assigned grace period numbers are invoked.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09389925&OS=09389925&RS=09389925
owner: International Business Machines Corporation
number: 09389925
owner_city: Armonk
owner_country: US
publication_date: 20131203
---
The present disclosure relates to computer systems and methods in which data resources are shared among data consumers while preserving data integrity and consistency relative to each consumer. More particularly the disclosure concerns a mutual exclusion mechanism known as read copy update. 

By way of background read copy update also known as RCU is a mutual exclusion technique that permits shared data to be accessed for reading without the use of locks writes to shared memory memory barriers atomic instructions or other computationally expensive synchronization mechanisms while still permitting the data to be updated modify delete insert etc. concurrently. The technique is well suited to both uniprocessor and multiprocessor computing environments wherein the number of read operations readers accessing a shared data set is large in comparison to the number of update operations updaters and wherein the overhead cost of employing other mutual exclusion techniques such as locks for each read operation would be high. By way of example a network routing table that is updated at most once every few minutes but searched many thousands of times per second is a case where read side lock acquisition would be quite burdensome.

The read copy update technique implements data updates in two phases. In the first initial update phase the actual data update is carried out in a manner that temporarily preserves two views of the data being updated. One view is the old pre update data state that is maintained for the benefit of read operations that may have been referencing the data concurrently with the update. The other view is the new post update data state that is seen by operations that access the data following the update. In the second deferred update phase the old data state is removed following a grace period that is long enough to ensure that the first group of read operations will no longer maintain references to the pre update data. The second phase update operation typically comprises freeing a stale data element to reclaim its memory. In certain RCU implementations the second phase update operation may comprise something else such as changing an operational state according to the first phase update.

It is assumed that the data element list of is traversed without locking by multiple readers and occasionally updated by updaters that delete insert or modify data elements in the list. In the data element B is being referenced by a reader r as shown by the vertical arrow below the data element. In an updater u wishes to update the linked list by modifying data element B. Instead of simply updating this data element without regard to the fact that r is referencing it which might crash r u preserves B while generating an updated version thereof shown in as data element B and inserting it into the linked list. This is done by u acquiring an appropriate lock to exclude other updaters allocating new memory for B copying the contents of B to B modifying B as needed updating the pointer from A to B so that it points to B and releasing the lock. In current versions of the Linux kernel pointer updates performed by updaters can be implemented using the rcu assign pointer primitive. As an alternative to locking during the update operation other techniques such as non blocking synchronization or a designated update thread could be used to serialize data updates. All subsequent post update readers that traverse the linked list such as the reader r will see the effect of the update operation by encountering B as they dereference B s pointer. On the other hand the old reader r will be unaffected because the original version of B and its pointer to C are retained. Although r will now be reading stale data there are many cases where this can be tolerated such as when data elements track the state of components external to the computer system e.g. network connectivity and must tolerate old data because of communication delays. In current versions of the Linux kernel pointer dereferences performed by readers can be implemented using the rcu dereference primitive.

At some subsequent time following the update r will have continued its traversal of the linked list and moved its reference off of B. In addition there will be a time at which no other reader process is entitled to access B. It is at this point representing an expiration of the grace period referred to above that u can free B as shown in .

In the context of the read copy update mechanism a grace period represents the point at which all running tasks e.g. processes threads or other work having access to a data element guarded by read copy update have passed through a quiescent state in which they can no longer maintain references to the data element assert locks thereon or make any assumptions about data element state. By convention for operating system kernel code paths a context switch an idle loop and user mode execution all represent quiescent states for any given CPU running non preemptible code as can other operations that will not be listed here . The reason for this is that a non preemptible kernel will always complete a particular operation e.g. servicing a system call while running in process context prior to a context switch.

In four tasks and running on four separate CPUs are shown to pass periodically through quiescent states represented by the double vertical bars . The grace period shown by the dotted vertical lines encompasses the time frame in which all four tasks that began before the start of the grace period have passed through one quiescent state. If the four tasks and were reader tasks traversing the linked lists of or none of these tasks having reference to the old data element B prior to the grace period could maintain a reference thereto following the grace period. All post grace period searches conducted by these tasks would bypass B by following the updated pointers created by the updater.

Grace periods may be synchronous or asynchronous. According to the synchronous technique an updater performs the first phase update operation then blocks waits until a grace period has completed and a second phase update operation such as removing stale data is performed. According to the asynchronous technique an updater performs the first phase update operation specifies the second phase update operation as a callback then resumes other processing with the knowledge that the callback will eventually be processed at the end of a grace period. Advantageously callbacks requested by one or more updaters can be batched e.g. on a multi part callback lists and processed as a group at the end of an asynchronous grace period. This allows asynchronous grace period overhead to be amortized over plural deferred update operations. An RCU state machine mechanism is used to start and end grace periods and advance the RCU callbacks one grace period at a time through several stages of callback list processing. A typical RCU callback list may comprise three or four sublist portions that segregate the batched RCU callbacks into callback groups that are processed at the end of different grace periods. A callback must advance through each sublist before it is ready for invocation. In RCU implementations used in the Linux kernel there is typically one RCU callback list per processor and list tail pointers are used to divide each callback list into four sublists. The callback sublists are named for their corresponding tail pointers. Newly arrived callbacks that must await a subsequent grace period before they can be invoked are placed on a first sublist named RCU NEXT TAIL. As grace periods elapse the callbacks first advance to a second sublist named RCU NEXT READY TAIL then to a third sublist named RCU WAIT TAIL and finally arrive at a sublist named RCU DONE TAIL. All callbacks on the RCU DONE TAIL sublist are deemed to be ready for invocation.

More recently RCU grace period processing has been adapted to account for processor low power states such as on Intel processors the C1E halt state or the C2 or deeper halt states . Operating systems can take advantage of low power state capabilities by using mechanisms that withhold regular timer interrupts from processors in a low power state unless the processors need to wake up to perform work. The dynamic tick framework also called dyntick or nohz in existing versions of the Linux kernel is one such mechanism. In RCU implementations designed for low power applications in the Linux kernel a compiler configuration option called RCU FAST NO HZ is available. This option allows processors to be placed in low power states even if there are pending RCU callbacks provided none require immediate invocation and the processor is not needed for grace period advancement processing. Such processors will awaken after a short interval e.g. four scheduling clock periods at which time the processor will attempt to advance its RCU callbacks. This capability results in significant power savings for some workloads.

Unfortunately the RCU FAST NO HZ option can also result in greatly increased grace period latencies. This is due to the fact that the processors which are sleeping with callbacks cannot take full advantage of subsequent grace periods. A waking processor reacts only to a change in a tracked grace period completion number. It does not account for the actual number of additional grace periods that have elapsed. So even if several grace periods elapse while the processor was sleeping the processor will take advantage of only one thus potentially delaying its callbacks for another sleep period. On a busy system a callback will normally take roughly 1.5 grace periods to advance through the callback sublists. After arriving on the RCU NEXT TAIL sublist the callback will advance to the RCU NEXT READY TAIL sublist and as soon as the callback becomes known to the RCU subsystem. When the next grace period starts the callback advances to the RCU WAIT TAIL sublist. When that grace period ends the callback advances to the RCU DONE TAIL sublist for invocation. In the case of a sleeping processor if the processor goes idle before the callback advances to the WAIT sublist an additional 0.5 grace periods will be required to advance the callback when the processor wakes up. This is because the processor does not receive credit for more than one of the potentially many grace periods that elapsed while it was idle. As a consequence the scheduling clock tick is typically not deactivated for processors that have callbacks.

Another scenario causing increased grace period latency for a sleeping processor in a RCU FAST NO HZ kernel is when no other processor in the system needs a grace period to start. In that case the start of the next grace period will be delayed until the sleeping processor awakens further degrading grace period latency for another sleep period. Because no other processor sees any reason to start a new grace period the RCU subsystem remains idle while the processor is asleep. When the processor wakes up an additional grace period is required to advance its callbacks. Had the RCU subsystem been aware of the processor s need for an additional grace period while the processor slept the processor could have instead woken up to find that the grace period that it needed had already completed. Because of this latency issue the RCU FAST NO HZ option causes processors that are sleeping with callbacks to attempt several times to advance the RCU state machine on the off chance that some of the callbacks can then be invoked. This state machine work often has no effect and consumes processor time and thus energy.

Accordingly there is a need for a technique that allows processors to sleep with RCU callbacks without needing to expend large amounts of processor time on idle entry while taking full advantage of any grace periods that elapse during the sleep interval and by initiating grace periods as needed during the sleep interval without having to awaken to do so.

A method system and computer program product are provided to achieve low grace period latencies in an energy efficient environment in which processors with Read Copy Update RCU callbacks are allowed to enter low power states. In an example embodiment for each processor that has RCU callbacks different grace period numbers are assigned to different groups of the processor s RCU callbacks. New grace periods are periodically started and old grace periods are periodically ended. As old grace periods end groups of RCU callbacks having corresponding assigned grace period numbers are invoked.

In an example embodiment groups of RCU callbacks are maintained on sublists of RCU callback lists associated with the one or more processors. Grace period number assigning may then comprise a callback accelerate operation wherein grace period numbers are assigned to newly arrived groups of RCU callbacks and wherein the newly arrived groups of RCU callbacks are placed on selected sublists according to the assigned grace period numbers.

In an example embodiment the RCU callback lists include a done sublist for RCU callbacks that are ready to be invoked. A callback advance operation may then be performed for each of the one or more processors that is not offline or designated as a no callbacks processor and which has RCU callbacks that are waiting for a not yet completed grace period. The callback advance operation may include advancing to the done sublist any groups of RCU callbacks whose assigned grace period numbers correspond to grace periods that have ended. The callback advance operation may further include performing the callback accelerate operation.

In an example embodiment the RCU callback lists each comprise plural sublists and the callback advance operation further includes adjusting sublists other than the done sublist to maintain the RCU callback lists in a consistent state without holes therein.

In an example embodiment the callback advance operation is attempted as the one or more processors prepare for entry into a low power state the callback accelerate operation is attempted on entry into the low power state and the callback advance operation is attempted again on exit from the low power state.

In an example embodiment the assigning of grace period numbers includes recording future grace periods needed by the one or more processors so that the grace periods can be initiated without waking the one or more processors if they are in a low power state.

In an example embodiment the disclosed operations further include using the future grace period recording to offload callback invocation from specially designated ones of the one or more processors.

The present disclosure provides a technique that achieves low RCU grace period latency in an energy efficient environment in which processors with RCU callbacks are allowed to enter low power states. One example of such an environment would be an RCU implementation designed for low power applications in the Linux kernel compiled using the RCU FAST NO HZ configuration option. Embodiments of the disclosed technique achieve the desired goal by implementing one or more of the following concepts 

1. Numbering groups of RCU callbacks to allow processors to take advantage of multiple past grace periods with different grace period numbers being assigned to different groups of the processor s RCU callbacks. New grace periods are periodically started and old grace periods are periodically ended. As old grace periods end groups of RCU callbacks having corresponding assigned grace period numbers are invoked 

2. Proactively numbering groups of RCU callbacks particularly newly arrived callbacks on a given processor at idle entry exit time to minimize grace period latency thus eliminating the need to run through the RCU state machine in power optimized kernels such as a Linux CONFIG RCU FAST NO HZ kernel 

3. Recording the need for future grace periods so that the number of additional grace periods a processor requires may be determined and so that these grace periods will not be delayed due to a processor being asleep in dyntick idle mode at the time the preceding grace period ends thereby allowing a given processor s callback s grace periods to be initiated and completed without having to awaken that idle processor and promoting energy efficiency for processors that have callbacks by allowing them to enter deep sleep states without substantially degrading grace period latencies and

4. Using the future grace period recording to allow kernel threads to offload callback invocation from specially designated no callback processors.

Turning now to the figures wherein like reference numerals represent like elements in all of the several views illustrates an example multiprocessor computer system in which the grace period processing technique described herein may be implemented. In a computer system includes multiple processors . . . a system bus and a program memory . There are also cache memories . . . and cache controllers . . . respectively associated with the processors . . . . A conventional memory controller is associated with the memory . As shown the memory controller may reside separately from processors . . . e.g. as part of a chipset . As discussed below it could also comprise plural memory controller instances residing on the processors . . . .

The computer system may represent any of several different types of computing apparatus. Such computing apparatus may include but are not limited to general purpose computers special purpose computers portable computing devices communication and or media player devices set top devices embedded systems and other types of information handling machines. The term processor as used with reference to the processors . . . encompasses any program execution unit capable of executing program instructions including but not limited to a packaged integrated circuit device such as a microprocessor a processing core within a packaged integrated circuit device such as a microprocessor core or a hardware thread comprising one or more functional units within a processing core such as an SMT thread . Each such execution unit may also be referred to as a CPU central processing unit . The processors . . . may be situated within a single computing device or node e.g. as part of a single node SMP system or they may be distributed over plural nodes e.g. as part of a NUMA system a cluster or a cloud . The memory may comprise any type of tangible storage medium capable of storing data in computer readable form for use in program execution including but not limited to any of various types of random access memory RAM various flavors of programmable read only memory PROM such as flash memory and other types of primary storage i.e. program memory . The cache memories . . . may be implemented in several levels e.g. as level 1 level 2 and level 3 caches and the cache controllers . . . may collectively represent the cache controller logic that supports each cache level. As illustrated the memory controller may reside separately from processors . . . for example as part of a discrete chipset. Alternatively as previously mentioned the memory controller could be provided by plural memory controller instances that are respectively integrated with the processors . . . .

Each CPU embodied by a given processor is operable to execute program instruction logic under the control of a software program stored in the memory or elsewhere . As part of this program execution logic update operations updaters may execute within a process thread or other execution context hereinafter task on any of the processors . Each updater runs periodically to perform updates on a set of shared data that may be stored in the shared memory or elsewhere . In reference numerals . . . illustrate individual data updaters that respectively execute on the several processors . . . . As described in the Background section above the updates performed by an RCU updater can include modifying elements of a linked list inserting new elements into the list deleting elements from the list and other types of operations. To facilitate such updates the processors may be programmed from instructions stored in the memory or elsewhere to implement a read copy update RCU subsystem as part of their processor functions. In reference numbers . . . represent individual RCU instances that may periodically execute on the several processors . . . . Any given processor may also execute a read operation reader . Each reader runs from program instructions stored in the memory or elsewhere in order to periodically perform read operations on the set of shared data stored in the shared memory or elsewhere . In reference numerals . . . illustrate individual reader instances that may respectively execute on the several processors . . . . Such read operations will typically be performed far more often than updates this being one of the premises underlying the use of read copy update. Moreover it is possible for several of the readers to maintain simultaneous references to one of the shared data elements while an updater updates the same data element.

During operation of the computer system an updater will occasionally perform an update to one of the shared data elements . In accordance with the philosophy of RCU a first phase update may be performed in a manner that temporarily preserves a pre update view of the shared data element for the benefit of readers that may be concurrently referencing the shared data element during the update operation. Following the first phase update the updater may register a callback with the RCU subsystem for the deferred destruction of the pre update view second phase update following a grace period. As described in the Background section above this is known as asynchronous grace period processing. Alternatively the updater may request a synchronous expedited grace period.

The grace period processing performed by the RCU subsystem entails starting new grace periods and detecting the end of old grace periods so that the RCU subsystem knows when it is safe to free stale data or take other actions . Grace period processing may further entail the management of callback lists that accumulate callbacks until they are ripe for batch processing at the end of a given grace period.

Turning now to example components of the RCU subsystem are shown. Among these components is a set of RCU subsystem support functions namely an RCU reader API Application Programming Interface an RCU updater API and a set of grace period detection and callback functions .

As shown in the RCU reader API comprises a reader registration component and a reader unregistration component . These components are respectively invoked by readers as they enter and leave their RCU read side critical sections. This allows the RCU subsystem to track reader operations and determine when readers are engaged in RCU protected read side critical section processing. In an example embodiment the reader registration component and the reader unregistration component may be respectively implemented using the rcu read lock and rcu read unlock primitives found in existing read copy update implementations.

As also shown in the RCU updater API comprises a register callback component . The register callback component is used by updaters to register a callback following a first phase update to a shared data element . An invocation of the register callback component initiates processing that places the callback on an RCU callback list described in more detail below associated with the processor that runs the updater . This may start an asynchronous grace period if one is not already underway so that the callback can be subsequently processed as part of second phase update processing to remove stale data or perform other actions . In an example embodiment the register callback component may be implemented using the existing call rcu primitive found in conventional read copy update implementations.

With continuing reference to the grace period detection and callback processing component of the RCU subsystem includes various functions that participate in implementing the low grace period latency energy efficient technique disclosed herein. The operational details of these functions are described in more detail below. The grace period detection and callback processing component also includes a number of other conventional RCU components that are responsible for various operations such as starting new grace periods detecting the end of old grace periods and processing callbacks as grace periods end. A discussion of such components is omitted for ease of description and in order to focus attention on the low grace period latency energy efficient technique disclosed herein.

With continuing reference now to the RCU substyem further includes a set of RCU subsystem data structures . For purposes of the present discussion an example embodiment is presented in which the RCU subsystem is configured as a hierarchical RCU implementation as is conventionally provided in large multiprocessor computer systems running the Linux kernel. 305782 Nov. 4 2008. The RCU subsystem data structures may thus include an rcu state data structure comprising a set of rcu node data structures embedded as a linear array within the rcu state structure. The RCU subsystem data structures may also include set of a per processor rcu data structures and a set of per processor rcu dynticks data structures . The purpose of these data structures is described in more detail below. If the present embodiment implements several RCU flavors e.g. RCU bh RCU preempt and RCU bh each flavor would have one rcu state structure and a set of rcu node structures . Each processor would have one rcu data structure for each flavor. There are typically enough rcu node structures per RCU flavor to form a combining tree whose leaves are the rcu data structures and whose internal nodes and root are the rcu node structures. presents a simplified view of this hierarchy in which some number of rcu data structures would report to one of the leaf rcu node structures and wherein the leaf rcu node structures would report to higher level rcu node structures not explicitly shown until the root rcu node structure is reached. Returning now to it should be noted that a production read copy update implementation will typically include many additional data structures that are not shown in this Figure. A discussion of such data structures is omitted for ease of description and in order to focus attention on the low grace period latency energy efficient RCU technique disclosed herein.

As stated by way of introduction above the disclosed technique contemplates numbering of groups of RCU callbacks to allow processors to take advantage of multiple past grace periods. Different grace period numbers are assigned to different groups of the processor s RCU callbacks. This section describes various example functions that may be used to implement this functionality.

As is conventional the RCU subsystem enqueues RCU callbacks on a per processor basis with each processor having an associated multi tailed RCU callback list stored in an associated one of the rcu data structures . An example RCU callback list is shown in . A list pointer array which may be named nxttail in the rcu data structure contains a pointer in each array element that references the tail of one of four sublist portions of the RCU callback list . As discussed in the Background section above callback sublists are typically named after the array element that references the sublist s tail. Thus in the RCU callback list there is an RCU DONE TAIL sublist an RCU WAIT TAIL sublist an RCU NEXT READY TAIL sublist and an RCU NEXT TAIL sublist. In callback A in the RCU DONE TAIL sublist is ready to invoke. Callbacks B and C in the RCU WAIT TAIL sublist are waiting for the current grace period or more accurately for what the current processor believes to be the current grace period . Callback D in the RCU NEXT READY TAIL sublist is waiting for some later grace period. Callback E in the RCU NEXT TAIL sublist is just waiting.

In the Background section above it was noted that it takes roughly 1.5 grace periods for callbacks on an RCU callback list to advance to the RCU DONE TAIL sublist. As will now be explained this is a generalized case and the callback residency time may actually be shorter or longer than 1.5 grace periods. In some cases the transition of a callback from the RCU NEXT TAIL sublist to the RCU NEXT READY TAIL sublist can happen quite quickly. For example a callback can be advanced immediately if its processor knows that the next grace period cannot yet have started. This would occur if the processor has not yet informed the RCU subsystem of a quiescent state for the current grace period. The processor would also know that the next grace period has not started if it is the processor that is responsible for starting that grace period. A callback s transition from the RCU NEXT READY TAIL sublist to the RCU WAIT TAIL sublist happens when the processor becomes aware that the next grace period has started which would on average be 0.5 grace periods. It takes a full grace period for a callback to transition from RCU WAIT TAIL sublist to the RCU DONE TAIL sublist. Once the callback is on the RCU DONE TAIL sublist invocation could happen quite quickly. So typically a callback s residency time on the RCU callback list could actually be slightly more than 1.5 grace periods as little as one grace period but possibly several grace periods if timing does not work out right. An example of a slow transition scenario would be where a given processor passes through its quiescent state very early in a grace period and then receives a callback. The processor cannot move the callback from the RCU NEXT TAIL sublist to the RCU NEXT READY TAIL sublist because the grace period could end at any time if the processor does not have the means to stop it. So almost a full grace period elapses before this transition can happen. Then another grace period elapses before the transition from RCU NEXT READY TAIL to RCU WAIT TAIL and yet another before RCU WAIT TAIL to RCU DONE TAIL. That is almost three full grace periods plus however long is required for the processor to become aware of the grace period endings.

The above described callback list approach which is conventionally used in prior art RCU implementations was designed for situations in which a processor is aware of the start and end of each and every grace period. Before the advent of the RCU FAST NO HZ Linux kernel configuration option this was the case for all processors that had at least one RCU callback queued. The conventional callback list approach works well for those cases. In particular the use of an RCU NEXT READY TAIL sublist in addition to the RCU WAIT TAIL and RCU DONE TAIL lists requires callbacks to wait at least one grace period before being invoked. This allows all processors to invoke callbacks at the ends of consecutive grace periods provided each processor has a constant influx of callbacks . However this approach does not handle the case where the processor does not see all of the grace period transitions as can occur in RCU FAST NO HZ implementations.

Assigning grace period numbers to callbacks must be done carefully in hierarchical RCU implementations insofar as global synchronization is avoided. In particular hierarchical RCU uses the combining tree structure shown in . Within each of the rcu state rcu node and rcu data structures and there is a gpnum field recording the number of the last grace period to start indicated by the number following the g in and a completed field recording the number of the last grace period to complete indicated by the number following the c in the figure . If all of the gpnum and completed fields have the same value then the RCU subsystem is idle. Otherwise the gpnum fields are one greater than the completed fields so that the gpnum fields record the number of the current grace period while the completed fields record the number of the previous grace period.

Each rcu node structure including its gpnum and completed fields is protected by a lock field contained within that rcu node structure and the gpnum and completed fields in the rcu state structure are protected by a lock field in the root rcu node structure . The gpnum and completed fields in the rcu data structures are not protected by any lock but can only be accessed and modified by a corresponding one of the processors . These fields in the rcu data structures are thus unsynchronized and can in fact fall far behind the values in the rcu node and rcu state structures and when the corresponding processor is in dyntick idle mode and thus not performing any RCU updates . This is acceptable because the rcu data structure gpnum and completed fields are used as snapshots of those in the corresponding leaf rcu node structure . Their only purpose is to allow the associated processor to detect when grace periods start and end.

In view of the foregoing when grace period numbers are assigned to callbacks they must be assigned in accordance with the gpnum and completed fields of one of the rcu node structures or the rcu state structure and the corresponding lock must be held. To do otherwise i.e. to use the gpnum and completed fields of an rcu data structure risks premature callback invocation which would defeat the purpose of RCU. But at the same time grace period number assignment must not acquire the lock fields too often or the result will be poor performance and limited scalability.

Periodically a processor will invoke the grace period detection and callback processing component to check the completed field in its rcu data structure against the completed field of the leaf rcu node structure to which it reports. One example is at the end of a grace period when it is desired to advance the processor s callbacks. Advantageously this operation is performed while holding the lock of the processor s leaf rcu node structure so this is an opportune time to assign grace period numbers to any newly arrived callbacks. In addition a processor will periodically invoke the grace period detection and callback processing component to start a new grace period. This operation is also performed while holding the root rcu node structure s lock field so this is also a suitable time to assign grace period numbers to newly arrived callbacks. The same holds true for a processor that invokes the grace period detection and callback processing component to clean up after a newly completed grace period. Finally a processor will periodically invoke the grace period detection and callback processing component to report the last quiescent state for a grace period. This operation is performed while holding the root rcu node structure s lock so this too is a good place to assign grace period numbers to newly arrived callbacks.

An example technique for determining what grace period number to assign to newly arrived callbacks is covered in Section 1.1 entitled Determining Grace Period Number. An example technique for actually assigning the grace period number is covered in Section 1.2 entitled Accelerating Callbacks. Thereafter Section 1.3 entitled Advancing Callbacks covers an example technique for managing groups of callbacks whose grace period numbers have already been assigned and determining when they may be safely invoked. which represents a detailed view of the grace period detection and callback processing component of illustrates various functions that may be used to perform the described operations.

Before a grace period number can be assigned it is necessary to determine which grace period s completion will guarantee that a full grace period has elapsed for the callbacks in question. This job is carried out by the grace period detection and callback processing component using a function that may be named rcu cbs completed The rcu cbs completed function is part of the grace period detection and callback processing component and is shown in . Example C language pseudocode for the rcu cbs completed function is shown in Pseudocode Listing below.

As can be seen the rcu cbs completed function inspects the completed field of the rcu node structure whose lock is currently held and returns a number that is two greater than the value of the completed for that rcu node structure unless the specified rcu node structure is the root and the gpnum and completed fields are equal.

In the case of the root rcu node structure if the gpnum and completed fields are equal there is no grace period in progress so that the end of the next grace period will suffice. Line in Pseudocode Listing thus returns a value that adds one to the root rcu node structure s completed field. But if the root rcu node s gpnum and completed fields differ there is a grace period currently in progress so that it is necessary to wait for this in progress grace period as well as the next grace period. Line in Pseudocode Listing thus returns a value that adds two to the root rcu node structure s completed field.

The root rcu node structure is the only one that can decisively determine that there is no grace period in progress. After all the various gpnum and completed fields cannot be updated atomically and any attempt to do so would create a performance and scalability bottleneck. Because the root rcu node structure is updated first it is the only one that can be trusted to know exactly when the grace period started. Therefore the non root rcu node structures have to allow for a grace period being in progress even when their gpnum and completed fields are equal. Therefore for non root rcu node structures line of Pseudocode Listing unconditionally returns a value that adds two to the rcu node structure s completed field.

Once the grace period number is determined the next question is where to store it. In an example embodiment the grace period number is stored in a nxtcompleted array of a processor s rcu data structure parallel to the existing nxttail array that stores the tail pointers. This is shown in . The entry corresponding to RCU DONE TAIL is blank because it is meaningless once a callback has waited for a grace period the exact grace period it waited for is no longer relevant. The entry corresponding to RCU NEXT TAIL is also meaningless Callbacks in this sublist by definition have not yet been assigned a grace period number. In contrast in the example processor state shown callbacks B and C in the RCU WAIT TAIL sublist are waiting for grace period 2 to complete while callback D in the RCU NEXT READY TAIL sublist is waiting for grace period 3 to complete.

The job of assigning grace period numbers to callbacks is undertaken by another function that may be named rcu accelerate cbs The rcu accelerate cbs function is part of the grace period detection and callback processing component and is shown in . Example C language pseudocode for this function is shown in Pseudocode Listing below.

The rcu accelerate cbs function checks to see if a callback sublist is available for new callbacks or if the grace period number already assigned to one of the sublists works for the newly arrived callbacks and in either case merges the new callbacks into the sublist in question. Finally it cleans up pointers and values so that the RCU callback list is properly formatted. The acceleration is relative to avoiding calling the rcu accelerate cbs function and letting the next call to the rcu advance cbs function see below do the job.

Line of Pseudocode Listing checks to see if the processor is offline or is a no callbacks processor first condition or if it has no callbacks waiting for a not yet completed grace period second condition and if either of these two cases hold line returns to the caller. Line invokes the rcu cbs completed function to obtain the number of the grace period that newly arrived callbacks must wait for. Recall that newly arrived callbacks are those on the RCU NEXT TAIL sublist. The loop spanning lines scans backwards from RCU NEXT TAIL towards RCU DONE TAIL to find the smallest indexed sublist into which newly arrived callbacks can be placed but assuming that any waiting callbacks have been compressed to the beginning of the list see rcu advance cbs below . An example of an uncompressed callback list would have an empty RCU WAIT TAIL sublist but a non empty RCU NEXT READY TAIL sublist. The if statement spanning lines finds the first non empty sublist line whose assigned grace period number is unsuitable for newly arrived callbacks line at which point line breaks from the loop. Unsuitable in this context means that the grace period number in the sublist s nxtcompleted field is less than the grace period number required for newly arrived callbacks as determined in line . Because the if statement actually found the largest indexed sublist into which newly arrived callbacks could not be placed line increments to the next sublist and if that sublist is RCU NEXT TAIL which is where the newly arrived callbacks are to begin with then line returns to the caller. The loop spanning lines then moves the newly arrived callbacks from the RCU NEXT TAIL sublist to their destination setting the assigned grace period number to that determined by line . Finally lines do event tracing.

When the end of their assigned grace period is reached callbacks must be moved into RCU DONE TAIL so that they can be invoked. This job is handled by a function that may be named rcu advance cbs The rcu advance cbs function is part of the grace period detection and callback processing component and is shown in . Example C language pseudocode for this function is shown in Pseudocode Listing below.

Line of the Pseudocode Listing checks to see if the processor is offline or is a no callbacks processor first condition or if it has no callbacks waiting for a not yet completed grace period second condition and if any of these three cases hold line returns to the caller. The loop spanning lines handles the first stage of moving callbacks whose assigned grace period has completed to the RCU DONE TAIL with line sequencing through the sublists that have nxtcompleted numbers assigned. Line checks to see if the current sublist s grace period has not yet completed by comparing its nxtcompleted field against the completed field of the rcu node structure whose lock is held and if it has not line breaks out of the loop. Otherwise line merges the current sublist into the RCU DONE TAIL sublist.

The foregoing operation can leave the callback list in an inconsistent state. For example if the completed field of the rcu node structure was equal to 3 the loop of lines would transform the list as shown in into the state shown in . As can be seen the RCU WAIT TAIL pointer has not been advanced to catch up with the RCU DONE TAIL pointer. The loop spanning lines and does the needed correction so that the callback list is as shown in .

A scenario may arise in which the first two loops lines are run starting again with the callback list shown in but this time with the value of the completed field of the rcu node structure is equal to two. The result will be as shown in . The problem is that the RCU WAIT TAIL is empty but the RCU NEXT READY TAIL is not which leaves a hole in the callback list . Leaving the list in this state could needlessly prevent a future call to the rcu accelerate cbs function from assigning grace period numbers to newly arrived callbacks because the RCU NEXT READY TAIL sublist would already be occupied. Therefore the loop spanning lines copies later sublists down to eliminate holes so that the resulting list would be as shown in . Line sequences through the midlist sublists. If line finds that the potential destination sublist and all subsequent sublists are empty then line breaks from the loop. Otherwise if there are non empty sublists remaining lines and copy the next sublist down to fill in the hole. Finally line invokes the rcu accelerate cbs function to assign grace period numbers to any newly arrived callbacks. Acceleration of callbacks might seem unnecessary in the case where the loop spanning is exited via line insofar as the RCU NEXT TAIL sublist that would otherwise contain new arrivals is empty. However the rcu accelerate cbs function can also recalculate the grace period assignments. This recalculation is useful if the initial calculation was made using a non root rcu node structure but the root rcu node structure is currently being referenced.

As stated by way of introduction above the disclosed technique contemplates proactive numbering of groups of RCU callbacks particularly newly arrived callbacks on a given processor at idle entry exit time to minimize grace period latency thus eliminating the need to repeatedly run through the RCU state machine. The present section 2 describes various example functions that may be used to implement this functionality.

Current RCU FAST NO HZ processing runs though the RCU state machine including forcing quiescent states up to five times on each idle entry. This can consume significant processing resources especially on large systems where the quiescent state forcing does a full scan of all processors. Callback numbering permits a much less CPU intensive idle entry strategy to be used. In an example embodiment of the technique disclosed herein an attempt is made to advance callbacks during preparation for idle entry accelerate them on idle entry and advance them again on idle exit. A function named rcu needs cpu attempts to advance callbacks during preparation for idle entry. The rcu needs cpu function is shown by reference number in and is described in more detail below. The only condition in which the rcu needs cpu function refuses dyntick idle entry is if there are callbacks ready to invoke. In addition the rcu needs cpu function advances callbacks as much as possible so that another function named rcu prepare for idle can enter dyntick idle mode with a minimum of overhead It need only accelerate callbacks and check for the appearance of non lazy callbacks in case of dyntick idle re entry. Non lazy callbacks are RCU callbacks that take some action other than merely freeing memory for example awakening some task which cannot be safely delayed without the risk of delaying some important operation. For example indefinitely delaying the RCU callback corresponding to a synchronize rcu invocation could hang the system. In contrast lazy callbacks involve only the freeing of memory so that they may be delayed without harm on a system with ample quantities of free memory. The rcu prepare for idle function is shown by reference number in and is also described in more detail below. A third function named rcu cleanup after idle advances callbacks and causes any now ready callbacks to be invoked on idle exit. The rcu cleanup after idle function is shown by reference number in and is likewise described in more detail below.

The rcu needs cpu function the rcu prepare for idle function and the rcu cleanup after idle function rely on several fields in the per processor rcu dynticks data structures . Example C language versions of these fields are shown in Pseudocode Listing below.

In Pseudocode Listing the all lazy field indicates that all of a processor s callbacks were lazy at the time of the processor s last invocation of the rcu needs cpu function . The nonlazy posted field is a running count of the non lazy callbacks registered by this processor. The nonlazy posted snap field is a snapshot of the nonlazy posted field at the time of the processor s last invocation of the rcu needs cpu function . The last accelerate field records the value of the jiffies counter at the time of the last idle entry callback advancement or acceleration.

The rcu needs cpu function the rcu prepare for idle function and the rcu cleanup after idle function together with addition helper functions are described in the ensuing sections. Section 2.1 entitled Does a Processor Have RCU Callbacks describes an example callback determination technique. Section 2.2 entitled Attempting to Advance Callbacks describes attempting to advance callbacks. Section 2.3 entitled Does RCU Need This Processor describes determining whether the RCU subsystem needs the current processor. Section 2.4 entitled Preparing a Processor For Idle describes preparing a processor to enter dyntick idle mode. Section 2.5 entitled Cleaning Up After a Processor Exits Idle describes cleanup actions once a processor exits dyntick idle mode.

Pseudocode Listing below shows example C language pseudocode for a helper function that may be named rcu cpu has callbacks which checks for a processor having callbacks and if so whether all of them are lazy. The rcu cpu has callbacks helper function is shown by reference number in . Interrupts must be disabled by the caller if these determinations are to be exact otherwise callbacks might be enqueued or invoked while this function was running 

In Pseudocode Listing the loop spanning lines checks each RCU flavor implemented by the RCU subsystem . Line obtains a pointer to the specified processor s rcu data structure . If line determines that there is at least one non lazy callback line sets local variable al to false. If line determines that there is at least one callback present on the processor s callback list line sets local variable hc to true. Upon exit from the loop if line determines that the all lazy parameter in the processor s rcu dynticks data structures is non NULL line stores an indication of whether or not all callbacks are lazy. Either way line informs the caller as to whether this processor has RCU callbacks.

Pseudocode Listing shows example C language pseudocode that may be used to implement another helper function that may be named rcu try advance all cbs The rcu try advance all cbs helper function is shown by reference number in . This function attempts to advance callbacks for each RCU flavor for the current processor. The main concern is to avoid excessive contention on the lock fields of the leaf rcu node structures .

In Pseudocode Listing each pass through the loop spanning lines attempts to advance callbacks for one flavor of RCU implemented by the processor. Line obtains a reference to this processor s rcu data structure and line obtains a reference to the corresponding leaf rcu node structure . If line determines that a grace period has completed since the last attempt and line determines that there are callbacks that are not yet known to be ready to invoke line invokes a function named  rcu process gp end via a wrapper function named rcu process gp end . The  rcu process gp end function which is shown by reference number in handles end of grace period processing including invoking the rcu advance cbs function . In either case if line determines that this RCU flavor has callbacks ready to invoke line indicates that this processor has callbacks that are ready to invoke. Once all RCU flavors have been checked and possibly advanced line returns the indication as to whether this processor has callbacks ready to invoke.

Pseudocode Listing shows example C language pseudocode that may be used to implement the rcu needs cpu function which does initial idle entry preparation and then indicates whether RCU is prepared to allow a processor to enter dyntick idle mode in which scheduling clock interrupts are suppressed.

In Pseudocode Listing line takes a snapshot of the running count of the number of non lazy callbacks posted by this processor which will be used by the rcu prepare for idle function to determine whether the delay strategy should be shifted from the long delay associated with processors having only lazy callbacks queued to the much shorter delays associated with processors having at least one non lazy callback queued. If line determines that this processor has no callbacks then line sets an indefinite dyntick idle sojourn and line indicates to the caller that this processor does not need RCU. In either case line via its invocation of the rcu cpu has callbacks function also updates all lazy with an indication of whether or not this processor has any non lazy callbacks. Otherwise line invokes the rcu try advance callbacks function to advance callbacks on this processor. If there now callbacks ready to invoke line initiates later RCU core processing e.g. in kthread context to invoke those callbacks and line indicates to the caller that RCU does need this processor. If there are still callbacks queued on this processor but none are ready to invoke processing reaches line which records the current time as the last time that callbacks were accelerated. This is used by the rcu prepare for idle function to keep lock contention bounded in cases where the idle loop quickly enters and exits dyntick idle mode in rapid sequence as can happen due to tracing and power control operations in the idle loop when those operations use RCU. If line determines that all callbacks on this processor are lazy then line sets a long dyntick idle sojourn roughly six seconds by default but adjusted so that other timers can be serviced by the same power hungry processor wakeup operation as for RCU. Otherwise lines and set a much shorter delay rounded to a multiple of that delay so that if multiple processors in a given package are in dyntick idle mode with non lazy callbacks a single wakeup will handle all those processors. Finally line indicates to the caller that RCU does not immediately need this processor.

Pseudocode Listing shows example C language pseudocode that may be used to implement the rcu prepare for idle function which performs the actual idle entry processing preparing a processor for idle from RCU s perspective including checking to determine if any non lazy RCU callbacks have arrived at a processor that previously had only lazy RCU callbacks.

In Pseudocode Listing if line determines that this is a no callbacks processor line returns to the caller. If line determines that all callbacks were lazy at rcu needs cpu time and if line determines that non lazy callbacks have subsequently arrived then line initiates RCU core processing for this processor thus causing the rcu needs cpu function to be re invoked in turn causing the dyntick idle sojourn to be recalculated and line returns to the caller. If line determines that callbacks were recently accelerated then line returns to the caller otherwise lines accelerate callbacks. Line prevents overly aggressive re acceleration for workloads that switch in and out of idle frequently. Each pass through the loop spanning lines accelerates callbacks on the current processor belonging to one flavor of RCU. Line obtains a pointer to the rcu data structure corresponding to the current processor and flavor of RCU. If line determines that there are no callbacks that are waiting for a grace period to elapse e.g. all are ready to invoke then line continues with the next flavor of RCU. Line obtains a pointer to the leaf rcu node structure corresponding to this processor and RCU flavor. Line then obtains that rcu node structure s lock line invokes the rcu accelerate cbs function to assign grace period numbers to callbacks and line releases the lock.

Pseudocode Listing shows example C language pseudocode that may be used to implement the rcu cleanup after idle function which advances callbacks on exit from dyntick idle mode initiating invocation of any callbacks whose grace period has now elapsed.

In Pseudocode Listing line checks to see if this is a no callbacks processor and if so line returns to the caller. Otherwise line invokes the rcu try advance all cbs function . The loop spanning lines then initiates callback invocation for all RCU flavors that now have callbacks that are ready to invoke. Line obtains a pointer to the rcu data structure corresponding to the current RCU flavor and the current processor. If line determines that there are callbacks ready to invoke line initiates their invocation by causing RCU core processing to run.

As stated by way of introduction above the disclosed technique contemplates recording the need for future grace periods so that these grace periods are not delayed due to a processor being asleep in dyntick idle mode at the time that the preceding grace period ends. This section describes various example functions that may be used to implement such functionality.

Pseudocode Listing shows example fields in the rcu node data structures that may be used to register future grace periods.

In Pseudocode Listing the nocb gp wq array is used by no callbacks kthreads rcu nocb kthread to wait for the needed grace period to elapse. The need future gp is used to track requests for future grace periods. In both cases only two elements are required because it is only necessary to register for two grace periods in the future 1 Wait for the current grace period to complete and then 2 wait for the following grace period to complete.

In the ensuing discussion Section 3.1 entitled Initiating Grace Periods describes grace period initiation operations that may be used to allow the need for future grace periods to be recorded. Section 3.2 entitled Grace Period Recording covers the mechanics of the actual recording. Section 3.3 entitled Grace Period Cleanup covers cleanup actions at the end of each grace period.

Pseudocode Listing shows example C language pseudocode that may be used to implement a function named rcu start gp advanced lines and a function named rcu start gp lines . These functions are respectively shown by reference numbers and in . The rcu advance cbs function invokes the rcu start gp advanced function and other callers invoke the rcu start gp function . Both functions must be invoked with the root rcu node structure s lock held.

In Pseudocode Listing if line of the rcu start gp advanced function finds that it is too early in boot no RCU grace period kthread yet or that the processor does not need another grace period including any future grace periods registered then line returns. Otherwise line sets a flag indicating that a new grace period is required and line wakes up the grace period kthread. The rcu start gp function simply invokes rcu advance cbs line and then rcu start gp advanced line .

The main function used to register future needed grace periods may be named rcu start future gp This function is designated by reference number in . Example C language pseudocode that may be used to implement the rcu start future gp function is shown in Pseudocode Listing . The rcu start future gp function registers the grace period that would be required for a newly arrived callback and returns that grace period number. The function also adjusts any callbacks currently queued on the rcu data structure referenced by rdp and if any are waiting on a later grace period they are adjusted to wait on the grace period appropriate for a newly arrived callback. This can happen when callbacks are initially assigned a grace period number with reference to a leaf rcu node structure but then a later call to the rcu start future gp function references the root rcu node structure . Note that the caller is required to have acquired the lock for the rcu node structure referenced by the rnp parameter.

Line invokes the rcu cbs completed function to obtain the grace period number for newly arrived callbacks. If the rnp parameter references a leaf rcu node structure this grace period number might be overly conservative as noted above. Line does event tracing using a simple helper function. Line checks the need future gp array in the rcu node structure whose lock is held in order to determine whether the needed grace period number has already been requested. If it has line does event tracing and line returns to the caller. Note that the two element need future gp array in each rcu node structure suffices. To see this consider the following cases 

1. There is a grace period in progress. Any newly registered callback will be satisfied by the next grace period.

2. There is no grace period in progress. Again any newly registered callback will be satisfied by the next grace period.

3. A grace period might or might not be in progress for example when a leaf rcu node structure does not yet believe that a grace period has started. In this case a processor registering a future grace period need based on a leaf rcu node structure will wait for the current next grace period depending and on the one following that. In contrast a processor registering a future grace period need based on the root rcu node structure can wait only for the next grace period in the case where there is no grace period in progress.

This distinction between leaf and root rcu node structures is required to avoid race conditions that can result when requests for new grace periods arrive concurrently with initialization of a newly started grace period. The root rcu node structure has definite knowledge of whether or not a grace period is in progress while non root rcu node structures can only know when a grace period is definitely in progress They can never be sure that there is no grace period in progress. This is because a new grace period might have started but rcu node initialization might not yet have reached the non root rcu node structure in question.

In Pseudocode Listing if line for the current rcu node structure and line for the root rcu node structure but unsynchronized detect that a grace period is in progress then line requests the grace period number obtained by line line does event tracing and line returns to the caller. The lack of synchronization is permissible because the lock of the rcu node structure referenced by the rnp parameter is held. If this structure believes that a grace period is in progress then there definitely is a grace period in progress. On the other hand if this structure does not believe that a grace period is in progress and one has in fact started it will not be possible to finish starting it because the current processor holds the rnp lock. Therefore if the root rnp node structure believes that a grace period is in progress this grace period must remain in progress while the current processor holds the rnp lock and the comparison is stable. In contrast if the unsynchronized accesses on line determine that a grace period is not in progress this comparison is unstable because a grace period might start at any time. But this is acceptable because subsequent code rechecks while holding the root rcu node structure s lock. In contrast any comparison that leads to the body of the if statement on lines will be stable.

If there does not appear to be a grace period in progress execution proceeds to line which checks to see if the rcu start future gp function was invoked on the root rcu node structure and if not line acquires the root rcu node structure s lock. Line then calls the rcu cbs completed function to refresh the required grace period number but this time referencing the root rcu node structure thus obtaining an exact answer. The loop spanning lines scans the callbacks on the current processor s rcu data structure updating any callback groups waiting for a later grace period to instead wait for the grace period identified by line .

If line determines that the root rcu node structure has already registered for the required grace period number line does event tracing and line branches to the end of the rcu start future gp function so as to release locks and return. Otherwise line registers for the required grace period number in the root rcu node structure s need future gp array. In line if the root rcu node structure has no record of an ongoing grace period line invokes the rcu start gp advanced function to start one with the proviso that callback advancement has already taken place. Lines and do event tracing. Line is the label branched to by line . If line determines that rnp and rnp root reference distinct rcu node structures line releases the root rcu node structure s lock. Either way line returns the relevant grace period number to the caller.

The rcu accelerate cbs function shown earlier in Pseudocode Listing can now be modified as shown in Pseudocode Listing to register future grace periods. The only change is the addition of line .

The rcu start future gp function can also be used to allow specially designated no callbacks processors to initiate grace periods as shown by the rcu nocb wait gp function shown in . Example C language pseudocode for the rcu nocb wait gp function is shown in Pseudocode Listing .

The rcu nocb wait gp function invokes the rcu start future gp function while holding the processor s leaf rcu node structure s lock as required on lines . Line then does event tracing. Each pass through the loop spanning lines handles one wake up event. Lines block until either the grace period number advances past that returned by rcu start future gp on line or until a signal is received. If line sees that the grace period number has advanced sufficiently line breaks out of the loop. Otherwise line flushes the signal and line does event tracing. Once the loop is exited execution resumes on line which does yet more event tracing. Line then executes a memory barrier to ensure that any post grace period callback invocations are seen by all processors as happening after the actual grace period computation.

In order to allow future grace period registration it is necessary to clear out the need future gp array for each rcu node structure that previously requested the just ended grace period. It is also necessary to wake up any no callbacks kthreads waiting for this grace period to complete. These operations are handled by a function that may be named rcu future gp cleanup Example C language pseudocode for the rcu future gp cleanup function is shown in Pseudocode Listing . This function is called on each rcu node structure just after that rcu node structure s completed field has been advanced but while that rcu node structure s lock is still held.

Line invokes a function called rcu nocb gp cleanup which if the kernel is compiled with CONFIG RCU NOCB CPU y does a wake up on nocb gp wq c 0x1 thus waking up all no callbacks kthreads waiting for the end of the just completed grace period on this rcu node structure . Line zeroes the corresponding element of the need future array. Line checks to see if there is a need registered on this rcu node structure for an additional grace period line does event tracing and line returns an indication to the caller as to whether another grace period is required.

This section displays a full set of scenarios for callback acceleration and advancement. These scenarios were generated automatically as part of a full state space testing effort. Table 1 below shows the complete list of callback acceleration scenarios based on invocations of the rcu accelerate cb function . Table 2 below shows the complete list of callback advancement scenarios based on invocations of the rcu advance cbs function .

With reference to Table 1 above it will be recalled that callback acceleration using the rcu accelerate cbs  function assigns completed numbers to callbacks that are not yet ready to invoke where possible but does not attempt to move callbacks to the RCU DONE TAIL sublist. The first column Scenario gives the scenario identifier in which each bit indicates the presence or absence of callbacks in the corresponding segment of the callback list. Thus scenario 0xa or 0b1010 specifies callbacks in the RCU DONE TAIL and RCU NEXT READY TAIL portions of the list. This is depicted graphically in .

The second column completed indicates the value of rnp completed which is identical to the initial value of rnp gpnum. In other words Table 1 is showing results for an idle RCU state. Non idle RCU gives the same results for non root scenarios and for root scenarios gives the same result as for idle non root scenarios.

The third column Initial depicts the initial state of the callback list before the call to the rcu accelerate cbs function . The letters D W R and N denote the tails of the RCU DONE TAIL RCU WAIT TAIL RCU NEXT READY TAIL and RCU NEXT TAIL sublists respectively. The letters W and R are followed by a numeral that indicates the value of the corresponding element of the nxtcompleted array . The letters D and N do not need a numeral. For the corresponding sublists the nxtcompleted entry is irrelevant. As previously explained the callbacks in the RCU DONE TAIL sublist are ready to invoke so their completed value has already passed and the callbacks in the RCUNEXT TAIL sublist have not yet been assigned a completed value. A vertical bar followed by one of the lower case letters a through d denotes a non empty sequence of callbacks. Because N marks the end of the entire list there cannot be callbacks following it. Cells that are empty represent scenarios where the call to the rcu accelerate cbs function was a no op. Cells marked with n a represent scenarios that did not need to be tested due to being analogous to the next scenario. For example Scenario with completed of 1 for the root rcu node structure would accelerate all the callbacks to the RCU WAIT TAIL sublist to be ready to invoke at the end of the next grace period just as with the following row Scenario with completed of 2 for the root rcu node structure . The fourth column Non Root shows the result of callback advancement from a call to the rcu accelerate cbs function where the rcu node structure is not the root of the rcu node tree while the fifth column Root shows the analogous result where the rcu node structure is the root of the rcu node tree.

It is noted that the only scenarios where the rcu accelerate cbs function leaves holes empty RCU WAIT TAIL sublist with non empty RCU NEXT READY sublist are scenarios where the initial list also had such a hole. These are Scenarios and .

Table 2 below has a similar format but instead shows callback advancement scenarios based on calls to the rcu advance cbs function . Here all callbacks that are not ready to invoke are marked with their completed number but any callbacks that are now ready to invoke are also moved to the RCU DONE TAIL sublist. It should be noted that the rcu advance cbs function never leaves holes in the callback list even if there was a hole to start with. Because the callback sublists are initially empty which means no hole and because neither the rcu accelerate cbs function nor the rcu advance cbs function generate new holes the callback list will never have holes.

Turning now to a flow diagram illustrates salient features of the operations discussed in connection with and Pseudocode Listings . In block RCU callbacks that are newly arrived at a processor are assigned a grace period number and placed on the processor s callback list as per the rcu accelerate cbs function . In block the need for a future grace period is recorded in the processor s rnp node structure as per the rcu start future gp function . In block a new grace period is initiated as per the rcu start gp advanced function . In block callbacks are periodically advanced or accelerated as per the rcu advance cbs function including when preparing to enter advance enter accelerate and leave advance a dynticks idle state as per the rcu needs cpu function the rcu prepare for idle function and the rcu cleanup after idle function . In block a callback group on the callback list of a processor is invoked at the end of the grace period corresponding to the group s assigned grace period number.

Accordingly an RCU technique has been disclosed for achieving low grace period latencies in an energy efficient environment in which processors with RCU callbacks are allowed to enter low power states. It will be appreciated that the foregoing concepts may be variously embodied in any of a data processing system a machine implemented method and a computer program product in which programming logic is provided by one or more machine readable non transitory data storage media for use in controlling a data processing system to perform the required functions. Example embodiments of a data processing system and machine implemented method were previously described in connection with . With respect to a computer program product digitally encoded program instructions may be stored on one or more computer readable non transitory data storage media for use in controlling a computer or other digital machine or device to perform the required functions. The program instructions may be embodied as machine language code that is ready for loading and execution by the machine apparatus or the program instructions may comprise a higher level language that can be assembled compiled or interpreted into machine language. Example languages include but are not limited to C C assembly to name but a few. When implemented on a machine comprising a processor the program instructions combine with the processor to provide a particular machine that operates analogously to specific logic circuits which themselves could be used to implement the disclosed subject matter.

Example computer readable non transitory data storage media for storing such program instructions are shown by reference numerals memory and cache of the computer system of . The system may further include one or more secondary or tertiary storage devices not shown that could store the program instructions between system reboots. A further example of computer readable non transitory data storage media that may be used to store the program instructions is shown by reference numeral in . The data storage media are illustrated as being portable optical storage disks of the type that are conventionally used for commercial software sales such as compact disk read only memory CD ROM disks compact disk read write CD R W disks and digital versatile disks DVDs . Such data storage media can store the program instructions either alone or in conjunction with an operating system or other software product that incorporates the required functionality. The computer readable non transitory data storage media could also be provided by other portable data storage media such as floppy disks flash memory sticks etc. or data storage media combined with drive systems e.g. disk drives . As is the case with the memory and the cache of the computer readable non transitory data storage media may be incorporated in data processing platforms that have integrated random access memory RAM read only memory ROM or other semiconductor or solid state memory all of which represent further examples of computer readable non transitory data storage media. More broadly the computer readable non transitory data storage media could comprise any electronic magnetic optical infrared semiconductor system or apparatus or device or any other tangible non transitory entity representing a machine manufacture or composition of matter that can contain store communicate or transport the program instructions for use by or in connection with an instruction execution system apparatus or device such as a computer. For all of the above forms of computer readable non transitory data storage media when the program instructions are loaded into and executed by an instruction execution system apparatus or device the resultant programmed system apparatus or device becomes a particular machine for practicing embodiments of the method s and system s described herein.

Although various example embodiments have been shown and described it should be apparent that many variations and alternative embodiments could be implemented in accordance with the disclosure. It is understood therefore that the invention is not to be in any way limited except in accordance with the spirit of the appended claims and their equivalents.

