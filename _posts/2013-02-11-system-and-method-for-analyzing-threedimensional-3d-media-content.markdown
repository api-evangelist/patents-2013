---

title: System and method for analyzing three-dimensional (3D) media content
abstract: A system and method are provided that use point of gaze information to determine what portions of 3D media content are actually being viewed to enable a 3D media content viewing experience to be improved. Tracking eye movements of viewers to obtain such point of gaze information are used to control characteristics of the 3D media content during consumption of that media, and/or to improve or otherwise adjust or refine the 3D media content during creation thereof by a media content provider. Outputs may be generated to illustrate what in the 3D media content was viewed at incorrect depths. Such outputs may then be used in subsequent or offline analysis, e.g., by editors for media content providers when generating the 3D media itself, in order to gauge the 3D effects. A quality metric can be computed based on the point of gaze information, which can be used to analyze the interactions between viewers and the 3D media content being displayed. The quality metric may also be calibrated in order to accommodate offsets and other factors and/or to allow for aggregation of results obtained for multiple viewers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08913790&OS=08913790&RS=08913790
owner: Mirametrix Inc.
number: 08913790
owner_city: Westmount
owner_country: CA
publication_date: 20130211
---
This application is a continuation of International PCT Application No. PCT CA2011 000923 filed on Aug. 16 2011 which claims priority from U.S. Provisional Patent Application No. 61 373 974 filed on Aug. 16 2010 both incorporated herein by reference.

Eye tracking systems have been used to track the motion of a viewer s eyes and consequently the point of gaze of the viewer. An example of an eye tracking system can be found for example in U.S. Pat. No. 4 950 069 filed Nov. 4 1988 and entitled Eye Movement Detector with Improved Calibration and Speed . The point of gaze is most commonly tracked on a two dimensional 2D surface such as a computer television TV or any other 2D display that displays media content. In addition recent advances in eye trackers have enabled the ability to track the point of gaze on 3D displays and even in real world 3D space.

Having knowledge of where a viewer is looking on a display can provide behavioral insight into the viewer s cognitive processes while viewing the media of interest. Where the viewer is looking is often closely tied to what the user is thinking. With eye gaze information it is possible to tell what region of the media caught the user s attention first what the user spent the longest time looking at the order in which different regions were viewed the regions that were never seen by the viewer etc. Examples of systems that use eye tracking for the analysis of 2D content include U.S. Pat. No. 6 601 021 filed Dec. 8 2000 and entitled System and Method for Analyzing Eyetracker Data and U.S. Pat. No. 7 029 121 filed Feb. 4 2004 and entitled Techniques for Facilitating Use of Eye Tracking Data .

It will be appreciated that for simplicity and clarity of illustration where considered appropriate reference numerals may be repeated among the figures to indicate corresponding or analogous elements. In addition numerous specific details are set forth in order to provide a thorough understanding of the example embodiments described herein. However it will be understood by those of ordinary skill in the art that the example embodiments described herein may be practised without these specific details. In other instances well known methods procedures and components have not been described in detail so as not to obscure the example embodiments described herein. Also the description is not to be considered as limiting the scope of the example embodiments described herein.

While eye tracking has been used for the analysis of 2D content for several years eye tracking technology is also applicable to 3D content which is finding increasing popularity with mainstream audiences. Content in 3D is developed by creating media for the left and right eye views. Multi view cameras can be used to film in 3D while digital media may be rendered directly for the left and right eyes based on a given ocular separation typically estimated at 7 cm . A variety of methods may be used to display 3D content. For example passive methods such as those using red cyan and polarized glasses may be used to split the left and right eye views. Active techniques may also be used such as techniques that use liquid crystal display LCD shutter glasses with alternating video frames for the left and right eyes. More recently autosteroscopic displays show 3D content without the need for glasses using methods such as lenticular and parallax barrier screens. Heads up displays may also be used to show 3D content by having two separate displays one for each of the left and right eyes.

In addition to where the viewer is looking on the media binocular eye gaze tracking can provide significant insight into the impression of depth through tracking the vergence converging point of the left and right eye when looking at media displayed at differing depths. The quality of the depth effect however is difficult to determine unless it can be tested and quantified with human viewers.

Depth perception involves binocular cues such as stereopis and convergence as well as monocular cues such as accommodation object size and occlusion. With binocular eye tracking it is possible to gain insight into the quality of the 3D media displayed by tracking the convergence of the viewer s point of gaze.

It has been found that by using point of gaze information to determine what portions of 3D media content are actually being viewed a 3D media content viewing experience can be improved. Tracking eye movements of viewers to obtain such point of gaze information can be used to not only control characteristics of the 3D media content during consumption of that media but also to improve or otherwise adjust or refine the 3D media content during creation thereof by a media content provider. Outputs may be generated to illustrate what and where in the 3D media content was viewed and in which areas were the depth effects incorrect. Such outputs may then be used in subsequent or offline analyses e.g. by editors for media content providers when generating the 3D media itself in order to gauge the effectiveness of the 3D media content and 3D effects.

It has also been found that a quality metric can be computed based on the point of gaze information which can be used to analyze the interactions between viewers and the 3D media content being displayed. The quality metric may also be calibrated in order to accommodate offsets and other factors and or to allow for aggregation of results obtained for multiple viewers.

Turning now to the display of 3D media content on a display screen is shown. It can be appreciated that the display screen in this example includes a 2D display capable of rendering media content in 3D and the display used may be a stand alone unit such as a television or computer monitor or an embedded screen in for example a portable gaming device smart phone laptop or tablet computer or any other device utilizing a 3D capable 2D display. As shown in the display screen may be given a set of reference axes in this example wherein the x axis extends horizontally from left to right from the viewer s perspective the y axis extends vertically from bottom to top and the z axis extends perpendicularly through the display screen from the viewer. The 3D media content may include any type of content such as text images video etc. provided using any suitable medium such as television web wired and wireless etc.

In the example shown in the 3D media content is being viewed by a viewer having a left eye L and a right eye R. The viewer if required utilizes 3D glasses heads up display or other eyewear for achieving a 3D effect. The 3D media content is generated distributed or otherwise provided as 3D media data by a 3D media content provider . For example the 3D media data may be stored to a physical medium such as a DVD or provided as streaming data by for example a server. The 3D media data is used to display the 3D media content on the display screen using a 3D media player . The 3D media data may include any visual scene which is viewed in 3D where the left and right eyes of the viewer see the scene from different points of view such that the impression of depth is given. The left and right eye views of the 3D media may be provided through the use of shutter glasses polarized glasses anaglyph glasses auto stereoscopic displays random dot stereograms or any other such technique. 3D media content may include still images pictures magazines etc. video sequences movies TV etc computer generated imagery video games computing interfaces etc among others.

It can be appreciated that the 3D media player and 3D media data are shown separate from the display screen for ease of illustration and such components may be part of the same device.

An analysis module is also shown in which may be used to gather eye tracking data from an eye tracking system and or viewer position data from a viewer position tracking system and to determine characteristics of the 3D media data to analyze and control the output of the 3D media content and or display information associated with the viewer s interactions with the 3D media content . For example as will be described by way of example below data provided by the eye tracking system can be used to perform a gaze depth analysis for adjusting the depth of objects in the 3D media content .

A binocular eye tracking system may be used to perform a gaze depth analysis on the 3D media content being displayed. Such an eye tracking system should be configured to accurately identify the location of the point of gaze for both the left eye L and right eye R of the viewer on the display screen see also regardless of the technology used to display the 3D media. As will be explained by way of example below the left and right POG can be used to estimate a 3D POG see also . If the eye tracking system is capable of computing a 3D POG directly the 3D POG provides an estimate of the gaze depth. An example of a method and system capable of computing a 3D POG directly is PCT Patent Application No. PCT CA2008 000987 filed May 23 2008 and entitled Methods and Apparatus for Estimating Point of Gaze in Three Dimensions published as WO 2008 141460 the entire contents of which are incorporated herein by reference.

It can be appreciated that eye tracking systems that use active infrared lighting provide the ability to image the eyes L R through eyewear such as polarized or LCD shutter glasses as these glasses often limit the light entering and reflecting off the eyes L R. Alternatively visible light eye tracking systems may be used if there is sufficient ambient light for the eye tracking system to observe the eyes L R. If shutter glasses are used where the left and right eyes are alternately blocked the opening and closing of an eye tracker camera shutter may be synchronized with the toggling rate of the shutter glasses. If synchronized directly each tracker image frame will record the left eye L or right eye R. If synchronized with a percent duty cycle time offset for example 50 each camera image will be exposed to a percentage of the open shutter time for both the left eye L and right eye R allowing both eyes L R to be imaged at the same time. A similar effect could be achieved by operating the shutter glasses at higher toggling rates than the eye tracker shutter. For example running the eye tracking system at 60 Hz and the shutter glasses at 120 Hz would result in imaging both the left eye L and right eye R in a single camera image. In the event that one eye L or R is missing in the eye tracking system s camera image for a short duration the last known good eye position or point of gaze position may be used for that eye to allow for continued operation. If the eye L or R is lost for a larger duration the ability to estimate gaze depth may be lost.

It can be appreciated that the eye tracking system viewer tracking system and analysis module are shown as separate components and independent of the 3D media player 3D media data and display screen for illustrative purposes only. Any one or more of these components may be included in a same device or system. For example a home theatre system may include the analysis module in a 3D media player with an eye tracking system supported by or integrated with a TV which includes the display screen . Similarly a single portable electronic device such as a smart phone may include all of the components shown in including optionally the 3D media content provider e.g. if viewer user is creating content using their device .

The VQM generated by the VQM module may also be provided as an output itself e.g. to another system not shown such as a video editing monitor using a data output interface in order to enable the other system to perform an analysis of the 3D media content . The VQM can also be provided to a depth output module to enable the analysis module to generate one or more numerical or visual depth outputs DEPTH . For example numerical depth data contour mappings heat maps content analyses content of interest information etc. can be generated to provide a visual output to a viewer content provider or both. The depth outputs can also be provided to other systems using the data output interface . As shown in the VQM module may also have access to a 3D media content interface for obtaining the 3D media data or portions thereof or otherwise interacting directly with the 3D media data e.g. to alter the 3D media data . In examples where the viewer s position with respect to the display screen can be ascertained e.g. if the analysis module has access to data provided by a viewer position tracking system a viewer position tracking system interface may also be provided to enable the VQM module to obtain viewer position data not shown .

It can be appreciated that the viewer s position is typically relevant to auto stereoscopic displays wherein the viewer does not need to wear special glasses. For example the display screen may use a barrier or lenses to project different images to the left eye L and right eye R. This only works effectively if the eyes L R are located at the appropriate position sweet spot where the system is projecting the left and right eye images to the left and right eyes. If you know where the eyes are you can project the left and right eye images directly towards the true positions of the eyes with steerable lenses or other techniques.

It can also be appreciated that the interfaces and shown in may be any physical wired or wireless connection or coupling.

Any module or component exemplified herein that executes instructions may include or otherwise have access to computer readable media such as storage media computer storage media or data storage devices removable and or non removable such as for example magnetic disks optical disks or tape. Computer storage media may include volatile and non volatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Examples of computer storage media include RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by an application module or both. Any such computer storage media may be part of the analysis module 3D media player eye tracking system viewer position tracking system etc. or other computing or control device that utilizes similar principles or accessible or connectable thereto. Any application or module herein described may be implemented using computer readable executable instructions that may be stored or otherwise held by such computer readable media.

Eye tracking and or viewer position data are obtained at e.g. by interfacing with the eye tracking system and viewer position tracking system . The eye tracking system provides POG data such as the viewer s POG on the display screen for each of the left eye L and right eye R or by obtaining a depth measurement from a direct 3D POG computation. It can be appreciated that the eye tracking system may also determine other information such as pupil dilation fixations saccades eye positions and additional parameters of interest to an analysis. The eye tracking system may also be equipped or programmed to track heart rate perspiration and other biometric measurements which can be used to estimate a viewer s emotional state and response to the content shown. The data obtained from the eye tracking system and if provided details associated with the 3D media data itself is collected and analyzed at . The results of the analysis can be used in providing i.e. adjusting or modifying the 3D media data for display and or to present a numerical or visual analysis output at . As discussed above the results of the analyses can be provided in various ways both using the display screen or by providing the results to other systems such as video editing equipment.

For binocular eye tracking systems that compute the POG for the left eye L and right eye R on a 2D display screen the VQM may be determined according to the relative positioning of the left and right points of gaze. Assuming the viewers eyes are horizontally displaced i.e. the head is not tilted to the side in this example the VQM is computed as the difference between the X coordinates of the left and right eye points of gaze on a 2D display screen. When observing standard 2D media on a display the left and right eyes typically converge to the same point and the VQM would be zero where POG POG and VQM POG POG 0.

When 3D media content is displayed the eye tracking system can determine not only where on the display screen the viewer was looking but also the depth by comparing the 2D left and right eye POG positions on the 2D display screen . If the 3D media content is displayed or drawn with a depth further from the viewer than the screen the VQM is 0. The larger the VQM the further into the scene the viewer is looking and the greater the perceived depth effect where VQM POG POG 0.

When 3D media content is displayed closer to the viewer than the screen the left and right eye points of gaze on the 2D display screen cross and the VQM becomes increasingly negative where VQM POG POG

Turning to determining the vergence point on a 2D display screen is illustrated with the corresponding point of gaze positions recorded by the eye tracking system on the 2D display screen . The illustrations in are shown from two viewpoints a front view shown on the left and a top view shown on the right. In at the left eye L and right eye R are observing a point of interest on a plane defined by the 2D display screen in which both the left eye gaze and right eye gaze converge to the same point on the plane and the VQM 0.

In at the point of interest is beyond the plane of the display screen which results in a VQM 0. In this example the X coordinate of the right point of gaze is larger than the X coordinate of the left point of gaze .

It may be noted that in the event the head is tilted the VQM can be computed as the distance between the left eye point of gaze and right eye point of gaze for example using the mathematical vector norm POG POG and the sign of the VQM taken to be positive if POGis to the right of POG see and negative if POGis to the left of POG see on the display. As many 3D displays require the eyes to be horizontal the norm reduces to the horizontal subtraction described above. The sign of the VQM was chosen to be positive behind the screen and negative closer to the viewer however if the subtraction was reversed i.e. POG POG the VQM sign would be reversed and should be accounted for appropriately.

For binocular eye tracking systems that compute the point of gaze in 3D the VQM can be obtained by determining the signed value of the Z coordinate of a 3D point of gaze where VQM POG.

As discussed above in the examples shown herein the origin of the 3D POG coordinate system may be defined on the surface of the 2D display typically the lower left corner with the X axis defined as the horizontal axis and increasing from left to right the Y axis defined as the vertical axis and is increasing from bottom to top and the Z axis defined perpendicular to the monitor display plane increasing positively away from the viewer and negatively towards the viewer.

If the content is drawn with a depth further behind the screen the VQM is 0 while content displayed closer to the viewer results in the VQM becoming increasingly negative VQM

It can be appreciated that the measurement used to compute the VQM can be related in various ways to a reference point in the 3D media. For example the VQM can be measured in pixels or a percentage of the screen for a 2D eye tracking system wherein the relative distance between the POGs L and R is determined e.g. by comparing the X coordinates or relative percentages of the screen from a reference point such as one edge of the display screen . For a 3D eye tracking system the POG may be measured in a real world distance e.g. centimeters . The number of pixels percentage of the display screen and real world distance will typically not map directly to the distances in the actual 3D media content since the 3D media content may include various scenes of different scale e.g. outer space football game living room in a sitcom etc. where the depths depend on the content being shown. Moreover the real world depths may vary depending on the size of the display screen e.g. home theatre TV versus laptop versus smart phone etc. and the position of the viewer in front of the screen. Therefore the VQM will likely need to be mapped to reference points specific to the 3D media content in order to be able to compare the POG to actual objects to be able to use the VQM to control the 3D media content or to generate other outputs such as depth mappings discussed in greater detail below . An example reference point could be a small shape such as a cube or sphere rendered at various positions and depths in front of the viewer.

Since the VQM is a relative measurement the VQM may require calibration to correct for offsets such as when POGdoes not equal POGat zero depth and to allow for aggregation of the results between multiple viewers. The calibration method performed at may include displaying test objects at known depths e.g. as illustrated in . In the example shown in a test object is displayed behind the plane of the display screen at four 4 units of depth beyond the plane . However as illustrated in in this example the difference in X units between the right POG R and the left POG L is five 5 . In this example this would translate to a POG depth at 5 units of depth as shown in . As such the VQM would not correspond to the 5 units measured using the X coordinates in and would need to be calibrated as shown in . Not the units of depth may typically be in cm and units of distance between left and right point of gaze points in pixels while in the example shown they are unit less.

In the VQM based on the data shown in is at units of depth . By applying a correction factor or otherwise computing a function of the VQM namely VQM a calibrated VQM can be determined as shown in . In this way offsets that cause the eye tracking system to perceive POGs L and R when the test object is drawn at 4 units of depth can be determined and applied to compensate accordingly.

It can be appreciated that the calibration methods described herein may be applied to other metrics in addition to the VQM . In general therefore a function of the POG information obtained may be used to correct the computed depth measurement e.g. POG POG or POG etc.

Various calibration methods can be used to compensate for the factors affecting the 3D media content in different applications. An example plot is shown in which illustrates an example calibration technique. In the example plot the left POGvalues and right POGvalues in pixels on the screen and the VQM is plotted over 5 seconds while looking at objects shown at different real world depths on a computer screen 30 cm 0 cm and 30 cm. The VQM is approximately 290 100 and 80 respectively in this example. In the example shown real world distances e.g. cm measured by a ruler were used. However it can be appreciated that for various rendered 3D media the units may be different.

An example calibration can involve looking at the DIST 30 DIST 0 and DIST 30 objects while measuring the respective VQM values. In the data shown in this would be VQM 290 VQM 100 VQM 80.

This example uses a linear mapping of the form a X b . It can be appreciated that an inverse power exponential or other type of function may also result in an acceptable fit.

As is shown in the VQM is computed over time such that the metric is continuously computed at each estimated point of gaze POG t resulting in VQM t . If the 3D media content is static such as an image the VQM t will correspond to the location of the point of gaze POG t on the image. If the 3D media content is dynamic such as a video or computer game the VQM t will correspond to both the location of the point of gaze POG t of the image and the video frame image or computer generated scene shown on the display at time T. For simplicity VQM as discussed herein may refer to any appropriate VQM value whether or not time is a factor e.g. VQM t as discussed above. Given the time sequence of the VQM t various filtering techniques may be employed to enhance the measurement depending on the desired application for example finite impulse response filtering may be used to smooth the estimated VQM values over time.

The calibration may be affected by various factors. Some factors include without limitation the quality of the eye tracking system used overall system noise camera sensor noise accuracy of the algorithms used natural high speed movement of the eye and camera sampling rate eye health eye dominance and non linearity. Eye health may be affected by a cross eyed condition among other things wherein a dominant eye may be more significant while the other eye wanders. Non linearity s when looking at the edge of the display screen may also affect the calibration. For example as shown in when looking at point A the left and right eye POG x coordinates are equal distance apart whereas looking at point B this is no longer the case. Such an effect may be compensated for using the known locations of the eyes the screen and the point of gaze estimates along with ray tracing or if the effect is minor may simply be ignored. In particular when a 3D POG measurement is used the Z depth estimate is the same regardless of whether the viewer is looking at point A or B.

One way in which to obtain data for calibrating the VQM may include showing a simple 3D scene in which test points for example a small cube or sphere are drawn at known depths that are a further than the screen b at the screen and c closer than screen with respect to the viewer . The VQM can be determined at each known position and a relationship computed for example a linear function quadradic function lookup table between the VQM and depth wherein as discussed above 

Rather than prompting the viewer to look at test points at known depths calibration may be performed without conscious user interaction. Content can be displayed with known depths and known visual attraction such as a soccer ball kicked out towards the viewer in the course of regular viewing. The VQM can then be calibrated with the assumption the viewer was looking at the object at the object depth.

To accommodate the event where the viewer was not looking at the object the calibration process can also be performed multiple times to accumulate redundant measurements since the calibration process in this example is unobtrusive to the viewer as they are not required to consciously participate. The collected measurements may then be filtered and sorted to identify the most frequent calibration measurement which is then selected as the correct value. Various filtering sorting and selecting algorithms may be employed. For example if 4 objects are shown over time at a similar depth and the computed VQMs are VQM 10 VQM 4 VQM 10 VQM 10 then for that depth based on the highest frequency measure recorded the VQM can be estimated to be 10.

As shown in the calibrated VQMmay then be output for use by for example a 3D media player 3D media content provider or other system. The calibrated VQM may also be used by the analysis module to generate a control instruction for adjusting the 3D media content in various ways. Various other data may also be used in conjunction with the VQM e.g. by obtaining viewer position data from a viewer position tracking system .

For example knowledge of the viewer s position such as seating position in a theater or the eye positions determined by the eye tracking system when viewing a TV or other screen can be used to determine the actual point of gaze using ray tracing from eye to POG position on the 2D display screen and for computing the intersection point. For eye tracking systems that provide eye positions the 3D capable display screen may incorporate this information to more appropriately render the 3D media content based on the correct distance between the left eye L and right eye R. In addition knowledge of the gaze position in depth may be used to develop human computer interfaces such as 3D pointing and gaze contingent rendering.

The 3D media player can be configured to adjust the associated media content in real time i.e. as the media is being consumed. For a 3D scene a depth image as shown in is used wherein a grayscale image of the scene shown in is obtained or computed with the intensity of objects being lower the further away they are in the scene . For example the persons hand in the foreground in is lighter than the persons other arm shown deeper in the scene . The analysis module can therefore be used to track where on the 3D scene the viewer is looking with content at that depth in sharp focus and or rendered at high resolution and content away from that depth blurred and or at lower resolution. For example if a viewer was looking at the persons hand in the foreground the hand would be rendered in focus while the rest of the image is blurred .

For 3D media content shown on a stereoscopic display screen a three dimensional scene is rendered with objects in the foreground and background. When the viewer looks at the foreground objects and the 3D POG is closer to the viewer at some arbitrary distance the background objects are blurred. Such a technique may be used for example in a video game where images are generated on the fly. It can be appreciated that burring may be performed by applying a Gaussian blur to an image or portion of the image or any other suitable available technique.

While depth information for content that is rendered in real time is easily determined from the 3D models used to render the scene for 3D media content like a movie a depth mapping may be needed similar to . In this case the viewer depth measurements are compared against the depth map while the 3D media is displayed to the viewer.

Human visual systems typically use various cues to determine the depth of an object. Monocular cues include motion parallax depth from motion perspective relative size familiar size aerial perspective accommodation occlusion etc. while binocular cues include stereopsis and convergence.

It has been found that in modern 3D capable display screens most of these depth cues can be accurately simulated to provide a viewer with the perception of depth with one exception the accommodation of the eyes. Since the display screen is actually a 2D surface the natural blurring of a scene at depths further and nearer from the point of gaze is typically not represented as the viewer tends to be accommodating focused on the exact depth of the 2D display screen at all times. This mismatch between accommodation and other visual depth cues may cause disorientation and discomfort that afflicts some viewers when viewing 3D media content .

Using the 3D POG and the VQM computed by the analysis module a control instruction can be generated to have the scene defocused or blurred based on the position of the 3D point of gaze on the display screen and the relative depth of the scene at this gaze position. As shown in the computer generated image sequence of the same object a teapot in this example is rendered in a sequence from far to near with the viewer s POG focused on the object when it was at the middle depth of . Accordingly at the object is drawn in focus while when rendered at the far depth in and the near depth at the object is blurred to simulate the viewer s true visual depth of field.

In another example viewing the face of an actor in a scene may result in the actor being rendered with full clarity while the background scene image surrounding the actor is defocused or blurred as shown in . If the viewer then switches to viewing the background scene identified by an increase in depth of the 3D point of gaze the actor can then be blurred and the background scene brought into sharper focus. To differentiate content such as actor and background a depth map may be provided or the media may have be segmented e.g. content regions outlined previously and saved as meta data in relation to the overall 3D media. A technique such as this is particularly useful when a one to one viewer to display relationship exists.

As shown in the VQM obtained at may also be used in association with media content or details thereof obtained at to generate a depth output at e.g. by the depth output module . The depth output can be the VQM reported numerically or displayed graphically using a contour map or false colour shading to indicate the magnitude of the positive and negative depths perceived across the 3D media content . illustrates an example of a grayscale depth image which is overlaid with a heat map that shows regions which were viewed but at an incorrect depth. Such a heatmap may be used not only for subsequent analyses but during production of 3D media content e.g. in order to allow 3D content providers to edit or otherwise adjust the 3D media content to improve the viewing experience based on the feedback from the heatmap .

The depth error or mismatch between the desired depth effect and the viewer s perceived depth can be displayed as an error heatmap on an image output as shown in wherein the hotter whiter temperature indicates increased error or mismatch between actual depth and perceived depth .

The analysis module can also be used in conjunction with other systems not shown for generating outputs related to content analysis wherein the POG of the viewer is used to determine what content is of interest etc. For example a system for determining content of interest could be applied to the 3D analyses being conducted. Examples of such systems may be found in U.S. patent application Ser. No. 12 727 284 filed Mar. 19 2010 entitled Method for Automatic Mapping of Eye Tracker Data to Hypermedia Content published as U.S. 2010 0295774 and U.S. Provisional Patent Application No. 61 413 964 filed Nov. 15 2010 entitled Method and System for Media Display Interaction Based on Eye Gaze Tracking the contents of both applications being incorporated herein by reference.

It will be appreciated that the example embodiments and corresponding diagrams used herein are for illustrative purposes only. Different configurations and terminology can be used without departing from the principles expressed herein. For instance components and modules can be added deleted modified or arranged with differing connections without departing from these principles.

The steps or operations in the flow charts and diagrams described herein are just for example. There may be many variations to these steps or operations without departing from the spirit of the invention or inventions. For instance the steps may be performed in a differing order or steps may be added deleted or modified.

Although the above principles have been described with reference to certain specific example embodiments various modifications thereof will be apparent to those skilled in the art as outlined in the appended claims.

