---

title: Apparatus, system, and method for accessing memory
abstract: Apparatuses, systems, methods, and computer program products are disclosed for providing access to auto-commit memory. An auto-commit memory module is configured to cause a volatile memory buffer to commit data from the volatile memory buffer to a non-volatile memory medium in response to a trigger. A mapping module is configured to determine whether to associate a range of data with the volatile memory buffer. A bypass module is configured to service a request for the range of data directly from the volatile memory buffer in response to the mapping module determining to associate the range of data with the volatile memory buffer.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09208071&OS=09208071&RS=09208071
owner: SanDisk Technologies, Inc.
number: 09208071
owner_city: Plano
owner_country: US
publication_date: 20130315
---
This disclosure relates to auto commit memory and more particularly to an interface for accessing auto commit memory.

Volatile memory such as random access memory RAM typically has faster access times than non volatile storage such as NAND flash magnetic hard disk drives or the like. While the capacities of volatile memory continue to increase as the price of volatile memory decreases volatile memory remains more expensive per unit of capacity than most non volatile storage.

This often leads to design tradeoffs between the speed and performance of volatile memory and the lower price of non volatile storage at larger capacities. Further to achieve the speed and performance benefits of volatile memory a system typically sacrifices the persistence of non volatile memory causing data to be irretrievably lost without power.

Methods for providing access to auto commit memory are presented. In one embodiment a method includes receiving a request for data. A request in certain embodiments includes a namespace identifier. A method in one embodiment includes identifying a relationship between a namespace identifier and a memory. In one embodiment a method includes satisfying a request using a memory without passing the request through an operating system storage stack in response to an identified relationship associating a namespace identifier with the memory.

Apparatuses for providing access to auto commit memory are presented. In one embodiment an auto commit memory module is configured to cause a volatile memory buffer to commit data from the volatile memory buffer to a non volatile memory medium in response to the data filling the volatile memory buffer. A mapping module in a further embodiment is configured to determine whether to associate a range of addresses for data with a volatile memory buffer. In certain embodiments a bypass module is configured to service a request for a range of addresses for data directly from a volatile memory buffer in response to an auto commit mapping module determining to associate a range of addresses for data with the volatile memory buffer.

An apparatus in one embodiment includes means for associating a logical identifier with a page of volatile memory. In a further embodiment an apparatus includes means for bypassing an operating system storage stack to satisfy a storage request for data of a page of volatile memory directly. In certain embodiments an apparatus includes means for preserving data of a page of volatile memory in response to a failure condition.

Systems for providing access to auto commit memory are presented. In one embodiment a system includes a recording device comprising one or more auto commit pages configured to preserve data of the auto commit pages in response to a restart event. A system in a further embodiment includes a device driver for a recording device. A device driver in certain embodiments is configured to cause data of auto commit pages to be mapped from kernel space into virtual memory. A device driver in one embodiment is configured to service requests from user space for data of auto commit pages.

Computer program products comprising a computer readable storage medium storing computer usable program code executable to perform operations for providing access to auto commit memory is also presented. In one embodiment an operation includes intercepting in user space a storage request for a memory device. A storage request in certain embodiments comprises a file identifier and an offset. An operation in a further embodiment includes servicing a storage request in user space directly from a volatile memory of a memory device in response to determining that an offset and a file identifier are mapped to the volatile memory. An operation in one embodiment includes mapping an offset and a file identifier to a volatile memory in response to determining that a file identifier is not mapped to the volatile memory.

Reference throughout this specification to features advantages or similar language does not imply that all of the features and advantages that may be realized with the present disclosure should be or are in any single embodiment of the disclosure. Rather language referring to the features and advantages is understood to mean that a specific feature advantage or characteristic described in connection with an embodiment is included in at least one embodiment of the present disclosure. Thus discussion of the features and advantages and similar language throughout this specification may but do not necessarily refer to the same embodiment.

Furthermore the described features advantages and characteristics of the disclosure may be combined in any suitable manner in one or more embodiments. One skilled in the relevant art will recognize that the disclosure may be practiced without one or more of the specific features or advantages of a particular embodiment. In other instances additional features and advantages may be recognized in certain embodiments that may not be present in all embodiments of the disclosure. These features and advantages of the present invention will become more fully apparent from the following description and appended claims or may be learned by the practice of the disclosure as set forth hereinafter.

Many of the functional units described in this specification have been labeled as modules in order to more particularly emphasize their implementation independence. For example a module may be implemented as a hardware circuit comprising custom VLSI circuits or gate arrays off the shelf semiconductors such as logic chips transistors or other discrete components. A module may also be implemented in programmable hardware devices such as field programmable gate arrays programmable array logic programmable logic devices or the like.

Modules may also be implemented in software for execution by various types of processors. An identified module of executable code may for instance comprise one or more physical or logical blocks of computer instructions which may for instance be organized as an object procedure or function. Nevertheless the executables of an identified module need not be physically located together but may comprise disparate instructions stored in different locations which when joined logically together comprise the module and achieve the stated purpose for the module.

Indeed a module of executable code may be a single instruction or many instructions and may even be distributed over several different code segments among different programs and across several memory devices. Similarly operational data may be identified and illustrated herein within modules and may be embodied in any suitable form and organized within any suitable type of data structure. The operational data may be collected as a single data set or may be distributed over different locations including over different storage devices and may exist at least partially merely as electronic signals on a system or network. Where a module or portions of a module are implemented in software the software portions are stored on one or more computer readable media.

Reference throughout this specification to one embodiment an embodiment or similar language means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present disclosure. Thus appearances of the phrases in one embodiment in an embodiment and similar language throughout this specification may but do not necessarily all refer to the same embodiment.

Reference to a computer readable medium may take any form capable of storing machine readable instructions on a digital processing apparatus. A computer readable medium may be embodied by a compact disk digital video disk a magnetic tape a Bernoulli drive a magnetic disk a punch card flash memory integrated circuits or other digital processing apparatus memory device.

Furthermore the described features structures or characteristics of the disclosure may be combined in any suitable manner in one or more embodiments. In the following description numerous specific details are provided such as examples of programming software modules user selections network transactions database queries database structures hardware modules hardware circuits hardware chips etc. to provide a thorough understanding of embodiments of the disclosure. One skilled in the relevant art will recognize however that the disclosure may be practiced without one or more of the specific details or with other methods components materials and so forth. In other instances well known structures materials or operations are not shown or described in detail to avoid obscuring aspects of the disclosure.

The schematic flow chart diagrams included herein are generally set forth as logical flow chart diagrams. As such the depicted order and labeled steps are indicative of one embodiment of the presented method. Other steps and methods may be conceived that are equivalent in function logic or effect to one or more steps or portions thereof of the illustrated method. Additionally the format and symbols employed are provided to explain the logical steps of the method and are understood not to limit the scope of the method. Although various arrow types and line types may be employed in the flow chart diagrams they are understood not to limit the scope of the corresponding method. Indeed some arrows or other connectors may be used to indicate only the logical flow of the method. For instance an arrow may indicate a waiting or monitoring period of unspecified duration between enumerated steps of the depicted method. Additionally the order in which a particular method occurs may or may not strictly adhere to the order of the corresponding steps shown.

The host stores data in the storage device and communicates data with the storage device via a communications connection not shown . The storage device may be internal to the host or external to the host . The communications connection may be a bus a network or other manner of connection allowing the transfer of data between the host and the storage device . In one embodiment the storage device is connected to the host by a PCI connection such as PCI express PCI e . The storage device may be a card that plugs into a PCI e connection on the host .

The storage device also has a primary power connection that connects the storage device with a primary power source that provides the storage device with the power that it needs to perform data storage operations such as reads writes erases etc. The storage device under normal operating conditions receives the necessary power from the primary power source over the primary power connection . In certain embodiments such as the embodiment shown in the primary power connection connects the storage device to the host and the host acts as the primary power source that supplies the storage device with power. In certain embodiments the primary power connection and the communications connection discussed above are part of the same physical connection between the host and the storage device . For example the storage device may receive power over a PCI connection.

In other embodiments the storage device may connect to an external power supply via the primary power connection . For example the primary power connection may connect the storage device with a primary power source that is a power converter often called a power brick . Those in the art will appreciate that there are various ways by which a storage device may receive power and the variety of devices that can act as the primary power source for the storage device .

The storage device provides nonvolatile storage memory and or recording media for the host . shows the storage device comprising a write data pipeline a read data pipeline nonvolatile memory a storage controller an auto commit memory and a secondary power supply . The storage device may contain additional components that are not shown in order to provide a simpler view of the storage device .

The nonvolatile memory stores data such that the data is retained even when the storage device is not powered. Examples of nonvolatile memory include flash memory nano random access memory nano RAM or NRAM nanocrystal wire based memory silicon oxide based sub 10 nanometer process memory graphene memory Silicon Oxide Nitride Oxide Silicon SONOS Resistive random access memory RRAM programmable metallization cell PMC conductive bridging RAM CBRAM magneto resistive RAM MRAM dynamic RAM DRAM phase change RAM PRAM or other non volatile solid state storage media. In other embodiments the non volatile memory may comprise magnetic media optical media or other types of non volatile storage media. For example in those embodiments the non volatile storage device may comprise a hard disk drive an optical storage drive or the like.

While the non volatile memory is referred to herein as memory media in various embodiments the non volatile memory may more generally comprise a non volatile recording media capable of recording data the non volatile recording media may be referred to as a non volatile memory media a non volatile storage media or the like. Further the non volatile storage device in various embodiments may comprise a non volatile recording device a non volatile memory device a non volatile storage device or the like.

The storage device also includes a storage controller that coordinates the storage and retrieval of data in the nonvolatile memory . The storage controller may use one or more indexes to locate and retrieve data and perform other operations on data stored in the storage device . For example the storage controller may include a groomer for performing data grooming operations such as garbage collection.

As shown the storage device in certain embodiments implements a write data pipeline and a read data pipeline an example of which is described in greater detail below with regard to . The write data pipeline may perform certain operations on data as the data is transferred from the host into the nonvolatile memory . These operations may include for example error correction code ECC generation encryption compression and others. The read data pipeline may perform similar and potentially inverse operations on data that is being read out of nonvolatile memory and sent to the host .

The storage device also includes a secondary power supply that provides power in the event of a complete or partial power disruption resulting in the storage device not receiving enough electrical power over the primary power connection . A power disruption is any event that unexpectedly causes the storage device to stop receiving power over the primary power connection or causes a significant reduction in the power received by the storage device over the primary power connection . A significant reduction in power in one embodiment includes the power falling below a predefined threshold. The predefined threshold in a further embodiment is selected to allow for normal fluctuations in the level of power from the primary power connection . For example the power to a building where the host and the storage device may go out. A user action such as improperly shutting down the host providing power to the storage device a failure in the primary power connection or a failure in the primary power supply may cause the storage device to stop receiving power. Numerous varied power disruptions may cause unexpected power loss for the storage device .

The secondary power supply may include one or more batteries one or more capacitors a bank of capacitors a separate connection to a power supply or the like. In one embodiment the secondary power supply provides power to the storage device for at least a power hold up time during a power disruption or other reduction in power from the primary power connection . The secondary power supply in a further embodiment provides a power hold up time long enough to enable the storage device to flush data that is not in nonvolatile memory into the nonvolatile memory . As a result the storage device can preserve the data that is not permanently stored in the storage device before the lack of power causes the storage device to stop functioning. In certain implementations the secondary power supply may comprise the smallest capacitors possible that are capable of providing a predefined power hold up time to preserve space reduce cost and simplify the storage device . In one embodiment one or more banks of capacitors are used to implement the secondary power supply as capacitors are generally more reliable require less maintenance and have a longer life than other options for providing secondary power.

In one embodiment the secondary power supply is part of an electrical circuit that automatically provides power to the storage device upon a partial or complete loss of power from the primary power connection . Similarly the system may be configured to automatically accept or receive electric power from the secondary power supply during a partial or complete power loss. For example in one embodiment the secondary power supply may be electrically coupled to the storage device in parallel with the primary power connection so that the primary power connection charges the secondary power supply during normal operation and the secondary power supply automatically provides power to the storage device in response to a power loss. In one embodiment the system further includes a diode or other reverse current protection between the secondary power supply and the primary power connection to prevent current from the secondary power supply from reaching the primary power connection . In another embodiment the auto commit memory may enable or connect the secondary power supply to the storage device using a switch or the like in response to reduced power from the primary power connection .

An example of data that is not yet in the nonvolatile memory may include data that may be held in volatile memory as the data moves through the write data pipeline . If data in the write data pipeline is lost during a power outage i.e. not written to nonvolatile memory or otherwise permanently stored corruption and data loss may result.

In certain embodiments the storage device sends an acknowledgement to the host at some point after the storage device receives data to be stored in the nonvolatile memory . The write data pipeline or a sub component thereof may generate the acknowledgement. It is advantageous for the storage device to send the acknowledgement as soon as possible after receiving the data.

In certain embodiments the write data pipeline sends the acknowledgement before data is actually stored in the nonvolatile memory . For example the write data pipeline may send the acknowledgement while the data is still in transit through the write data pipeline to the nonvolatile memory . In such embodiments it is highly desirable that the storage device flush all data for which the storage controller has sent an acknowledgement to the nonvolatile memory before the secondary power supply loses sufficient power in order to prevent data corruption and maintain the integrity of the acknowledgement sent.

In addition in certain embodiments some data within the write data pipeline may be corrupted as a result of the power disruption. A power disruption may include a power failure as well as unexpected changes in power levels supplied. The unexpected changes in power levels may place data that is in the storage device but not yet in nonvolatile memory at risk. Data corruption may begin to occur before the auto commit memory is even aware or notified that there has been a disruption in power.

For example the PCI e specification indicates that in the event that a power disruption is signaled data should be assumed corrupted and not stored in certain circumstances. Similar potential corruption may occur for storage devices connected to hosts using other connection types such as PCI serial advanced technology attachment serial ATA or SATA parallel ATA PATA small computer system interface SCSI IEEE 1394 FireWire Fiber Channel universal serial bus USB PCIe AS or the like. A complication may arise when a power disruption occurs meaning that data received from that point to the present time may be presumed corrupt a period of time passes the disruption is sensed and signaled and the auto commit memory receives the signal and becomes aware of the power disruption. The lag between the power disruption occurring and the auto commit memory discovering the power disruption can allow corrupt data to enter the write data pipeline . In certain embodiments this corrupt data should be identified and not stored to the nonvolatile memory . Alternately this corrupt data can be stored in the nonvolatile memory and marked as corrupt as described below. For simplicity of description identifying corrupt data and not storing the data to the nonvolatile memory will be primarily used to describe the functions and features herein. Furthermore the host should be aware that this data was not stored or alternatively data for which integrity is a question is not acknowledged until data integrity can be verified. As a result corrupt data should not be acknowledged.

The storage device also includes the auto commit memory . In certain embodiments the auto commit memory is in communication with managed by or at least partially integrated with the storage controller . The auto commit memory may for instance cooperate with a software driver and or firmware for the storage device . In one embodiment at least a portion of the auto commit memory is implemented on the storage device so that the auto commit memory continues to function during a partial or complete power loss using power from the secondary power supply even if the host is no longer functioning.

In one embodiment the auto commit memory initiates a power loss mode in the storage device in response to a reduction in power from the primary power connection . During the power loss mode the auto commit memory in one embodiment flushes data that is in the storage device that is not yet stored in nonvolatile memory into the nonvolatile memory . In particular embodiments the auto commit memory flushes the data that has been acknowledged and is in the storage device that is not yet stored in nonvolatile memory into the nonvolatile memory . In certain embodiments described below the auto commit memory may adjust execution of data operations on the storage device to ensure that essential operations complete before the secondary power supply loses sufficient power to complete the essential operations i.e. during the power hold up time that the secondary power supply provides.

In certain embodiments the essential operations comprise those operations for data that has been acknowledged as having been stored such as acknowledged write operations. In other embodiments the essential operations comprise those operations for data that has been acknowledged as having been stored and erased. In other embodiments the essential operations comprise those operations for data that have been acknowledged as having been stored read and erased. The auto commit memory may also terminate non essential operations to ensure that those non essential operations do not consume power unnecessarily and or do not block essential operations from executing for example the auto commit memory may terminate erase operations read operations unacknowledged write operations and the like.

In one embodiment terminating non essential operations preserves power from the secondary power supply allowing the secondary power supply to provide the power hold up time. In a further embodiment the auto commit memory quiesces or otherwise shuts down operation of one or more subcomponents of the storage device during the power loss mode to conserve power from the secondary power supply . For example in various embodiments the auto commit memory may quiesce operation of the read data pipeline a read direct memory access DMA engine and or other subcomponents of the storage device that are associated with non essential operations.

The auto commit memory may also be responsible for determining what data was corrupted by the power disruption preventing the corrupt data from being stored in nonvolatile memory and ensuring that the host is aware that the corrupted data was never actually stored on the storage device . This prevents corruption of data in the storage device resulting from the power disruption.

In one embodiment the system includes a plurality of storage devices . The auto commit memory in one embodiment manages power loss modes for each storage device in the plurality of storage devices providing a system wide power loss mode for the plurality of storage devices . In a further embodiment each storage device in the plurality of storage devices includes a separate auto commit memory that manages a separate power loss mode for each individual storage device . The auto commit memory in one embodiment may quiesce or otherwise shut down one or more storage devices of the plurality of storage devices to conserve power from the secondary power supply for executing essential operations on one or more other storage devices .

In one embodiment the system includes one or more adapters for providing electrical connections between the host and the plurality of storage devices . An adapter in various embodiments may include a slot or port that receives a single storage device an expansion card or daughter card that receives two or more storage devices or the like. For example in one embodiment the plurality of storage devices may each be coupled to separate ports or slots of the host . In another example embodiment one or more adapters such as daughter cards or the like may be electrically coupled to the host i.e. connected to one or more slots or ports of the host and the one or more adapters may each provide connections for two or more storage devices .

In one embodiment the system includes a circuit board such as a motherboard or the like that receives two or more adapters such as daughter cards or the like and each adapter receives two or more storage devices . In a further embodiment the adapters are coupled to the circuit board using PCI e slots of the circuit board and the storage devices are coupled to the adapters using PCI e slots of the adapters. In another embodiment the storage devices each comprise a dual in line memory module DIMM of non volatile solid state storage such as Flash memory or the like. In one embodiment the circuit board the adapters and the storage devices may be external to the host and may include a separate primary power connection . For example the circuit board the adapters and the storage devices may be housed in an external enclosure with a power supply unit PSU and may be in communication with the host using an external bus such as eSATA eSATAp SCSI FireWire Fiber Channel USB PCIe AS or the like. In another embodiment the circuit board may be a motherboard of the host and the adapters and the storage devices may be internal storage of the host .

In view of this disclosure one of skill in the art will recognize many configurations of adapters and storage devices for use in the system . For example each adapter may receive two storage devices four storage devices or any number of storage devices. Similarly the system may include one adapter two adapters three adapters four adapters or any supported number of adapters. In one example embodiment the system includes two adapters and each adapter receives four storage devices for a total of eight storage devices .

In one embodiment the secondary power supply provides electric power to each of a plurality of storage devices . For example the secondary power supply may be disposed in a circuit on a main circuit board or motherboard and may provide power to several adapters. In a further embodiment the system includes a plurality of secondary power supplies that each provide electric power to a subset of a plurality of storage devices . For example in one embodiment each adapter may include a secondary power supply for storage devices of the adapter. In a further embodiment each storage device may include a secondary power supply for the storage device . In view of this disclosure one of skill in the art will recognize different arrangements of secondary power supplies for providing power to a plurality of storage devices .

The systems methods and apparatus described above may be leveraged to implement an auto commit memory capable of implementing memory semantic write operations e.g. persistent writes at CPU memory write granularity and speed. By guaranteeing that certain commit actions for the write operations will occur even in the case of a power failure or other restart event in certain embodiments volatile memory such as DRAM SRAM BRAM or the like may be used as considered or represented as non volatile.

A restart event as used herein comprises an intentional or unintentional loss of power to at least a portion of the host computing device and or a non volatile storage device. A restart event may comprise a system reboot reset or shutdown event a power fault power loss or power failure event or another interruption or reduction of power. By guaranteeing certain commit actions the auto commit memory may allow storage clients to resume execution states even after a restart event may allow the storage clients to persist different independent data sets or the like.

As used herein the term memory semantic operations or more generally memory operations refers to operations having a granularity synchronicity and access semantics of volatile memory accesses using manipulatable memory pointers or the like. Memory semantic operations may include but are not limited to load store peek poke write read set clear and so on. Memory semantic operations may operate at a CPU level of granularity e.g. single bytes words cache lines or the like and may be synchronous e.g. the CPU waits for the operation to complete . In certain embodiments providing access at a larger sized granularity such as cache lines may increase access rates provide more efficient write combining or the like than smaller sized granularity access.

The ACM may be available to computing devices and or applications both local and remote using one or more of a variety of memory mapping technologies including but not limited to memory mapped I O MMIO port I O port mapped IO PMIO Memory mapped file I O and the like. For example the ACM may be available to computing devices and or applications both local and remote using a PCI e Base Address Register BAR or other suitable mechanism. ACM may also be directly accessible via a memory bus of a CPU using an interface such as a double data rate DDR memory interface HyperTransport QuickPath Interconnect QPI or the like. Accordingly the ACM may be accessible using memory access semantics such as CPU load store direct memory access DMA 3party DMA remote DMA RDMA atomic test and set and so on. The direct memory semantic access to the ACM disclosed herein allows many of the system and or virtualization layer calls typically required to implement committed operations to be bypassed e.g. call backs via asynchronous Input Output interfaces may be bypassed . In some embodiments an ACM may be mapped to one or more virtual ranges e.g. virtual BAR ranges virtual memory addresses or the like . The virtual mapping may allow multiple computing devices and or applications to share a single ACM address range e.g. access the same ACM simultaneously within different virtual address ranges . An ACM may be mapped into an address range of a physical memory address space addressable by a CPU so that the CPU may use load store instructions to read and write data directly to the ACM using memory semantic accesses. A CPU in a further embodiment may map the physically mapped ACM into a virtual memory address space making the ACM available to user space processes or the like as virtual memory.

The ACM may be pre configured to commit its contents upon detection of a restart condition or other pre determined triggering event and as such operations performed on the ACM may be viewed as being instantly committed. For example an application may perform a write commit operation on the ACM using memory semantic writes that operate at CPU memory granularity and speed without the need for separate corresponding commit commands which may significantly increase the performance of applications affected by write commit latencies. As used herein a write commit operation is an operation in which an application writes data to a memory location e.g. using a memory semantic access and then issues a subsequent commit command to commit the operation e.g. to persistent storage or other commit mechanism .

Applications whose performance is based on write commit latency the time delay between the initial memory write and the subsequent persistent commit operation typically attempt to reduce this latency by leveraging a virtual memory system e.g. using a memory backed file . In this case the application performs high performance memory semantic write operations in system RAM but in order to commit the operations must perform subsequent commit commands to persist each write operation to the backing file or other persistent storage . Accordingly each write commit operation may comprise its own separate commit command. For example in a database logging application each log transaction must be written and committed before a next transaction is logged. Similarly messaging systems e.g. store and forward systems must write and commit each incoming message before receipt of the message can be acknowledged. The write commit latency therefore comprises a relatively fast memory semantic write followed by a much slower operation to commit the data to persistent storage. Write commit latency may include several factors including access times to persistent storage system call overhead e.g. translations between RAM addresses backing store LBA etc. and so on. Examples of applications that may benefit from reduced write commit latency include but are not limited to database logging applications filesystem logging messaging applications e.g. store and forward semaphore primitives and so on.

The systems apparatus and methods for auto commit memory disclosed herein may be used to significantly increase the performance of write commit latency bound applications by providing direct access to a memory region at any suitable level of addressing granularity including byte level page level cache line level or other memory region level that is guaranteed to be committed in the event of a system failure or other restart event without the application issuing a commit command. Accordingly the write commit latency of an application may be reduced to the latency of a memory semantic access a single write over a system bus .

Accordingly when data is written to the ACM it may not initially be committed per se is not necessarily stored on a persistent memory media and or state rather a pre configured process is setup to preserve the ACM data and its state if a restart event occurs while the ACM data is stored in the ACM . The pre configuring of this restart survival process is referred to herein as arming. The ACM may be capable of performing the pre configured commit action autonomously and with a high degree of assurance despite the system experiencing failure conditions or another restart event. As such an entity that stores data on the ACM may consider the data to be instantaneously committed or safe from loss or corruption at least as safe as if the data were stored in a non volatile storage device such as a hard disk drive tape storage media or the like.

In embodiments where the ACM comprises a volatile memory media the ACM may make the volatile memory media appear as a non volatile memory may present the volatile memory as a non volatile medium or the like because the ACM preserves data such as ACM data and or ACM metadata across system restart events. The ACM may allow a volatile memory media to be used as a non volatile memory media by determining that a trigger event such as a restart or failure condition has occurred copying the contents of the volatile memory media to a non volatile memory media during a hold up time after the trigger event and copying the contents back into the volatile memory media from the non volatile memory media after the trigger event is over power has been restored the restart event has completed or the like.

In one embodiment the ACM is at least byte addressable. A memory media of the ACM in certain embodiments may be natively byte addressable directly providing the ACM with byte addressability. In another embodiment a memory media of the ACM is not natively byte addressable but a volatile memory media of the ACM is natively byte addressable and the ACM writes or commits the contents of the byte addressable volatile memory media to the non byte addressable memory media of the ACM in response to a trigger event so that the volatile memory media renders the ACM byte addressable.

The ACM may be accessible to one or more computing devices such as the host . As used herein a computing device such as the host refers to a computing device capable of accessing an ACM. The host may be a computing device that houses the ACM as a peripheral the ACM may be attached to a system bus of the host the ACM may be in communication with the host over a data network and or the ACM may otherwise be in communication with the host . The host in certain embodiments may access the ACM hosted by another computing device. The access may be implemented using any suitable communication mechanism including but not limited to CPU programmed IO CPIO port mapped IO PMIO memory mapped IO MMIO a Block interface a PCI e bus Infiniband RDMA or the like. The host may comprise one or more ACM users . As used herein an ACM user refers to any operating system OS virtual operating platform e.g. an OS with a hypervisor a guest OS application process thread entity utility user or the like that is configured to access the ACM .

The ACM may be physically located at one or more levels of the host . In one embodiment the ACM may be connected to a PCI e bus and may be accessible to the host with MMIO. In another embodiment the ACM may be directly accessible to a CPU of the host via a memory controller. For example the ACM may be directly attached to and or directly e.g. Quick Path Interconnect QPI in communication with a CPU of the host or the like. Volatile media of the ACM and non volatile backing media of the ACM in certain embodiments may not be physically co located within the same apparatus but may be in communication over a communications bus a data network or the like. In other embodiments as described below hardware components of the ACM may be tightly coupled and integrated in a single physical hardware apparatus. Volatile memory media and or non volatile memory media of the ACM in one embodiment may be integrated with or may otherwise cooperate with a CPU cache hierarchy of the host to take advantage of CPU caching technologies such as write combining or the like.

One or more ACM buffers in certain embodiments may be mapped into an address range of a physical memory address space addressable by a CPU a kernel or the like of the host device such as the memory system described below. For example one or more ACM buffers may be mapped as directly attached physical memory as MMIO addressable physical memory over a PCI e bus or otherwise mapped as one or more pages of physical memory. At least a portion of the physically mapped ACM buffers in a further embodiment may be mapped into a virtual memory address space accessible to user space processes or the like as virtual memory.

Allowing ACM users to directly address the ACM buffers in certain embodiments bypasses one or more layers of the traditional operating system memory stack of the host device providing direct load store operation access to kernel space and or user space applications. An operating system using a kernel module an application programming interface the storage management layer SML described below or the like in one embodiment maps and unmaps ACM buffers to and from the memory system for one or more ACM users and the ACM users may directly access an ACM buffer once the operating system maps the ACM buffer into the memory system . In a further embodiment the operating system may also service system flush calls for the ACM buffers or the like.

The SML and or the SML API described below in certain embodiments provide an interface for ACM users an operating system and or other entities to request certain ACM functions such as a map function an unmap function a flush function and or other ACM functions. To perform a flush operation in response to a flush request the ACM may perform a commit action for each ACM buffer associated with the flush request. Each ACM buffer is committed as indicated by the ACM metadata of the associated ACM buffer . A flush function in various embodiments may be specific to one or more ACM buffers system wide for all ACM buffers or the like. In one embodiment a CPU an operating system or the like for the host may request an ACM flush operation in response to or as part of a CPU cache flush a system wide data flush for the host or another general flush operation.

An ACM user an operating system or the like may request a flush operation to maintain data consistency prior to performing a maintenance operation such as a data snapshot or a backup to commit ACM data prior to reallocating an ACM buffer to prepare for a scheduled restart event or for other circumstances where flushing data from an ACM buffer may be beneficial. An ACM user an operating system or the like in certain embodiments may request that the ACM map and or unmap one or more ACM buffers to perform memory management for the ACM buffers to reallocate the ACM buffers between applications or processes to allocate ACM buffers for new data applications or processes to transfer use of the ACM buffers to a different host in shared ACM embodiments or to otherwise manipulate the memory mapping of the ACM buffers . In another embodiment the SML may dynamically allocate map and or unmap ACM buffers using a resource management agent as described below.

Since the ACM is guaranteed to auto commit the data stored thereon in the event of a trigger event the host or ACM user may view data written to the ACM as being instantaneously committed or non volatile as the host or ACM user may access the data both before and after the trigger event. Advantageously while the restart event may cause the ACM user to be re started or re initialized the data stored in the ACM is in the same state condition after the restart event as it was before the restart event. The host may therefore write to the ACM using memory write semantics and at CPU speeds and granularity without the need for explicit commit commands by relying on the pre configured trigger of the ACM to commit the data in the event of a restart or other trigger event .

The ACM may comprise a plurality of auto commit buffers each comprising respective ACM metadata . As discussed below the ACM metadata may include data to facilitate committing of ACM data in response to a triggering event for the auto commit buffer such as a logical identifier for data in the ACM buffer an identifier of a commit agent instructions for a commit process or other processing procedure security data or the like. The auto commit buffers may be of any suitable size from a single sector page byte or the like to a virtual or logical page size e.g. 80 to 400 kb . The size of the auto commit buffers may be adapted according to the storage capacity of the underlying non volatile storage media and or hold up time available from the secondary power supply .

In one embodiment the ACM may advertise or present to the host to ACM users or the like a storage capacity of the ACM buffers that is larger than an actual storage capacity of memory of the ACM buffers . To provide the larger storage capacity the ACM may dynamically map and unmap ACM buffers to the memory system and to the non volatile backing memory of the ACM such as the non volatile memory described above. For example the ACM may provide virtual address ranges for the ACM buffers and demand page data and or ACM buffers to the non volatile memory as ACM buffer accesses necessitate. In another embodiment for ACM buffers that are armed to commit to one or more predefined LBAs of the non volatile memory the ACM may dynamically move the ACM data and ACM metadata from the ACM buffers to the associated LBAs of the non volatile memory freeing storage capacity of the ACM buffers to provide a larger storage capacity. The ACM may further return the ACM data and ACM metadata back to one or more ACM buffers as ACM buffers become available certain addresses outside the data of currently loaded ACM buffers is requested or the like managing storage capacity of the ACM buffers .

The ACM is pre configured or armed to implement one or more triggered commit actions in response to a restart condition or other pre determined condition . As used herein a restart condition or event may include but is not limited to a software or hardware shutdown restart of a host a failure in a host computing device a failure of a component of the host e.g. failure of the bus a software fault e.g. an fault in software running on the host or other computing device a loss of the primary power connection an invalid shutdown or another event that may cause the loss of data stored in a volatile memory.

In one embodiment a restart event comprises the act of the host commencing processing after an event that can cause the loss of data stored within a volatile memory of the host or a component in the host . The host may commence resume processing once the restart condition or event has finished a primary power source is available and the like.

The ACM is configured to detect that a restart event condition has occurred and or respond to a restart event by initiating a recovery stage. During a recovery stage the ACM may restore the data of the ACM to the state prior to the restart event. Alternatively or in addition during the recovery stage the ACM may complete processing of ACM data or ACM metadata needed to satisfy a guarantee that data in the ACM is available to ACM users after the restart event. Alternatively or in addition during the recovery stage the ACM may complete processing of ACM data or ACM metadata needed to satisfy a guarantee that data in the ACM is committed after the restart event. As used herein commit means data in the ACM is protected from loss or corruption even after the restart event and is persisted as required per the arming information associated with the data. In certain embodiments the recovery stage includes processing ACM data and ACM metadata such that the ACM data is persisted even though the restart event occurred.

As used herein a triggered commit action is a pre configured commit action that is armed to be performed by the ACM in response to a triggering event e.g. a restart event a flush command or other pre determined event . In certain embodiments the triggered commit action persists at least enough ACM data and or ACM metadata to make data of the ACM available after a system restart to satisfy a guarantee of the ACM that the data will be accessible to an ACM user after a restart event in certain embodiments this guarantee is satisfied at least in part by committing and or persisting data of the ACM to non volatile memory media. A triggered commit action may be completed before during and or after a restart event. For example the ACM may write ACM data and ACM metadata to a predefined temporary location in the nonvolatile memory during a hold up time after a restart event and may copy the ACM data back into the ACM buffers to an intended location in the nonvolatile memory or perform other processing once the restart event is complete.

A triggered commit action may be armed when the ACM is requested and or a particular ACM buffer is allocated for use by a host . In some embodiments an ACM may be configured to implement a triggered commit action in response to other non restart conditions. For example an operation directed to a particular logical address e.g. a poke may trigger the ACM a flush operation may trigger the ACM or the like. This type of triggering may be used to commit the data of the ACM during normal operation e.g. non restart or non failure conditions .

The arming may occur when an auto commit buffer is mapped into the memory system of the host . Alternatively arming may occur as a separate operation. As used herein arming an auto commit buffer comprises performing the necessary configuration steps needed to complete the triggered action when the action is triggered. Arming may include for example providing the ACM metadata to the ACM or the like. In certain embodiments arming further includes performing the necessary configuration steps needed to complete a minimal set of steps for the triggered action such that the triggered action is capable of completing after a trigger event. In certain embodiments arming further includes verifying the arming data e.g. verifying that the contents of the auto commit buffer or portion thereof can be committed as specified in the ACM metadata and verifying that the ACM is capable and configured to properly perform the triggered action without error or interruption.

The verification may ensure that once armed the ACM can implement the triggered commit action when required. If the ACM metadata cannot be verified e.g. the logical identifier or other ACM metadata is invalid corrupt unavailable or the like the arming operation may fail memory semantic operations on the auto commit buffer may not be allowed unit the auto commit buffer is successfully armed with valid ACM metadata . For example an auto commit buffer that is backed by a hard disk having a one to one mapping between LBA and physical address may fail to arm if the LBA provided for the arming operation does not map to a valid and operational physical address on the disk. Verification in this case may comprise querying the disk to determine whether the LBA has a valid corresponding physical address and or using the physical address as the ACM metadata of the auto commit buffer .

The armed triggered commit actions are implemented in response to the ACM or other entity detecting and or receiving notification of a triggering event such as a restart condition. In some embodiments an armed commit action is a commit action that can be performed by the ACM and that requires no further communication with the host or other devices external to the isolation zone of the ACM discussed below . Accordingly the ACM may be configured to implement triggered commit actions autonomously of the host and or other components thereof. The ACM may guarantee that triggered commit actions can be committed without errors and or despite external error conditions. Accordingly in some embodiments the triggered commit actions of the ACM do not comprise and or require potentially error introducing logic computations and or calculations. In some embodiments a triggered commit action comprises committing data stored on the volatile ACM to a persistent storage location. In other embodiments a triggered commit action may comprise additional processing of committed data before during and or after a triggering event as described below. The ACM may implement pre configured triggered commit actions autonomously the ACM may be capable of implementing triggered commit actions despite failure or restart conditions in the host loss of primary power or the like. The ACM can implement triggered commit actions independently due to arming the ACM as described above.

The ACM metadata for an ACM buffer in certain embodiments identifies the data of the ACM buffer . For example the ACM metadata may identify an owner of the data may describe the data itself or the like. In one embodiment an ACM buffer may have multiple levels of ACM metadata for processing by multiple entities or the like. The ACM metadata may include multiple nested headers that may be unpackaged upon restart and used by various entities or commit agents to determine how to process the associated ACM data to fulfill the triggered commit action as described above. For example the ACM metadata may include block metadata file metadata application level metadata process execution point or callback metadata and or other levels of metadata. Each level of metadata may be associated with a different commit agent or the like. In certain embodiments the ACM metadata may include security data such as a signature for an owner of the associated ACM data a pre shared key a nonce or the like which the ACM may use during recovery to verify that a commit agent an ACM user or the like is authorized to access committed ACM metadata and or associated ACM data. In this manner the ACM may prevent ownership spoofing or other unauthorized access. In one embodiment the ACM does not release ACM metadata and or associated ACM data until a requesting commit agent ACM user or the like provides valid authentication such as a matching signature or the like.

One or more commit agents such as the commit management apparatus described below with regard to in certain embodiments process ACM data based on the associated ACM metadata to execute a triggered commit action. A commit agent in various embodiments may comprise software such as a device driver a kernel module the SML a thread a user space application or the like and or hardware such as the controller described below that is configured to interpret ACM metadata and to process the associated ACM data according to the ACM metadata . In embodiments with multiple commit agents the ACM metadata may identify one or more commit agents to process the associated ACM data. The ACM metadata may identify a commit agent in various embodiments by identifying a program function of the commit agent to invoke e.g. a file path of the program by including computer executable code of the commit agent e.g. binary code or scripts by including a unique identifier indicating which of a set of registered commit agents to use and or by otherwise indicating a commit agent associated with committed ACM metadata . The ACM metadata in certain embodiments may be a functor or envelope which contains the information such as function pointer and bound parameters for a commit agent to commit the ACM data upon restart recovery.

In one embodiment a primary commit agent processes ACM metadata and hands off or transfers ACM metadata and or ACM data to one or more secondary commit agents identified by the ACM metadata . A primary commit agent in one embodiment may be integrated with the ACM the controller or the like. An ACM user or other third party in certain embodiments may provide a secondary commit agent for ACM data that the ACM user or other third party owns and the primary commit agent may cooperate with the provided secondary commit agent to process the ACM data. The one or more commit agents for ACM data in one embodiment ensure and or guarantee that the ACM data remains accessible to an owner of the ACM data after a restart event. As described above with regard to triggered commit actions a commit agent may process ACM metadata and associated ACM data to perform one or more triggered commit actions before during and or after a trigger event such as a failure or other restart event.

In one embodiment a commit agent in cooperation with the ACM or the like may store the ACM metadata in a persistent or non volatile location in response to a restart or other trigger event. The commit agent may store the ACM metadata at a known location may store pointers to the ACM metadata at a known location may provide the ACM metadata to an external agent or data store or the like so that the commit agent may process the ACM metadata and associated ACM data once the restart or other trigger event has completed. The known location may include one or more predefined logical block addresses or physical addresses of the non volatile memory a predefined file or the like. In certain embodiments hardware of the ACM is configured to cooperate to write the ACM metadata and or pointers to the ACM metadata at a known location. In one embodiment the known location may be a temporary location that stores the ACM data and ACM metadata until the host has recovered from a restart event and the commit agent may continue to process the ACM data and ACM metadata . In another embodiment the location may be a persistent location associated with the ACM metadata .

In response to completion of a restart event or other trigger event during recovery in one embodiment a commit agent may locate and retrieve the ACM metadata from the non volatile memory from a predefined location or the like. The commit agent in response to locating and retrieving the ACM metadata locates the ACM data associated with the retrieved ACM metadata . The commit agent in certain embodiments may locate the ACM data in a substantially similar manner as the commit agent locates the ACM metadata retrieving ACM data from a predefined location retrieving pointers to the ACM data from a predefined location receiving the ACM data from an external agent or data store or the like. In one embodiment the ACM metadata identifies the associated ACM data and the commit agent uses the ACM metadata to locate and retrieve the associated ACM data. For example the commit agent may use a predefined mapping to associate ACM data with ACM metadata e.g the Nth piece of ACM data may be associated with the Nth piece of ACM metadata or the like the ACM metadata may include a pointer or index for the associated ACM data or another predefined relationship may exist between committed ACM metadata and associated ACM data. In another embodiment an external agent may indicate to the commit agent where associated ACM data is located.

In response to locating and retrieving the ACM metadata and associated ACM data the commit agent interprets the ACM metadata and processes the associated ACM data based on the ACM metadata . For example in one embodiment the ACM metadata may identify a block storage volume and LBA s where the commit agent is to write the ACM data upon recovery. In another embodiment the ACM metadata may identify an offset within a file within a file system where the commit agent is to write the ACM data upon recovery. In a further embodiment the ACM metadata may identify an application specific persistent object where the commit agent is to place the ACM data upon recovery such as a database record or the like. The ACM metadata in an additional embodiment may indicate a procedure for the commit agent to call to process the ACM data such as a delayed procedure call or the like. In an embodiment where the ACM advertises or presents volatile ACM buffers as nonvolatile memory the ACM metadata may identify an ACM buffer where the commit agent is to write the ACM data upon recovery.

In certain embodiments the ACM metadata may identify one or more secondary commit agents to further process the ACM metadata and or associated ACM data. A secondary commit agent may process ACM metadata and associated ACM data in a substantially similar manner to the commit agent described above. Each commit agent may process ACM data in accordance with a different level or subset of the ACM metadata or the like. The ACM metadata may identify a secondary commit agent in various embodiments by identifying a program function of the secondary commit agent to invoke e.g. a file path of the program by including computer executable code of the secondary commit agent by including a unique identifier indicating which of a set of registered secondary commit agents to use and or by otherwise indicating a secondary commit agent associated with committed ACM metadata .

In one embodiment a secondary commit agent processes a remaining portion of the ACM metadata and or of the ACM data after a previous commit agent has processed the ACM metadata and or the ACM data. In a further embodiment the ACM metadata may identify another non volatile medium separate from the ACM for the secondary commit agent to persist the ACM data even after a host experiences a restart event. By committing the ACM metadata and the associated ACM data from the ACM buffers in response to a trigger event such as a failure or other restart condition and processing the ACM metadata and the associated ACM data once the trigger event has completed or recovered the ACM may guarantee persistence of the ACM data and or performance of the triggered commit action s defined by the ACM metadata .

The ACM is communicatively coupled to a host which like the host described above may comprise operating systems virtual machines applications a processor complex a central processing unit CPU and the like. In the example these entities are referred to generally as ACM users . Accordingly as used herein an ACM user may refer to an operating system a virtual machine operating system e.g. hypervisor an application a library a CPU fetch execute algorithm or other program or process. The ACM may be communicatively coupled to the host as well as the ACM users via a bus such as a system bus a processor s memory exchange bus or the like e.g. HyperTransport QuickPath Interconnect QPI PCI bus PCI e bus or the like . In some embodiments the bus comprises the primary power connection e.g. the non volatile storage device may be powered through the bus . Although some embodiments described herein comprise solid state storage devices such as certain embodiments of the non volatile storage device the disclosure is not limited in this regard and could be adapted to use any suitable recording memory storage device and or recording memory storage media .

The ACM may be tightly coupled to the device used to perform the triggered commit actions. For example the ACM may be implemented on the same device peripheral card or within the same isolation zone as the controller and or secondary power source . The tight coupling of the ACM to the components used to implement the triggered commit actions defines an isolation zone which may provide an acceptable level of assurance based on industry standards or other metric that the ACM is capable of implementing the triggered auto commit actions in the event of a restart condition. In the example the isolation zone of the ACM is provided by the tight coupling of the ACM with the autonomous controller and secondary power supply discussed below .

The controller may comprise an I O controller such as a network controller e.g. a network interface controller storage controller dedicated restart condition controller or the like. The controller may comprise firmware hardware a combination of firmware and hardware or the like. In the example the controller comprises a storage controller such as the storage controller and or non volatile storage device controller described above. The controller may be configured to operate independently of the host . As such the controller may be used to implement the triggered commit action s of the ACM despite the restart conditions discussed above such as failures in the host and or ACM users and or loss of the primary power connection .

The ACM is powered by a primary power connection which like the primary power connection described above may be provided by a system bus bus external power supply the host or the like. In certain embodiments the ACM also includes and or is coupled to a secondary power source . The secondary power source may power the ACM in the event of a failure to the primary power connection . The secondary power source may be capable of providing at least enough power to enable the ACM and or controller to autonomously implement at least a portion of a pre configured triggered commit action s when the primary power connection has failed. The ACM in one embodiment commits or persists at least enough data e.g. ACM data and ACM metadata while receiving power from the secondary power source to allow access to the data once the primary power connection has been restored. In certain embodiments as described above the ACM may perform at least a portion of the pre configured triggered commit action s after the primary power connection has been restored using one or more commit agents or the like.

The ACM may comprise volatile memory storage. In the example the ACM includes one or more auto commit buffers . The auto commit buffers may be implemented using a volatile Random Access Memory RAM . In some embodiments the auto commit buffers may be embodied as independent components of the ACM e.g. in separate RAM modules . Alternatively the auto commit buffers may be implemented on embedded volatile memory e.g. BRAM available within the controller a processor complex an FPGA or other component of the ACM .

Each of the auto commit buffers may be pre configured armed with a respective triggered commit action. In some embodiments each auto commit buffer may comprise its own respective ACM metadata . The ACM metadata in some embodiments identifies how and or where the data stored on the auto commit buffer is to be committed. In some examples the ACM metadata may comprise a logical identifier e.g. an object identifier logical block address LBA file name or the like associated with the data in the auto commit buffer . The logical identifier may be predefined. In one embodiment when an auto commit buffer is committed the data therein may be committed with the ACM metadata e.g. the data may be stored at a physical storage location corresponding to the logical identifier and or in association with the logical identifier . To facilitate committing of ACM data during a hold up time after a restart event the ACM may write ACM data and ACM metadata in a single atomic operation such as a single page write or the like. To permit writing of ACM and ACM metadata in a single atomic operation the ACM buffers may be sized to correspond to a single write unit for a non volatile storage media that is used by the ACM . In some embodiments the ACM metadata may comprise a network address an LBA or another identifier of a commit location for the data.

In a further embodiment a logical identifier may associate data of an auto commit buffer with an owner of the data so that the data and the owner maintain the ownership relationship after a restart event. For example the logical identifier may identify an application an application type a process ID an ACM user or another entity of a host device so that the ACM data is persistently associated with the identified entity. In one embodiment a logical identifier may be a member of an existing namespace such as a file system namespace a user namespace a process namespace or the like. In other embodiments a logical identifier may be a member of a new or separate namespace such as an ACM namespace. For example a globally unique identifier namespace as is typically used in distributed systems for identifying communicating entities may be used as an ACM namespace for logical identifiers. The ACM may process committed ACM data according to a logical identifier for the data once a restart event has completed. For example the ACM may commit the ACM data to a logical identifier associated with a temporary location in response to a restart event and may write the ACM data to a persistent location identified by another logical identifier during recovery after the restart event.

As described above the ACM may be tightly coupled with the components used to implement the triggered commit actions e.g. the ACM is implemented within an isolation zone which ensures that the data on the ACM will be committed in the event of a restart condition. As used herein a tight coupling refers to a configuration wherein the components used to implement the triggered commit actions of the ACM are within the same isolation zone or two or more distinct trusted isolation zones and are configured to operate despite external failure or restart conditions such as the loss of power invalid shutdown host failures or the like. illustrates a tight coupling between the ACM the auto commit buffers the controller which is configured to operate independently of the host and the secondary power source which is configured to power the controller and the ACM including the auto commit buffers while the triggered commit actions are completed. Examples of a tight coupling include but are not limited to including the controller the secondary power source and the auto commit buffers on a single printed circuit board PCB within a separate peripheral in electronic communication with the host and the like. In other embodiments the ACM may be tightly coupled to other a different set of components e.g. redundant host devices redundant communication buses redundant controllers alternative power supplies and so on .

The ACM may be accessible by the host and or ACM users running thereon. Access to the ACM may be provided using memory access semantics such as CPU load store commands DMA commands 3rd party DMA commands RDMA commands atomic test and set commands manipulatable memory pointers and so on. In some embodiments memory semantic access to the ACM is implemented over the bus e.g. using a PCI e BAR as described below .

In a memory semantic paradigm ACM users running on the host may access the ACM via a memory system of the host . The memory system may comprise a memory management unit virtual memory system virtual memory manager virtual memory subsystem or similar memory address space implemented by an operating system a virtualization system e.g. hypervisor an application or the like. A portion of the ACM e.g. one or more auto commit buffers may be mapped into the memory system such that memory semantic operations implemented within the mapped memory address range ACM address range are performed on the ACM .

The SML in certain embodiments allocates and or arbitrates the storage capacity of the ACM between multiple ACM users using a resource management agent or the like. The resource management agent of the SML may comprise a kernel module provided to an operating system of the host device a device driver a thread a user space application or the like. In one embodiment the resource management agent determines how much storage capacity of the ACM buffers to allocate to an ACM user and how long the allocation is to last. Because in certain embodiments the ACM commits or persists data across restart events the resource management agent may allocate storage capacity of ACM buffers across restart events.

The resource management agent may assign different ACM buffers to different ACM users such as different kernel and or user space applications. The resource management agent may allocate ACM buffers to different usage types may map ACM buffers to different non volatile memory locations for destaging or the like. In one embodiment the resource management agent may allocate the ACM buffers based on commit agents associated with the ACM buffers by the ACM metadata or the like. For example a master commit agent may maintain an allocation map in ACM metadata identifying allocation information for ACM buffers of the ACM and identifying in one embodiment one or more secondary commit agents and the master commit agent may allocate a portion of the ACM buffers to each of the secondary commit agents . In another embodiment commit agents may register with the resource management agent may request resources such as ACM buffers from the resource management agent or the like. The resource management agent may use a predefined memory management policy such as a memory pressure policy or the like to allocate and arbitrate ACM buffer storage capacity between ACM users .

In some embodiments establishing an association between an ACM address range within the memory system and the ACM may comprise pre configuring arming the corresponding auto commit buffer s with a triggered commit action. As described above this pre configuration may comprise associating the auto commit buffer with a logical identifier or other metadata which may be stored in the ACM metadata of the buffer . As described above the ACM may be configured to commit the buffer data to the specified logical identifier in the event of a restart condition or to perform other processing in accordance with the ACM metadata .

Memory semantic access to the ACM may be implemented using any suitable address and or device association mechanism. In some embodiments memory semantic access is implemented by mapping one or more auto commit buffers of the ACM into the memory system of the host . In some embodiments this mapping may be implemented using the bus . For example the bus may comprise a PCI e or similar communication bus and the mapping may comprise associating a Base Address Register BAR of an auto commit buffer of the ACM on the bus with the ACM address range in the memory system e.g. the host mapping a BAR into the memory system .

The association may be implemented by an ACM user e.g. by a virtual memory system of an operating system or the like through an API of a storage layer such as the storage management layer SML . The SML may be configured to provide access to the auto commit memory to ACM users . The storage management layer may comprise a driver kernel level application user level application library or the like. One example of an SML is the Virtual Storage Layer of Fusion io Inc. of Salt Lake City Utah. The SML may provide a SML API comprising inter alia an API for mapping portions of the auto commit memory into the memory system of the host for unmapping portions of the auto commit memory from the memory system of the host for flushing the ACM buffers and the like. The SML may be configured to maintain metadata which may include a forward index comprising associations between logical identifiers of a logical address space and physical storage locations on the auto commit memory and or persistent storage media. In some embodiments ACM may be associated with one or more virtual ranges that map to different address ranges of a BAR or other addressing mechanism . The virtual ranges may be accessed e.g. mapped by different ACM users . Mapping or exposing a PCI e ACM BAR to the host memory may be enabled on demand by way of a SML API call.

The SML API may comprise interfaces for mapping an auto commit buffer into the memory system . In some embodiments the SML API may extend existing memory management interfaces such as malloc calloc or the like to map auto commit buffers into the virtual memory range of ACM user applications e.g. a malloc call through the SML API may map one or more auto commit buffers into the memory system . Alternatively or in addition the SML API may comprise one or more explicit auto commit mapping functions such as ACM alloc ACM free or the like. Mapping an auto commit buffer may further comprise configuring a memory system of the host to ensure that memory operations are implemented directly on the auto commit buffer e.g. prevent caching memory operations within a mapped ACM address range .

The association between the ACM address range within the host memory system and the ACM may be such that memory semantic operations performed within a mapped ACM address range are implemented directly on the ACM without intervening system RAM or other intermediate memory in a typical write commit operation additional layers of system calls or the like . For example a memory semantic write operation implemented within the ACM address range may cause data to be written to the ACM on one or more of the auto commit buffers . Accordingly in some embodiments mapping the ACM address range may comprise disabling caching of memory operations within the ACM address range such that memory operations are performed on an ACM and are not cached by the host e.g. cached in a CPU cache in host volatile memory or the like . Disabling caching within the ACM address range may comprise setting a non cacheable flag attribute associated with the ACM range when the ACM range is defined.

As discussed above establishing an association between the host memory system and the ACM may comprise arming the ACM to implement a pre determined triggered commit action. The arming may comprise providing the ACM with a logical identifier e.g. a logical block address a file name a network address a stripe or mirroring pattern or the like . The ACM may use the logical identifier to arm the triggered commit action. For example the ACM may be triggered to commit data to a persistent storage medium using the logical identifier e.g. the data may be stored at a physical address corresponding to the logical identifier and or the logical identifier may be stored with the data in a log based data structure . Arming the ACM allows the host to view subsequent operations performed within the ACM address range and on the ACM as being instantly committed enabling memory semantic write granularity e.g. byte level operations and speed with instant commit semantics.

Memory semantic writes such as a store operation for a CPU are typically synchronous operations such that the CPU completes the operation before handling a subsequent operation. Accordingly memory semantic write operations performed in the ACM memory range can be viewed as instantly committed obviating the need for a corresponding commit operation in the write commit operation which may significantly increase the performance of ACM users affected by write commit latency. The memory semantic operations performed within the ACM memory range may be synchronous. Accordingly ACM may be configured to prevent the memory semantic operations from blocking e.g. waiting for an acknowledgement from other layers such as the bus or the like . Moreover the association between ACM address range and the ACM allow memory semantic operations to bypass system calls e.g. separate write and commit commands and their corresponding system calls that are typically included in write commit operations.

Data transfer between the host and the ACM may be implemented using any suitable data transfer mechanism including but not limited to the host performing processor IO operations PIO with the ACM via the bus the ACM or other device providing one or more DMA engines or agents data movers to transfer data between the host and the ACM the host performing processor cache write flush operations or the like.

As discussed above an ACM may be configured to automatically perform a pre configured triggered commit action in response to detecting certain conditions e.g. restart or failure conditions . In some embodiments the triggered commit action may comprise committing data stored on the ACM to a persistent storage media. Accordingly in some embodiments an ACM such as the ACM described above may be comprise persistent storage media. is a block diagram of a system depicting an embodiment of an ACM configured to implement triggered commit actions which may include committing data to a persistent solid state and or non volatile storage.

The ACM of the example may be tightly coupled to the non volatile storage device which comprises a controller . The controller may comprise a write data pipeline and a read data pipeline which may operate as described above. The non volatile storage device may be capable of persisting data on a non volatile memory such as solid state storage media.

A commit management apparatus is used to commit data to the non volatile memory in response to a trigger event such as loss of primary power connection or other pre determined trigger event. Accordingly the commit management apparatus may comprise and or be configured to perform the functions of the auto commit memory described above. The commit management apparatus may be further configured to commit data on the ACM e.g. the contents of the auto commit buffers to the non volatile memory in response to a restart condition or on request from the host and or ACM users and in accordance with the ACM metadata . The commit management apparatus is one embodiment of a commit agent .

The data on the ACM may be committed to the persistent storage in accordance with the ACM metadata such as a logical identifier or the like. The ACM may commit the data to a temporary location for further processing after a restart event may commit the data to a final intended location or the like as described above. If the non volatile memory is sequential storage device committing the data may comprise storing the logical identifier or other ACM metadata with the contents of the auto commit buffer e.g. in a packet or container header . If the non volatile memory comprises a hard disk having a 1 1 mapping between logical identifier and physical address the contents of the auto commit buffer may be committed to the storage location to which the logical identifier maps. Since the logical identifier or other ACM metadata associated with the data is pre configured e.g. armed the ACM implements the triggered commit action independently of the host . The secondary power supply supplies power to the volatile auto commit buffers of the ACM until the triggered commit actions are completed and or confirmed to be completed or until the triggered commit actions are performed to a point at which the ACM may complete the triggered commit actions during recovery after a restart event.

In some embodiments the ACM commits data in a way that maintains an association between the data and its corresponding logical identifier per the ACM metadata . If the non volatile memory comprises a hard disk the data may be committed to a storage location corresponding to the logical identifier which may be outside of the isolation zone e.g. using a logical identifier to physical address conversion . In other embodiments in which the non volatile memory comprises a sequential media such as solid state storage media the data may be stored sequentially and or in a log based format as described in above and or in U.S. Provisional Patent Application Publication No. 61 373 271 entitled APPARATUS SYSTEM AND METHOD FOR CACHING DATA and filed 12 Aug. 2010 which is hereby incorporated by reference in its entirety. The sequential storage operation may comprise storing the contents of an auto commit buffer with a corresponding logical identifier as indicated by the ACM metadata . In one embodiment the data of the auto commit buffer and the corresponding logical identifier are stored together on the media according to a predetermined pattern. In certain embodiments the logical identifier is stored before the contents of the auto commit buffer . The logical identifier may be included in a header of a packet comprising the data or in another sequential and or log based format. The association between the data and logical identifier may allow a data index to be reconstructed as described above.

As described above the auto commit buffers of the ACM may be mapped into the memory system of the host enabling the ACM users of access these buffers using memory access semantics. In some embodiments the mappings between logical identifiers and auto commit buffers may leverage a virtual memory system of the host .

For example an address range within the memory system may be associated with a memory mapped file. As discussed above a memory mapped file is a virtual memory abstraction in which a file portion of a file or block device is mapped into the memory system address space for more efficient memory semantic operations on data of the non volatile storage device . An auto commit buffer may be mapped into the host memory system using a similar abstraction. The ACM memory range may therefore be represented by a memory mapped file. The backing file must be stored on the non volatile memory within the isolation zone See below or another network attached non volatile storage device also protected by an isolation zone . The auto commit buffers may correspond to only a portion of the file the file itself may be very large exceeding the capacity of the auto commit buffers and or the non volatile memory . When a portion of a file is mapped to an auto commit buffer the ACM user or other entity may identify a desired offset within the file and the range of blocks in the file that will operate with ACM characteristics e.g. have ACM semantics . This offset will have a predefined logical identifier and the logical identifier and range may be used to trigger committing the auto commit buffer s mapped within the file. Alternatively a separate offset for a block or range of blocks into the file may serve as a trigger for committing the auto commit buffer s mapped to the file. For example anytime a memory operation load store poke etc. is performed on data in the separate offset or range of blocks may result in a trigger event that causes the auto commit buffer s mapped to the file to be committed.

The underlying logical identifier may change however e.g. due to changes to other portions of the file file size changes etc. . When a change occurs the SML via the SML API an ACM user or other entity may update the ACM metadata of the corresponding auto commit buffers . In some embodiments the SML may be configured to query the host operating system hypervisor or other application for updates to the logical identifier of files associated with auto commit buffers . The queries may be initiated by the SML API and or may be provided as a hook callback mechanism into the host . When the ACM user no longer needs the auto commit buffer the SML may de allocate the buffer as described above. De allocation may further comprise informing the host that updates to the logical identifier are no longer needed.

In some embodiments a file may be mapped across multiple storage devices e.g. the storage devices may be formed into a RAID group may comprise a virtual storage device or the like . Associations between auto commit buffers and the file may be updated to reflect the file mapping. This allows the auto commit buffers to commit the data to the proper storage device. The ACM metadata of the auto commit buffers may be updated in response to changes to the underlying file mapping and or partitioning as described above. Alternatively the file may be locked to a particular mapping or partition while the auto commit buffers are in use. For example if a remapping repartitioning of a file is required the corresponding auto commit buffers may commit data to the file and then be re associated with the file under the new mapping partitioning scheme. The SML API may comprise interfaces and or commands for using the SML to lock a file release a file and or update ACM metadata in accordance with changes to a file.

Committing the data to solid state non volatile storage may comprise the storage controller accessing data from the ACM auto commit buffers associating the data with the corresponding logical identifier e.g. labeling the data and injecting the labeled data into the write data pipeline as described above. In some embodiments to ensure there is a page program command capable of persisting the ACM data the storage controller maintains two or more pending page programs during operation. The ACM data may be committed to the non volatile memory before writing the power loss identifier power cut fill pattern described above.

In some embodiments the ACMs A and B may implement a striping scheme e.g. a RAID scheme . In this case different portions of the host data may be sent to different ACMs A and or B. Driver level software such as a volume manager implemented by the SML and or operating system may map host data to the proper ACM per the striping pattern.

In some configurations the memory access semantics provided by the ACMs may be adapted according to a particular storage striping pattern. For example if host data is mirrored from the ACM A to the ACM B a memory semantic write may not complete and or an acknowledgement may not be returned until the ACM A verifies that the data was sent to the ACM B under the instant commit semantic . Similar adaptations may be implemented when ACMs are used in a striping pattern e.g. a memory semantic write may be not return and or be acknowledged until the striping pattern for a particular operation is complete . For example in a copy on write operation the ACM A may store the data of an auto commit buffer and then cause the data to be copied to the ACM B. The ACM A may not return an acknowledgment for the write operation or allow the data to be read until the data is copied to the ACM B.

The use of mirrored ACM devices A and B may be used in a high availability configuration. For example the ACM devices A and B may be implemented in separate host computing devices. Memory semantic accesses to the devices A and B are mirrored between the devices as described above e.g. using PCI e access . The devices may be configured to operate in high availability mode such that device proxying may not be required. Accordingly trigger operations as well as other memory semantic accesses may be mirrored across both devices A and B but the devices A and B may not have to wait for a acknowledge from the other before proceeding which removes the other device from the write commit latency path.

The commit management apparatus includes a monitor module which may be configured to detect restart conditions such as power loss or the like. The monitor module may be configured to sense triggering events such as restart conditions e.g. shutdown restart power failures communication failures host or application failures and so on and in response to initiate the commit module to initiate the commit loss mode of the apparatus failure loss mode and or to trigger the operations of other modules such as modules and or . The commit module includes an identification module terminate module corruption module and completion module which may operate as described above.

The identification module may be further configured to identify triggered commit actions to be performed for each ACM buffer of the ACM . As discussed above the identification module may prioritize operations based on relative importance with acknowledged operations being given a higher priority than non acknowledged operations. The contents of auto commit buffers that are armed to be committed may be assigned a high priority due to the instant commit semantics supported thereby. In some embodiments the ACM triggered commit actions may be given a higher priority than the acknowledged contents of the write data pipeline . Alternatively the contents of armed auto commit buffers may be assigned the next highest priority. The priority assignment may be user configurable via an API IO control IOCTL or the like .

The termination module terminates non essential operations to allow essential to continue as described above. The termination module may be configured to hold up portions of the ACM that are armed to be committed e.g. armed auto commit buffers and may terminate power to non armed unused portions of the auto commit memory . The termination module may be further configured to terminate power to portions of the ACM individual auto commit buffers as the contents of those buffers are committed.

The corruption module identifies corrupt or potentially corrupt data in the write data pipeline as described above. The module may be further configured to identify corrupt ACM data data that was written to the ACM during a power disturbance or other restart condition . The corruption module may be configured to prevent corrupt data on the ACM from being committed in a triggered commit action.

An ACM module is configured to access armed auto commit buffers in the auto commit memory identify the ACM metadata associated therewith e.g. label the data with the corresponding logical identifier per the ACM metadata and inject the data and metadata into the write data pipeline of the non volatile storage controller . In some embodiments the logical identifier or other ACM metadata of the auto commit buffer may be stored in the buffer itself. In this case the contents of the auto commit buffer may be streamed directly into a sequential and or log based storage device without first identifying and or labeling the data. The ACM module may inject data before or after data currently in the write data pipeline . In some embodiments data committed from the ACM is used to fill out the remainder of a write buffer of the write data pipeline after removing potentially corrupt data . If the remaining capacity of the write buffer is insufficient the write buffer is written to the non volatile storage and a next write buffer is filled with the remaining ACM data.

As discussed above in some embodiments the non volatile storage controller may maintain an armed write operation logical page write to store the contents of the write data pipeline in the event of power loss. When used with an ACM two or more armed write operations logical page writes may be maintained to ensure the contents of both the write data pipeline and all the armed buffers of the ACM can be committed in the event of a restart condition. Because a logical page in a write buffer may be partially filled when a trigger event occurs the write buffer is sized to hold at least one more logical page of data than the total of all the data stored in all ACM buffers of the ACM and the capacity of data in the write data pipeline that has been acknowledged as persisted. In this manner there will be sufficient capacity in the write buffer to complete the persistence of the ACM in response to a trigger event. Accordingly the auto commit buffers may be sized according to the amount of data the ACM is capable of committing. Once this threshold is met the SML may reject requests to use ACM buffers until more become available.

The completion module is configured to flush the write data pipeline regardless of whether the certain buffers packets and or pages are completely filled. The completion module is configured to perform the flush and insert the related padding data after data on the ACM if any has been injected into the write data pipeline . The completion module may be further configured to inject completion indicator into the write data pipeline which may be used to indicate that a restart condition occurred e.g. a restart condition fill pattern . This fill pattern may be included in the write data pipeline after injecting the triggered data from the ACM .

As discussed above the secondary power supply may be configured to provide sufficient power to store the contents of the ACM as well as data in the write data pipeline . Storing this data may comprise one or more write operations e.g. page program operations in which data is persistently stored on the non volatile storage media . In the event a write operation fails another write operation on a different storage location may be attempted. The attempts may continue until the data is successfully persisted on the non volatile storage media . The secondary power supply may be configured to provide sufficient power for each of a plurality of such page program operations to complete. Accordingly the secondary power supply may be configured to provide sufficient power to complete double or more page program write operations as required to store the data of the ACM and or write data pipeline .

The host may be communicatively coupled to the ACM via a bus which may comprise a PCI e bus or the like. Portions of the ACM are made accessible to the host may mapping in auto commit buffers into the host . In some embodiments mapping comprises associating an address range within the host memory system with an auto commit buffer of the ACM . These associations may be enabled using the SML API and or SML available on the host .

The SML may comprise libraries and or provide interfaces e.g. SML API to implement the memory access semantics described above. The API may be used to access the ACM using memory access semantics via a memory semantic access module . Other types of access such as access to the non volatile storage may be provided via a block device interface .

The SML may be configured to memory map auto commit buffers of the ACM into the memory system via the SML API . The memory map may use a virtual memory abstraction of the memory system . For example a memory map may be implemented using a memory mapped file abstraction. In this example the operating system or application designates a file to be mapped into the memory system . The file is associated with a logical identifier LID e.g. logical block address which may be maintained by a file system an operating system or the like.

The memory mapped file may be associated with an auto commit buffer of the ACM . The association may be implemented by the SML using the bus . The SML associates the address range of the memory mapped file in the memory system with a device address of an auto commit buffer on the ACM . The association may comprise mapping a PCI e BAR into the memory system . In the example the ACM address range in the memory system is associated with the auto commit buffer .

As discussed above providing memory access semantics to the ACM may comprise arming the ACM to commit data stored thereon in the event of failure or other restart. The pre configured arming ensures that in the event of a restart data stored on the ACM will be committed to the proper logical identifier. The pre configuration of the trigger condition enables applications to access the auto commit buffer using instant commit memory access semantics. The logical identifier used to arm the auto commit buffer may be obtained from an operating system the memory system e.g. virtual memory system or the like.

The SML may be configured to arm the auto commit buffers with a logical identifier e.g. automatically by callback and or via the SML API . Each auto commit buffer may be armed to commit data to a different logical identifier different LBA persistent identifier or the like which may allow the ACM to provide memory semantic access to a number of different concurrent ACM users . In some embodiments arming an auto commit buffer comprises setting the ACM metadata with a logical identifier. In the example the ACM address range is associated with the logical identifier and the ACM metadata of the associated auto commit buffer is armed with the corresponding logical identifier .

The SML may arm an auto commit buffer using an I O control IOCTL command comprising the ACM address range the logical identifier and or an indicator of which auto commit buffer is to be armed. The SML through the SML API may provide an interface to disarm or detach the auto commit buffer . The disarm command may cause the contents of the auto commit buffer to be committed as described above e.g. committed to the non volatile storage device . The detach may further comprise disarming the auto commit buffer e.g. clearing the ACM metadata . The SML may be configured to track mappings between address ranges in the memory system and auto commit buffers so that a detach command is performed automatically.

Alternatively or in addition the SML may be integrated into the operating system or virtual operating system e.g. hypervisor of the host . This may allow the auto commit buffers to be used by a virtual memory demand paging system. The operating system may through the SML API or other integration technique map arm auto commit buffers for use by ACM users . The operating system may issue commit commands when requested by an ACM user and or its internal demand paging system. Accordingly the operating system may use the ACM as another generally available virtual memory resource.

Once an ACM user has mapped the ACM address range to an auto commit buffer and has armed the buffer the ACM user may access the resource using memory access semantics and may consider the memory accesses to be logically committed as soon as the memory access has completed. The ACM user may view the memory semantic accesses to the ACM address range to be instantly committed because the ACM is configured to commit the contents of the auto commit buffer to the logical identifier regardless of experiencing restart conditions. Accordingly the ACM user may not be required to perform separate write and commit commands e.g. a single memory semantic write is sufficient to implement a write commit . Moreover the mapping between the auto commit buffer and the ACM disclosed herein removes overhead due to function calls system calls and even a hypervisor if the ACM user is running in a virtual machine that typically introduce latency into the write commit path. The write commit latency time of the ACM user may therefore be reduced to the time required to access the ACM itself.

As described above in certain embodiments the host may map one or more ACM buffers into an address range of a physical memory address space addressable by a CPU a kernel or the like of the host device such as the memory system as directly attached physical memory as MMIO addressable physical memory over a PCI e bus or otherwise mapped as one or more pages of physical memory. The host may further map at least a portion of the physically mapped ACM buffers into a virtual memory address space accessible to user space processes or the like as virtual memory. The host may map the entire capacity of the physically mapped ACM buffers into a virtual memory address space a portion of the physically mapped ACM buffers into a virtual memory address space or the like.

In a similar manner the host may include a virtual machine hypervisor host operating system or the like that maps the physically mapped ACM buffers into an address space for a virtual machine or guest operating system. The physically mapped ACM buffers may appear to the virtual machine or guest operating system as physically mapped memory pages with the virtual machine hypervisor or host operating system spoofing physical memory using the ACM buffers . A resource management agent as described above may allocate arbitrate storage capacity of the ACM buffers among multiple virtual machines guest operating systems or the like.

Because in certain embodiments virtual machines guest operating systems or the like detect the physically mapped ACM buffers as if they were simply physically mapped memory the virtual machines can sub allocate arbitrate the ACM buffers into one or more virtual address spaces for guest processes or the like. This allows processes within guest operating systems in one embodiment to change ACM data and or ACM metadata directly without making guest operating system calls without making requests to the hypervisor or host operating system or the like.

In another embodiment instead of spoofing physical memory for a virtual machine and or guest operating system a virtual machine hypervisor a host operating system or the like of the host device may use para virtualization techniques. For example a virtual machine and or guest operating system may be aware of the virtual machine hypervisor or host operating system and may work directly with it to allocate arbitrate the ACM buffers or the like. When the ACM is used in a virtual machine environment in which one or more ACM users operate within a virtual machine maintained by a hypervisor the hypervisor may be configured to provide ACM users operating within the virtual machine with access to the SML API and or SML .

The hypervisor may access the SML API to associate logical identifiers with auto commit buffers of the ACM as described above. The hypervisor may then provide one or more armed auto commit buffers to the ACM users e.g. by mapping an ACM address range within the virtual machine memory system to the one or more auto commit buffers . The ACM user may then access the ACM using memory access semantics e.g. efficient write commit operations without incurring overheads due to inter alia hypervisor and other system calls. The hypervisor may be further configured to maintain the ACM address range in association with the auto commit buffers until explicitly released by the ACM user e.g. the keep the mapping from changing during use . Para virtualization and cooperation in certain embodiments may increase the efficiency of the ACM in a virtual machine environment.

In some embodiments the ACM user may be adapted to operate with the instant commit memory access semantics provided by the ACM . For example since the armed auto commit buffers are triggered to commit in the event of a restart without an explicit commit command the order in which the ACM user performs memory access to the ACM may become a consideration. The ACM user may employ memory barriers complier flags and the like to ensure the proper ordering of memory access operations.

For example read before write hazards may occur where an ACM user attempts to read data through the block device interface that is stored on the ACM via the memory semantic interface . In some embodiments the SML may maintain metadata tracking the associations between logical identifiers and or address ranges in the memory system and auto commit buffers . When an ACM user or other entity attempts to access a logical identifier that is mapped to an auto commit buffer e.g. through the block device interface the SML directs the request to the ACM via the memory semantic interface preventing a read before write hazard.

The SML may be configured to provide a consistency mechanism for obtaining a consistent state of the ACM e.g. a barrier snapshot or logical copy . The consistency mechanism may be implemented using metadata maintained by the SML which as described above may track the triggered auto commit buffers in the ACM . A consistency mechanism may comprise the SML committing the contents of all triggered auto commit buffers such that the state of the persistent storage is maintained e.g. store the contents of the auto commit buffers on the non volatile storage or other persistent storage .

As described above ACM users may access the ACM using memory access semantics at RAM granularity with the assurance that the operations will be committed if necessary in the event of restart failure power loss or the like . This is enabled by inter alia a mapping between the memory system of the host and corresponding auto commit buffers memory semantic operations implemented within an ACM memory range mapped to an auto commit buffer are implemented directly on the buffer . As discussed above data transfer between the host and the ACM may be implemented using any suitable data transfer mechanism including but not limited to the host performing processor IO operations PIO with the ACM via the bus e.g. MMIO PMIO and the like the ACM or other device providing one or more DMA engines or agents data movers to transfer data between the host and the ACM the host performing processor cache write flush operations or the like. Transferring data on the bus may comprise issuing a bus write operation followed by a read. The subsequent read may be required where the bus e.g. PCI bus does not provide an explicit write acknowledgement.

In some embodiments an ACM user may wish to transfer data to the ACM in bulk as opposed to a plurality of small transactions. Bulk transfers may be implemented using any suitable bulk transfer mechanism. The bulk transfer mechanism may be predicated on the features of the bus . For example in embodiments comprising a PCI e bus bulk transfer operations may be implemented using bulk register store CPU instructions.

Similarly certain data intended for the ACM may be cached in processor cache of the processor complex . Data that is cached in a processor cache may be explicitly flushed to the ACM to particular auto commit buffers using a CPU cache flush instruction or the like such as the serializing instruction described below.

The DMA engines described above may also be used to perform bulk data transfers between an ACM user and the ACM . In some embodiments the ACM may implement one or more of the DMA engines which may be allocated and or accessed by ACM users using the SML through the SML API . The DMA engines may comprise local DMA transfer engines for transferring data on a local system bus as well as RDMA transfer engines for transferring data using a network bus network interface or the like.

In some embodiments the ACM may be used in caching applications. For example the non volatile storage device may be used as cache for other backing store such as a hard disk network attached storage or the like not shown . One or more of the ACM auto commit buffers may be used as a front end to the non volatile storage cache a write back cache by configuring one or more of the auto commit buffers of the ACM to commit data to the appropriate logical identifiers in the non volatile storage . The triggered buffers are accessible to ACM users as described above e.g. by mapping the buffers into the memory system of the host . A restart condition causes the contents of the buffers to be committed to the non volatile storage cache. When the restart condition is cleared the cached data in the non volatile storage committed by the auto commit buffers on the restart condition will be viewed as dirty in the write cache and available for use and or migration to the backing store. The use of the ACM as a cache front end may increase performance and or reduce wear on the cache device.

In some embodiments auto commit buffers of the ACM may be leveraged as a memory write back cache by an operating system virtual memory system and or one or more CPUs of the host . Data cached in the auto commit buffers as part of a CPU write back cache may be armed to commit as a group. When committed the auto commit buffers may commit both data and the associated cache tags. In some embodiments the write back cache auto commit buffers may be armed with an ACM address or armed with a predetermined write back cache address . When the data is restored logical identifier information such as LBA and the like may be determined from a log or other data.

In some embodiments the SML may comprise libraries and or publish APIs adapted to a particular set of ACM users . For example the SML may provide an Instant Committed Log Library ICL adapted for applications whose performance is tied to write commit latency such as transaction logs database file system and other transaction logs store and forward messaging systems persistent object caching storage device metadata and the like.

The ICL provides mechanisms for mapping auto commit buffers of the ACM into the memory system of an ACM user as described above. ACM users or the ICL itself may implement an efficient supplier consumer paradigm for auto commit buffer allocation arming and access. For example a supplier thread or process in the application space of the ACM users may be used to allocate and or arm auto commit buffers for the ACM user e.g. map auto commit buffers to address ranges within the memory system of the host arm the auto commit buffers with a logical identifier and so on . A consumer thread or process of the ACM user may then accesses the pre allocated auto commit buffers . In this approach allocation and or arming steps are taken out of the write commit latency path of the consumer thread. The consumer thread of the ACM user may consider memory semantic accesses to the memory range mapped to the triggered auto commit buffers the ACM memory range as being instantly committed as described above.

Performance of the consumer thread s of the ACM user may be enhanced by configuring the supplier threads of an Instant Committed Log Library ICL or ACM user to allocate and or arm auto commit buffers in advance. When a next auto commit buffer is needed the ACM user have access a pre allocated armed buffer from a pool maintained by the supplier. The supplier may also perform cleanup and or commit operations when needed. For example if data written to an auto commit buffer is to be committed to persistent storage a supplier thread or another thread outside of the write commit path may cause the data to be committed using the SML API . Committing the data may comprise re allocating and or re arming the auto commit buffer for a consumer thread of the ACM user as described above.

The supplier consumer approach described above may be used to implement a rolling buffer. An ACM user may implement an application that uses a pre determined amount of rolling data. For example an ACM user may implement a message queue that stores the last 20 inbound messages and or the ACM user may manage directives for a non volatile storage device e.g. persistent trim directives or the like . A supplier thread may allocate auto commit buffers having at least enough capacity to hold the rolling data needed by the ACM user e.g. enough capacity to hold the last 20 inbound messages . A consumer thread may access the buffers using memory access semantics load and store calls as described above. The SML API or supplier thread of the ACM user may monitor the use of the auto commit buffers . When the consumer thread nears the end of its auto commit buffers the supplier thread may re initialize the head of the buffers by causing the data to be committed if necessary mapping the data to another range within the memory system and arming the auto commit buffer with a corresponding logical identifier. As the consumer continues to access the buffers the consumer stores new data at a new location that rolls over to the auto commit buffer that was re initialized by the supplier thread and continues to operate. In some cases data written to the rolling buffers described above may never be committed to persistent storage unless a restart condition or other triggering condition occurs . Moreover if the capacity of the auto commit buffers is sufficient to hold the rolling data of the ACM user the supplier threads may not have to perform re initialize re arming described above. Instead the supplier threads may simply re map auto commit buffers that comprise data that has rolled over and or discard the rolled over data therein .

In its simplest form a rolling buffer may comprise two ACM buffers and the SML may write to one ACM buffer for an ACM user while destaging previously written data from the other ACM buffer to a storage location such as the non volatile memory or the like. In response to filling one ACM buffer and completing a destaging process of the other ACM buffer the SML may transparently switch the two ACM buffers such that the ACM user writes to the other ACM buffer during destaging of the one ACM buffer in a ping pong fashion. The SML may implement a similar rolling process with more than two ACM buffers . The ICL in certain embodiments includes and or supports one or more transactional log API functions. An ACM user may use the ICL in these embodiments to declare or initialize a transactional log data structure.

As a parameter to a transactional log API command to create a transactional log data structure in one embodiment the ICL receives a storage location such as a location in a namespace and or address space of the non volatile storage or the like to which the SML may commit empty and or destage data of the transactional log from two or more ACM buffers in a rolling or circular manner as described above. Once an ACM user has initialized or declared a transactional log data structure in one embodiment the use of two or more ACM buffers to implement the transactional log data structure is substantially transparent to the ACM user with the performance and benefits of the ACM . The use of two or more ACM buffers in certain embodiments is transparent when the destage rate for the two or more ACM buffers is greater than or equal to the rate at which the ACM user writes to the two or more ACM buffers . The ICL in one embodiment provides byte level writes to a transactional log data structure using two or more ACM buffers .

In another example a supplier thread may maintain four 4 or more ACM buffers . A first ACM buffer may be armed and ready to accept data from the consumer as described above. A second ACM buffer may be actively accessed e.g. filled by a consumer thread as described above. A third ACM buffer may be in a pre arming process e.g. re initializing as described above and a fourth ACM buffer may be emptying or destaging e.g. committing to persistent storage as described above .

In some embodiments the ICL and or rolling log mechanisms described above may be used to implement an Intent Log for Synchronous Writes for a filesystem e.g. the ZFS file system . The log data ZIL may be fairly small 1 to 4 gigabytes and is typically write only. Reads may only be performed for file system recovery. One or more auto commit buffers may be used to store filesystem data using a rolling log and or demand paging mechanism as described above.

The ICL library may be configured to operate in a high availability mode as described above in conjunction with . In a high availability mode the SML and or bus sends commands pertaining to memory semantic accesses to two or more ACM each of which may implement the requested operations and or be triggered to commit data in the event of a restart condition.

The ACM disclosed herein may be used to enable other types of applications such as durable synchronization primitives. A synchronization primitive may include but is not limited to a semaphore mutex atomic counter test and set or the like.

A synchronization primitive may be implemented on an auto commit buffer . ACM users or other entities that wish to access the synchronization primitive may map the auto commit buffer into the memory system . In some embodiments each ACM user may map the synchronization primitive auto commit buffer into its own respective address range in the memory system . Since the different address ranges are all mapped to the same auto commit buffer all will show the same state of the synchronization primitive. ACM users on remote computing devices may map the synchronization primitive auto commit buffer into their memory system using an RDMA network or other remote access mechanism e.g. Infiniband remote PCI etc. .

In some embodiments the SML may comprise a Durable Synchronization Primitive Library DSL to facilitate the creation of and or access to synchronization primitives on the ACM . The DSL may be configured to facilitate one to many mappings as described above one auto commit buffer to many address ranges in the memory system .

The ACM users accessing the semaphore primitive may consider their accesses to be durable since if a restart condition occurs while the synchronization primitive is in use the state of the synchronization primitive will be persisted as described above the auto commit buffer of the synchronization primitive will be committed to the non volatile storage or other persistent storage .

As described above the SML may be used to map a file into the memory system virtual address space of the host . The file may be mapped in an Instant Committed Memory ICM mode. In this mode all changes made to the memory mapped file are guaranteed to be reflected in the file even if a restart condition occurs. This guarantee may be made by configuring the demand paging system to use an auto commit buffer of the ACM for all dirty pages of the ICM file. Accordingly when a restart condition occurs the dirty page will be committed to the file and no data will be lost.

In some embodiments the SML may comprise an ICM Library ICML to implement these features. The ICML may be integrated with an operating system and or virtual memory system of the host . When a page of an ICM memory mapped file is to become dirty the ICML prepares an auto commit buffer to hold the dirty page. The auto commit buffer is mapped into the memory system of the host and is triggered to commit to a logical identifier associated with the memory mapped file. As described above changes to the pages in the memory system are implemented on the auto commit buffer via the memory semantic access module .

The ICML may be configured to commit the auto commit buffers of the memory mapped file when restart conditions occur and or when the demand paging system of the host needs to use the auto commit buffer for another purpose. The determination of whether to detach the auto commit buffer from a dirty page may be made by the demand paging system by the SML e.g. using a least recently used LRU metric or the like or by some other entity e.g. an ACM user . When the auto commit buffer is detached the SML may cause its contents to be committed. Alternatively the contents of the auto commit buffer may be transferred to system RAM at which point the virtual memory mapping of the file may transition to use a RAM mapping mechanisms.

In some embodiments the SML or ICML may be configured to provide a mechanism to notify the operating system virtual memory system or the like that a page of a memory mapped file is about to become dirty in advance of an ACM user writing the data. This notification may allow the operating system to prepare an auto commit buffer for the dirty page in advance and prevent stalling when the write actually occurs while the auto commit buffer is mapped and armed . The notification and preparation of the auto commit buffer may implemented in a separate thread e.g. a supplier thread as described above .

The SML and or ICML may provide an API to notify the operating system that a particular page that is about to be written has no useful contents and should be zero filled. This notification may help the operating system to avoid unnecessary read operations.

The mechanisms for memory mapping a file to the ACM may be used in log type applications. For example the ICL library may be implemented to memory map a log file to one or more auto commit buffers as described above. A supplier thread may provide notifications to the operating system regarding which pages are about to become dirty and or to identify pages that do not comprise valid data.

Alternatively or in addition the ICML may be implemented without integration into an operating system of the host . In these embodiments the ICML may be configured to monitor and or trap system signals such as mprotect mmap and manual segmentation fault signals to emulate the demand paging operations typically performed by an operating system.

At step an auto commit buffer of the ACM may be mapped into the memory system of a computing device e.g. the host . The mapping may comprise associating a BAR address of the auto commit buffer with an address range in the memory system.

At step the auto commit buffer may be armed with ACM metadata configured to cause the auto commit buffer to be committed to a particular persistent storage and or at a particular location in the persistent storage in the event of a restart condition. In some embodiments the ACM metadata may comprise a logical identifier such as a LBA object identifier or the like. Step may comprise verifying that the ACM metadata is valid and or can be used to commit the contents of the auto commit buffer.

At step an ACM user such as an operating system application or the like may access the armed auto commit buffer using memory access semantics. The ACM user may consider the accesses to be instantly committed due to the arming of step . Accordingly the ACM user may implement instant committed writes that omit a separate and or explicit commit command. Moreover since the memory semantic accesses are directly mapped to the auto commit buffer via the mapping of step the memory semantic accesses may bypass systems calls typically required in virtual memory systems.

At step an auto commit buffer of an ACM is mapped into the memory system of a computing device e.g. the host and is armed as described above.

At step an ACM user accesses the auto commit buffer using memory access semantics e.g. by implementing memory semantic operations within the memory range mapped to the auto commit buffer at step .

At step a restart condition is detected. As described above the restart condition may be a system shutdown a system restart a loss of power a loss of communication between the ACM and the host computing device a software fault or any other restart condition that precludes continued operation of the ACM and or the host computing device.

At step the ACM implements the armed triggered commit actions on the auto commit buffer. The triggered commit action may comprise committing the contents of the auto commit buffer to persistent storage such as a solid state or other non volatile storage or the like.

At step the method ends until a next auto commit buffer is mapped and or armed or a restart condition is detected.

At step the method accesses armed auto commit buffers on the ACM if any . Accessing the armed auto commit buffer may comprise the method determining whether an auto commit buffer has been armed by inspecting the triggered ACM metadata thereof. If no triggered ACM metadata exists or the ACM metadata is invalid the method may determine that the auto commit buffer is not armed. If valid triggered ACM metadata does exist for a particular auto commit buffer the method identifies the auto commit buffer as an armed buffer and continues to step .

At step the triggered commit action for the armed auto commit buffers is performed. Performing the triggered commit action may comprise persisting the contents of the auto commit buffer to a sequential and or log based storage media such as a solid state or other non volatile storage media. Accordingly the triggered commit action may comprise accessing a logical identifier of the auto commit buffer labeling the data with the logical identifier and injecting the labeled data into a write data pipeline. Alternatively the triggered commit action may comprise storing the data on a persistent storage having a one to one mapping between logical identifier and physical storage address e.g. a hard disk . The triggered commit action may comprise storing the contents of the armed auto commit buffer to the specified physical address.

Performing the triggered commit action at step may comprise using a secondary power supply to power the ACM solid state storage medium and or other persistent non volatile storage medium until the triggered commit actions are completed.

In certain embodiments instead of or in addition to using a volatile memory namespace such as a physical memory namespace a virtual memory namespace or the like and or instead of or in addition to using a storage namespace such as a file system namespace a logical unit number LUN namespace or the like one or more commit agents as described above may implement an independent persistent memory namespace for the ACM . For example a volatile memory namespace which is typically accessed using an offset in physical and or virtual memory is not persistent or available after a restart event such as a reboot failure event or the like and a process that owned the data in physical and or virtual memory prior to the restart event typically no longer exists after the restart event. Alternatively a storage namespace is typically accessed using a file name and an offset a LUN ID and an offset or the like. While a storage namespace may be available after a restart event a storage namespace may have too much overhead for use with the ACM . For example saving a state for each executing process using a file system storage namespace may result in a separate file for each executing process which may not be an efficient use of the ACM .

The one or more commit agents and or the controller in certain embodiments provide ACM users with a new type of persistent memory namespace for the ACM that is persistent through restart events without the overhead of a storage namespace. One or more processes such as the ACM user in one embodiment may access the persistent memory namespace using a unique identifier such as a globally unique identifier GUID universal unique identifier UUID or the like so that data stored by a first process for an ACM user prior to a restart event is accessible to a second process for the ACM user after the restart event using a unique identifier without the overhead of a storage namespace a file system or the like.

The unique identifier in one embodiment may be assigned to an ACM user by a commit agent the controller or the like. In another embodiment an ACM user may determine its own unique identifier. In certain embodiments the persistent memory namespace is sufficiently large and or ACM users determine a unique identifier in a predefined known manner e.g. based on a sufficiently unique seed value nonce or the like to reduce limit and or eliminate collisions between unique identifiers. In one embodiment the ACM metadata includes a persistent memory namespace unique identifier associated with an owner of an ACM buffer an owner of one or more pages of an ACM buffer or the like.

In one embodiment the one or more commit agents and or the controller provide a persistent memory namespace API to ACM users over which the ACM users may access the ACM using the persistent memory namespace. In various embodiments the one or more commit agents and or the controller may provide a persistent memory namespace API function to transition convert map and or copy data from an existing namespace such as a volatile memory namespace or a storage namespace to a persistent memory namespace a persistent memory namespace API function to transition convert map and or copy data from a persistent memory namespace to an existing namespace such as a volatile memory namespace or a storage namespace a persistent memory namespace API function to assign a unique identifier such as a GUID a UUID or the like a persistent memory namespace API function to list or enumerate ACM buffers associated with a unique identifier a persistent memory namespace API function to export or migrate data associated with a unique identifier so that an ACM user such as an application and or process may take its ACM data to a different host to a different ACM or the like and or other persistent memory namespace API functions for the ACM .

For example an ACM user in one embodiment may use a persistent memory namespace API function to map one or more ACM buffers of a persistent memory namespace into virtual memory of an operating system of the host or the like and the mapping into the virtual memory may end in response to a restart event while the ACM user may continue to access the one or more ACM buffers after the restart event using the persistent memory namespace. In certain embodiments the SML may provide the persistent memory namespace API in cooperation with the one or more commit agents and or the controller .

The persistent memory namespace in certain embodiments is a flat non hierarchical namespace of ACM buffers and or associated ACM pages indexed by the ACM metadata . The one or more commit agents and or the controller in one embodiment allow the ACM buffers to be queried by ACM metadata . In embodiments where the ACM metadata includes a unique identifier in certain embodiments an ACM user may query or search the ACM buffers by unique identifier to locate ACM buffers and or stored data associated with a unique identifier. In a further embodiment the one or more commit agents and or the controller may provide one or more generic metadata fields in the ACM metadata such that an ACM user may define its own ACM metadata in the generic metadata field or the like. The one or more commit agents and or the controller in one embodiment may provide access control for the ACM based on unique identifier or the like.

In one embodiment an ACM buffer may be a member of a persistent memory namespace and one or more additional namespaces such as a volatile namespace a storage namespace or the like. In a further embodiment the one or more commit agents and or the controller may provide multiple ACM users with simultaneous access to the same ACM buffers . For example multiple ACM users of the same type and or with the same unique identifier multiple instances of a single type of ACM user multiple processes of a single ACM user or the like may share one or more ACM buffers . Multiple ACM users accessing the same ACM buffers in one embodiment may provide their own access control for the shared ACM buffers such as a locking control turn based control moderator based control or the like. In a further embodiment using a unique identifier a new ACM user an updated ACM user or the like on the host may access

In certain embodiments the ACM may comprise a plurality of independent access channels buses and or ports and may be at least dual ported e.g. dual ported triple ported quadruple ported . In embodiments where the ACM is at least dual ported the ACM is accessible over a plurality of independent buses . For example the ACM may be accessible over redundant bus connections with a single host may be accessible to a plurality of hosts over separate buses with the different hosts or the like. In embodiments where the ACM is at least dual ported if one node and or access channel fails e.g. a host a bus one or more additional nodes and or access channels to the ACM remain functional obviating the need for redundancy replication or the like between multiple hosts .

In one embodiment the ACM comprises a PCI e attached dual port device and the ACM may be connected to and in communication with two hosts over independent PCI e buses . For example the ACM may comprise a plurality of PCI e edge connectors for connecting to a plurality of PCI e slot connectors or the like. In a further embodiment the power connection may also be redundant with one power connection per bus or the like. At least one of the plurality of connections in certain embodiments may comprise a data network connection such as a NIC or the like. For example the ACM may comprise one or more PCI e connections and one or more data network connections.

In one embodiment the controller may arbitrate between a plurality of hosts to which the ACM is coupled such that one host may access the ACM buffers at a time. The controller in another embodiment may accept a reservation request from a host and may provide the requesting host with access to the ACM buffers in response to receiving the reservation request. The ACM may natively support a reservation request as an atomic operation of the ACM . In other embodiments the ACM may divide ACM buffers between hosts may divide ACM buffers between hosts but share backing non volatile memory between hosts or may otherwise divide the ACM buffers the non volatile memory and or associated address spaces between hosts .

In one embodiment the controller the one or more commit agents and or other elements of the ACM may be dual headed split brained or the like each head or brain being configured to communicate with a host and with each other to provide redundant functions for the ACM . By being at least dual ported in certain embodiments the ACM may be redundantly accessible without the overhead of replication duplication or the like which would otherwise reduce I O speeds of the ACM especially if such replication duplication were performed over a data network or the like.

In general the ACM module services auto commit requests from an ACM user or other client for the ACM . As described above with regard to the ACM users as used herein a client may comprise one or more of an operating system OS virtual operating platform e.g. an OS with a hypervisor guest OS application process thread entity utility user or the like that is configured to access or use the ACM . In the depicted embodiment the ACM module includes a request module a mapping module and a bypass module . The ACM module in certain embodiments provides an interface whereby an ACM user or other client may access data stored in the byte addressable ACM buffers whether the ACM buffers are natively volatile or non volatile regardless of the type of media used for the ACM buffers .

Instead of or in addition to the above methods of accessing the ACM such as using a memory map e.g. mmap interface in certain embodiments the ACM module may expose the auto commit buffers directly to ACM users or other clients bypassing one or more operating system and or kernel layers which may otherwise reduce performance of the ACM increasing access times introducing delays or the like. The ACM module may provide access to the ACM using an existing I O interface such as a standard read write API or the like so that ACM users or other clients may access the ACM and receive its benefits with little or no modification or customization. In another embodiment the ACM module may provide a custom or modified ACM interface which may provide ACM users and other clients more control over operation of the ACM than may be provided by existing interfaces.

As described above in certain embodiments the ACM module and or the ACM enable clients such as the ACM users to access fast byte addressable persistent memory combining benefits of volatile memory and non volatile storage. Auto commit logic inside the hardware of the storage device such as the auto commit memory described above with regard to in certain embodiments provides power cut protection for data written to the auto commit buffers of the ACM . The ACM module and or its sub modules in various embodiments may at least partially be integrated with a device driver executing on the processor of the host computing device such as the SML may at least partially be integrated with a hardware controller of the ACM and or non volatile storage device as microcode firmware logic circuits or the like or may be divided between a device driver and a hardware controller or the like.

In one embodiment the request module is configured to monitor detect intercept or otherwise receive requests for data of the non volatile memory device from clients such as the ACM users described above another module a host computing device or the like. The request module may receive data requests over an API a shared library a communications bus or another interface. As used herein a data request may comprise a storage request a memory request an auto commit request or the like to access data such as the open read write trim load and or store requests described above.

The request module may receive data requests using an existing or standard I O interface such as read and write requests over the block device interface load and store commands over the memory semantic interface or the like. By using the auto commit buffers to support standard requests or commands in certain embodiments the request module may allow the ACM users or other clients to access the ACM transparently with little or no modification or customization using the standard requests or commands. For example an ACM user may send data requests to the request module over the block device interface the memory semantic interface or the like using standard requests or commands with no knowledge of whether the ACM module services or satisfies the request using the auto commit buffers or the non volatile memory media allowing the mapping module described below to dynamically determine how to allocate data between the non volatile memory media and the auto commit buffers . The request module may intercept data requests using an existing or standard interface using a filter driver overloading an interface using LD PRELOAD intercepting or trapping a segmentation fault or the like.

In certain embodiments the request module may receive data requests using a custom or modified ACM interface such as an ACM API the SML API or the like. Data requests received over a custom or modified interface in certain embodiments may indicate whether a requesting ACM user or other client intends the data request to be serviced using the auto commit buffers or the non volatile memory medium e.g. whether data of the request is to be associated with the auto commit buffers or the non volatile memory medium . For example the request module may receive data requests including an auto commit flag indicating whether data of the request is associated with or is to be associated with an auto commit buffer of the ACM . An auto commit flag may comprise a bit a field a variable a parameter a namespace identifier or other logical identifier or another indicator.

In certain embodiments instead of a separate auto commit flag a data request may indicate whether the data is associated with an auto commit buffer or with the non volatile memory media based on a namespace identifier or other logical indicator of the data request. As used herein a namespace comprises a container or range of logical or physical identifiers that index or identify data data locations or the like. As described above examples of namespaces may include a file system namespace a LUN namespace a logical address space a storage namespace a virtual memory namespace a persistent ACM namespace a volatile memory namespace an object namespace a network namespace a global or universal namespace a BAR namespace or the like.

A namespace identifier as used herein comprises an indication of a namespace to which data belongs. In one embodiment a namespace identifier may comprise a logical identifier as described above. For example a namespace identifier may include a file identifier and or an offset from a file system namespace a LUN ID and an offset from a LUN namespace an LBA or LBA range from a storage namespace one or more virtual memory addresses from a virtual memory namespace an ACM address from a persistent ACM namespace a volatile memory address from a volatile memory namespace of the host device an object identifier a network address a GUID UUID or the like a BAR address or address range from a BAR namespace or another logical identifier. In a further embodiment a namespace identifier may comprise a label or a name for a namespace such as a directory a file path a device identifier or the like. In another embodiment a namespace identifier may comprise a physical address or location for data. As described above certain namespaces and therefore namespace identifiers may be temporary or volatile and may not be available to an ACM user after a restart event. Other namespaces and therefore namespace identifiers may be persistent such as a file system namespace a LUN namespace a persistent ACM namespace or the like and data associated with the persistent namespace may be accessible to an ACM user or other client after a restart event using the persistent namespace identifier.

An address or range of addresses may be associated with a namespace if the address or range of addresses comprises an identifier from the namespace if the address or range of addresses is mapped into the namespace or the like. Data or a range of data may be associated with a namespace if the data is stored in a storage medium of the namespace such as the auto commit buffers or the non volatile memory media if the data is mapped to the namespace in a logical to physical mapping structure if the data is associated with a namespace identifier for the namespace or the like.

A logical namespace may be associated with both the auto commit buffers and the non volatile memory media with different logical identifiers from the logical namespace mapped to different physical identifiers or locations for the auto commit buffers and or the non volatile memory media . For example certain data associated with file identifiers of a file system may be stored in the auto commit buffers while other data associated with file identifiers of the file system may be stored in the non volatile memory media even data at different offsets within the same file.

The request module may receive an open request to initialize a namespace identifier or other logical identifier such as opening a file or the like. The request module may receive a write request a store request or the like to store data in the auto commit buffers and or the non volatile memory medium of the non volatile memory device . The request module may receive a read request a load request or the like to read data from the auto commit buffers and or the non volatile memory medium of the non volatile memory device . In one embodiment a namespace identifier of a data request identifies both a namespace for and data of the data request such as the logical identifiers described above. In another embodiment a data request may comprise both a namespace identifier and a separate logical identifier for the data.

The request module in certain embodiments may receive data requests in user space. As used herein kernel space may comprise an area of memory e.g. volatile memory virtual memory main memory of the host computing device a set of privileges libraries or functions a level of execution or the like reserved for a kernel operating system or other privileged or trusted processes or applications. User space as used herein may comprise an area of memory e.g. volatile memory virtual memory main memory of the host computing device a set of privileges libraries or functions a level of execution or the like available to untrusted unprivileged processes or applications.

Due to access control restrictions privilege requirements or the like for kernel space providing a device driver library API or the like for the ACM in kernel space may have greater delays than in user space. Further use of a storage stack of a kernel or operating system in certain embodiments may introduce additional delays. An operating system or kernel storage stack as used herein may comprise one or more layers of device drivers translation layers file systems caches and or interfaces provided in kernel space for accessing a data storage device. As described in greater detail below with regard to the bypass module the ACM module may provide direct access to the ACM by bypassing and or replacing one or more layers of an operating system or kernel storage stack reading and writing data directly between the ACM buffers and user space or the like.

In one embodiment the mapping module is configured to map or associate namespace identifiers logical identifiers or the like to the ACM buffers and or the non volatile memory media . In certain embodiments the mapping module may maintain a logical to physical mapping structure as described below with regard to mapping logical identifiers or other namespace identifiers to physical locations in the non volatile memory media and or the ACM buffers . In one embodiment the mapping module may access and or maintain separate logical to physical mapping structures one for the non volatile memory media and one for the ACM buffers . As described above in certain embodiments the ACM buffers and the non volatile memory media may be accessible and or addressable at different granularities. For example the ACM buffers may be byte addressable while the non volatile memory media may be block addressable e.g. 512 byte blocks 4 KiB blocks or the like .

In response to the request module receiving a data request for a range of data for a logical identifier or other namespace identifier or the like such as an open request a write request a read request a load request a store request or the like the mapping module may determine whether there is a relationship between the data and or namespace identifier and one or more auto commit buffers . Data and or a logical identifier or other namespace identifier for the data may have a relationship with an auto commit buffer if the data is stored in the auto commit buffer if the data is targeted for or intended to be stored in the auto commit buffer if the data is identified in a data request for an auto commit buffer or the like. The mapping module in one embodiment may determine whether an existing association or mapping exists between requested data and or a namespace identifier and the auto commit buffers . In a further embodiment the mapping module may determine whether or not to map or create an association between requested data and an auto commit buffer .

In one embodiment the mapping module maps or associates data with an auto commit buffer in response to an auto commit flag of a data request for the data as described above. For example as described above in embodiments where the request module receives data requests over a custom or extended interface an ACM user or other client may indicate which data is to be stored in and associated with the auto commit buffers using auto commit flags or other indicators.

In a further embodiment where the request module receives data requests transparently using an existing standard interface or the like the mapping module may dynamically determine which data is stored in and associated with the auto commit buffers and which data is stored in the non volatile memory media . The mapping module may be configured to optimally distribute data between the auto commit buffers and the non volatile memory media based on one or more efficiency factors for namespace identifiers for data or the like. An efficiency factor as used herein may comprise an indicator or representation of an effect or impact of storing or associating data within the auto commit buffers .

The mapping module may monitor or track efficiency factors for different data different ACM users different namespace identifiers or the like. In one embodiment an efficiency factor may include an access frequency for data. For example the mapping module may be more likely to store data in the auto commit buffers that is more frequently accessed. In various embodiments efficiency factors may include a size of data a type of data a quality of service QoS for data or for an ACM user a service level agreement with an ACM user an age of data an amount of available storage capacity in the auto commit buffers and or in the non volatile memory medium or the like. The mapping module may balance or weigh multiple efficiency factors to determine whether to associate or store data of a certain namespace identifier or range of namespace identifiers with the auto commit buffers .

In one embodiment the mapping module cooperates with the SML to determine mappings for data in a logical address space or other namespace of the non volatile memory media and to preserve the mappings as metadata or a forward index such as the logical to physical mapping structure described below with regard to . In other embodiments the mapping module may cooperate with an operating system a file manager a storage stack a memory system or the like to create mappings to assign namespace identifiers or the like.

In certain embodiments mapping a namespace identifier such as a filename and an offset to an ACM buffer or otherwise initializing or creating a mapping may be a privileged operation performed in kernel space or the like. The mapping module may use an IOCTL call a shared memory queue between user space and kernel space or the like so that data requests for the auto commit buffers can be serviced or satisfied from user space while mappings may be performed at least partially in kernel space. In one embodiment the mapping module as part of or in addition to mapping namespace identifiers such as filenames and offsets to the auto commit buffers maps the associated page of an ACM buffer into a virtual address space of the requesting ACM user as described above so that the data is accessible to the ACM user as virtual memory of the host computing device .

The mapping module may map and or store an entire data object such as a file or the like to an ACM buffer . In certain embodiments the mapping module may map and or store a portion of a data object such as a particular offset or range of data within a file to an ACM buffer . The mapping module may map and or store the remainder of a file mapped partially to an ACM buffer to the non volatile memory media .

The mapping module in certain embodiments cooperates with the ACM module and or a commit agent to arm ACM buffers with ACM metadata including mappings of namespace identifiers or the like so that the ACM buffers are configured to perform appropriate commit actions for the data in the ACM buffers to remain persistently associated with the namespace identifiers even after a restart event. In this manner the ACM users may continue to access the data using the same namespace identifiers even after the restart event. As described above the ACM metadata may include multiple sections or parts. In one embodiment the ACM metadata includes a logical identifier to which the ACM buffer is to commit the data in the non volatile memory media e.g. an LBA or the like and a namespace identifier e.g. a filename a filename and an offset an inode number a LUN address or the like for the data which the commit agent may use to recover the data after a restart event allowing the ACM users to continue to access the data using the namespace identifier.

In one embodiment the bypass module is configured to service and or satisfy requests that the request module receives using the ACM buffers and or the non volatile memory media . In response to the mapping module determining that a namespace identifier of a data request is associated with the ACM buffers the bypass module may service or satisfy the data request using the ACM buffers e.g. storing the data in the ACM buffers in response to a write or store request reading the data from the ACM buffers in response to a read or load request or the like .

In certain embodiments the bypass module services or satisfies data requests directly from the ACM buffers accessing hardware of the ACM buffers directly from user space without using an operating system or kernel storage stack writing data directly to the ACM buffers reading data directly from the ACM buffers or the like. The bypass module in embodiments where one or more pages of the ACM buffers are mapped into virtual memory of an ACM user on the host device may access the hardware of the ACM buffers directly and copy data from the ACM buffers directly into or from the virtual memory at an offset indicated by a namespace identifier of the data request from user space without any kernel space libraries calls memory accesses or the like.

For example the bypass module may be integrated with and or cooperate with a user space device driver for the non volatile memory device executing on the processor of the host device and may service or satisfy data requests by mapping or copying data to and from hardware of the auto commit buffers and a virtual memory of a requesting client such as a shared virtual memory for a plurality of ACM users separate virtual memory spaces of different ACM users or the like all from user space. By servicing data requests in user space directly from an auto commit buffer without passing through an operating system or kernel storage stack in certain embodiments the bypass module may reduce operating system or kernel overhead associated with accessing the non volatile memory device decrease access times or the like.

For data requests that the mapping module determines are not associated with an auto commit buffer the bypass module may service or satisfy the requests using the non volatile memory medium e.g. storing the data in the non volatile memory medium in response to a write request reading the data from the non volatile memory medium in response to a read request or the like . For certain data requests the mapping module may determine that a range of data and or range of namespace identifiers is partially associated with the auto commit buffers and partially associated with the non volatile memory medium and the bypass module may split the data request satisfying it partially from the auto commit buffers and partially from the non volatile memory medium may consolidate the data in either the auto commit buffers or the non volatile memory medium or the like.

In one embodiment the bypass module uses the read module to service or satisfy read requests for data. The read module in response to the mapping module determining that the namespace identifier of a read request is mapped to the auto commit buffers reads the data specified in the read request e.g. data at a specified offset within a file or the like directly from the mapped location in the auto commit buffers from user space bypassing or skipping an operating system or kernel storage stack. If the mapping module determines that the namespace identifier of the read request is not mapped to or associated with the auto commit buffers the read module may read the data from the non volatile memory media . The bypass module may use the read module to return the read data to a requesting client such as an ACM user mapping or copying the read data into virtual memory for the requesting client sending the data to the requesting client or the like.

In one embodiment the bypass module uses the write module to service or satisfy write requests for data. In response to the mapping module determining that the namespace identifier of a write request is mapped to the auto commit buffers the write module may write the data specified in the write request directly to the mapped location in the auto commit buffers from user space bypassing or skipping an operating system or kernel storage stack. If the mapping module determines that the namespace identifier of the write request is not mapped to or associated with the auto commit buffers the write module may write the data to the non volatile memory media . The write module may read or copy the write data from virtual memory for the requesting client sending the data to the auto commit buffers and or the non volatile memory media or the like.

In one embodiment the has been written module may track which portions of data of the auto commit buffers have been updated are not yet stored in the non volatile memory media or the like. In certain embodiments portions of the data of the auto commit buffers may already be stored in and or committed to the non volatile memory media . In response to a restart event or another commit trigger it may be more efficient for the auto commit buffers to commit flush or destage just data that is not already stored in the non volatile memory media instead of committing all of the data. Further the commit agent may need to know which portions of a page or other storage region have been updated in order to recover the page or other storage region after a restart event.

Similarly reading an entire page s contents back into the auto commit buffers from the backing non volatile memory media may also be an expensive or time consuming operation. For example if the cost of reading the page contents in from the non volatile memory media is 50 us and each write to the auto commit buffers takes 500 ns or less even if the page is written 100 times after the initial read the cost of the initial read will still represent 50 of the latency associated with accessing the page.

The has been written module may track which data in the auto commit buffers has been updated and is not stored by the non volatile memory media which data is already stored in the non volatile memory media or the like. For example the has been written module may maintain a bitmap or other data structure such as a bitmap bitmask bit field table vector or the like populated with indicators of which data has been updated since the data was loaded since a previous commit operation or the like. The has been written module periodically or in response to a restart event may persist a has been written bitmap or other data structure to the non volatile memory media and the has been written module may cooperate with the commit agent to merge updates to data and or different versions of data. In one embodiment the has been written module allows the auto commit buffers to commit or copy just data that has been updated in response to a commit trigger or restart event and the commit agent may merge the updates with a previous version of the data preserved in a sequential log of the non volatile memory media after recovery from the restart event or the like.

In one embodiment the has been written module associates a has been written bitmap or other has been written metadata with each ACM page of the auto commit buffers . The has been written module may track updates or changes to data in the auto commit buffers at a byte level with a bit in a has been written bitmap for each byte or the like indicating whether or not the corresponding byte has been written or updated. Upon destaging instead of using a read modify write the controller may cooperate with the has been written module to identify updated regions of the page allowing sub block writes or the like.

In one embodiment the has been written module may provide ACM users with access to has been written bitmaps. For example an ACM page of the ACM buffers may store a last page block of a log file. Each update to the ACM page may increase the size of the file. Instead of noting and storing each change to the file length to reduce the overhead of system calls a has been written bitmap from the has been written module may be used to derive a new file length while maintaining the ACM efficiency.

In a further embodiment the has been written module may maintain one or more has been written data structures at a sub page granularity such as a byte granularity an error correcting code ECC chunk or block granularity or the like. A has been written data structure in certain embodiments may allow the commit agent or the like to determine what data within a page is dirty and not stored by the non volatile memory media if there are holes in a range of data due to out of order delivery or the like.

The has been written module in certain embodiments provides access to a has been written data structure using memory access e.g. load store semantics provides a clear all byte to clear a set of has been written bits at once or the like. The has been written module may clear or reset has been written metadata from a has been written data structure in response to the auto commit buffers committing destaging flushing or otherwise copying the data to the non volatile memory media . The has been written module in one embodiment may use a has been written data structure stored in volatile memory to locate data to commit destage or flush to the non volatile memory media without accessing or reading the non volatile memory media preventing an extra read modify write operation or the like.

The has been written module in one embodiment maintains the has been written data structure such that it parallels every byte of virtual memory with a corresponding bit that automatically indicates which bytes have indeed had data stored to them been written been modified been updated or the like.

In certain embodiments the has been written module and or the SML may provide one or more has been written data structures as part of a persistent storage namespace itself such as a filesystem namespace a logical unit number LUN namespace or the like. For example the has been written module and or the SML may provide a has been written data structure as a shadow file or the like that is designated to contain the bitmask of another file. ACM users may perform MMIO writes or other operations for both of these files or pages. In another embodiment a has been written data structure may be interleaved within the data it represents such as a 512 byte bitmask interleaved after each 4 kibibyte block within the same file or the like.

In one embodiment the security module is configured to provide access controls enforce permissions protect against attacks or the like for data stored in the auto commit buffers and or the non volatile memory media . Because the ACM module may provide access to the ACM buffers in user space the ACM buffers may be susceptible to denial of service DoS or other attacks. For example an ACM user may maliciously monopolize bandwidth of the communications bus such as a PCIe bus or the like. The security module in one embodiment monitors or tracks traffic on the communications bus access to each page of the auto commit buffers or the like. The security module in a further embodiment may disable access to an ACM user by unmapping an ACM page of data from the ACM user s virtual memory in response to the monitored access to the ACM page in virtual memory exceeding a traffic threshold or the like.

As described above a user space library process or application may be an untrusted entity. In certain embodiments file system access permissions that are normally enforced by the operating system or kernel in kernel space may be bypassed by the bypass module which operates in user space as described above. To present this from happening in one embodiment the security module is configured to use virtual memory access controls to enforce file system access permissions associated with data files of the auto commit buffers mapped or copied into virtual memory. For example if the file access permission for a file stored in an ACM page is read only the security module may cooperate with the mapping module to map the ACM page into virtual memory as read only. As described above in certain embodiments the mapping module performs mappings in kernel space which may allow the security module to maintain access controls even if the bypass module provides access in user space.

As described above once data has been stored in the auto commit buffers the ACM preserves or persists the data in non volatile memory media and provides the data from the non volatile memory media to clients such as ACM users after recovery from the restart event.

The ACM module and its various sub modules as described above may be disposed in a device driver for the ACM executing on a processor of the host device such as the SML may be disposed in a storage controller for the ACM and or may comprise portions in each of a device driver and a storage controller or the like

The address mapping structure in the depicted embodiment includes a plurality of nodes. Each node in the depicted embodiment is capable of storing two entries. In other embodiments each node may be capable of storing a greater number of entries the number of entries at each level may change as the address mapping structure grows or shrinks through use or the like.

Each entry in the depicted embodiment maps a variable length range of LBAs of the non volatile storage device to a physical location in the storage media for the non volatile storage device . Further while variable length ranges of LBAs in the depicted embodiment are represented by a starting address and an ending address in other embodiments a variable length range of LBAs may be represented by a starting address and a length or the like. In one embodiment the capital letters A through M represent a logical or physical erase block in the physical storage media of the non volatile storage device that stores the data of the corresponding range of LBAs. In other embodiments the capital letters may represent other physical addresses or locations of the non volatile storage device . In the depicted embodiment the capital letters A through M are also depicted in the log based writing structure which represents the physical storage media of the non volatile storage device .

In the depicted embodiment membership in the address mapping structure denotes membership or storage in the non volatile storage device . In another embodiment an entry may further include an indicator of whether the non volatile storage device stores data corresponding to a logical block within the range of LBAs data of a reverse map and or other data.

In the depicted embodiment the root node includes entries with noncontiguous ranges of LBAs. A hole exists at LBA 208 between the two entries of the root node. In one embodiment a hole indicates that the non volatile storage device does not store data corresponding to one or more LBAs corresponding to the hole. In one embodiment the non volatile storage device supports block I O requests read write trim etc. with multiple contiguous and or noncontiguous ranges of LBAs e.g. ranges that include one or more holes in them . A hole in one embodiment may be the result of a single block I O request with two or more noncontiguous ranges of LBAs. In a further embodiment a hole may be the result of several different block I O requests with LBA ranges bordering the hole. 

In the depicted embodiment similar holes or noncontiguous ranges of LBAs exist between the entries of the node between the entries of the left child node of the node between entries of the node and between entries of the node . In one embodiment similar holes may also exist between entries in parent nodes and child nodes. For example in the depicted embodiment a hole of LBAs 060 071 exists between the left entry of the node and the right entry of the left child node of the node .

The hole at LBA 003 in the depicted embodiment can also be seen in the logical address space of the non volatile storage device at logical address 003 . The hash marks at LBA 003 represent an empty location or a location for which the non volatile storage device does not store data. The hole at LBA in the logical address space is due to one or more block I O requests with noncontiguous ranges a trim or other deallocation command to the non volatile storage device or the like. The address mapping structure supports holes noncontiguous ranges of LBAs and the like due to the sparse and or thinly provisioned nature of the logical address space .

The logical address space of the non volatile storage device in the depicted embodiment is sparse and or thinly provisioned and is larger than the physical storage capacity and corresponding storage device address space of the non volatile storage device . In the depicted embodiment the non volatile storage device has a 64 bit logical address space beginning at logical address 0 and extending to logical address 264 1 . Because the storage device address space corresponds to only a subset of the logical address space of the non volatile storage device the rest of the logical address space may be allocated mapped and used for other functions of the non volatile storage device .

The sequential log based append only writing structure in the depicted embodiment is a logical representation of the physical storage media of the non volatile storage device . In certain embodiments the non volatile storage device stores data sequentially appending data to the log based writing structure at an append point . The non volatile storage device in a further embodiment uses a storage space recovery process such as a garbage collection module or other storage space recovery module that re uses non volatile storage media storing deallocated unused logical blocks. Non volatile storage media storing deallocated unused logical blocks in the depicted embodiment is added to an available storage pool for the non volatile storage device . By clearing invalid data from the non volatile storage device as described above and adding the physical storage capacity corresponding to the cleared data back to the available storage pool in one embodiment the log based writing structure is cyclic ring like and has a theoretically infinite capacity.

In the depicted embodiment the append point progresses around the log based append only writing structure in a circular pattern . In one embodiment the circular pattern wear balances the non volatile storage media increasing a usable life of the non volatile storage media . In the depicted embodiment a garbage collection module or other storage capacity recovery process has marked several blocks as invalid represented by an X marking on the blocks . The garbage collection module in one embodiment will recover the physical storage capacity of the invalid blocks and add the recovered capacity to the available storage pool . In the depicted embodiment modified versions of the blocks have been appended to the log based writing structure as new blocks in a read modify write operation or the like allowing the original blocks to be recovered.

If the mapping module determines that the data of the request is not associated with the auto commit memory the mapping module determines whether to associate the data with the auto commit memory . If the mapping module determines to associate the data with the auto commit memory the mapping module maps or associates the data of the request with the auto commit memory otherwise the storage controller satisfies or services the received request from the non volatile memory media . The mapping module may map the data or cause the data to be mapped to the auto commit memory from kernel space.

If the mapping module determines that the data of the received request is associated with the auto commit memory or if the mapping module determines to map the data to the auto commit memory the bypass module satisfies or services the received request directly from the auto commit memory bypassing an operations system or kernel storage stack or the like to satisfy the request from user space. The request module continues to monitor or otherwise receive or intercept requests for data of the non volatile memory device .

A means for associating a logical identifier or other namespace identifier with a page of auto commit memory in various embodiments may include a storage management layer a device driver a storage controller a mapping module other logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for associating a namespace identifier with a page of auto commit memory .

A means for bypassing an operating system storage stack to satisfy a storage request for data of a page of auto commit memory in various embodiments may include a storage management layer a device driver a storage controller a mapping module other logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for bypassing an operating system storage stack to satisfy a storage request for data of a page of auto commit memory .

A means for preserving data of a page of auto commit memory in response to a failure condition or restart event in various embodiments may include a secondary power supply an auto commit memory an auto commit buffer a commit agent a commit management module a commit module an ACM module other logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for preserving data of a page of auto commit memory in response to a failure condition.

A means for providing access to preserved data after a failure condition or restart event in various embodiments may include a non volatile storage device a non volatile memory media a storage management layer a commit agent an auto commit memory an auto commit buffer logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for providing access to preserved data after a failure condition or restart event.

The present disclosure may be embodied in other specific forms without departing from its spirit or essential characteristics. The described embodiments are to be considered in all respects only as illustrative and not restrictive. The scope of the disclosure is therefore indicated by the appended claims rather than by the foregoing description. All changes which come within the meaning and range of equivalency of the claims are to be embraced within their scope.

