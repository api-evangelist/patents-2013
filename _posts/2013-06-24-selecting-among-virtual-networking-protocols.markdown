---

title: Selecting among virtual networking protocols
abstract: Techniques are disclosed for determining a virtual networking framework for computing nodes to use where they are part of a plurality of computing nodes that have heterogeneous virtual networking framework capabilities. Each node may report its capabilities to a mapping server, which serves as a centrally-managed selector of policy capabilities for the two computing nodes to use in communications with each other. The mapping server selects virtual networking framework capabilities for the two computing nodes to use in communicating with each other, instructs the nodes of these selected capabilities, and the two nodes then communicate according to these selected capabilities.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09398121&OS=09398121&RS=09398121
owner: Amazon Technologies, Inc.
number: 09398121
owner_city: Reno
owner_country: US
publication_date: 20130624
---
Compute service providers sometimes referred to as cloud service providers provide services to various entities such as corporations universities government agencies and other types of customers to compute resources hosted in one or more datacenters. There are a number of reasons for entities to run their compute resources or at least some of their compute resources on a compute service. For example running their compute resources in datacenters operated by the compute service may provide the entities with flexibility in resource allocation as well as providing scalability improved resource allocation reduced operating costs and the like.

Often a cloud service provider virtualizes some or all of the necessary compute resources to generate virtual private clouds of topologies specific to its customers. This virtualization allows the cloud service provider to dynamically scale hardware and software of the compute resources to meet needs and requirements of its customers. The virtual private cloud of one customer is typically isolated from a virtual private cloud of another customer of the same cloud service provider even when the two virtual private clouds are hosted on compute resources operating in the same datacenter. The isolation protects each customer from security breaches among other things and renders each virtual private cloud a private network inaccessible by the other customers of the same cloud service provider.

These virtual private clouds may be built on a virtual networking framework sometimes referred to as a virtual networking protocol than enables customers to provision their own virtual private cloud or virtual datacenter. Different virtual networking frameworks may support different features for virtual networks built on the respective virtual networking frameworks.

Networks that support compute services may need to update the virtual networking framework to a new version or may want to have the ability to run different versions of the framework that have different capabilities. Because there may be so many devices associated with implementing a virtual networking framework and because a service provider that implements a virtual networking framework may attempt to have at least some of its devices online at all times so that it has constant uptime it may be infeasible to update and reboot all these devices with the new version virtual networking framework at once versions are sometimes referred to herein as capabilities . In order to update a virtual networking framework one technique may be to negotiate the version or capabilities of a virtual networking framework to be used between two computing nodes sometimes referred to as computers or physical hosts herein at the time that those two computing nodes are to communicate.

It may be that computing nodes are not configured to send information about their virtual networking framework capabilities sometimes referred to herein as communication capabilities to other computing nodes or are not configured to trust information about virtual networking framework capabilities that are received from other computing nodes. In such embodiments there may be a central control plane referred to herein as a mapping server that is configured to receive from computing nodes an indication of their virtual networking framework capabilities select among those virtual networking framework capabilities for a communication to occur between two particular computing nodes and direct those two computing nodes to use the selected virtual networking framework capabilities when communicating with each other.

Three different types of networks are primarily described herein to illustrate aspects of the disclosure. One type of network referenced herein is the physical network itself which may be referred to as the substrate network . It may be that a physical network is not visible to virtual networks built on top of the physical network or to virtual machine instances within a service provider that communicate via a virtual network.

A second of type of network referenced herein is an overlay network. An overlay network may be a virtual network that virtual machine instances and other virtual entities use and see. An overlay network generally may not resemble the physical network on which it resides. When a customer configures and deploys a virtual network on the overlay network the customer may be unaware of how the substrate network is intra connected or what the physical network is using for communications. This overlay network may function in part because the edge devices such as routers and hosts upon which virtual machine instances execute are aware of the mapping between a physical network and an overlay network. For example a host partition of a physical host may intercept or receive network packets destined for an instance and rewrite them from identifying a physical network to identifying an overlay network. Routers may perform similar rewriting functions for packets received at them.

A third type of network referenced herein is a virtual cloud sometimes referred to as a virtual private cloud . A virtual cloud allows a customer to create a network of virtual machine instances and other devices set up in a customer specified topology and that is not shared with any other customer. The customer may create a VPN virtual private network connection to their virtual cloud from their customer home network thus extending their home network to include the virtual cloud. In a virtual cloud in addition to creating the network topology of the customer s choice a customer may address a server as it chooses. The virtual cloud s topology may be entirely virtualized the routers firewalls and hosts may all be virtual. A virtual cloud may be built on top of an overlay network with the overlay network itself being built on top of the physical network .

A virtual networking framework is a framework that allows communications on a virtual cloud or overlay network to occur through a physical network. A virtual networking framework may include mappings between addresses in the virtual cloud or overlay network and addresses in the physical network as well as capabilities or versions of the virtual networking framework. In this manner using a virtual networking framework virtualized devices such as virtual machine instances may operate on the overlay network without having been modified in order to communicate on an overlay network. While embodiments described herein mainly deal with a virtual network built directly on top of a physical network it may be appreciated that these embodiments may be applied where one network is built on top of another network such as a first virtual network built on top of a second virtual network which itself is built on top of a physical network .

Regarding a virtual networking framework how a virtual networking framework may be implemented and how a virtual networking framework may be used to communicate between various computing nodes e.g. a computing node in a virtual cloud a computing node in a VPN network and a computing node on the general Internet will now be addressed. A virtual networking framework may be implemented at the networking layer of multiple devices to enable them to rewrite overlay network communications. For instance a virtual networking framework may be implemented in the host partition of a physical host that supports virtual machine instances. The virtual networking framework in the host partition may intercept packets from the virtual machine instances and ensure that the identification of overlay network addresses in the packets are rewritten into identification of physical addresses. This translation may be done so that the devices of the physical network are given addresses that they understand on the physical network and may therefore transmit the packets toward their destination. The virtual networking framework may also be implemented on other devices such as edge routers to ensure that e.g. NAT and PAT network address translation and port address translation work correctly for entities using an overlay network. A mapping service may exist at the physical layer that maps between overlay IP addresses and physical IP addresses.

In using an overlay network with a physical network an overlay IP address and guest network identifier GNI may be encoded into a physical network packet so that the information of the ultimate destination and source on the overlay network is preserved. One way to do this is to use IPv6 Internet Protocol version 6 . Another way to do this involves the virtual networking framework and protocols that it supports. An overlay network packet on the virtual networking framework may contain information relevant to the virtual networking framework in the data portion of the IP packet similar to how a UDP User Datagram Protocol packet may be an IP packet with UDP information in the data portion of the IP packet . The data portion in a packet that utilizes this virtual networking framework may contain the data portion of an IP packet generated by the overlay device this may be a TCP Transmission Control Protocol or UDP packet along with GNI information. The IP header for this original packet may then be reconstructed using information from the data portion e.g. the data portion may include an identifier of the destination address on the overlay network and when the packet is transmitted from a physical network device to the destination device on the overlay network such as from the host partition to the virtual machine instance the host partition may reconstruct the packet on the overlay network by using the destination address identified in the data portion as the destination address for the packet .

There are several categories of communication that may be implemented using a virtual networking framework such as 1 an instance on one virtual cloud may send a packet to an instance on the same virtual cloud in the same subnet or across subnets 2 an instance on a virtual cloud may send a packet to a VPN 3 an instance on a virtual cloud may send a packet to the Internet 4 an instance on a virtual cloud may receive a packet from the Internet which may be return traffic or unsolicited traffic and 5 an instance on a virtual cloud may receive a packet from a VPN which may be return traffic or unsolicited traffic . Some of these examples are described below to illustrate how a virtual networking framework may operate.

How these categories of communication may be implemented in a computing environment that supports a virtual networking framework will now be addressed with reference to . illustrates an example environment in which embodiments of the invention may be implemented. It may be that embodiments that implement using selected capabilities of virtual networking frameworks may deal with communications between two virtual machine instances or where communications are sent between an instance and the Internet or a VLAN communications between the instance and an edge device between the instance and the Internet or VLAN. With that in mind how some additional communications may be implemented is described herein to give a fuller picture of how a virtual networking framework may operate.

The following is an example of an instance on a virtual cloud communicating with another instance on the same virtual cloud and the same subnet.

The following is an example of an instance on a virtual cloud communicating with another instance on the same virtual cloud and a different subnet. Customers may configure multiple subnets within their virtual cloud. However since the overlay topology may differ from the physical topology there may be a phantom router implemented in the virtual cloud to route packets between instances in the virtual cloud. This phantom router may be implemented in software at the host partition X And mapping server levels.

Where a virtual cloud has multiple subnets mapping server may configure a phantom router that exists on that network s x.x.x.1 device e.g. using the 10.1.2.30 and 10.1.2.50 example above this may be a device with address 10.1.2.1 .

The following is an example of an instance on a virtual cloud communicating with a VPN. Traffic to and from a VPN may introduce operations performed by an edge router such as edge router A A that bridges the overlay and physical networks that is aware of the virtual networking framework and that can rewrite packets to and from a virtual cloud. The communication flow for a packet from an instance to a VPN may be similar to the communication flow for a packet between two instances on the same virtual cloud and different subnets. The VPN may have a virtual instantiation on the virtual cloud which may be on a different subnet. There are multiple approaches to processing this communication such as 

In either of these approaches the edge router that services the VPN for that virtual cloud receives the packet in the virtual networking framework which contains the VPN packet on its internal interface. The edge router may query the mapping server and cache the received mapping. The cached mapping may also contain GNI information so that the edge router knows how to correctly rewrite return packets from the VPN to the instance. The edge router may not be performing a NAT function where it is merely extracting the VPN packet from the packet in the virtual networking framework it received reconstructing IP headers so that the VPN packet appears to be from the instance and noting the GNI information to use later for return packets. When the packet is received on the customer home network via Internet the packet source address appears as the overlay network address of the instance there is no visibility into the physical network.

Communications from an instance on a virtual cloud to the Internet may be performed in a similar manner as the VPN example above. A difference in this scenario may be that the edge router may be rewriting packets in the virtual networking framework and performing either NAT or PAT.

For return traffic from the Internet to an instance on a virtual cloud the following may occur. If the edge router receives a packet from the Internet for which it has a state table entry then the packet may be un NAT ed or un Pat ed appropriately and encapsulated into a packet in the virtual networking framework. The edge router may then send the packet to the appropriate physical network entity with the appropriate GNI incorporated into the packet in the virtual networking framework.

For unsolicited as opposed to return traffic from the Internet to an instance on a virtual cloud the following may occur. If the edge router receives a packet from the Internet and there is no corresponding state table entry it may be assumed that the packet is invalid and the edge router may drop the packet. If there is a valid state table entry because the customer is using a static IP address that remaps to another instance in case of failure of a first instance then the packet may be treated the same as return traffic from the Internet to an instance on a virtual cloud as described above.

For return traffic from a VPN to an instance on a virtual cloud the following may occur. If the edge router servicing the VPN receives a packet from the customer home network for which there is a state table entry then the packet is appropriately encapsulated into a packet in the virtual networking framework including physical network addressing and GNI information and then forwarded to the appropriate device on the physical network. It may be that there is no NAT or PAT performed on the VPN connection.

For unsolicited traffic from a VPN to an instance on a virtual cloud the following may occur. If the edge router servicing the VPN receives a packet from the customer home network for which there is no state table entry then the edge router may query the mapping server for the virtual cloud to determine a corresponding physical network address for the overlay network address that is the address of the instance on the virtual cloud .

If the mapping server has an entry for this overlay address this information is returned to the edge router. The edge router may then encapsulate the VPN packet into a packet in the virtual networking framework including physical network addressing and GNI information and forwards the packet on.

If the mapping server does not have an entry for this overlay address the mapping server may return an indication of this to the edge router. The edge router may then drop the packet. This is because the instance identified by that overlay address may or may not have existed in the past but there currently is no instance with that overlay address.

The example packets of continue with the example addressing scheme of where customer instance A A has a network address of 10.1.2.30 on an overlay network and a network address of 7.8.9.10 on a corresponding physical network the physical network address of 7.8.9.10 being depicted as part of host partition X A because customer instance A A may be unaware of this physical network or physical network address . Similarly customer instance B B has a network address of 10.1.2.50 on the overlay network and a network address of 1.2.3.4 on the corresponding physical network the physical network address of 1.2.3.4 being depicted as part of host partition Y B because customer instance B B may be unaware of this physical network or physical network address .

Packet is an example packet as it is generated by customer instance A A. Packet identifies the source of the packet as 10.1.2.30 the destination of the packet as 10.1.2.50 and contains a data payload.

Packet is an example packet depicting how host partition X A may modify packet upon receiving it. Host partition X modifies packet to generate packet by encapsulating packet with a virtual networking framework header that identifies physical network address 7.8.9.10 as corresponding to overlay network address 10.1.2.30 and identifies physical network address 1.2.3.4 as corresponding to overlay network address 10.1.2.50. Since 10.1.2.30 is the source network address identified in packet corresponding physical network address 7.8.9.10 is the source network address identified in packet . Likewise since 10.1.2.50 is the source network address identified in packet corresponding physical network address 1.2.3.4 is the source network address identified in packet . Packet also differs from packet in that host partition A has added a source and destination global network identifier GNI to the packet. A GNI may be a customer identifier that uniquely identifies a customer among customers that host instances at a particular service provider. A GNI may be used where two different customers have virtual machine instances hosted on the same host and these two virtual machine instances may have the same IP address.

Packet depicts packet as it is received by host partition Y B from host partition X A. Here packet is depicted as being the same as packet because it has not been modified between host partition X A and host partition Y B. In embodiments the packet may be modified between host partition X A and host partition Y B such as by encapsulating it for transmission on another network but the packet is returned into the condition that it was in when it left host partition X A as it reaches host partition Y B.

Host partition Y B then modifies packet to produce packet . This may comprise removing encapsulation information of source 7.8.9.10 destination 1.2.3.4 and GNI information for the source and destination from packet so that packet is the same as packet .

It may also be appreciated that the embodiments described herein mainly deal with determining virtual networking framework capabilities for two physical hosts to use when their respective virtual machine instances are communicating these techniques are not limited to such embodiments. These techniques may be applied to other computing nodes that communicate via virtual networking frameworks such as switches and edge routers.

The operating procedures of begin with operation and move to operation . Operation depicts receiving an indication of the source host s virtual networking framework capabilities. This may occur for example in response to the mapping server querying the source host for its capabilities or the source host sending an indication of its capabilities to the mapping server at a predetermined periodic time or when the source host boots up. Examples of these capabilities may include a version of a protocol or a protocol to use. The capabilities may include a format to use when encoding the packet for transmission over a physical network a wire format a change in the size of a field in bits or an order in which fields appear in packets e.g. switching the order that the source and destination address fields appear in a packet whether encryption is required or disallowed integrity checks whether to use compression and an indication of a configuration for flow control. After operation the operating procedures of move to operation .

Operation depicts receiving an indication of the destination host s capabilities. Operation may be implemented in a manner similar to operation . After operation the operating procedures of move to operation .

Operation depicts determining whether there is a policy that deals with communications between the source and destination. A policy may affect whether certain capabilities that both the source and the destination have are used in their communications. For example all computing nodes on a network may be updated at the same time to use version 2.0 of a virtual networking capability in addition to version 1.0. Even though all nodes support version 2.0 a policy may indicate that two nodes are not to use this new version in a production environment but only in a testing environment. Thus when two particular nodes are to communicate with each other as part of a production environment the policy may indicate that they are to use version 1.0 even though they both support version 2.0. Another example of a policy may be that particular capabilities are to be used only for certain customers. In embodiments policies may be set by an administrator of a mapping server or by a customer. Where there is a policy that deals with communications between the source and destination the operating procedures of move to operation . Where there is not a policy that deals with communications between the source and destination the operating procedures of move to operation .

Operation depicts selecting from the common capabilities of the source and destination without regard to a policy . Where the capabilities are versions of a virtual networking framework selecting capabilities may comprise selecting the highest version that is common to both hosts. For example where one host implements versions 1.0 1.1 and 3.0 of a virtual networking framework and another host implements versions 1.0 1.1 and 2.0 of a virtual networking framework this may comprise selecting version 1.1 of the virtual networking framework for use by the two hosts in communicating with each other. Here neither version 2.0 nor 3.0 is selected because only one of the two hosts implements that version and version 1.0 is not selected even though both hosts implement it because it is not the highest common version since both hosts also implement version 1.1.

Where the capabilities are several features selecting the capabilities may comprise selecting the features common to both hosts. Where the capabilities are different virtual networking frameworks that do not have an inherent order in the manner that multiple versions of a single framework have selecting the capabilities may comprise the mapping server referring to a policy of which of these different virtual networking frameworks to select when they are present. In embodiments selecting the capabilities may comprise receiving user input that identifies the selected communication capability. After operation the operating procedures of move to operation .

In embodiments virtual networking framework capabilities may be selected to be used by two hosts for all of their communications with possibly differing virtual machine instances on those hosts. In other embodiments virtual networking framework capabilities may be selected on a per virtual machine instance basis that two virtual machine instances on two physical hosts may have one set of virtual networking framework capabilities used for their communications and two other virtual machine instances on those same two physical hosts may have a different set of virtual networking framework capabilities used for their communications.

Operation depicts selecting from the common capabilities of the source and destination with regard to a policy. This operation may be implemented in a similar manner as operation with the added requirement that the selected capabilities must adhere to a policy. For example where in operation version 3.0 of a protocol would be selected but the policy indicates that version 3.0 is not to be used operation may be implemented in a similar manner as operation as though version 3.0 was not a capability indicated by either the source or destination. After operation the operating procedures of move to operation .

Operation depicts indicating to the source and destination to use the selected capabilities for communications between the source and destination. For example where these selected capabilities are a version of a virtual networking framework and the latest version that each host supports is the same version this may comprise the mapping server indicating to each host to use the latest version of the virtual networking framework in communications with each other. Where these indicated capabilities are sets of features this operation may likewise comprise the mapping server indicating to each host to use those sets of features. In embodiments the mapping server may send the source and destination different selected capabilities e.g. the source is to encrypt when sending to the destination and the destination is to use plaintext when sending back to the source . In other embodiments the mapping server may indicate the capabilities to only the source and the destination may determine the selected capabilities based on packets received from the source. After operation the operating procedures of move to operation where they end.

Operation depicts attempting to determine the virtual networking framework capabilities of the source physical host. This may comprise the source physical host looking for a virtual networking framework process that executes on the source physical host and querying that process for its version or capabilities. Where these virtual networking framework capabilities are determined the operating procedures of move to operation . Where these virtual networking framework capabilities are not determined the operating procedures of move to operation where an error is raised.

Operation depicts sending the determined virtual networking framework capabilities to a mapping server. These determined virtual networking capabilities may be virtual networking capabilities that were successfully determined in operation . After operation the operating procedures of move to operation .

Operation depicts receiving an indication of virtual networking framework capabilities to use when communicating with the destination physical host. This indication may be received from a mapping server which makes the determination of which virtual networking framework capabilities to use in this communication in operation of . After operation the operating procedures of move to operation .

Operation depicts receiving a network packet from a virtual machine instance hosted on the source physical host that is destined for a destination virtual machine instance hosted on the destination physical host. This may be considered to be intercepting the network packet because the virtual machine instance is configured to put the network packet on a physical network interface card while the packet is actually placed on a virtual network interface card as part of the virtualization process. Where the host partition provides a hypervisor type functionality to the virtual machine instance hosted on the source physical host this host partition may provide a shared memory area to the virtual machine instance as part of virtualizing a network interface card NIC to the virtual machine instance. When the virtual machine instance attempts to send a network packet to a destination the virtual machine instance may write the network packet to this shared memory area where it may be received by the host partition since the shared memory area is shared between the virtual machine instance and the host partition. After operation the operating procedures of move to operation .

Operation depicts determining whether there is a mapping for the destination virtual machine instance. This may comprise determining the destination address in the overlay network that is indicated in the network packet and querying the mapping server for a corresponding network address in the physical network. Where it is determined that there is a mapping for the destination virtual machine instance the operating procedures of move to operation . Where it is determined that there is not a mapping for the destination virtual machine instance the operating procedures of move to operation where an error is raised.

Operation depicts modifying the network packet received from the virtual machine instance hosted on the source physical host that is destined for the destination virtual machine instance hosted on the destination physical host based on the capabilities indicated in operation . This may be similar to modifying packet to packet in . After operation the operating procedures of move to operation .

Operation depicts sending the modified network packet to the destination physical host. This may comprise host partition X A sending the modified packet to host partition Y B via internal networking infrastructure in . After operation the operating procedures of move to operation where the operating procedures of end.

Operation depicts raising an error. Operation may be reached from operation where the virtual networking framework capabilities of the source physical host cannot be determined or operation where there is not a valid mapping for the destination physical host . In response to the error being raised the host partition may for example drop the packet received from the virtual machine instance and perform no further actions on it. After operation the operating procedures of move to operation where the operating procedures of end.

Operation depicts attempting to determine the virtual networking framework capabilities of the destination physical host. This operation may be implemented in a similar manner as operation of . Where these virtual networking framework capabilities are determined the operating procedures of move to operation . Where these virtual networking framework capabilities are not determined the operating procedures of move to operation where an error is raised.

Operation depicts sending the determined virtual networking framework capabilities to a mapping server. This operation may be implemented in a similar manner as operation of . After operation the operating procedures of move to operation .

Operation depicts receiving an indication of virtual networking framework capabilities to expect when communicating with the source physical host. This operation may be implemented in a similar manner as operation of . Additionally these capabilities may be capabilities that the source host will use when modifying network packets to send to the destination host but not vice versa. In such cases the destination host may use the indication of these capabilities to know how to validate or decode received packets. In embodiments both the source and destination hosts use the same capabilities when communicating with each other. In other embodiments the source and destination hosts may use different capabilities when sending packets to each other. For example the source host may use encryption when sending packets to the destination host and the destination host may use plaintext when sending packets to the source host. After operation the operating procedures of move to operation .

Operation depicts receiving a network packet from the source physical host that was originated on a source virtual machine instance hosted on the source physical host and that is destined for a destination virtual machine instance hosted on the destination physical host. In the environment of this may be for instance host partition Y B receiving a network packet from host partition X A that was originated by instance A A via internal networking infrastructure . After operation the operating procedures of move to operation .

Operation depicts determining whether the mapping used is valid. This may comprise determining the indicated overlay network source and destination addresses and physical network source and destination addresses in the packet. Then the mapping server may be queried for the correct mappings such as by sending the mapping server the overlay addresses and comparing the received mapping against the physical addresses indicated in the packet. Where it is determined that the mapping used is valid the operating procedures of move to operation . Where it is determined that the mapping used is invalid the operating procedures of move to operation where an error is raised.

Operation depicts determining whether the proper virtual networking framework capabilities were used in the received network packet. The proper virtual networking framework capabilities to use in this communication may be those virtual networking framework capabilities indicated by the mapping server in operation . The destination host partition may analyze the received packet to determine whether these capabilities were in fact used. For example where the indicated capabilities include encryption the destination host partition may determine whether this packet is encrypted. Where the packet includes metadata that indicates the capabilities used in modifying this packet for the virtual networking framework by the source host partition this may include checking those capabilities indicated in the packet against the proper virtual networking framework capabilities indicated by the mapping server. Where it is determined that the proper virtual networking framework capabilities were used in the received network packet the operating procedures of move to operation . Where it is determined that the proper virtual networking framework capabilities were not used in the received network packet the operating procedures of move to operation where an error is raised.

Operation depicts modifying the network packet received from the source physical host that is destined for the destination virtual machine instance hosted on the destination physical host based on the capabilities indicated in operation . This operation may comprise for example modifying packet of to produce packet of . After operation the operating procedures of move to operation .

Operation depicts sending the modified network packet to the destination virtual machine instance. Using the example architecture described with respect to operation of operation may comprise writing the modified network packet to a shared memory area that is shared with the destination virtual machine instance and which is part of a virtual network interface card NIC for that destination virtual machine instance and indicating to the destination virtual machine instance that it has received a network packet. After operation the operating procedures of move to operation where the operating procedures of end.

Operation depicts raising an error. Operation may be reached from operation where the virtual networking framework capabilities of the destination physical host cannot be determined operation where there is not a valid mapping used in the received network packet or operation where the proper virtual networking framework capabilities were not used in the received network packet . Where an error is raised stemming from analyzing a received network packet such as in operations or the received network packet may be dropped and not sent to the destination virtual machine instance. After operation the operating procedures of move to operation where the operating procedures of end.

Operation depicts receiving an indication to instantiate a virtual machine on a physical host that has both a specified required virtual networking framework capability and a desired virtual networking framework capability. In embodiments only required or only desired virtual networking framework capabilities may be specified. This indication may be received from a customer that is instantiating the virtual machine instance. After operation the operating procedures of move to operation .

Operation depicts determining whether any hosts of a plurality of physical hosts support both the desired and required virtual networking capabilities. If there is at least one host that supports both the desired and required virtual networking capabilities then the operating procedures of move to operation . If there is not at least one host that supports both the desired and required virtual networking capabilities then the operating procedures of move to operation .

Operation depicts determining whether any hosts of the plurality of physical hosts supports the required virtual networking capabilities. If there is at least one host that supports the required virtual networking capabilities then the operating procedures of move to operation . If there is not at least one host that supports the required virtual networking capabilities then the operating procedures of move to operation .

Operation depicts selecting a host of the plurality of physical hosts that has the required capability. Where there is only one host that has the required capability this operation may comprise selecting this host if the host has available load to execute the virtual machine instance and returning an error otherwise. Where there are multiple hosts that have the required capability this operation may comprise selecting among the multiple hosts such as based on the host with the most available load a round robin selection policy or similar placement determinations. In embodiments additional considerations may be taken into account in determining a host on which to place the instance. For example selecting the host may involve determining that the host has sufficient capacity to handle the load of the new instance. Additionally a policy decision may be incorporated in selecting the host. For example it may be a policy to try to avoid placing one customer s instances on the same physical rack of hosts because the customer may be executing multiple instances to increase reliability and where these instances are placed on a single rack that single rack becomes a single point of failure such as due to a power failure. After operation the operating procedures of move to operation .

Operation depicts selecting a host of the plurality of physical hosts that has the required and desired capability. Operation may be implemented in a similar manner as operation is implemented. After operation the operating procedures of move to operation .

Operation depicts instantiating the virtual machine instance on the selected host. This may comprise copying a virtual machine image file to the selected host and instructing instance manager of to instantiate a virtual machine instance from the copied virtual machine image file. After operation the operating procedures of move to operation where the operating procedures of end.

Operation depicts raising an error. This may comprise sending the customer that requested instantiating the virtual machine instance in operation an indication that a virtual machine instance could not be placed on a physical host that meets the customer s required and desired virtual networking framework capabilities. After operation the operating procedures of move to operation where the operating procedures of end.

It may be appreciated that these operating environments of may be used to implement aspects of the operating environment of . For example physical host A A physical host B B mapping server and internal networking infrastructure may be implemented in a datacenter of or across multiple datacenters of . Likewise Internet of may be wide area network of and customer home network may be customer computing system of .

Within a datacenter of physical host A A and physical host B B may each be a server computer of which itself may be computer of host partition X A and host partition Y B may each be an instance of instance manager where a host partition serves a hypervisor type role and instance A and instance B may each be an instance of . Internal networking infrastructure of may be local area network of and mapping server of may be server computer of .

Turning now to details of depicts an example of a suitable computing environment in which embodiments described herein may be implemented. A cloud service provider such as compute service platform may configure the illustrated computing environment to host virtual clouds of entities and to enable communication paths between these virtual clouds that may otherwise be isolated. In particular is a system and network diagram that shows an illustrative operating environment that includes a compute service platform compute service for implementing virtual clouds and for providing on demand access to compute resources such as virtual machine instances. Compute service platform can provide compute resources for executing applications on a permanent or an as needed basis and may be configured as a private network. These compute resources may include various types of resources such as data processing resources data storage resources data communication resources and the like. Each type of compute resource may be general purpose or may be available in a number of specific configurations. For example data processing resources may be available as virtual machine instances. The instances may be configured to execute applications including Web servers application servers media servers database servers and the like. Data storage resources may include file storage devices block storage devices and the like.

Each type or configuration of compute resource may be available in different sizes such as large resources consisting of many processors large amounts of memory and or large storage capacity and small resources consisting of fewer processors smaller amounts of memory and or smaller storage capacity. Entities may choose to allocate a number of small processing resources as Web servers and or one large processing resource as a database server for example.

The compute resources provided by compute service platform may be enabled by one or more datacenters A N which may be referred herein singularly as datacenter or in the plural as datacenters . Datacenters may be facilities that house and operate computer systems and associated components and may include redundant and backup power communications cooling and security systems. Datacenters may be located in a same geographical area such as in a same facility and may be interconnected using private networks such as high speed fiber optic networks controlled and managed by a service provider of compute service platform . Datacenters may also be distributed across geographically disparate locations and may be interconnected in part using public networks such as the Internet. One illustrative configuration for datacenter that implements the concepts and technologies disclosed herein is described below with regard to .

Entities of compute service platform may access the compute resources provided by datacenters over a wide area network WAN . Although a WAN is illustrated in it should be appreciated that a local area network LAN the Internet or any other networking topology known in the art that connects datacenters to remote entities and other users may be utilized. It should also be appreciated that combinations of such networks may also be utilized.

An entity or other entities that are customers of compute service platform may utilize a computing system to access the compute resources provided by datacenters . Customer computing system comprise a computer capable of accessing compute service platform such as a server computer a desktop or laptop personal computer a tablet computer a wireless telephone a PDA an e reader a game console a set top box or any other computing device.

As is described in greater detail below customer computing system may be utilized to configure aspects of the compute resources provided by compute service platform . In this regard compute service platform may provide a Web interface through which aspects of its operation may be configured through the use of a Web browser application program executing on customer computing system . Alternatively a stand alone application program executing on customer computing system may access an application programming interface API exposed by compute service platform for performing the configuration operations. Other mechanisms for configuring the operation of compute service platform including launching new virtual machine instances on compute service platform may also be utilized.

According to embodiments disclosed herein capacities of purchased compute resources provided by compute service platform can be scaled in response to demand. In this regard scaling refers to the process of instantiating which may also be referred to herein as launching or creating or terminating which may also be referred to herein as de scaling instances of compute resources in response to demand.

Auto scaling may be one mechanism for scaling compute resources in response to increases or lulls in demand for the resources. Auto scaling may allow entities of compute service platform to scale their purchased compute resources according to conditions defined by the entity. For instance rules may be defined for scaling up capacity in a particular manner in response to the occurrence of specified conditions such as a spike in demand. Similarly rules may also be defined to scale down capacity in a particular manner in response to the occurrence of other conditions such as a lull in demand. The mechanisms disclosed herein for launching virtual machine instances may be utilized when instances are manually launched by an entity or when instances are launched by an auto scaling component in compute service platform .

compute service platform may also be configured with a deployment component to assist entities in the deployment of new instances of compute resources. The deployment component may receive a configuration from an entity that may include data describing how new instances should be configured. For example the configuration may specify one or more applications or software components that should be installed in new instances provide scripts and or other types of code to be executed in new instances provide cache warming logic specifying how an application cache should be prepared and other types of information. The deployment component utilizes the entity provided configuration and cache warming logic to launch configure and prime new instances of compute resources.

Instances A N which may be referred herein singularly as instance or in the plural as instances may be virtual machine instances. As known in the art a virtual machine instance is an instance of a software implementation of a machine i.e. a computer that executes programs like a physical machine. In the example of virtual machine instances each server may be configured to execute an instance manager capable of executing the instances. Instance manager may be a hypervisor or another type of program configured to enable the execution of multiple instances on a single server for example. As discussed above each of instances may be configured to execute all or a portion of an application.

It should be appreciated that although the embodiments disclosed herein are described primarily in the context of virtual machine instances other types of instances can be utilized with the concepts and technologies disclosed herein. For instance the technologies disclosed herein may be utilized with instances of storage resources instances of data communications resources and with other types of resources. The embodiments disclosed herein may also execute all or a portion of an application directly on a computer system without utilizing virtual machine instances.

Datacenter shown in may also include a server computer reserved for executing software components for managing the operation of datacenter server computers and instances . In particular server computer may execute a management component . As discussed above an entity of compute service platform may utilize customer computing system to access management component to configure various aspects of the operation of compute service platform and instances purchased by the entity. For example the entity may purchase instances and make changes to the configuration of the instances. The entity may also specify settings regarding how the purchased instances are to be scaled in response to demand. The entity may also provide requests to launch instances to management component .

As also described briefly above an auto scaling component may scale instances based upon rules defined by an entity of compute service platform . For example auto scaling component may allow an entity to specify scale up rules for use in determining when new instances should be instantiated and scale down rules for use in determining when existing instances should be terminated.

Auto scaling component may execute on a single server computer or in parallel across multiple server computers in compute service platform . In addition auto scaling component may consist of a number of subcomponents executing on different server computers or other computing devices in compute service platform . Auto scaling component may be implemented as software hardware or any combination of the two. Auto scaling component may monitor available compute resources in compute service platform over an internal management network for example.

As discussed briefly above datacenter may also be configured with a deployment component to assist entities in the deployment of new instances of compute resources. Deployment component may receive a configuration from an entity that includes data describing how new instances should be configured. For example the configuration may specify one or more applications that should be installed in new instances provide scripts and or other types of code to be executed for configuring new instances provide cache warming logic specifying how an application cache should be prepared and other types of information.

Deployment component may utilize the entity provided configuration and cache warming logic to configure prime and launch new instances . The configuration cache warming logic and other information may be specified by an entity using management component or by providing this information directly to deployment component . Other mechanisms may also be utilized to configure the operation of deployment component .

In the example datacenter shown in an appropriate LAN may be utilized to interconnect server computers A N and server computer . LAN may also be connected to WAN illustrated in . It should be appreciated that the network topology illustrated in has been greatly simplified and that many more networks and networking devices may be utilized to interconnect the various computing systems disclosed herein. Appropriate load balancing devices or software modules may also be utilized for balancing a load between each of datacenters A N between each of server computers A N in each datacenter and between instances purchased by each entity of compute service platform . These network topologies and devices should be apparent to those skilled in the art.

It should be appreciated that datacenter described in is merely illustrative and that other implementations may be utilized. In particular functionality described herein as being performed by management component auto scaling component and deployment component may be performed by one another may be performed by other components or may be performed by a combination of these or other components. Additionally it should be appreciated that this functionality may be implemented in software hardware or a combination of software and hardware. Other implementations should be apparent to those skilled in the art.

Computer may include a baseboard or motherboard which is a printed circuit board to which a multitude of components or devices may be connected by way of a system bus or other electrical communication paths. One or more central processing units CPUs may operate in conjunction with a chipset . CPUs may be standard programmable processors that perform arithmetic and logical operations necessary for the operation of computer .

CPUs may perform the necessary operations by transitioning from one discrete physical state to the next through the manipulation of switching elements that differentiate between and change these states. Switching elements may generally include electronic circuits that maintain one of two binary states such as flip flops and electronic circuits that provide an output state based on the logical combination of the states of one or more other switching elements such as logic gates. These basic switching elements may be combined to create more complex logic circuits including registers adders subtractors arithmetic logic units floating point units and the like.

Chipset may provide an interface between CPUs and the remainder of the components and devices on the baseboard. Chipset may provide an interface to a random access memory RAM used as the main memory in computer . Chipset may further provide an interface to a computer readable storage medium such as a read only memory ROM or non volatile RAM NVRAM for storing basic routines that may help to start up computer and to transfer information between the various components and devices. ROM or NVRAM may also store other software components necessary for the operation of computer in accordance with the embodiments described herein.

Computer may operate in a networked environment using logical connections to remote computing devices and computer systems through network . Chipset may include functionality for providing network connectivity through a network interface controller NIC such as a gigabit Ethernet adapter. NIC may be capable of connecting the computer to other computing devices over network . It should be appreciated that multiple NICs may be present in computer connecting the computer to other types of networks and remote computer systems.

Computer may be connected to a mass storage device that provides non volatile storage for the computer. Mass storage device may store system programs application programs other program modules and data which have been described in greater detail herein. Mass storage device may be connected to computer through a storage controller connected to chipset . Mass storage device may consist of one or more physical storage units. Storage controller may interface with the physical storage units through a serial attached SCSI SAS interface a serial advanced technology attachment SATA interface a fiber channel FC interface or other type of interface for physically connecting and transferring data between computers and physical storage units.

Computer may store data on mass storage device by transforming the physical state of the physical storage units to reflect the information being stored. The specific transformation of a physical state may depend on various factors and on different implementations of this description. Examples of such factors may include but are not limited to the technology used to implement the physical storage units whether mass storage device is characterized as primary or secondary storage and the like.

For example computer may store information to mass storage device by issuing instructions through storage controller to alter the magnetic characteristics of a particular location within a magnetic disk drive unit the reflective or refractive characteristics of a particular location in an optical storage unit or the electrical characteristics of a particular capacitor transistor or other discrete component in a solid state storage unit. Other transformations of physical media are possible without departing from the scope and spirit of the present description with the foregoing examples provided only to facilitate this description. Computer may further read information from mass storage device by detecting the physical states or characteristics of one or more particular locations within the physical storage units.

In addition to mass storage device described above computer may have access to other computer readable storage media to store and retrieve information such as program modules data structures or other data. It should be appreciated by those skilled in the art that computer readable storage media can be any available media that provides for the storage of non transitory data and that may be accessed by computer .

By way of example and not limitation computer readable storage media may include volatile and non volatile transitory and non transitory removable and non removable media implemented in any method or technology. Computer readable storage media includes but is not limited to RAM ROM erasable programmable ROM EPROM electrically erasable programmable ROM EEPROM flash memory or other solid state memory technology compact disc ROM CD ROM digital versatile disk DVD high definition DVD HD DVD BLU RAY or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium that can be used to store the desired information in a non transitory fashion.

Mass storage device may store an operating system utilized to control the operation of the computer . According to one embodiment the operating system comprises a version of the LINUX operating system. According to another embodiment the operating system comprises a version of the WINDOWS SERVER operating system from the MICROSOFT Corporation. According to further embodiments the operating system may comprise a version of the UNIX operating system. It should be appreciated that other operating systems may also be utilized. Mass storage device may store other system or application programs and data utilized by computer such as management component and or the other software components described above.

Mass storage device or other computer readable storage media may also be encoded with computer executable instructions which when loaded into computer transforms the computer from a general purpose computing system into a special purpose computer capable of implementing the embodiments described herein. These computer executable instructions transform computer by specifying how CPUs transition between states as described above. Computer may have access to computer readable storage media storing computer executable instructions which when executed by computer may perform operating procedures depicted in .

Computer may also include an input output controller for receiving and processing input from a number of input devices such as a keyboard a mouse a touchpad a touch screen an electronic stylus or other type of input device. Similarly input output controller may provide output to a display such as a computer monitor a flat panel display a digital projector a printer a plotter or other type of output device. It will be appreciated that computer may not include all of the components shown in may include other components that are not explicitly shown in or may utilize an architecture completely different than that shown in .

It should be appreciated that the network topologies illustrated in the figures have been greatly simplified and that many more networks and networking devices may be utilized to interconnect the various computing systems disclosed herein. These network topologies and devices should be apparent to those skilled in the art.

It should also be appreciated that the systems in the figures are merely illustrative and that other implementations might be used. Additionally it should be appreciated that the functionality disclosed herein might be implemented in software hardware or a combination of software and hardware. Other implementations should be apparent to those skilled in the art. It should also be appreciated that a server gateway or other computing device may comprise any combination of hardware or software that can interact and perform the described types of functionality including without limitation desktop or other computers database servers network storage devices and other network devices PDAs tablets cellphones wireless phones pagers electronic organizers Internet appliances television based systems e.g. using set top boxes and or personal digital video recorders and various other consumer products that include appropriate communication capabilities. In addition the functionality provided by the illustrated modules may in some embodiments be combined in fewer modules or distributed in additional modules. Similarly in some embodiments the functionality of some of the illustrated modules may not be provided and or other additional functionality may be available.

Each of the operations processes methods and algorithms described in the preceding sections may be embodied in and fully or partially automated by code modules executed by one or more computers or computer processors. The code modules may be stored on any type of non transitory computer readable medium or computer storage device such as hard drives solid state memory optical disc and or the like. The processes and algorithms may be implemented partially or wholly in application specific circuitry. The results of the disclosed processes and process steps may be stored persistently or otherwise in any type of non transitory computer storage such as e.g. volatile or non volatile storage.

The various features and processes described above may be used independently of one another or may be combined in various ways. All possible combinations and sub combinations are intended to fall within the scope of this disclosure. In addition certain method or process blocks may be omitted in some implementations. The methods and processes described herein are also not limited to any particular sequence and the blocks or states relating thereto can be performed in other sequences that are appropriate. For example described blocks or states may be performed in an order other than that specifically disclosed or multiple blocks or states may be combined in a single block or state. The example blocks or states may be performed in serial in parallel or in some other manner. Blocks or states may be added to or removed from the disclosed example embodiments. The example systems and components described herein may be configured differently than described. For example elements may be added to removed from or rearranged compared to the disclosed example embodiments.

It will also be appreciated that various items are illustrated as being stored in memory or on storage while being used and that these items or portions of thereof may be transferred between memory and other storage devices for purposes of memory management and data integrity. Alternatively in other embodiments some or all of the software modules and or systems may execute in memory on another device and communicate with the illustrated computing systems via inter computer communication. Furthermore in some embodiments some or all of the systems and or modules may be implemented or provided in other ways such as at least partially in firmware and or hardware including but not limited to one or more application specific integrated circuits ASICs standard integrated circuits controllers e.g. by executing appropriate instructions and including microcontrollers and or embedded controllers field programmable gate arrays FPGAs complex programmable logic devices CPLDs etc. Some or all of the modules systems and data structures may also be stored e.g. as software instructions or structured data on a computer readable medium such as a hard disk a memory a network or a portable media article to be read by an appropriate drive or via an appropriate connection. The systems modules and data structures may also be transmitted as generated data signals e.g. as part of a carrier wave or other analog or digital propagated signal on a variety of computer readable transmission media including wireless based and wired cable based media and may take a variety of forms e.g. as part of a single or multiplexed analog signal or as multiple discrete digital packets or frames . Such computer program products may also take other forms in other embodiments. Accordingly the present invention may be practiced with other computer system configurations.

Conditional language used herein such as among others can could might may e.g. and the like unless specifically stated otherwise or otherwise understood within the context as used is generally intended to convey that certain embodiments include while other embodiments do not include certain features elements and or steps. Thus such conditional language is not generally intended to imply that features elements and or steps are in any way required for one or more embodiments or that one or more embodiments necessarily include logic for deciding with or without author input or prompting whether these features elements and or steps are included or are to be performed in any particular embodiment. The terms comprising including having and the like are synonymous and are used inclusively in an open ended fashion and do not exclude additional elements features acts operations and so forth. Also the term or is used in its inclusive sense and not in its exclusive sense so that when used for example to connect a list of elements the term or means one some or all of the elements in the list.

While certain example embodiments have been described these embodiments have been presented by way of example only and are not intended to limit the scope of the inventions disclosed herein. Thus nothing in the foregoing description is intended to imply that any particular feature characteristic step module or block is necessary or indispensable. Indeed the novel methods and systems described herein may be embodied in a variety of other forms furthermore various omissions substitutions and changes in the form of the methods and systems described herein may be made without departing from the spirit of the inventions disclosed herein. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of certain of the inventions disclosed herein.

