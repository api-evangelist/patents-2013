---

title: Forwarding using maximally redundant trees
abstract: Network devices can use maximally redundant trees (MRTs) for delivering traffic streams across a network, and for transitioning traffic to a new set of MRTs after a topology change, without dropping traffic. The disclosure describes distributed computation of a set of MRTs from one or more ingress devices to one or more egress devices of the network. In one example, network devices in a network compute a set of MRTs, and establish a set of LSPs along the paths of the set of MRTs. After a change to the network topology, convergence sequencing is managed by a central controller, which centrally orchestrates the sequence for moving traffic from being sent on the old MRT paths to being sent on newly computed MRT paths after the controller determines that all new MRT forwarding state has been installed on the network devices.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09571387&OS=09571387&RS=09571387
owner: Juniper Networks, Inc.
number: 09571387
owner_city: Sunnyvale
owner_country: US
publication_date: 20130830
---
This application claims priority to U.S. Provisional Application No. 61 846 999 filed Jul. 16 2013 and this application is a continuation in part of U.S. patent application Ser. No. 13 418 212 entitled FAST REROUTE FOR MULTICAST USING MAXIMALLY REDUNDANT TREES filed on Mar. 12 2012 this application is a continuation in part of U.S. patent application Ser. No. 13 610 520 filed Sep. 11 2012 entitled CONSTRAINED MAXIMALLY REDUNDANT TREES FOR POINT TO MULTPOINT LSPS the entire contents of each of which being incorporated by reference herein.

The disclosure relates to computer networks and more particularly to forwarding network traffic within computer networks.

The term link is often used to refer to the connection between two devices on a computer network. The link may be a physical medium such as a copper wire a coaxial cable any of a host of different fiber optic lines or a wireless connection. In addition network devices may define virtual or logical links and map the virtual links to the physical links. As networks grow in size and complexity the traffic on any given link may approach a maximum bandwidth capacity for the link thereby leading to congestion and loss.

Multi protocol Label Switching MPLS is a mechanism used to engineer traffic patterns within Internet Protocol IP networks. By utilizing MPLS a source device can request a path through a network i.e. a Label Switched Path LSP . An LSP defines a distinct path through the network to carry packets from the source device to a destination device. A short label associated with a particular LSP is affixed to packets that travel through the network via the LSP. Routers along the path cooperatively perform MPLS operations to forward the MPLS packets along the established path. LSPs may be used for a variety of traffic engineering purposes including bandwidth management and quality of service QoS .

Historically MPLS label distribution was driven by protocols such as label distribution protocol LDP Resource ReserVation Protocol with Traffic Engineering extensions RSVP TE and labeled Border Gateway Protocol LBGP . Procedures for LDP by which label switching routers LSRs distribute labels to support MPLS forwarding along normally routed paths are described in L. Anderson LDP Specification RFC 3036 Internet Engineering Task Force IETF January 2001 the entire contents of which are incorporated by reference herein. RSVP TE uses constraint information such as bandwidth availability to compute and establish LSPs within a network. RSVP TE may use bandwidth availability information accumulated by a link state interior routing protocol such as the Intermediate System Intermediate System IS IS protocol or the Open Shortest Path First OSPF protocol.

Head end routers of an LSP are commonly known as ingress routers while routers at the tail end of the LSP are commonly known as egress routers. Ingress and egress routers as well as intermediate routers along the LSP that support MPLS are referred to generically as label switching routers LSRs . A set of packets to be forwarded along the LSP is referred to as a forwarding equivalence class FEC . A plurality of FECs may exist for each LSP but there may be only one active LSP for any given FEC. Typically a FEC definition includes the IP address of the destination of the LSP e.g. an IP address assigned to the egress router of the LSP. The ingress label edge router LER uses routing information propagated from the egress LER to determine the LSP to assign labels for the LSP and to affix a label to each packet of the FEC. The LSRs use MPLS protocols to receive MPLS label mappings from downstream LSRs and to advertise MPLS label mappings to upstream LSRs. When an LSR receives an MPLS packet from an upstream router it switches the MPLS label according to the information in its forwarding table and forwards the packet to the appropriate downstream LSR or LER. The egress LER removes the label from the packet and forwards the packet to its destination in accordance with non label based packet forwarding techniques.

In general each router along the LSP maintains a context that associates a FEC with an incoming label and an outgoing label. In this manner when an LSR receives a labeled packet the LSR may swap the label i.e. the incoming label with the outgoing label by performing a lookup in the context. The LSR may then forward the packet to the next LSR or LER along the LSP. The next router along the LSP is commonly referred to as a downstream router or a next hop.

In some instances a node or link along an LSP may no longer be available. For example a link along the LSP or a node may experience a failure event such as when one or more components of a router fail or the router is brought down by a user such as a network operator. In these instances signaling of a new LSP would fail when the LSP was to be explicitly routed along a path that traverses the unavailable link or node. An LSR along the path of the new LSP would detect the failed link or node and may send an error message indicating that the new LSP cannot be established as requested.

When a link or router in the network fails routers using traditional link state protocols such as OSPF and or IS IS may take a long time to adapt their forwarding tables in response to the topological change resulting from node and or link failures in the network. The process of adapting the forwarding tables is known as convergence. This time delay occurs because each node must update its representation of the network topology and execute the shortest path algorithm to calculate the next hop for each destination within the updated network topology. Until the next hops are re computed traffic being sent toward the failed links may be dropped.

In general techniques are described for using maximally redundant trees MRTs for delivering traffic streams across a network and for transitioning traffic to a new set of MRTs after a topology change without dropping traffic due to the transition. MRTs are a set of trees where for each of the MRTS a set of paths from a root node of the MRT to one or more leaf nodes share a minimum number of nodes and a minimum number of links. Techniques are described herein that make use of distributed computation of a set of MRTs from one or more ingress devices to one or more egress devices of the network. In one example a plurality of network devices in a network compute a set of MRTs on a network graph representing nodes in the network and establish a set of destination rooted trees LSPs along the paths of the set of MRTs from one or more ingress network devices to one or more egress network devices. In another example the network devices can use the computed set of MRTs for forwarding IP traffic where packets are encapsulated in IP where the IP destination is an address that is associated with a particular multi topology identifier for one of the MRTs and a router or destination. After a change to the network topology that necessitates re computation of the MRTs convergence sequencing is managed by a central controller which centrally orchestrates the sequence for moving traffic from being sent on the old MRT paths to being sent on the new MRT paths after the controller determines that all new MRT forwarding state has been installed on the network devices in a network.

Using MRTs for computing destination rooted spanning trees generally provides link and node disjointness of the spanning trees to the extent physically feasible regardless of topology based on the topology information distributed by a link state Interior Gateway Protocol IGP . The techniques set forth herein provide mechanisms for handling real networks which may not be fully 2 connected due to previous failure or design. A 2 connected graph is a graph that requires two nodes to be removed before the network is partitioned.

The techniques may provide one or more advantages. For example the techniques can allow for dynamically adapting in network environments in which the same traffic is sent on two or more diverse paths such as in unicast live live or multicast live live. Unicast live live functionality can be used to reduce packet loss due to network failures on any one of the paths. The techniques of this disclosure can provide for dynamically adapting to network changes in the context of unicast and multicast live live. The techniques can provide a mechanism that is responsive to changes in network topology without requiring manual configuration of explicit route objects or heuristic algorithms. The techniques of this disclosure do not require operator involvement to recalculate the LSPs in the case of network topology changes. The use of MRTs in this manner can provide live live functionality and provide a mechanism for sending live live streams across an arbitrary network topology so that the disjoint trees can be dynamically recalculated in a distributed fashion as the network topology changes.

As another example using MRTs for unicast live live may also scale better than an RSVP TE based solution because MRT unicast uses destination based trees so there is state based only on the number of egress nodes being used. Likewise multicast live live uses source based trees so there is state based only on the number of ingress nodes. The techniques of this disclosure may also be operationally simpler than an RSVP TE solution.

In one aspect a method includes receiving by a centralized controller of a network an indication from each of a plurality of network devices confirming that each of the plurality of network devices has installed forwarding state associated with an updated set of maximally redundant trees wherein the updated set of maximally redundant trees were computed based on an updated network graph responsive to a change in topology of the network wherein the updated set of maximally redundant trees replaces a first set of maximally redundant trees that were computed based on a network graph representing the network prior to the change in topology. The method further includes responsive to determining that the indication has been received from each of the plurality of network devices from which confirmation is needed and by the centralized controller instructing one or more ingress network devices of the plurality of network devices to begin forwarding network traffic according to the forwarding state associated with the updated set of maximally redundant trees and removing by the controller from a routing information base of each of the network devices forwarding state associated with the first set of maximally redundant trees.

In another aspect a network device includes one or more hardware based processors and a maximally redundant tree management module executing on the one or more processors wherein the maximally redundant tree management module is configured to receive an indication from each of a plurality of network devices confirming that each of the plurality of network devices has installed forwarding state associated with an updated set of maximally redundant trees wherein the updated set of maximally redundant trees were computed based on an updated network graph responsive to a change in topology of the network wherein the updated set of maximally redundant trees replaces a first set of maximally redundant trees that were computed based on a network graph representing the network prior to the change in topology. The maximally redundant tree management module is configured to responsive to determining that the indication has been received from each of the plurality of network devices from which confirmation is needed instruct one or more ingress network devices of the plurality of network devices to begin forwarding network traffic according to the forwarding state associated with the updated set of maximally redundant trees and remove from a routing information base of each of the network devices forwarding state associated with the first set of maximally redundant trees.

In another aspect a computer readable storage medium includes instructions. The instructions cause a programmable processor to send by a network device to a centralized controller of a network an indication confirming that the network device has installed forwarding state associated with an updated set of maximally redundant trees wherein the updated set of maximally redundant trees were computed based on an updated network graph responsive to a change in topology of the network wherein the updated set of maximally redundant trees replaces a first set of maximally redundant trees that were computed based on a network graph representing the network prior to the change in topology receive from the centralized controller instructions to begin forwarding network traffic according to the forwarding state associated with the updated set of maximally redundant trees and receive from the centralized controller instructions to remove from a routing information base of the network device forwarding state associated with the first set of maximally redundant trees.

The details of one or more examples are set forth in the accompanying drawings and the description below. Other features objects and advantages will be apparent from the description and drawings and from the claims.

Ingress devices egress devices and transit nodes are coupled by links which may be a number of physical and logical communication links that interconnect ingress devices egress devices and transit nodes to facilitate control and data communication between the devices. Physical links of network may include for example Ethernet PHY Synchronous Optical Networking SONET Synchronous Digital Hierarchy SDH Lambda or other Layer data links that include packet transport capability. Logical links of network may include for example an Ethernet Virtual local area network LAN a Multi Protocol Label Switching MPLS Label Switched Path LSP or an MPLS traffic engineered TE LSP.

In the example of system also includes source device that sends unicast traffic into network via ingress devices and access network A and receiver device that receives the unicast traffic from egress device A and or egress device B through access network B. The unicast traffic may be for example video or multimedia traffic.

Network may be a service provider network that operates as a private network that provides packet based network services to receiver device which may be a subscriber device for example. Network B may include multiple receiver devices not shown . Receiver device may be for example any of personal computers laptop computers or other types of computing device associated with subscribers. Subscriber devices may comprise for example mobile telephones laptop or desktop computers having e.g. a 3G wireless card wireless capable netbooks video game devices pagers smart phones personal data assistants PDAs or the like. Subscriber devices may run a variety of software applications such as word processing and other office support software web browsing software software to support voice calls video games videoconferencing and email among others.

For some traffic flows such as sent on pseudo wires it can be useful to send traffic on two diverse routes from one or more ingress network devices to one or more egress network devices in the network. This disclosure describes how to use Maximally Redundant Trees and a central controller to create such routes with the ability to partially consider traffic load. The controller manages convergence events. The techniques of this disclosure can be used whether the diverse routes are desired for resiliency i.e. 1 1 or for throughput sending different traffic on both paths .

For distribution of unicast traffic including time sensitive or critical unicast traffic network can be set up to employ unicast live live techniques in which the same traffic is sent on two or more diverse unicast paths. Unicast live live functionality can be used to reduce packet loss due to network failures on any one of the paths. As explained in further detail below in some examples controller can instruct ingress device s A and or B to compute a set of spanning trees that are maximally redundant trees from ingress device s A and or B to egress device s A and or B. Maximally Redundant Trees MRTs can be used to compute diverse paths between any two nodes in a network that has such paths . The MRTs are spanning trees to reach all routers in the network graph rooted at egress PE routers . In some examples the set of MRTs includes a pair of MRTs. The pair of MRTs may sometimes be referred to as the Blue MRT and the Red MRT. The nodes in network can each use a common algorithm to compute the MRTs as rooted at one of egress network devices . Where both of egress network devices are needed the MRTs are computed as rooted at a proxy node connected to both of egress network devices . Ingress devices and TNs compute the MRTs as a pair of MRTs that traverse maximally disjoint paths from the one or more of ingress devices to one or more of egress devices . Ingress devices can dynamically recalculate maximally redundant trees e.g. responsive to detecting changes to the topology of network or at the direction of controller .

In some aspects ingress devices egress devices and transit nodes may be Internet Protocol IP routers that implement Multi Protocol Label Switching MPLS techniques and operate as label switching routers LSRs . In some examples at the direction of controller or based on local configuration ingress device A may establish a set of LSPs A B LSPs along the maximally redundant trees for concurrently sending the same unicast traffic from source device to receiver device . For example ingress devices can assign a label to each incoming packet received from source device based on the destination of the packets and the forwarding equivalence classes of the labels before forwarding the packet to a next hop transit node . Each transit node makes a forwarding selection and determines a new substitute label by using the label found in the incoming packet as a reference to a label forwarding table that includes this information. The paths taken by packets that traverse the network in this manner are referred to as LSPs.

Each of ingress devices egress devices and transit nodes may each compute in a distributed fashion using a common algorithm a pair of MRTs based on the current network topology. The ingress devices egress devices and transit nodes will obtain the same pair of MRTs from the computation and will advertise labels to their LDP peers for reaching the destination root of the MRTs via MRT red and MRT blue and install in forwarding information the labels and next hops for sending traffic toward the destination via MRT red and MRT blue. In this manner LSPs can be set up in network along the MRTs. For example LSP A may follow the red MRT and LSP B may follow the blue MRT.

After establishing the LSPs ingress devices may receive unicast data traffic from source device and ingress devices can forward the multicast data traffic along both of LSPs A and B. That is ingress device A and ingress device B concurrently send the same unicast traffic received from source device to receiver device on both of the first LSP A and the second LSP B. In this manner ingress device A and ingress device B send redundant unicast data traffic along both of LSPs A and B which provides unicast live live service.

In the example of multicast live live ingress devices egress devices and transit nodes can likewise establish Point to Multipoint P2MP LSPs along source rooted MRTs for sending multicast live live traffic to receiver device .

In some examples ingress devices egress devices and transit nodes can use IP encapsulation for forwarding IP traffic along the computed MRTs where the IP destination is an address that is associated with a particular multi topology identifier for each of the MRTs and a router or destination.

Controller manages convergence events. In some examples when a topology change occurs in network the re computation of MRTs needed in view of the topology change may affect both the blue and red MRT forwarding trees. So when the topology changes all ingress nodes continue to send on the MRT Red 1 or MRT Blue 1. In some examples responsive to detecting the topology change controller re computes new MRTs e.g. MRT Red 2 and MRT Blue 2 and installs the new forwarding state for the new MRTs into all LSRs or relevant LSRs known to be on path from an ingress to an egress for MRT Red 2 and MRT Blue 2. In other examples responsive to detecting the topology change controller instructs all LSRs or the relevant LSRs to compute and install the new MRTs.

Responsive to confirming that the new LFIB entries are installed in each LSR controller instructs each ingress node to switch from using MRT Red 1 and MRT Blue 1 to using MRT Red 2 and MRT Blue 2. To confirm in some examples controller may wait a time period or may await a response from each of the LSRs indicating that the new forwarding state is installed in the labeled forwarding information base LFIB of each LSR. Controller also confirms that the ingress nodes that the ingress nodes have switched to the new MRTs and that traffic is no longer flowing on the old MRTs. Responsive to confirming this controller can remove the old MRT Red 1 and MRT Blue 1 state from Ingress devices egress devices and transit nodes e.g. by directly interfacing with the routing information bases of these devices. This completes the convergence and the controller is prepared to do the same sequence for the next topology event.

Changes in the network topology may be communicated among devices and transit nodes in various ways for example by using a link state protocol such as interior gateway protocols IGPs like the Open Shortest Path First OSPF and Intermediate System to Intermediate System IS IS protocols. That is devices and transit nodes can use the IGPs to learn link states and link metrics for communication links between the nodes within the interior of network . If a communication link fails or a cost value associated with a network node changes after the change in the network s state is detected by one of devices and transit nodes that device may flood an IGP Advertisement communicating the change to the other devices in the network. In other examples devices and transit nodes can communicate the network topology using other network protocols such as an interior Border Gateway Protocol iBGP e.g. BGP Link State BGP LS . In this manner each of the routers eventually converges to an identical view of the network topology.

For example devices and transit nodes may use OSPF or IS IS to exchange routing information with routers . Devices and transit nodes may store the routing information to a routing information base that devices and transit nodes use to compute optimal routes to destination addresses advertised within network . In addition devices and transit nodes can store to a traffic engineering database TED any traffic engineering TE metrics or constraints received via the IGPs advertisements.

In some examples ingress devices may create LSPs by computing the entire trees as MRTs and signaling the LSPs e.g. using a resource reservation protocol such as RSVP TE . In other examples devices and transit nodes perform the MRT computation and establishment of LSPs A B in a distributed fashion e.g. using LDP for exchanging labels for the LSPs . As a further example a path computation element PCE of controller may alternatively or additionally provide configuration information to one or more of devices and transit nodes e.g. may compute the MRTs and provide them to ingress devices . For example controller may include a topology module that can learn the topology from IGP BGP or another mechanism and then perform the MRT computation and provide the result to devices and transit nodes .

For controller to orchestrate convergence controller may include various capabilities a way to rapidly learn topology e.g. an IGP feed the ability to quickly install and remove LFIB entries the ability to be notified when LFIB entries have successfully been installed to the forwarding plane the ability to direct pseudowires from using one LSP to using another and the ability to be notified when the pseudo wire has changed to using the new LSP. In some examples controller could request and be allocated a range of MPLS labels to use in the LFIB and directly install state to all LSRs as quickly as controller can work. Controller may use an Interface to the Routing System I2RS framework. Examples of an interface for a controller to communicate with routers in a network for routing and forwarding data is described in A. Atlas Interface to the Routing System Framework Internet Draft Network Working Group Jul. 30 2012 N. Bahadur Routing Information Base Info Model Internet Draft Network Working Group Jul. 15 2013 AND A. Atlas An Architecture for the Interface to the Routing System Internet Draft Network Working Group Aug. 13 2013 the entire contents of each of which are incorporated by reference herein.

If there are multiple ingress nodes or egress nodes in a network graph these can be represented in the network graph by creating a proxy node attached to both of the ingresses or both of the egresses. When computing the MRTs to the egress devices A and B the ingress devices will attach a proxy node to egress devices A and B to represent a shared egress. Ingress device A computes a path to reach the proxy node via egress device A and ingress device B computes a path to reach the proxy node via egress device B. Because of the way the proxy node is attached to egress devices A and B one of egress devices A and B will attach to the proxy node via blue MRT and the other will attach to the proxy node via red MRT and so egress device A and egress device B will end up taking MRT blue and MRT red respectively based on how the destination is connected up to the proxy node. More details on using proxy nodes are described in U.S. patent application Ser. No. 13 418 212 entitled FAST REROUTE FOR MULTICAST USING MAXIMALLY REDUNDANT TREES filed on Mar. 12 2012 and A. Atlas An Architecture for IP LDP Fast Reroute Using Maximally Redundant Trees Internet Draft Routing Area Working Group Jul. 12 2013 the entire contents of each of which are incorporated by reference herein. One advantage of MRTs is that there is a lot less computation so devices and can compute the MRTs for all sources for multicast and all destinations for unicast and each device computes its part of each of those MRTs.

In some examples devices and transit nodes or controller can use a method of computing the maximally redundant trees that also considers traffic engineering constraints such as bandwidth link color priority and class type for example. In accordance with one example aspect of this disclosure devices and transit nodes or controller computes a set of spanning trees that are maximally redundant trees MRTs over a network graph that represents at least a portion of links and nodes in network .

If controller is aware of the flows and link capacities then controller could compute destination based MRTs separately for each egress node and prune the topology in between computations based upon whether the links would have enough capacity for the next set of flows to the egress node. Controller would then be able to revert the topology to the way it was before pruning if the pruning would cause additional common failure points cut links or cut vertices . Examples of constraint based MRT pruning are described in U.S. application Ser. No. 13 610 520 filed Sep. 11 2012 entitled CONSTRAINED MAXIMALLY REDUNDANT TREES FOR POINT TO MULTPOINT LSPS the entire contents of which are incorporated by reference herein.

In this manner the techniques of this disclosure allow for using distributed computation of MRTs and setting up LSPs but using a centralized approach in order to sequence the traffic during re convergence after topology change so that traffic delivery is not disrupted as a result of the re convergence. Controller manages the process that is implemented in a distributed fashion by the network devices of system . The process is therefore not dependent on the controller to understand or implement MRTs or to even know the topology of network . In some example aspects controller does not need to have a topology database and does not have to have a path computation element. All controller needs to know is which routers exist and the ability to assign new MT IDs to use for the next topology event. Controller can orchestrate the convergence sequence for computing a new path and moving the traffic from the old path to the new path in a way that a distributed convergence cannot do as quickly.

In another example controller could do phasing with a worst case network reconvergence time as follows a Stay on old MRT Red and old MRT Blue. b Compute and install temp MRT Red and temp MRT Blue. c Wait the worst case network convergence time. d Move traffic to temp MRT Red and temp MRT Blue. e Wait the worst case network convergence time or as necessary for all traffic to be moved . f Update old MRT Red and old MRT Blue to be the same as temp MRT Red and temp MRT Blue. g Wait the worst case network convergence time. h Move traffic from the temp MRT Red and temp MRT Blue to the updated MRT Red and MRT Blue. i Wait the worst case network convergence time or as necessary for all traffic to be moved . j Remove the temp MRT Red and temp MRT Blue state.

Although techniques of this disclosure are described by way of example in terms of unicast live live forwarding the techniques of this disclosure can also readily be applied for controller managing the sequencing for convergence in multicast live live forwarding.

When network device receives a packet via one of input links control unit determines via which of output links to send the packet. Control unit includes routing component and forwarding component . Routing component determines one or more routes through a network e.g. through interconnected devices such as other routers. Control unit provides an operating environment for protocols which are typically implemented as executable software instructions. As illustrated protocols include OSPF A and intermediate system to intermediate system IS IS B. Network device uses LDP C to exchange labels for setting up LSPs. Protocols also include Protocol Independent Multicast D which can be used by network device for transmitting multicast traffic. Protocols may include other routing protocols in addition to or instead of OSPF A and IS IS B such as other Multi protocol Label Switching MPLS protocols including RSVP TE or routing protocols such as Internet Protocol IP routing information protocol RIP border gateway protocol BGP interior routing protocols other multicast protocols or other network protocols.

By executing the routing protocols routing component identifies existing routes through the network and determines new routes through the network. Routing component stores routing information in a routing information base RIB that includes for example known routes through the network. RIB may simultaneously include routes and associated next hops for multiple topologies such as the Blue MRT topology and the Red MRT topology.

Forwarding component stores forwarding information base FIB that includes destinations of output links . FIB may be generated in accordance with RIB . FIB and LFIB may be a radix tree programmed into dedicated forwarding chips a series of tables a complex database a link list a radix tree a database a flat file or various other data structures. FIB or LFIB may include MPLS labels such as for LDP LSPs . FIB or LFIB may simultaneously include labels and forwarding next hops for multiple topologies such as the Blue MRT topology and the Red MRT topology.

Network device includes a data plane that includes forwarding component . In some aspects IFCs may be considered part of data plane . Network device also includes control plane . Control plane includes routing component and user interface UI . Although described for purposes of example in terms of a router network device may be in some examples any network device capable of performing the techniques of this disclosure including for example a network device that includes routing functionality and other functionality.

A system administrator ADMIN may provide configuration information to network device via UI included within control unit . For example the system administrator may configure network device or install software to provide MRT functionality as described herein. As another example the system administrator may configure network device with a request to establish a set of LSPs or IP tunnels from one or more ingress routers to one or more egress routers. As a further example a path computation element PCE of controller may alternatively or additionally provide configuration information to network device . In some examples a PCE of controller may compute the set of MRTs and provide them to network device with instructions to network device to install forwarding state for LSPs or IP tunnels along the MRTs.

In some examples controller may request that network device establish unicast live live forwarding state for LSPs and may provide network device with at least two pairs of multi topology identifiers MT IDs . The MT IDs provided may include a first set of multi topology identifiers each multi topology identifier of the first set of set of multi topology identifiers for identifying a different maximally redundant tree of a first set of maximally redundant trees. The provided MT IDs may also include a second set of multi topology identifiers each multi topology identifier of the second set of set of multi topology identifiers for identifying a different maximally redundant tree of the second set of maximally redundant trees. As shown in control plane of network device has a modified CSPF module referred to as MRT module that computes the trees using an MRT algorithm. MRT module may associate the first set of multi topology identifiers with a first set of computed MRTs and may associate the second set of multi topology identifiers with a later computed set of MRTs to be used after a change in network topology. MRT module may compute MRTs for delivering unicast live live traffic. In some cases network device is configured to compute the MRTs on its own without requiring a request from controller to do so.

The following terminology is used herein. A network graph is a graph that reflects the network topology where all links connect exactly two nodes and broadcast links have been transformed into the standard pseudo node representation. The term 2 connected as used herein refers to a graph that has no cut vertices i.e. a graph that requires two nodes to be removed before the network is partitioned. A cut vertex is a vertex whose removal partitions the network. A cut link is a link whose removal partitions the network. A cut link by definition must be connected between two cut vertices. If there are multiple parallel links then they are referred to as cut links in this document if removing the set of parallel links would partition the network.

A 2 connected cluster is a maximal set of nodes that are 2 connected. The term 2 edge connected refers to a network graph where at least two links must be removed to partition the network. The term block refers to either a 2 connected cluster a cut edge or an isolated vertex. A Directed Acyclic Graph DAG is a graph where all links are directed and there are no cycles in it. An Almost Directed Acyclic Graph ADAG is a graph that if all links incoming to the root were removed would be a DAG. A Generalized ADAG GADAG is a graph that is the combination of the ADAGs of all blocks. Further information on MRTs may be found at A. Atlas An Architecture for IP LDP Fast Reroute Using Maximally Redundant Trees Internet Draft Routing Area Working Group Jul. 12 2013 A. Atlas Algorithms for Computing Maximally Redundant Trees for IP LDP Fast Reroute Internet Draft Routing Area Working Group draft enyedi rtgwg mrt frr algorithm 03 Jul. 15 2013 A. Atlas An Architecture for Multicast Protection Using Maximally Redundant Trees Internet Draft Routing Area Working Group draft atlas rtgwg mrt mc arch 02 Jul. 12 2013 the entire contents of each of which are incorporated by reference herein.

Redundant trees are directed spanning trees that provide disjoint paths towards their common root. These redundant trees only exist and provide link protection if the network graph is 2 edge connected and node protection if the network graph is 2 connected. Such connectiveness may not be the case in real networks either due to architecture or due to a previous failure. Maximally redundant trees are useful in a real network because they may be computable regardless of network topology. Maximally Redundant Trees MRT are a set of trees where the path from any node X to the root R along one tree and the path from the same node X to the root along any other tree of the set of trees share the minimum number of nodes and the minimum number of links. Each such shared node is a cut vertex. Any shared links are cut links. That is the maximally redundant trees are computed so that only the cut edges or cut vertices are shared between the multiple trees. In any non 2 connected graph only the cut vertices and cut edges can be contained by both of the paths. That is a pair of MRTs such as MRT A and MRT B are a pair of trees that share a least number of links possible and share a least number of nodes possible. Any RT is an MRT but many MRTs are not RTs. MRTs are practical to maintain redundancy even after a single link or node failure. If a pair of MRTs is computed rooted at each destination all the destinations remain reachable along one of the MRTs in the case of a single link or node failure. The MRTs of a pair of MRTs may be individually referred to as a Red MRT and a Blue MRT.

Computationally practical algorithms for computing MRTs may be based on a common network topology database. A variety of algorithms may be used to calculate MRTs for any network topology. These may result in trade offs between computation speed and path length. Many algorithms are designed to work in real networks. For example just as with SPF an algorithm is based on a common network topology database with no messaging required. In one example aspect MRT computation for multicast Live Live may use a path optimized algorithm based on heuristics. Some example algorithms for computing MRTs can be found in U.S. patent application Ser. No. 13 418 212 entitled Fast Reroute for Multicast Using Maximally Redundant Trees filed on Mar. 12 2012 the entire contents of which are incorporated by reference herein.

Although described for purposes of example in terms of a set of MRTs being a pair of MRTs in other examples ingress devices may compute a set of constrained MRTs that includes more than two MRTs where each MRT of the set traverses a different path where each path is as diverse as possible from each other path. In such examples more complex algorithms may be needed to compute a set of MRTs that includes more than two MRTs.

In some aspects MRT module may compute the MRTs in response to receiving a request to traffic engineer a diverse set of P2MP LSPs to a plurality of egress routers such as to be used for multicast live live redundancy in forwarding multicast content. For example administrator may configure network device with the request via UI . The request may specify that the P2MP LSPs satisfy certain constraints. TE constraints specified by the request may include for example bandwidth link color Shared Risk Link Group SRLG and the like. Network device may store the specified TE constraints to TED .

In this example MRT module computes a set of MRTs from network device as the ingress device to a plurality of egress devices. Network device may compute the set of MRTs on a network graph having links that each satisfy stored traffic engineering TE constraints obtained from TE constraints database in the control plane of network device . In some examples constrained MRT module may obtain the network graph having links that each satisfy the TE constraints by starting with an initial network graph based on network topology information obtained from TED and pruning links of the initial network graph to remove any network links that do not satisfy the TE constraints resulting in a modified network graph. In this example constrained MRT module uses the modified network graph for computing the set of MRTs.

After computing the set of MRTs whether for unicast or multicast network device installs forwarding state for multiple LSPs from network device toward the egress network devices. For unicast LDP module C of network device may send an LDP label mapping message and for multicast network device may use a resource reservation protocol e.g. RSVP TE to send Path messages that specify a constrained path for setting up the P2MP LSPs. For example MRT module can communicate with LDP module C to provide LDP module C with the computed set of MRTs to be used for signaling the LSP and the MT IDs to use with the advertised labels. Network device can send different label mapping messages for different LSPs for each MRT of the set of MRTs.

After establishing the LSPs network device may receive unicast data traffic from multicast source device and network device can forward the unicast data traffic along both of LSPs . That is network device concurrently sends unicast traffic received from source device to receiver device on both of the first LSP A and the second LSP B. In this manner ingress devices send redundant multicast data traffic along both of LSPs which provides unicast live live service.

The techniques of this disclosure provide a mechanism for dynamically adapting unicast live live that is responsive to changes in network topology without requiring manual configuration of explicit route objects or heuristic algorithms. Operator involvement is not needed to recalculate the LSPs in the case of network topology changes.

If network device detects that changes have occurred to the topology of system MRT module of network device may re compute MRTs or portions of MRTs to determine whether changes are needed to LSPs . In the example of multicast live live if network device detects that that a new multicast receiver is added to system network device can send an updated Path message to add a branch to each of P2MP LSPs without needing to re signal the entire P2MP LSPs.

In some aspects network device may be configured to run a periodic re optimization of the MRT computation which may result in a similar transition from old MRTs to new MRTs. For example MRT module of network device can periodically recompute the pair of MRTs to determine whether a more optimal pair of MRTs exists on the network graph.

Once MRT module computes the MRTs the two sets of MRTs may be seen by the forwarding plane as essentially two additional topologies. Thus the same considerations apply for forwarding along the MRTs as for handling multiple topologies. For LDP it may be desirable to avoid tunneling because for at least node protection tunneling requires knowledge of remote LDP label mappings and thus requires targeted LDP sessions and the associated management complexity. Two different example mechanisms that network device can use for handling the multiple topologies and marking packets being sent onto the different MRT topologies are described below.

A first option referred to herein as Option A is to encode MRT topology in labels. For example in addition to sending a single label for a FEC LDP module C of network device would provide two additional labels with their associated MRT colors. Each additional label specifies an MRT topology blue or red associated with one of the maximally redundant trees. This approach may be simple but can reduce the label space for other uses. This approach may also increase the memory needed to store the labels and the communication required by LDP.

A second option referred to herein as Option B is to create topology identification labels topology id labels . The topology id labels may be stored in LFIB and may be LDP labels. In this approach LDP module C uses the label stacking ability of multi protocol label switching MPLS and specifies only two additional labels one for each associated MRT color by a new FEC type. When sending a packet onto an MRT LDP module C first swaps the LDP label and then pushes the topology id label for that MRT color. When receiving a packet with a topology id label LDP module C pops the topology id label and uses the topology id label to guide the next hop selection in combination with the next label in the stack. For example LDP module C does a lookup within FIB on the next inner label or the IP address of the packet in the case of IP and that lookup returns a set of sets of next hops. LDP module C then uses the topology id label to select among the sets. For example if the topology id label indicates the blue MRT should be used LDP module C uses the next hop or stack of next hops for the blue MRT. Similarly if the topology id label indicates the red MRT should be used LDP module C uses the next hop or stack of next hops for the red MRT. If there is no topology id label then LDP module C may just use the shortest path tree SPT next hop primary next hop . LDP module C then swaps the remaining label if appropriate and pushes the topology id label if needed for use the next hop and outputs the labeled packet on the outbound interface associated with the next hop.

The topology id label approach has minimal usage of additional labels memory and LDP communication. The topology id label approach does increase the size of packets and the complexity of the required label operations and look ups. The topology id label approach can for example use the same mechanisms as are needed for context aware label spaces. For example the top level topology id label may give context and the next label may give next hops. Further details on context aware label spaces can be found within U.S. application Ser. No. 12 419 507 entitled TRANSMITTING PACKET LABEL CONTEXTS WITHIN COMPUTER NETWORKS filed Apr. 9 2009 the entire content of which is incorporated by reference herein.

Note that with LDP unicast forwarding regardless of whether topology identification label or encoding topology in label is used no additional loopbacks per router are required as are required in the IP unicast forwarding case. This is because LDP labels are used on a hop by hop basis to identify MRT blue and MRT red forwarding trees.

LDP module C may be configured with extensions to LDP in various ways. For example LDP module C may be configured to specify the topology in the label. That is when sending a Label Mapping in a label mapping message that maps a FEC to a label LDP module C may have the ability to include a topology identifier in the FEC TLV and send an associated Label TLV. The FEC TLV would include a multi topology identifier MT ID that is assigned to specify MRT and the associated MRT color. In the example of Topology Identification Labels LDP would be extended to define a new FEC type that describes the topology for MRT and the associated MRT color. Another example option may be for LDP module C to advertise per interface a label indicating what the original incoming interface would have been.

For IP unicast traffic tunneling may be used. The tunnel egress could be the original destination in the area the next next hop etc. If the tunnel egress is the original destination router then the traffic remains on the redundant tree with sub optimal routing. If the tunnel egress is the next next hop then protection of multi homed prefixes and node failure for ABRs is not available. Selection of the tunnel egress is a router local decision.

The following are a few options for marking IP packets with which MRT the receiving device should use for forwarding the received IP packets. First a network device may tunnel IP packets via an LDP LSP. This has the advantage that more installed routers can do line rate encapsulation and decapsulation. Also no additional IP addresses would need to be allocated or signaled. Option A within this approach is to use a LDP Destination Topology Label. MRT module may use a label that indicates both the destination and the MRT. This method allows easy tunneling to the next next hop as well as to the IGP area destination. For multi homed prefixes this requires that additional labels be advertised for each proxy node. Option B within this approach is to use a LDP Topology Label. MRT module may use a Topology Identifier label on top of the IP packet. This is simple and does not require additional labels for proxy nodes. If tunneling to a next next hop is desired then a two deep label stack can be used with Topology ID label Next Next Hop Label .

Another approach is to tunnel IP packets in IP. Each router supporting this option may announce two additional loopback addresses and their associated MRT color. Those addresses are used as destination addresses for MRT blue and MRT red IP tunnels respectively. The announced additional loopback addresses allow the transit nodes to identify the traffic as being forwarded along either MRT blue or MRT red tree topology to reach the tunnel destination. The IGP such as OSPF A or IS IS B may be extended to employ announcements of these two additional loopback addresses per router with the associated MRT color. Another option that might be used is a pure IP unicast option that uses a new IP either IPv4 or IPv6 hop by hop option to specify the MRT color. Possibly only some platforms or linecards would be able to support this approach at line rate. A network device that uses the IP option may process the IP option in the fast path rather than feeding the packet to a line card to process.

For proxy nodes associated with one or more multi homed prefixes the problem is harder because there is no router associated with the proxy node so its loopbacks can t be known or used. In this case each router attached to the proxy node could announce two common IP addresses with their associated MRT colors. This would require configuration as well as the previously mentioned IGP extensions. Similarly in the LDP case two additional FEC bindings could be announced.

In general when network device receives a packet forwarding component may do a lookup of FIB or LFIB using the label of the received packet as a key. FIB or LFIB may return a set of next hops including the primary next hop and any alternate next hops e.g. LFA and MRT next hops . When a topology id label is used forwarding component may do a lookup on the topology id label to find the correct context and then use the next label of the received packet as a key and FIB or LFIB may return a set of next hops associated with that topology id for the second label those next hops would be for either the Blue MRT or the Red MRT topology. In another approach forwarding component may do a lookup of FIB or LFIB the second label of the received packet as a key. FIB or LFIB may return multiple sets of next hops and the topology id label is used to pick the appropriate set of next hops to use.

Network device may detect a change to the topology of network e.g. a link or node failure or in case of multicast a change to a multicast receiver. For example network device may be local to the change or may detect it based on a link state advertisement LSA received from a peer device. Responsive to detecting the topology change network device a device local to the change may begin forwarding network traffic on the other color MRT in the earlier computed pair of MRTs. Once aware of the topology change network device can re compute the pair of MRTs based on a modified network graph reflecting the topology change while traffic is still being received in the meantime by receiver device via the one MRT path that is still working of the first set of MRTs. MRT modules of network device can then create and install new forwarding state based on the updated re computed MRTs.

Upon installing the new forwarding state based on the updated re computed MRTs network device can notify controller that the updated MRT forwarding state has been installed. For example controller communication module of network device can notify controller . In some examples the notification may specify a network topology on which the updated set of maximally redundant trees was computed by network device . For example the indication may specify a network topology by including at least an indication of a LSA received by network device that triggered computation of the updated set of maximally redundant trees. In some cases the notification may also specify a number of seconds in which the updated set of maximally redundant trees was computed so that controller can determine whether the topology on which the MRTs were computed is still current. The notification may also specify the MT IDs of the old MRTs that have been impacted by the topology change and or the MT IDs of the new MRTs for which forwarding state has been installed.

Network device may receive instructions from controller e.g. via controller communication module to begin forwarding traffic on the new updated MRTs using the new forwarding state that controller has confirmed to be installed on all of the network devices. In response network device will begin forwarding traffic on the LSPs associated with the new MRTs. Controller may also delete forwarding state for the old MRTs e.g. from RIB FIB and or LFIB .

Controller includes a control unit coupled to a network interface to exchange packets with other network devices by inbound link and outbound link . Control unit may include one or more processors not shown in that execute software instructions such as those used to define a software or computer program stored to a computer readable storage medium again not shown in such as non transitory computer readable mediums including a storage device e.g. a disk drive or an optical drive or a memory such as Flash memory or random access memory RAM or any other type of volatile or non volatile memory that stores instructions to cause the one or more processors to perform the techniques described herein. Alternatively or additionally control unit may comprise dedicated hardware such as one or more integrated circuits one or more Application Specific Integrated Circuits ASICs one or more Application Specific Special Processors ASSPs one or more Field Programmable Gate Arrays FPGAs or any combination of one or more of the foregoing examples of dedicated hardware for performing the techniques described herein.

Control unit provides an operating environment for network services applications path computation element and MRT management module . In one example these modules may be implemented as one or more processes executing on one or more virtual machines of one or more servers. That is while generally illustrated and described as executing on a single controller aspects of these modules may be delegated to other computing devices.

MRT management module of controller communicates via network interface to direct ingress devices egress devices and transit nodes to create and install MRT forwarding state. In so doing MRT management module can provide the network devices with respective multi topology identifiers MT IDs to use when advertising labels e.g. by sending LDP label map messages for setting up LSPs along the set of maximally redundant trees. MRT management module may provide two sets of multi topology identifiers one for use with a first set of MRTs and another for use with a second updated set of MRTs that the network devices may compute after the topology change. MRT management module may store the MT IDs in MRT state .

Ingress devices egress devices and transit nodes can notify controller and MRT management module of controller may receive this notification via network interface . MRT management module may store an indication of which network devices have updated the state to MRT state .

Once MRT management module of controller has determined that confirmation has been received from each of the ingress devices egress devices and transit nodes of network from which it was expected to be MRT management module may delete the old MRT forwarding state from each of the network devices e.g. by accessing the routing information base e.g. RIB in the control plane of each of the network devices using network interface and modifying entries of the RIB to remove the old MRT forwarding state. In some examples MRT management module may access FIB and or LFIB in the data plane of the network devices to delete the old MRT state.

Also once MRT management module of controller has determined that confirmation has been received from each of the ingress devices egress devices and transit nodes of network from which it was expected to be received MRT management module instructs the ingress devices to begin forwarding traffic on the new updated MRTs using the new forwarding state that was confirmed to be installed on all of the network devices. MRT management module may re purpose the old MT IDs from the old MRTs for use with other future MRTs that may later be computed.

Network services applications represent one or more processes that provide services to clients of a service provider network that includes controller to manage connectivity in the path computation domain. Network services applications may provide for instance include Voice over IP VoIP Video on Demand VOD bulk transport walled open garden IP Mobility Subsystem IMS and other mobility services and Internet services to clients of the service provider network. Networks services applications may require services provided by path computation element such as node management session management and policy enforcement. Each of network services applications may include client interface by which one or more client applications request services. Client interface may represent a command line interface CLI or graphical user interface GUI for instance. Client may also or alternatively provide an application programming interface API such as a web service to client applications.

In some examples network services applications may issue path requests to path computation element to request paths in a path computation domain controlled by controller . Path computation element accepts path requests from network services applications to establish paths between the endpoints over the path computation domain. Path computation element reconciling path requests from network services applications to multiplex requested paths onto the path computation domain based on requested path parameters and anticipated network resource availability.

To intelligently compute and establish paths through the path computation domain path computation element includes topology module to receive topology information describing available resources of the path computation domain including ingress devices egress devices and transit nodes interfaces thereof and interconnecting communication links.

Path computation module of path computation element may compute requested paths through the path computation domain. In general paths are unidirectional. Upon computing paths path computation module may schedule the paths for provisioning by path provisioning module . A computed path includes path information usable by path provisioning module to establish the path in the network. Provisioning a path may require path validation prior to committing the path to provide for packet transport.

MRT management module of controller communicates via network interface to direct ingress devices egress devices and transit nodes to create and install MRT forwarding state . Ingress devices egress devices and transit nodes will receive the instructions from controller and will create and install the MRT forwarding state in response . For example each of ingress devices egress devices and transit nodes may each compute in a distributed fashion using a common algorithm a pair of MRTs based on the current network topology. The ingress devices egress devices and transit nodes will obtain the same pair of MRTs from the computation and will advertise labels to their LDP peers for reaching the destination root of the MRTs via MRT red and MRT blue and install in forwarding information the labels and next hops for sending traffic toward the destination via MRT red and MRT blue. In this manner LSPs are set up in network along the MRTs. For example LSP A may follow the red MRT and LSP B may follow the blue MRT.

One or more of ingress devices egress devices and transit nodes may detect a change to the topology of network e.g. a link or node failure or in case of multicast a change to a multicast receiver. Responsive to detecting the topology change a device local to the change may begin forwarding network traffic on the other color MRT in the first pair of MRTs. Once aware of the topology change ingress devices egress devices and transit nodes can re compute the pair of MRTs based on a modified network graph reflecting the topology change while traffic is still being received in the meantime by receiver device via the one MRT path that is still working of the first set of MRTs. Respective MRT modules of ingress devices egress devices and transit nodes can then create and install new forwarding state based on the updated re computed MRTs . Upon installing the new forwarding state based on the updated re computed MRTs each of ingress devices egress devices and transit nodes can individually notify controller that the updated MRT forwarding state has been installed . For example controller communication module of each of ingress devices egress devices and transit nodes can notify controller and MRT management module of controller may receive this notification via network interface . MRT management module may store an indication of which network devices have updated the state to MRT state .

Once MRT management module of controller has determined that confirmation has been received from each of the ingress devices egress devices and transit nodes of network from which it was expected to be received YES branch of MRT management module may delete the old MRT forwarding state from each of the network devices e.g. by accessing the routing information base e.g. RIB in the control plane of each of the network devices using network interface and modifying entries of the RIB to remove the old MRT forwarding state.

Also once MRT management module of controller has determined that confirmation has been received from each of the ingress devices egress devices and transit nodes of network from which it was expected to be received YES branch of MRT management module instructs the ingress devices to begin forwarding traffic on the new updated MRTs using the new forwarding state that was confirmed to be installed on all of the network devices . Ingress devices will begin forwarding traffic on the LSPs associated with the new MRTs . MRT management module and MRT module may re purpose the old MT IDs from the old MRTs for use with other future MRTs that may later be computed.

The techniques described in this disclosure may be implemented at least in part in hardware software firmware or any combination thereof. For example various aspects of the described techniques may be implemented within one or more processors including one or more microprocessors digital signal processors DSPs application specific integrated circuits ASICs field programmable gate arrays FPGAs or any other equivalent integrated or discrete logic circuitry as well as any combinations of such components. The term processor or processing circuitry may generally refer to any of the foregoing logic circuitry alone or in combination with other logic circuitry or any other equivalent circuitry. A control unit comprising hardware may also perform one or more of the techniques of this disclosure.

Such hardware software and firmware may be implemented within the same device or within separate devices to support the various operations and functions described in this disclosure. In addition any of the described units modules or components may be implemented together or separately as discrete but interoperable logic devices. Depiction of different features as modules or units is intended to highlight different functional aspects and does not necessarily imply that such modules or units must be realized by separate hardware or software components. Rather functionality associated with one or more modules or units may be performed by separate hardware or software components or integrated within common or separate hardware or software components.

The techniques described in this disclosure may also be embodied or encoded in a computer readable medium such as a computer readable storage medium containing instructions. Instructions embedded or encoded in a computer readable medium may cause a programmable processor or other processor to perform the method e.g. when the instructions are executed. Computer readable media may include non transitory computer readable storage media and transient communication media. Computer readable storage media which is tangible and non transitory may include random access memory RAM read only memory ROM programmable read only memory PROM erasable programmable read only memory EPROM electronically erasable programmable read only memory EEPROM flash memory a hard disk a CD ROM a floppy disk a cassette magnetic media optical media or other computer readable storage media. It should be understood that the term computer readable storage media refers to physical storage media and not signals carrier waves or other transient media.

Various aspects of this disclosure have been described. These and other aspects are within the scope of the following claims.

