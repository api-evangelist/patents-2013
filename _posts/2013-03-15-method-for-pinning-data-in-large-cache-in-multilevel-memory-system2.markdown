---

title: Method for pinning data in large cache in multi-level memory system
abstract: A method to request memory from a far memory cache and implement, at an operating system (OS) level, a fully associative cache on the requested memory. The method includes pinning the working set of a program into the requested memory (pin buffer) so that it is not evicted due to cache conflicts and is served from the fast cache and not the slower next level memory. The requested memory extends the physical address space and is visible to and managed by the OS. The OS has the ability to make the requested memory visible to the user programs. The OS has the ability to manage the requested memory from the far memory cache as both a fully associative cache and a set associative cache.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09645942&OS=09645942&RS=09645942
owner: Intel Corporation
number: 09645942
owner_city: Santa Clara
owner_country: US
publication_date: 20130315
---
Embodiments described herein generally relate to memory management and in particular a method for managing memory in a multi level memory system.

In a microprocessor employing a load store register architecture a significant percentage of the instructions in almost any code sequence are memory access instructions in which data must be read from or written to a main memory address indicated by the instructions. Because main memory accesses whether involving a read or a write are slow relative to instruction execution cache memories are utilized to reduce memory access latency. Often a cache memory architecture includes multiple tiers of cache memories where each tier is relatively larger but also relatively slower than the preceding tier.

In at least one embodiment a disclosed method to request memory from a far memory cache includes implementing at an operating system OS level a fully associative cache. In at least one embodiment performing a memory configuration operation includes allocating a first portion of a near memory to a pin buffer and a second portion of the near memory to a far memory cache. In at least one embodiment a system memory space is expanded to include the pin buffer in response to a memory configuration operation and an entry of a page table associated with a virtual address is modified to map the virtual address to the pin buffer. In at least one embodiment the memory configuration operation for a far memory cache may be implemented by special control registers in hardware or microcode.

In at least one embodiment a disclosed method to manage the pin buffer includes pinning data to the pin buffer and unpinning the data from the pin buffer. Pinning data to the pin buffer may in at least one embodiment include copying data associated with a virtual address that maps to a region of the system memory space associated with the far memory into the pin buffer by modifying an entry of a page table associated with the virtual address to map the virtual address to the pin buffer in response to a pin request indicative of the virtual address. In at least one embodiment after performing the pinning of the data subsequent references to the pinned page are served from the pin buffer. Unpinning data that was previously pinned to the pin buffer may be accomplished by saving the data stored in the pinner buffer to a temporary storage location and then restoring the page table mapping associated with the virtual memory address to map the virtual memory address back to the original system memory space region. The data may then be transferred from the temporary storage location to the system memory address region to complete the unpinning process.

In at least one embodiment a disclosed processor includes an execution core a near memory controller to communicate with a near memory and a far memory interface to communicate with a far memory controller coupled to a far memory. In at least one embodiment the processor includes resources e.g. configuration registers to signal the near memory controller to allocate a first portion of the near memory to a far memory cache and a second portion of the near memory to a pin buffer. In at least one embodiment the near memory configuration operation signals the near memory controller to expand the system memory space to include in addition to the far memory the pin buffer and to modify an entry of a page table associated with a virtual address to map the virtual address to the pin buffer.

In at least one embodiment a first portion of near memory is a set associative cache and allocating the pin buffer during a memory configuration operation includes allocating the pin buffer to a portion of the sets. In at least one embodiment the near memory is an n way set associative cache wherein allocating the pin buffer includes allocating for each way in sets that are allocated to the pin buffer. In at least one embodiment the near memory controller is operable to pin a portion of far memory to the pin buffer so that the applicable far memory address is mapped to the corresponding address in the pin buffer responsive to determining that the pin buffer portion is currently unused.

In at least one embodiment the near memory controller tracks memory configuration information including a pin buffer base physical address a pin buffer end physical address a near memory pin buffer base address and a near memory pin buffer end address. In at least one embodiment modifying a page table entry during a memory configuration operation includes determining a near memory address by determining an address offset from the virtual address and adding the offset address to the near memory pin buffer base address. Determining the address in near memory is performed in at least one embodiment by the near memory controller when accessing the near memory. In at least one embodiment a contiguous region of the physical address space is allocated to the pin buffer a contiguous address space of the near memory is allocated to the pin buffer and a byte to byte mapping of physical address to near memory address is employed. In some of the embodiments the pin buffer may be allocated in stripes or other non contiguous patterns of the physical address space the near memory or both.

In at least one embodiment the near memory includes a near memory tag array that is indexed during accesses to the first portion of the near memory and bypassed during accesses to the pin buffer. In at least one embodiment the tag array is bypassed during pin buffer access because the page table provides a direct translation mapping to the applicable near memory address. In these embodiment s the pin buffer functions as a fully associative cache that is forcibly maintained or pinned to the previously allocated pin buffer portion of the near memory.

In at least one embodiment far memory may be a byte addressable random access persistent storage technology such as a solid state drive having a capacity that exceeds the capacity of near memory by at least an order of magnitude and the near memory is a random access volatile storage technology e.g. a dynamic random access memory DRAM . In at least one other embodiment however the near memory is a persistent storage technology the far memory is a volatile storage technology or both.

In at least one embodiment a disclosed system includes a processor and a multiple tier main memory architecture that includes a far memory and a near memory. In at least one embodiment the far memory may have a far memory capacity a far memory latency and a far memory storage technology while the near memory may have a near memory capacity a near memory latency and a near memory storage technology where the far memory capacity exceeds the near memory capacity and the far memory latency exceeds the near memory latency.

In at least one embodiment the system includes a near memory controller to allocate a first portion of the near memory as a far memory cache and to allocate a second portion of the near memory as a pin buffer that supplements a main memory address space.

In at least one embodiment the processor includes a core cache and a capacity of the near memory exceeds a capacity of the core cache by at least an order of magnitude and a capacity of the far memory exceeds the capacity of the near memory by at least an order of magnitude. In at least one embodiment the far memory cache portion of the near memory is a set associative cache including a cache tag array while the pin buffer portion of the near memory can function as a tagless fully associative cache that is mapped directly to virtual addresses by a page table. In at least one embodiment the pin buffer can also function as a flat unarranged extension of the physical address space e.g. after memory configuration operation but before any pinning of data that can be managed by the operating system. In at least one embodiment the page table maps virtual addresses within a pin buffer portion of a physical address space to a pin buffer portion of the near memory space.

In the following description details are set forth in conjunction with embodiments to facilitate discussion of the disclosed subject matter. It should be apparent to a person of ordinary skill in the field however that the disclosed embodiments are exemplary and not exhaustive of all possible embodiments.

Throughout this disclosure a hyphenated form of a reference numeral refers to a specific instance of an element and the un hyphenated form of the reference numeral refers to the element generically or collectively. Thus widget refers to an instance of a widget class which may be referred to collectively as widgets and any one of which may be referred to generically as a widget .

System memory or main memory generally refers to physical and addressable memory that is recognized by an operating system. A cache memory generally has less capacity and a lower access time than system memory and is not generally thought of as being visible to the operating system. When an application program declares a variable the operating system generally does not allocate a specific portion of cache memory to the variable. Instead microprocessor hardware and microcode maintain copies of recently accessed main memory addresses and their neighboring addresses to take advantage of temporal locality the observation that a memory address that was recently accessed is likely to be accessed soon in the future and spatial locality the observation that a memory address in close proximity to a recently accessed memory address is also likely to be accessed soon in the future. A cache memory is beneficial to the extent that over a confined interval of time the percentage of memory accesses that are cached coupled with the performance benefit attributable to the lower latency of a cached memory access justifies the cost and complexity of maintaining the cache memory itself.

Memory architecture designs are influenced by the available storage technologies. DRAM technology has been used extensively for main memory because it is a relatively dense and inexpensive technology although relatively slow. Static random access memories SRAMs on the other hand have been used extensively for cache memories because of their relative speed despite being more expensive on a per bit basis than DRAMs. Moreover both DRAMs and SRAMs are volatile storage technologies in the sense that stored data persists only so long as power is applied to the storage cell. To implement systems that can power down from time to time a source of persistent storage is necessary. Historically magnetic storage in the form of hard disk drives has provided the bulk of persistent memory for microprocessor based computers.

Thus in many traditional systems program instructions and data structures associated with an operating system or application program are stored persistently in a magnetic storage and copied to a known page into DRAM main memory when the system is powered on and the operating system or application is executing. When specific individual instructions and data locations are accessed during execution they may be cached in one or more SRAM cache memories. More recently alternative storage architectures for allocating dynamic and persistent storage have devolved in conjunction with the development of relatively inexpensive and dense non volatile memory technologies including solid state drives. A memory architecture referred to herein as a two level memory 2LM architecture is described below.

In some embodiments processor includes a core region and an uncore region . In at least one embodiment core region includes an execution core . In some embodiments execution core includes a front end an execution pipeline and a core cache . In some embodiments front end may be responsible for pre fetching instructions perhaps in conjunction with an instruction pointer not depicted decoding instructions and pre fetching of data corresponding to instruction operands that are available. In at least one embodiment execution pipeline may be responsible for scheduling instructions for execution executing instructions in one or more execution ports not shown in and retiring the results of the instructions. For memory access instructions instruction execution may include accessing a data cache referred to in as core cache . While in some embodiments processor includes just one execution core processor may include multiple execution cores .

In at least one embodiment as suggested by its name uncore region of processor refers generally to elements of processor not directly associated with the execution core of processor . In some embodiments uncore region encompasses elements of processor that may interface execution core with system memory input output I O devices and so forth. In some embodiments uncore may communicate with chipset devices not depicted that provide interfaces for various types of I O devices. Although in some embodiments the functionality depicted may be illustrated as being implemented in uncore region of processor other embodiments may delegate analogous functionality to a chipset device. Similarly while in some embodiments uncore region may suggest that some functionality is located in a chipset device other embodiments may incorporate analogous functionality into uncore region .

In at least one embodiment uncore region includes resources to implement a 2LM architecture implementation of main memory . In some embodiments main memory includes a portion of near memory and a far memory . In some embodiments near memory is smaller in capacity than far memory but has less access latency than far memory from the perspective of processor . Moreover in some embodiments near memory is or includes volatile memory while far memory is or includes persistent memory. In some embodiments the volatile memory of near memory may include DRAM while the persistent memory of far memory may include a solid state drive a phase change memory device or another suitable persistent storage technology.

In some embodiments a ratio of the capacity of far memory to the capacity of near memory may vary based on the intended system use but may in some embodiments exceed a factor of 10 or more. Moreover because far memory may be comprised of persistent storage that is relatively inexpensive the total capacity of main memory in some embodiments can be cost effectively increased independent of the capacity of near memory .

In at least one embodiment processor includes a 2LM engine in uncore region that coordinates the two major components of main memory namely near memory and far memory . In some embodiments 2LM engine communicates with a near memory controller NMC that provides memory control functions for near memory . In at least one embodiment 2LM engine also communicates with a far memory controller FMC that provides memory control functions for far memory . In some embodiments 2LM engine beneficially maintains near memory as a combination of a conventional cache memory and a specially allocated portion referred to herein as a pin buffer that comprises a portion of the addressable memory space visible to the operating system. In these embodiments a majority of the system memory space visible to the operating system is represented by far memory while a small portion of the system memory includes a portion of near memory . In some embodiments a remaining portion of near memory functions as a far memory cache that is not visible to the operating system but stores copies of portions of far memory to improve performance and decrease memory access latency.

To support functionality for pinning data in near memory in some embodiments system supports an instruction memory configuration operation message or application programming interface API function issued by the operating system or an application program requesting that a portion of main memory be expanded by reserving space for pin buffer in a portion of near memory . In at least one embodiment processor may include functionality to manage pin buffer as a fully associative cache memory. Some embodiments of near memory processor NMC and or 2LM engine implement resources that may be used to partition near memory into two parts each part behaving in a different manner. In some embodiments near memory is initialized as a set associative cache memory for far memory . In some embodiments after a pin buffer request is received near memory may be partitioned into a cache memory portion which continues to behave substantially as it did previously but with fewer cache line entries available for caching data. In at least one embodiment the other portion of near memory may behave as a flat memory that extends the physical address space apparent to the operating system and potentially to application programs as well. In some embodiments pin buffer is implemented via a software level manager that manages requested space as a fully associative cache. In at least one embodiment the near memory management may also extend a paging subsystem of the operating system to map virtual addresses directly to the pin buffer portion of the near memory i.e. map virtual addresses directly to addresses within near memory . In some embodiments 2LM engine may implement the use of a tag table not depicted to arbitrate which data segments will be included in near memory and which data segment will be fetched from far memory . It is therefore understood that because near memory acts as a cache of far memory the 2LM engine may in some embodiments execute data pre fetching or similar cache processes known in the art.

In at least one embodiment far memory cache includes a plurality of cache line entries . In some embodiments cache line entries include status information and data . In some embodiments status information includes a replaced R indicator a validity V indicator and a pinned P indicator . In at least one embodiment validity indicator may indicate whether the corresponding cache line data is valid i.e. coherent with main memory . Pinned indicator may in some embodiments indicate if the corresponding line is pinned in a specifically allocated portion of system while a replaced indicator indicates whether the line may be eligible to be replaced. In addition in at least one embodiment status information may include additional or information other than is illustrated.

In at least one embodiment near memory is divided into two contiguous parts namely a far memory cache functionally equivalent to the far memory cache illustrated in although of smaller size and a pin buffer . In some embodiments pin buffer represents a portion of near memory that has been allocated. In some embodiments far memory cache is transparent to the OS or the user level applications and is managed by the hardware. In at least one embodiment pin buffer is visible to the OS and represents a flat extension of the main memory address space.

In at least one embodiment the OS may pin virtual address V in pin buffer by copying an entire page or some other segment of memory space from far memory to a segment of near memory that is within pin buffer and encompasses address A . In some embodiments after the mapping to main memory address M is removed address M is freed up to store new data. In some embodiments after pinning an entire page to pin buffer is performed all subsequent references to the pinned page are served directly from pin buffer . In at least one embodiment the caching and page table mapping of address M from the main memory from the split far memory cache remains unchanged. These operations can be implemented entirely in software as part of the OS in microcode or in hardware performed by the processor

In order to unpin data from pin buffer the OS may in some embodiments copy the applicable page from pin buffer to far memory and update the corresponding virtual to physical page table entries so that they represent the actual address mappings.

In some embodiments it is to be understood that the addressing of the data array in near memory is such that the reserved cache blocks are all consecutive. Therefore in at least one embodiment near memory is configured with the start address of this consecutive region NPB BASE . During normal operation an OS assigned physical address obtained from conventional page tables may be used in all the upper level caches. In some embodiments the near memory controller on the other hand first checks if an incoming memory operation s address referred to as physical address PA falls within the pin buffer address space i.e. an address between PPB BASE and PPB END . If so in some embodiments the address to the near memory data array is formed by adding the PA s offset relative to the start of the pin buffer address space. In at least one embodiment pin buffer accesses do not incur tag lookup and are directly sent to the near memory data array. In some embodiments while the addressing is consecutive it is noteworthy that it is not necessary to have the reserved blocks laid out consecutively in the near memory data array. In some embodiments the 2LM engine can determine the near memory location from the physical address assuming a simple functional one to one mapping is utilized.

In at least one embodiment method may begin in operation when an operating system issues operation a pin data request identifying a specific address or page of memory. In some embodiments a determination is made operation whether the address indicated in the pin request is already pinned in the pin buffer. In at least one embodiment if the address has already been pinned to the pin buffer then no operation is performed and the program execution operation continues. In some embodiments if a determination is made in operation that the address indicated in the pin request is not pinned in the pin buffer then in operation the pin buffer is checked to see if it is full. In some embodiments if it is determined that the pin buffer is not full one entry from the queue of free pin buffer entries is removed in operation . In at least one embodiment the entry is then copied from far memory into the pin buffer operation by the OS or a microcode and the corresponding virtual to physical address mappings are updated operation to reflect that the virtual address points to the new physical address where the data is copied and the old location is freed. After this change all subsequent references to the pinned page are in some embodiments served directly from the pin buffer as if the data is cached in the far memory cache. In some embodiments once the corresponding virtual to physical address mappings are updated in operation the program execution continues operation .

In at least one embodiment if it is determined that the pin buffer is full in operation then a determination needs to be made whether a pinned page can be implicitly replaced based on a replacement policy operation . In some embodiments if a pinned page cannot be replaced based on a replacement policy acknowledgement is made that the data cannot be pinned operation and program execution continues in operation .

In some embodiments if it is determined that the pinned page can be replaced based on a replacement policy in operation the replacement policy is used to decide which pages to unpin operation . In at least one embodiment in operation the data is unpinned and method resumes to request to pin data in operation .

In some embodiments because data is explicitly pinned in the pin buffer when requested it is necessary for the OS to keep track of the available space in the pin buffer and implement a replacement policy. In at least one embodiment a replacement policy utilized may be the implementation of a least recently used replacement where the queue orders the entries from the most recently used to the least recently used page. In some embodiments the OS maintains an additional data structure such as a queue FIFO which contains the offsets of the empty slots in the pin buffer. In at least one embodiments if the queue is empty i.e. the pin buffer is full the OS may either ignore the pin request or automatically replace already pinned data with the new data.

In some embodiments when a program which contains pinned data terminates without unpinning its data the pinned data is automatically unpinned by the OS by releasing the virtual to physical address mappings in the translation page table for the corresponding process which represents the program execution.

In at least one embodiment if it is determined that the data is already pinned in operation the data from the pin buffer is unpinned and copied operation to main memory. In some embodiments the translation table is remapped operation and an entry is then inserted into the queue of empty slots in the queue in operation . In at least one embodiment once unpinning the data is accomplished program execution continues operation .

Embodiments may be implemented in many different system types and platforms. illustrates elements of an embodiment of a processor system used in conjunction with at least one embodiment. In one embodiment a processor memory and input output devices are interconnected by a number of point to point P P interfaces as will be described in further detail. However in other embodiments the processor system may employ different bus architectures such as a front side bus a multi drop bus and or another implementation. Although a processor is illustrated in for descriptive clarity in various embodiments a different number of processors may be employed using elements of the illustrated architecture.

In at least one embodiment processor system is a point to point interconnect system and includes processor . In some embodiments processor is a multi core processor including first core and second core . It is noted that other elements of processor besides cores may be referred to as an uncore. In different embodiments a varying number of cores may be present in a particular processor. In at least one embodiment cores may comprise a number of sub elements not shown in also referred to as clusters that provide different aspects of overall functionality. In some embodiments cores may each include a memory cluster not shown in that may comprise one or more levels of cache memory. In some embodiments other clusters not shown in in cores may include a front end cluster and an execution cluster.

In particular embodiments first core and second core within processor are not equipped with direct means of communicating with each other but rather communicate via crossbar which may include intelligent functionality such as cache control data queuing P P protocols and multi core interfacing. In some embodiments crossbar may thus represent an intelligent uncore controller that interconnects cores with memory controller hub MCH last level cache memory LLC and P P interface among other elements. In particular to improve performance in such an architecture cache controller functionality within crossbar may in some embodiments enable selective caching of data within a cache hierarchy including LLC and one or more caches present in cores . In at least one embodiment crossbar includes memory management unit MMU that handles access to virtual memory addresses and maintains translation look aside buffers not shown in for improved performance with regard to memory access. In some embodiments crossbar is referred to as a global queue.

In at least one embodiment LLC may be coupled to a pair of processor cores respectively. In some embodiments LLC may be shared by core and core . LLC may in some embodiments be fully shared such that any single one of cores may fill or access the full storage capacity of LLC . Additionally in at least one embodiment MCH may provide for direct access by processor to memory via memory interface . In at least one embodiment memory may be a double data rate DDR type DRAM while memory interface and MCH comply with a DDR interface specification. In some embodiments memory may represent a bank of memory interfaces or slots that may be populated with corresponding memory circuits for a desired DRAM capacity.

In at least one embodiment processor may also communicate with other elements of processor system such as near hub and far hub which are also collectively referred to as a chipset that supports processor . In some embodiments P P interface may be used by processor to communicate with near hub via interconnect link . In certain embodiments P P interfaces and interconnect link are implemented using Intel QuickPath Interconnect architecture.

In at least one embodiment near hub includes interface to couple near hub with first bus which may support high performance I O with corresponding bus devices such as graphics and or other bus devices. In particular embodiments graphics may represent a high performance graphics engine that outputs to a display device not shown in . In one embodiment first bus is a Peripheral Component Interconnect PCI bus such as a PCI Express PCIe bus and or another computer expansion bus. In some embodiments near hub may also be coupled to far hub at interface via interconnect link . In certain embodiments interface is referred to as a south bridge. In some embodiments far hub may provide I O interconnections for various computer system peripheral devices and interfaces and may provide backward compatibility with legacy computer system peripheral devices and interfaces. Thus in some embodiments far hub provides network interface and audio I O as well as provides interfaces to second bus third bus and fourth bus as will be described in further detail.

In at least one embodiment second bus may support expanded functionality for processor system with I O devices and may be a PCI type computer bus. In some embodiments third bus may be a peripheral bus for end user consumer devices represented by desktop devices and communication devices which may include various types of keyboards computer mice communication devices data storage devices bus expansion devices etc. In certain embodiments third bus represents a Universal Serial Bus USB or similar peripheral interconnect bus. In some embodiments fourth bus may represent a computer interface bus for connecting mass storage devices such as hard disk drives optical drives and disk arrays which are generically represented by persistent storage shown including OS that may be executable by processor .

In some embodiments system emphasizes a computer system that incorporates various features that facilitate handheld or tablet type of operation and other features that facilitate laptop or desktop operation. In addition in particular embodiments system includes features that cooperate to aggressively conserve power while simultaneously reducing latency associated with traditional power conservation states.

In at least one embodiment sensor API provides application program access to one or more sensors not depicted that may be included in system . Sensors that system might have in some embodiments include an accelerometer a global positioning system GPS device a gyro meter an inclinometer and a light sensor. In some embodiments resume module may be implemented as software that when executed performs operations for reducing latency when transitioning system from a power conservation state to an operating state. In some embodiments resume module may work in conjunction with solid state drive SSD to reduce the amount of SSD storage required when system enters a power conservation mode. Resume module may in some embodiments flush standby and temporary memory pages before transitioning to a sleep mode. In some embodiments by reducing the amount of system memory space that system is required to preserve upon entering a low power state resume module beneficially reduces the amount of time required to perform the transition from the low power state to an operating state. In at least one embodiment connect module may include software instructions that when executed perform complementary functions for conserving power while reducing the amount of latency or delay associated with traditional wake up sequences. In some embodiments connect module may periodically update certain dynamic applications including email and social network applications so that when system wakes from a low power mode the applications that are often most likely to require refreshing are up to date. In at least one embodiment touchscreen user interface supports a touchscreen controller that enables user input via touchscreens traditionally reserved for handheld applications. In some embodiments the inclusion of touchscreen support in conjunction with support for communication devices enables system to provide features traditionally found in dedicated tablet devices as well as features found in dedicated laptop and desktop type systems.

Additionally a circuit level model with logic and or transistor gates may be produced at some stages of the design process. This model may be similarly simulated sometimes by dedicated hardware simulators that form the model using programmable logic. This type of simulation taken a degree further may be an emulation technique. In any case re configurable hardware is another embodiment that may involve a tangible machine readable medium storing a model employing the disclosed techniques.

Furthermore most designs at some stage reach a level of data representing the physical placement of various devices in the hardware model. In the case where conventional semiconductor fabrication techniques are used the data representing the hardware model may be the data specifying the presence or absence of various features on different mask layers for masks used to produce the integrated circuit. Again this data representing the integrated circuit embodies the techniques disclosed in that the circuitry or logic in the data can be simulated or fabricated to perform these techniques.

In any representation of the design the data may be stored in any form of a tangible machine readable medium. In some embodiments an optical or electrical wave modulated or otherwise generated to transmit such information a memory or a magnetic or optical storage such as a disc may be the tangible machine readable medium. Any of these mediums may carry the design information. The term carry e.g. a tangible machine readable medium carrying information thus covers information stored on a storage device or information encoded or modulated into or on to a carrier wave. The set of bits describing the design or the particular part of the design are when embodied in a machine readable medium such as a carrier or storage medium an article that may be sold in and of itself or used by others for further design or fabrication.

Embodiment 1 is a method for requesting memory comprising i performing a memory configuration operation including allocating a first system portion of a near memory to a pin buffer allocating a second cache portion of the near memory to a cache for a far memory and expanding a system memory space to include the pin buffer and ii managing the pin buffer wherein the managing includes pinning data associated with a virtual address that maps to a portion of the system memory space associated with the far memory into the pin buffer by modifying an entry of a page table associated with the virtual address to map the virtual address to the pin buffer in response to a pin request indicating the virtual address and unpinning data associated with the pin request from the pin buffer by modifying the entry to map the virtual address to a system memory address.

In embodiment 2 the second portion of near memory included in the subject matter of embodiment 1 can optionally include a set associative cache and the managing of the pin buffer included in the subject matter of embodiment 1 can optionally include allocating the pin buffer to a portion of sets.

In embodiment 3 the near memory included in the subject matter of embodiment 1 can optionally include an n way set associated cache and the managing of the pin buffer included in the subject matter of embodiment 1 can optionally include allocating all n ways of each set that is allocated in the pin buffer to a first way of the near memory.

In embodiment 4 the modifying of the entry of the page table included in the subject matter of embodiment 1 can optionally include determining a near memory address by adding a physical address offset associated with the virtual address to a near memory pin buffer base address.

In embodiment 5 the near memory included in the subject matter of embodiment 1 can optionally include a near memory tag array the near memory tag array included in the subject matter of embodiment 1 is optionally indexed during accesses to the second portion of the near memory and the near memory tag array included in the subject matter of embodiment 1 is optionally bypassed during accesses to the pin buffer.

In embodiment 6 the subject matter of embodiment 1 can optionally include a near memory controller wherein the near memory controller pins a portion of the pin buffer to cache a far memory address responsive to determining that the pin buffer portion is currently unused.

In embodiment 7 byte addressable far memory included in the subject matter of embodiment 1 can optionally include persistent storage.

In embodiment 8 byte addressable far memory included in the subject matter of embodiment 7 can optionally include random access storage.

In embodiment 9 a memory configuration operation included in the subject matter of embodiment 1 can optionally be implemented by resources including special control registers and microcode.

Embodiment 10 is a computer system comprising a near memory controller to communicate with a near memory a far memory interface to communicate with a far memory controller coupled to a far memory and an execution unit including memory configuration operation resources to signal the near memory controller to allocate a pin buffer wherein the allocating of the pin buffer includes allocating a first portion of the near memory to a pin buffer and a second portion of the near memory to a cache for a far memory expand a system memory space to include the pin buffer responsive to memory configuration operation and modify an entry of a page table associated with a virtual address to map the virtual address to the pin buffer responsive to a pin request.

In embodiment 11 the second portion of near memory included in the subject matter of embodiment 10 can optionally include a set associative cache and the allocating of the pin buffer included in the subject matter of embodiment 10 can optionally include allocating the pin buffer to a portion of sets.

In embodiment 12 the near memory included in the subject matter of embodiment 10 can optionally include an n way set associative cache and the allocating of the pin buffer included in the subject matter of embodiment 10 can optionally include allocating all n ways of each set that is allocated in the pin buffer to a first way of the near memory.

In embodiment 13 modifying the entry of the page table included in the subject matter of embodiment 10 can optionally include determining a near memory address by adding a physical address offset associated with the virtual address to a near memory pin buffer base address.

In embodiment 14 the near memory included in the subject matter of embodiment 10 can optionally include a near memory tag array the near memory tag array included in the subject matter of embodiment 10 is optionally indexed during accesses to the second portion of the near memory and the near memory tag array included in the subject matter of embodiment 10 is optionally bypassed during accesses to the pin buffer.

In embodiment 15 the near memory controller included in the subject matter of embodiment 10 can optionally pin a portion of the pin buffer to cache a far memory address responsive to determining that the pin buffer portion is currently unused.

In embodiment 16 byte addressable far memory included in the subject matter of embodiment 10 can optionally include persistent storage.

In embodiment 17 byte addressable far memory included in the subject matter of embodiment 16 can optionally include random access storage.

Embodiment 18 is a system comprising i first storage to store an operating system ii a processor iii a multiple tier main memory architecture including a far memory having a far memory capacity a far memory latency and a far memory storage technology and a near memory having a near memory capacity a near memory latency and a near memory storage technology and iv a near memory controller to allocate a first portion of the near memory as a far memory cache and allocate responsive to execution by the processor of a memory configuration operation a second portion of the near memory as a pin buffer occupying a portion of a main memory address space.

In embodiment 19 the far memory storage technology included in the subject matter of embodiment 18 can optionally include a byte addressable persistent storage technology.

In embodiment 20 the far memory storage technology included in the subject matter of embodiment 19 can optionally include a byte addressable randomly accessible storage technology.

In embodiment 21 the near memory storage technology included in the subject matter of embodiment 20 can optionally include a volatile random access storage technology.

In embodiment 22 the processor included in the subject matter of embodiment 18 can optionally include a core cache and wherein a capacity of the near memory exceeds a capacity of the core cache by at least an order of magnitude and wherein a capacity of the far memory exceeds the capacity of the near memory by at least an order of magnitude.

In embodiment 23 the far memory cache included in the subject matter of embodiment 18 can optionally include a set associative cache including a cache tag array and wherein the pin buffer functions as a tagless fully associative cache.

In embodiment 24 the subject matter of embodiment 23 can optionally include a page table mapping virtual addresses to physical addresses wherein the page table maps virtual addresses within a pin buffer portion of a physical address space to a pin buffer portion of the near memory.

In embodiment 25 the second portion of the near memory included in the subject matter of embodiment 18 can optionally include a set associative cache and the allocating of the pin buffer included in the subject matter of embodiment 18 can optionally include allocating the pin buffer to a portion of sets.

In embodiment 26 the near memory controller included in the subject matter of embodiment 25 is optionally operable to pin a reserved way of the pin buffer for the far memory cache responsive to determining that the reserved way is currently unused by the pin buffer.

In embodiment 27 a memory configuration operation included in the subject matter of any one of embodiments 1 7 can optionally be implemented by resources including special control registers and microcode.

In embodiment 28 byte addressable far memory included in the subject matter of any one of embodiments 10 16 can optionally include random access storage.

In embodiment 29 the far memory storage technology included in the subject matter of any one of embodiments 18 or 19 can optionally include a byte addressable randomly accessible storage technology.

In embodiment 30 the processor included in the subject matter of any one of embodiments 18 or 19 can optionally include a core cache and wherein a capacity of the near memory exceeds a capacity of the core cache by at least an order of magnitude and wherein a capacity of the far memory exceeds the capacity of the near memory by at least an order of magnitude.

To the maximum extent allowed by law the scope of the present disclosure is to be determined by the broadest permissible interpretation of the following claims and their equivalents and shall not be restricted or limited to the specific embodiments described in the foregoing detailed description.

