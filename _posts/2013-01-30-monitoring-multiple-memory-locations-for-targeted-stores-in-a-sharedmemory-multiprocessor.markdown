---

title: Monitoring multiple memory locations for targeted stores in a shared-memory multiprocessor
abstract: A system and method for supporting targeted stores in a shared-memory multiprocessor. A targeted store enables a first processor to push a cache line to be stored in a cache memory of a second processor. This eliminates the need for multiple cache-coherence operations to transfer the cache line from the first processor to the second processor. More specifically, the disclosed embodiments provide a system that notifies a waiting thread when a targeted store is directed to monitored memory locations. During operation, the system receives a targeted store which is directed to a specific cache in a shared-memory multiprocessor system. In response, the system examines a destination address for the targeted store to determine whether the targeted store is directed to a monitored memory location which is being monitored for a thread associated with the specific cache. If so, the system informs the thread about the targeted store.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08990503&OS=08990503&RS=08990503
owner: Oracle International Corporation
number: 08990503
owner_city: Redwood Shores
owner_country: US
publication_date: 20130130
---
The disclosed embodiments relate to shared memory multiprocessor systems. More specifically the disclosed embodiments relate to a shared memory multiprocessor system that monitors multiple memory locations on behalf of a thread and then notifies the thread when a targeted store is directed to one of the monitored memory locations.

Shared memory multiprocessor systems are continuing to grow in size with increases both in the number of cores per chip and the number of chips in a system. Moreover there are differences in the number and size of caches how they are shared or not and latencies among various levels of cache within and between chips and to local and remote memory. Despite these differences as systems grow the latency of accessing remote elements e.g. cache or memory inherently grows relative to the latency of accessing local elements. That is systems are increasingly NUMA Non Uniform Memory Access and the NUMA constants ratios of latencies to access remote and local elements are growing.

Significant challenges for programmers accompany these changes. Software that has performed acceptably on smaller systems can suffer severe performance degradation when scaled to larger systems especially due to NUMA effects.

Consider for example a hypothetical application running on a single socket multi core system. Suppose the working set of the application is such that it fits comfortably in an on chip cache say L2 so that it exhibits good cache locality and performs well. In particular when one thread accesses a memory location that has recently been modified by another thread the location is likely to be in the on chip L2 cache in which case the access hits in the cache and no off chip communication is required to satisfy the memory request. Otherwise the location is stored in a memory that is physically close to the single processor chip.

Consider now a larger system with multiple processor sockets. Memory that is located physically close to one processor is necessarily further from others. Similarly the caches of other processors are physically further away than a processor s own caches. Broadly systems meeting this description are referred to as NUMA Non Uniform Memory Access . If the same application is configured now to run on such a system even though its working set may still fit comfortably in cache now threads are running on different chips and therefore inter chip communication is required to keep the caches on the multiple chips coherent. In this case when one thread accesses a memory location that has recently been modified by another it is likely that the other thread is on a different chip. In this case if the location is still in a cache near the thread that recently modified it then it needs to be invalidated or downgraded in that cache and brought into the cache of the thread performing the subsequent access. Alternatively the location may no longer be cached it may be stored at its home memory node which is likely to be memory other than the memory located physically close to the thread performing the subsequent access.

The first problem in this scenario is obvious the latency to access a memory location can increase significantly as system sizes grow. Perhaps less obviously the bandwidth available for coherence and data communication is not growing at the same rate that the number of cores in systems is growing. Therefore the problem may be further exacerbated when the coherence and memory traffic produced by an application or set of applications approach the bandwidth limitations of the system.

Therefore techniques for reducing the amount of remote communication required by applications are needed as well as techniques for reducing the cost in terms of latency bandwidth or both.

The present embodiments provide a system that supports targeted stores in a shared memory multiprocessor. A targeted store enables a first processor to push a cache line to be stored in a cache memory of a second processor in the shared memory multiprocessor. This eliminates the need for multiple cache coherence operations to transfer the cache line from the first processor to the second processor.

In a multi processor system that supports targeted stores a thread which is waiting for a targeted store may have to continually poll one or more memory locations to determine when the targeted store arrives. This keeps the associated cache lines in the thread s local cache and in doing so keeps the cache lines away from the threads that want to store to them. The thread can also consume a significant amount of power while performing these polling operations.

To alleviate these problems the disclosed embodiments provide a system that monitors specific memory locations for targeted stores and then notifies waiting threads when a targeted store is directed to one of the specific memory locations. This eliminates the need for the waiting threads to continually poll the specific memory locations. During operation the system receives a targeted store which is directed to a specific cache in a shared memory multiprocessor system. In response the system examines a destination address for the targeted store to determine whether the targeted store is directed to a monitored memory location which is being monitored on behalf of a thread associated with the specific cache. If so the system informs the thread about the targeted store.

In some embodiments the monitored memory location is one of a set of multiple memory locations being monitored for the thread. For example if the system is monitoring a range of addresses all addresses falling within this monitored range can be considered monitored addresses. 

In some embodiments the specific cache comprises a single cache memory or a set of proximate cache memories in the shared memory multiprocessor.

In some embodiments upon receiving a request to monitor a set of monitored addresses for a thread the system configures a monitoring circuit associated with the specific cache to look for a targeted store having a destination address that matches a monitored address for the thread.

In some embodiments the system uses a monitoring circuit which is proximate to but separate from the specific cache to determine whether the destination address for the targeted store is directed to a monitored memory location.

In some embodiments the system uses a monitoring circuit which is incorporated into the specific cache to determine whether the destination address for the targeted store is directed to a monitored memory location.

In some embodiments informing the thread about the targeted store includes communicating the destination address for the targeted store to the thread.

In some embodiments informing the thread about the targeted store includes communicating a bitmap to the thread wherein the bitmap provides summary information indicating sets of addresses in a set of multiple memory locations which are being monitored for the thread that may have received targeted stores.

In some embodiments informing the thread about the targeted store includes informing the thread that an unspecified location in a set of multiple memory locations which are being monitored for the thread has received a targeted store.

The following description is presented to enable any person skilled in the art to make and use the present embodiments and is provided in the context of a particular application and its requirements. Various modifications to the disclosed embodiments will be readily apparent to those skilled in the art and the general principles defined herein may be applied to other embodiments and applications without departing from the spirit and scope of the present embodiments. Thus the present embodiments are not limited to the embodiments shown but are to be accorded the widest scope consistent with the principles and features disclosed herein.

The data structures and code described in this detailed description are typically stored on a computer readable storage medium which may be any device or medium that can store code and or data for use by a computer system. The computer readable storage medium includes but is not limited to volatile memory non volatile memory magnetic and optical storage devices such as disk drives magnetic tape CDs compact discs DVDs digital versatile discs or digital video discs or other media capable of storing computer readable media now known or later developed.

The methods and processes described in the detailed description section can be embodied as code and or data which can be stored in a computer readable storage medium as described above. When a computer system reads and executes the code and or data stored on the computer readable storage medium the computer system performs the methods and processes embodied as data structures and code and stored within the computer readable storage medium. Furthermore the methods and processes described below can be included in hardware modules. For example the hardware modules can include but are not limited to application specific integrated circuit ASIC chips field programmable gate arrays FPGAs and other programmable logic devices now known or later developed. When the hardware modules are activated the hardware modules perform the methods and processes included within the hardware modules.

The disclosed embodiments relate to a system that notifies a waiting thread when a targeted store is directed to a monitored memory location. Before we describe this monitoring mechanism we first describe a shared memory multiprocessor system that performs targeted stores.

Note that each of L1 caches can be implemented using a combined instruction and data cache. Alternatively each of L1 caches can be implemented using a separate L1 instruction cache and a separate L1 data cache.

Moreover all of the illustrated caches including L1 caches L2 caches and and L3 cache collectively form a cache coherent memory system which operates in accordance with a specific cache coherence policy such as the MOESI cache coherence policy.

Also note that semiconductor chip maintains state information which includes thread specific state information that can be used to determine which processor cores and associated caches are accessed by a specific thread. Similarly semiconductor chip maintains state information which can be used to determine which processor cores and associated caches are accessed by a specific thread that executes within semiconductor chip . Note that this state information and can be used to determine where a specific thread is located as is described below with reference to .

As illustrated in system software controls the execution of a number of threads on processor cores . For example system software can allocate thread to execute on processor core while thread executes on processor core and thread executes on processor core . Note that system software can include lower level system software such as a hypervisor and can also include higher level system software such as an operating system. System software also contains state information wherein state information includes thread specific state information which can be used to determine which processor cores and associated caches are accessed by a specific thread.

Interface can alternatively be a system call interface which is exposed by the operating system to provide system calls to perform the thread location and targeted store operations. In this case thread location mechanism and targeted store mechanism are implemented as system calls. These system calls can access concurrent data structures make further system calls to operating system or can execute instructions to execute on and or access hardware .

Interface can also be implemented using hardware that executes instructions defined within an instruction set architecture. In this case the thread location mechanism may be implemented through a special purpose thread location instruction and the targeted store mechanism may be implemented through a special purpose targeted store instruction.

During system operation when a thread executes thread location instruction thread location instruction returns a location which can be used to identify a cache that the thread is likely to be accessing within the multiprocessor system. Next location can be inserted into the target field of targeted store instruction which performs a targeted store directed to the specified target cache.

This location can be determined in a number of ways. In some embodiments the thread location instruction makes a system call to system software and system software examines thread specific state information to identify a processor core on which the thread is executing. By identifying a specific processor core the system implicitly identifies a specific L1 or L2 cache that the processor core is likely to access. However a system call typically takes a long time to execute. Hence to improve performance in other embodiments the thread location instruction directly accesses state information contained in semiconductor chip that the thread is executing on and state information is used to determine which cache the thread is likely to access.

The following discussion relates to the ideas disclosed in U.S. patent application Ser. No. 13 625 700 the 700 application entitled Supporting Targeted Stores in a Shared Memory Multiprocessor System by the same inventors as the instant application filed 24 Sep. 2012 which is hereby incorporated by reference. The instant application builds on the ideas disclosed in the 700 application by describing mechanisms that threads can use to wait for targeted stores. We can characterize these ideas as multi location MONITOR MWAIT because they are somewhat similar in spirit to the MONITOR MWAIT instructions already implemented in some processor architectures such as the Intel x86 processor architecture. However the problem we intend to solve requires significantly different implementation approaches.

Briefly existing MONITOR MWAIT implementations are able to await modifications to addresses within a single cache line and they achieve this by loading this line in to the local cache and then monitoring the cache to detect changes to locations in that cache line. When another thread modifies a location in this cache line it requests exclusive ownership of the cache line thus allowing the cache to detect the modification and inform the waiting thread that it should load the location. Because the waiting thread does not need to actively monitor the location for example in a spin loop it can be put into a state in which it consumes less energy and does not compete with other threads for resources such as functional units and cache bandwidth.

Existing MONITOR MWAIT implementations do not fulfill the purposes of our system that supports targeted stores for two primary reasons. First they are limited to monitoring locations in only a single cache line. In contrast we want to be able to monitor multiple locations not limited to fit in a single cache line. Although numerous variations are possible for concreteness in this description we assume threads can monitor locations in a single address range defined by a base address and size. Because existing implementations require the locations being waited upon to be held in cache and that the cache be monitored for changes extending these implementations to support monitoring of larger address ranges is likely to be impractical.

More importantly however waiting for updates to a location by holding and monitoring its cache line is contradictory with the goals of targeted stores. The primary motivation for targeted stores is to reduce or eliminate unnecessary coherence communication which increases both latency and consumption of coherence bandwidth involved when one thread S is waiting for stores to a memory location and another thread C stores to it. In typical existing systems C will have to send a coherence message to S in order to request ownership of the cache line then S must acknowledge the request before C can even perform its store. For S to then load the stored value similar round trip communication is required again. Targeted stores aim to overcome this problem by allowing C to target its store toward S say to a cache near the core where S is executing so that S does not need to load the location before it is stored setting up the above described sequence of excessive communication.

The 700 application describes a number of possible approaches for implementing targeted stores. At first glance it appears that we may have to design multi location MONITOR MWAIT implementations especially for each targeted store implementation. However it turns out that this is not necessary as the implementations of the two mechanisms can be largely independent. The key observation that led us to this realization was that in all targeted store implementations a store that is targeted to a given cache will always send a message to that cache identifying the address to be stored. This is sufficient to enable notification of threads that may be waiting for the location to be modified via a targeted store the particular details of whether the cache is able to accept the targeted store what state it uses for the cache line underlying the targeted location what it does in case it is unable to accept the store etc. are not important.

As discussed in detail in the 700 application in some targeted store implementations it may be difficult for a cache to accept a cache line sent via a targeted store because the receiving cache has not previously allocated a way for it. Note that it is not only acceptable but also desirable to notify a thread waiting for a targeted store when a message arrives with that targeted store even if the receiving cache is unable to accept and store the line with the stored value. The reason is that in this case the thread should load the location in order to retrieve the value that could not be accepted. Note that when a load is initiated by the receiver a way can be allocated in the cache in advance to hold the cache line when it is received in response to the load in contrast to messages resulting from targeted stores which may arrive at any moment making it difficult in some cases for the receiving cache to accept the line. 

In the following description we describe possible implementation approaches for multi location MONITOR MWAIT assuming only that a targeted store results in a message to the target cache identifying the address being stored to.

In some embodiments a thread may wait for any targeted store eliminating the need to specify the address es for which it is waiting. Such embodiments may be appropriate for configurations in which targeted stores are targeted to specific hardware threads or to non shared caches. In general however in embodiments in which stores are targeted to shared caches threads on all cores that are waiting when a targeted store is received would need to be notified resulting in inefficiency and duplication.

Therefore in general it will be desirable for a core to be able to request the range of addresses it wishes to monitor. In some embodiments this would be achieved via a special instruction that specifies a base address and a number of words to monitor starting at that address. The instruction would inform the target cache that the core requests to monitor targeted stores to the specified address range. Thus when a cache receives a targeted store it can detect whether a core is monitoring a range including the targeted address informing the core of the targeted store if so.

In addition to being informed that a location it is monitoring has received a targeted store it would be useful for a thread to be able to receive information about which address has been stored to in order to reduce the latency involved in loading the stored value. For example we may want to implement a concurrent data structure that uses a service thread to perform operations that are requested by client threads via targeted stores. In general these service threads may monitor a number of locations for requests. Once a service thread has been informed of a request being made we would like it to be able to receive the request by loading from the location to which the targeted store was performed as soon as possible.

For this reason it may be desirable for a cache receiving a targeted store to provide more information to a thread monitoring an address range including the targeted location than the mere fact that some address in this range has received a targeted store. Ideally the cache would inform the thread of each location that has been stored to. However storing such information for long enough to be useful to the service thread may be problematic for at least two reasons.

First it may be undesirable to devote enough resources to be able to store all of the addresses that have received targeted stores. Furthermore there is no way to bound how many targeted stores may be received during the time a receiving thread is processing requests so even if it is acceptable to devote considerable resources to this purpose in general we must still deal with the possibility that these resources are insufficient to record enough addresses to allow the receiving thread to process requests before the resources used to record their addresses must be reclaimed.

An intermediate possibility is for the cache to maintain a summary of addresses for which targeted stores have been received which would help the receiving thread to quickly determine which address ranges or sub ranges it should scan for requests. As a simple example this summary information could be in the form of a fixed size bitmap. For example the summary might be 64 bits with the ith bit being true if a targeted store request has been received for an address in the range base i size 64 base i 1 size 64 since the last time the summary was reset. This allows a receiving thread to quickly acquire a summary of which sub ranges it should scan to find requests that have been stored into the range it is monitoring

Moreover these techniques may be used in combination. For example resources may be allocated to store a fixed number of addresses. If the receiving thread is able to process requests fast enough that there is always space to store the address for an incoming targeted store then the receiving thread will always have a specific address to load from thus minimizing time spent scanning for requests.

In case a targeted store is received and no resources are available to store its address the corresponding bit in a bitmap summary as described above may be set. This way while a receiving thread is able to keep up with the targeted stores it receives it is able to immediately determine the address from which it should load its next request. If it is unable to keep up forcing bits to be recorded in a summary bitmap it still receives information to focus its search for locations that have received targeted stores.

In addition or alternatively one or more bits of information may be maintained that allow a cache to communicate to a thread that it has been unable to record information about all incoming messages requesting targeted stores to addresses the thread is monitoring. In this case the thread may need to initiate a scan of the entire address range it is monitoring because it has no information with the possible exception of the fact that a targeted store has been performed to one of the locations in the address range it is monitoring . It is desirable that this option is not required too often especially if the requests are sparse in which case scanning yields relatively few requests .

Nonetheless we note that for at least some targeted store implementations a service thread in the concurrent data structure implementation alluded to above must periodically scan all request locations. This may be because the service thread may occasionally be migrated to a different core so that it will no longer be notified in response to a targeted store that is targeted to a cache near the core the service thread was running on previously. Information provided by a good multi location MONITOR MWAIT implementation such as described above may be able to significantly reduce how often such scanning must be performed.

We further note that the information provided to the thread about which locations have received targeted stores is merely heuristic in many embodiments because a receiving thread will occasionally scan all locations it is monitoring so a lost notification will not result in losing a request though it will delay its processing similarly because the thread loads a location and checks its contents to determine if there is a request a spurious notification also will not cause incorrect behavior.

This observation is important because it gives the implementation more freedom. For example it implies that there is no need for notification state such as addresses and summary bitmaps to be preserved by context switches. We note however that the more reliable this information is the less often it is necessary for a thread to scan the locations it is monitoring. For example if a thread is able to atomically clear the summary bitmap without interfering with the setting of a bit due to a concurrent incoming request then there is less chance of a lost notification and thus the scanning policy can be less aggressive without harming performance. On a related note if a receiving thread that is preempted and subsequently rescheduled receives an indication that some address and or summary information may have been lost due to the context switch it can immediately initiate a full scan and then avoid another one until the next context switch.

We next describe a monitoring circuit which can be used to monitor for targeted stores which are directed to a specific addresses.

Next if the targeted store is directed to a monitored memory location the system informs thread about the targeted store step . This allows thread to leave an idle state or to wake up to process the targeted store. The message which informs thread about the targeted store can also include information which specifies a destination address for the targeted store. Alternatively thread can make a special system call execute a special instruction or read a special register to obtain this destination address information from monitoring circuit .

Monitoring circuit can maintain this destination address information for targeted stores in a number of ways. For example monitoring circuit can maintain a queue to hold such targeted store destination addresses. However if a large number of targeted stores are received it may not be possible to store all of the addresses in such a queue. In this case monitoring circuit can maintain a bitmap for each thread wherein the bitmap provides summary information indicating sets of addresses in a set of multiple memory locations which are being monitored for the thread that may have received targeted stores. In another example the system can simply inform thread that a targeted store directed to a monitored address has been received without specifying the destination address for the targeted store. In this case thread will have to search through the monitored addresses to determine which one received the targeted store.

The foregoing descriptions of embodiments have been presented for purposes of illustration and description only. They are not intended to be exhaustive or to limit the present description to the forms disclosed. Accordingly many modifications and variations will be apparent to practitioners skilled in the art. Additionally the above disclosure is not intended to limit the present description. The scope of the present description is defined by the appended claims.

