---

title: Automated node fencing integrated within a quorum service of a cluster infrastructure
abstract: A quorum service detects liveness failures of at least two failed nodes in a domain of a cluster infrastructure layer of a cluster environment within a limited time frame and adds the at least two failed nodes to a list of nodes set to pending to be fenced by a group leader node. The quorum service determines whether the at least two failed nodes include the group leader node. The quorum service, responsive to the at least two failed nodes not including the group leader node, triggers the group leader node to trigger at least one fencing operation to fence the at least two failed nodes in the list of nodes. The quorum service, responsive to the at least two failed nodes including the group leader node, sets a new node as the group leader node and triggers the new node set as the group leader node to trigger the at least one fencing operation to fence the at least two failed nodes in the list of nodes.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09037899&OS=09037899&RS=09037899
owner: INTERNATIONAL BUSINESS MACHINES CORPORATION
number: 09037899
owner_city: Armonk
owner_country: US
publication_date: 20131120
---
This application is a continuation of commonly assigned U.S. patent application Ser. No. 13 718 197 filed Dec. 18 2012 which is a continuation of U.S. patent application Ser. No. 12 784 011 filed May 20 2010.

This invention relates in general to automated management of cluster systems and more particularly to integrating automated node fencing into quorum services of a cluster infrastructure for providing automated failure and recovery services at the cluster infrastructure level and for reporting a consistent reliable view of cluster node health to distributed applications.

Computer clusters or cluster environments are groups of interconnected computing elements or nodes associated in such a way as to facilitate interoperability and management. The nodes in a cluster may work in tandem to provide more efficient performance and availability than is typically available in a single computer. One or more nodes in a cluster may access one or more resources and one or more nodes in a cluster may share a particular resource.

A cluster infrastructure may provide for organizing nodes of a cluster into domains where a quorum service manages the configuration or membership database indicating the role of each node in each domain as either active or in stand by. The cluster infrastructure may provide quorum services for maintaining a membership status of each node in a domain. In addition the cluster infrastructure may provide quorum services that upon a network failure that partitions the cluster from a group of nodes that communicate directly with one another over dedicated network connections into two sub domains with nodes in one sub domain that cannot communicate with nodes in the other sub domain control which partition retains the quorum and is allowed to continue operating an application after the failure occurs. Distributed applications running atop a cluster infrastructure may request the cluster infrastructure to provide a health status of each node for use by the distributed applications in safe control and failover of shared resources however the cluster health status reported by quorum services alone merely indicates the membership status of a node within a quorum. Node quorum membership status alone however may be insufficient to guarantee safe management of shared resources when partitions occur within a cluster environment because the network failures may prevent cross node communication between the partition pieces. Because a cluster health status which indicates node quorum membership status alone may be insufficient to guarantee safe management of shared resources a programmer may insert code into distributed applications to manage a network failure by attempting to block one or more nodes from accessing shared resources. In one example the programmer may insert code to attempt to fence a node prior to processing a failover to prevent corrupting shared resources where the node fencing may direct power or I O controls to prevent one node from accessing a shared resource even when cross node communication is not available. In particular node fencing separate nodes which may have access to a shared resource from nodes which must not have access to a shared resource.

In view of a cluster infrastructure with quorum services to provide a node health to distributed applications for managing safe control and failover of shared resources but the quorum membership status alone being insufficient for safe control and failover of shared resources there is a need for a method system and program for integrating node fencing into the quorum services of a cluster infrastructure to provide automated failover and recovery services at the cluster infrastructure level and to report a reliable consistent cluster health status for each node that represents the health of each node in a cluster with respect to each node s ability to control and access shared resources.

In one embodiment a method for managing quorum services in a cluster infrastructure for reliable failover of shared resources is directed to responsive to detecting using a processor by a quorum service a plurality of liveness failures of at least two failed nodes from among a plurality of nodes in a domain of a cluster infrastructure layer of a cluster environment within a limited time frame adding the at least two failed nodes to a list of nodes set to pending to be fenced by a group leader node from among the plurality of nodes. The method is directed to determining by the quorum service whether the at least two failed nodes include the group leader node. The method is directed responsive to the at least two failed nodes not including the group leader node to triggering by the quorum service the group leader node to trigger at least one fencing operation to fence the at least two failed nodes in the list of nodes. The method is directed to responsive to the at least two failed nodes including the group leader node setting a new node as the group leader node and triggering by the quorum service the new node set as the group leader node to trigger the at least one fencing operation to fence the at least two failed nodes in the list of nodes.

In another embodiment a computer system for managing quorum services in a cluster infrastructure for reliable failover of shared resources comprises a quorum service implemented in a cluster environment on at least one processor coupled to at least one memory. The computer system comprises the quorum service responsive to detecting a plurality of liveness failures of at least two failed nodes from among a plurality of nodes in a domain of a cluster infrastructure layer of the cluster environment within a limited time frame operative to add the at least two failed nodes to a list of nodes set to pending to be fenced by a group leader node from among the plurality of nodes. The computer system comprises the quorum service operative to determine whether the at least two failed nodes include the group leader node. The computer system comprises the quorum service responsive to the at least two failed nodes not including the group leader node operative to trigger the group leader node to trigger at least one fencing operation to fence the at least two failed nodes in the list of nodes. The computer system comprises the quorum service responsive to the at least two failed nodes including the group leader node operative to set a new node as the group leader node and operative to trigger the new node set as the group leader node to trigger the at least one fencing operation to fence the at least two failed nodes in the list of nodes.

In another embodiment a computer program product for managing quorum services in a cluster infrastructure for reliable failover of shared resources comprises one or more computer readable tangible storage devices. The computer program product comprises program instructions stored on at least one of the one or more storage devices responsive to detecting a plurality of liveness failures of at least two failed nodes from among a plurality of nodes in a domain of a cluster infrastructure layer of a cluster environment within a limited time frame to add the at least two failed nodes to a list of nodes set to pending to be fenced by a group leader node from among the plurality of nodes. The computer program product comprises program instructions stored on at least one of the one or more storage devices to determine whether the at least two failed nodes include the group leader node. The computer program product comprises program instructions stored on at least one of the one or more storage devices responsive to the at least two failed nodes not including the group leader node to trigger the group leader node to trigger at least one fencing operation to fence the at least two failed nodes in the list of nodes. The computer program product comprises program instructions stored on at least one of the one or more storage devices responsive to the at least two failed nodes including the group leader node to set a new node as the group leader node and to trigger the new node set as the group leader node to trigger the at least one fencing operation to fence the at least two failed nodes in the list of nodes.

In the following description for the purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the present invention. It will be apparent however to one skilled in the art that the present invention may be practiced without these specific details. In other instances well known structures and devices are shown in block diagram form in order to avoid unnecessarily obscuring the present invention.

In addition in the following description for purposes of explanation numerous systems are described. It is important to note and it will be apparent to one skilled in the art that the present invention may execute in a variety of systems including a variety of computer systems and electronic devices operating any number of different types of operating systems.

With reference now to the Figures and in particular with reference to a block diagram illustrates one example of a cluster environment implementing a cluster infrastructure with quorum services integrated with at least one automated fencing operation. In the example a cluster environment includes an interconnection network for connecting multiple nodes illustrated as nodes and .

Each of nodes and may include multiple types of computing nodes including but not limited to one or more of a processing node an input output I O node a singe processor system a multiple processor system a cluster system of multiple computing nodes a grid environment and other systems including a network interface for connecting to interconnection network . Those of ordinary skill in the art will appreciate that while illustrates nodes and cluster environment may include any number of nodes implemented in one or more network protocol architectures.

In the example depicted each of nodes and may include one or more adapters as part of the network interface for connecting to interconnection network and for interfacing with one or more resources such as resources and . It will be understood that multiple types of adapters may be implemented by each of nodes and where each adapter may communicate with another adapter a switch or other component within interconnection network .

Interconnection network may include one or more types of network elements switching elements nodes clusters adapters and other elements for communicatively connecting with nodes and . In addition interconnection network may implement one or more types of underlying network architectures and services. For example interconnection network may include but is not limited to the Internet a wide area networks WANs a local area networks LANs an Ethernet a token ring and a network service providers. Further interconnection network may transport data between nodes using one or more types of communication protocols. In one example interconnection network implements protocols for transferring data within a packet switching network such as using the transmission control protocol internet protocol TCP IP however interconnection network may include multiple gateways routers and other hardware software and other elements to enable integration of networks using additional or alternate protocols and additional or alternate layers of protocols. The hardware and software implemented by interconnection network may provide one or more of reliable and unreliable passing of messages.

A cluster infrastructure includes one or more software components implemented locally and globally across nodes and for managing one or more clusters of nodes from among nodes and within cluster environment . Distributed applications may call one or more functions application programming interfaces processes commands and other elements of cluster infrastructure for running distributed applications across one or more of nodes and .

In one example cluster infrastructure may implement Reliable Scalable Cluster Technology RSCT including multiple software components that hold together nodes and within cluster environment manage system availability scalability and other services for cluster environment and provide components for implementing quorum services . One of ordinary skill in the art will appreciate that cluster infrastructure may implement additional or alternate types of cluster software infrastructure.

As illustrated cluster infrastructure includes quorum services . Quorum services may organize one or more of nodes and into a peer domain that provides a cluster of nodes configured for high availability. In one example quorum services may manage a quorum membership database with a quorum membership configuration and status of each node within a peer domain and may implement membership services for monitoring for node and network failures within a peer domain and for cross node and process coordination.

Quorum services include checking for the liveness of nodes interconnected in a peer domain. In one example quorum services may use heartbeats to check the liveness of nodes within a peer domain. One or more types of events may occur within a peer domain which result in one or more of the nodes failing a liveness check and the peer domain being partitioned or divided into two or more sub domains where nodes in one sub domain are no longer aware of the nodes in any other sub domain. Upon detection of a failure within a peer domain quorum services may select which sub domain has operational quorum and may continuing operating. In particular selection of which sub domain has operational quorum is critical when there is a shared resource such as resource because data corruption can occur if nodes in different sub domains attempt to access the same shared resource.

In the present invention quorum services detect shared resources such as resource that is shared between nodes and and provide automated failure and recovery support services at the level of cluster infrastructure . First quorum services provide automated failure and recovery support services at the cluster infrastructure level by automatically implementing node fencing integrated into membership and quorum management. Second quorum services provide automated failure and recovery support services at the cluster infrastructure level by managing a reliable consistent cluster health status guaranteeing the reported status represents the ability of each node to provide service and access shared resources and persistently reporting the cluster health status to distributed applications . To integrate automated node fencing into quorum services the cluster health status is updated with a usability state of each node where the usability state refers to each node s ability to provide service and access shared resources.

By incorporating automated node fencing into quorum services node fencing is driven by cluster infrastructure level quorum management rather than by distributed applications allowing the node fencing to be performed asynchronously from node failures to better support continued automated operations by cluster infrastructure and maintenance of a consistent view of cluster node health. Node fencing integrated within quorum management guarantees the current usability state of each node following failures allowing distributed applications to trust that the cluster health status reported by cluster infrastructure with current usability states reflects the actual status of the node removing the need for programmers to insert code for node fencing code into distributed applications to control cluster infrastructure for failure and recovery from the application level. In particular while cluster infrastructure may still provide a separate node fencing command that a programmer may call from distributed applications such as a STONITH command callable by distributed applications by incorporating automated node fencing into quorum services a programmer may rely on cluster infrastructure failure and recovery services to report a reliable cluster health status and forego inserting additional code to call node fencing mechanisms.

One skilled in the art will appreciate that the number and configuration of nodes switches and links shown in is provided as an example cluster environment. Numerous implementations and arrangements of a number of nodes switches and links in all types of data and computing networks may be implemented.

Referring now to a block diagram illustrates one example of a computer system in which the present invention may be implemented. The present invention may be performed in a variety of systems and combinations of systems made up of functional components such as the functional components described with reference to computer system and may be communicatively connected to a network such interconnection network . As described herein one or more functional components of computer system may represent a node such as one of nodes and or a grouping of multiple instances of one or more functional components of computer system may represent a node such as one of nodes and .

Computer system includes a bus or other communication device for communicating information within computer system and at least one hardware processing device such as processor coupled to bus for processing information. Bus preferably includes low latency and higher latency paths that are connected by bridges and adapters and controlled within computer system by multiple bus controllers. When implemented as a server or node computer system may include multiple processors designed to improve network servicing power. Where multiple processors share bus additional controllers not depicted for managing bus access and locks may be implemented.

Processor may be at least one general purpose processor such as IBM s PowerPC PowerPC is a registered trademark of International Business Machines Corporation processor that during normal operation processes data under the control of software which may include at least one of application software an operating system middleware and other code and computer executable programs accessible from a dynamic storage device such as random access memory RAM a static storage device such as Read Only Memory ROM a data storage device such as mass storage device or other data storage medium. Software may include but is not limited to applications protocols interfaces and processes for controlling one or more systems within a network including but not limited to an adapter a switch a cluster system and a grid environment.

In one embodiment the operations performed by processor may control the operations of flowchart of and other operations described herein. Operations performed by processor may be requested by software or other code or the steps of the present invention might be performed by specific hardware components that contain hardwired logic for performing the steps or by any combination of programmed computer components and custom hardware components.

Those of ordinary skill in the art will appreciate that aspects of one embodiment of the invention may be embodied as a system method or computer program product. Accordingly aspects of one embodiment of the invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment containing software and hardware aspects that may all generally be referred to herein as circuit module or system. Furthermore aspects of one embodiment of the invention may take the form of a computer program product embodied in one or more tangible computer readable medium s having computer readable program code embodied thereon.

Any combination of one or more computer readable tangible medium s may be utilized. The computer readable tangible medium is a computer readable storage medium. A computer readable storage medium is an electronic magnetic optical semiconductor system apparatus or device including a portable computer diskette a hard disk such as mass storage device a random access memory RAM such as RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory a portable compact disc read only memory CDROM an optical storage device or a magnetic storage device. In the context of this document a computer readable storage medium is any tangible medium that can contain or store a program for use by or in connection with an instruction executing system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable radio frequency RF etc. or any suitable combination of the foregoing.

Computer program code for carrying out operations of on embodiment of the invention may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer such as computer system partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network such as interconnection network through a communication interface such as network interface over a network link that may be connected for example to interconnection network .

In the example network interface includes an adapter for connecting computer system to interconnection network through a link. Although not depicted network interface may include additional software such as device drivers additional hardware and other controllers that enable communication. When implemented as a server computer system may include multiple communication interfaces accessible via multiple peripheral component interconnect PCI bus bridges connected to an input output controller for example. In this manner computer system allows connections to multiple clients via multiple separate ports and each port may also support multiple connections to multiple clients.

The present invention is described below with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. Those of ordinary skill in the art will appreciate that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that can direct a computer such as computer system or other programmable data processing apparatus to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instruction means which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer such as computer system or other programmable data processing apparatus to cause a series of operational steps to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

Network interface the network link to interconnection network and interconnection network may use electrical electromagnetic or optical signals that carry digital data streams. The signals through the various networks and the signals on interconnection network the network link to interconnection network and network interface which carry the digital data to and from computer system may be forms of carrier waves transporting the information.

In addition computer system may include multiple peripheral components that facilitate input and output. These peripheral components are connected to multiple controllers adapters and expansion slots such as input output I O interface coupled to one of the multiple levels of bus . For example input device may include for example a microphone a video capture device an image scanning system a keyboard a mouse or other input peripheral device communicatively enabled on bus via I O interface controlling inputs. In addition for example a display device communicatively enabled on bus via I O interface for controlling outputs may include for example one or more graphical display devices audio speakers and tactile detectable output interfaces but may also include other output interfaces. In alternate embodiments of the present invention additional or alternate input and output peripheral components may be added.

Those of ordinary skill in the art will appreciate that the hardware depicted in may vary. Furthermore those of ordinary skill in the art will appreciate that the depicted example is not meant to imply architectural limitations with respect to the present invention.

With reference now to a block diagram illustrates one example of components of quorum services of a cluster infrastructure with integrated automated node fencing. In the example quorum services include membership services to manage node membership status by peer domain. For example membership services may implement a resource manager component to provide a command line interface for creating a new peer domain joining new nodes to the domain changing the domain membership of nodes listing nodes in the domain and other functions for creating and administering a peer domain. Cluster environment may include one or more peer domains.

Membership services manage node membership by domain in a global membership status database where global membership status database indicates quorum configuration including but not limited to domain membership of each node characteristics of each peer domain quorum requirements and other membership and peer domain factors. Global membership status database may represent a global instance of the membership status database or may represent an instance of the membership status database stored locally and persistently maintained at one of the nodes. Membership status may be maintained in global membership status database and membership services may propagate a local copy of the membership status of nodes within each node s peer domain to each node illustrated as local membership statuses and . In one example membership status of a node may indicate whether the node is active or in standby .

Membership services may automatically monitor for the liveness of each node in a peer domain by performing heartbeat rings or other communication liveness checks between the communication interfaces of nodes in a peer domain. In particular membership services may monitor the communication between nodes in a peer domain for communication indicative of liveness according to characteristics set for the particular peer domain including but not limited to the number of missed heartbeats that constitute a failure the number of seconds between heartbeats whether or not broadcast should be used and whether or not source routing should be used.

Membership services may also monitor each peer domain for quorum requirements where quorum requirements refers to the minimum number of nodes within the peer domain required to carry out a particular operation. Membership services may monitor for multiple kinds of quorum requirements including but not limited to startup quorum specifying the number of nodes needed to bring a peer domain online configuration quorum specifying the minimum number of nodes needed to perform operations that modify the peer domain s configuration information and operational quorum specifying the minimum number of nodes needed to safely activate resources without creating conflicts with another subdomain. In particular membership services apply operational quorum requirements to protect data on shared resources following domain partitioning where domain partitioning occurs when a peer domain is divided into one or more sub domains because of failures within the peer domain or other events.

Fencing operations provide automated node fencing operations for quorum services . First fencing operations maintain a usability state of each node within a sub domain updated within a global usability state database and persistently propagated to each node for local storage by each node as illustrated by local usability states and . In one example the usability state of each node is either set to usable pending or unusable . As will be further described with reference to fencing operations manages the usability state of each node in conjunction with membership services managing node domain membership and quorum requirements of nodes within peer domains. Second fencing operations implement automated node fencing mechanisms for fencing one or more nodes in conjunction with membership services applying quorum requirements to protect data on shared resources following events which trigger domain partitioning. Fencing operations may implement one or more types of node fencing mechanisms for fencing a node including but not limited to applying power or input output I O controls to restrict node access to a shared resource.

In one example fencing operations may propagate usability state updates to nodes from global usability state database for updating local usability state records of nodes illustrated as local usability states and . In addition fencing operations implemented locally at each node may detect events triggering changes in usability states of one or more nodes and broadcast the usability state changes to each node as well as updating the state recorded in the local usability state recorded at the broadcasting node.

Quorum services may report a cluster health status to distributed applications where cluster health status may include the usability state of one or more nodes from global usability state database . The usability state of each node reported in cluster health status provides a reliable status of each node s ability to provide service and access to shared resources. In addition cluster health status may include the membership status of one or more nodes from global membership status database .

It is important to note that components of membership services and fencing operations may be distributed across nodes such that each node may locally trigger membership services and fencing operations and that an administrative or management node may also coordinate global components of membership services and fencing operations .

Referring now to a block diagram illustrates a flow diagram depicting the flow of usability states for tracking the usability state of a node. In the example flow diagram illustrates examples of three states to which a node usability state may transition as managed by fencing operations illustrated as usable state pending state and unusable state . One of ordinary skill in the art will recognize that additional or alternate states may be implemented for tracking a usability state of a node.

In the example usable state represents a state in which the node is unrestricted from controlling shared resources. Usable state indicates that the node has never been fenced or was successfully fenced following a failure and may safely resume service when it rejoins the cluster.

Pending state represents a state in which a problem was detected at the node and the node will be fenced by the cluster sub domain that retains quorum. While in pending state shared resources should not be changed with respect to the node and no operational changes should be made to a node until fencing can be completed or fencing fails and the node is transitioned to unusable state .

Unusable state represents a state indicating an attempt to fence a node has failed. The value of unusable state persists over the life of a cluster or domain even if the node leaves and then later rejoins. Once a node reaches unusable state fencing operations may require that the usability state of the node be reset by a cluster administrator when the node is determined to be safe to resume service or may implement a resetting service for automatically determining when a node is safe to resume service and resetting the usability state of the node.

In the example as illustrated at reference numeral when a node is initialized fencing operations set the initial usability state of a node in global usability state database to usable state and the node is unrestricted from controlling resources.

Next as illustrated by the transition at reference numeral if membership services detects a node failure fencing operations transition the node usability state of the node to pending state the node s usability state in global usability state database is set to pending and fencing operations will trigger a node set to the fencing role to attempt to fence the failed node. When the node s usability status is set to pending operational changes to the node should be blocked and any shared resources associated with the node should not be changed.

Next as illustrated at reference numeral if fencing operations are able to successfully fence the node then fencing operations transition the node usability state of the node to usable state indicating the shared resources may be failed over from the node and the node is once again unrestricted for cluster service when it rejoins the cluster.

As illustrated at reference numeral if fencing operations are not able to successfully fence the node then fencing operations transition the node usability state of the node to unusable state . When the node s usability state is set to unusable the state of the node is unknown the node could not be fenced and resources should not be changed with respect to the node to avoid corrupting data.

As illustrated at reference numeral if the usability state of a node is set to unusable but the node usability state is reset then the node is once again available rejoin the cluster and is unrestricted from controlling resources.

With reference now to a block diagram illustrates one example of automatically implementing a node fencing mechanism at the cluster infrastructure level to manage a failover of a shared resource. In the example a peer domain membership for a cluster is configured to include a node node node and node connected via interconnection network and managed by cluster infrastructure . In the example node and node each access a shared resource with node controlling shared resource as indicated by SR .

In the example membership services detect a failure in node that triggers domain partitioning. Membership services detect the failure event triggering a domain partitioning and determine which sub domain will retain quorum. In the example domain partitioning results in one sub domain with node and another sub domain with node node and node . In the example membership services determine that the sub domain with node node and node will retain quorum.

Because quorum services integrate automated fencing operations with membership services responsive to detecting the failure in node triggering domain partitioning fencing operations transition the state of node from usable to pending . In the event of the state of node transitioning to pending fencing operations will select a single node from the sub domain retaining quorum to control the fencing of the failed node. In one example the single node selected to perform the fencing role is also illustrated as the group leader GL node. The GL node illustrated in the example may be the same node selected within an RSCT based cluster infrastructure as the GL. In particular when a peer domain is established membership services may assign a GL node. When a peer domain is partitioned membership services will assign a new GL node within the sub domain retaining quorum if the sub domain retaining quorum does not include the GL. In the example node of sub domain is the GL of the sub domain retaining quorum.

The node set to perform the fencing role for example node as the GL automatically triggers node fencing mechanism from node fencing operations to fence node . In particular even though node may not be able to communicate directly with node via the communication channels of interconnection network node may trigger node fencing mechanism to control the power or input output channels of node to restrict node from controlling or accessing shared resource .

Fencing operations may determine whether node fencing mechanism is successful in fencing node . If node fencing mechanism is successful in fending node then fencing operations will transition the usability state for node from pending to usable and shared resource may failover with node assuming control of shared resource .

Referring now to a block diagram illustrated one example of membership services implementing a tie breaker to determine quorum with automated node fencing integrated in quorum services. When membership services detects an event triggering a domain partitioning membership services may require a tie breaker operation to determine which sub domain will retain quorum. In particular following domain partitioning membership services may include a preference to assign operational quorum to the sub domain with the majority of nodes. Following domain partitioning however each sub domain may include the same number of nodes or otherwise require additional decision making to determine which sub domain will retain quorum.

In one example illustrates an example where node controls a shared resource accessible to node within a peer domain. A network failure occurs between node and node and both nodes detect the other as down. After the failure the domain partitioning results in a bisected cluster with an equal number of nodes in each sub domain illustrated by sub domains and and where node is in sub domain and node is in another sub domain . Membership services detect the bisected cluster and acquire a tie breaker to determine operational quorum. Tie breaker is a process function or resource defined by membership services that specifies how tie situations should be resolved.

In one example a particular node within one of the sub domains such as node in the example may actually acquire tie breaker . A node acquiring tie breaker to determine operational quorum may be asynchronous to the node failure.

When membership services acquires tie breaker because fencing operations are integrated into quorum services fencing operations detect that a tie breaker has been acquired and transition the usability state of nodes in both sub domains from usable to pending to block operational changes until the tie is resolved and fencing is completed.

In the example tie breaker determines that sub domain containing node should be assigned quorum and membership services assigns operational quorum to sub domain . Node calls a node fencing mechanism to fence node . Fencing operations may determine whether node fencing mechanism is successful in fencing node . If node fencing mechanism is successful in fending node then fencing operations will transition the usability states for node and node from pending to usable and shared resource may failover with node assuming control of shared resource .

With reference now to a block diagram illustrates one example of locally maintaining usability states upon nodes joining a cluster. While a node usability state may be maintained globally in global usability state database node usability state for each node is updated both locally and propagated through cluster protocols for efficiency and consistency even when multiple failures occur in succession.

In the example upon initializing a node prior to the node joining a cluster membership services and fencing operations running locally on a node initialize the node with a local usability state for all cluster nodes set to a presumed value of usable . For the join protocol where nodes join the cluster as managed by membership services fencing operations propagate the global usability status of any nodes that do not have a global usability status of usable to the joining nodes. In particular join protocols membership change and node usability update protocols are serialized by quorum services guaranteeing the node usability state will be consistent on all nodes.

In the example illustrated during a startup stage shows the startup of a five node cluster domain. In the example node and node join the cluster together with local usability states and set with each node usability state set as usable and node set as the group leader GL . Next node joins the cluster with local usability states set with each node usability state set as usable . Finally node and node join the cluster with local usability states and set with each node usability state set as usable . In the example quorum services do not propagate the usability state during any of the joins because all nodes are set to the usable state.

Referring now to a block diagram illustrates one example of locally maintaining persistent usability states upon node failure. In the example node node node node and node have previously joined a cluster as illustrated in reference numeral with node set as the GL. illustrates node failure stage in which membership services detects network failures in node and node . The network failure in node and node trigger domain partitioning with node node and node in one sub domain isolated from node and node . Membership services reports the node failures to all nodes in the cluster and fencing operations at each node may locally change the local node usability states of nodes in the other sub domain from usable to pending . For example as illustrated in local usability states and based on the node failure reports propagated by membership services to node node and node the node usability state locally set for node and node for each of these nodes is changed from usable to pending . In addition as illustrated in local usability states and based on the node failure reports propagated by membership services to node and node the node usability state locally set for node node and node for each of these nodes is changed from usable to pending .

In the example membership services assigns quorum to the sub domain with node node and node with node as the current GL. Node upon detecting the local usability states for node and node set to pending attempts to fence node and node by calling node fencing mechanisms of fencing operations . In the example the node fence mechanism for node reports to node that fencing is successful and the node fence mechanism for node reports to node that the node fencing attempt failed.

Referring now to a block diagram illustrates one example of locally maintaining persistent usability states upon node failure following node fencing. In the example node has received the results from node fencing mechanisms of attempts to fence node and node as described with reference to node failure stage of . illustrates a node usability broadcast stage . Since node detects node fencing was successful node sets the node usability state of node to usable . Since node detects the attempt at fencing node was not successful node sets the node usability state of node to unusable . Node then broadcasts the node usability states of node and node to the surviving nodes including node and node in a node usability state protocol to update local usability states and as well as updating local usability state during the broadcast to ensure consistency.

With reference now to a block diagram illustrates one example of locally maintaining persistent usability states upon rejoinder of a node to a cluster. In the example node was previously successfully fenced as described with reference to and . illustrates a node rejoinder stage . Node begins the rejoinder process by first resetting all states in local usability state to usable . Membership services detect the request to rejoin node to the cluster and propagate the unusable usability state of node to node . Node updates the usability state set for node in local usability state to unusable as illustrated. Node continues to remain unusable and must be reset before joining the cluster.

Referring now to a high level logic flowchart depicts a process and program for controlling quorum service events where the quorum services integrate node fencing. In the example the process starts at block and thereafter proceeds to block . Block illustrates a determination at to the type of quorum activity detected from among node initialization node joins and rejoins and liveness failures. One of ordinary skill in the art will appreciate that additional quorum services may detect and respond to additional types of quorum activity.

In the example at block when the quorum activity indicates initialize node the process passes to block . Block depicts setting the usability state of all nodes in the local usability state for the initialized node to usable in addition to performing other quorum service level node initialization steps and the process ends.

In the example at block when the quorum activity indicates join or rejoin the process passes to block . Block depicts a determination whether the global usability state for a node is set to usable . If the global usability state for a node is set to usable then the process passes to block . If the global usability state for a node is not set to usable then the process passes to block . Block illustrates requiring a node to reset before allowing the node to join and the process ends.

Block depicts a determination whether there are any node usability states not set to usable in the global usability state database. If there are not any nodes not set to usable in the global usability state database then the process passes to block . If there are nodes not set to usable in the global usability state database then the process passes to block . Block illustrates updating the node states for any node not set to usable in the local usability state for the joining node and the process passes to block . Block illustrates joining the node to the peer domain and updating the joined node usability state to the other nodes in the peer domain list and the process ends.

Returning to block in the example at block when the quorum activity indicates liveness failure then the process passes to block . Block depicts determining the sub domains resulting from the failure. Next block illustrates a determination whether the node liveness failure has resulted in a bisected cluster domain. If the node failure has resulted in a bisected cluster domain then the process passes to block . Block depicts updating the node usability state for all tie nodes in the bisected cluster domain accessing a shared resource to pending . Next block illustrates selecting one of the nodes to call a tie breaker. Therefore block depicts a determination whether the tie breaker returns a quorum decision. Once the tie breaker returns a quorum decision then the process passes to block . Block depicts updating the node usability state for the tie nodes in the quorum to usable and selecting a group leader GL node for the selected quorum and the process passes to block .

Returning to block if the node failure has not resulted in a bisected cluster domain then the process passes to block . Block depicts updating the node usability state for the failed node to pending . Next block depicts selecting the sub doman retaining quorum and setting a GL node. Thereafter block illustrates the group leader for the quorum attempting to fence the failed node. Thereafter block depicts a determination whether the fencing is successful. If the fencing is successful then the process passes to block . Block depicts updating the node usability state for the fenced nodes to usable and allowing the shared resource to failover and the process ends. Returning to block if the fencing is not successful then the process passes to block . Block depicts updating the node usability state for the node not successfully fenced to unusable and the process ends.

With reference to a high level logic flowchart depicts a process and program for controlling quorum services for handling multiple synchronous or closely timed failures where the quorum services integrate node fencing. In the example the process starts at block and thereafter proceeds to block . Block depicts a determination whether multiple liveness failures which are synchronous or within a limited time frame are detected. If multiple liveness failures are detected then the process passes to block . Block depicts adding each failed node to a list of nodes set to pending to be fenced by the GL node. Next block illustrates a determination whether the failures include the currentl GL node. If the failures do not include the current GL node then the process passes to block . Block depicts triggering the current GL node to fence all the nodes in the list of nodes set to pending as described in and the process ends. Returning to block if the failure do include the current GL node then the process passes to block . Block illustrates determining a new GL node. Next block depicts triggering the new GL node to fence all the nodes in the list of nodes set to pending whether or not the nodes were fenced by the previous GL before the previous GL failed and the process ends. In particular by triggering the new GL to fence all the nodes in the list of nodes set to pending duplicate fencing attempts by the previous GL and the new GL may occur however the new GL will attempt to fence all nodes set to pending to guarantee safe operations and to guarantee the reliability of the node usability state of each node.

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that in some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact occur substantially concurrently or the blocks may sometimes occur in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

The corresponding structures materials acts and equivalents of all means or step plus function elements in the claims below are intended to include any structure material or act for performing the function in combination with other claimed elements as specifically claimed. The description of the one or more embodiments of the invention has been presented for purposes of illustration and description but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The embodiment was chosen and described in order to best explain the principles of the invention and the practical application and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.

While the invention has been particularly shown and described with reference to one or more embodiments it will be understood by those skilled in the art that various changes in form and detail may be made therein without departing from the spirit and scope of the invention.

