---

title: Replication between sites using keys associated with modified data
abstract: Systems and methods are disclosed for replicating data stored in an in-memory data cache to a remote site. An example system includes an in-memory data cache and an in-memory keys cache. The system also includes a key insert module that detects a modification to the in-memory data cache, identifies one or more keys of the plurality of keys based on the modification, and inserts the identified one or more keys into the in-memory keys cache. The system further includes an update module that retrieves from the in-memory keys cache a set of keys, retrieves from the in-memory data cache modified data associated with the set of keys, and transmits to a remote site a modification list including the set of keys and the modified data associated with the set of keys.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09411869&OS=09411869&RS=09411869
owner: Red Hat, Inc.
number: 09411869
owner_city: Raleigh
owner_country: US
publication_date: 20130709
---
The present disclosure generally relates to in memory caches and more particularly to replication of in memory caches.

An in memory cache may be synchronously replicated across sites. Synchronous replication may be reliable because the source node sending a copy of the in memory data receives acknowledgements from one or more destination nodes indicating that the data was received. A large amount of data may be sent to the destination nodes e.g. multiple modifications of the same data . If a single node of the destination nodes fails to send the source node an acknowledgement the whole operation may fail. The time it takes to achieve a successful commit or a roll back may be quite long.

Additionally the more nodes in the set of participating nodes the higher the likelihood is that something may go wrong. For example if the set of participating nodes includes N nodes and the probability of one participating node failing to apply the transaction based on the first communication is two percent then the overall probability of the transaction failing is N 2 . For example if N is five the failure probability is 10 and if N is ten the failure probability is 20 and so on. Further communications between the local and remote sites include multiple components e.g. the bridge and site master that dispatches to the right target at the remote site. Accordingly N may include not only the participants but also these components.

It may be desirable to provide a fast and reliable technique to replicate in memory caches that overcome the disadvantages discussed above. This disclosure relates to replication of in memory caches between sites. Methods systems and techniques for asynchronously replicating data from an in memory data cache in a first site to a remote in memory data cache in a second site are provided.

According to an embodiment a system for replicating an in memory data cache includes an in memory data cache that stores a plurality of keys and data associated with the plurality of keys. The system also includes an in memory keys cache that stores keys associated with modified data. The system further includes a key insert module that detects a modification to the in memory data cache identifies one or more keys of the plurality of keys based on the modification and inserts the identified one or more keys into the in memory keys cache. The system also includes an update module that retrieves from the in memory keys cache a set of keys retrieves from the in memory data cache modified data associated with the set of keys and transmits to a remote site a modification list including the set of keys and the modified data associated with the set of keys. At least one node in the remote site is updated using the set of keys and the modified data associated with the set of keys.

According to another embodiment a method of replicating an in memory data cache includes detecting by one or more processors a modification to an in memory data cache the in memory data cache storing a plurality of keys and data associated with the plurality of keys. The method also includes identifying one or more keys of the plurality of keys based on the modification. The method further includes inserting the identified one or more keys into an in memory keys cache. The method also includes retrieving from the in memory keys cache a set of keys and retrieving from the in memory data cache modified data associated with the set of keys. The method further includes transmitting to a remote site a modification list including the set of keys and the modified data associated with the set of keys. At least one node in the remote site is updated using the set of keys and the modified data associated with the set of keys.

According to another embodiment a non transitory machine readable medium includes a plurality of machine readable instructions that when executed by one or more processors are adapted to cause the one or more processors to perform a method including detecting a modification to an in memory data cache the in memory data cache storing a plurality of keys and data associated with the plurality of keys identifying one or more keys of the plurality of keys based on the modification inserting the identified one or more keys into an in memory keys cache retrieving from the in memory keys cache a set of keys retrieving from the in memory data cache modified data associated with the set of keys transmitting to a remote site a modification list including the set of keys and the modified data associated with the set of keys where at least one node in the remote site is updated using the set of keys and the modified data associated with the set of keys.

Embodiments of the present disclosure and their advantages are best understood by referring to the detailed description that follows. It should be appreciated that like reference numerals are used to identify like elements illustrated in one or more of the figures. The drawing in which an element first appears is generally indicated by the left most digit in the corresponding reference number.

It is to be understood that the following disclosure provides many different embodiments or examples for implementing different features of the present disclosure. Some embodiments may be practiced without some or all of these specific details. Specific examples of components modules and arrangements are described below to simplify the present disclosure. These are of course merely examples and are not intended to be limiting.

Data may be asynchronously replicated between geographically separate sites. A site may refer to a local cluster of nodes. Cross site replication may allow backing up of data from one site to one or more other sites potentially situated in different geographical locations. Asynchronous data replication may be faster than synchronous data replication and may provide other benefits as described in this disclosure. Although the disclosure may describe data being replicated between geographically separate sites it should also be understood that data may be replicated within a local site using the techniques described in the present disclosure.

For example in asynchronous data replication if a source node in a local site replicates data to a set of nodes in a remote site and the source node fails to receive an acknowledgement from a node of the set of nodes in the remote site it may be unnecessary for the source node to resend the data to each node of the set of nodes in the remote site. Rather the source node may send the data to the particular node that had failed to successively deliver an acknowledgement. Sending the data to this particular node rather than all of the nodes may reduce the amount of data sent between sites. Additionally asynchronous replication may be faster than synchronous replication because it may be unnecessary for the source node to wait for the acknowledgements of the destination nodes.

Synchronous replication may run a two phase commit. In a first phase when the transaction is committed a prepare message is sent to the participating nodes. When the acknowledgements have been received a commit message or a rollback message is sent. The commit message may be sent when all of the nodes apply the prepare message. The rollback message may be sent when one or more nodes fails to apply the prepare message or when some nodes crash or have left e.g. were not reachable the process. Performing this two phase commit may be time consuming.

In another example synchronous replication across sites may include sending the prepare message to the site master of a remote site which in turn applies the prepare message to its local cluster e.g. by forwarding the prepare message to all nodes that have keys involved in the transaction and are waiting for acknowledgements . When the site master has collected the prepare responses it sends the acknowledgement back to the originator of the transaction e.g. in the originating site . Performing this two phase commit may also be time consuming.

Sites may be connected via a high throughput high latency link e.g. approximately 50 70 milliseconds . In an example neglecting the cost of a local two phase commit a two phase commit across two sites may cost assuming 50 milliseconds of latency approximately 150 200 milliseconds. The prepare call to the remote site plus the response approximately 100 milliseconds and similar for the commit or rollback message 100 milliseconds may cost approximately 200 milliseconds. If the second phase were asynchronous the cost may be reduced to approximately 150 milliseconds.

It may be desirable to avoid sending the prepare message to the remote site s as part of the two phase commit. This may reduce latency and the quantity of participants which mitigates the probability of failure.

Further in asynchronous replication a high quantity of nodes in the remote site may receive replicated data without significantly contributing to the failure probability. This is in contrast to synchronous data replication which may have a significantly higher failure rate with each added destination node. For example as discussed above regarding synchronous data replication the more nodes in the set of participating nodes the higher the likelihood is that something may go wrong.

An embodiment of the present disclosure may provide the following advantages among others. An embodiment may be highly reliable because no updates are lost unless the entire primary site crashes. In this case updates applied during the lag time period may be lost. Communication lost between the primary and backup site s however may not lead to data loss because when connectivity is restored a resynchronization of the backup site s may be triggered.

An embodiment may have the correct ordering of updates. In an example all updates are applied at the backup site s in the order in which they were sent. An embodiment may provide for fast replication. In an example it may take no more than a few milliseconds to commit a synchronous transaction. An embodiment may have a configurable lag time. In an example the backup site s are no more than N milliseconds configurable behind the primary site. The present disclosure provides techniques that combine the speed of asynchronous replication with the reliability of synchronization replication as further discussed below.

Diagram includes a site in London and a site in New York City. Site is remote from site and each node in site is remote from the nodes in site . Each site may have one or more nodes. A node local to a site may refer to a node residing in that site.

Site includes four nodes and that are local to site . Node includes an in memory data cache A update module A in memory keys cache A and key insert module A. Node includes an in memory data cache B update module B in memory keys cache B and key insert module B. Node includes an in memory data cache C update module C in memory keys cache C and key insert module C. Node includes an in memory data cache D and in memory keys cache D. Although all of the nodes in site include the in memory data cache and in memory keys cache this is not intended to be limiting. In another embodiment a site may include a node that does not include the in memory data cache and or in memory keys cache.

In site a quantity of nodes that includes the in memory data cache and in memory keys cache may increase or decrease depending on various factors such as whether a new node is launched in site or whether a currently existing node in site leaves. Depending on the role of the node the components in a node may be active or inactive in the node. In an example each node includes the in memory keys cache and the in memory keys cache is active only on a node that replicates changes. In another example the update module is active only on a node that sends updates to the remote site.

Site includes two nodes and that are local to site . Node includes an in memory data cache A and node includes an in memory data cache B.

Sites and are coupled to a network . Network may connect two or more sites and nodes in one site may communicate with nodes in another site via network . Network may include various configurations and use various protocols including the Internet World Wide Web intranets wide area networks WAN and various combinations of the foregoing. In an example network is a WAN with high latency.

Sites and may be located in different geographic regions. Thus sending data from site to site may result in high latency. This disclosure provides techniques to make the latency of in memory cache replication between sites a non dominating factor when replicating between geographically separate sites.

Data stored in in memory data cache A in site may be asynchronously replicated to one or more nodes e.g. node A and or node B in remote site . In memory data cache A stores a plurality of keys and data associated with the plurality of keys. In memory data cache A may be for example a data store maintained by a movie rental company that provides movies online. The movie rental company may provide a movie to a customer for a limited amount of time e.g. 2 days and allow the user to access the movie during this time. The time may start for example once the user has requested the movie or when the user first accesses the movie. Once the allotted time expires the movie rental company may deny the user access to the movie until the user pays another fee.

Table includes an account number column movies column start date column and access column . Account number column includes account numbers that are assigned to customers. An account number may be a unique number given by the movie rental company to identify its customers. The account number may also be used as a primary key in table . A primary key may identify one or more columns in a table that make a row of data unique. The customer may receive the account number by for example registering with the movie rental company.

Movies column includes movies that the customer has previously rented start date column indicates a date on which the user requested access to the movie and access column indicates whether the customer still has access to the movie. In table a customer having account number K1 requested access to movie A on Jan. 29 2013 and still has access to movie A a customer having account number K2 requested access to movie A on Jan. 1 2013 and no longer has access to movie A and a customer having account number K3 requested access to movie B on Jan. 15 2013 and still has access to movie B.

Referring back to the following is a description of node . This description may apply as well to nodes and . Node includes in memory data cache A that stores a plurality of keys and data associated with the plurality of keys. Node also includes in memory keys cache A that stores keys associated with modified data.

Node further includes a key insert module A and an update module A. In an embodiment key insert module A detects a modification to in memory data cache A and identifies one or more keys of the plurality of keys based on the modification. The identified one or more keys may be a primary key associated with the modified entry. Key insert module A may insert the identified one or more keys into in memory keys cache A. Rather than replicating every modification of in memory data cache A to remote site in memory keys cache A may store keys associated with the modified data and retrieve and send the most up to date data associated with the keys to remote site .

Modified data may refer to the data that updates in memory data cache A. A modification to in memory data cache A may include an operation executed against in memory data cache A such as an insertion of an entry into in memory data cache A change of value in an entry in in memory data cache A or a deletion of an entry from in memory data cache A.

Referring to customer may wish to rent movie B from the movie rental company. On Feb. 14 2013 customer may request access to movie B by for example logging into the movie rental company s website and requesting access to the movie. Doing so may modify table by inserting an entry into table . The modified data may refer to at least some of the inserted data.

Customer may have had access to movie B for the maximum amount of allotted time and the movie rental company may want to deny customer access to movie B. Denying customer access to movie B may include changing in entry the Y in access column to N . Doing so may modify table by changing a value in entry in in memory data cache A. If customer pays another fee the movie rental company may allow customer to access the movie. The modified data may refer to the data that updated in memory data cache A e.g. N in access column .

Customer may no longer wish to be a customer of the movie rental company and decide to delete his account. Customer may log into the movie rental company s website and delete his account. Doing so may modify table by deleting an entry from table . The modified data may refer to the data that was deleted from in memory data cache A.

Although key insert module A has been described as identifying the account number as the key to insert into in memory keys cache A other identifiers in table may be identified as the key to insert into in memory keys cache A. Any identifier that is unique and able to uniquely identify the associated data may be used as the key.

Further for simplicity the modifications described above affected one key K1 in table . Another modification however may affect more than one table and the one or more keys identified based on the modification may include hundreds of keys.

In an embodiment update module A retrieves from in memory keys cache A a set of keys retrieves from an in memory data cache e.g. in memory data caches A B C or D modified data associated with the set of keys and transmits to remote site a modification list including the set of keys and the modified data associated with the set of keys. In an embodiment each unique key is replicated only once to reduce bandwidth requirements. More details on key replication are below.

At least one node in remote site may be updated using the set of keys and the modified data associated with the set of keys. In an example site includes a site master that receives updates e.g. the modification list and forwards the updates to the owner nodes of the respective keys. The site master may apply an assignment function e.g. hash function to determine which nodes own which keys. More details on key ownership are below.

If node in site detects a modification to in memory data cache A node may wish to broadcast this update to a set of local backup instances in site . The set of backup instances may be a subset of nodes in site . In an example update module A transmits to a subset of nodes in site e.g. nodes and or a update message including the identified one or more keys and the modified data associated with the identified one or more keys. In this way if node crashes the set of local backup instances in local site has a copy of the identified one or more keys and the modified data associated with the identified one or more keys.

One or more nodes in the set of backup instances in local site may transmit to the remote site the modification list including the identified one or more keys and the modified data associated with the identified one or more keys. Further transmitting the update message to the set of backup instances in local site may provide a high reliability that no updates to in memory data cache A are lost.

In an embodiment each key included in the update message is listed only once. In an example if data associated with key K1 has been modified twenty times data associated with key K2 has been modified three times and data associated with key K3 has been modified once update module A broadcasts to the set of backup instances in local site the update message including K1 K2 K3 and the most recent modified data associated with these keys. In this way update module A may transmit the most up to date modified data to the set of backup instances in local site while transmitting less data compared to sending to the set of backup instances each modification that occurs to in memory data cache A. Further replicating each unique key only once reduces bandwidth requirements.

Node of the set of backup instances in local site may receive from update module A the update message including one or more keys and the modified values associated with the one or more keys. For each key in the update message node may determine whether the key is stored in in memory keys cache B. When the key is determined to not be stored in in memory keys cache B key insert module B may insert the key into in memory keys cache B. In contrast when the key is determined to be stored in in memory keys cache B key insert module B identifies the modified data associated with the key included in the update message stores the identified modified data in in memory data cache B and discards the key. Accordingly node may have the most up to date data associated with the key. Further it may be unnecessary for node to store the key again in in memory keys cache B. Update module A may also send the update message to nodes and . Nodes and may update their in memory data cache and in memory keys cache accordingly.

The in memory data cache and or in memory keys cache in node may be inactive because node does not replicate data to remote site . These caches may store the updated data but may be inactive with respect to sending data to remote site .

Node in site may send the update message to any nodes local to site . In an example node sends the update message to node . If a key included in the update message is not present in in memory data cache B the key and its associated modified data may be a new entry into in memory data cache B. Accordingly the key and the data associated with the key may be inserted as a new entry into in memory data cache B.

Alternatively if a key included in the modification list is present in in memory data cache B the modified data included in the modification list may replace one or more values in in memory data cache B. Accordingly a key or data associated with the key in in memory data cache B may be modified. Additionally if a value included in the modification list is null the associated key and data may be deleted from in memory data cache B. Accordingly in memory data cache B may identify entries including the key and delete from its memory the identified entries.

In an embodiment update module A in node in site broadcasts the update message to each node local to site e.g. nodes and or . In an example each node in site is in the set of backup instances in site . In another example a subset of nodes in site is in the set of backup instances in site . The subset of nodes in site may be assigned to store the key along with its associated modified data. If a node that is in the set of backup instances in site receives the update message the node may store the appropriate keys with its associated modified data. In contrast if a node that is not in the set of backup instances receives the update message the node may discard the message.

In an embodiment update module A in site fetches from another node local to site of the set of backup instances the data associated with a key. In an example update module A may determine which local nodes are of the set of backup instances by applying a function. For instance update module A may apply a consistent hash function and given K as input may determine the one or more nodes in site that store the pair. Update module may then retrieve data from the in memory data cache of the determined local node.

Node may also send a update message to one or more nodes local to site . In an example if node in site detects a modification to in memory data cache A node may wish to broadcast this update to a set of key updaters local to site . For instance update module A may transmit to the set of key updaters in site e.g. nodes and or a update message including the identified one or more keys based on a modification. In this way if node crashes the set of key updaters has a copy of the keys that are associated with modified data where the modified data has not yet been replicated to a remote site e.g. site . Although node is illustrated as not having a key updater node may still store the keys in inactive in memory keys cache D. Node may store the up to date keys but may be inactive with respect to sending updates to remote site . In an example node may retrieve the set of keys from node to send to remote site .

It may be beneficial to send to the set of key updaters only the list of keys associated with the modified data and not include the modified data because the keys are small values e.g. integers or longs and are of minimal size saving bandwidth and computing cycles. In an example at least one node of the set of backup instances in site is also of the set of key updaters in site . In another example nodes of the set of backup instances in site are mutually exclusive of nodes of the set of key updaters in site .

In an embodiment each key included in the update message is listed only once. In an example if data associated with key K1 has been modified twenty times data associated with key K2 has been modified three times and data associated with key K3 has been modified once update module A broadcasts the update message including K1 K2 K3 to the set of key updaters in site .

In an embodiment nodes and in site are of the set of key updaters. In an example for each key in the update message node determines whether the key is stored in in memory keys cache B C. When the key is determined to not be stored in in memory keys cache B C key insert module B C may insert the key into in memory keys cache B C. In contrast when the key is determined to be stored in in memory keys cache B C key insert module B C discards the key. Accordingly each key in the in memory keys cache of a respective node may store a particular key only once. This is not intended to be limiting and a node may store a key more than once. If a node stores a key more than once the node may delete other instances of the key from the respective in memory keys cache before the data to be replicated is sent to the remote site.

In an embodiment update module A in node in site broadcasts the update message to each node in site . In an example each node in site is in the set of key updaters in site . In another example a subset of nodes in site is in the set of key updaters in site . The subset of nodes in site may be assigned to store the key. If a node that is in the set of key updaters in site receives the update message the node in site may store the appropriate keys. In contrast if a node that is not in the set of key updaters in site receives the update message the node in site may discard the message.

Each node of the set of key updaters local to site may be responsible for the same set of keys. In this way different updates to the same key in site may be transmitted to the same node.

In an embodiment implementing the two phase commit may include storing the keys associated with modified data in stable storage. The update module may periodically retrieve the keys and associated values and send the updates to the remote site s . These actions may be performed in the background so that the replication is completed quickly. The set of key updaters local to site may provide stable storage and a certain probability against node failures. Additionally the work of updating the one or more remote sites e.g. site may be distributed among the set of key updaters.

In an example the keys in in memory keys cache are stored in memory so that the keys are quickly retrievable. In another example the keys in in memory keys cache are stored on disk. In another example in memory keys cache is stored in a shared file system. Storing data to and reading data from the shared file system may not be as quick compared to having in memory keys cache stored in memory. For example synchronizing to disk may entail a remote procedure call e.g. with Network File System and messaging between the nodes prolonging the time it takes to replicate data to a remote site.

In an example the data in in memory data cache A is stored in memory so that the data is quickly retrievable. Additionally a portion of data associated with in memory data cache A may be stored on disk.

Data from in memory data cache in site e.g. in memory data caches A B C and or D may be asynchronously replicated to remote site by using keys associated with modified data and stored locally in in memory keys cache in site e.g. in memory keys cache A B C and or D . In an example inserting into local in memory keys cache A B C D keys associated with modified data in site enables the locally stored keys in the in memory keys caches to track modifications to the local in memory data caches and to easily identify the data to replicate and transmit to remote site . Further the keys may typically be small values e.g. integers or longs that are stored in in memory keys cache A B C D in site . In this way the updates to the local in memory data caches may easily be collected and transmitted to one or more nodes in remote site . Further no updates are lost unless the entire primary site e.g. local site crashes. Communication lost between sites and may not lead to data loss but may trigger a resynchronization of the backup site e.g. remote site when connectivity is restored.

In an embodiment update module A in site retrieves from in memory keys cache A a set of keys retrieves from a local in memory data cache in memory data cache A B C and or D modified data associated with the set of keys and transmits to remote site a modification list including the set of keys and the modified data associated with the set of keys. Update module A may perform these actions in the background so that the replication is performed quickly. As an example and in reference to update module A may retrieve from in memory keys cache A keys K1 and K3 and retrieve from in memory data cache A the modified data associated with these keys. Update module A may generate a modification list including keys K1 and K3 and the modified data associated with these keys and transmit the modification list to remote site .

Saving the keys associated with the modified data in the in memory keys cache rather than the modified data or the operations executed against in memory data cache A may use less memory. Maintaining the set of keys in the in memory keys cache in site to track modifications to in memory data cache A B C and or D in site may provide advantages compared to maintaining and transmitting to a remote site e.g. site a transaction log including the operations executed against the in memory data cache. For example if user changes his address three times the transaction log sent to the remote site would include these three modifications. In contrast in the present disclosure to replicate user s change of address to the remote site update module A retrieves from in memory keys cache A key K1 user s account number retrieves from an in memory data cache the most recent address and sends to the remote site the modification list including K1 and the most recent address. Because update module retrieves the most up to date modified data associated with the keys in keys in memory keys cache A it may be unnecessary to transmit the three different address changes to the remote site.

In an embodiment site is the remote site and includes one or more site masters that receives updates e.g. the modification list and forwards the updates to the owner nodes of the respective keys. Multiple site masters may be used to mask transient failures of a site master. Requests may be load balanced to any site master or a primary site master may be selected and a failover site master may be selected if the primary site master goes down. The appropriate site master may apply an assignment function e.g. hash function to determine which nodes own which keys.

In an example node in site is the site master. Node may determine that it and node are to store the updates included in the modification list. Accordingly in memory data caches and in remote site may be updated using the set of keys and the modified data associated with the set of keys included in the modification list. In this way data from one or more in memory data caches in site may be asynchronously replicated to remote site . In an example if a key included in the modification list is not present in in memory data cache the key and its associated modified data may be a new entry into in memory data cache . Accordingly the key and the data associated with the key may be inserted as a new entry into in memory data cache .

Alternatively if a key included in the modification list is present in in memory data cache the modified data included in the modification list may replace one or more values in in memory data cache . Accordingly a key or data associated with the key in in memory data cache may be modified. Additionally if a value included in the modification list is null the data associated with the key along with the key may be deleted from in memory data cache . Accordingly in memory data cache may identify entries including the key and delete from its memory the identified entries.

When a condition is satisfied data in the in memory keys cache may be replicated to the remote site. In an example the condition is satisfied based on expiration of a timer. For instance data may be replicated every 500 milliseconds. In another example the condition is satisfied based on a threshold number of keys stored in the in memory keys cache.

In memory keys cache A B C and or D may store a plurality of keys based on modifications to in memory data cache A B C and or D. In an embodiment each node in site may own a set of keys and each update module in the respective node only transmits to the remote site keys and data associated with the keys that the respective update module owns. In an example update module A retrieves from an in memory keys cache the set of keys owned by node in site retrieves from an in memory data cache the modified data associated with the set of keys owned by node in site and transmits to remote site a modification list including the set of keys owned by update module A and the modified data associated with the set of keys owned by update module AA. Update module A may exclude from the modification list keys that are stored in in memory keys cache A but not owned by node .

A node may own a key based on various determinations. In an example a node owns a key based on an assignment function. Update module A may determine the set of keys stored in in memory keys cache A to transmit to remote site based on the assignment function. In an example the assignment function is a hash function and in memory keys cache includes one or more hash tables. Ownership may be determined based on a consistent hash function over the key set. For instance update module A in node in site may apply a hash function to a key stored in in memory keys cache A in node to determine whether to transmit to the remote site the key and its associated data. Given keys K1 K2 K3 K4 K5 K6 K7 K8 K9 K10 in site update module A may process K1 K2 and K3 update module B may process K4 K5 K6 K7 and update module C may process K8 K9 K10 . Although each of nodes and may stores keys other than the ones it owns update module A transmits to remote site K1 K2 K3 along with the data associated with these keys update module B transmits to remote site K4 K5 K6 K7 along with the data associated with these keys and update module C transmits to remote site K8 K9 K10 along with the data associated with these keys. This is merely an example and there may be other ways to determine whether a node owns a key.

A transmit node in site may transmit to remote node and or node in site the modification list including the set of keys and the modified data associated with the set of keys. In an example the transmit node sends the modification list to the site master in remote site and the site master forwards the updates to the owner nodes of the respective keys. The site master may be node node or another node in site not shown .

In response to receiving the modification list from the site master the remote node in site may send an acknowledgement to the transmit node in site to signal successful completion of the replication. If the replication of data failed e.g. if node crashes before node saved the data in the modification list the node in site that received the modification list may request from the site master in site or the transmit node in remote site that the modification list be sent again and or may fail to send to the transmit node in site an acknowledgement of successful completion of the replication. The transmit node in site may then retry sending the modification list to the failed node.

In an example if node and node receive a modification list including updates for their respective keys and node successfully saves the data in the modification list and node does not successfully save the data in the modification list node may send the modification list to only node and it may be unnecessary to send the modification list to node . Accordingly if node receives from node an acknowledgement that signals successful completion of the replication node may avoid sending to node the modification list. This provides advantageous over synchronous replication because if one node fails it may prevent the other nodes from committing that particular transaction. Further asynchronous replication of in memory caches may have a high quantity of nodes participating in the replication without significantly contributing to failure probability.

After update module A transmits to remote site the modification list update module A may remove from in memory keys cache A the transmitted set of keys. In this way the next time update module A replicates data from the in memory data cache to the remote site update module A will not re send to the remote site the previously transmitted information. In an embodiment update module A removes from in memory keys cache A the transmitted set of keys responsive to a notification that the data has been successfully replicated.

Update module A may also broadcast a message to nodes in site to remove from the in memory keys cache of the respective node the transmitted set of keys. In an embodiment update module A broadcasts the message responsive to a notification that the data has been successfully replicated.

In this way the other nodes in site that store keys associated with modified data will also store the most up to date keys associated with data to be transmitted to the remote site. For example if update module A in site sends the update message to M backup instance nodes local to site update module A may notify the M backup instance nodes that the keys and modified data associated with the keys have been successfully replicated to the remote site. In response to the notification the M backup instance nodes may remove from their respective in memory keys cache the transmitted set of keys. Additionally if update module A in site sends the update message to P key updaters local to site update module A may notify the P key updaters that the modified data associated with the keys have been successfully replicated to the remote site. In response to the notification the P key updaters may remove from their respective in memory keys cache the transmitted set of keys. Accordingly removing the transmitted set of keys from in memory keys cache A may prevent duplicate data from being sent to the remote site.

After a key is removed from in memory keys cache A the key may be inserted again if the key is identified based on another modification to the local in memory data cache. In an example the modification to in memory data cache A is part of a transaction. After key insert module A inserts the identified one or more keys into in memory keys cache A update module A commits the transaction. After update module A commits the transaction update module A may transmit to the remote site the modification list including the set of keys and the data associated with the set of keys. Further after update module A transmits to the remote site the modification list update module A removes from in memory keys cache A the transmitted set of keys.

Data stored in in memory data cache A B C and or D of site may be asynchronously replicated to remote site based on the keys stored in the in memory keys caches in site . If only one update module e.g. update module A transmits to site the modification list including the keys and modified data associated with the keys then all of the modifications may arrive at site in total order. If more than one update module e.g. update modules A B and C transmit to site the modification list then a First In First Out FIFO may be maintained per key so that the modifications are in order.

Node and or node in site may send to site e.g. to node an acknowledgement of the modification list. After node receives the acknowledgement from nodes and or in site node may send a communication to a subset of nodes in site to cause the subset of nodes to remove from the in memory keys cache the transmitted set of keys. In an example if in memory keys cache A in node in site in memory keys cache B in node in site and in memory keys cache C in node in site store key K1 and update module A owns key K1 and transmits to remote site the modification list including key K1 and receives an acknowledgement from remote site that the replication was successful then update module A may transmit to local nodes in site a message that causes these nodes to remove from their respective in memory keys cache key K1. In this way the successfully transmitted keys may be removed from the in memory keys cache and the data associated with the keys are prevented from being sent in duplicate to remote site .

When the nodes local to site receive the communication from update module A to cause the nodes to remove from their respective in memory keys cache the transmitted set of keys the nodes may assume that the replication was successful and that these keys and the data associated with these keys do not need to be transmitted to remote site . Accordingly the keys remain stored in in memory keys cache until the associated replication is determined to be successful.

Further if a node in site crashes a subset of nodes in site may determine which keys are owned by the crashed node so that another node local to site may take over responsibility from the crashed node for transmitting the data associated with the keys. In an example node owns key K1. If node crashes a set of backup instances may transmit to a set of key updaters one or more keys and the modified data associated with the one or more keys that the set of backup instances received from update module A in node in a update message. In an embodiment all the key updaters store all keys associated with modified data. In another embodiment all the key updaters store a subset of all keys associated with modified data.

In an example node in site may receive from update module A a update message including keys K1 and K3 and modified data associated with keys K1 and K3. If node detects that node has crashed node may transmit to node keys K1 and K3 and or the modified data associated with keys K1 and K3. In this way another node local to site may participate in ensuring that the keys associated with modified data e.g. K1 and K3 are not lost. Additionally update module B and or C may take over update module A s duties and own keys K1 and or K3. In an example after node crashes update module C in node takes over responsibility from local node and owns key K1 and transmits to remote site the modification list including key K1 and the modified data associated with key K1. In another example after node crashes update module B in node takes over responsibility from local node and owns key K3 and transmits to remote site the modification list including key K3 and the modified data associated with key K3.

Further when a new node launches in local site the keys owned by one or more nodes in site may be rebalanced. In an example the keys owned by each node in site are rebalanced. The newly launched node in site may take over responsibility from one or more local nodes in site and own the keys previously owned by the one or more local nodes and transmit to the remote site the modification list including the keys owned by the new node and the associated modified data.

Moreover when a site goes down or updates fail for an extended period of time a site may be marked offline. When the site comes back up it may be synchronized with the data in other sites. In an example if site crashes and comes back up the nodes in site may be synchronized with the nodes in remote site . The newly started site may grab the state from a local or remote node and the state may be transferred to the newly started site. A state transfer may include sending an update for all the keys e.g. in memory and on disk in the primary site and the mechanism already in place for replication may ensure that the updates are sent to the newly started site.

In an example if node in site crashes and comes back up one or more nodes in site may send data to site so that node has the most up to date data. In an example update module A in node may retrieve from an in memory data cache e.g. in memory data cache A B C and or D a first set of keys owned by node and data associated with the first set of keys and transmit to site a first state list including the first set of keys and the data associated with the first set of keys. Additionally update module B in node may retrieve from an in memory data cache e.g. in memory data cache A B C and or D a second set of keys owned by node and data associated with the second set of keys and transmit to site a second state list including the second set of keys and the data associated with the second set of keys. In this way portions of in memory data caches A B C and or D may be sent to site in parallel.

As discussed above and further emphasized here is merely an example which should not unduly limit the scope of the claims. For example it should be understood that one or more modules in may be combined with another module. In an example key insert module A and update module A are combined into one module. It should also be understood that one or more modules in may be separated into more than one module. In an example key insert module A is split into a first key insert module and a second key insert module.

Further although block diagram is described herein with reference to two sites there may be more than two sites without departing from the spirit of the present disclosure.

Additionally different replication mechanisms may be used between sites. For example site may be configured to backup data synchronously to site and asynchronously to another site not shown . The backup policy for each in memory data cache may be defined in a configuration file residing at the node. Further a system may have any number of sites and each site may have any number of nodes.

In an example a local site includes seven nodes and three of the seven nodes are updaters. An updater may be chosen based on its rank in a group view e.g. JGroups view . For example if the seven nodes are nodes A B C D E F and G then the first three may be updaters A B and C . When a new view is installed every member may determine based on the view and its rank whether the member is an updater. Each node may have the same sequence of views. Accordingly the determination of whether a node is an updater may be purely local and an election process e.g. involving messages being sent around may be unnecessary. When a transaction commits e.g. as part of the prepare phase of the two phase commit the node that is committing the transaction may broadcast an update message including only the updated keys not the modified data .

With respect to the two phase commit an update message may be broadcast upon the prepare message or the commit message. Broadcasting the update message upon the prepare message may include sending a second commit or rollback message. Broadcasting the update message upon the commit message may include the avoidance of blocking the transaction but risk the loss of the update if the node crashes before applying the final commit.

Each key may be listed only once. In an example if a transaction has modified data associated with key K1 20 times then modified data associated with key K2 and then modified data associated with key K3 the list is K1 K2 K3 . It may be unnecessary to send the operation type e.g. put remove replace with the update message because the updaters will fetch the associated values later.

In an example each node in the local site receives the update message but only updaters A B and C store the keys. The updaters send an acknowledgement back to the sender and this may complete the transaction. An updater owns a key when the updater is responsible for replicating the key to the remote backup site s . The updater may own a certain range of keys. The determination of which updater owns which keys may be made by using a consistent hash mapping keys to updaters. For example for keys K1 K2 K3 K4 K5 K6 K7 K8 K9 K10 updater A may process K1 K2 K3 updater B may process K4 K5 K6 K7 and updater C may process K8 K9 K10 . In another example a set of nodes N may store modified keys in memory and a set of updaters K where K

Referring to stable storage as long as no more than N 1 updaters crash at exactly the same time the keys to be processed replicated are not lost. Further if only a few updaters crash new updaters may be picked immediately and the keys may be rebalanced to ensure that they are not lost.

After the keys associated with the modified data are stored in stable storage e.g. in memory keys cache this data may be periodically sent to the remote backup site s . Each updater in the local site may have a replication task that runs when for example a queue exceeds a certain number of keys or a timer expires. The updater may then send a replicate message including the keys and data associated with the keys to the site master s which in turn apply the updates in their local site s . This may be performed using the site s configuration e.g. asynchronously synchronously with or without a transaction . When done the site master sends back an acknowledgement to the sender signaling to the sender the successful completion of the update task.

A site having N updaters may provide a certain probability against node failures e.g. stable storage . The N updaters may shoulder all of the updates distributing the work among them. Further the queues in the updaters only store a given key once. Accordingly the key may be updated only once in the backup sites and may be included only once in the replicate message saving bandwidth and CPU cycles. Further bandwidth to the cloud or costs associated with the bandwidth used to ship updates to the cloud may be reduced. Additionally storing only the keys and not their associated values in the updaters queues may result in less memory usage. Updaters may be stateless beyond storing the keys as they fetch the current value when triggering an update to a remote site. The updater fetches the current value from the in memory data cache. Further each updater may be responsible for the same set of keys so different updates to the same key end up in the same updater queue so that updates are sent in an ordered way.

In an example node in site includes three queues a first queue my keys a second queue all keys and a third queue pending keys not shown . The queues may be FIFO ordered and duplicate free e.g. a FIFO ordered set . On reception of a message to update keys in in memory keys cache A node determines whether it is a key updater. If node determines that it is not a key updater node discards the update message. If node determines that it is a key updater node may store the keys included in the update message. If no update to a remote site is in progress node inserts the keys into queue all keys and inserts the keys that node is responsible for into queue my keys. If an update to a remote site is in progress however node inserts the keys into queue pending keys. 

When a condition is satisfied e.g. a timer expires or when a queue exceeds a threshold size node block insertions into queue all keys fetches the keys from queue my keys along with the data associated with the keys from queue my keys and replicates them to the remote site s . If the replication is successful node removes these keys from queue my keys. Node may then process the keys in queue pending keys by inserting these keys into queue all keys and inserting the keys that node is responsible for into queue my keys. Node may then remove all keys from queue pending keys and unblock insertions into queue all keys. 

Further responsive to a view change node may determine which keys in queue all keys are to be processed by node and insert these determined keys into queue my keys. 

Node may replicate data to a remote site by creating a replicate message containing a hashmap with keys and values. For each key node may fetch the value for the key from the local site and add the value to the hashmap. If the value is null node may mark the value as removed. Node may send the replicate message to the site master s of the remote site s and wait for the acknowledgement of a successful replication. If node receives the acknowledgement of a successful replication node may broadcast a remove message to local nodes in site to remove those particular keys associated with the successfully replicated data.

In an example node in site sends the remove message including the particular keys to nodes local to site . The local nodes that receive the remove message may determine whether they are updaters. If a node is not an updater the node may discard the remove message. If the node is an updater the node may remove the keys included in their respective queue all keys to avoid data associated with the removed keys from being sent in duplicate to the remote site. The node may leave the keys in queue pending keys because they might have been updated again meanwhile.

If an update to the same key occurs in different sites they may be inconsistent. To avoid this inconsistency each updater may acquire local write locks e.g. by using the lock application programming interface API on the keys that are about to be replicated to the remote site. When the keys have been replicated the local write locks on the keys are released. In this way inconsistencies between the sites may be prevented.

In an embodiment the last updater determines the updated value associated with a key K. In an example updater A in London and updater B in New York City both want to replicate key K. Updater A tries to set K V1 and updater B tries to set K V2. Updater A write locks K in London and updater B write locks K in New York City and sends the replicate K V2 message to London. Updater A sends the replicate K V1 message to New York City and updater B times out trying to acquire the lock for K in London releases the write lock for K in New York City and sleeps randomly before retrying. Updater A acquires the lock in New York City and sets K V1 in New York City then releases the remote lock in New York City and the local lock in London. Updater B may wake up and retry. In an example updater B fetches the value for K which has been set as V1 by updater A and locks it locally in New York City. Updater B sends the replicate K V2 message to London acquires the lock in London and sets K V2 both in London and New York City. Updater B then releases the remote and local lock. K is now V2 in London and New York City. In this way K may be consistent across all sites.

Method includes steps . In a step a modification to an in memory data cache is detected by one or more processors the in memory data cache storing a plurality of keys and data associated with the plurality of keys. In an example key insert module A detects by one or more processors a modification to an in memory data cache the in memory data cache storing a plurality of keys and data associated with the plurality of keys.

In a step one or more keys of the plurality of keys is identified based on the modification. In an example key insert module identifies one or more keys of the plurality of keys based on the modification.

In a step the identified one or more keys is inserted into an in memory keys cache. In an example key insert module inserts the identified one or more keys into an in memory keys cache.

In a step a set of keys is retrieved from the in memory keys cache. In an example update module retrieves from the in memory keys cache a set of keys.

In a step modified data associated with the set of keys is retrieved from the in memory data cache. In an example update module retrieves from the in memory data cache modified data associated with the set of keys.

In a step a modification list including the set of keys and the modified data associated with the set of keys is transmitted to a remote site where at least one node in the remote site is updated using the set of keys and the modified data associated with the set of keys. In an example update module transmits to a remote site a modification list including the set of keys and the modified data associated with the set of keys where at least one node in the remote site is updated using the set of keys and the modified data associated with the set of keys.

It is also understood that additional method steps may be performed before during or after steps discussed above. For example method may include a step of removing from the in memory keys cache the transmitted set of keys. It is also understood that one or more of the steps of method described herein may be omitted combined or performed in a different sequence as desired.

Computer system includes a bus or other communication mechanism for communicating information data signals and information between various components of computer system . Components include an input output I O component that processes a user action such as selecting keys from a keypad keyboard selecting one or more buttons or links etc. and sends a corresponding signal to bus . I O component may also include an output component such as a display and an input control such as a cursor control such as a keyboard keypad mouse etc. . An optional audio input output component may also be included to allow a user to use voice for inputting information by converting audio signals into information signals. Audio I O component may allow the user to hear audio. A transceiver or network interface transmits and receives signals between computer system and other devices via a communication link to a network. In an embodiment the transmission is wireless although other transmission mediums and methods may also be suitable. A processor which may be a micro controller digital signal processor DSP or other processing component processes these various signals such as for display on computer system or transmission to other devices via communication link . Processor may also control transmission of information such as cookies or IP addresses to other devices.

Components of computer system also include a system memory component e.g. RAM a static storage component e.g. ROM and or a disk drive . Computer system performs specific operations by processor and other components by executing one or more sequences of instructions contained in system memory component . Logic may be encoded in a computer readable medium which may refer to any medium that participates in providing instructions to processor for execution. Such a medium may take many forms including but not limited to non volatile media volatile media and transmission media. In various implementations non volatile media includes optical or magnetic disks or solid state drives volatile media includes dynamic memory such as system memory component and transmission media includes coaxial cables copper wire and fiber optics including wires that include bus . In an embodiment the logic is encoded in non transitory computer readable medium. In an example transmission media may take the form of acoustic or light waves such as those generated during radio wave optical and infrared data communications.

Some common forms of computer readable media include for example floppy disk flexible disk hard disk magnetic tape any other magnetic medium CD ROM any other optical medium punch cards paper tape any other physical medium with patterns of holes RAM PROM EEPROM FLASH EEPROM any other memory chip or cartridge or any other medium from which a computer is adapted to read.

In various embodiments of the present disclosure execution of instruction sequences to practice the present disclosure may be performed by computer system . In various other embodiments of the present disclosure a plurality of computer systems coupled by communication link to the network e.g. such as a LAN WLAN PTSN and or various other wired or wireless networks including telecommunications mobile and cellular phone networks may perform instruction sequences to practice the present disclosure in coordination with one another.

Where applicable various embodiments provided by the present disclosure may be implemented using hardware software or combinations of hardware and software. Also where applicable the various hardware components and or software components set forth herein may be combined into composite components including software hardware and or both without departing from the spirit of the present disclosure. Where applicable the various hardware components and or software components set forth herein may be separated into sub components including software hardware or both without departing from the spirit of the present disclosure. In addition where applicable it is contemplated that software components may be implemented as hardware components and vice versa.

Application software in accordance with the present disclosure may be stored on one or more computer readable mediums. It is also contemplated that the application software identified herein may be implemented using one or more general purpose or specific purpose computers and or computer systems networked and or otherwise. Where applicable the ordering of various steps described herein may be changed combined into composite steps and or separated into sub steps to provide features described herein.

The foregoing disclosure is not intended to limit the present disclosure to the precise forms or particular fields of use disclosed. As such it is contemplated that various alternate embodiments and or modifications to the present disclosure whether explicitly described or implied herein are possible in light of the disclosure. Changes may be made in form and detail without departing from the scope of the present disclosure. Thus the present disclosure is limited only by the claims.

