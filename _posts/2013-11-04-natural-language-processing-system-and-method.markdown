---

title: Natural language processing system and method
abstract: A natural language processing system is disclosed herein. Embodiments of the NLP system perform hand-written rule-based operations that do not rely on a trained corpus. Rules can be added or modified at any time to improve accuracy of the system, and to allow the same system to operate on unstructured plain text from many disparate contexts (e.g. articles as well as twitter contexts as well as medical articles) without harming accuracy for any one context. Embodiments also include a language decoder (LD) that generates information which is stored in a three-level framework (word, clause, phrase). The LD output is easily leveraged by various software applications to analyze large quantities of text from any source in a more sophisticated and flexible manner than previously possible. A query language (LDQL) for information extraction from NLP parsers' output is disclosed, with emphasis on its embodiment implemented for LD. It is also presented, how to use LDQL for knowledge extraction on the example of application named Knowledge Browser.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09152623&OS=09152623&RS=09152623
owner: Fido Labs, Inc.
number: 09152623
owner_city: Palo Alto
owner_country: US
publication_date: 20131104
---
This application claims the benefit of U.S. Provisional Patent Application No. 61 721 792 filed Nov. 2 2012 which is incorporated by reference herein in its entirety.

Natural language processing NLP systems are computer implemented methods for taking natural language input for example computer readable text and operating on the input so as to generate output that is useful for computers to derive meaning. Examples of NLP systems applications include spell checkers grammar checkers machine translation systems and speech to text systems. Increasingly there is interest in developing methods for machines to more intelligently interpret human language input data such as text for the purpose of directing the computer as if it were another person who could understand speech. One application for such methods is search engines that receive a typed query from a person and perform web searches to attempt to generate a set of meaningful answers to the query. An important subclass of NLP systems is NLP parsers especially grammatical parsers such as Part Of Speech tagger constituency parsers dependency parsers and shallow semantic parsers such as SRL Semantic Role Labeling . Their role is to preprocess text and add additional information to words to prepare it for further usage. Current NLP systems are mostly built on top of NLP parsers and current NLP systems rely heavily on the information produced by these parsers to provide features and accuracy. Quality of the information delivered by these parsers is strongly correlated with the efficiency of NLP systems.

All current parsers are dependent on corpora and therefore on the context in which they were written. Typically corpora are in a context of correctly written grammatically correct sentences and common syntactic structures which are manually annotated by humans. The system is then trained using this corpus.

This is one reason that traditional NLP parsers are most accurate on the same type of content they were trained on the same corpus . That is why always changing language such as user generated content e.g. reviews tips comments tweets social media content presents a challenge for NLP parsers built with machine learning techniques. Such content often includes grammatically incorrect sentences and non standard usage of language as well as emoticons acronyms strings of non letter characters and so on. This content is constantly changing and expanding with different words or syntactic structures. All of this content has meaningful information and it is easy to understand by humans but it is still difficult for NLP applications to extract meaning out of it.

One way in which current NLP parsers can be updated or improved for better accuracy or extracting additional information is to modify the existing corpus or create a new corpus or re annotate existing one and retrain the system with it to understand new content. However this is a tedious costly and time consuming process. For example all current NLP parsers are using corpus as a training data annotated by linguists with predefined tags e.g. Penn Treebank especially use machine learning algorithms.

If there was a need to distinguish the pronominal or adjectival aspect of that giving them different POS tags in different context linguists would need to manually re annotate all the sentences in the whole corpus that contain the word that regarding the context of each usage of that and retrain the parser.

Building a particular application on top of an NLP parser requires building a module to transform the NLP parser output into usable data. The application using the parser s output could be coded in a programming language use a rule based systems or be trained with machine learning techniques or created with combination of any of the above methods.

Using NLP parsers can be challenging due to the need to understand the structure of the output and parameter s requires expert knowledge . One of the challenges in NLP parsers is to provide constant consistent structure of information. Also the output of the NLP parsers rely on the quality of the input text data.

By using grammar parsers in each case you will get different notations for the object that John likes.

In constituency parsers the number of levels depth in a parse tree depends on the length and the grammatical structure of the processed sentences. In the given example above the first sentence has 3 levels the second sentence has 0.5 levels and the third example has 6 levels in a tree representation.

In state of the art dependency parsers the structure of the output and number of levels in the dependency tree representation also vary. Adding even one word in the sentence can alter the grammatical information of all the other words.

The given example about John would produce different structure for each sentence. The first sentence require extracting dependents of dobj relation connected to the word likes in the second all dependents of xcomp relation connected to the word likes and in the third example there is a need for analyzing all governors connected to dependents of xcomp related to the word likes 

All of the above is the reason why it is difficult for people and especially non linguists e.g. developers analysts to use the parser output and write rules to adjust it to their current needs. For example to write an information extraction engine to extract information about product features from reviews you could use a constituency parser or a dependency parser but you need to write complex algorithms to search through the parse tree. To move to another domain e.g. extracting information from twitter the algorithms must be redesigned and part of the code rewritten.

To deal with these problems NLP systems use machine learning techniques. This approach has some limitations in terms of accuracy and amount of extracted information.

There are query languages to process structured data e.g. SQL for relational databases Cypher for graph databases SPARQL RDF tagged texts resource description framework but there are no languages designed directly to query the structure of the natural language output of the NLP parser .

It would be desirable to have an efficient framework for storing information decoded from text. It should provide an invariant and consistent way of storing information which would be insensitive to different types of input. Having such a framework it would be possible for non experts to write efficient rules on top of the NLP parser s output.

It would be desirable to have a parser for natural language processing that is built fully algorithmically so it allows for constantly improvement in accuracy and the addition of new features without building or re annotating any corpus. It would desirable to have an NLP system that is more capable than current NLP parsers of dealing with non typical grammatical input deals well with constantly changing language on the web and produces accurate output which can be stored into an efficient framework of information.

It would also be desirable to have a query language that can be used on the logical layer across different input contexts allowing humans to write efficient rules for extracting information and that is capable of effectively leveraging many NLP systems.

Embodiments of inventions disclosed herein include improvements on current NLP systems and methods that are especially relevant to processing input that consists of plain text from different types of context. As such the embodiments disclosed herein provide a highly accessible platform for natural language processing. The output of the disclosed NLP system is easily leveraged by various software applications to analyze large quantities of text from any source in a more sophisticated and flexible manner than previously possible.

The Language Decoder is a novel fully algorithmic NLP parser that decodes information out of text and stores it into a three level framework which is capable of handling various type of texts from different domain like reviews news formal documents tweets comments etc.

This three level hierarchical framework of processed text is leveraged by embodiments of a language decoder query language LDQL as further described herein. The LDQL is particularly easy to use for developers without requiring specific linguistics training. However other rule based query languages could conceivably be developed for extraction by query of information from text processed by the LD.

Other systems and applications are systems including commercial systems and associated software applications that have the capability to access and use the output of the NLP system through one or more application programming interface APIs as further described below. For example other systems applications can include an online application offering its users a search engine for answering specific queries. End users includes individuals who might use applications through one or more of end user devices A. User devices A include without limitation personal computers smart phones tablet computers and so on. In some embodiments end users access NLP system directly through one or more APIs presented by NLP system .

The LD output can also be operated on by embodiments of an LD query language LDQL . LDQL is described according to various embodiments below as an improved query language designed to take advantage of LD output . However LDQL can also operate on the output of any prior NLP system. Also provided in various embodiments is a higher level of APIs as compared to LD output for providing machine learning systems and other systems applications more intuitive access to LD output . Other systems applications can include semantic databases Freebase Wordnet etc.

In general LD high stack APIs are relatively easy to use to for other systems seeking to manipulate LD output . However machine learning systems and other systems applications can also directly write queries using LDQL .

Contrary to all prior art word taggers present word tagger does not use any machine learning techniques and contains only human written disambiguation rules.

Word tagger rules are described by a language resembling well known regular expressions which is comprehensible and easy to change for human operators. Hence the process of making changes is relatively fast especially in comparison to the prior art machine learning based algorithms.

A set of rules is divided into two groups referred to herein as exact group and inexact group along with different usages within the tagging algorithm.

Contrary to machine learning approaches the present word tagger uses its own dictionary to recognize words and assign all possible tags to them. It is proven that word taggers using machine learning techniques achieves higher accuracy for tagging known tokens words than unknown. The term known token means that token appeared in the training corpus at least once. Differential in accuracy for known and unknown tokens is about 7 8 for good parsers see for example http nlp.stanford.edu pubs CICLing2011 manning tagging.pdf Embodiments of the present word tagger allow new words to be added directly to the referenced dictionary with all possible tags. For example google can be added with tags related to noun as well as verb. The present word tagger contains rules responsible for resolving that kind of ambiguity so it can automatically handle that problem.

The input to the word tagger consists of plain text. The text can come from any source such as an article a document user generated content and any other source. Referring to a word tagging process according to an embodiment is illustrated. In an embodiment the input to the word tagger element A is text that is tokenized into sentences and words by preceding elements tokenize A and sentence divider A. A dictionary is accessed to find words with possible tags. All possible tags are assigned to each word . The words assigned with all possible tags are input to a dehomonymizer which resolves ambiguities i.e. situations in which for one word there is more than one possible matching tag . The dehomonymizer operates using exact rules and inexact rules as further described below. After ambiguities if any are resolved tagged text is output by the word tagger A. The word tagger output consists of text tokenized into sentences and words each of which has assigned exactly one tag.

Exact rules and inexact rules are two groups of rules. Each rule examines the context of a word with an ambiguous tag that is more than one possible tag . Such context is not limited or restricted it can address any element of the text in both directions at the same time. This is a range aspect of the claimed invention describing the facility to access or examine any element of the text before or after an ambiguous element without limit.

Another aspect of the word tagger A is the manner of expression employed. In order to examine the context of a word rules permit the creation of patterns and sub patterns which address words and or their tags and which can be subject to following operations 

This form of rules expands their expressiveness in comparison to prior static look ups and ranges next word is . . . there is a . . . word in range of 3 words before etc. This in effect allows better description of situations in which a given rule should apply.

In an embodiment the rules are applied separately for each sentence according to following algorithm 

At an exception indication is received within the system . It is determined which word caused the exception . The indicated word is searched for in the dictionary . If the word is not in the dictionary it is added to the dictionary at . Then it is determines whether possible tags for the newly word have been previously defined. If no tags were previously defined a new set of rules is created for a new set of tags is created at . Then if the exception is resolved at the process ends. If the exception is not resolved a check is performed to find which rule was responsible for the exception . The responsible rule is edited or a new rule is added . If at possible tags for the newly added word have been previously defined the process continues to .

If after checking the word is in the dictionary it is determined which rule is responsible for the exception and the responsible rule is edited or a new rule is added . In an embodiment the editing is performed by a person. In another embodiment the editing is performed by a machine according to a separate rule making algorithm.

Referring to the LD includes a predicate finder B and a clause divider C. The output of the word tagger A is received by the predicate finder B. Predicate is understood here as a function over arguments i.e. it consists of a main verb grouped with other verb components like modals auxiliaries negations and particles contrary to the traditional approach where predicate also contains other sentence elements like objects predicative expressions and adjuncts. Predicate finder B can include any of the known predicate finder methods. The output of the predicate finder B is received by the clause divider C which performs a clause divider methodology according to embodiments.

Embodiments of the clause divider C execute an algorithm emulating human reasoning behind dividing text into clauses. To this end a sequential approach is employed rather than applying patterns to an entire sentence. In effect collections of one or more words scraps are sequentially considered which is similar to what a human actually does while hearing or reading a sentence and an attempt is made to assign each scrap to build a clause on the fly constantly trying and rejecting different alternatives. This approach simplifies aspects of language decoding such as handling of nested relative clauses and facilitates the creation of simpler and more accurate rules than those based on patterns.

According to embodiments the clause divider provides division for clauses. Clauses can possess at the most one predicate either finite or non finite like infinitives participles and gerunds and all of its arguments. The LD provides connections between clauses based on criteria concerning their function towards their superiors. Clauses can be connected directly or through nodes. Nodes are sentence elements provided to connecting clauses e.g. coordinating or subordinating conjunctions. The LD provides unique classifications of clauses which correspond to the LD system classification of phrases. Main clauses are distinguished. Other clauses can function as subjects objects complements and attributes and therefore those can be labeled with a proper function name e.g. attribute clause .

1. Embodiments provide implicit division into clauses. In referenced parser derivation from the relations structure is required.

2. Coordinating nodes elements connecting two coordinated clauses e.g. and but of are distinguished by embodiments. Referenced parser does not provide a distinction between a relation connecting two coordinated clauses and a relation connecting two coordinated phrases or words. In addition elements connecting a subordinated clause to its superior clause e.g. which after when are distinguished by embodiments. In typical dependency parser the main connection holds between two verbs representing the clauses hence the equivalent of a subordinate node has to be connected with a proper verb. 3. In the clause divider C classification is based on different criteria than in the typical dependency parser and in consequence not every type from one classification is equivalent to a subset of the types of the other classification although in a number of situations it is . 4. Types of clauses in the LD are equivalent to types of phrases making the LD system more coherent in comparison to the typical parser. For example some of referenced parser s relations corresponding to clause types are common with those for phrases and or words.

1. Typical constituency parser provides classification that is based on criteria concerning grammatical construction. This is in sharp contrast to the clause divider C whose criteria is based on a clause s function towards its superior. In effect every type from one classification can match almost any type from the other depending on the particular situation. 2. The LD treats nodes connecting clauses e.g. and but which after as separate elements on a clause level whereas the referenced parser includes them into the following clause. The Process of the Clause Divider

More precisely a scrap is a maximal set of words that will certainly form a phrase either by itself or along with some other scrap s . That is if two elements are able to form two separate phrases but this is not evident at the point of creating scraps they become separate scraps. There is an exception to the foregoing. Specifically prepositions are joined in scraps with following noun phrases in principle despite the fact that they will later be divided into two separate phrases.

At the clause divider C makes a preliminary estimate of which scraps are the strongest candidates to introduce a new clause that is which scraps are responsible for dividing the sentence into clauses. The output of the preliminary estimate process is received by a main divider engine which includes a scrap dispatcher and a correctness evaluator. The scrap dispatcher assigns scraps to appropriate clauses. In an embodiment this assignment is based on constant interaction between the scrap dispatcher and the correctness evaluator. The correctness evaluator evaluates the decision made by the scrap dispatcher by determining whether the decision generated a correct clause or not. The output of the main divider engine is received by a nodes extraction process . This process extracts nodes as separate elements on a clause level. Relationships between clauses are established by a clause connection detection process . Clause types are detected e.g. subject object complement by a clause type classification process . The output of the clause divider is text divided into sentences and clauses which contain words along with their tags from the word tagger grouped into scraps. The grouping serves only as an aid in prior and subsequent processes and is not itself a crucial element of the output. The words can also be considered as assigned directly to clauses. Moreover each clause has its proper grammatical information assigned along with type and connection to its superior clause.

At an exception indication is received within the system . It is determined if the clause is correctly divided . If not it is determined if the clause dividing scrap was chosen correctly from among candidates scraps . If not a new candidate is added or the rules responsible for choice are changed .

After the changes or if it was determined that the dividing scraps were chosen correctly it is determined whether scraps were correctly distributed to clauses. If not scrap dispatcher or correctness evaluator rules are adjusted .

After any changes are made to the scrap dispatcher rules or correctness evaluator rules or if it was earlier determined that either scraps were correctly distributed to clauses it is determined whether connections between clauses are correct. If not the rules governing connection determination are adjusted .

After adjusting the determination rules or if the connections between clauses were determined as correct it is determined at whether the clause classification is correct. If the clause classification is correct the process ends. Otherwise the clause classification rules are adjusted .

Referring to the nodes are extracted. In the considered example there are no scraps that function as nodes hence the process leaves the clause unaltered. Afterwards the connections of clauses are detected. Because the clause I saw last night is interjected right after NP scrap we consider it a dependent clause and hence determine a connection from it to The man became my friend clause. Moreover in this situation a connection between the clause I saw last night and the phrase The man is established. The clause connection detection then ends.

Referring to the clause The man became my friend has no connection to a superior clause hence it obtains the type main . The clause I saw last night is connected to the superior clause and to the superior phrase as well thus it obtains the type complement . The process of the Clause Divider C ends and returns output composed of the sentence The man I saw last night became my friend which contains two clauses The man became my friend and I saw last night along with their types connections and respective scraps.

By analyzing cases of wrong divisions connections and types if clauses or adjusting the clause divider C into different type of content one can decide whether to 

Referring briefly to the output of clause divider C is received by an accommodator module D. is a block diagram of an accommodator module D architecture according to an embodiment. In an embodiment the accommodator D uses a set of types of phrases corresponding to simple grammatical types such as predicate subject object complement etc . The accommodator employs a sequential algorithm which in one embodiment is based on hand written rules but embodiments are not so limited Maintaining control over rules and having the ability to modify and add rules to the system at will enables the LD to work properly on different kinds of domains or language contexts at the same time. This tight control over rules and the adaptability of the rules also provides the ability to handle new domains while avoiding any distortion in the original or prior domain s . A separate semantic database is referenced to resolve ambiguities with connections e.g. a semantic n gram database . In addition the accommodator D module uses a method for determining connections between phrases the it test described below with reference to .

The accommodator D receives words grouped into scraps and further into clauses along with connections and types . In other embodiments the accommodator is based on words grouped directly into clauses. A phrase creator module detects boundaries of every phrase. Then a phrase connection detection module detects connections between phrases relying on a semantic n gram database . A phrase type classification module denotes types for each phrase in every clause in a sentence. Then a word component classification module assigns components to each word within all of the phrases. Finally a word connection detection module detects connections between each word within a phrase.

The accommodator D output consists of words grouped into phrases which are further grouped into clauses. The phrases and words have proper connections and types assigned. In an embodiment the accommodator D output is the final output of the LD system or LD output API as shown in . As further described in other parts of this document the output is useable by many other systems via APIs or other programming tools or interfaces. In an embodiment types of the phrases are determined after the connections they are evaluated on the basis of the contained words along with their tags types of the scraps from which a phrase comes and types of scraps from which connected phrases come. Although it has proved efficient and accurate to first connect the phrases and classify them afterward it is just as viable to do it the other way around.

An advantage of the accommodator over prior modules that perform analogous tasks is that the accommodator D method determines connections between phrases and their grammatical functions in way that is similar to the way in which a human processes language. The accommodator D sequentially considers separate scraps and chooses for a set of syntactic possibilities the one that semantically makes most sense. In an embodiment this is achieved in part though using a database containing n grams representing these syntactic possibilities along with the quantity of their occurrence in a large corpus. Ambiguities in interpretation are always reduced to a set of syntactic possibilities consisting of a few elements and are then solved on a semantic basis. The result is simple and intuitive for a human to understand. Thus a human can readily see and understand the decisions the LD makes and when appropriate correct its mistakes by modifying rules so that the system is continually improved.

At an exception indication is received within the system . It is determined if the phrase is built correctly . If not it is determined if the problem results from the phrase creator . If so the rules of creating phrases are adjusted . If it was determined that the wrongly built phrase did not result from phrase creator it is then determined whether it is a result of scraps . If so the scrapper engine is repaired . Once the scrapper engine was repaired it is determined whether the connections between phrases are correct . If these connections are correct it is determined whether the classification of phrases is correct . If the classification of phrases is correct the process ends. If the classification of phrases is not correct the classification rules are changed .

Returning again to if the phrase is built correctly it is determined whether connections between phrases are correct . The determination of is also resorted to when the exception is determined not to result from scraps and when the phrase creator rules have been corrected . If at connections between phrases are correct the process proceeds to . If at connections between phrases are not correct connections rules are changed or the n gram semantic databased entries are edited . Next it is determined whether the classification of phrases is correct . If the classification of phrases is not correct the classification rules are changed .

If the rules of creating phrases were adjusted or if it was determined that the problem does not result from scraps or that the phrase was built correctly it is determined if the connections of the phrases are correct . If no the connection rules are changed or the semantic n gram database is edited . After these changes or if the connection between phrases is determined as correct it is determined if the type of the phrase is correct . If so the process ends. Otherwise the rules for determining type of phrase are adjusted .

The accommodator employs an unambiguous criterion for determining connections between phrases which causes its performance to be reliable and its output to be easily understood by humans. This allows the output of the LD to be more readily incorporated into other existing language applications and their respective rules. As an example to determine the connection between certain prepositional phrases the accommodator employs the so called it test which is illustrated in . An it algorithm according to an embodiment is based on an implementation of the it test aided by the semantic n gram database . Referring to the example of consider the sentence consisting of one clause I recommend coffee with sweetener for your father. To determine if a given clause element is superior it is replaced with it . If the original meaning of the sentence is not preserved the replaced element is indeed the superior element. Otherwise the replaced element is not the superior element. As shown for this clause it is determined that coffee is a superior phrase to prepositional phrase with sweetener 

Referring to the phrases are then classified. The Construction worker phrase gets SUBJECT for its type because of its position and connection to the predicate. The predicate phrase type was assigned earlier by the clause divider module C. The coffee phrase gets OBJECT for its type because of its position and relationship to the predicate. The with scrap type is changed to a PREPOSITION phrase type. Referring to the classification of phrases continues. The milk phrase gets ATTRIBUTE for its type because of its relationship to a preposition phrase. And gets CONNECTOR for its type because it links other phrases Cigarettes gets OBJECT for its type because of its relationship to a particular connector.

The next operation is word categorization. The word construction gets a specifier component because it is a noun specifying a phrase core. Referring to word categorization continues. The rest of the words in the sentence are classified as core components because they are single within corresponding phrases and are not modifying any other phrases. Next words are connected within phrases. All specifying words are connected to the core of the respective phrase. Other words in the sentence are not considered here because they are single within their corresponding phrases.

At an exception indication is received within the system. It is determined if the input text is correctly tokenized . If not adding or changing existing rules in the tokenizer is performed and the process proceeds to determining whether the text is correctly divided into sentences. If at the text is not correctly divided into sentences adding or changing existing rules in the sentence divider is performed and then it is determined if the word tagger worked correctly . is also performed if the test is correctly divided into sentences as determined at . If the word tagger did not work correctly then a word tagger exception resolution process is initiated see .

After the changes it is determined whether the predicate finder worked correctly . If not adding or changing existing rules in the predicate finder is performed .

After the changes or in case if it is determined that the clause divider worked correctly it is then determined whether the clause divider worked correctly . If not clause divider exception resolving process is started see .

After the changes or if it is determined that the clause divider worked correctly it is then determined whether the accommodator worked correctly . If not an accommodator exception resolving process is started . If the accommodator worked correctly the process is at an end.

The output of the LD module is a three level framework for storing information words phrases and clauses . The three level structure enables efficient organization of information. Embodiments of a three level LD output framework capture a maximum amount of the information coded into text while maintaining the simplest possible structure for easy access. In an embodiment an efficient framework is designed for storing information and then using only algorithms written by humans rather than machine learning techniques and a corpus to decode the information from any given text. The output of the LD thus conveys the logical structure of information stored in text along with the grammatical structure.

The three level LD output structure is compositional and predictable and consists of a relatively small number of elements on each level. Having a minimal set of components and relations speeds up the learning process simplifies writing extraction rules and can reduce chances of miscategorization. The output of the LD is effectively an easy to access interface standardizing the information coded into natural language.

A three level structure consisting of word level phrase level and clause level stores the information coded in text. The LD is a three level structure irrespective of the text processed through the system. The text could be short simple sentences written in proper grammar short non proper grammar twitter content or long non proper grammar reviews. This contrasts with prior systems in which the parameters are responsible for attributes and the structure at the same time which can produce varying results depending on the context of some fragment of text.

When using the LD parser in each case one gets consistent notation for the object which John likes. In the first example math is an object in the second example to learn is an object clause its role is the same but on a different level and in the third example learning math in the evening is also an object clause. This approach allows separation of the grammatical layer from the logical layer so that a single rule can cover many different syntactic structures of the sentence.

As a result information extraction rules written on top of the LD are efficient. Fewer rules need to be written to capture more information and the information is less ambiguous.

In an embodiment a clause is a group of phrases that form a single minimal statement. Its internal meaning can be analyzed separately and this is done by the LD on a phrase level but only its context in combination with other clauses can lead to understanding the text and its meaning. This is caused by the fact that the original literal meaning of the clause is often significantly altered by its relation towards some other clauses. The LD provides such information about relations between clauses as well as their functions in the whole utterance.

The three level framework consists of elements on each level and allows for the addition of layers storing new kinds of information. This three level structure allows integration of the decoded information with semantic databases e.g. Freebase Wordnet ontologies and taxonomies in order to add additional semantic information into existing components on the phrase and or word level.

The clause level has an additional layer of information about the tense e.g. present simple past simple and construction e.g. positive negative question . In an embodiment different layers of information are kept separate for several reasons. Layers of abstraction can be easily formed by combining only the relevant types of information and ignoring the others e.g. if one needs only information about phrases division and not their types . It is possible to add other new layers e.g. coreference in addition to the existing ones without distorting the already present information thanks to the separation of layers.

The three level framework can be seen and treated as an interface for information extraction from text. The information can be coded into different layers of the text. For example in He owns that red car the information about the car is described as red on the word level. In The boy threw that ball the information about which object was thrown is coded on a phrase level. In It started to rain after we got home the circumstances are stored on the clause level. It is also possible to include higher levels on which the information can be stored e.g. the causation can be seen as a pattern of clauses connected with certain nodes If you eat too much chocolate then your stomach will hurt therefore the information is coded into a higher level than the clause level. Likewise it is possible to include yet higher levels building patterns on top of other patterns. The three level framework of the LD output reflects the that natural division of information into separate levels and provides fundaments for creating higher level patterns to capture information stored in multiple sentences.

LDQL is a declarative domain specific querying language for text structuring the output of the NLP system s information extraction process. It is based on first order predicate calculus and enables users to predefine their own formulas including recursive definitions .

LDQL queries can express a wide range of actions from simply listing all subjects to actions as complicated as finding opinions about people.

LDQL queries have SQL like syntax which is relatively easy to write and read for a human operator and does not require linguistic knowledge from the operator. The queries can be also created automatically which is described below.

In an embodiment the implementation of the LDQL is optimized for the LD however LDQL can also be implemented for other text structuring NLP systems e.g. Stanford Parser or Illinois SRL.

It is possible e.g. with the use of LD and LDQL to formulate queries in natural language and then translate them to high level LDQL queries.

There are some prior art query languages which were used for NLP e.g. Prolog SPARQL IQL . As compared with them LDQL has some unique features 

LDQL queries have SQL like syntax a SELECT section containing goals of extraction an optional FROM section for fixing the search range search by clauses sentences or whole text at once and an optional WHERE section containing restrictions on extracted objects in the form of syntactically sugared first order formulas where the variables range over a three level structure of words phrases and clauses regardless of particular parser s output structure .

For example to extract pairs of subject predicate phrases from given text we could use the following LDQL query 

LDQL s orientation towards formula composition encourages users to building new formulas out of previously defined ones which we consider crucial in dealing with natural language s complexity for example instead of the following pair of queries.

This represents the first use of full first order query language for unstructured text information extraction. The presence of an EXISTS quantifier in addition to the Boolean connectives AND OR and NOT present in e.g. SQL Prolog or in some flavor SPARQL with the ability to write recursive formula definitions makes LDQL an expressively stronger formalism than e.g. pure SQL Prolog or even SPARQL which may have some quantification constructs but no recursive formula definitions . An example a benefit of having such an expressive power also present in e.g. G del or indirectly Cypher or any imperative language like Java is the ability to describe closures of relations 

Suppose we would like to know whether two phrases are linked by a sequence of one or more f connections. In LDQL we could simply write a recursive formula 

Which literally reads x and y are linked if x y or x z for some z such that z is linked with y notice the circularity recursion . Because of LDQL s expressive strength some measures need to be taken to avoid infinite evaluations the x z AND NOT z x and z x AND z y restrictions .

LDQL could be implemented to operate on the output of any NLP text structuring system e.g. Stanford Parser provided that the implementation will provide access to attributes that the given system offers.

LDQL can be connected to external semantic bases e.g. Freebase Wordnet lexical data bases e.g. Wordnet domain specific data bases ontologies word banks taxonomies etc in order to support it with more semantic content.

LDQL rules can be hand written generated automatically or semi automatically by either connecting LDQL to sources of semantic content as above or by machine learning means in the case of LD s output it is possible to extract common structures out of annotated text corpora by means of unification heuristics or e.g. Koza s genetic programming or both or generated semi automatically by merging any of the methods mentioned.

The LDQL parser module processes text of LDQL script i.e. list of definitions and lists of queries in order to produce its abstract representation.

The optimizer module takes the output of parser and performs additional annotations to guide an LDQL compiler module

The compiler module takes an annotated abstract representation and generates a php c some imperative language output program which given valid NLP parser output returns the query s results for that output.

The word tagger A labels each word with a tag from the proprietary tagset previously described. The predicate finder B builds predicates from verb components. Then the clause divider C joins words into scraps groups them into separate clauses and determines relationships and types of those clauses. Next the accommodator D converts scraps within clauses into phrases and determines relationships and types of those phrases. At the end of the process the accommodator determines types and relationships of each word within each phrase. The output of the accommodator as shown is in a human readable form in contrast to tree style output of various prior systems. The original sequence of the input words is preserved and the relationships types tags etc are simple to view.

With using output of Language Decoder it is possible to build language understanding applications or language understanding engines. It can be done by applying on top of LD one or more of the following LDQL rules raw code e.g. PHP Java C NET C machine learning algorithms supervised or unsupervised .

The Language Decoder is the component technology for building applications in many areas including but not limited to 

Systems built on top of the language decoder can represent many domains including but not limited to 

Bubble visualization is a concept of showing a multi level structure of knowledge representation LDQL extracted in a form of clickable bubbles. This representation can vary in size color and position of bubbles representing frequency and additional properties for example the color palette can represent polarity of detected object sentiments . This multilevel approach allows a user to browse e.g. zoom in or zoom out through summarized knowledge encoded in analysed text which can help to understand the wider context of the data along with more exact properties of extracted data.

This concept does not restrict data to a specific format or order. Pairs object opinion triplets quadruples suggestion aim suggestion modifier aim modifier and many more are all suitable here. Also additional categorization can be applied on top of extracted data to improve knowledge integrity and understanding. This can be achieved in multiple ways starting from applying external lexicons or semantic databases e.g. Freebase FrameNet through human made categorization to logical correlations.

The bubble visualization concept is used in the information extraction example application described below.

With reference to after choosing a particular restaurant by search input or by clicking on one from a list of popular restaurants the application displays a set of bubbles grouped into five clusters. The central bubble of each cluster represents a different meta category. Each meta category has different sets of category bubbles attached to it. This set depends on what people wrote in reviews about chosen restaurant.

With reference to after clicking on a category bubble the application displays a set of extracted objects attached to the chosen category FOOD in this example . Bubbles can differ in size and color to visually convey information. In this example the size of a bubble represents object frequency in processed user reviews. More frequent objects appear on bigger bubbles. The present example shows that people wrote more about meal and dish than course or amount. The color of a bubble can vary from green through orange to red to represent the sum of positive and negative opinions about the particular object. In the present example object portion has relatively more negative opinions than object brunch so it is closer to being red in color.

With reference to after clicking on an object bubble the application shows the set of opinions about the chosen object portion in this example . Bubbles can differ in size and color here as well. Here the size of a bubble or radius represents opinion about the particular object frequency in processed user reviews. In this example small portion appeared more frequently in reviews than good portion. In an embodiment the color of the opinion bubble is constrained to three values red green and grey. Red opinions are meant to be negative green positive and grey are treated as neutral. In this example opinions about size of a portion are rather negative small tiny extremely small but opinions about general quality of portion are certainly positive good perfect healthy .

Referring to double clicking on an opinion bubble shows a general view of every object which was connected to the chosen opinion. This view gives a better insight into what generally despite categorization was related to the chosen opinion good in this example . For example the user can look up what was good in the selected restaurant. Bubbles shown in this screen have the same properties color and size as in previous screens.

Referring to a similar view can be obtained by clicking on a central category bubble on the screen of . This displays a graphic of a set of opinions about an entire category FOOD in this example which means that all of those opinion bubbles are related to some objects categorized as FOOD.

Referring to the application provides an option for comparing two restaurants. By placing two instances of bubbles next to each other the user can browse through categories and objects in one instance representing restaurant one and simultaneously choices are transferred to the second instance restaurant two showing differences.

Bubble size and bubble color are just examples of visual properties that can be used to convey the desired information. Any other visual characteristic that can be varied in a similar manner would be just as appropriate.

The various functions or processes disclosed herein may be described as data and or instructions embodied in various computer readable media in terms of their behavioral register transfer logic component transistor layout geometries and or other characteristics. Computer readable media in which such formatted data and or instructions may be embodied include but are not limited to non volatile storage media in various forms e.g. optical magnetic or semiconductor storage media and carrier waves that may be used to transfer such formatted data and or instructions through wireless optical or wired signaling media or any combination thereof. Examples of transfers of such formatted data and or instructions by carrier waves include but are not limited to transfers uploads downloads e mail etc. over the internet and or other computer networks via one or more data transfer protocols e.g. HTTP FTP SMTP etc When received within a computer system via one or more computer readable media such data and or instruction based expressions of components and or processes under the system described may be processed by a processing entity e.g. one or more processors within the computer system in conjunction with execution of one or more other computer programs.

Aspects of the systems and methods described herein may be implemented as functionality programmed into any of a variety of circuitry including programmable logic devices PLDs such as field programmable gate arrays FPGAs programmable array logic PAL devices electrically programmable logic and memory devices and standard cell based devices as well as application specific integrated circuits ASICs . Some other possibilities for implementing aspects of the system include microcontrollers with memory such as electronically erasable programmable read only memory EEPROM embedded microprocessors firmware software etc. Furthermore aspects of the system may be embodied in microprocessors having software based circuit emulation discrete logic sequential and combinatorial custom devices fuzzy neural logic quantum devices and hybrids of any of the above device types. Of course the underlying device technologies may be provided in a variety of component types e.g. metal oxide semiconductor field effect transistor MOSFET technologies like complementary metal oxide semiconductor CMOS bipolar technologies like emitter coupled logic ECL polymer technologies e.g. silicon conjugated polymer and metal conjugated polymer metal structures mixed analog and digital etc.

Unless the context clearly requires otherwise throughout the description and the claims the words comprise comprising and the like are to be construed in an inclusive sense as opposed to an exclusive or exhaustive sense that is to say in a sense of including but not limited to. Words using the singular or plural number also include the plural or singular number respectively. Additionally the words herein hereunder above below and words of similar import refer to this application as a whole and not to any particular portions of this application. When the word or is used in reference to a list of two or more items that word covers all of the following interpretations of the word any of the items in the list all of the items in the list and any combination of the items in the list.

The above description of illustrated embodiments of the systems and methods is not intended to be exhaustive or to limit the systems and methods to the precise forms disclosed. While specific embodiments of and examples for the systems components and methods are described herein for illustrative purposes various equivalent modifications are possible within the scope of the systems components and methods as those skilled in the relevant art will recognize. The teachings of the systems and methods provided herein can be applied to other processing systems and methods not only for the systems and methods described above.

The elements and acts of the various embodiments described above can be combined to provide further embodiments. These and other changes can be made to the systems and methods in light of the above detailed description.

In general in the following claims the terms used should not be construed to limit the systems and methods to the specific embodiments disclosed in the specification and the claims but should be construed to include all processing systems that operate under the claims. Accordingly the systems and methods are not limited by the disclosure but instead the scope of the systems and methods is to be determined entirely by the claims.

While certain aspects of the systems and methods are presented below in certain claim forms the inventors contemplate the various aspects of the systems and methods in any number of claim forms. For example while only one aspect of the systems and methods may be recited as embodied in machine readable medium other aspects may likewise be embodied in machine readable medium. Accordingly the inventors reserve the right to add additional claims after filing the application to pursue such additional claim forms for other aspects of the systems and methods.

