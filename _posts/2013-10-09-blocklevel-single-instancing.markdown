---

title: Block-level single instancing
abstract: Described in detail herein are systems and methods for single instancing blocks of data in a data storage system. For example, the data storage system may include multiple computing devices (e.g., client computing devices) that store primary data. The data storage system may also include a secondary storage computing device, a single instance database, and one or more storage devices that store copies of the primary data (e.g., secondary copies, tertiary copies, etc.). The secondary storage computing device receives blocks of data from the computing devices and accesses the single instance database to determine whether the blocks of data are unique (meaning that no instances of the blocks of data are stored on the storage devices). If a block of data is unique, the single instance database stores it on a storage device. If not, the secondary storage computing device can avoid storing the block of data on the storage devices.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09058117&OS=09058117&RS=09058117
owner: CommVault Systems, Inc.
number: 09058117
owner_city: Tinton Falls
owner_country: US
publication_date: 20131009
---
This application is a continuation of U.S. patent application Ser. No. 12 647 906 filed on Dec. 28 2009 entitled BLOCK LEVEL SINGLE INSTANCING which claims the benefit of U.S. Patent Application No. 61 180 791 filed on May 22 2009 entitled BLOCK LEVEL SINGLE INSTANCING and is related to U.S. patent application Ser. No. 12 565 576 filed on Sep. 23 2009 entitled SYSTEMS AND METHODS FOR MANAGING SINGLE INSTANCING DATA each of which is incorporated by reference in its entirety.

Single instancing in a data storage system typically involves attempting to store only a single instance of a file on a storage device. In certain single instancing systems a separate folder on the file system of the storage device is created for each single instancing storage operation performed. Each file that has been single instanced is stored as a separate individual file in the separate folder.

Because there may be numerous computing systems in the data storage system each requiring one or more storage operations these techniques may result in the creation of numerous folders each containing numerous files. For example if there are hundreds of computing systems each having thousands of files backing up or copying all of these files may potentially result in the creation of millions of files on the storage device.

Certain file systems of storage devices may not be capable of adequately providing for storing such large numbers of files. Other file systems may be equipped to handle storing millions of files or more but may not perform optimally in such situations.

The need exists for systems and methods that overcome the above problems as well as that provide additional benefits. Overall the examples herein of some prior or related systems and their associated limitations are intended to be illustrative and not exclusive. Other limitations of existing or prior systems will become apparent to those of skill in the art upon reading the following Detailed Description.

The headings provided herein are for convenience only and do not necessarily affect the scope or meaning of the claimed invention.

This application describes in detail among other things systems and methods for single instancing alternatively called deduplicating blocks of data in a data storage system alternatively called a data storage network a data storage environment or a data storage enterprise . The data storage system stores single instanced blocks of data alternatively referred to as deduplicated blocks of data in one or more files and maintains one or more data structures e.g. index files that keep track of which blocks of data are referenced. This allows the data storage system to among other things 1 single instance data at a more granular level at a block level instead of at a file level 2 reduce or eliminate redundantly stored data thereby saving storage space 3 store very large numbers of blocks of data without regard to file system limitations and 4 delete data that no longer needs to be stored while still maintaining data that needs to be stored.

The data storage system for example may include multiple computing devices or computing systems e.g. client computing devices that store primary data e.g. production data such as system files user files etc. . The data storage system may also include a secondary storage computing device a single instance database and one or more storage devices that store copies of the primary data e.g. secondary copies tertiary copies etc. . The secondary storage computing device receives blocks of data from the computing devices and accesses the single instance database to determine whether the blocks of data are unique unique meaning that no instances of the blocks of data are already stored on the storage devices . If a block of data is unique the single instance database stores it in a file on a storage device. If not the secondary storage computing device can avoid storing the block of data on the storage devices.

The primary data of the computing devices can be divided into data that is eligible for single instancing and data that is not eligible for single instancing. An example of the latter is metadata e.g. Master File Table MFT information and an example of the former is data e.g. operating system and or application files . A file typically comprises one or more blocks as tracked by the file systems of the computing devices.

The computing devices align data that is eligible for single instancing into blocks of data which may comprise one or more blocks as tracked by the file systems of the computing devices and generate identifiers for the blocks of data that the secondary storage computing device uses to determine if the blocks of data are unique. This allows the secondary storage computing device to avoid generating identifiers for the blocks of data which may be computationally expensive and or require a long time to perform. Therefore the distribution of the task of generating identifiers which can be computationally expensive operations across numerous computing devices frees up the secondary storage computing device to perform other operations e.g. storing data retrieving data pruning data etc. .

The computing devices send the blocks of data and other data e.g. metadata and or the data that is not eligible for single instancing in a data stream to the secondary storage computing device. The secondary storage computing device receives the data stream and stores blocks of data and their identifiers in buffers in random access memory RAM . The secondary storage computing device determines whether a block of data is already stored on a storage device. To do this the secondary storage computing device determines by analyzing data structures in the single instance database in view of the block s identifier whether the block of data is already stored on a storage device. If it is then the secondary storage computing device 1 stores a link to the already stored block of data in a metadata file and 2 discards the block of data from the memory buffer. If it is not then the secondary storage computing device stores the block of data in a container file.

Because the size of a block of data and associated metadata is typically less then the size of a memory buffer the secondary storage computing device can keep a single block of data in a single memory buffer while it looks up its identifier in the single instance database. This allows the secondary storage computing device to avoid writing the block of data to disk an operation which is typically slower than storing the block of data in a RAM buffer until the secondary storage computing device determines that it needs to store the block of data in a container file on a storage device. The secondary storage computing device stores data that is not eligible for single instancing in metadata files.

By storing multiple blocks of data in a single container file the secondary storage computing device avoids storing each block of data as a separate file on the file systems of the storage devices. This reduces the number of files that would be stored on the file systems of the storage devices thereby ensuring that the storage devices can adequately store the data of the computing devices in the data storage system.

One advantage of these techniques is that they significantly reduce the number of files stored on a file system of a computing device or storage device. This is at least partly due to the storage of data blocks within the container files. Even if the secondary storage computing device performs numerous storage operations these techniques will result in storing far fewer files on the file system than storage operations where each data block is stored as a separate file. Therefore the file system of the computing device or storage device may not necessarily have to contend with storing excessively large numbers of files such as millions of files or more. Accordingly these techniques enable very large numbers of blocks of data to be stored without regard to limitations of the file system of the computing device or storage device.

However the storage of blocks of data in container files may create additional complexities when it comes time to prune data. This is because a container file may contain blocks of data that are referenced by links in metadata files and thus cannot be deleted because referenced blocks of data typically still need to be stored on the storage devices. Furthermore because the blocks of data are not stored as files on the file systems of the storage devices they cannot be directly referenced by the file system.

The systems and methods described herein provide solutions to these problems. The secondary storage computing device creates the container files as sparse files typically only on operating systems that support sparse files e.g. Windows operating systems and other operating systems that support sparse files . A sparse file is type of file that may include empty space e.g. a sparse file may have real data within it such as at the beginning of the file and or at the end of the file but may also have empty space in it that is not storing actual data such as a contiguous range of bytes all having a value of zero . Second the secondary storage computing device maintains a separate index that stores an indication of whether blocks of data in container files are referred to by links in metadata files. In some examples this can be analogized to using another non native file system that keeps track of blocks of data in the container files on top of the existing native file systems of the storage devices.

When a block of data is not referred to and does not need to be stored the secondary storage computing device can prune it. To prune data the secondary storage computing device accesses the separate index to determine the blocks of data that are not referred to by links. On operating systems that support sparse files the secondary storage computing device can free up space in the container files corresponding to those blocks of data by marking the portions of the physical media corresponding to the unreferenced portions of the container file as available for storage e.g. by zeroing out the corresponding bytes in the container files . On operating systems that do not support sparse files the secondary storage computing device can free up space in the container files by truncating the extreme portions of the container files e.g. the extremities such as the beginnings and or the ends of the container files thereby making the corresponding portions of the physical media available to store other data. Freeing up space in container files allows the operating system to utilize the freed up space in other fashions e.g. other programs may utilize the freed up space .

Various examples of the invention will now be described. The following description provides specific details for a thorough understanding and enabling description of these examples. One skilled in the relevant art will understand however that the invention may be practiced without many of these details. Likewise one skilled in the relevant art will also understand that the invention may include many other obvious features not described in detail herein. Additionally some well known structures or functions may not be shown or described in detail below so as to avoid unnecessarily obscuring the relevant description.

The terminology used below is to be interpreted in its broadest reasonable manner even though it is being used in conjunction with a detailed description of certain specific examples of the invention. Indeed certain terms may even be emphasized below however any terminology intended to be interpreted in any restricted manner will be overtly and specifically defined as such in this Detailed Description section.

While aspects of the invention such as certain functions are described as being performed exclusively on a single device the invention can also be practiced in distributed environments where functions or modules are shared among disparate processing devices which are linked through a communications network such as a Local Area Network LAN Wide Area Network WAN and or the Internet. In a distributed computing environment program modules may be located in both local and remote memory storage devices.

Aspects of the invention may be stored or distributed on computer readable media including tangible computer readable storage media such as magnetically or optically readable computer discs hard wired or preprogrammed chips e.g. EEPROM semiconductor chips nanotechnology memory biological memory or other data storage media. Alternatively computer implemented instructions data structures screen displays and other data under aspects of the invention may be distributed over the Internet or over other networks including wireless networks on a propagated signal on a propagation medium e.g. an electromagnetic wave s a sound wave etc. over a period of time or they may be provided on any analog or digital network packet switched circuit switched or other scheme .

Aspects of the invention will now be described in detail with respect to . illustrates an example of a data storage system that may employ aspects of the invention. illustrates in more detail certain components illustrated in that may be used to implement a block level single instancing system. These components include a secondary storage computing device a single instancing database and a storage device that stores only a single instance of blocks of data of one or more computing devices e.g. client computing devices .

The system may generally include combinations of hardware and software components associated with performing storage operations on electronic data. Storage operations include copying backing up creating storing retrieving and or migrating primary storage data e.g. data stores and or and secondary storage data which may include for example snapshot copies backup copies hierarchical storage management HSM copies archive copies and other types of copies of electronic data stored on storage devices . The system may provide one or more integrated management consoles for users or system processes to interface with in order to perform certain storage operations on electronic data as further described herein. Such integrated management consoles may be displayed at a central control facility or several similar consoles distributed throughout multiple network locations to provide global or geographically specific network data storage information.

In one example storage operations may be performed according to various storage preferences for example as expressed by a user preference a storage policy a schedule policy and or a retention policy. A storage policy is generally a data structure or other information source that includes a set of preferences and other storage criteria associated with performing a storage operation. The preferences and storage criteria may include but are not limited to a storage location relationships between system components network pathways to utilize in a storage operation data characteristics compression or encryption requirements preferred system components to utilize in a storage operation a single instancing or variable instancing policy to apply to the data and or other criteria relating to a storage operation. For example a storage policy may indicate that certain data is to be stored in the storage device retained for a specified period of time before being aged to another tier of secondary storage copied to the storage device using a specified number of data streams etc.

A schedule policy may specify a frequency with which to perform storage operations and a window of time within which to perform them. For example a schedule policy may specify that a storage operation is to be performed every Saturday morning from 2 00 a.m. to 4 00 a.m. A retention policy may specify how long data is to be retained at specific tiers of storage or what criteria must be met before data may be pruned or moved from one tier of storage to another tier of storage. In some cases the storage policy includes information generally specified by the schedule policy and or the retention policy. Put another way the storage policy includes the schedule policy and or the retention policy. Storage policies schedule policies and or retention policies may be stored in a database of the storage manager to archive media as metadata for use in restore operations or other storage operations or to other locations or components of the system .

The system may comprise a storage operation cell that is one of multiple storage operation cells arranged in a hierarchy or other organization. Storage operation cells may be related to backup cells and provide some or all of the functionality of backup cells as described in the assignee s U.S. patent application Ser. No. 09 354 058 now U.S. Pat. No. 7 395 282 which is incorporated herein by reference in its entirety. However storage operation cells may also perform additional types of storage operations and other types of storage management functions that are not generally offered by backup cells.

Storage operation cells may contain not only physical devices but also may represent logical concepts organizations and hierarchies. For example a first storage operation cell may be configured to perform a first type of storage operations such as HSM operations which may include backup or other types of data migration and may include a variety of physical components including a storage manager or management agent a secondary storage computing device a client and other components as described herein. A second storage operation cell may contain the same or similar physical components however it may be configured to perform a second type of storage operations such as storage resource management SRM operations and may include monitoring a primary data copy or performing other known SRM operations.

Thus as can be seen from the above although the first and second storage operation cells are logically distinct entities configured to perform different management functions i.e. HSM and SRM respectively each storage operation cell may contain the same or similar physical devices. Alternatively different storage operation cells may contain some of the same physical devices and not others. For example a storage operation cell configured to perform SRM tasks may contain a secondary storage computing device client or other network device connected to a primary storage volume while a storage operation cell configured to perform HSM tasks may instead include a secondary storage computing device client or other network device connected to a secondary storage volume and not contain the elements or components associated with and including the primary storage volume. The term connected as used herein does not necessarily require a physical connection rather it could refer to two devices that are operably coupled to each other communicably coupled to each other in communication with each other or more generally refer to the capability of two devices to communicate with each other. These two storage operation cells however may each include a different storage manager that coordinates storage operations via the same secondary storage computing devices and storage devices . This overlapping configuration allows storage resources to be accessed by more than one storage manager such that multiple paths exist to each storage device facilitating failover load balancing and promoting robust data access via alternative routes.

Alternatively or additionally the same storage manager may control two or more storage operation cells whether or not each storage operation cell has its own dedicated storage manager . Moreover in certain embodiments the extent or type of overlap may be user defined through a control console or may be automatically configured to optimize data storage and or retrieval.

The clients typically include application software for performing various operations. Clients typically also include an operating system on which the application software runs. A file system can be provided to facilitate and control file access by the operating system and application software. File systems can facilitate access to local and remote storage devices for file or data access and storage. Clients can also include local storage such as a media module media drive with fixed or removable media.

In some examples the clients include storage mechanisms for allowing computer programs or other instructions or data to be loaded into memory for execution. Such storage mechanisms might include for example a fixed or removable storage unit and an interface. Examples of such storage units and interfaces can include a program cartridge and cartridge interface a removable memory for example a flash memory or other removable memory module and memory slot a PCMCIA slot and card and other fixed or removable storage units and interfaces that allow software and data to be transferred from the storage unit to memory.

Data agent may be a software module or part of a software module that is generally responsible for performing storage operations on the data of the client stored in data store or other memory location. Each client may have at least one data agent and the system can support multiple clients . Data agent may be distributed between client and storage manager and any other intermediate components or it may be deployed from a remote location or its functions approximated by a remote process that performs some or all of the functions of data agent .

As used herein the term module might describe a given unit of functionality that can be performed in accordance with one or more embodiments of the present invention. As used herein a module might be implemented utilizing any form of hardware software firmware or a combination thereof. For example one or more processors controllers ASICs PLAs logical components software routines or other mechanisms might be implemented to make up a module. In implementation the various modules described herein might be implemented as discrete modules or the functions and features described can be shared in part or in total among one or more modules. In other words as would be apparent to one of ordinary skill in the art after reading this description the various features and functionality described herein may be implemented in any given application and can be implemented in one or more separate or shared modules in various combinations and permutations. Even though various features or elements of functionality may be individually described or claimed as separate modules one of ordinary skill in the art will understand that these features and functionality can be shared among one or more common software and hardware elements and such description shall not require or imply that separate hardware or software components are used to implement such features or functionality.

The overall system may employ multiple data agents each of which may perform storage operations on data associated with a different application. For example different individual data agents may be designed to handle Microsoft Exchange data Lotus Notes data Microsoft Windows file system data Microsoft Active Directory Objects data Microsoft SQL Server data Microsoft Sharepoint Server data and other types of data known in the art. Other embodiments may employ one or more generic data agents that can handle and process multiple data types rather than using the specialized data agents described above.

If a client has two or more types of data one data agent may be required for each data type to perform storage operations on the data of the client . For example to back up migrate and restore all the data on a Microsoft Exchange server the client may use one Microsoft Exchange Mailbox data agent to back up the Exchange mailboxes one Microsoft Exchange Database data agent to back up the Exchange databases one Microsoft Exchange Public Folder data agent to back up the Exchange Public Folders and one Microsoft Windows File System data agent to back up the file system of the client . These data agents would be treated as four separate data agents by the system even though they reside on the same client .

Alternatively the overall system may use one or more generic data agents each of which may be capable of handling two or more data types. For example one generic data agent may be used to back up migrate and restore Microsoft Exchange Mailbox data and Microsoft Exchange Database data while another generic data agent may handle Microsoft Exchange Public Folder data and Microsoft Windows File System data etc.

Data agents may be responsible for arranging or packing data to be copied or migrated into a certain format such as an archive file. Nonetheless it will be understood that this represents only one example and any suitable packing or containerization technique or transfer methodology may be used if desired. Such an archive file may include metadata a list of files or data objects copied the file and data objects themselves. Moreover any data moved by the data agents may be tracked within the system by updating indexes associated with appropriate storage managers or secondary storage computing devices . As used herein a file or a data object refers to any collection or grouping of bytes of data that can be viewed as one or more logical units.

Generally speaking storage manager may be a software module or other application that coordinates and controls storage operations performed by the system . Storage manager may communicate with some or all elements of the system including clients data agents secondary storage computing devices and storage devices to initiate and manage storage operations e.g. backups migrations data recovery operations etc. .

Storage manager may include a jobs agent that monitors the status of some or all storage operations previously performed currently being performed or scheduled to be performed by the system . One or more storage operations are alternatively referred to herein as a job or jobs. Jobs agent may be communicatively coupled to an interface agent e.g. a software module or application . Interface agent may include information processing and display software such as a graphical user interface GUI an application programming interface API or other interactive interface through which users and system processes can retrieve information about the status of storage operations. For example in an arrangement of multiple storage operations cell through interface agent users may optionally issue instructions to various storage operation cells regarding performance of the storage operations as described and contemplated herein. For example a user may modify a schedule concerning the number of pending snapshot copies or other types of copies scheduled as needed to suit particular needs or requirements. As another example a user may employ the GUI to view the status of pending storage operations in some or all of the storage operation cells in a given network or to monitor the status of certain components in a particular storage operation cell e.g. the amount of storage capacity left in a particular storage device .

Storage manager may also include a management agent that is typically implemented as a software module or application program. In general management agent provides an interface that allows various management agents in other storage operation cells to communicate with one another. For example assume a certain network configuration includes multiple storage operation cells hierarchically arranged or otherwise logically related in a WAN or LAN configuration. With this arrangement each storage operation cell may be connected to the other through each respective interface agent . This allows each storage operation cell to send and receive certain pertinent information from other storage operation cells including status information routing information information regarding capacity and utilization etc. These communications paths may also be used to convey information and instructions regarding storage operations.

For example a management agent in a first storage operation cell may communicate with a management agent in a second storage operation cell regarding the status of storage operations in the second storage operation cell. Another illustrative example includes the case where a management agent in a first storage operation cell communicates with a management agent in a second storage operation cell to control storage manager and other components of the second storage operation cell via management agent contained in storage manager .

Another illustrative example is the case where management agent in a first storage operation cell communicates directly with and controls the components in a second storage operation cell and bypasses the storage manager in the second storage operation cell. If desired storage operation cells can also be organized hierarchically such that hierarchically superior cells control or pass information to hierarchically subordinate cells or vice versa.

Storage manager may also maintain an index a database or other data structure . The data stored in database may be used to indicate logical associations between components of the system user preferences management tasks media containerization and data storage information or other useful data. For example the storage manager may use data from database to track logical associations between secondary storage computing device and storage devices or movement of data as containerized from primary to secondary storage .

Generally speaking the secondary storage computing device which may also be referred to as a media agent may be implemented as a software module that conveys data as directed by storage manager between a client and one or more storage devices such as a tape library a magnetic media storage device an optical media storage device or any other suitable storage device. In one embodiment secondary storage computing device may be communicatively coupled to and control a storage device . A secondary storage computing device may be considered to be associated with a particular storage device if that secondary storage computing device is capable of routing and storing data to that particular storage device .

In operation a secondary storage computing device associated with a particular storage device may instruct the storage device to use a robotic arm or other retrieval means to load or eject a certain storage media and to subsequently archive migrate or restore data to or from that media. Secondary storage computing device may communicate with a storage device via a suitable communications path such as a SCSI or Fibre Channel communications link. In some embodiments the storage device may be communicatively coupled to the storage manager via a SAN.

Each secondary storage computing device may maintain an index a database or other data structure that may store index data generated during storage operations for secondary storage SS as described herein including creating a metabase MB . For example performing storage operations on Microsoft Exchange data may generate index data. Such index data provides a secondary storage computing device or other external device with a fast and efficient mechanism for locating data stored or backed up. Thus a secondary storage computing device index or a database of a storage manager may store data associating a client with a particular secondary storage computing device or storage device for example as specified in a storage policy while a database or other data structure in secondary storage computing device may indicate where specifically the data of the client is stored in storage device what specific files were stored and other information associated with storage of the data of the client . In some embodiments such index data may be stored along with the data backed up in a storage device with an additional copy of the index data written to index cache in a secondary storage device. Thus the data is readily available for use in storage operations and other activities without having to be first retrieved from the storage device .

Generally speaking information stored in cache is typically recent information that reflects certain particulars about operations that have recently occurred. After a certain period of time this information is sent to secondary storage and tracked. This information may need to be retrieved and uploaded back into a cache or other memory in a secondary computing device before data can be retrieved from storage device . In some embodiments the cached information may include information regarding format or containerization of archives or other files stored on storage device .

One or more of the secondary storage computing devices may also maintain one or more single instance databases . More details as to single instancing may be found in one or more of the following commonly assigned U.S. patent applications 1 U.S. patent application Ser. No. 11 269 512 entitled SYSTEM AND METHOD TO SUPPORT SINGLE INSTANCE STORAGE OPERATIONS 2 U.S. patent application Ser. No. 12 145 347 entitled APPLICATION AWARE AND REMOTE SINGLE INSTANCE DATA MANAGEMENT or 3 U.S. patent application Ser. No. 12 145 342 entitled APPLICATION AWARE AND REMOTE SINGLE INSTANCE DATA MANAGEMENT 4 U.S. patent application Ser. No. 11 963 623 entitled SYSTEM AND METHOD FOR STORING REDUNDANT INFORMATION 5 U.S. patent application Ser. No. 11 950 376 entitled SYSTEMS AND METHODS FOR CREATING COPIES OF DATA SUCH AS ARCHIVE COPIES or 6 the previously referenced U.S. patent application Ser. No. 12 565 576 each of which is incorporated by reference herein in its entirety.

In some examples the secondary storage computing devices maintain one or more variable instance databases. Variable instancing generally refers to storing in secondary storage one or more instances but fewer than the total number of instances of each data block or data object in a set of data e.g. primary data . More details as to variable instancing may be found in the commonly assigned U.S. Pat. App. No. 61 164 803 entitled STORING A VARIABLE NUMBER OF INSTANCES OF DATA OBJECTS .

In some embodiments certain components may reside and execute on the same computer. For example in some embodiments a client such as a data agent or a storage manager coordinates and directs local archiving migration and retrieval application functions as further described in the previously referenced U.S. patent application Ser. No. 09 610 738. This client can function independently or together with other similar clients .

As shown in each secondary storage computing device has its own associated metabase . Each client may also have its own associated metabase . However in some embodiments each tier of storage such as primary storage secondary storage tertiary storage etc. may have multiple metabases or a centralized metabase as described herein. For example rather than a separate metabase or index associated with each client in the metabases on this storage tier may be centralized. Similarly second and other tiers of storage may have either centralized or distributed metabases. Moreover mixed architecture systems may be used if desired that may include a first tier centralized metabase system coupled to a second tier storage system having distributed metabases and vice versa etc.

Moreover in operation a storage manager or other management module may keep track of certain information that allows the storage manager to select designate or otherwise identify metabases to be searched in response to certain queries as further described herein. Movement of data between primary and secondary storage may also involve movement of associated metadata and other tracking information as further described herein.

In some examples primary data may be organized into one or more sub clients. A sub client is a portion of the data of one or more clients and can contain either all of the data of the clients or a designated subset thereof. As depicted in the data store includes two sub clients. For example an administrator or other user with the appropriate permissions the term administrator is used herein for brevity may find it preferable to separate email data from financial data using two different sub clients having different storage preferences retention criteria etc.

In addition to the data agent the client includes data . The data includes single instanceable data SI data and non single instanceable data non SI data . SI data includes data that is eligible for single instancing. Non SI data includes data that is not eligible for single instancing. Non SI data may include metadata such as access control lists ACLs disk partition information Master File Table MFT or File Allocation Table FAT information and or other metadata. Non SI data may also include other data that is determined not to be single instanceable. SI data may include data of the client other than non SI data e.g. system files application files user files etc. .

The secondary storage computing device includes a data stream reception component and an identifier comparison component . Various functions performed by these components are also described in detail herein. The secondary storage computing device also includes a memory which includes multiple buffers . The secondary storage computing device may also include other components such as a decompression component and or a decryption component. The single instance database includes data structures that are used to store data such as metadata about SI data . The storage device also includes data structures that are used to store data such as SI data and non SI data . In some examples the secondary storage computing device includes the components that the client includes and performs the functions that the client performs.

The process begins at step where the data agent receives an indication to copy data of the client . The storage manager may send the indication to the data agent e.g. according to a storage policy an administrator may manually start the process and or the process may be automatically started according to a schedule policy.

At step the data agent accesses the data of the client . The data agent e.g. the data identification component determines which portions of the data are SI data and which portions are non SI data . For example the data agent may determine that metadata e.g. MFT FAT volume information transaction logs etc. on the file system of the client is non SI data and that data other than metadata is SI data e.g. system files user files etc. . At step the data agent e.g. the data stream generation component forms a data stream of multiple pairs of stream header and stream payload from the SI data and the non SI data . An example data stream is illustrated in and is described in detail below. A data stream therefore comprises multiple pairs of stream header and stream payload. However those of skill in the art will understand that data streams may contain data organized in other fashions. For the SI data the data agent may set a flag in the stream header to indicate that the corresponding stream payload contains single instanceable data.

At step the data agent e.g. the identifier generation component aligns the stream header and stream payload into one or more fixed size blocks of data. An example data stream with stream header and stream payload aligned into multiple blocks is illustrated in and is described in detail below. A block of data alternatively called a data block is a sequence of bits or bytes having a nominal length a data block size . The file system of the client may track its data in blocks alternatively called clusters in sizes of 512 bytes 4 KB 16 KB or other sizes. Put another way a block may be a subset of one or more data objects. A file on the file system of the client typically spans one or more blocks e.g. a file of size 10 KB may span 3 blocks of size 4 KB . The data agent typically aligns data blocks such that they have the same size which may be 32 KB 64 KB 128 KB 256 KB 512 KB or other sizes. Accordingly the term data block as used herein may comprise one or more blocks as tracked by the file system of the clients . For example if the file system of a client tracks its data in blocks of size 4 KB and if the data agent aligns the client s data into data blocks of size 128 KB then these 128 KB data blocks comprise 32 blocks of data as tracked by the file system of the client .

At step the data agent determines whether a data block is single instanceable. The data agent does so by analyzing the portion of the one or more corresponding stream headers that indicates whether the data block is single instanceable. For example the stream headers may contain a flag or bit that indicates whether the successive stream payload contain single instanceable data. For example see FIG. A illustrating stream headers containing such flags. If the data block is single instanceable the process continues at step where the data agent e.g. the identifier generation component generates an identifier for the data block.

Examples of identifiers include a hash value message digest checksum digital fingerprint digital signature or other sequence of bytes that substantially uniquely identifies the data block in the data storage system. For example identifiers could be generated using Message Digest Algorithm 5 MD5 or Secure Hash Algorithm SHA 512. In some instances the phrase substantially unique is used to modify the term identifier because algorithms used to produce hash values may result in collisions where two different data objects when hashed result in the same hash value. However depending upon the algorithm or cryptographic hash function used collisions should be suitably rare and thus the identifier generated for a block should be unique throughout the data storage system. The term probabilistically unique identifier may also be used. In this case the phrase probabilistically unique is used to indicate that collisions should be low probability occurrences and therefore the identifier should be unique throughout the data storage system.

At step the data agent e.g. the identifier generation component inserts the generated identifier into the data stream. The generated identifier may be comprised in an identifier header and identifier data pair that immediately follows the data block for which it is generated. See and the accompanying description for additional details of the identifier header and identifier data pair. At step the data agent determines whether there are more data blocks. If so the process returns to step . If not the process continues at step where the data agent transfers the data stream to the secondary storage computing device . The process then ends. In some examples the data agent may perform additional operations upon the stream header and or stream payload such as encrypting the stream payload e.g. using the encryption component and or compressing the stream payload e.g. using the compression component .

At step the secondary storage computing device determines whether the data block is single instanceable. The secondary storage computing device may do so for example by analyzing the metadata in the stream header that indicates whether the data block is single instanceable e.g. a flag or bit that indicates whether the data block is single instanceable .

If the data block is single instanceable the process continues at step where the secondary storage computing device e.g. the identifier comparison component obtains the identifier corresponding to the data block e.g. from the identifier data of the data stream and looks up the identifier. The secondary storage computing device looks up the identifier in the primary table in the single instance database . Example data structures used by the single instance database are illustrated in and described with reference to these figures .

At step if the secondary storage computing device finds the identifier of the data block in the primary table this indicates that an instance of the data block is already stored on the storage device and that the block of data should not be stored. Accordingly the secondary storage computing device can avoid storing another instance of the data block and can instead store a link alternatively called a pointer to the location s of the already stored instance. At step the secondary storage computing device adds a link to the location s of the already stored instance of the data block to a metadata file. The link refers or points to the already stored instance of the data block. For example the secondary storage computing device may add as the link to the metadata file the record of the already stored instance of the data block in the primary table. At step the secondary storage computing device adds an entry to the secondary table in the single instance database. The entry includes the location of the link in the metadata file. The secondary storage computing device also increments a reference count corresponding to the data block in the primary table. The reference count indicates the number of links to the already stored instance of the data block. At step the secondary storage computing device discards the stream header and stream payload corresponding to the data block from the buffer of the memory . Additionally or alternatively the secondary storage computing device may indicate that the buffer is available for storing another pair of stream header and stream payload.

If the secondary storage computing device does not find the identifier of the block in the primary table step this indicates that no instances of the data block are already stored on the storage device and that the block of data should be stored. Accordingly at step the secondary storage computing device stores the data block in a container file on the storage device . See and the accompanying description for additional details of container files. At step the secondary storage computing device adds an entry to the primary table in the single instance database. The entry includes the location of the data block in the container file.

If the data block is not single instanceable step the process continues at step where the secondary storage computing device stores the block in a metadata file. See and the accompanying description for additional details of metadata files. The three branches of the process converge at step where the secondary storage computing device determines whether there are more data blocks. If so the process returns to step . If not the process concludes.

In some examples the secondary storage computing device may perform additional operations during the process such as decrypting the stream payload e.g. using a decryption component and or decompressing the stream payload e.g. using a decompression component . The secondary storage computing device may also store in the index for the data blocks information mapping an archive file and offset to the physical location of the data blocks. An archive file is a logical entity that is created during a storage operation and that corresponds to physical locations of data blocks on the storage device . The storage manager may map archive files to physical locations and keep such information in index .

In some examples a variable number of instances of data blocks e.g. more than one instance and up to N 1 instances where N is the number of instances of the data block in primary data is stored on the storage devices . In such examples the secondary storage computing devices may use techniques described in the previously referenced U.S. Pat. App. No. 61 164 803 to ensure that a sufficient number of instances of the blocks of data are stored on the storage devices . Storing multiple instances up to N 1 of N data blocks provides for less risk of data loss than single instance storage techniques and generally nearly as less risk of data loss as conventional data protection techniques which store N instances of N data blocks . Storing multiple instances up to N 1 of N data blocks also provides for more efficient use of available storage space than conventional data protection techniques and almost as efficient use as single instance storage techniques. Accordingly the storing of a variable number of instances of data blocks enables an administrator to tailor data protection to strike an appropriate balance between 1 minimizing the risk of data loss and 2 making efficient use of available data storage space in accordance with the administrator s requirements.

Referring to the data stream has the stream header and stream payload aligned into multiple data blocks. In this example the data blocks are of size 64 KB. The first two stream header and stream payload pairs comprise a first data block of size 64 KB. The first stream header indicates that the length of the succeeding stream payload is 63 KB and that it is the start of a data block. The stream header may also include the metadata discussed with reference to the stream headers illustrated in . The next stream header indicates that the succeeding stream payload has a length of 1 KB and that it is not the start of a new data block. Immediately following stream payload are an identifier header and identifier data pair. The identifier header includes an indication that the succeeding identifier data includes the identifier for the immediately previous data block. The identifier data includes the identifier that the data agent e.g. the identifier generation component generated for the data block. The data stream also includes other stream header and stream payload pairs which may be for SI data and or for non SI data .

Referring to the primary table includes an identifier column in which a data block identifier is stored a location column in which a location of the data block in a container file is stored an offset column indicating the offset within the container file corresponding to the location of the data block and a reference count column which contains a reference count of the number of links that refer to the data block. For example row includes information about a data block for which the identifier is 0xA1B3FG. This data block is located in the container file that is indicated in the location column at an offset of 10 within the container file. As indicated in the reference count column this data block is referred to twice meaning that there are two links that refer to the data block. As another example row includes information about a data block for which the identifier is 0xC13804. The location of this data block is indicated in the location column at an offset of 38 within the container file and it is referred to one other time by one link.

Referring to the secondary table includes information about links that refer to data blocks. The secondary table includes an identifier column a referring location column and an offset column . For example row includes information about a reference to the data block having the identifier of 0xA1B3FG row in the primary table . The location of the link is indicated in column at an offset of five within the indicated metadata file. As another example row includes information about another reference to the data block having the identifier of 0xA1B3FG. This link is located at the location indicated in column at an offset of 15 within the indicated metadata file. As another example row includes information about a reference to the block for which the identifier is 0xC13804 row in the primary table . The location of the link is indicated in column at an offset of 19 within the indicated metadata file.

The data structures include one or more volume folders one or more chunk folders within a volume folder and multiple files within a chunk folder . Each chunk folder includes a metadata file a metadata index file one or more container files and a container index file . The metadata file stores non SI data blocks as well as links to SI data blocks stored in container files. The metadata index file stores an index to the data in the metadata file . The container files store SI data blocks. The container index file stores an index to the container files . Among other things the container index file stores an indication of whether a corresponding block in a container file is referred to by a link in a metadata file . For example data block B in the container file is referred to by a link in the metadata file in the chunk folder . Accordingly the corresponding index entry in the container index file indicates that the data block B in the container file is referred to. As another example data block B in the container file is referred to by a link in the metadata file and so the corresponding index entry in the container index file indicates that this data block is referred to.

As an example the data structures illustrated in may have been created as a result of two storage operations involving two clients . For example a first storage operation on a first client could result in the creation of the first chunk folder and a second storage operation on a second client could result in the creation of the second chunk folder . The container files in the first chunk folder would contain the blocks of SI data of the first client . If the two clients have substantially similar data the second storage operation on the data of the second client would result in the secondary storage computing device storing primarily links to the data blocks of the first client that are already stored in the container files . Accordingly while a first storage operation may result in storing nearly all of the data subject to the storage operation subsequent storage operations involving similar data may result in substantial data storage space savings because links to already stored data blocks can be stored instead of additional instances of data blocks.

If the operating system of the secondary storage computing device supports sparse files then when the secondary storage computing device creates container files it can create them as sparse files. As previously described a sparse file is type of file that may include empty space e.g. a sparse file may have real data within it such as at the beginning of the file and or at the end of the file but may also have empty space in it that is not storing actual data such as a contiguous range of bytes all having a value of zero . Having the container files be sparse files allows the secondary storage computing device to free up space in the container files when blocks of data in the container files no longer need to be stored on the storage devices . In some examples the secondary storage computing device creates a new container file when a container file either includes 100 blocks of data or when the size of the container file exceeds 50 Mb. In other examples the secondary storage computing device creates a new container file when a container file satisfies other criteria e.g. it contains from approximately 100 to approximately 1000 blocks or when its size exceeds approximately 50 Mb to 1 Gb . Those of skill in the art will understand that the secondary storage computing device can create a new container file when other criteria are met.

In some cases a file on which a storage operation is performed may comprise a large number of data blocks. For example a 100 Mb file may be comprised in 400 data blocks of size 256 KB. If such a file is to be stored its data blocks may span more than one container file or even more than one chunk folder. As another example a database file of 20 Gb may comprise over 40 000 data blocks of size 512 KB. If such a database file is to be stored its data blocks will likely span multiple container files multiple chunk folders and potentially multiple volume folders. As described in detail herein restoring such files may thus requiring accessing multiple container files chunk folders and or volume folders to obtain the requisite data blocks.

One advantage of the data structures illustrated in and or of the techniques described herein is that they significantly reduce the number of files stored on a file system of the storage device . This is at least partly due to the storage of data blocks within the container files . Even if numerous storage operations using these data structures are performed this will result in far fewer files on the storage device than storage operations where each data block is stored as a separate file. Therefore the file system of the storage device may not necessarily have to contend with storing excessively large numbers of files such as millions of files or more. Accordingly the systems and methods described herein enable very large numbers of blocks of data to be stored without regard to limitations of the file system of the storage device .

Another advantage is that the data storage system enables a reduction in the amount of blocks of data stored on the storage devices while still maintaining at least one instance of each block of primary data. In examples where the data storage system stores a variable number of instances of blocks of primary data blocks of primary data can be distributed across two or more storage devices thereby adding a further aspect of redundancy.

Another advantage is that the metadata files the metadata index files the container files and or the container index files could be used to replicate the data stored in the single instance database or reconstruct the single instance database if the data of the single instance database is ever lost and or corrupted.

The storage of data blocks in the container files may create additional complexities when it comes time to prune data blocks pruning data blocks may be alternatively referred to as deleting or removing data blocks that the data storage system no longer need retain. This is because the data blocks are not stored as files on the file system on the storage device and thus cannot be directly referenced by the file system using the file system s data structures the data structures that are built into or provided with the file system . As described in detail with reference to the secondary storage computing device uses the container index files to keep track of which blocks of data are referenced and thus which blocks are not prunable deletable .

In some examples the use of the container index files the metadata index files and or the primary and secondary tables to track data is analogous to a driver agent or an additional file system that is layered on top of the existing file system of the storage device . This driver agent additional file system allows the data storage system to efficiently keep track of very large numbers of blocks of data without regard to any limitations of the file systems of the storage devices . Accordingly the data storage system can store very large numbers of blocks of data.

Accordingly the data structures illustrated in and the techniques described herein enable the performance of multiple storage operations cumulatively involving very large amounts of data while still allowing for recovery of space on the storage device when storage of certain data blocks is no longer required. For example the data of numerous clients can be protected without having to store redundant copies or instances of data blocks. Space on the storage device can also be recovered when it is no longer necessary to store certain data blocks. Accordingly storage operations involving very large amounts of data are enabled and optimized by the techniques described herein.

At step the secondary storage computing device determines volume folders and chunk folders corresponding to the archive file and offset. The secondary storage computing device may do so by analyzing the index to determine the volume folders and chunk folders. The determined volume folders and chunk folders contain the requested data. At step the secondary storage computing device accesses an index file within the determined volume folders and chunk folders that corresponds to the data to be restored. This may be the metadata index file when the requested data is non SI data or the container index file when the requested data is SI data . At step the secondary storage computing device determines from the index file the offset within the metadata file or the offset within the container file corresponding to the requested data. At step the secondary storage computing device accesses the metadata file or the container file and seeks to the determined offset. At step the secondary storage computing device retrieves the data from the metadata file or the container file . At step the secondary storage computing device restores the data to a selected location e.g. to a client and or to another location . The process then concludes.

As previously noted restoring a file may necessitate accessing multiple container files chunk folders and or volume folders to obtain the data blocks that comprise the file. The secondary storage computing device may thus have to obtain a first data block from a first container file and a second data block from a second container file. As another example the secondary storage computing device may thus have to obtain a first data block from a first container file within a first folder and a second data block from a second container file within a second folder. To do so the secondary storage computing device may have to access multiple index files or other data structures to locate the requisite blocks of data. Those of skill in the art will understand that various techniques may be used to restore data such as files and other data.

As previously noted the data structures illustrated in may have been created as a result of two jobs involving two clients . For example a first job on a first client could result in the creation of the first chunk folder and a second job on a second client could result in the creation of the second chunk folder . The process is described using this example. More specifically the process is described below as pruning the data created as a result of the first job. Of course a similar process may be used to delete other jobs or even smaller increments of data or data objects such as individual files or blocks.

At step the secondary storage computing device determines the file e.g. archive file and the volume folders and chunk folder corresponding to the job to be pruned. The secondary storage computing device may do so for example by analyzing the index and or the index to determine this information. At step the secondary storage computing device deletes the metadata file and the metadata index file in the chunk folder . The secondary storage computing device can delete the metadata file and the metadata index file in this example because these files include non SI data which is not referenced by any other data.

At step the secondary storage computing device accesses the container file and the container index file in the chunk folder . The secondary storage computing device begins iterating through the data blocks in the container files . At step beginning with a first block in the container file the secondary storage computing device accesses the primary table in the single instance database . The secondary storage computing device determines from the primary table whether the reference count of a data block in the container file is equal to zero. If so this indicates that there are no references to the data block. The process then continues at step where the secondary storage computing device sets the entry in the container index file corresponding to the data block equal to zero thus indicating that there are no references to the data block and therefore prunable.

If the reference count of a data block is not equal to zero then the data block is not prunable and the process continues at step . At this step the secondary storage computing device determines whether there are more data blocks in the container file . If so the process returns to step where it accesses the next data block. If there are no more data blocks in the container file the process continues at step where the secondary storage computing device determines whether all the entries in the container index file corresponding to the container file are equal to zero. As illustrated in the second index entry in the container index file is not equal to zero thus indicating that the corresponding block in container file is referenced by data in the chunk folder as earlier described . Accordingly the container file cannot be deleted.

However if the container file did not contain any referenced data blocks then at step the secondary storage computing device would delete the container file . The process would then continue at step where the secondary storage computing device determines whether there are more container files. According to the example as illustrated in there is an additional container file . The process then returns to step where it performs the same steps for container file . As a result of performing these steps the secondary storage computing device would also determine that the container file cannot be deleted because it contains a data block that is referenced by data in the chunk folder as earlier described .

After processing container files the process continues at step where the secondary storage computing device determines whether to free up storage space in the container files . The secondary storage computing device may do so using various techniques. For example if the operating system of the secondary storage computing device supports sparse files then the secondary storage computing device may free up space by zeroing out the bytes in the container files corresponding to the space to be freed up. For a certain number of contiguous blocks e.g. a threshold number of contiguous blocks such as three contiguous blocks for which the corresponding entries in the container index file indicate that the blocks are not being referred to then the secondary storage computing device may mark these portions of the container files as available for storage by the operating system or the file system. The secondary storage computing device may do so by calling an API of the operating system to mark the unreferenced portions of the container files as available for storage.

The secondary storage computing device may use certain optimizations to manage the number of times portions of the container file are specified or marked as available for storage such as only zeroing out bytes in container files when a threshold number of unreferenced contiguous blocks is reached e.g. three or more unreferenced contiguous blocks . These optimizations may result in less overhead for the operating system because it reduces the number of contiguous ranges of zero value bytes in the container files that the operating system must keep track of e.g. it reduces the amount of metadata about portions of the container files that are available for storage .

If the operating system of the secondary storage computing device does not support sparse files then the secondary storage computing device may free up space by truncating either the beginning or the end of the container files removing or deleting data at the beginning or end of the container files . The secondary storage computing device may do so by calling an API of the operating system or by operating directly on the container files . For example if a certain number of the last blocks of the container file are not being referred to the secondary storage computing device may truncate these portions of the container files . Other techniques may be used to free up space in the container files for storage of other data. At step the secondary storage computing device frees up space in the container files . The process then concludes.

As a result of the process the chunk folder would contain only the container files and the container index file . At a later time when the chunk folder is pruned that is when the job that created this chunk folder is selected to be pruned then the container files in the chunk folder can be deleted because they no longer contain data blocks that is referenced by other data. Therefore pruning data corresponding to a job may also result in pruning data corresponding to an earlier job because the data corresponding to the earlier job is no longer referenced by the later job.

Although the process is described with reference to the pruning of data corresponding to jobs one or more storage operations other data can also be pruned. For example an administrator may wish to delete SI data but retain non SI data . In such case the administrator may instruct the secondary storage computing device to delete the container files but retain the metadata files and metadata index files . As another example an administrator or storage policy may delete one or more specific files. In such case the secondary storage computing device deletes the data blocks in the container files corresponding to the specific files but retains other data blocks. The process may include fewer or more steps than those described herein to accommodate these other pruning examples. Those of skill in the art will understand that data can be pruned in various fashions and therefore that the process is not limited to the steps described herein.

One advantage of the process and the techniques described herein is that they enable the deletion of data on the storage devices that no longer needs to be stored while still retaining data that needs to be stored and doing so in a space efficient manner. Space previously allocated for data blocks that no longer need to be stored can be reclaimed by the data storage system and used to store other data. Accordingly the techniques described herein provide for efficient use of available storage space available on physical media .

From the foregoing it will be appreciated that specific examples of data storage systems have been described herein for purposes of illustration but that various modifications may be made without deviating from the spirit and scope of the invention. For example although copy operations may have been described the system may be used to perform many types of storage operations e.g. backup operations restore operations archival operations copy operations Continuous Data Replication CDR operations recovery operations migration operations HSM operations etc. . As another example although block level single instancing has been described the systems and methods detailed herein may be used to single instance files. As another example the secondary storage computing device may keep track of which blocks of data in container files are not referenced instead of keeping track of which blocks of data are referred to by links. As another example non SI data may not be aligned into blocks of data. Accordingly the invention is not limited except as by the appended claims.

Terms and phrases used in this document and variations thereof unless otherwise expressly stated should be construed as open ended as opposed to limiting. As examples of the foregoing the term including should be read as meaning including without limitation or the like the term example is used to provide exemplary instances of the item in discussion not an exhaustive or limiting list thereof the terms a or an should be read as meaning at least one one or more or the like and adjectives such as conventional traditional normal standard known and terms of similar meaning should not be construed as limiting the item described to a given time period or to an item available as of a given time but instead should be read to encompass conventional traditional normal or standard technologies that may be available or known now or at any time in the future. Likewise where this document refers to technologies that would be apparent or known to one of ordinary skill in the art such technologies encompass those apparent or known to the skilled artisan now or at any time in the future.

The presence of broadening words and phrases such as one or more at least but not limited to or other like phrases in some instances shall not be read to mean that the narrower case is intended or required in instances where such broadening phrases may be absent. The use of the term module does not imply that the components or functionality described or claimed as part of the module are all configured in a common package. Indeed any or all of the various components of a module whether control logic or other components can be combined in a single package or separately maintained and can further be distributed in multiple groupings or packages or across multiple locations.

If a synchronization process or synchronization processes are described herein it is not intended to require that multiple synchronizations occur simultaneously or that multiple computing systems being synchronized each receive the same data. Although in some examples the data can be broadcast to all participating computing systems simultaneously or close to simultaneously in other examples the data can be sent to different computing systems or groups of computing systems at different times. Likewise in some examples the same data or the same subset of the data can be sent to all computing systems. However in other examples subsets of the data can be tailored for a given computing system or group of computing systems.

Unless the context clearly requires otherwise throughout the description and the claims the words comprise comprising and the like are to be construed in an inclusive sense as opposed to an exclusive or exhaustive sense that is to say in the sense of including but not limited to. The word coupled as generally used herein refers to two or more elements that may be either directly connected or connected by way of one or more intermediate elements. Additionally the words herein above below and words of similar import when used in this application shall refer to this application as a whole and not to any particular portions of this application. Where the context permits words in the above Detailed Description using the singular or plural number may also include the plural or singular number respectively. The word or in reference to a list of two or more items that word covers all of the following interpretations of the word any of the items in the list all of the items in the list and any combination of the items in the list.

The above detailed description of embodiments of the invention is not intended to be exhaustive or to limit the invention to the precise form disclosed above. While specific embodiments of and examples for the invention are described above for illustrative purposes various equivalent modifications are possible within the scope of the invention as those skilled in the relevant art will recognize. For example while processes or blocks are presented in a given order alternative embodiments may perform routines having steps or employ systems having blocks in a different order and some processes or blocks may be deleted moved added subdivided combined and or modified. Each of these processes or blocks may be implemented in a variety of different ways. Also while processes or blocks are at times shown as being performed in series these processes or blocks may instead be performed in parallel or may be performed at different times.

The teachings of the invention provided herein can be applied to other systems not necessarily the system described above. The elements and acts of the various embodiments described above can be combined to provide further embodiments.

Any patents and applications and other references noted above including any that may be listed in accompanying filing papers are incorporated herein by reference. Aspects of the invention can be modified if necessary to employ the systems functions and concepts of the various references described above to provide yet further implementations of the invention.

These and other changes can be made to the invention in light of the above Detailed Description. While the above description details certain embodiments of the invention and describes the best mode contemplated no matter how detailed the above appears in text the invention can be practiced in many ways. Details of the system may vary considerably in implementation details while still being encompassed by the invention disclosed herein. As noted above particular terminology used when describing certain features or aspects of the invention should not be taken to imply that the terminology is being redefined herein to be restricted to any specific characteristics features or aspects of the invention with which that terminology is associated. In general the terms used in the following claims should not be construed to limit the invention to the specific embodiments disclosed in the specification unless the above Detailed Description section explicitly defines such terms. Accordingly the actual scope of the invention encompasses not only the disclosed embodiments but also all equivalent ways of practicing or implementing the invention under the claims.

While certain aspects of the invention are presented below in certain claim forms the inventors contemplate the various aspects of the invention in any number of claim forms. For example while only one aspect of the invention is recited as embodied in a computer readable medium other aspects may likewise be embodied in a computer readable medium. As another example while only one aspect of the invention is recited as a means plus function claim under 35 U.S.C. 112 sixth paragraph other aspects may likewise be embodied as a means plus function claim or in other forms such as being embodied in a computer readable medium. Any claims intended to be treated under 35 U.S.C. 112 6 will begin with the words means for. Accordingly the inventors reserve the right to add additional claims after filing the application to pursue such additional claim forms for other aspects of the invention.

