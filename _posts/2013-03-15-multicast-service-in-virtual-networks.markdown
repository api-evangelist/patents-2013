---

title: Multicast service in virtual networks
abstract: Techniques are described to provide multicast service within a virtual network using a virtual network controller and endpoint replication without requiring multicast support in the underlying network. The virtual network controller is configured to create a multicast tree for endpoint devices of a multicast group in the virtual network at a centralized location instead of in a distributed fashion. The virtual network controller communicates the multicast tree to one or more of the endpoint devices of the multicast group to instruct the endpoint devices to replicate and forward multicast packets to other endpoint devices according to the multicast tree. The replication and forwarding of multicast packets is performed by virtual switches executed on the endpoint devices in the virtual network. No replication is performed within the underlying network. The techniques enable multicast service within a virtual network without requiring multicast support in the underlying network.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09374270&OS=09374270&RS=09374270
owner: Juniper Networks, Inc.
number: 09374270
owner_city: Sunnyvale
owner_country: US
publication_date: 20130315
---
This application claims the benefit of U.S. Provisional Application No. 61 723 685 filed Nov. 7 2012 U.S. Provisional Application No. 61 722 696 filed Nov. 5 2012 U.S. Provisional Application No. 61 721 979 filed Nov. 2 2012 U.S. Provisional Application No. 61 721 994 filed Nov. 2 2012 U.S. Provisional Application No. 61 718 633 filed Oct. 25 2012 U.S. Provisional Application No. 61 656 468 filed Jun. 6 2012 U.S. Provisional Application No. 61 656 469 filed Jun. 6 2012 and U.S. Provisional Application No. 61 656 471 filed Jun. 6 2012 the entire content of each of which being incorporated herein by reference.

Techniques of this disclosure relate generally to computer networks and more particularly to virtual networks.

In a typical cloud data center environment there is a large collection of interconnected servers that provide computing and or storage capacity to run various applications. For example a data center may comprise a facility that hosts applications and services for subscribers i.e. customers of the data center. The data center may for example host all of the infrastructure equipment such as networking and storage systems redundant power supplies and environmental controls. In a typical data center clusters of storage systems and application servers are interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers e.g. a Clos network. More sophisticated data centers provide infrastructure spread throughout the world with subscriber support equipment located in various physical hosting facilities.

In general multicast service involves delivering data through a network to a group of subscribers e.g. customers of a data center or servers within a data center substantially simultaneously in a single transmission from a source device e.g. a server. As an example when two or more subscribers are interested in receiving the same multicast data each of the subscribers may request to join a multicast group associated with the multicast data. The multicast data from the source is replicated as needed by physical network switches and routers within the network in order to reach the subscribers of the multicast group. The network switches and routers used to replicate and forward the multicast data for the multicast group may create a multicast distribution tree through the network for delivery of the multicast data.

In a case where a large number of servers are interconnected by a switch fabric each pair of servers may have a large number of equal cost paths between them. In this scenario layer two L2 connectivity between the physical servers may become unmanageable and the physical servers may only be connected to each other using layer three L3 connectivity. The physical servers connected via the switch fabric may communicate using unicast service but multicast service between the physical servers may be more difficult to manage due to the large number of redundant paths in the network. In some cases a virtual overlay network may be built on top of the switch fabric to emulate an L2 network between virtual switches executed on the servers using a tunnel encapsulation protocol e.g. IP in IP NvGRE or VXLAN. In order for the virtual overlay network to emulate L2 multicast service however the underlying switch fabric must also support multicast service.

In general techniques are described to provide multicast service within a virtual network without requiring multicast support in the underlying network. More specifically the techniques enable support of L2 multicast service in a virtual overlay network of a switch fabric using a logically centralized virtual network controller and endpoint replication. The virtual network controller is configured to create a multicast tree for endpoint devices e.g. servers of a multicast group in the virtual network. The virtual network controller then communicates the multicast tree to one or more of the endpoint devices of the multicast group to instruct the endpoint devices to replicate and forward multicast packets to other endpoint devices according to the multicast tree. The multicast tree may be a unidirectional multicast tree or a bidirectional multicast tree.

According to the techniques the multicast tree is calculated at a centralized location of the virtual network controller instead of in a distributed fashion by network switches and routers that service the multicast group. In addition the replication and forwarding of multicast packets is performed by virtual switches executed on the endpoint devices in the virtual network. No replication is performed within the underlying network. In this way the techniques enable multicast service within a virtual network without requiring multicast support in the underlying network. In addition the centralized virtual network controller may create the multicast tree to minimize a replication limit such that several endpoints of the virtual network may replicate and forward a small number of multicast packets instead of the source endpoint having to incur the load of replicating and forwarding the packets to all the endpoints that belong to the multicast group. Further the virtual network controller may configure a unique virtual network tunnel encapsulation for each link direction in the multicast tree for the multicast group in order to efficiently use bandwidth in the network.

In one example a method comprises creating with a virtual network controller of a virtual network a multicast tree for endpoint devices of a multicast group in the virtual network storing the multicast tree in a memory within the virtual network controller and communicating with the virtual network controller the multicast tree to one or more of the endpoint devices of the multicast group in order to instruct virtual switches executed on the endpoint devices to replicate and forward multicast packets according to the multicast tree.

In another example a method comprises receiving with an endpoint device of a multicast group in a virtual network multicast packets for the multicast group to be forwarded on the virtual network according to a multicast tree for the multicast group the multicast tree created by a virtual network controller of the virtual network replicating with a virtual switch executed on the endpoint device the multicast packets for the multicast group according to the multicast tree and forwarding with the virtual switch executed on the endpoint device the replicated multicast packets using tunnel encapsulations to one or more other endpoint devices of the multicast group according to the multicast tree.

In a further example a virtual network controller of a virtual network comprises a memory and one or more processor configured to create a multicast tree for endpoint devices of a multicast group in the virtual network store the multicast tree in the memory of the virtual network controller and communicate the multicast tree to one or more of the endpoint devices of the multicast group in order to instruct virtual switches executed on the endpoint devices to replicate and forward multicast packets according to the multicast tree.

In an additional example an endpoint device in a virtual network comprises one or more processors configured to receive multicast packets for a multicast group to which the endpoint device belongs to be forwarded on the virtual network according to a multicast tree for the multicast group wherein the multicast tree is created by a virtual network controller of the virtual network and a virtual switch executed on the processors configured to replicate the multicast packets for the multicast group according to the multicast tree and forward the replicated multicast packets using tunnel encapsulations to one or more other endpoint devices of the multicast group according to the multicast tree.

In another example a system of a virtual network the system comprises a virtual network controller configured to create a multicast tree for endpoint devices of a multicast group in the virtual network store the multicast tree in a memory within the virtual network controller and communicate the multicast tree to one or more of the endpoint devices of the multicast group and one of the endpoint devices of the multicast group configured to receive multicast packets for the multicast group to be forwarded on the virtual network and execute a virtual switch to replicate multicast packets for the multicast group according to the multicast tree and forward the replicated multicast packets using tunnel encapsulations to one or more of the other endpoint devices of the multicast group according to the multicast tree.

In a further example a computer readable storage medium comprises instructions that when executed cause one or more processor to create with a virtual network controller of a virtual network a multicast tree for endpoint devices of a multicast group in the virtual network store the multicast tree in a memory within the virtual network controller and communicate with the virtual network controller the multicast tree to one or more of the endpoint devices of the multicast group in order to instruct virtual switches executed on the endpoint devices to replicate and forward multicast packets according to the multicast tree.

In another example a computer readable storage medium comprises instructions that when executed cause one or more processor to receive with an endpoint device of a multicast group in a virtual network multicast packets for the multicast group to be forwarded on the virtual network according to a multicast tree for the multicast group the multicast tree created by a virtual network controller of the virtual network replicate with a virtual switch executed on the endpoint device the multicast packets for the multicast group according to the multicast tree and forward with the virtual switch executed on the endpoint device the replicated multicast packets using tunnel encapsulations to one or more other endpoint devices of the multicast group according to the multicast tree.

In some examples data center may represent one of many geographically distributed network data centers. As illustrated in the example of data center may be a facility that provides network services for customers . Customers may be collective entities such as enterprises and governments or individuals. For example a network data center may host web services for several enterprises and end users. Other exemplary services may include data storage virtual private networks traffic engineering file service data mining scientific or super computing and so on. In some embodiments data center may be individual network servers network peers or otherwise.

In this example data center includes a set of storage systems and application servers A X herein servers interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers e.g. a Clos network. Switch fabric is provided by a set of interconnected top of rack TOR switches A BN TOR switches coupled to a distribution layer of chassis switches . Although not shown data center may also include for example one or more non edge switches routers hubs gateways security devices such as firewalls intrusion detection and or intrusion prevention devices servers computer terminals laptops printers databases wireless mobile devices such as cellular phones or personal digital assistants wireless access points bridges cable modems application accelerators or other network devices.

In this example TOR switches and chassis switches provide servers with redundant multi homed connectivity to IP fabric and service provider network . Chassis switches aggregates traffic flows and provides high speed connectivity between TOR switches . TOR switches A and B may be network devices that provide layer 2 MAC address and or layer 3 IP address routing and or switching functionality. TOR switches and chassis switches may each include one or more processors and a memory and that are capable of executing one or more software processes. Chassis switches are coupled to IP fabric which performs layer 3 routing to route network traffic between data center and customers using service provider network .

Virtual network controller VNC provides a logically centralized controller for facilitating operation of one or more virtual networks within data center in accordance with one or more examples of this disclosure. In some examples virtual network controller may operate in response to configuration input received from a network administrator . As described in further detail below servers may include one or more virtual switches that create and manage one or more virtual networks as virtual overlay networks of switch fabric .

In a case where a large number of servers e.g. 2000 servers are interconnected by switch fabric each pair of servers may have a large number of equal cost paths between them. In this scenario layer two L2 connectivity between physical servers may become unmanageable and physical servers may only be connected to each other using layer three L3 connectivity. In the case where servers include virtual switches managed by VNC however a virtual overlay network may be built on top of switch fabric to emulate an L2 network between the virtual switches executed on servers using a tunnel encapsulation protocol e.g. IP in IP NvGRE or VXLAN.

In order to provide network services for customers servers of data center exchange large amounts of data with each other via switch fabric . In general it may be desirable for servers to communicate using multicast service. Multicast service involves delivering data through a network to a group of subscribers substantially simultaneously in a single transmission from a source device. In the example of when two or more of servers are interested in receiving the same multicast data from a source server A for example the interested servers may request to join a multicast group associated with the multicast data.

In a conventional data center in order to forward the multicast data from a source server to two or more other servers the multicast data is replicated as needed by the physical network switches and routers within the switch fabric. The network switches and routers used to replicate and forward the multicast data for the multicast group may create a multicast distribution tree through the switch fabric to manage the replication and delivery of the multicast data. In a case where a large number of servers e.g. 2000 servers are interconnected by the switch fabric each pair of servers may have a large number of equal cost paths between them. The physical servers may communicate using unicast service but multicast service between the physical servers may be more difficult to manage due to the large number of redundant paths in the switch fabric.

In addition multicast protocols used in the conventional data center may waste bandwidth by not efficiently using all of the available links within the switch fabric. Moreover the physical switches and routers within the switch fabric may only be able to support a very small number of multicast tables such that the conventional data center will be unable to scale to support the large number of multicast groups necessary for the large number of interconnected servers.

The above issues with providing L3 multicast service between servers in a large data center may not be solved however with a virtual overlay network built on the switch fabric to emulate an L2 network between virtual switches executed on the servers. Conventionally in order for the virtual overlay network to emulate L2 multicast service the underlying switch fabric must also support multicast service.

In general this disclosure describes techniques to provide multicast service within a virtual network without requiring multicast support in the underlying network. More specifically the techniques enable support of L2 multicast service in a virtual overlay network of switch fabric using virtual network controller and endpoint replication. The virtual network controller is configured to create a multicast tree for endpoint devices e.g. servers of a multicast group in the virtual network. The multicast tree is considered a multicast tree because it is created in a virtual overlay network emulating L2 multicast such that any of servers can be the source server of the multicast traffic known as bidirectional multicast. The virtual network controller then communicates the multicast tree to one or more of servers of the multicast group to instruct servers to replicate and forward multicast packets to the two or more of servers that belong to the multicast group according to the multicast tree.

According to the techniques the multicast tree is calculated at virtual network controller instead of in a distributed fashion by network switches and routers in switch fabric that service the multicast group. In addition the replication and forwarding of multicast packets is performed by virtual switches executed on servers of the virtual network. No replication is performed within the underlying switch fabric . In this way the techniques enable multicast service within a virtual network without requiring multicast support in the underlying network. For example switch fabric does not need to support L3 multicast which may make switch fabric simple and easy to mange.

In addition virtual network controller may create the multicast tree to minimize a replication limit such that several of servers may replicate and forward a small number of multicast packets instead of a single source server having to incur the load of replicating and forwarding the packets to all the servers that belong to the multicast group. Further virtual network controller may configure a unique virtual network tunnel encapsulation for each link direction in the multicast tree for the multicast group in order to efficiently use bandwidth in switch fabric . The techniques are described in more detail with respect to virtual network controller and servers in .

Each virtual switch may execute within a hypervisor a host operating system or other component of each of servers . In the example of virtual switch executes within hypervisor also often referred to as a virtual machine manager VMM which provides a virtualization platform that allows multiple operating systems to concurrently run on one of host servers . In the example of virtual switch A manages virtual networks each of which provides a network environment for execution of one or more virtual machines VMs on top of the virtualization platform provided by hypervisor . Each VM is associated with one of the virtual subnets VN VN managed by the hypervisor .

In general each VM may be any type of software application and may be assigned a virtual address for use within a corresponding virtual network where each of the virtual networks may be a different virtual subnet provided by virtual switch A. A VM may be assigned its own virtual layer three L3 IP address for example for sending and receiving communications but may be unaware of an IP address of the physical server A on which the virtual machine is executing. In this way a virtual address is an address for an application that differs from the logical address for the underlying physical computer system i.e. server A in the example of .

In one implementation each of servers includes a virtual network agent A VN agents that controls the overlay of virtual networks and that coordinates the routing of data packets within server . In general each of VN agents communicates with virtual network controller which generates commands to control routing of packets through data center . VN agents may operate as a proxy for control plane messages between virtual machines and virtual network controller . For example a VM may request to send a message using its virtual address via the VN agent A and VN agent A may in turn send the message and request that a response to the message be received for the virtual address of the VM that originated the first message. In some cases a VM may invoke a procedure or function call presented by an application programming interface of VN agent A and the VN agent A may handle encapsulation of the message as well including addressing.

In one example network packets e.g. layer three L3 IP packets or layer two L2 Ethernet packets generated or consumed by the instances of applications executed by virtual machines within the virtual network domain may be encapsulated in another packet e.g. another IP or Ethernet packet that is transported by the physical network. The packet transported in a virtual network may be referred to herein as an inner packet while the physical network packet may be referred to herein as an outer packet. Encapsulation and or de capsulation of virtual network packets within physical network packets may be performed within virtual switches e.g. within the hypervisor or the host operating system running on each of servers . As another example encapsulation and de capsulation functions may be performed at the edge of switch fabric at a first hop TOR switch that is one hop removed from the application instance that originated the packet. This functionality is referred to herein as tunneling and may be used within data center to create one or more overlay networks. Other example tunneling protocols may be used including IP in IP IP over GRE VxLAN NvGRE MPLS over GRE etc.

As noted above virtual network controller provides a logically centralized controller for facilitating operation of one or more virtual networks within data center . Virtual network controller may for example maintain a routing information base e.g. one or more routing tables that store routing information for the physical network as well as the overlay network of data center . Switches and virtual switches also maintain routing information such as one or more routing and or forwarding tables. In one example implementation virtual switch A of hypervisor implements a network forwarding table NFT for each virtual network . In general each NFT stores forwarding information for the corresponding virtual network and identifies where data packets are to be forwarded and whether the packets are to be encapsulated in a tunneling protocol such as with one or more outer IP addresses.

The routing information may for example map packet key information e.g. destination IP information and other select information from packet headers to one or more specific next hops within the networks provided by virtual switches and switch fabric . In some case the next hops may be chained next hop that specify a set of operations to be performed on each packet when forwarding the packet such as may be used for flooding next hops and multicast replication. In some cases virtual network controller maintains the routing information in the form of a radix tree having leaf nodes that represent destinations within the network. U.S. Pat. No. 7 184 437 provides details on an exemplary embodiment of a router that utilizes a radix tree for route resolution the contents of U.S. Pat. No. 7 184 437 being incorporated herein by reference in its entirety.

As shown in each virtual network provides a communication framework for encapsulated packet communications for the overlay network established through switch fabric . In this way network packets associated with any of virtual machines may be transported as encapsulated packet communications via the overlay network. In addition in the example of each virtual switch includes a default network forwarding table NFTand provides a default route that allows packets to be forwarded to virtual subnet VN without encapsulation i.e. non encapsulated packet communications per the routing rules of the physical network of data center . In this way subnet VN and virtual default network forwarding table NFTprovide a mechanism for bypassing the overlay network and sending non encapsulated packet communications to switch fabric . Moreover virtual network controller and virtual switches may communicate using virtual subnet VN in accordance with default network forwarding table NFTduring discovery and initialization of the overlay network and during conditions where a failed link has temporarily halted communication via the overlay network.

The techniques described in this disclosure provide multicast service within the virtual overlay network without requiring multicast support in the underlying physical network. In accordance with the techniques virtual network controller includes a tree unit configured to create or calculate a multicast tree for one or more of servers that belong to a given multicast group in the virtual network . In general tree unit may create a different multicast tree for each multicast group within each of virtual networks . Tree unit may create the multicast trees to facilitate delivery of multicast packets for the multicast groups between two or more servers in the virtual networks . Tree unit then stores the different multicast trees in a memory within virtual network controller .

In some cases the multicast trees may be unidirectional multicast trees in which a root node of the multicast tree operates as the source of the multicast packets for the multicast group and the multicast packets are communicated in a single downstream direction from the root node. In other cases the multicast trees may be bidirectional multicast trees in which any node of the multicast tree may operate as the source of the multicast packets for the multicast group and the multicast packets may be communicated in either an upstream or downstream direction from a root node of the multicast tree. According to the techniques virtual network controller may create bidirectional multicast trees because virtual network emulates L2 multicast which supports bidirectional multicast.

According to the techniques the multicast trees are calculated at the centralized location of virtual network controller instead of in a distributed fashion by TOR switches and chassis switches that service the multicast group in the underlying physical network. In this way the location of servers connected to the underlying network may be considered. For example in a data center it may be desirable for tree unit to create a multicast tree such that replication by servers within a given physical rack does not cross rack boundaries.

In the context of multicasting in virtual networks VMs executed in servers may exchange messages to learn about neighboring multicast enabled VMs in other servers . In the contest of virtual networks servers may be considered logical neighbors that are directly reachable by tunneling over the underlying physical network. In one example servers may exchange Protocol Independent Multicast PIM messages in virtual networks . As described above the underlying physical network may not support multicast service such that the multicast neighbor messages may be communicated between servers and virtual network controller using virtual networks .

In order to create the multicast trees virtual network controller may receive join requests from one or more of servers interested in receiving multicast packets associated with a particular multicast group. Virtual network controller may similarly receive leave requests from servers that would like to stop receiving multicast packets for the multicast group. For example virtual network controller may receive the join or leave requests as packets that conform to Internet Group Management Protocol IGMP or another multicast protocol. The join and leave request packets may include a multicast group identifier and a virtual address of a VM executed on one of servers that is interested in joining or leaving the identified multicast group. As described above the underlying network may not support multicast service such that join and leave requests for the multicast groups may be communicated between servers and virtual network controller using virtual networks . Virtual network controller may maintain membership information mapping VMs to multicast groups for each virtual network .

Tree unit of virtual network controller may then create a multicast tree of a given multicast group based on the membership information and the routing information maintained for the physical network and the associated virtual network . For example tree unit may determine a topology of the multicast tree based on the servers that include VMs that belong to the multicast group the available paths in the physical network between the servers included in the multicast group and the tunnels between the servers in the virtual network .

In addition tree unit may determine a topology of the multicast tree based on a replication limit for each of servers included in the multicast group of the multicast tree. In some cases tree unit may calculate the multicast tree in order to minimize a replication limit for each of the servers and balance the replication across the tree. In this way tree unit may create the multicast tree such that each of the servers performs a similar small amount of replication instead of one or two servers having to incur the load of replicating and forwarding the packets to all the servers that belong to the multicast group.

As an example tree unit may create a multicast tree to minimize the replication limit such that each of servers sends at most a quantity N of packet copies. The replication limit may be selected to be greater than or equal to 2 and substantially less than the quantity M of servers in the multicast group i.e. 2 N

In some cases tree unit may be configured to determine the replication limit N at least based on the number of servers M in the multicast group and latency requirements which are proportional to a number of replication stages or levels i.e. depth of the multicast tree . In this case the replication limit may be determined based on balancing two factors minimizing the tree depth D of the multicast tree and minimizing the replication limit N for each of the servers in the multicast tree. Tree unit may be configurable to give greater weight to either tree depth or replication limit. For example the replication limit may be determined in order to satisfy LOG M D. In some cases the number of replications for a given server may be a function of the performance of the replication unit of the server . For example tree unit may maintain a table in memory that gives replication count based on a type of the server . If the smallest table entry for a server in the multicast group is equal to K then 2 N K such that if LOG M is less than D and LOG M D. Several methods for minimizing the replication limit and balancing the replications limit and the tree depth are described in greater detail below with respect to .

Virtual network controller may also be configured to generate a unique tunnel encapsulation for each link direction of the generated multicast tree. In the multicast tree the tunnel encapsulation in each direction is unique because each direction of a link between two of servers has a different combination of source IP address SIP destination IP address DIP and multicast group ID MGID . This combination in an ordered tuple SIP DIP MGID will not repeat for any other group or link or other direction of the same link. In this way servers may use a different tunnel encapsulation header for each packet copy.

In data center the paths between servers through switch fabric may be substantially equally probable such that forwarding may not be limited to certain links chosen by multicast algorithms in the underlying physical network. The different tunnel encapsulation headers allow multiple equal cost paths in the physical network to be used for the same multicast group which efficiently utilizes bandwidth of the network. A configuration in which each link direction in the multicast tree has a unique tunnel encapsulation header may enable virtual network controller to randomize the distribution of traffic over the multiple equal cost paths in the underlying physical network.

In order to perform multicast in a virtual network virtual network controller communicates a multicast tree created for specific multicast group in the virtual network to servers that belong to the multicast group. In some examples virtual network controller communicates a multicast tree to one of servers as tree forwarding state including one or more forwarding entries of the multicast tree relevant to the particular one of servers . The forwarding entries may be stored in NFT corresponding to the virtual network . The tree forwarding state may include next hop information of the multicast tree for virtual switch of the server . The next hops may be chained next hop that specify replication to be performed on each multicast packet for the multicast group by virtual switch when forwarding the packet.

The communicated multicast tree instructs virtual switches of the servers to replicate and forward multicast packets to other servers according to the multicast tree. As illustrated in virtual switches each include a corresponding one of replication units A X rep. units . Replication units replicate a received multicast packet based on the topology of the multicast tree. Virtual switches then forward the packet copies to one or more other servers of the multicast group using tunnel encapsulation in accordance with one of NFTs as configured by virtual network controller .

According to the techniques server A for example receives a multicast tree for a multicast group in a virtual network to which server A belongs from virtual network controller . Server A may then receive multicast packets for the multicast group to be forwarded on the virtual network according to the multicast tree for the multicast group. Virtual switch A of server A uses replication unit A to replicate the multicast packets for the multicast group according to the received bidirectional multicast. If server A is the source server or an intermediate server in the multicast tree then replication unit A may create one or more copies of the packet as required by the multicast tree. According to the multicast tree replication unit A of server A may generate no more than N copies of the multicast packet. One or more of servers B X that belong to the same multicast group may also receive the multicast tree from virtual network controller . The replication units B X of each of the servers B X may also generate a number of copies of the multicast packet according to the multicast tree. In some cases the number of copies may be the same as the number of copies generated by replication unit A. In other cases the number of the copies may be different than generated by replication unit A.

Virtual switch A then forwards the replicated multicast packets using tunnel encapsulation to the other servers of the multicast group in the virtual network according to the multicast tree. Virtual switch A may encapsulate each of the copies of the packet in a unique tunnel encapsulation header as specified by one of NFTs in virtual switch A as configured by virtual network controller . In this way multiple equal cost paths in the underlying network may be used for the same multicast group to efficiently use bandwidth. The unique tunnel encapsulation headers may be configured by virtual network controller such that each link direction in the multicast tree has a unique virtual network tunnel encapsulation. The replication and forwarding of multicast packets is only performed by virtual switches executed on servers in the virtual network. No replication is performed within the underlying network. In this way the techniques enable multicast service within a virtual network without requiring multicast support in the underlying network.

In this example chassis switch CH which may be any of chassis switches of is coupled to Top of Rack TOR switches A B TORs by chassis link A and chassis link B respectively chassis links . TORs may in some examples be any of TORs of . In the example of TORs are also coupled to servers A B servers by TOR links A D TOR links . Servers may be any of servers of . Servers communicate with TORs and may physically reside in either associated rack. TORs each communicate with a number of network switches including chassis switch .

Chassis switch has a processor A in communication with an interface for communication with a network as shown as well as a bus that connects a memory not shown to processor A. The memory may store a number of software modules. These modules include software that controls network routing such as an Open Shortest Path First OSPF module not shown containing instructions for operating chassis switch in compliance with the OSPF protocol. Chassis switch maintains routing table RT table A containing routing information for packets which describes a topology of a network. Routing table A may be for example a table of packet destination Internet protocol IP addresses and the corresponding next hop e.g. expressed as a link to a network component. TORs each includes a respective processor B C an interface in communication with chassis switch and a memory not shown . Each memory contains software modules including an OSPF module and routing table B C as described above.

TORs and chassis switch may exchange routing information specifying available routes such as by using a link state routing protocol such as OSPF or IS IS. TORs may be configured as owners of different routing subnets. For example TOR A is configured as the owner of Subnet 1 which is the subnet 10.10.10.0 24 in the example of and TOR A is configured as the owner of Subnet 2 which is the subnet 10.10.11.0 24 in the example of . As owners of their respective Subnets TORs locally store the individual routes for their subnets and need not broadcast all route advertisements up to chassis switch . Instead in general TORs will only advertise their subnet addresses to chassis switch .

Chassis switch maintains RT table A which includes routes expressed as subnets reachable by TORs based on route advertisements received from TORs . In the example of RT table A stores routes indicating that traffic destined for addresses within the subnet 10.10.11.0 24 can be forwarded on link B to TOR B and traffic destined for addresses within the subnet 10.10.10.0 24 can be forwarded on link A to TOR A.

In typical operation chassis switch receives Internet Protocol IP packets through its network interface reads the packets destination IP address looks up these addresses on routing table A to determine the corresponding destination component and forwards the packets accordingly. For example if the destination IP address of a received packet is 10.10.10.0 i.e. the address of the subnet of TOR A the routing table of chassis switch indicates that the packet is to be sent to TOR A via link A and chassis switch transmits the packet accordingly ultimately for forwarding to a specific one of the servers .

Similarly each of TORs receives IP packets through its network interface reads the packets destination IP addresses looks up these addresses on its routing table B C to determine the corresponding destination component and forwards the packets according to the result of the lookup.

Virtual network controller VNC of illustrates a distributed implementation of a VNC that includes multiple VNC nodes A N collectively VNC nodes to execute the functionality of a data center VNC including managing the operation of virtual switches for one or more virtual networks implemented within the data center. Each of VNC nodes may represent a different server of the data center e.g. any of servers of or alternatively on a server or controller coupled to the IP fabric by e.g. an edge router of a service provider network or a customer edge device of the data center network. In some instances some of VNC nodes may execute as separate virtual machines on the same server.

Each of VNC nodes may control a different non overlapping set of data center elements such as servers individual virtual switches executing within servers individual interfaces associated with virtual switches chassis switches TOR switches and or communication links. VNC nodes peer with one another using peering links to exchange information for distributed databases including distributed databases A K collectively distributed databases and routing information e.g. routes for routing information bases A N collectively RIBs . Peering links may represent peering links for a routing protocol such as a Border Gateway Protocol BGP implementation or another peering protocol by which VNC nodes may coordinate to share information according to a peering relationship.

VNC nodes of VNC include respective RIBs each having e.g. one or more routing tables that store routing information for the physical network and or one or more overlay networks of the data center controlled by VNC . In some instances one of RIBs e.g. RIB A may store the complete routing table for any of the virtual networks operating within the data center and controlled by the corresponding VNC node e.g. VNC node A .

In general distributed databases define the configuration or describe the operation of virtual networks by the data center controlled by distributed VNC . For instance distributed databases may include databases that describe a configuration of one or more virtual networks the hardware software configurations and capabilities of data center servers performance or diagnostic information for one or more virtual networks and or the underlying physical network the topology of the underlying physical network including server chassis switch TOR switch interfaces and interconnecting links and so on. Distributed databases may each be implemented using e.g. a distributed hash table DHT to provide a lookup service for key value pairs of the distributed database stored by different VNC nodes .

As illustrated in the example of distributed virtual network controller VNC includes one or more virtual network controller VNC nodes A N collectively VNC nodes . Each of VNC nodes may represent any of VNC nodes of virtual network controller of . VNC nodes peer with one another according to a peering protocol operating over network . Network may represent an example instance of switch fabric and or IP fabric of . In the illustrated example VNC nodes peer with one another using a Border Gateway Protocol BGP implementation an example of a peering protocol. VNC nodes provide to one another using the peering protocol information related to respective elements of the virtual network managed at least in part by the VNC nodes . For example VNC node A may manage a first set of one or more servers operating as virtual network switches for the virtual network. VNC node A may send information relating to the management or operation of the first set of servers to VNC node N by BGP A. Other elements managed by VNC nodes may include network controllers and or appliances network infrastructure devices e.g. L2 or L3 switches communication links firewalls and other VNC nodes for example. Because VNC nodes have a peer relationship rather than a master slave relationship information may be sufficiently easily shared between the VNC nodes . In addition hardware and or software of VNC nodes may be sufficiently easily replaced providing satisfactory resource fungibility.

Each of VNC nodes may include substantially similar components for performing substantially similar functionality said functionality being described hereinafter primarily with respect to VNC node A. VNC node A may include an analytics database A for storing diagnostic information related to a first set of elements managed by VNC node A. VNC node A may share at least some diagnostic information related to one or more of the first set of elements managed by VNC node A and stored in analytics database A and may receive at least some diagnostic information related to any of the elements managed by other VNC nodes . Analytics database A may represent a distributed hash table DHT for instance or any suitable data structure for storing diagnostic information for network elements in a distributed manner in cooperation with other VNC nodes . Analytics databases A N collectively analytics databases may represent at least in part one of distributed databases of distributed virtual network controller of .

VNC node A may include a configuration database A for storing configuration information related to a first set of elements managed by VNC node A. Control plane components of VNC node A may store configuration information to configuration database A using interface A which may represent an Interface for Metadata Access Points IF MAP protocol implementation. VNC node A may share at least some configuration information related to one or more of the first set of elements managed by VNC node A and stored in configuration database A and may receive at least some configuration information related to any of the elements managed by other VNC nodes . Configuration database A may represent a distributed hash table DHT for instance or any suitable data structure for storing configuration information for network elements in a distributed manner in cooperation with others of VNC nodes . Configuration databases A N collectively configuration databases may represent at least in part one of distributed databases of distributed virtual network controller of .

Virtual network controller may perform any one or more of the illustrated virtual network controller operations represented by modules which may include orchestration user interface VNC global load balancing and one or more applications . VNC executes orchestration module to facilitate the operation of one or more virtual networks in response to a dynamic demand environment by e.g. spawning removing virtual machines in data center servers adjusting computing capabilities allocating network storage resources and modifying a virtual topology connecting virtual switches of a virtual network. VNC global load balancing executed by VNC supports load balancing of analytics configuration communication tasks e.g. among VNC nodes . Applications may represent one or more network applications executed by VNC nodes to e.g. change topology of physical and or virtual networks add services or affect packet forwarding.

User interface includes an interface usable to an administrator or software agent to control the operation of VNC nodes . For instance user interface may include methods by which an administrator may modify e.g. configuration database A of VNC node A. Administration of the one or more virtual networks operated by VNC may proceed via user interface that provides a single point of administration which may reduce an administration cost of the one or more virtual networks.

VNC node A may include a control plane virtual machine VM A that executes control plane protocols to facilitate the distributed VNC techniques described herein. Control plane VM A may in some instances represent a native process. In the illustrated example control VM A executes BGP A to provide information related to the first set of elements managed by VNC node A to e.g. control plane virtual machine N of VNC node N. Control plane VM A may use an open standards based protocol e.g. BGP based L3VPN to distribute information about its virtual network with other control plane instances and or other third party networking equipment. Given the peering based model according to one or more aspects described herein different control plane instances e.g. different instances of control plane VMs A N may execute different software versions. In one or more aspects e.g. control plane VM A may include a type of software of a particular version and the control plane VM N may include a different version of the same type of software. The peering configuration of the control node devices may enable use of different software versions for the control plane VMs A N. The execution of multiple control plane VMs by respective VNC nodes may prevent the emergence of a single point of failure.

Control plane VM A communicates with virtual network switches e.g. illustrated VN switch executed by server using a communication protocol operating over network . Virtual network switches facilitate overlay networks in the one or more virtual networks. In the illustrated example control plane VM A uses Extensible Messaging and Presence Protocol XMPP A to communicate with at least virtual network switch by XMPP interface A. Virtual network route data statistics collection logs and configuration information may in accordance with XMPP A be sent as XML documents for communication between control plane VM A and the virtual network switches. Control plane VM A may in turn route data to other XMPP servers such as an analytics collector or may retrieve configuration information on behalf of one or more virtual network switches. Control plane VM A may further execute a communication interface A for communicating with configuration VM A associated with configuration database A. Communication interface A may represent an IF MAP interface.

VNC node A may include configuration VM A to store configuration information for the first set of element to and manage configuration database A. Configuration VM A although described as a virtual machine may in some aspects represent a native process executing on an operating system of VNC node A. Configuration VM A and control plane VM A may communicate using IF MAP by communication interface A and using XMPP by communication interface A. In some aspects configuration VM A may include a horizontally scalable multi tenant IF MAP server and a distributed hash table DHT based IF MAP database that represents configuration database A. In some aspects configuration VM A may include a configuration translator which may translate a user friendly higher level virtual network configuration to a standards based protocol configuration e.g. a BGP L3VPN configuration which may be stored using configuration database A. Communication interface may include an IF MAP interface for communicating with other network elements. The use of the IF MAP may make the storage and management of virtual network configurations very flexible and extensible given that the IF MAP schema can be dynamically updated. Advantageously aspects of virtual network controller may be flexible for new applications .

VNC node A may further include an analytics VM A to store diagnostic information and or visibility information related to at least the first set of elements managed by VNC node A. Control plane VM and analytics VM may communicate using an XMPP implementation by communication interface A. Analytics VM A although described as a virtual machine may in some aspects represent a native process executing on an operating system of VNC node A.

Analytics VM A may include analytics database A which may represent an instance of a distributed database that stores visibility data for virtual networks such as one of distributed database of distributed virtual network controller of . Visibility information may describe visibility of both distributed VNC itself and of customer networks. The distributed database may include an XMPP interface on a first side and a REST JASON XMPP interface on a second side.

Virtual network switch of server may implement the layer 3 forwarding and policy enforcement point for one or more endpoints and or one or more hosts. The one or more endpoints or one and or one or more hosts may be classified into a virtual network due to configuration from control plane VM A. Control plane VM A may also distribute virtual to physical mapping for each endpoint to all other endpoints as routes. These routes may identify the next hop mapping virtual IP to physical IP and the tunnel encapsulation technique e.g. one of IPinIP NVGRE VXLAN etc. . Virtual network switch may be agnostic to which actual tunneling encapsulation is used. Virtual network switch may also trap interesting layer 2 L2 packets broadcast packets and or implement proxy for the packets e.g. using one of Address Resolution Protocol ARP Dynamic Host Configuration Protocol DHCP Domain Name Service DNS etc.

In some cases different VNC nodes may be provided by different suppliers. However the peering configuration of VNC nodes may enable use of different hardware and or software provided by different suppliers for implementing the VNC nodes of distributed VNC . A system operating according to the techniques described above may provide logical view of network topology to end host irrespective of physical network topology access type and or location. Distributed VNC provides programmatic ways for network operators and or applications to change topology to affect packet forwarding and or to add services as well as horizontal scaling of network services e.g. firewall without changing the end host view of the network.

In some cases virtual network controller may create the multicast tree to balance the replication limit N and the tree depth D . Minimizing the replication limit for all the servers in the multicast tree avoids having an overly horizontal tree topology in which one or two servers in the tree incur the load of replicating and forwarding packets to a majority of the servers. The replication limit may be balanced against the tree depth to avoid having an overly vertical tree topology in which each server is making only a few e.g. one copies of the multicast packet for further transmission. In this way computing resources and network resources may be conserved and the computing load may be distributed across all the servers of the multicast group.

In the example illustrated in the number of servers in the multicast group of the multicast tree is equal to 10 M 10 and the multicast tree has two stages or levels of servers D 2 . Virtual network controller therefore may create the multicast tree illustrated in based on a replication limit equal to 3 N 3 which satisfies 2 N 9 2 N

When total number of nodes is not N 1 N 1 then some of the penultimate leaf nodes may not have N leafs. This is true of the multicast tree illustrated in in which the total number of nodes is not equal to 13 so not all of S S and S can have 3 leaf nodes. In such cases virtual network controller may generate a balanced multicast tree as illustrated in such that each penultimate leaf node will have a number of leaf nodes that differ with each other by N 2 .

The multicast tree illustrated in may be created by a virtual network controller e.g. virtual network controller from . Virtual network controller may create the illustrated multicast tree in order to further minimize a replication limit for each of the servers. In the example multicast trees in and the replication limit N was set equal to 3 based on the number of servers in the multicast group of the multicast tree being equal to 10 M 10 and the multicast trees having two stages or levels of servers D 2 . In the example multicast tree illustrated in the number of servers in the multicast group of the multicast tree is again equal to 10 M 10 except the tree topology has changed to include three stages or levels of servers D 3 . Virtual network controller therefore may create the multicast tree illustrated in based on a replication limit equal to 2 N 2 which satisfies 2 N 9 2 N

As shown in assuming the root server S is the source server S makes two packet copies and transmits the copies to S and S . S then makes two packet copies and transmits the copies to S and S . S also makes two packet copies and transmits the copies to S and S . S makes two packet copies and transmits the copies to S and S . S then makes one packet copy and transmits the copy to S . As can be seen each of the servers in the multicast tree is within the replication limit of 2 and the replication load is evenly distributed with each of S S S and S generating 2 copies N 2 and the S generating 1 copy 1

As shown in the specific example of computing device includes one or more processors one or more communication units one or more input devices one or more output devices and one or more storage devices . Computing device in the specific example of further includes operating system virtualization module and one or more applications A N collectively applications . Each of components and may be interconnected physically communicatively and or operatively for inter component communications. As one example in components and may be coupled by one or more communication channels . In some examples communication channels may include a system bus network connection interprocess communication data structure or any other channel for communicating data. Virtualization module and applications as well as operating system may also communicate information with one another as well as with other components in computing device .

Processors in one example are configured to implement functionality and or process instructions for execution within computing device . For example processors may be capable of processing instructions stored in storage devices . Examples of processors may include any one or more of a microprocessor a controller a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or equivalent discrete or integrated logic circuitry.

One or more storage devices may be configured to store information within computing device during operation. Storage devices in some examples are described as a computer readable storage medium. In some examples storage devices are a temporary memory meaning that a primary purpose of storage devices is not long term storage. Storage devices in some examples are described as a volatile memory meaning that storage devices do not maintain stored contents when the computer is turned off. Examples of volatile memories include random access memories RAM dynamic random access memories DRAM static random access memories SRAM and other forms of volatile memories known in the art. In some examples storage devices are used to store program instructions for execution by processors . Storage devices in one example are used by software or applications running on computing device e.g. operating system virtualization module and the like to temporarily store information during program execution.

Storage devices in some examples also include one or more computer readable storage media. Storage devices may be configured to store larger amounts of information than volatile memory. Storage devices may further be configured for long term storage of information. In some examples storage devices include non volatile storage elements. Examples of such non volatile storage elements include magnetic hard discs tape cartridges or cassettes optical discs floppy discs flash memories or forms of electrically programmable memories EPROM or electrically erasable and programmable memories EEPROM .

Computing device in some examples also includes one or more communication units . Computing device in one example utilizes communication units to communicate with external devices. Communication units may communicate in some examples by sending data packets over one or more networks such as one or more wireless networks via inbound and outbound links. Communication units may include one or more network interface cards IFCs such as an Ethernet card an optical transceiver a radio frequency transceiver or any other type of device that can send and receive information. Other examples of such network interfaces may include Bluetooth 3G and WiFi radio components.

Computing device in one example also includes one or more input devices . Input devices in some examples are configured to receive input from a user through tactile audio or video feedback. Examples of input devices include a presence sensitive display a mouse a keyboard a voice responsive system video camera microphone or any other type of device for detecting a command from a user. In some examples a presence sensitive display includes a touch sensitive screen.

One or more output devices may also be included in computing device . Output devices in some examples are configured to provide output to a user using tactile audio or video stimuli. Output devices in one example include a presence sensitive display a sound card a video graphics adapter card or any other type of device for converting a signal into an appropriate form understandable to humans or machines. Additional examples of output devices include a speaker a cathode ray tube CRT monitor a liquid crystal display LCD or any other type of device that can generate intelligible output to a user.

Computing device may include operating system . Operating system in some examples controls the operation of components of computing device . For example operating system in one example facilitates the communication of applications with processors communication units input devices output devices and storage devices . Applications may each include program instructions and or data that are executable by computing device . As one example application A may include instructions that cause computing device to perform one or more of the operations and actions described in the present disclosure.

In accordance with techniques of the present disclosure computing device may operate as an endpoint device of a virtual network such as one of servers in data center from . More specifically computing device may use virtualization module to execute one or more virtual switches not shown that create and manage one or more virtual networks as virtual overlay networks of a data center switch fabric. Communication units of computer device may receive communications from a virtual network controller for the virtual networks.

According to the techniques communication units may receive a multicast tree for a multicast group of a virtual network from the virtual network controller and communicate the multicast tree to a replication unit rep. unit executed on virtualization module . Communication units may then receive multicast packets for the multicast group to be forwarded on the virtual network. The multicast tree may instruct replication unit to replicate and forward the multicast packets to other endpoint devices according to the multicast tree. The multicast tree is calculated for the virtual network by the virtual network controller in a centralized location instead of in a distributed fashion by components in an underlying physical network. In addition the replication and forwarding of multicast packets is only performed by virtual switches executed on computing device and the other endpoint devices of the virtual network. No replication is performed within the underlying physical network. In this way the techniques enable multicast service between computing device and the other endpoint devices within the virtual network without requiring multicast support in the underlying network.

Virtual network controller uses tree unit to create a multicast tree for servers of a multicast group in a virtual network . In this way the multicast tree is created in a logically centralized location i.e. virtual network controller instead of in a distributed fashion by components in the underlying network that service the multicast group. Tree unit may create the multicast tree to facilitate delivery of multicast packets for the multicast group between two or more endpoints or servers in the virtual network.

In some cases the multicast tree may be a unidirectional multicast tree in which a root node of the multicast tree operates as the source of the multicast packets for the multicast group and the multicast packets are communicated in a single downstream direction from the root node. In other cases the multicast tree may be a bidirectional multicast tree in which any node of the multicast tree may operate as the source of the multicast packets for the multicast group and the multicast packets may be communicated in either an upstream or downstream direction from a root node of the multicast tree. According to the techniques tree unit may create bidirectional multicast trees because the virtual overlay network emulates L2 multicast which supports bidirectional multicast.

Tree unit may calculate the multicast tree based on topology information of the underlying physical network received e.g in accordance with a routing protocol executed by VNC . In addition tree unit may calculate the multicast tree in order to minimize a replication limit at each of the servers and balance the replication across the tree. In this way tree unit may create the multicast tree such that each of the source and intermediate servers performs a similar small amount of replication instead of the source server having to incur the load of replicating and forwarding the packets to all the servers that belong to the multicast group. Virtual network controller stores the multicast tree in memory . Virtual network controller then communicates the multicast tree to one or more of the servers of the multicast group .

Server A for example receives the multicast tree for the multicast group to which server A belongs from virtual network controller . Server A also receives multicast packets for the multicast group to be forwarded on the virtual network according to the multicast tree . Server A executes virtual switch A for the virtual network within hypervisor . Server A uses replication unit A of virtual switch A to replicate the multicast packets for the multicast group according to the multicast tree . For example if server A is the source server or an intermediate server in the multicast tree then replication unit A may create one or more copies of the packet as required by the multicast tree.

Server A then uses virtual switch A to forward the replicated multicast packets using tunnel encapsulation to the other servers of the multicast group in the virtual network according to the multicast tree . Virtual switch A may encapsulate each of the copies of the packet in a unique tunnel encapsulation header. In this way multiple equal cost paths in the underlying network may be used for the same multicast group to efficiently use bandwidth. The unique tunnel encapsulation headers may be configured by virtual network controller such that each link direction in the multicast tree has a unique virtual network tunnel encapsulation. The replication and forwarding of multicast packets is only performed by virtual switches executed on servers in the virtual network. No replication is performed within the underlying network. In this way the techniques enable multicast service within a virtual network without requiring multicast support in the underlying network.

The techniques described in this disclosure may be implemented at least in part in hardware software firmware or any combination thereof. For example various aspects of the described techniques may be implemented within one or more processors including one or more microprocessors digital signal processors DSPs application specific integrated circuits ASICs field programmable gate arrays FPGAs or any other equivalent integrated or discrete logic circuitry as well as any combinations of such components. The term processor or processing circuitry may generally refer to any of the foregoing logic circuitry alone or in combination with other logic circuitry or any other equivalent circuitry. A control unit including hardware may also perform one or more of the techniques of this disclosure.

Such hardware software and firmware may be implemented within the same device or within separate devices to support the various techniques described in this disclosure. In addition any of the described units modules or components may be implemented together or separately as discrete but interoperable logic devices. Depiction of different features as modules or units is intended to highlight different functional aspects and does not necessarily imply that such modules or units must be realized by separate hardware firmware or software components. Rather functionality associated with one or more modules or units may be performed by separate hardware firmware or software components or integrated within common or separate hardware firmware or software components.

The techniques described in this disclosure may also be embodied or encoded in an article of manufacture including a computer readable storage medium encoded with instructions. Instructions embedded or encoded in an article of manufacture including a computer readable storage medium encoded may cause one or more programmable processors or other processors to implement one or more of the techniques described herein such as when instructions included or encoded in the computer readable storage medium are executed by the one or more processors. Computer readable storage media may include random access memory RAM read only memory ROM programmable read only memory PROM erasable programmable read only memory EPROM electronically erasable programmable read only memory EEPROM flash memory a hard disk a compact disc ROM CD ROM a floppy disk a cassette magnetic media optical media or other computer readable storage media. In some examples an article of manufacture may include one or more computer readable storage media.

A computer readable storage medium comprises a non transitory medium. The term non transitory indicates that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM or cache .

Various examples have been described. These and other examples are within the scope of the following claims.

