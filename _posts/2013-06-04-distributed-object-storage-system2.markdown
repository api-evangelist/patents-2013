---

title: Distributed object storage system
abstract: A distributed object storage system has a monitoring agent and/or a maintenance agent configured to determine for each of a plurality of repair tasks the actual concurrent failure tolerance of a corresponding repair data object. The actual concurrent failure tolerance corresponds to the number of storage elements that store sub blocks of the repair data object and are allowed to fail concurrently.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09588862&OS=09588862&RS=09588862
owner: Amplidata NV
number: 09588862
owner_city: Lochristi
owner_country: BE
publication_date: 20130604
---
The present invention generally relates a distributed data storage systems. Typically such distributed storage systems are targeted at storing large amounts of data such as objects or files in a distributed and fault tolerant manner with a predetermined level of redundancy. The present invention relates more particularly to a distributed object storage system.

The advantages of object storage systems which store data objects referenced by an object identifier versus file systems such as for example US2002 0078244 which store files referenced by an inode or block based systems which store data blocks referenced by a block address in terms of scalability and flexibility are well known. Object storage systems in this way are able to surpass the maximum limits for storage capacity of file systems in a flexible way such that for example storage capacity can be added or removed in function of the needs without degrading its performance as the system grows. This makes such object storage systems excellent candidates for large scale storage systems.

Such large scale storage systems are required to distribute the stored data objects in the object storage system over multiple storage elements such as for example hard disks or multiple components such as storage nodes comprising a plurality of such storage elements. However as the number of storage elements in such a distributed object storage system increase equally the probability of failure of one or more of these storage elements increases. To cope therewith it is required to introduce a level of redundancy into the distributed object storage system. This means that the distributed object storage system must be able to cope with a failure of one or more storage elements without data loss. In its simplest form redundancy is achieved by replication this means storing multiple copies of a data object on multiple storage elements of the distributed object storage system. In this way when one of the storage elements storing a copy of the data object fails this data object can still be recovered from another storage element holding a copy. Several schemes for replication are known in the art in general replication is costly as the storage capacity is concerned. This means that in order to survive two concurrent failures of a storage element of a distributed object storage system at least two replica copies for each data object are required which results in storage capacity overhead of 200 which means that for storing 1 GB of data objects a storage capacity of 3 GB is required. Another well known scheme is referred to as RAID systems of which some implementations are more efficient than replication as storage capacity overhead is concerned. However often RAID systems require a form of synchronisation of the different storage elements and require them to be of the same type and in the case of drive failure require immediate replacement followed by a costly and time consuming rebuild process. Therefor known systems based on replication or known RAID systems are generally not configured to survive more than two concurrent storage element failures. Therefor it has been proposed to use distributed object storage systems that are based on erasure encoding such as for example described in WO2009135630 US2007 0136525 or US2008 313241. Such a distributed object storage system stores the data object in encoded sub blocks that are spread amongst the storage elements in such a way that for example a concurrent failure of six storage elements can be tolerated with a corresponding storage overhead of 60 that means that 1 GB of data objects only require a storage capacity of 1.6 GB.

Such an erasure encoding based districted object storage system for large scale data storage also requires a form a self healing functionality in order to restore the required redundancy policy after for example the failure of a storage element. However most in known systems these self healing methods lack efficiency and consume considerable amounts of processing power and or network bandwidth in order for example to cope with restoring the redundancy for the stored data objects on a failed storage element. One system that tries to improve efficiency is for example described in WO2010 091101 however this system could result to data loss after subsequent generations of node failure. Furthermore this system is only able to handle the restore of a complete storage element and all objects on it. It is further not able to handle simultaneous replacement of a plurality of storage elements reliably and efficiently as for every failing storage element a new storage element needs to be provided for the restore operation.

In general during maintenance of a large scale distributed object storage system adding removing and or replacing storage elements or even complete storage nodes is an activity that is performed almost constantly. However in prior art systems the efficiency of repair activity during normal operation does not suffice to reliably cope with these maintenance activities resulting in manual configuration or supplementary restore operations to be performed in order to sufficiently safeguard the reliability of the distributed object storage system.

Therefor there still exists a need for an efficient and reliable monitoring and repair process for a distributed object storage system that does not result in data loss in the long term and is able to realize a large scale self healing distributed object storage system. Further there exists a need for the self healing efficiency being sufficiently high such that the need for manual configuration or supplementary restore operations is reduced even during extensive changes to the available storage elements or storage nodes.

According to a first aspect of the invention there is provided a distributed object storage system comprising 

said monitoring agent and or said maintenance agent are configured to determine the actual concurrent failure tolerance of said repair data object said actual concurrent failure tolerance corresponding to the number of storage elements that actually store sub blocks of said repair data object and are allowed to fail concurrently.

This enables a distributed object storage system with a self healing function repair function with a high efficiency which increases reliability of a large scale distributed object storage system not even on a very large storage capacity scale and in a long term time scale and during intensive maintenance operations.

According to a second aspect of the invention there is provided a method for maintaining a distributed object storage system according to the first aspect of the invention 

Characterized In That said method comprises the step of said at least one monitoring agent and or said at least one maintenance agent determining the actual concurrent failure tolerance of said repair data object said actual concurrent failure tolerance corresponding to the number of storage elements that actually store sub blocks of said repair data object and are allowed to fail concurrently.

According to alternative embodiments of the distributed object storage system could comprise any other suitable number of storage nodes and for example two three or more controller nodes also connected to these storage nodes . These controller nodes and storage nodes can be built as general purpose computers however more frequently they are physically adapted for arrangement in large data centres where they are arranged in modular racks comprising standard dimensions. Particular controller nodes and storage nodes such as for example the Amplistor AS20 storage node as manufactured by Amplidata are dimensioned to take up a single unit of such rack which is generally referred to as 1 U.

As shown in several storage nodes can be grouped together for example because they are housed in a single rack . For example storage nodes . . . . . . . and . . each are respectively grouped into racks . . . . . .. Controller node could for example be located in rack .. These racks are not required to be located at the same location they are often geographically dispersed across different data centres such as for example rack . . can be located at a data centre in Europe . . at a data centre in the USA and . . at a data centre in China.

Taking into account the distributed object storage system comprises a plurality of redundant storage elements . The storage nodes each comprise a share of these storage elements . As shown in storage node . comprises ten storage elements . .. Other storage nodes could comprise a similar amount of storage elements but this is however not essential. Storage node . could for example comprise eight storage elements . .. As will be explained in further detail below with respect to the distributed object storage system is operable to store and retrieve a data object comprising object data for example 64 MB of binary data and a data object identifier for addressing this data object for example a universally unique identifier such as a globally unique identifier GUID . Storing the data offered for storage by the application in the form of a data object also referred to as object storage has specific advantages over other storage schemes such as conventional block based storage or conventional file based storage such as scalability and flexibility which are of particular importance in a distributed object storage system that is directed to large scale redundant storage applications sometimes also referred to as cloud storage.

The storage elements are redundant and operate independently of one another. This means that if one particular storage element fails its function can easily be taken on by another storage element in the distributed storage system. However as will be explained in more detail further below there is no need for the storage elements to work in synchronism as is for example the case in many well known RAID configurations which sometimes even require disc spindle rotation to be synchronised. Furthermore the independent and redundant operation of the storage elements allows to use any suitable mix of types storage elements to be used in a particular distributed object storage system . It is possible to use for example storage elements with differing storage capacity storage elements of differing manufacturers using different hardware technology such as for example conventional hard disks and solid state storage elements using different storage interfaces such as for example different revisions of SATA PATA and so on. All this results in specific advantages for scalability and flexibility of the distributed object storage system as it allows to add replace or remove storage elements without imposing specific requirements to their design in correlation to other storage elements already in use in that distributed object storage system .

According to an alternative embodiment the controller node could have an identical design as a storage node or according to still a further alternative embodiment one of the storage nodes of the distributed object storage system could perform both the function of a controller node and a storage node . According to still a further embodiment the device on which the application runs is a controller node .

As schematically shown in controller node comprises four modules an encoding module a spreading module a clustering module and a decoding module . These modules can be implemented as programming instructions stored in local memory of the controller node for execution by its processor .

The functioning of these modules will now be explained to . The distributed object storage system stores a data object offered by the application in function of a reliability policy which guarantees a level of redundancy. That means that the distributed object storage system must for example guarantee that it will be able to correctly retrieve data object even if a number of storage elements would be unavailable for example because they are damaged or inaccessible. Such a reliability policy could for example require the distributed object storage system to be able to retrieve the data object in case of six concurrent failures of the storage elements it comprises. In large scale data storage massive amounts of data are stored on storage elements that are individually unreliable as such redundancy must be introduced into the storage system to improve reliability. However the most commonly used form of redundancy straightforward replication of the data on multiple storage elements is only able to achieve acceptable levels of reliability at the cost of unacceptable levels of overhead. For example in order to achieve sufficient redundancy to cope with six concurrent failures of storage elements data objects would need to be replicated six times and stored on redundant storage elements . This means that next to the master copy of a data object stored on one storage element six replica s must be stored on six other storage elements. As such storing 1 GB of data objects in this way would result in the need of 7 GB of storage capacity in a distributed object storage system this means an increase in the storage cost by a factor of seven or an additional storage overhead of 600 . Therefor the distributed object storage system according to the invention makes use of erasure coding techniques in order to achieve the requirements of the reliability policy with considerably less overhead. As will be explained in further detail below when using an erasure encoding with a rate of encoding r 10 16 six concurrent failures of storage element can be tolerated which only require a storage overhead of 60 or a storage cost by a factor of 1.6. This means that storing 1 GB of data objects in this way only results in the need of 1.6 GB of storage capacity in the distributed object storage system . Some erasure encoding techniques make use of Reed Solomon codes but also fountain codes or rateless erasure codes such as online codes LDPC codes raptor codes and numerous other coding schemes are available.

Subsequently as shown in the spreading module will store the predetermined number x n 800 16 12800 of encoded redundant sub blocks . . on a number of storage elements which corresponds to said desired spreading width n 16 such as for example storage elements . .. The spreading module will store on each of these storage elements . . said predetermined multiple x 800 of these sub blocks . As shown in sub blocks . . are stored on storage element . the next x 800 of these sub blocks are stored on storage element . and so on until the last x 800 of these sub blocks . are stored on storage element .. As shown in storage elements . . are arranged in storage node . and storage elements . . are arranged in storage node ..

According to an alternative embodiment the sub blocks could be spread by the spreading module on a number of storage elements which is larger than said desired spreading width n 16 for example n 1 16 1 17 storage elements . This could be implemented by for example storing sub blocks . . on storage element . and storing sub blocks . on storage element .. It is clear that this would still allow for the storage system to cope with f 6 concurrent failures of storage elements . Alternative methods for determining the share of sub blocks to be stored on specific storage elements are well known to the person skilled in the art and are for example described in WO2009135630.

It is clear that according to alternative embodiments of the invention other values could have been chosen for the parameters x f k n k f and r k n mentioned in embodiment above such as for example x 400 f 4 k 12 n k f 12 4 16 and r 12 16 or any other possible combination that conforms to a desired reliability policy for redundancy and a corresponding desired concurrent failure tolerance of storage elements of the distributed object storage system .

According to still a further alternative there could be provided a safety margin to the number of concurrent failures f that a distributed object storage system needs to be able to cope with. In such an embodiment some of the efficiency is traded in for some additional redundancy over what is theoretically required. This preventively increases the tolerance for failures and the time window that is available for a repair activity. However according to a preferred embodiment this safety margin will be rather limited such that it only accounts for an increase in sub blocks that must be generated and stored of for example approximately 10 to 30 such as for example 20 .

As shown in the distributed object storage system further comprises a monitoring agent which monitors whether the distributed object storage system still correctly stores all data objects that were offered for storage by the application . The monitoring agent could perform this monitoring activity on a periodic basis for example it could check every storage element every ten days or it could be triggered by a specific event such as for example a signal indicating that one or more particular storage elements or one or more storage nodes are not available. The monitoring agent can be implemented as programming instructions stored in local memory of the controller node for execution by its processor or alternatively in local memory of one or more of the storage nodes for execution by their processor or any other suitable combination. Multiple monitoring agents could be active concurrently within a distributed object storage system according to the invention. The only requirement is that the monitoring agent is able to connect to the storage nodes comprising data objects being stored in the distributed object storage system which need to be subject to the monitoring process.

When during the monitoring process the monitoring agent detects a data object that is not correctly stored in the distributed object storage system the monitoring agent will treat the data object identifier of this incorrectly stored data object also referred to as a repair data object as a repair data object identifier and is then able to create a repair task comprising this repair data object identifier . Repair tasks created by one or more monitoring agents can for example be aggregated in a suitable programmable storage structure such as for example a repair task list in the memory of the controller node or one or more of the storage nodes of the distributed object storage system . In this way for data objects stored in the distributed data storage system in need of repair a corresponding entry in the repair task list will be available. An embodiment of such a repair task list comprising a plurality of repair tasks is schematically shown in and will be explained in more detail below. The column REP.OBJ.ID of the repair task list comprising the repair data object identifier for each of the corresponding repair tasks .

According to a preferred embodiment of the invention the monitoring agent checks the status of the storage elements of the distributed object storage system regularly. These checks result in the monitoring agent storing the status of these storage elements in a central infrastructure metadata storage . Such a central infrastructure metadata storage is schematically shown in . This central infrastructure metadata storage could be implemented as any type of programmable storage structure for example in the memory of the controller node . The central infrastructure metadata storage according to this embodiment stores infrastructure metadata comprising an identifier for each of the storage elements in the column SE.ID and the status of these storage elements in the column ERROR. According to this embodiment the status is indicated with a zero if the respective storage element is available and a non zero value if the respective storage element is not available. In this way the infrastructure metadata storage allows the status of all storage elements used in the distributed object storage system to be centrally recorded. Possible states for a storage element are for example available and unavailable. Alternatively each of these states can be further identified by a plurality of suitable identifiers indicating more specific status information for example according to an embodiment of the invention instead of a single non zero identifier indicating that the storage element is unavailable specific non zero identifiers in the status column could indicate whether the storage element is offline abandoned or some other state.

In order to check the status of the storage elements of the distributed object storage system according to a specific embodiment the monitoring agent contacts on a periodic basis each of the storage nodes to get current status of each of their storage elements these storage elements being for example hard disk drives. Each storage node periodically assessing the quality of its these hard disk drives by for example obtaining the SMART disk information by doing explicit disk tests or by keeping its own log of read and write errors or by using some other methods. Once the status of these hard disk drives is determined the storage node reports this to the central infrastructure metadata storage updating the entries for the corresponding list of storage element statuses . Once a storage element is marked unavailable in the infrastructure metadata storage it will not be used for further storage or retrieval operations and the distributed object storage system can for example based on policies try to power down this storage element if it is still reachable in order to conserve energy use. Optionally an operator could be provided with a signal to remove or replace this storage element . This is especially useful during maintenance operations on large scale distributed object storage systems where adding removing and or replacing storage elements or even complete storage nodes is an activity that is performed almost constantly. Such maintenance operations on the distributed object storage system can be performed while it is in operation reducing the need for manual reconfiguration and even reducing the need for some of the maintenance operations such as for example removing defective storage elements .

Subsequently the monitoring agent could mark data objects for repair of which sub blocks are stored on a storage element of which the infrastructure metadata indicates it is unavailable. This can for example be implemented by combining both the metadata of the data objects and the infrastructure metadata of said storage elements as will be explained with reference to .

Alternatively instead of monitoring the status of the storage elements of the distributed object storage system the monitoring agent could monitor the availability of all storage nodes and record this centrally in the infrastructure metadata storage . A storage node can be available or unavailable or some other state. According to one embodiment a monitoring agent running on a controller node can record the availability of its connected storage nodes in the central infrastructure metadata storage by updating the status of the storage elements of the respective storage nodes accordingly. Whenever all storage elements of a storage node become unavailable in the infrastructure metadata storage this storage node will not be used for further storage or retrieval operations and the distributed object storage system can for example based on policies try to powered down this storage node if it is still reachable in order to conserve energy use. Optionally an operator could be provided with a signal to remove or replace this storage node . As stated above this is especially useful during maintenance operations on large scale distributed object storage systems where adding removing and or replacing storage elements or even complete storage nodes is an activity that is performed almost constantly.

As shown in the embodiment of the distributed storage system also comprises a metadata storage . Such a metadata storage could be implemented as a central metadata storage arranged in a controller node and or a plurality of distributed local metadata storages arranged in the storage nodes . Several alternative embodiments implementing a metadata storage in this way have been described in more detail in co pending application EP11187082.0 which are hereby referred to. During a storage operation the encoding module and or the spreading module add for every data object they store a respective entry for its data object identifier a list of identifiers of the storage elements on which sub blocks of this data object are stored and an identifier for the type of encoding policy that was used to disassemble this data object . When the metadata storage is implemented as a central metadata storage the controller node comprises such a metadata storage which is for example implemented as a suitable programmable storage facility in its memory as this allows the encoding module spreading module clustering module and decoding module optimal access to this centrally arranged metadata storage during storage retrieval and repair operations. According to this embodiment the spreading module will add to this central metadata storage the metadata for each data object stored by the controller node comprising this metadata storage . As schematically shown in such a metadata storage comprises for example metadata for three data objects identified as A B and C this metadata comprising the data object identifier in the OBJ ID column a list of the identifiers of the storage elements where the sub blocks of the respective data objects were stored by the spreading module in the SE.ID column and the encoding policy used by the encoding module in the ENC column. In this example the encoding policy used is identified as for example 16 10 which identifies both the minimal spreading requirement k 10 and the desired spreading width n 16. The desired concurrent failure tolerance f n k 16 10 6 can be easily derived from this identifier. It is clear that alternative identifiers for the encoding policy could be used like for example 16 6 indicating the desired spreading width n 16 an de desired concurrent failure tolerance f 6 or an integer identifier which is linked to a specific encoding policy or any other suitable identifier for the type of encoding used etc. Amongst other things the metadata storage allows for efficient retrieval as the clustering module which is able to ascertain from the metadata storage which in this embodiment also resides centrally in the controller node on which specific storage elements a specific data object is stored. It also allows the decoding module to efficiently ascertain which encoding policy was used. Furthermore such a metadata storage also allows for flexibility in the determination of encoding policies on a per data object basis this means that the redundancy policy does not need to be fixed for all data objects stored in the distributed object storage system but can be set individually for each specific data object . It is clear that according to still further embodiments of the metadata storage the metadata stored for a data object could comprise other suitable entries comprising properties of the data object such as for example version information or the state of the data object which could for example comprise an indication whether the data object was subject to a delete operation.

As explained above the metadata storage allows the controller node to determine on which storage elements sub blocks of a particular data object can be retrieved. This can be done with high efficiency when consulting the metadata storage . In the schematic example shown in it is shown in its metadata that sub blocks of data object with object identifier A are stored on storage elements with identifiers and that the type of encoding used is characterised by a minimal spreading requirement k 10 and a desired spreading width n 16 which leads to a desired concurrent failure tolerance f n k 16 10 6. Sub blocks of data object with object identifier B are stored on storage elements with identifiers and that the type of encoding used is characterised by a minimal spreading requirement k 10 and a desired spreading width n 18 which leads to a desired concurrent failure tolerance f n k 18 10 8. Sub blocks of data object with object identifier C are stored on storage elements with identifiers and that the type of encoding used is characterised by a minimal spreading requirement k 10 and a desired spreading width n 16 which leads to a desired concurrent failure tolerance f n k 18 10 6.

When now combining the information provided in the infrastructure metadata storage as shown in which shows that the storage elements with identifiers and are unavailable and the metadata storage as shown in it is possible to determine that each of the objects A B and C is no longer correctly stored as each of them comprises sub blocks which are stored on a storage element that is unavailable. Therefor based upon this information the monitoring agent will be able to add corresponding repair tasks with corresponding repair data object identifiers A B and C to the repair task list as shown in . However in a large scale distributed object storage system when a plurality of storage elements or storage nodes becomes unavailable for example during maintenance operations or when a large number of data objects are stored on specific storage elements or in specific storage nodes there is the risk that a large number of repair tasks are added to the repair task list . It is then important to restore the repair data objects to their desired concurrent failure tolerance as soon as possible. According to the invention the monitoring agent will therefore determine the actual concurrent failure tolerance of the repair data object . The actual concurrent failure tolerance corresponds to the number of storage elements that actually store sub blocks of the repair data object and are still allowed to fail concurrently. Therefor according to the embodiment shown in the actual concurrent failure tolerance for data object A is 4 as storage elements storing its sub blocks with identifier and are unavailable and the desired concurrent failure tolerance was 6. The actual concurrent failure tolerance for data object B is equally 4 as storage elements storing its sub blocks with identifier and are unavailable and the desired concurrent failure tolerance was 8. The actual concurrent failure tolerance for data object C is 5 as only storage elements storing its sub blocks with identifier is unavailable and the desired concurrent failure tolerance was 8. According to the embodiment as shown in the monitoring agent adds the actual concurrent failure tolerance for each of the repair data objects to the respective repair task in the repair task list as illustrated with the ACFT column.

Now that the monitoring agent has determined the actual concurrent failure tolerance for each of the repair data objects the maintenance agent will be able to execute these repair tasks from the repair task list with an increased efficiency as the maintenance agent will now be able to execute first repair tasks of which said actual concurrent failure tolerance is the lowest. In this way the maintenance agent will be able to more efficiently focus first on the repair data objects that are most critically in need of repair which will contribute to the overall reliability of the distributed object storage system . It is clear that the maintenance agent will in this way enable the maximum mean actual concurrent failure tolerance for all repair data objects being processed. This allows for reliably processing even a large number of repair tasks in a large scale distributed object storage system automatically and by means of the already available monitoring and repair facilities. According to a particular embodiment as shown in this means that the maintenance agent will first process the repair tasks associated with repair data objects A or B as they have the lowest actual concurrent failure tolerance of 4 and afterwards process repair data object C as its actual concurrent failure tolerance is 5. According to an alternative embodiment of the invention it is equally possible for the maintenance agent to determine the actual concurrent failure tolerance for the repair tasks of the repair task list before it starts processing it instead of relying on the monitoring agent to do this.

In order to still further enhance the efficiency of the repair process optionally the monitoring agent and or the maintenance agent are able to determine for the repair data objects the difference between the actual concurrent failure tolerance and the desired concurrent failure tolerance of the repair data object . In the example shown in this difference corresponds to 2 6 4 for data object A 4 8 4 for data object B and 1 6 5. In this example this difference could also be determined from the number of storage elements that store sub blocks for the respective repair data objects and are unavailable. Optionally the monitoring agent and or the maintenance agent could store this difference in the repair task list as schematically shown with column DIFF in . This will subsequently allow the maintenance agent to execute first the repair tasks of the repair task list of which this difference between said actual concurrent failure tolerance and said desired concurrent failure tolerance is the highest. This means that although for both repair data object A and B the actual concurrent failure tolerance is 4 the maintenance agent will first process the repair task associated with repair data object B as this difference is 4 for repair data object B which is larger than this difference for data object A which is 2. In this way the maintenance agent will restore the repair data objects that are removed the furthest from their desired concurrent failure tolerance first increasing the overall reliability of the distributed object storage system .

As further shown in the distributed object storage system further comprises a maintenance agent that is able to process the repair tasks by checking the repair task list on a periodic basis and processing one of the repair tasks in it. Optionally the distributed object storage system comprises a plurality of these maintenance agents . The maintenance agent can be implemented as programming instructions stored in local memory of the controller node for execution by its processor or alternatively in local memory of one or more of the storage nodes for execution by their processor or any other suitable combination. The maintenance agent must however be enabled to connect to the controller node in order to process the repair tasks from the repair task list and thereby repairing the corresponding repair data object stored in the distributed object storage system .

According to a preferred embodiment as disclosed in co pending application EP11187082.0 the maintenance agent retrieves a repair task that was created by the monitoring agent from the repair task list as described above and determines the corresponding repair data object identifier comprised within this repair task . Subsequently the maintenance agent will instruct the clustering module to execute a repair retrieval operation for the repair data object which corresponds to the repair data object identifier . When executing this repair retrieval operation the clustering module will report back to maintenance agent the number of missing sub blocks which corresponds to the number of sub blocks said clustering module is not able to collect. As such the maintenance agent is able to determine the number of missing sub blocks for this repair data object. For example a repair task could have been created for the data object as described in for example because a signal was generated by the distributed object storage system indicating to the monitoring agent that storage node . was no longer reachable and subsequently the monitoring agent determined that this data object was no longer correctly stored in the distributed object storage system . The clustering module when now processing the corresponding repair retrieval operation will initiate an attempt to retrieve the predetermined number x n 16 800 12800 sub blocks . . relating to said repair object identifier. The clustering module will be able to retrieve sub blocks . . stored on storage elements . . that are part of storage node . while the clustering module will not be able to retrieve sub blocks . . that were stored on storage elements . . that are part of storage node .. The clustering module in this particular case will report to the maintenance agent that there are 4800 missing sub blocks for this repair data object identifier . However as the clustering module was able to retrieve a number of collected sub blocks . . equal to or greater than the predetermined multiple of said minimal spreading requirement x k 800 10 8000 these sub blocks . . will subsequently enable the decoding module as instructed by the maintenance agent to successfully assemble the repair data object as the original data object as shown in thereby successfully completing the repair retrieval request.

Subsequently as shown in the maintenance agent will initiate a repair storage operation by instructing the encoding module to disassemble this repair data object into a specific number of repair sub blocks . . specific number of repair sub blocks being equal to said number of missing sub blocks . .. In this particular example the number of missing sub blocks as well as this specific number of repair sub blocks being equal to 4800.

It is not necessary to know exactly which sub blocks are missing as the probability of generating repair sub blocks which overlap with the already stored sub blocks is sufficiently low as the encoding technology such as for example when based on online codes makes use of randomly generated numbers such as for example described in more detail in WO2009135630 which reduces the probability of overlap significantly as is well known to the man skilled in the art. In order to still further reduce the probability of overlap in the case of online codes as for example described in WO2009135630 the random number generator could be seeded during the repair storage operation with a seed that is known to be different from the seeds used during a normal storage operation. Alternative examples for generating sub block without a realistic probability of overlap with erasure encoding technology are within the knowledge of the man skilled in the art.

The repair storage operation is finalised by the spreading module which stores this specific number of repair sub blocks . . such that these repair sub blocks and the collected sub blocks . . are stored on a number of said storage elements being larger or equal to said desired spreading width n 16. In the example shown in this is accomplished by the spreading module storing repair sub blocks . . on storage elements . . the share of sub blocks to be stored on the respective storage elements again being determined according to methods being well known to the person skilled in the art such as for example as shown in repair sub blocks . . on storage element . . . . repair sub blocks . . on storage element .. Alternative methods for determining the share of sub blocks to be stored on specific storage elements are well known to the person skilled in the art and are for example described in WO 2009135630.

It is clear that the maintenance agent in this way is able to process a repair task far more efficiently as compared to prior art systems which make use of the regular retrieval and or storage operation. There is no need for the maintenance agent of the distributed object storage system according to the invention to determine and keep track of exactly what sub blocks are missing and as such the repair retrieval operation and repair storage operation can be executed more efficiently than prior art repair processes which focus on restoring the specific sub blocks which are missing or corrupt. Furthermore prior art systems don t provide a tailored retrieval or storage operation for the repair operation and as such a fall back on the regular retrieval and storage process the latter of which will store said predetermined number x n 800 16 12800 of sub blocks and will thus consume more resources such as processing power for the encoding module network bandwidth for the spreading module and storage capacity of the storage elements of the distributed object storage system .

It is clear that the maintenance agent will still be able to process a repair task more efficiently then prior art systems when the number of repair sub blocks generated is equal to the number of missing sub blocks augmented by a safety margin. In such an embodiment some of the efficiency is traded in for some additional redundancy in order to preventively cope with future repair needs. As long as the number of missing sub blocks augmented by the safety margin is less then said predetermined number the repair process will be more efficient then known systems however according to a preferred embodiment this safety margin will be a rather limited amount such as for example an integer value approximating a share of 10 to 20 of the number of missing sub blocks. It is clear however that the most efficient repair process can be achieved when the safety margin is equal to zero.

In order to still further improve the efficiency of the repair operation the infrastructure metadata storage could also be used to increase the efficiency of the repair retrieval operation. For example when the maintenance agent instructs the clustering module to retrieve said predetermined number of sub blocks comprising said repair data object identifier is operable to further instruct said clustering module to consult said central infrastructure metadata storage . During the subsequent retrieval of a number of stored sub blocks equal to or greater then said predetermined multiple of said minimal spreading requirement x k 800 10 8000 the clustering module will exclusively consult storage elements of which the status in said infrastructure metadata indicates they are available. In this way no time and network traffic is lost on accessing unavailable storage elements or on waiting for sub blocks that are unavailable.

However in order to still further enhance the efficiency and scalability of the repair process there is provided a preferred embodiment as schematically represented in . In a large scale distributed object storage system where numerous data objects are stored and a plurality of monitoring agents that concurrently add repair tasks the repair task list for every repair data object they encounter during the monitoring process this repair task list could be too dynamic and or grow too large to handle efficiently for example during a sorting operation or an insertion operation for arranging the repair tasks of the repair task list in a suitable order with respect to the actual concurrent failure tolerance for handling by the one or more maintenance agents . Therefor according to this preferred embodiment at least one of the monitoring agents will perform a specific data mining operation on the distributed object storage system in order to limit the number of repair tasks added to the repair task list such that it remains manageable even in a large scale distributed object storage system and simplifies or eliminates the sorting operation or insertion operation of repair tasks in the repair task list during the monitoring and or repair process. During this data mining operation the monitoring agent will scan the distributed object storage system in order to determine how many repair data objects correspond to a specific actual concurrent failure tolerance. This can be done quickly and efficiently by for example combining the infrastructure metadata with the metadata as explained with reference to . As schematically shown in this monitoring agent could for example aggregate an actual concurrent failure tolerance list comprising the count of repair data objects shown in the DO column for each instance of the actual concurrent failure tolerance shown in the ACFT column. As shown in the example there are 0 repair data objects with an actual concurrent failure tolerance of 0 or 1 there are 3 repair data objects with an actual concurrent failure tolerance of 2 10 repair data objects with an actual concurrent failure tolerance of 3 80 repair data objects with an actual concurrent failure tolerance of 4 etc.

As schematically shown in based on this actual concurrent failure tolerance list all other monitoring agents that concurrently add repair tasks to the repair task list will be able to determine a first predetermined threshold. The number of repair tasks aggregated in the repair task list by these monitoring agents can then be kept manageable when repair tasks are aggregated in the repair task list only if the actual concurrent failure tolerance of the corresponding repair data object is smaller than or equal to this first predetermined threshold. Determining the first predetermined threshold is preferably done in function of the lowest actual concurrent failure tolerance for which the corresponding count of repair data objects is larger than zero which in the example shown in is the actual concurrent failure tolerance of 2. The first predetermined threshold could simple be equal to this actual concurrent failure tolerance with a non zero count in which case the monitoring agents will only add repair tasks for repair data objects of which the actual concurrent failure tolerance is smaller than or equal to 2. But alternatively the first predetermined threshold could be determined in function of this smallest actual concurrent failure tolerance with a non zero count for example augmenting it with an integer value of 1 or 2 so that the monitoring agents would add repair tasks for repair data objects of which the actual concurrent failure tolerance is smaller than or equal to 3 or 4. It is clear that many alternative ways of determining a suitable first predetermined threshold are available that can be calculated as a function of at least this lowest actual concurrent failure tolerance for which the count of repair data objects is larger than zero.

In this way the repair tasks aggregated in the repair task list will be limited to those with an actual concurrent failure tolerance lower than the first predetermined threshold. This will enable the maintenance agent to prioritise the repair process of repair data objects with the lowest actual concurrent failure tolerance even if the repair task list is implemented as a simple first in first out FIFO structure while keeping the size of the repair task list manageable. In the example shown in and in an embodiment in which the first predetermined threshold is set to the lowest non zero actual concurrent failure tolerance. Then the first predetermined threshold will be set initially to 2 and the monitoring agents operating on the distributed object storage system during their monitoring operation will only add the 3 repair data objects with an actual concurrent failure rate of 2 when they detect them. This can be done efficiently as explained above with respect to by combining the information of the infrastructure metadata storage and the metadata storage . The one or more maintenance agent can start processing these repair tasks as soon as they are added to the repair task list . The repair process in itself which is inherently slower than the monitoring process in this way does not create a bottleneck for the monitoring process. As soon as the repair process for these 3 repair data objects is complete or has at least reached a level at which these 3 repair data objects are stored with an actual concurrent failure tolerance that is higher than 2 then the monitoring agent performing the data mining operation will produce an updated actual concurrent failure tolerance list in which the repair data object count for the actual concurrent failure tolerance instance 2 has reached 0. The monitoring agents will then automatically lift the threshold to the next instance of an actual concurrent failure tolerance with a non zero repair data object count which in the example will then be 3. The monitoring agents will then be adding repair tasks for the 10 repair data objects with an actual concurrent failure tolerance of 3 to the repair task list . Once the repair agents have handled these repair tasks also the repair data object count for the actual concurrent failure tolerance of 3 will fall to 0 and the same process will be repeated for the 80 repair data objects with a concurrent failure tolerance of 4 and so on until all repair data objects have been processed.

The monitoring and repair process as described above with reference to can run on a continuous basis because as soon as the monitoring agent aggregating the actual concurrent failure tolerance list updates this list with a non zero repair data object count for a lower instance of the actual concurrent failure tolerance then the first predetermined threshold will be adapted accordingly and the monitoring agents will again be adding only repair tasks to the repair task list for this lower instance of the actual concurrent failure tolerance. In order to guarantee a specific turnaround time for the repair operations it is preferred to limit the maximum number of repair tasks that can created in the repair task list when it is implemented as a FIFO structure which can be calculated in function of desired worst case turnaround time of this FIFO structure for handling a higher priority repair task . For example if the maximum number of repair tasks in the repair task list is limited to 60.000 and the worst case time for handling a repair task is 10 ms then the worst case turnaround time for handling a completely full repair task list will be 600 s or 10 minutes. As in this case the number of repair tasks in the repair task list is limited alternatively suitable sorting or insertion operations may be applied to the repair tasks so that those associated with the repair data objects with the lowest actual concurrent failure tolerance are handled first by the maintenance agent even further optimizing turnaround time for the most critical repair tasks .

According to a further preferred alternative embodiment the repair task list could be implemented as a last in first out LIFO structure. In that case repair tasks added last to the repair task list by the monitoring agents will be handled first by the repair agents . When combined with the repair and monitoring process as described above with reference to the repair task added last will always be for a repair data object with an actual concurrent failure tolerance lower or equal to the actual first predetermined threshold even in a situation where the first predetermined threshold is lowered as described above with reference to the alternative FIFO structure. In this way without requiring complex computing or memory intensive data operations the monitoring and repair operation is able to achieve an even further advance in worst case turnaround time for handling the most critical repair tasks without the need to put a limit on the number of repair tasks in the repair task list .

In order to still improve the scalability and robustness of the monitoring and repair process according to further optional embodiments in that the monitoring agents or said maintenance agent are configured to determine the difference between the actual concurrent failure tolerance and the desired concurrent failure tolerance of a repair data object such as for example described with reference to . The monitoring agents are then able to aggregate repair tasks in the repair task list only if the difference of the repair data object is larger than or equal to a second predetermined threshold which is similar as the first predetermined threshold determined in function of at least the highest difference for which the corresponding count of repair data objects is larger than zero. In order to aggregate the count of repair data objects for each instance of said difference the actual concurrent failure tolerance list could be provided with an extra column enumerating for each instance of the concurrent failure tolerance in the instances of this difference. For example for the 80 repair data objects with an actual concurrent failure tolerance of 4 as shown in there could be provided two rows in the list one row for example indicating a count of 20 of these repair data objects where this difference is 4 which means these repair data objects with a desired concurrent failure tolerance of 8 and a second row indicating a count of 60 of these repair data objects where this difference is 2 which means that these repair data objects need to be stored with a desired concurrent failure tolerance of 6. As explained above with reference to the handling repair data objects with the lowest actual concurrent failure tolerance several similar embodiments can be implemented so that the maintenance agent when executing the of repair tasks from the repair task list first executes the repair tasks of which this difference between the actual concurrent failure tolerance and the desired concurrent failure tolerance for the corresponding repair data object is the highest.

It is clear that although the infrastructure metadata the metadata storage and the actual concurrent failure tolerance list as described above for the preferred embodiments of the invention although they enable the most efficient embodiments of the invention can according to alternative embodiments of the invention be left out while still providing substantial benefits for the repair process when compared to the prior art. It is clear that other embodiments of the monitoring agent can add repair tasks to the repair task list for example by directly checking whether sufficient sub blocks are stored for a specific data object or by directly adding repair tasks for all data objects that are stored on a storage element that is detected to be unavailable or any other alternative embodiment of the monitoring agent that is able to detect that a data object is not correctly stored such as for example described in co pending patent application EP11187082.0.

In general the method for maintaining a distributed object storage system only requires the monitoring agent and or the maintenance agent determining the actual concurrent failure tolerance of the repair data objects . Merely providing this information could for example already enable an operator of the distributed object storage system to handle a repair operation more efficiently. However this also enables the one or more maintenance agents to execute first the repair tasks associated with repair data objects of which said actual concurrent failure tolerance is the lowest and as such to repair the most critical repair data objects first increasing the long term reliability of the object storage system in a scalable and efficient way.

Although the present invention has been illustrated by reference to specific embodiments it will be apparent to those skilled in the art that the invention is not limited to the details of the foregoing illustrative embodiments and that the present invention may be embodied with various changes and modifications without departing from the scope thereof. The present embodiments are therefore to be considered in all respects as illustrative and not restrictive the scope of the invention being indicated by the appended claims rather than by the foregoing description and all changes which come within the meaning and range of equivalency of the claims are therefore intended to be embraced therein. In other words it is contemplated to cover any and all modifications variations or equivalents that fall within the scope of the basic underlying principles and whose essential attributes are claimed in this patent application. It will furthermore be understood by the reader of this patent application that the words comprising or comprise do not exclude other elements or steps that the words a or an do not exclude a plurality and that a single element such as a computer system a processor or another integrated unit may fulfil the functions of several means recited in the claims.

Any reference signs in the claims shall not be construed as limiting the respective claims concerned. The terms first second third a b c and the like when used in the description or in the claims are introduced to distinguish between similar elements or steps and are not necessarily describing a sequential or chronological order. Similarly the terms top bottom over under and the like are introduced for descriptive purposes and not necessarily to denote relative positions. It is to be understood that the terms so used are interchangeable under appropriate circumstances and embodiments of the invention are capable of operating according to the present invention in other sequences or in orientations different from the one s described or illustrated above.

