---

title: Self-service testing
abstract: An architecture and techniques for implementing a unified and extensible meta-testing framework within a distributed environment. This framework allows entities within the distributed environment to run tests written in different testing frameworks in a unified way. In addition, this disclosure describes techniques for allowing an entity within the distributed environment to test itself, both from its own perspective as well as from the perspective of other entities within the distributed environment.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09053084&OS=09053084&RS=09053084
owner: Amazon Technologies, Inc.
number: 09053084
owner_city: Seattle
owner_country: US
publication_date: 20131226
---
The present disclosure is a continuation of and claims priority to co pending U.S. patent application Ser. No. 12 890 956 filed Sep. 27 2010 which is incorporated herein by reference.

In a distributed services environment different operators may operate and control each of the multiple different services within the environment to perform a particular task. These services however may rely upon one another to achieve these tasks. For instance a first service may make calls to a second service and to a third service and potentially many more within the environment when executing the task of the first service. As such the first service is dependent upon the second and third services while still generally operating and controlling the task of the first service independently. In addition the second and third services depend upon the first service when the second and third services perform their own respective tasks.

In the above example the first service may make calls into the second and third services when the first service is in operation or is being tested. However because each of these services operates and controls their own service independently these services may write and run testing frameworks in entirely different formats. Because of this it is difficult to test and hence understand how changes made to a particular service of the distributed services environment may or may not affect other services operating within the environment as well as the stability of the environment as a whole.

This disclosure describes an architecture and techniques for implementing a unified and extensible meta testing framework within a distributed environment. This framework described below allows entities within the distributed environment to run tests written in different testing frameworks in a unified way. In addition this disclosure describes techniques for allowing an entity within the distributed environment to test itself both from its own perspective as well as from the perspective of other entities within the distributed environment.

Distributed environments often consist of multiple different entities that each operate to perform a discrete task while at the same time depending upon the help of one or more other entities of the environment. For instance a distributed services environment may consist of multiple different services that each performs a discrete task with these discrete tasks often collectively performing a larger task. For instance a business or other organization or a collection of businesses or organizations may include multiple different services to form the distributed services environment.

Within the environment each of the services may implement a different type and or format of testing frameworks for testing software of the service. In many cases allowing each service to define these custom testing frameworks is efficient and effective as the services are often in the best position to know how to best test themselves. However because these services often depend upon other services within the environment when one service makes a change this change may affect other dependent service. Therefore the meta testing framework described below allows for services to run multiple different testing frameworks regardless of the type or format of the frameworks to allow the service to learn how its changes will affect itself as well as other services that depend from this service.

To provide this ability the unified and extensible meta testing framework provides a plug in mechanism that allows services or other entities in a distributed environment to plug in their different testing frameworks to the meta testing framework. Once a service plugs in its testing framework the tests written in the framework can be run in a unified way with tests written in a different format. For instance if two different services plug in two different types of testing frameworks that relate to a common project of the distributed environment the meta testing framework may run both of the individual testing frameworks when testing the project despite this format difference. This plug in mechanism may allow services to plug in tests of any format such as JUnit TestNG and the like.

In addition the unified and extensible meta testing framework described below may allow services to specify whether to execute a test in a single thread or in multiple threads that run in parallel with one another. Executing tests in multiple threads may shorten an amount of time needed to execute the different types of testing frameworks.

The meta testing framework may also provide the ability to search and run tests written in different formats using tags. That is operators or other users of the distributed services may associate different tags e.g. textual or otherwise with different tests. By doing so other operators or users running tests may request to execute subsets of tests that are associated with a particular set of one or more tags. In some instances the meta testing framework may provide a drop down menu or the like allowing the users to select tags. Conversely the framework may allow the user to enter a free form natural language expression to locate and execute tests associated with a particular set of tags. For instance a user may request that the meta testing framework run all project X tests and narrow down the set tests to Priority 1 tests. In response the meta testing framework may analyze the tags associated with each plugged in test and may execute those tests satisfying the natural language expression while refraining to execute those tests that do not. In addition the meta testing framework may allow users to select tests to run by filtering a set of tests based on any other metadata.

The meta testing framework described herein may also include an extensible reporting infrastructure that allows users to specify which types of reports the users would like to receive. For instance a user may request to receive certain reports for certain tests during execution of the tests by the meta testing framework while requesting to receive reports for other tests after execution of the tests. Users may request to receive reports via email short messaging service SMS text messages multimedia messaging service MMS text messages or in any other manner. Further the user may request that the meta testing framework take additional actions in response to detecting a predefined occurrence. For instance envision that a service makes a change to piece of software of the service and that the service would like to test this piece of software to determine whether or not the alteration affects operability of the service. The service may accordingly instruct the meta testing to inform the service immediately if the test fails to allow the service to automatically revert back to a previous state of the service prior to the alteration.

In addition to allowing users to specify custom reports the meta testing framework may allow users to specify a custom retry strategy for failed tests. For instance the framework may allow the users to instruct the framework to rerun a certain set of test cases that fail a certain number of times and a different set of test cases a different number of times. In addition the users may instruct the framework to refrain from rerunning yet another set of test cases at all even in the event that these test cases fail.

Finally the unified and extensible testing framework may include additional toolsets that allow users to further customize the running of multiple different testing frameworks within a distributed environment. For instance these toolsets may allow users to request that the framework auto run certain tests when a service deploys a new version of software. Additionally theses toolsets may allow a user to compare two different runs of tests and or to receive a comprehensive view of multiple executions of test cases. While a few example toolsets are provided here and below multiple other tools sets are similarly envisioned.

Several example distributed environments and architectures that may implement a unified and extensible meta testing framework are described below. However these example environments are but a few of many environments that may implement these techniques. For instance while some of the example distributed environments consist of distributed services that communicate over a network other distributed environments consisting of any other type of entity may employ these techniques. For instance the distributed environment may comprise a server farm or a data center that includes respective servers or respective sets of servers that each executes tests with use of a central testing platform. In another example the entities of the distributed service may comprise respective software modules or respective sets of software modules that operate on a common operating system within a single housing of a computing device.

Each of the services N may operate to perform a particular task for a common or different entities. For instance each service may perform one or more discrete tasks for a common company such as an e commerce retailer an educational institution a governmental agency or any other entity that operates different services to perform different tasks.

In the example of the e commerce retailer for instance a first service may implement and maintain a catalog of items offered for acquisition a second service may implement customer ordering a third service may implement customer payments a fourth service may implement and maintain user accounts and so on. In this and other examples the discrete services are operated and controlled separately but rely upon one another for various inputs. For instance a customer ordering service may place and control orders on behalf of customers. To do so however the customer ordering service may send requests to and receive replies from the catalog service the payments service the user account service and the like. For instance the catalog service may provide information about the item that a customer wishes to purchase the payments service may maintain information regarding payment instruments associated with the customer the user account service may maintain shipping preferences for the customer and the like.

As illustrated each of the services N and the central testing platform is embodied as one or more servers that collectively have processing and storage capabilities to receive and send requests. These servers may be embodied in any number of ways including as a single server a cluster of servers a server farm or data center and so forth although other server architectures e.g. mainframe may also be used. Alternatively some or all of the services N and the central testing platform may be embodied as a client device such as desktop computer a laptop computer and so forth.

The central testing platform includes a unified and extensible meta testing framework . The unified and extensible meta testing framework may provide an interface to the multiple different services N to permit each respective service to plug one or more testing frameworks of the respective service into the unified and extensible meta testing framework . In some instances at least some of the testing frameworks of the services N are written in different formats. For instance illustrates that the service employs testing frameworks . . . M while the service employs testing frameworks . . . P and the service N employs testing frameworks . . . Q . These services N may plug in some or all of each of these different testing frameworks which may be of differing formats. For instance the service may have written some or all of the testing frameworks M in JUnit while the service may have written some or all of the testing frameworks P in TestNG while the service may have written some or all of the testing frameworks Q in yet another format and so on.

In addition the unified and extensible meta testing framework may receive requests from the services N to execute different ones of the multiple testing frameworks written in the different formats. In response the unified and extensible meta testing framework may execute these testing frameworks associated the multiple services in the different formats in a unified way.

For instance operators of the service may desire to alter a portion of the service but may first desire to ensure that both the service and the distributed services environment as a whole will operate correctly after the alteration. As such the service may first run its own service level tests or simply service tests from the testing frameworks M to ensure that the service itself still functions properly after the alteration. If so the service may choose to execute its own integration level tests or simply integration tests of the testing frameworks P to see if the service will still function properly within the distributed services environment from the perspective the service . That is the integration tests of the service may determine how well the altered service operates with those services that depend from the service .

If the service passes these integration tests the service may request that the framework now execute integration tests for those services that depend upon the service . By doing so the service may learn whether the alterations made by and to the service will adversely affect i.e. break clients of the service . To do so the service and or the central testing platform may identify those services that utilize data from the service when in operation. In some instances the service may define a level of dependency in order to determine which services of the distributed services to test. For instance the service may define that only those services that directly depend from the service are to be considered dependent for the purpose of a particular test run. Or the service may define dependent services as those services that directly depend upon the service as well as those services that depend from those directly dependent services. Of course the service and other services utilizing the central testing platform may define what it means to be a dependent service in any other way which may vary on a per test run basis.

For instance in the e commerce retailer example the service may control payments for the retailer while the service may control ordering. As such when a customer of the e commerce retailer requests to purchase an offered item with a particular payment instrument from the ordering service the ordering service may access data from the payments service that identifies whether or not the particular payment instrument is a valid payment instrument for the transaction. As such the service and or other services within the environment may depend upon the service and the service may wish to ensure that its own changes do not adversely affect the dependent service .

In response to receiving the request from the service the framework of the central testing platform may execute certain tests e.g. the integration tests of the testing frameworks P against the service . The framework may do so without regard for the format in which the service has written these tests. That is the framework may execute these tests even if the format is entirely different from the format of the testing frameworks M of the service . The framework may similarly run these integration tests for other services that depend upon the service .

By executing testing frameworks of the altered service as well as frameworks of the services that depend upon the altered service the service may determine whether the alterations affect the service itself as well as whether the alterations affect the dependent services. With this information the service may choose to deploy the alterations or may choose to re alter the service by for example rolling back the service to its previous state or by attempting to otherwise fix the alterations.

In some instances the meta testing framework of the central testing platform may run each of these different testing frameworks against the services N themselves. In other instances meanwhile the distributed services environment includes a stubbing service . The stubbing service is configured to receive and record requests and replies of one or more of the services N during previous executions of testing frameworks. Thereafter the stubbing may play back these stored requests and replies when the testing frameworks are re executed. Therefore the meta testing framework may execute some or all of the different testing frameworks against the stubbing service rather than directly against the services N themselves. In addition the testing service such as the service may introduce different scenarios by for example instructing the stubbing service to operate or respond in a certain way. For instance the service may learn how it will perform in a brown out situation by requesting that the stubbing service introduce a certain amount of latency during test execution.

The service and or any other services N of the environment may create any similar or different what if scenarios. For instance the distributed services environment may employ some or all of the stubbing service techniques described in U.S. patent application Ser. No. 12 851 176 filed on Aug. 5 2010 which claims priority to U.S. Provisional Application Ser. No. 61 331 132 filed on May 4 2010. Both of these applications are entitled Stubbing Techniques in Distributed Services Environments and both are herein incorporated in their entirety by reference.

Here the memory stores one or more testing frameworks . . . R one or more reusable libraries . . . S one or more reports . . . T as well as the unified and extensible meta testing framework . The testing frameworks R may comprise in whole or in part testing frameworks of the services N that the services have plugged into the meta testing framework . The reusable libraries S meanwhile may allow the services N to recreate common scenarios e.g. create an order with specific conditions . Finally the reports T may comprise the results that the meta testing framework has generated in response to executing different ones of the testing frameworks R in accordance with received requests. As discussed below the central testing platform may provide these reports T to clients that employ the framework .

While illustrates several example components of the unified and extensible testing framework of other implementations may employ more fewer and or different components than those illustrated. In this example the framework includes a framework interface a unified testing module an execution configuration module a tagging module a reporting module a retry module and one or more additional toolsets .

The framework interface may comprise a set of application programming interfaces APIs a graphical user interface e.g. a command line tool and or any other type of interface with which an operator or other user of a service may access the framework . With use of the interface an operator of one of the services N may plug different tests of the service into the framework as well as request to execute different sets of testing frameworks.

After receiving such a request via the interface the unified testing module of the framework may comply with the request and may execute the multiple different testing frameworks. This module may execute each set of tests despite the fact that the formats of the tests may differ from one another having been written by different ones of the services N .

In addition the framework interface may in some instances allow the operator to specify whether a particular set of tests is to be executed in a single thread or in multiple threads operating in parallel to one another. As such the framework includes the execution configuration module . This module configures the execution of the tests in the single thread or in the multiple threads as specified by the received request. By enabling the operator to request to execute the tests via multiple threads the framework shortens an amount of time needed to complete the testing.

The tagging module meanwhile may function to associate tags with tests of the testing frameworks in accordance with user requests and or may identify tags that users of the respective services N have previously associated with these tests prior to plugging the tests into the meta testing framework. For instance operators or other users of the services N and or the central testing platform may request via the framework interface to associate tags with tests plugged into the framework . In response the tagging module may associate these tags with the tests. Additionally or alternatively operators or other users of the services N may associate the tags with the tests prior to plugging the tests into the framework . for instance illustrates that a particular test R has been associated with a set of tags .

In addition to allowing users to associate tags with the tests the tagging module may allow users to search for and or execute tests with reference to the associated tags. For instance users may request to execute those tests that have been associated with a particular set of one or more tags. In response the tagging module may identify and cause execution of the subset of tests that include the tags while refraining from causing execution of those tests that have not been associated with such tags.

Next the reporting module allows users to select a reporting strategy to employ for execution of a given set of tests. For instance the users may request that the reporting module send certain types of communications or take certain types of actions when certain conditions are triggered during and or after the execution of the tests. For instance a user may request to receive a certain type of communication e.g. an email SMS text message etc. when a certain test passes or fails after completion of a test or in response to any other trigger.

In some instances the reporting module generates reports to provide to the user both during the execution of the tests as well as after execution of the tests if the reporting strategy of the user dictates. In some instances the request itself specifies the reporting strategy while in other instances the requesting service may be associated with a custom and default reporting strategy that the reporting module employs for tests executed in response to requests from the particular service.

In addition to specifying a custom reporting strategy the framework includes the retry module that allows users to specify custom retry strategies for tests that fail during the execution of the testing frameworks. For instance the user may specify down to a per test basis or at any other level of granularity a number of times that the unified testing module should retry the failed tests if at all . Further the retry strategy may specify whether to rerun the failed test after the initial test execution process finishes or before completion of the initial test execution process.

Finally the additional toolsets may provide operators or other users of the services N with other tools that further benefit the executing of testing frameworks from the central testing platform . For instance an operator of the service may specify that the framework should auto run a particular set of testing frameworks when the service deploys any sort of modification or new service. The addition toolsets may also allow the operator to directly compare two different testing executions to view a comprehensive outline of a multiple executions of tests cases or the like.

As illustrated the UI includes a heading Create a New Custom Run an area for the user to enter a name of the run an area for the user to specify any overrides that she wishes to implement and an area that allows the user to select the test cases to include in the run. These tests cases may include some or all of the testing frameworks that the services N have plugged into the meta testing framework . In this example the area includes an area and an area that allows the user to select the tests for the run via tags and an identifier of the tests respectively. In this example the user may specify the tests by selecting tests that are associated with certain tags and or by selecting tests that have certain IDs. While this example of the UI illustrates that the user may make these selections via respective drop down menus other implementations of the UI may allow the user to make these selections via free form text entry radio buttons or in any other manner.

In addition this example UI includes a link Advanced Option that when selected may render another UI that allows the user to make further selections regarding the custom run. For instance these advanced options may allow the user to specify a number of threads in which to execute the tests specify a retry strategy specify any sort of metadata on which to filer and hence select the tests to run and the like. Finally the UI includes an icon Submit that when selected sends the request to create and execute the custom run to the central testing platform . In response to receiving the request the unified and extensible meta testing framework may execute the custom execution run in accordance with the request.

For discussion purposes the process and the other processes is described with reference to the environment of although other environments may implement this process.

At operation the meta testing framework of the central testing platform exposes an interface to allow multiple services to plug respective testing frameworks into the central testing platform. For instance an operator of the service may employ the interface to plug some or all of the testing frameworks M into the central testing platform .

Operation meanwhile represents maintaining the central testing platform which stores or otherwise has access to multiple different testing frameworks associated with multiple different services of the distributed services environment . As discussed above these testing frameworks may be of different formats such as JUnit TestNG and the like.

At operation the central testing platform receives a request from a service to execute at least a portion of the multiple different testing frameworks stored on or accessible by the central testing platform . As illustrated the request may specify an execution configuration a retry strategy one or more tags and or a reporting strategy . As discussed above the execution configuration may specify whether the user would like the meta testing framework of the central testing platform to execute the run in a single thread or in multiple threads. The retry strategy meanwhile may specify a manner in which the user would like the central testing platform to implement retries of failed tests. Finally the tags serve to identify those tests that the user would like to run while the reporting strategy specifies a manner in which the user would like the central testing platform to report result of the run to the user.

At operation the central testing platform executes the requested portion of the testing frameworks in accordance with the request and without regard to a format of the respective testing frameworks. For instance despite the fact that the services N may have written their tests using different formats the central testing platform may implement the tests in a unified manner.

At operation the central testing platform re executes failed tests in accordance with the retry strategy . Finally at operation the central testing platform may report the results of the executed and re executed tests in accordance with the reporting strategy . For instance the central testing platform may provide the results to the user via a user interface or in any other manner.

At 1 the service runs its own service tests after operators of the service make an alteration e.g. a change update or addition to the service . These service tests determine an operability of the service in isolation. That is the service tests determine whether the service will itself continue to operate at a sufficient level i.e. at a predefined threshold level after the changes to the service. In some instances the service determines the suitable or threshold operability level with reference to a quality of service QoS agreement that the service is subject to with reference to a level of operability of the service prior to the alteration or in any other manner.

In instances where the service passes the service level tests of 1 and potentially in instances when it does not the service may run its own integration tests to determine an operability of the service within the distributed services environment at 2. That is these integration tests which the service owns help the service understand whether or not the service still functions within the environment even after the alterations.

If so and in some instances even if not the service may proceed at 3 to obtain integration tests of those services within the environment that depend from the service . That is the service may obtain integration tests for each client of the service . At 4a c meanwhile the service runs the integration tests for each of the clients of the service. The arrows labeled 4A 4B and 4C represent running integration tests associated with the service N . By running integrations tests of dependent services the service learns whether the alteration to the service breaks i.e. adversely affects any clients of the service . With this knowledge the service may refrain from deploying the service with the alteration and or may re alter the service .

At operation a user or operator of a first service may alter a portion of the first service. At operation the service e.g. at the request of the operator may then execute service level tests of the first service. At operation the service queries as to whether an operability of the first service as determined by the testing of the operation is greater than an operability threshold. This operability threshold as well as each threshold described herein may comprise an operability defined in a quality of service QoS agreement that the service is subject to an operability level of the service prior to the alteration a fully functional operational level or any other level of operability.

If the operability of the first service is less than the threshold then at operation the service may refrain from deploying the alteration and may proceed to re alter the portion of the first service. This re altering may comprise rolling back the service to its state prior to the alteration or fixing the alteration in any other manner.

If however the operability of the first service is greater than the threshold then at operation the first service executes integration level tests written by and or owned by the first service. As discussed above these tests determine how well the first service would integrate with other services of the distributed services environment if deployed with the changes. At operation the service queries as to whether an operability of the first service as determined by the testing of the operation is greater than an operability threshold. This operability threshold may be the same or different than the threshold described above with reference to the operation .

If the operability of the first service is less than the threshold then the process again proceeds to the operation at which point the service refrains from deploying the alteration and proceeds to re alter the portion of the first service. If however the operability of the first service is greater than the threshold then at operation the first service identifies at least one service a second service that is at least partially dependent upon the first service within the distributed services environment. Next at the operation the service executes integration level tests written by and or owned by the second service and or the other identified dependent services . As discussed above these tests determine whether the changes to the first service would adversely affect or break the second service of the distributed services environment if deployed with the changes.

At operation the first service or another entity queries as to whether an operability of the second service as determined by the testing of the operation is greater than an operability threshold. This operability threshold may be the same or different than one or both of the thresholds described above with reference to the operations and . If the operability of the second service is less than the threshold then the process again proceeds to the operation at which point the service refrains from deploying the alteration to the first service and proceeds to re alter the portion of the first service. If however the operability of the second service is greater than the threshold then at operation the first service may deploy the altered portion knowing that the alteration does not adversely affect itself nor any of the services of the environment that depend from the service.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as exemplary forms of implementing the claims.

