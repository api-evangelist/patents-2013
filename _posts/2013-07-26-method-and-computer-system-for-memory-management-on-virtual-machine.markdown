---

title: Method and computer system for memory management on virtual machine
abstract: A method and a computer system for memory management on a virtual machine system are provided. The memory management method includes the following steps. A least recently used (LRU) list is maintained by at least one processor according to a last access time, wherein the LRU list includes a plurality of memory pages. A first portion of the memory pages are stored in a virtual memory, a second portion of the memory pages are stored in a zram driver, and a third portion of the memory pages are stored in at least one swap disk. A space in the zram driver is set by the at least one processor. The space in the zram driver is adjusted by the processor according to a plurality of access probabilities of the memory pages in the zram driver, an overhead of a pseudo page fault, and an overhead of a true page fault.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09256532&OS=09256532&RS=09256532
owner: Industrial Technology Research Institute
number: 09256532
owner_city: Hsinchu
owner_country: TW
publication_date: 20130726
---
This application claims the priority benefits of U.S. provisional application Ser. No. 61 712 279 filed on Oct. 11 2012. The entirety of the above mentioned patent applications is hereby incorporated by reference herein and made a part of this specification.

Computer virtualization is a technique involved in creation of a virtual machine that acts like a physical computing machine with an operating system and a computer virtualization architecture is generally defined by the ability to concurrently support multiple operating systems on a single physical computer platform. For example a computer that is running Microsoft Windows may host a virtual machine with a Linux operating system. A host machine is an actual physical machine on which the virtualization takes place while a virtual machine is considered as a guest machine. A hypervisor literally referred to as a virtual machine monitor VMM is a software layer that virtualizes hardware resources and presents a virtual hardware interface to at least one virtual machine. The hypervisor resembles to the way that a traditional operating system manages the hardware resources for processing and performs certain management functions with respect to an executing virtual machine. The virtual machine may be referred to as a guest and the operating system running inside the virtual machine may be referred to as a guest OS .

The virtualized environment is currently memory bound which means that the physical memory of the host machine is the bottleneck of the resource utilization in a data center. Memory virtualization decouples the physical memory resources from the data center and then aggregates the resources into a virtualized memory pool which is accessible to the guest OS or applications running on top of the guest OS. In terms of memory virtualization memory compression is one of the crucial topics to the memory resource management and utilization.

Similar to the traditional operating system the last resort to increase memory utilization of the hypervisor is to reclaim the memory from the virtual machine by host swapping i.e. to shift the memory pages of virtual machines to a physical swap disk referred to as swap out mark a corresponding page table entry PTE of the virtual machines physical address to machine address P2M table to be not present and then free the corresponding page to the free memory pool of the hypervisor wherein the page table is a data structure used by the virtual machines to store the mapping between the virtual addresses and physical addresses. Later on if the page is accessed again by the virtual machine a page fault is triggered and the copy on access COA mechanism is performed to bring the page content from a swap disk into a newly allocated memory page referred to as swap in. However the overhead is highly unsatisfactory due to the long latency of disk input output I O .

As another way to increase the memory utilization memory compression may be done by compressing swapped out pages of the virtual machines into smaller size of data and putting them together in a memory to save the physical memory disk used to store the original content. That is memory compression may substantially mitigate the performance degradation due to memory pressures. The idea is that the swapin from compressed memory would be faster than the swapin from the disk because the memory access is faster than the disk access.

As to compress the swapped out pages a zram driver an experimental module of a Linux kernel may present as a swap disk in the virtual machines compress and store the swapped out pages in guest memory. With the zram driver the pseudo page fault may trigger the compressed pages of the zram driver to be decompressed and stored into a newly allocated guest memory page which is intuitively faster than the real page fault from the swap disk. Nonetheless to store the compressed pages in the zram driver the guest OS needs to consume the guest memory and may bring more swap in out operations. To resolve such dilemma the size of the zram driver may require to be adjusted dynamically.

The disclosure embodiment provides a memory management method for a virtual machine system. The memory management method includes the following steps. First a least recently used LRU list is maintained by at least one processor according to a last access time wherein the LRU list includes a plurality of memory pages. A first portion of the memory pages are stored in a virtual memory on the virtual machine system a second portion of the memory pages are stored in a zram driver on the virtual machine system and a third portion of the memory pages are stored in at least one swap disk. A space in the zram driver is set by the at least one processor. Then the space in the zram driver is adjusted by the at least one processor according to a plurality of access probabilities of the memory pages in the zram driver an overhead of a pseudo page fault and an overhead of a true page fault.

The disclosure embodiment provides a computer system including at least one system memory at least one swap disk and at least one processor. The swap disk is coupled to the at least one system memory and the at least one processor is coupled to the at least one system memory and performs the following operation for memory management on a virtual machine system. The at least one processor maintains a LRU list according to a last access time wherein the LRU list includes a plurality of memory pages. A first portion of the memory pages are stored in a virtual memory on the virtual machine system a second portion of the memory pages are stored in a zram driver on the virtual machine system and a third portion of the memory pages are stored in at least one swap disk. The at least one processor sets a space in the zram driver and adjusts the space of the zram driver according to a plurality of access probabilities of the memory pages in the zram driver an overhead of a pseudo page fault and an overhead of a true page fault.

Several exemplary embodiments accompanied with figures are described in detail below to further describe the disclosure in details.

Reference will now be made in detail to the present exemplary embodiments of the disclosure examples of which are illustrated in the accompanying drawings. Wherever possible the same reference numbers are used in the drawings and the description to refer to the same or like parts.

For illustration purpose one processor one system memory one swap disk etc are used in the following exemplary embodiments and yet the present disclosure is not limited thereto. In other exemplary embodiments more than one processor more than one system memory more than one swap disk etc could be used.

The processor may be a dedicated or specialized processor configured to perform particular tasks by executing machine readable software code languages that define functions related to operations to carry out the functional operations by communicating with other components of the computer system .

The system memory stores software such as an operating system and temporarily stores data or application programs which are currently active or frequently used. Hence the system memory also referred to as a physical memory may be a faster memory such as random access memory RAM a static random access memory SRAM or a dynamic random access memory DRAM for much faster access time.

The swap disk is configured for storing and retrieving data. For example the swap disk may be an area on a hard disk drive HDD or a solid state drive SSD on the computer system to offload excessive data from the system memory .

Virtual memory is a technique for managing the resources of the system memory . It provides an illusion of a large amount of memory. Both the virtual memory and the system memory are divided into blocks of contiguous memory addresses which are also referred to as memory pages. The system memory may for example include a compressed memory which is associated with at least one virtual machine running on the computer system . The compressed memory temporarily stores less recently accessed memory pages in a compressed format to make more spaces available in the system memory . For implementation a Linux guest OS is specifically focused and yet the present disclosure may also be leveraged to other guest OSs such as Microsoft Windows. In one of exemplary embodiments a zram driver a module of the Linux kernel is inserted into the Linux guest OS as a virtual disk device and configured as a swap device by a system management tool i.e. swapon of the Linux guest OS so that all swap in and swap out operations enter the zram driver as disk I O requests. A hypervisor is installed on the computer system and supports virtual machine execution space within which at least one virtual machine may be concurrently instantiated and executed.

Referring to along with a virtual machine system includes a virtual machine with a guest OS and other applications not shown a hypervisor and a virtual hardware . The guest OS includes a guest kernel with a LRU list and a zram driver . The virtual hardware including a processor a memory and I O devices is abstracted and allocated as a virtual processor a virtual memory and virtual I O devices to the upper running virtual machine . The hypervisor manages the virtual machine and provides emulated hardware and firmware resources. In one of exemplary embodiments a Linux distribution may be installed as the guest OS within the virtual machine to execute any supported application and open source software Xen supporting most Linux distributions may be provided as the hypervisor and the guest kernel may be the domU kernel. In conjunction with the hypervisor a balloon driver may allocate or de allocate the virtual memory for the guest OS by invoking memory management algorithms. Memory compression may be efficiently done by leveraging the page reclamation mechanism and compressing the memory pages outside of a working set of the virtual machine . Intuitively speaking the working set of the virtual machine is defined as the amount of memory being actively used by the virtual machine in the recent past.

For page reclamation the processor uses a least recently used LRU criteria to determine an order in which to evict pages and maintains the LRU list that orders all the memory pages ever accessed by the virtual machine according to the last access time for two major types of memory an anonymous memory and a page cache. The memory pages of the anonymous memory are used by the heap and stack of user processes and the memory pages of the page cache are backed by disk data where the content is cached in memory after the first access to the disk data to reduce future disk I O. The memory pages accessed more frequently are referred to as hot pages the pages accessed less frequently are referred to as cold pages.

On a virtual machine system if a memory page on the LRU list is the anonymous memory a guest kernel may swap the content to a swap disk mark the corresponding PTE of the process to be not present and then free the corresponding memory page. Later on if the memory page is accessed again the COA mechanism is performed by bringing the page content from the swap disk into a newly allocated memory page i.e. swap in. Alternatively if the memory page on the LRU list belongs to the page cache the guest kernel may flush the page content to the swap disk if it has been dirtied and then the page is freed. Upon a next file access the guest kernel has to again perform the disk access referred to as refault to bring the content back to a newly allocated page in the page cache.

When the virtual memory allocated to the virtual machine is between the working set and the minimum memory requirement memory compression may substantially mitigate the performance degradation due to memory pressures. To compress evicted pages from the virtual machine the zram driver may intercept the swap in and swap out operations on the virtual machine . When a swapped out page arrives at the zram driver it is compressed into a sub page size by for example the Lempel Ziv Oberhumer LZO1X algorithm and stored in a memory area allocated from the guest kernel without being sent to the swap disk . One exception is zero evicted pages which the zram driver recognizes based on the page type information and skips the compression step. When a swap in page arrives the zram driver decompresses the swap in page and returns it to the process that causes the page fault triggering the swap in.

In one of exemplary embodiments the processor may not compress evicted cache pages and focus on swap in events associated with anonymous memory due to the following reasons. First the lifetime of anonymous pages of a process is the same as that of the process itself because they are released when the process dies. However cache pages are not explicitly owned by any process because they may be allocated by one process and then used to satisfy disk accesses by another process. Second compared with anonymous memory pages cache pages are typically backed by a larger disk volume and thus may require too much memory to compress. While intercepting swap in and swap out of anonymous memory pages is relatively straightforward because it may be done through a well defined application programming interface API the same thing may not be said about intercepting eviction of cache pages whose logic is embedded in a virtual file system VFS layer of the guest kernel .

Since the virtual machine is backed by the zram driver and the swap disk when a page fault occurs the missing page may be fetched from the zram driver in which case the fault leads to a pseudo page fault or from the swap disk in which case the page fault leads to a true page fault. When a page is swapped in from the zram driver to the swap disk the overhead is mainly due to the time required to decompress the page. On the other hand when a page is swapped out to the zram driver from the swap disk the overhead is mainly due to the time required to compress the page. In one of exemplary embodiments in terms of a quantitative comparison between the swap in time and swap out time associated with the pseudo page fault and the true page fault there is a factor of at least 50 differences between the overheads. The overhead of the true page fault is at least 50 times longer than the over head of the pseudo page fault.

When a larger portion of the virtual memory is given to the zram driver less memory is available to the applications running on the virtual machine and the pseudo page fault rate is then increased. However as the zram driver is given more memory more memory pages are held in memory effectively due to compression and fewer page faults may result in true page faults because they are more likely to be satisfied by the compressed pages in the zram driver . Therefore the amount of memory given to the zram driver represents a trade off between the pseudo page fault rate and the true page fault rate.

Suppose the amount of memory allocated to the virtual machine is M C of which is allocated to the zram driver and the average compression ratio of the pages stored in the zram driver is X. The key question is to find the optimal C such that PPFR M C Overhead TPPR M C Overheadis minimized. PPFR M C is the pseudo page fault rate of the virtual machine when the allocated memory is M and C of which is allocated to the zram driver . TPPR M C is the true page fault rate of the virtual machine when the allocated memory size is M and C of which is allocated to the zram driver . To automatically deduce the optimal percentage of the allocated memory that may be assigned to the zram driver and the subset of memory pages evicted to the zram driver that should be sent to the swap disk the processor may dynamically adjust a space of the zram driver as shown in .

Referring to along with the components in and the LRU list is maintained by the processor according to the last access time Step S . The memory pages stored in the LRU list are divided into three portions. A first portion of the memory pages on the LRU list are stored in the virtual memory not shown outside the zram driver in an uncompressed format. A second portion of the memory pages on the LRU list are stored in the zram driver in a compressed format. A third portion of the memory pages are stored in the swap disk .

Then a space in the zram driver is set by the processor Step S . In other words the processor may provide the zram driver a control parameter which specifies the number of memory pages i.e. an amount of memory assigned to the zram driver . When the amount of used memory in the zram driver exceeds the control parameter the processor may directs all future swapped out memory pages from the virtual machine to the swap disk without attempting to compress them. Initially the processor may set the control parameter based on a balloon target of the balloon driver a minimum memory requirement of the virtual machine system and a basic memory requirement of the zram driver . For example the control parameter may be a value of balloon target MMR Mby default wherein MMR is the minimum memory requirement of the virtual machine system and Mis the basic memory requirement of the zram driver .

In one of exemplary embodiments assume that the number of memory pages available to the virtual machine is N1 and the amount of memory allocated to the zram driver is K. According to the last access time to the memory pages the hottest N1 memory pages i.e. a first portion on the LRU list are stored in the virtual memory outside the zram driver in an uncompressed format. The next hottest N2 memory pages i.e. a second portion on the LRU list are stored in the zram driver in a compressed format wherein the accumulative size of the next hottest N2 memory pages is K. In other words the N1 1 memory page to the N1 N2 memory page on the LRU list are stored in the zram driver . The remaining memory pages i.e. a third portion on the LRU list are stored in the swap disk .

If N2 is decremented by one so that N1 is incremented by one some of the coldest pages in the zram driver in a compressed format may have to reside in the swap disk and be explicitly brought into memory when they are accessed. Meanwhile the hottest page in the zram driver i.e. the N1 1 memory page on the LRU list may be held in the virtual memory. That is the pseudo page fault rate is decreased but the true page fault rate is increased. Therefore it is preferable to decrement the number of the memory pages assigned to the zram driver if a reduction in the overhead of the pseudo page fault out weights an increment in the overhead of the true page fault. In one of exemplary embodiments the overhead of the pseudo page fault is a time cost of the pseudo page fault and the overhead of the true page fault is a time cost of the true page fault. Similarly it is preferable to increment the number of the memory pages assigned to the zram driver if the reduction in the overhead of the true page fault out weights the increment in the overhead of the pseudo page fault. Hence the space in the zram driver may be adjusted by the processor according to a plurality of access probabilities of the memory pages in the zram driver the overhead of the pseudo page fault and the overhead of the true page fault Step S .

Since the N1 1 memory page to the N1 N2 memory page on the LRU list are stored in the zram driver in the present exemplary embodiment the processor decrements N2 if the condition of Eq. 1 is met 

It is noted that the access probability of each of the pages on the LRU list is estimated by an inverse of a page idle time by the processor wherein the page idle time is a difference between each of the memory pages being swapped out and a current time. That is the access time of the memory pages on the LRU list may be recorded by the processor . The estimate of the page idle time is an approximation because it equates a swapped out time of a memory page as its last access time. When the virtual machine evicts the memory pages more frequently the approximation is more accurate. When the virtual machine does not evict pages frequently there is no need to adjust the memory space of the zram driver dynamically and the fact that such approximation is less accurate does not have much impact.

It is also noted that the LRU list includes all the memory pages accessed by the virtual machine and may incurs too much performance overhead to build in practice. In another exemplary embodiment the processor may build and maintain another local LRU list based on the memory pages swapped out to the zram driver .

Through the aforementioned memory management method when the memory of the system memory goes lower than the working set of the virtual machine the processor may dynamically adjust the size of the zram driver by evicting the cold memory pages in the zram driver to the swap disk . By means that more resources from the system memory may be preserved while keeping the application performance without noticeable degradation.

In an exemplary embodiment the above mentioned memory management method may be implemented by executing a prepared program on a computer such as a personal computer and a workstation. The program is stored on a computer readable recording medium such as a hard disk a flexible disk a CD ROM an MO and a DVD read out from the computer readable medium and executed by the computer. The program may be distributed through a network such as the Internet.

To sum up by leveraging an existing page reclamation mechanism of a guest OS the memory management method in the present disclosure is designed to deduce the optimal percentage of the virtual memory that should be assigned to a zram driver and the subset of memory pages evicted to the zram driver that should be sent to a swap disk for mitigating the performance degradation due to memory pressure. Based on access probabilities of memory pages in the zram driver with overheads of a pseudo page fault and a true page fault taken into account the amount of the memory assigned to the zram driver may be adjusted dynamically by evicting the cold memory pages in the zram driver to the swap disk and further provides advantages of memory resource management.

It will be apparent to those skilled in the art that various modifications and variations can be made to the structure of the disclosed embodiments without departing from the scope or spirit of the disclosure. In view of the foregoing it is intended that the disclosure cover modifications and variations of this disclosure provided they fall within the scope of the following claims and their equivalents.

