---

title: Non-blocking data transfer via memory cache manipulation
abstract: A cache controller in a computer system is configured to manage a cache such that the use of bus bandwidth is reduced. The cache controller receives commands from a processor. In response, a cache mapping maintaining information for each block in the cache is modified. The cache mapping may include an address, a dirty bit, a zero bit, and a priority for each cache block. The address indicates an address in main memory for which the cache block caches data. The dirty bit indicates whether the data in the cache block is consistent with data in main memory at the address. The zero bit indicates whether data at the address should be read as a default value, and the priority specifies a priority for evicting the cache block. By manipulating this mapping information, commands such as move, copy swap, zero, deprioritize and deactivate may be implemented.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08812817&OS=08812817&RS=08812817
owner: Microsoft Corporation
number: 08812817
owner_city: Redmond
owner_country: US
publication_date: 20130628
---
This application is a continuation of U.S. patent application Ser. No. 12 619 571 filed Nov. 16 2009 entitled NON BLOCKING DATA TRANSFER VIA MEMORY CACHE MANIPULATION now U.S. Pat. No. 8 495 299 issued Jul. 23 2013. The entirety of this afore mentioned application is incorporated herein by reference.

Today s high speed processors can execute program code and process data at rates much faster than data can be retrieved from or stored in main memory. To reduce the time spent by the processor waiting to access memory a high speed memory cache acts as an intermediary between the processor and main memory. A cache may have a controller and a memory component. The cache memory contains a copy of a subset of the data in the main memory. The cache controller responds to memory access operations from the processor and depending on what data is in the cache memory may quickly access the cache memory in order to complete the memory operation. If the cache is maintaining data necessary to respond to the memory access operation the cache is able to respond more quickly to the operation than if the main memory needs to be accessed directly.

The cache controller in addition to responding to memory access operations maintains data in the cache sometimes copying data from the main memory into the cache or writing back data from the cache into the main memory. The cache controller uses a mapping to keep track of which addresses of the main memory are cached. For example a cache block a memory unit of the cache may be associated with an address in main memory. The cache controller may maintain a mapping that identifies associations between blocks of the cache and addresses in main memory. When a processor issues a memory access operation identifying an address in main memory the cache controller can determine based on the mapping whether there is a block of cache memory associated with a portion of the main memory containing that address.

Because the cache is almost always smaller than the main memory a cache algorithm is used to select what subset of the main memory is maintained in the cache. Various cache algorithms are known but each generally has as a goal increasing the likelihood that a memory access operation can be completed using data in the cache. In practice however the cache algorithm is imperfect and operations for uncached addresses are received. When an operation on an uncached address is received the cache controller may copy data from the address of main memory into the cache. If all of the blocks of cache memory are full the cache controller may be said to remove some addresses from the cache by writing over the data in blocks associated with those addresses with data from other addresses. The controller may then change the mapping to show the new addresses corresponding to data in that block. The cache algorithm may set a priority for determining which addresses to keep or remove from the cache when more data is to be cached than there are free blocks in the cache to hold it. When data in the cache is replaced with data at another address in the main memory in this fashion the cache is said to evict the lower priority cached address.

The cache processor and main memory are routinely involved in memory transfer operations. Memory access operations typically involve transmitting data between the processor cache and main memory over one or more communication buses. Transfer operations may be initiated by the processor in the course of executing software. Common memory transfer operations include copy move swap and zero.

Other techniques for improving the efficiency of memory transfer operations are also known to reduce the load on the processor Programmed input output PIO is a technology by which the processor may control the read and write operations needed to complete a memory transfer. Another technology is direct memory access DMA . DMA allows hardware other than the processor to control the memory transfer operation. In both PIO and DMA operations data may be communicated over a bus to which the processor is connected which may slow operation of the processor as some of its operations may also require access to the processor bus and will contend for the bus bandwidth.

Computer system performance may be improved by a cache controller that can implement one or more memory operations initiated by commands from a processor. In some embodiments these memory operations may be implemented by altering a mapping between cache blocks and blocks of main memory and or exchanging data between the cache. Such memory operations may be implemented in a way that uses little or no bus bandwidth and processor cycles. Memory operations that may be performed by a cache controller may include move copy zero deprioritize and or invalidate.

In some embodiments and for some memory operations the cache controller manipulates the cache mapping independently from manipulations of the cached data. For example the cache controller may receive a command to move data from a source address to a destination address. If the data at the source address is maintained in a cache block the mapping may be changed such that the cache block is associated with the destination address rather than the source address. The copy of the data maintained in the cache may be propagated to main memory as part of the move operation or may be propagated to main memory at a later time such as upon eviction of the destination address or as a lazy write.

In some embodiments and for some memory operations the cache controller executes a command without remapping a block in the cache to an address in main memory but alters the way that data in the cache is maintained for improved efficiency. For example an invalidate command may indicate to the cache controller that data stored at particular main memory addresses will not subsequently be accessed such that operations to maintain coherency between a cached copy of that data and the copy of the data in main memory need not be performed. An invalidate command may also indicate that any blocks of cache memory allocated to maintaining a copy of that data can be used for other data.

The inventors have recognized and appreciated that drawbacks of available mechanisms for performing memory transfer operations may be avoided with an improved cache controller and methods of operation of a cache. Conventional architectures using programmed input output PIO in addition to placing a load on the processor typically consume bus bandwidth on a bus to which a processor may be connected. Consuming processor bus bandwidth limits the availability of the processor bus to transfer data for other operations the processor is to perform causing delay and reducing performance in ways that may be avoided in accordance with the components and techniques described herein.

Direct memory access DMA also has several limitations and disadvantages that may be avoided. DMA transfers may also consume bus bandwidth. Additionally when data spanning a range of addresses is transferred data at those addresses may be in an inconsistent state until the entire transfer is complete requiring wait time for the entire transfer. Accordingly performance may be improved by reducing the need for DMA operations.

The inventors have recognized and appreciated that appropriate manipulation of a cache mapping may reduce the number of conventional memory operations such as DMA transfers that are performed in a computer system. A corresponding decrease in time processor load and or processor bus bandwidth utilization may be achieved. In some aspects manipulation of the cache mapping may involve remapping a cache block associated with a first range of addresses in main memory to be associated with a second range of addresses in main memory. Remapping may reduce eliminate or delay use of bus bandwidth. Such manipulations may be managed by a cache controller in a way that is transparent to the processor such that the use of processor cycles is also reduced or eliminated.

Accordingly a cache controller may be provided with an interface through which it may receive commands from a processor. These commands may be generated by programs executing on the processor and may signal higher level memory operations are to be performed including memory operations of the type that might otherwise have been performed as a DMA operation. The cache controller may be adapted to execute these commands by manipulating a cache mapping. Such commands may be non blocking in the sense that execution of the command does not directly block execution of further instructions by the processor on which the operation is being performed. These operations also need not indirectly block further operations by the processor because they need not consume bandwidth on a processor bus while they are performed.

Examples of commands that may be implemented in this fashion include moving or copying a block of data from one block in main memory to another block of main memory. Another example of such a command is a zeroing command.

The inventors have further recognized and appreciated that a cache controller provided to receive commands to perform remapping operations additionally or alternatively may receive commands that alter operation of the cache controller in ways that also achieve more efficient operation of a computer system. Caching algorithms for prioritizing cached data may be enhanced by the ability for software components executing on a processor to indicate that certain address ranges in main memory are no longer being processed or are unlikely to be accessed. The cache controller may be configured with an interface to receive commands from a processor such as a command to invalidate or deprioritize one or more addresses.

Processors may include any suitable number of processors. For example processors may include one or two or more processors. Here N processors are shown namely processors . . . and . Processors may include any processor known in the art or any suitable processing device. For example and not limitation processors may include any of a central processing unit CPU digital signal processor DSP controller addressable controller general or special purpose microprocessor microcontroller addressable microprocessor programmable processor programmable controller dedicated processor dedicated controller or any other suitable processing device.

Main memory may store data on which one or more memory operations are to be performed based on programs executing on the processors . Main memory may be a computer readable storage medium as is known in the art or any suitable type of computer readable storage medium. For example and not limitation main memory may be any suitable type of random access memory RAM .

The use or significance of the data in memory is not critical to the invention and memory operations as described herein may be performed on data in main memory regardless of its meaning or use. For example the data in main memory may constitute software modules containing computer executable instructions that when executed by a processor perform a desired function. Though the data in main memory may alternatively or additionally represent parameters or other information accessed when computer executable instructions are executed. In this case computer system may contain additional memory including other memory for storing computer executable instructions.

Main memory has memory blocks for storing data that are indexed by memory addresses . Each memory block may store multiple bits of information. The number of bits of information stored by a memory block is not critical to the invention. A block could be a group of bits termed a word or a page. Though a block may be of any suitable size and may be identified in any suitable way. For example a block may be identified by a starting address and a block size or by a starting address and an ending address. In some embodiments a computer system may operate on memory blocks of predetermined sizes. In this case when an operation is to be performed on a block of memory a single address may be adequate to identify the block. Further it should be appreciated that a block of main memory need not correspond in any way to the structure of the components used to implement main memory .

In the example shown main memory has P addresses identifying P blocks on which operations as described herein may be performed. P may be any suitable value. In some embodiments the total amount of data that may be stored on main memory may be between ten and one hundred times the amount that can be maintained in cache . Though any suitable size cache and main memory may be used.

Cache acts as an intermediary between processors and main memory . Instructions that access memory locations issued by processors may be implemented using data maintained in cache or may alter data maintained in cache . Accordingly cache may perform one or more data caching functions as are known in the art. Additionally cache may include a controller that responds to additional commands as described in greater detail below.

Here cache is illustrated as a shared cache accessible to processors over processor bus . Though cache may be a cache for a specific processor. While computer system is illustrated as having one cache some embodiments may have multiple caches. For example each of processors may also include a dedicated processor cache. Some embodiments with multiple caches are discussed below with reference to computer system shown in .

Cache includes a cache controller and a cache memory . The implementation of these components is not critical to the invention. For example cache controller and cache memory may be integrated on the same integrated circuit IC or on separate ICs connected through a suitable communications medium.

Cache controller manages cache including cache memory . Cache controller may be a form of processor that executes computer executable instructions for managing cache . These commands may be burned as microcode in the cache controller or stored in any other suitable location or format. Execution of instructions by cache controller may be responsive to commands received over bus or bus . For example cache controller may respond to memory transfer commands issued from processors which are received over processor bus .

The computer executable instructions that are executed by cache controller may include one or more software modules for performing functions in response to received commands. The modules may include a mapping module a replacement module and a zeroing module . Mapping module may be configured to control the mapping of cache blocks of cache memory to addresses of main memory . A replacement module may implement a cache algorithm for determining the replacement priorities of cached addresses. A zeroing module may be used to set the data at one or more addresses to a default value. The modules may additionally or alternatively include any suitable modules for performing methods of managing cache . In some embodiments the modules may be implemented as microcode.

Cache memory includes a suitable number of cache blocks for storing data. Cache controller may manage the data stored in these blocks such that cache blocks may each contain a cached version of data from some portion of main memory. In the example illustrated cache memory includes M cache blocks . M may be any suitable value.

Cache may also include a mapping for maintaining an association between data in the cache and locations in main memory . For simplicity of illustration each block in the cache is treated as containing the same amount of data as in a block of main memory. In this embodiment an association is indicated by identifying a block in main memory associated with a cache block. However it should be recognized that a one to one relationship between cache blocks and main memory blocks is not required and any suitable mechanism may be used to form associations between data in the cache and data in main memory.

Mapping may be managed by cache controller . In the illustrated example mapping is shown as part of cache memory . Though it is not critical where the information constituting mapping is stored. This information may for example be stored in a centralized data structure within cache or may be distributed across multiple data structures stored in one or more memory devices in any suitable way.

For each cache block in cache memory mapping may contain a block ID a mapped address a dirty bit a zero bit and an eviction priority . Block ID identifies the block in the cache to which the other parameters correspond. Block ID may be stored in writable memory read only memory or may be inferred from the hardware design of cache memory .

Mapped address associates the cache block with a memory block in main memory . Mapped address may identify the corresponding block in main memory by its address or in any other suitable way. For simplicity of illustration the amount of data each cache block may store is assumed to equal the size stored by each memory block. Those of skill in the art will appreciate that known mapping techniques may be used when cache blocks differ in size from the memory blocks in main memory .

Mapping may also includes a consistency mark or dirty bit for each cache block. Dirty bit indicates whether the data stored in the cache block is consistent not dirty or inconsistent dirty with the data in the corresponding block in main memory . Dirty bit may be set to indicate dirty data when data in the cache is modified other than by copying from the corresponding memory block in main memory and may be cleared when the data in the cache is written into the corresponding memory block in main memory . Collectively the dirty bits for all cache blocks in mapping may be referred to as consistency memory.

Mapping may also include a zeroing mark or Zero bit for each cache block. Zero bit may be set when the corresponding block in main memory is to be set to a default value such as zero. Collectively the zeroing marks for all cache blocks in mapping may be referred to as zeroing memory.

Mapping may also include an eviction priority for each cache block. Eviction priority may specify a priority for evicting data from the cache block when an uncached block from main memory is to be cached. A low priority indicates that the data stored by the cache block is more likely to be evicted by memory controller and replaced with data stored in a block of main memory indexed by a different address. In some embodiments eviction priority is only explicitly indicated for those cache blocks caching data at an address to be evicted.

Computer system may also include any other suitable computer components not shown in . For example and not limitation computer system may include peripherals other processors and memories input output devices such as a display keyboard and mouse and any other type of suitable computer component.

Processors may execute software applications and operating system OS . OS may manage the activities and sharing of resources of computer system . OS may provide various functions and manage computer system through various components. These components may include for example and not limitation dynamically linked libraries e.g. a dynamic link library programming interfaces component object models COMs globally unique identifiers registry keys or any uniquely identifiable part of OS .

OS may provide functions for software applications executing on computer system for example through function calls from applications . In some embodiments OS includes programming interfaces . Programming interfaces may allow software applications to request memory transfer and cache management operations. In some embodiments memory operations as described herein may be implemented as part of the operating system though implementation within an operating system is not a requirement of the invention.

In some embodiments software applications are assigned virtual address spaces. The virtual address spaces may be mapped by OS to physical addresses of main memory . Programming interfaces may be configured to receive commands from software applications specifying manipulations to memory in terms of virtual addresses and translate the virtual addresses into physical addresses of main memory . Though the point within the system at which logical to physical address translation occurs is not a limitation of the invention. In some embodiments for example cache controller may manage virtual to physical address conversion. In some embodiments programming interfaces are application programming interfaces APIs .

Cache controller receives and performs commands from programming interfaces sent on behalf of software applications . Cache controller uses mapping to determine if and where the data at the physical addresses are stored in blocks of cache memory . For example cache controller may search the mapped addresses in mapping for a physical address. If the physical address is found in mapping cache controller may determine that the data at the physical address is cached by the corresponding cache block.

Having described exemplary computer systems operation of a computer system according to some embodiments is described with reference to the following methods and examples. Herein memory addresses are taken to be physical addresses. Those of skill in the art will appreciate that where virtual addresses are specified conversion to physical addresses may be performed for example in ways known in the art.

At step the cache controller establishes a mapping from blocks of memory in a cache of the computer system and memory addresses in a main memory of the computer system. In some embodiments the mapping is established by storing a mapped address with each cache block that identifies an address in the memory corresponding to the block. The mapped address may identify the address in main memory in any suitable way. For example in a fully associative cache where each address of the main memory may be mapped to any block in the cache the cache may store the full address as the mapped address. In a direct mapped cache where each address may only be mapped to exactly one block in the cache the map may only store enough bits to distinguish among those addresses mapped to the specific block in the cache. For example the least significant bits of the address may be dropped when these bits are common to all addresses that may be written to the cache block. A similar technique may be used to map cache blocks to addresses in an X way set associative cache where each address of main memory may be mapped to X cache blocks where X is any positive integer. The discussion herein is applicable to a fully associative cache though those of skill in the art will appreciate that embodiments may be implemented with any suitable type of cache.

Establishing a mapping at step may include establishing any of a dirty bit a zero bit and an eviction priority for each cache block. In some embodiments a mapping is established which maintains for each cache block a mapped address a dirty bit a zero bit and an eviction priority. Though the established mapping may have any suitable entries. In some embodiments the mapping may be established as a result of a cache controller responding to memory access operations over a period of time using techniques as are known in the art.

At step a command is received by the cache controller indicating a modification of the mapping. Such a command for example may be generated by an operating system in response to an application program expressly calling a function or by the operating system detecting through one or more actions taken by one or more applications that cache controller may efficiently perform an operation. The received command may be any suitable type of command. For example and not limitation the received command may be a memory transfer operation command such a copy move swap command or zeroing command. As another example the received command may be an operational hint that allows the cache controller to manage its mapping more efficiently such as a deprioritize or invalidate command. Though any suitable type of command may be received. In some embodiments the command is received from a processor of the computer system. For example the command may be received as a result of software executing on the processor.

At step in response to the command the cache controller modifies the cache mapping. For example in an embodiment where the established mapping includes mapped addresses a cache block initially mapped to a first address in the main memory may be updated at step to be mapped to a second address in the main memory without changing the data stored in the cache block. As another example in an embodiment where the established mapping includes eviction priorities a cache block s eviction priority may be modified to deprioritize the cached data. After step method ends.

At step a copy command is received by the cache controller. The copy command may specify the source of data for the copy in any suitable way such as by specifying a source address a set of source addresses or a range of source addresses at which data to copy is stored. A destination for the copy may also be specified in any suitable way such as a destination address set of destination addresses or a range of destination addresses may specify the destination of the data.

Steps through are performed for each source and destination address pair. These steps may be performed simultaneously sequentially or in any suitable way for each address pair.

At step the cache controller determines whether the data at the source address is currently mapped to a block of the cache. The cache controller may make the determination for example by checking whether any block in the cache has a mapped address corresponding to the source address.

If the determination at step is no method proceeds to step where the data at the source address is loaded from the main memory into a block of the cache. In loading the source data into the cache block the mapped address for the block may be set to identify the source address. Any suitable cache algorithm may be used to identify to which block in the cache the source data should be written. Though in some embodiments if a block is already mapped to the destination address that block may be selected for replacement. If selected that block may be invalidated. When a block in the cache is invalidated rather than evicted data in the cached block is not written back to main memory even if dirty. Invalidating an address is subsequently discussed with reference to .

If the determination at step is yes method proceeds to step . At step method prepares the cache to update the mapped address. When the cache block caching data at the source address is dirty the method writes back the dirty data from the cache block to the block in main memory at the source address. If the data is not dirty write back is not performed at step .

The method may also check whether the destination address is mapped to a block of the cache. If so the mapping may be updated to indicate that the block is no longer associated with the destination address. Clearing the mapping in this fashion prevents two separate cache blocks from being mapped to the same destination address. The destination block may be cleared from the mapping whether or not source data is loaded at block .

Method proceeds from steps and to step . At step the method changes the mapping such that the block initially mapped to the source address is mapped to the destination address. After updating the mapped address normal operation of the cache will result in memory access instructions directed at locations associated with the destination address being performed on the copied data in the cache.

At step additional update steps may optionally be performed. Upon completion of the step the cache may contain a block containing data for the destination address in main memory. However the data in main memory may not match that in the cache. Conversely the correct data may be stored in main memory associated with the source addresses but that data may not be cached. Accordingly post update operations may write data from the cache to the destination address or copy into the cache data from the source address in main memory.

These steps may be performed at any suitable time and may be performed under direction of the cache controller using memory bus . Accordingly completion of these steps need not block the processor either by requiring processor operations or processor bus bandwidth.

In some embodiments the data may be written to the destination address of the main memory from the cache block as part of the copy operation. However in some embodiments the write back may be deferred until a write back would otherwise occur as part of cache operation. In such a scenario a dirty bit may be set for the cache block associated with the destination address to indicate the data is dirty. In such an embodiment a write back to main memory may be performed for example upon eviction of the destination block from the cache or as a lazy write. Lazy writes are asynchronous writes from the cache to the main memory. In some embodiments lazy writes are performed while the cache and or memory bus is otherwise idle. Thus lazy writes may be performed several seconds after the copy command is functionally complete. In another embodiment a dirty bit is not kept and write back is always performed at eviction.

Likewise if it is desired to reload the source data in the cache data from the source address may be loaded into the cache at any suitable time. The reload may occur under the control of the cache controller using the memory bus such that the processor need not be blocked. Though data from the source destination may not be reloaded into the cache until that data is accessed again.

Those of skill in the art will appreciate that a variation of method may be used for processing a move command. A move command in as much as it is differs from a copy implies that the data at the source addresses is no longer valid. Accordingly post update operations at step need not entail reloading source data into the cache. To the contrary to the extent data was retained in the cache post update operations at step may entail invalidating the cache block holding that data. In some embodiments memory management software executing on the processor issuing the move command or otherwise within the computer system may ensure that the data at the source address is not subsequently accessed.

Another non blocking operation that may be supported in some systems is a swap command. is a flow chart of a method for managing memory in a computer system when a swap command is received. As the name suggests in processing a swap command data stored at a first address is swapped with data stored at a second address. In some embodiments method is a specific embodiment of steps and of method . Method may be performed in a computer system by operating a cache controller such as for example cache controller of computer system shown in .

At step a swap command is received by the cache controller. The swap command specifies an address set of addresses or a range of addresses as a first source source A at which first data to swap is stored. Similarly a second source source B at which second data to be swapped is stored may be specified by an address set of addresses or a range of addresses. Though source A and source B addresses may be specified in any suitable way. When more than one source A and source B address are specified by the command the address pairs to be swapped may be determined in any suitable way.

Steps through are performed for each pair of source addresses to be swapped. These steps may be performed simultaneously sequentially or in any suitable way for each address pair.

At step the cache controller determines whether the source A address is currently mapped by the cache. The cache controller may make the determination for example by checking whether any block in the cache has a mapped address corresponding to the source A address. A similar determination is made at step for the source B address.

If the determination at step is no method continues to step . At step the data at the source A address is loaded from the main memory into a block of the cache. In loading the data at the source A address into the cache block the mapped address for the cache block is set to identify the source A address.

Similarly if the determination at step is no method continues to step the data at the source B address is loaded from the main memory into a block of the cache. In loading the data at the source B address into the cache block the mapped address for the cache block is set to identify the source B address.

Once data from both source A and source B are confirmed to be in the cache method continues to step where the cache mapping is changed such that the mapped addresses for the respective cache blocks are swapped. Specifically the mapped address for the cache block storing the data at the source A address is changed from the source A address to the source B address. Similarly the mapped address for the block storing the data at the source B address is changed from the source B address to the source A address. The swap of mapped address may be performed simultaneously or as part of an atomic operation so as to prevent the possibility that a single address e.g. either the source A or source B address may be ambiguously mapped by two cache blocks.

At step additional update steps may optionally be performed. As described above in connection with step post update operations may involve write back of data from the cache to main memory. Here the blocks associated with source A and source B may be written back. Though none one or two of these blocks may be written back as part of the swap operation. Some or all of these write back operations may be deferred until eviction or other suitable time. Accordingly processing at step may simply entail setting a dirty bit for one or both blocks. Though in some embodiments not express steps may be taken to control write back if write back would otherwise occur as part of normal operation of the cache.

Having described methods and for processing a copy and swap command respectively an example of manipulation of the mappings associated with each operation each is given with reference to and .

In this example columns and represent entries in a portion of the cache mapping. Specifically column of the mapping identifies the cache block and column identifies an address of main memory to which the respective block is mapped. Column illustrates the data stored in each cache block corresponding to the identified cache block. The cache mapping may also store additional information not shown e.g. a dirty bit .

Main memory is also represented by a columned table. Column identifies an address of in the main memory and column provides the data stored in the memory block corresponding to the address.

In this example a copy command is received by the cache controller instructing copying of data from address 2 to address 5 see step . In this example the source address address 2 is not stored in cache see step .

As shown in address 5 is evicted from the cache to make room for the data copied from the source address. The address to be evicted may be chosen in any suitable way. For example a cache algorithm may be used to prioritize cache blocks and designate blocks for eviction. In this example address 5 is selected for eviction from block 2. Since block 2 is not dirty no write back to main memory is performed. The source data is copied into block 2 of cache see step over bus . The mapped address for block 2 is set to address 2 though this step may not be performed in all embodiments.

Upon updating the map optional steps may also be performed such as those described in step . illustrates an optional step of writing the data from block 2 to the memory address in main memory to which the block is mapped namely address 5. Write back may be done for example at the time of eviction or as part of a lazy write.

In this example a swap command is received by the cache controller instructing swapping of data at address 0 with data at address 6 see step . As shown in initially neither source address is stored in cache .

As shown in once it is determined that source address 0 is not mapped by cache see step the data at address 0 is copied from main memory into cache see step . Similarly once it is determined that source address 6 is not mapped by cache see step the data at address 6 is copied from main memory into cache see step . The blocks of cache to which the data at the source addresses is copied may be chosen in any suitable way. For example a cache algorithm may have identified blocks 1 and 2 as having the lowest priority in the cache. illustrates that data is moved from main memory to the cache over memory bus .

Upon updating the mapping optional steps may also be performed such as those described in step of method . illustrates an optional step of writing the data from cache back to main memory . Specifically data in cache block 1 is copied to a block in main memory having address 6 and block 2 is copied to a block in main memory having address 0 in accordance with the remapping.

In addition to copy and swap commands the cache controller may be configured to receive a deprioritize command. A deprioritize command indicates one or more addresses which are candidates for eviction. For example when a new address is to be cached the cache algorithm may select deprioritized addresses for eviction to make room in the cache. Deprioritizing may be used by software applications when processing is completed and access to data at a particular address or set of addresses is no longer expected. Some standard cache algorithms heuristics might otherwise consider these addresses as having high priority since they represent the most recently used addresses. A deprioritize command allows hardware to more intelligently use cache space.

At step a deprioritize command is received. The deprioritize command may specify an address set of addresses or a range of addresses as addresses to be deprioritized. Though addresses to be deprioritized may be specified in any suitable way.

At step the addresses indicated by the deprioritize command are deprioritized. The deprioritized addresses may be deprioritized in any suitable way. In some embodiments an eviction priority associated with cache blocks mapped to the deprioritized addresses is updated to reflect the address s deprioritized status. A cache algorithm may manage how the addresses are deprioritized. For example in some embodiments the cache blocks mapped to deprioritized addresses are evicted before other blocks in the cache. The order in which deprioritized addresses are evicted may be selected in any suitable way. In some embodiments blocks mapped to deprioritized addresses and that are not dirty are evicted before dirty blocks mapped to deprioritized addresses. After step method ends.

As shown in in this example block 0 initially has priority 1 and contains not dirty data mapped to address 5 block 1 has priority 2 and contains dirty data mapped to address 3 block 2 has priority 0 and contains dirty data mapped to address 4 and block 3 has priority 3 and contains not dirty data mapped to address 2.

Assume a deprioritize command specifying deprioritization of address in the range 0 to 3 is received by the cache controller see step . Recall that which addresses are cached is generally transparent to software executing on the processors of the computer system. It is seen from column that in the address range 0 3 only addresses 2 and 3 are actually cached in this example.

Another command the cache controller may receive is an invalidate command. An invalidate command indicates that the data at addresses specified by the command is obsolete and can be evicted without write back. Software applications may use the invalidate command to avoid consuming bus bandwidth and cache space for short lived data.

At step the cache controller receives an invalidate command. The invalidate command may specify one or more addresses to be invalidated. The addresses may be indicated in ways similar to in a deprioritize command. Though addresses may be specified in any suitable way.

At step the addresses indicated by the invalidate command are marked as not dirty . As discussed above not dirty addresses are not written back to the main memory. Thus the invalidate command may reduce use of memory bus bandwidth and or processing delays.

At step the addresses are optionally deprioritized. The deprioritization may be done in ways similar to those described at step of method . The invalidated blocks for example may be given the lowest possible priority so that they will be evicted when cache controller next requires a block to cache new data. Though in some alternative embodiments the invalidated blocks may be invalidated immediately. After step method ends.

Assume an invalidate command is received specifying address 2 to be invalidated see step . As shown in the cache block mapping to address 2 i.e. block 0 is updated to indicate the block is not dirty see step .

Another command the cache controller may receive is a zeroing command. The zeroing command sets the data of a specified address range to a default value for example zero. A zeroing command may be implemented in any suitable way.

At step a zeroing command is received. The zeroing command may specify data at one or more addresses be set to the default value. In some embodiments the value may be specified by the zeroing command another command or may be fixed.

At step the addresses specified by the zeroing command are marked as zeroed . The addresses may be marked as zeroed in any suitable way. In some embodiments the addresses may be mapped to cache blocks and a zero bit corresponding to each address may be marked see zero bit in . The zero bit may indicate that data at the address should be read as having the default value not the actual value stored by the cache block. In such an embodiment a cache controller may provide a zero value when a zeroed value is accessed whether that access is part of a read from a processor or a write back to main memory.

At step additional update steps may optionally be performed. For example if a block containing a zeroed address is evicted from the cache the default value may be written to the block in the main memory at the mapped address.

Specifically information used by the cache for mapping may include a global zero list rather than a zero bit for each cached block. The zero list may include addresses that are not otherwise cached by a cache block. If an addresses in the zero list is subsequently written to it may be removed from the zero list. In embodiments where an address may be in the zero list and also mapped to a cache block the cache controller may search the zero list when determining whether an address is cached before searching the mapped addresses.

At step a zeroing command is received by the cache controller. The zeroing command specifies one or more addresses to be zeroed. For example the addresses may be specified as a range.

At step the addresses are added to a zero list. The addresses may be specified in the zero list individually by one or more ranges or in any suitable way or combination of ways.

At step any additional post update steps such as a lazy write of zeros into main memory locations on the zero list may be optionally performed. After step method ends.

At step a zeroing command is received. The zeroing command may specify one or more addresses for which the data is to be zeroed. In some embodiments the zeroing command further specifies the default value. Steps are performed for each address for which the data is to be zeroed. The steps may be performed for each address simultaneously sequentially or in any suitable way.

When the address is not cached method continues to step where if there are no available cache blocks the lowest priority address is evicted from the cache.

At step the mapped address for the cache block evicted at step is updated to the address for which the data is to be zeroed.

At step all of the locations in the cache block mapped to the address for which the data is to be zeroed may be set to the default value such as zero. This step may be performed by actually writing data into the block. Alternatively this step may be performed similar to a move operation. However rather than moving data associated with a source address in the main memory the source of the data may be a cache block that was previously filled with zero values. Such a pre zeroed cache block for example may be created by cache controller while it is otherwise idle.

At step any additional update steps such as those discussed above following a move operation may optionally be performed. Method ends after all addresses for which the data is to be zeroed have been processed.

Initially as shown in cache block 0 is mapped to address 2 is dirty and stores data ADG block 1 is mapped to address 4 is dirty and stores data BEH block 2 is mapped to address 5 is not dirty and stores data PQR and block 3 is mapped to address 3 is not dirty and stores data JKL . In this example default value is fixed with data 000 .

Assume a zeroing command is received by cache specifying addresses 1 3 as addresses for which the data is to be zeroed. As shown in only address 2 and 3 are cached.

Accordingly address 1 is loaded into cache . shows that address 5 is evicted from block 2 in accordance with a suitable cache algorithm. Block 2 is remapped to address 1. Note that data from address 1 need not actually be read from main memory as the zeroing operation overwrites the data regardless.

With each of address 1 3 now mapped by the cache the zero bit is set for blocks 0 2 and 3 mapping to addresses 2 1 and 3 respectively . In some embodiments the blocks are also marked as dirty as shown in . Though in some embodiments dirty may be understood from the use of the zeroing bit.

Having thus described several aspects of at least one embodiment of this invention it is to be appreciated that various alterations modifications and improvements will readily occur to those skilled in the art.

Such alterations modifications and improvements are intended to be part of this disclosure and are intended to be within the spirit and scope of the invention. Accordingly the foregoing description and drawings are by way of example only.

The above described embodiments of the present invention can be implemented in any of numerous ways. For example the embodiments may be implemented using hardware software or a combination thereof. When implemented in software the software code can be executed on any suitable processor or collection of processors whether provided in a single computer or distributed among multiple computers.

Further it should be appreciated that a computer may be embodied in any of a number of forms such as a rack mounted computer a desktop computer a laptop computer or a tablet computer. Additionally a computer may be embedded in a device not generally regarded as a computer but with suitable processing capabilities including a Personal Digital Assistant PDA a smart phone or any other suitable portable or fixed electronic device.

Also a computer may have one or more input and output devices. These devices can be used among other things to present a user interface. Examples of output devices that can be used to provide a user interface include printers or display screens for visual presentation of output and speakers or other sound generating devices for audible presentation of output. Examples of input devices that can be used for a user interface include keyboards and pointing devices such as mice touch pads and digitizing tablets. As another example a computer may receive input information through speech recognition or in other audible format.

Such computers may be interconnected by one or more networks in any suitable form including as a local area network or a wide area network such as an enterprise network or the Internet. Such networks may be based on any suitable technology and may operate according to any suitable protocol and may include wireless networks wired networks or fiber optic networks.

Also the various methods or processes outlined herein may be coded as software that is executable on one or more processors that employ any one of a variety of operating systems or platforms. Additionally such software may be written using any of a number of suitable programming languages and or programming or scripting tools and also may be compiled as executable machine language code or intermediate code that is executed on a framework or virtual machine.

In this respect the invention may be embodied as a computer readable medium or multiple computer readable media e.g. a computer memory one or more floppy discs compact discs optical discs magnetic tapes flash memories circuit configurations in Field Programmable Gate Arrays or other semiconductor devices or other tangible computer storage medium encoded with one or more programs that when executed on one or more computers or other processors perform methods that implement the various embodiments of the invention discussed above. The computer readable medium or media can be transportable such that the program or programs stored thereon can be loaded onto one or more different computers or other processors to implement various aspects of the present invention as discussed above.

The terms program or software are used herein in a generic sense to refer to any type of computer code or set of computer executable instructions that can be employed to program a computer or other processor to implement various aspects of the present invention as discussed above. Additionally it should be appreciated that according to one aspect of this embodiment one or more computer programs that when executed perform methods of the present invention need not reside on a single computer or processor but may be distributed in a modular fashion amongst a number of different computers or processors to implement various aspects of the present invention.

Computer executable instructions may be in many forms such as program modules executed by one or more computers or other devices. Generally program modules include routines programs objects components data structures etc. that perform particular tasks or implement particular abstract data types. Typically the functionality of the program modules may be combined or distributed as desired in various embodiments.

Also data structures may be stored in computer readable media in any suitable form. For simplicity of illustration data structures may be shown to have fields that are related through location in the data structure. Such relationships may likewise be achieved by assigning storage for the fields with locations in a computer readable medium that conveys relationship between the fields. However any suitable mechanism may be used to establish a relationship between information in fields of a data structure including through the use of pointers tags or other mechanisms that establish relationship between data elements.

Various aspects of the present invention may be used alone in combination or in a variety of arrangements not specifically discussed in the embodiments described in the foregoing and is therefore not limited in its application to the details and arrangement of components set forth in the foregoing description or illustrated in the drawings. For example aspects described in one embodiment may be combined in any manner with aspects described in other embodiments.

Also the invention may be embodied as a method of which an example has been provided. The acts performed as part of the method may be ordered in any suitable way. Accordingly embodiments may be constructed in which acts are performed in an order different than illustrated which may include performing some acts simultaneously even though shown as sequential acts in illustrative embodiments.

Use of ordinal terms such as first second third etc. in the claims to modify a claim element does not by itself connote any priority precedence or order of one claim element over another or the temporal order in which acts of a method are performed but are used merely as labels to distinguish one claim element having a certain name from another element having a same name but for use of the ordinal term to distinguish the claim elements.

Also the phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of including comprising or having containing involving and variations thereof herein is meant to encompass the items listed thereafter and equivalents thereof as well as additional items.

