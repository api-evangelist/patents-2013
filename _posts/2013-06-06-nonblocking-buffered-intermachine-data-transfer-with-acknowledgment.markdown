---

title: Non-blocking buffered inter-machine data transfer with acknowledgment
abstract: A system, method and data structures for transmitting batched data over a network in asynchronous, non-blocking operations, with acknowledgements returned to the senders. Each machine includes a sender subsystem and a receiver subsystem. Transmission items corresponding to data to be sent are buffered until a send is triggered, at which time the transmission items are sorted according to their destinations and sent as a GUID-identified batch to each destination by a send thread. At each receiver, a receiving thread adds descriptions of the data to a receive list, which a transfer thread processes when triggered. The transfer thread sends the corresponding data to a recipient program, and sends GUID-identified acknowledgements corresponding to each GUID back to the originating senders. An acknowledge thread at each originating sender buffers the acknowledgements, and when triggered, a cleanup thread uses the GUID to identify which batched transmissions were received.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08938555&OS=08938555&RS=08938555
owner: Microsoft Corporation
number: 08938555
owner_city: Redmond
owner_country: US
publication_date: 20130606
---
This application is a continuation of U.S. patent application Ser. No. 11 509 438 filed on Aug. 23 2006 entitled NON BLOCKING BUFFERED INTER MACHINE DATA TRANSFER WITH ACKNOWLEDGMENT which issued as U.S. Pat. No. 8 463 937 on Jun. 11 2013 and which is a continuation of U.S. patent application Ser. No. 10 396 870 filed Mar. 24 2003 entitled NON BLOCKING BUFFERED INTER MACHINE DATA TRANSFER WITH ACKNOWLEDGEMENT and which issued as U.S. Pat. No. 7 103 890 on Sep. 5 2006 both of which are incorporated herein by reference in their entireties.

The invention relates generally to computer systems and networks and more particularly to network data transmission.

Application and other component e.g. driver developers often treat data transmission as a relatively simplistic operation. Most do not employ any means of data queuing or asynchronous delivery. For example a typical way in which data transmission that is guaranteed to be delivered is handled is to send the data from a source to a transmission related component via a function call and then block the function call waiting for an acknowledgement that the data was received at the destination. If successfully received the source transmission function returns the acknowledgement and the source unblocks. If no return acknowledgement is received the source may time out and will handle the lack of acknowledgement as a failure in some manner e.g. re transmit the data for some number of times or return an error. The source side blocking is made even longer by the receiver side delaying their return from the receiving function. Instead of queuing on the receive side the receiver might perform some computations with the received data before returning thus delaying the acknowledgement.

While such a data transmission operation and others like it work fairly well blocking is not desirable since it prevents other work from being done and does not make full use of the data channel. One way to avoid blocking is to send transmissions without requiring an acknowledgement however those transmissions are not known to have arrived at the destination and are thus inappropriate for many types of data transmissions.

Moreover problems arise when networks have thousands of machines with substantial numbers of transmissions flowing both ways. For example in a large network with many events alerts and performance monitoring data that needs to be transmitted along with conventional network traffic to handle web page serving file serving web services and so forth existing methods of data transmission can only scale to networks having machines numbering in the hundreds. Existing data transmission methods simply do not work for networks with machines on the order of thousands. Instead various sets of machines e.g. three hundred or so have to be grouped together with each set managed by its own managing server. As can be appreciated having to purchase and maintain so many managing servers is highly undesirable e.g. for a network of 20000 computers between sixty and seventy managing servers would be needed each handling a set of around 300 computers.

What is needed is an improved communication method system and protocol that scales to thousands of machines while operating in a non blocking asynchronous manner. At the same time the communication should be such that transmitted data is accomplished with a notification provided to the sender to acknowledge that the transmitted data was successfully received.

Briefly the present invention provides a system method and data structures that implement an improved communication protocol and mechanisms for handling the transmission of data in a non blocking manner with asynchronous notifications returned to the senders to acknowledge that the data was successfully received. In general the present invention batches many small transmissions together into a lesser number of larger transmissions thereby scaling to large numbers of networked machines.

In one implementation each machine whether server client or both includes a sender subsystem and a receiver subsystem. The sender subsystem includes an outgoing queue having a buffer for data to be sent and an incoming queue for buffering acknowledgements. The receiver subsystem includes an incoming queue with a buffer for received data and an outgoing queue for buffering and sending acknowledgements. Each queue enables the rapid acceptance of data with a buffer for later moving the data in bulk that is data is sent out as a batch directed towards each destination and accepted at each destination in a batch.

Data can be added to the current outgoing buffer via a transmission request e.g. submitted by any thread of a process. The data to be sent identifies the destination and the data to send and a single transmission request can specify multiple destinations and or multiple sets of data to send. In one implementation the data to be sent is provided in transmission items with each transmission item listing one or more destinations to send the data to and listing one or more sets of data e.g. identified by buffer pointers and sizes to send. As the transmission items are submitted they are added as an entry to a transmission item list an array of a currently active tracking element and placed in a transmission item queue buffer e.g. stored in the array.

Data transmission may be triggered by a configurable passage of time priority or by a threshold amount of data being achieved e.g. corresponding to a total size of data to be sent on the sender or received or possibly a number of acknowledgements to send back from the receiver . The times sizes and other variables may be adjusted to tune a given network or a machine for better performance. Once triggered the tracking element queue buffer closes by moving out of the active accumulation phase with a new buffer made available to start accumulating subsequent data for the next send operation. In other words once the time limit has passed or a size limit has been achieved for example the active transmission tracking list element is closed off and a new element is started to maintain subsequent transmission requests.

After triggering but before sending the data to be sent is processed so as to sort split the data by destination. To track the data per destination the tracking element e.g. of a send thread creates a data item destination list that corresponds to per destination data items constructed from the transmission items. For tracking purposes a GUID globally unique identifier or like identifier that is unique to at least the network is assigned by the send thread to each per destination transmission. The GUID will be later returned with the acknowledgment and used to free up the appropriate data and data structures once the data has been acknowledged as having been received. When the data is split by destination into sets of data items to send each set is sent out on the socket to its destination.

The receiver receives the data items off its socket and immediately e.g. via a receive thread places each item in a currently active receiving element of a receive queue or list. The GUIDs are placed in one list while the data items are listed in a parallel list. Once a configurable amount of time has passed or some size threshold achieved such as the amount of data in the queue the current receive list element is closed off and a new receive list element started for subsequently received data. The data corresponding to the closed off list s data items are passed to a receiving application program by a transfer thread and the list of acknowledgement GUIDs is used to send back acknowledgements to their destination origins. To reduce the number of acknowledgement transmissions the GUIDs may be similarly grouped batched by their origin.

When an acknowledgement transmission is returned by the receiver and received at the original sender the transmissions one or more acknowledgement GUIDs are placed into an acknowledgement list element queue e.g. by a separate acknowledge thread for later processing by a clean up thread. When a configurable time limit or size e.g. received data and or number of acknowledgements limit has been achieved the current active acknowledgement list element is closed off and a new current active acknowledgement list started. The closed off list is used by a clean up thread to search back into the transmission tracking element to mark destinations as having been successfully sent and accordingly free up the data structures as well as notify the program that provided the initial request so that the program can free up its data as desired. Any data that is not acknowledged as having been successfully received is dealt with in another manner e.g. re sent after a configurable amount of time up to some number of times until successful or determined to be an error and subject to garbage collection.

Other advantages will become apparent from the following detailed description when taken in conjunction with the drawings in which 

The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with the invention include but are not limited to personal computers server computers hand held or laptop devices tablet devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and the like.

The invention may be described in the general context of computer executable instructions such as program modules being executed by a computer. Generally program modules include routines programs objects components data structures and so forth which perform particular tasks or implement particular abstract data types. The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment program modules may be located in local and or remote computer storage media including memory storage devices.

With reference to an exemplary system for implementing the invention includes a general purpose computing device in the form of a computer . Components of the computer may include but are not limited to a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnect PCI bus also known as Mezzanine bus.

The computer typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by the computer and includes both volatile and nonvolatile media and removable and non removable media. By way of example and not limitation computer readable media may comprise computer storage media and communication media. Computer storage media includes volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can accessed by the computer . Communication media typically embodies computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media includes wired media such as a wired network or direct wired connection and wireless media such as acoustic RF infrared and other wireless media. Combinations of the any of the above should also be included within the scope of computer readable media.

The system memory includes computer storage media in the form of volatile and or nonvolatile memory such as read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computer such as during start up is typically stored in ROM . RAM typically contains data and or program modules that are immediately accessible to and or presently being operated on by processing unit . By way of example and not limitation illustrates operating system application programs other program modules and program data .

The computer may also include other removable non removable volatile nonvolatile computer storage media. By way of example only illustrates a hard disk drive that reads from or writes to non removable nonvolatile magnetic media a magnetic disk drive that reads from or writes to a removable nonvolatile magnetic disk and an optical disk drive that reads from or writes to a removable nonvolatile optical disk such as a CD ROM or other optical media. Other removable non removable volatile nonvolatile computer storage media that can be used in the exemplary operating environment include but are not limited to magnetic tape cassettes flash memory cards digital versatile disks digital video tape solid state RAM solid state ROM and the like. The hard disk drive is typically connected to the system bus through a non removable memory interface such as interface and magnetic disk drive and optical disk drive are typically connected to the system bus by a removable memory interface such as interface .

The drives and their associated computer storage media discussed above and illustrated in provide storage of computer readable instructions data structures program modules and other data for the computer . In for example hard disk drive is illustrated as storing operating system application programs other program modules and program data . Note that these components can either be the same as or different from operating system application programs other program modules and program data . Operating system application programs other program modules and program data are given different numbers herein to illustrate that at a minimum they are different copies. A user may enter commands and information into the computer through input devices such as a tablet or electronic digitizer a microphone a keyboard and pointing device commonly referred to as mouse trackball or touch pad. Other input devices not shown in may include a joystick game pad satellite dish scanner or the like. These and other input devices are often connected to the processing unit through a user input interface that is coupled to the system bus but may be connected by other interface and bus structures such as a parallel port game port or a universal serial bus USB . A monitor or other type of display device is also connected to the system bus via an interface such as a video interface . The monitor may also be integrated with a touch screen panel or the like. Note that the monitor and or touch screen panel can be physically coupled to a housing in which the computing device is incorporated such as in a tablet type personal computer. In addition computers such as the computing device may also include other peripheral output devices such as speakers and printer which may be connected through an output peripheral interface or the like.

The computer may operate in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be a personal computer a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer although only a memory storage device has been illustrated in . The logical connections depicted in include a local area network LAN and a wide area network WAN but may also include other networks. Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet. For example in the present invention the computer system may comprise source machine from which data is being migrated and the remote computer may comprise the destination machine. Note however that source and destination machines need not be connected by a network or any other means but instead data may be migrated via any media capable of being written by the source platform and read by the destination platform or platforms.

When used in a LAN networking environment the computer is connected to the LAN through a network interface or adapter . When used in a WAN networking environment the computer typically includes a modem or other means for establishing communications over the WAN such as the Internet. The modem which may be internal or external may be connected to the system bus via the user input interface or other appropriate mechanism. In a networked environment program modules depicted relative to the computer or portions thereof may be stored in the remote memory storage device. By way of example and not limitation illustrates remote application programs as residing on memory device . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

The present invention is in part directed towards large networks wherein significant numbers of data transmission requests are made. As will be understood however the present invention is not limited to any particular size type or configuration of network but rather provides significant advantages and benefits in virtually any computing environment needing data communication. Further the present invention is described as being an operating system component which receives calls e.g. through an application programming interface layer from at least one application program. However it should be understood that the present invention may be implemented at virtually any level of a computer system such as in an application program incorporated into a low level operating system driver just above the networking hardware or essentially anywhere in between.

Turning to of the drawings there is provided an example representation of a network having an arbitrary number of computing machines .sub. .sub.n. In practice with the present invention networks have been implemented that successfully accomplish needed inter machine data transfer with machines numbering on the order of five or even ten thousand given contemporary computing devices and contemporary transmission media. Notwithstanding no actual upper limit of the number of machines has been determined.

In accordance with an aspect of the present invention each machine is arranged with a sender subsystem S e.g. .sub. and a receiver subsystem R e.g. .sub. that operate at some level above network transport components Netw e.g. .sub. . Note that for clarity in only the machines labeled .sub. and .sub. have their sender subsystem S receiving sender subsystem R and networking components Netw numerically labeled .sub. .sub. and .sub. and .sub. .sub. and .sub. however it is understood that each machine communicating according to the present invention has like components. As also represented in by the dashed lines the machine .sub. is sending a transmission to the machine .sub. and e.g. shortly thereafter is receiving a transmission from the machine .sub..

As represented in by the circled numerals one and two data such as in the form of transmission items e.g. .sub. .sub. can be added to a current transmission item outgoing buffer by a data generator which may include essentially any thread in an appropriate process. To this end the outgoing queue conceptually comprises a transmission item tracking component e.g. in a send thread of the send subsystem .sub. which maintains an active tracking element .sub. comprising a list e.g. array of items to transmit and at a later time as described below creates and maintains a list array of per destination data items to transmit as grouped according to their destinations. As described above the transmission item buffer holds the transmission items e.g. .sub. and .sub. .

More particularly represents an example implementation showing the submission and handling of transmission items e.g. .sub. and .sub. in more detail. The data generator of such as in the form of an application program running a process executing a data generation thread calls the send subsystem .sub. with a send request such as via an API layer. The send request includes data in the form of a transmission item e.g. .sub. submitted to the transmission item tracking component . In this example implementation each transmission item .sub. .sub.m comprises a list of one or more destinations along with a list of one or more data items. Each data item in turn comprises a self describing structure that knows about its corresponding data set or sets and its size or sizes e.g. for each set of data the data item has a pointer to a buffer location e.g. .sub. containing the data to transmit and a size value. Note that in addition to a single transmission item being able to specify multiple destinations and multiple sets of data more than one transmission item can be submitted on a single call.

As represented in and in by the circled numeral two as transmission items e.g. .sub. .sub. are submitted they are tracked via a transmission item list of the currently active transmission item tracking element .sub. note that the data item destination list is not used at this point . More particularly as represented in as the data items are received the transmission item tracking component adds an entry e.g. offset pointers TI TI . . . TIm for each transmission item to its tracking list and places e.g. copies the transmission item e.g. .sub.m into the transmission item buffer . By way of example as represented in the transmission item .sub. indicates that the four items of data identified therein D D D and D are to be sent to three destinations identified in this simplified example as machines M N and W. As described above each data item D D D and D includes a location of the data and a size value.

In general the tracking element .sub. comprises a dynamic list that is active while being loaded and is closed while the data is being processed for transmission. As represented in each time a tracking element is closed a new tracking element .sub. is started such that there is always an active transmission item tracking list to track submitted transmission items. There may be one or more closed tracking lists tracking data currently being transmitted.

The transmission tracking element .sub. is closed when triggered by some event such as a configurable timer indicating a time limit has been reached the summed sizes of the data to transmit reaching a size threshold a high priority flag or the like accompanying the data or some other event for example an event indicating that whatever data is buffered should be sent right away because the application program or computer system is shutting down. Other priorities are also possible e.g. send without queuing put into a different e.g. low priority queue that is only sent when there is not a backup and so forth. Note that with respect to the timer there are many equivalent ways to time an event e.g. register with an event timer compare a starting timestamp with a current timestamp and so on and the present invention can use virtually any such timing mechanism.

As represented in and corresponding to the circled numeral three in upon such a trigger event the active transmission item tracking element .sub. is closed off from accepting further transmission items and a new tracking element .sub. is started that will accept subsequent transmission requests. At this time the tracking component e.g. a send thread now uses the closed tracking element .sub. to create the data item destination list part of the tracking element .sub.. As represented in the creation of the data item destination list essentially works by splitting the transmission items data into per destination data items. To this end a sort process or the like represented in by a splitter groups the data items by destination into a data item destination buffer containing per destination grouped data items e.g. .sub. .sub.. Such processes are well known and virtually any suitable one can be used e.g. select a transmission item and for each destination identified therein perform a union of the listed data items with any other previously established data items for that destination and repeat for each transmission item. Thus in the example represented in after splitting based on the transmission items .sub. and .sub. in and no others the destination M is to receive data corresponding to data items D D D D D and D the destination N is to receive data corresponding to data items D and D and the destination W is to receive data corresponding to data items D D D and D. Note that in the curved arrows represent one such correspondence between data item six and the set of data six to transmit it is understood that although similar curved arrows are not shown for the other data items to their corresponding data like correspondences exist.

In keeping with the present invention for tracking purposes a GUID is assigned to each per destination data item corresponding to a transmission which as described below will be used later to correlate the transmitted data item and data with its returned acknowledgement ACK . For example this allows the data item and the data itself to be freed up once the data has been acknowledged as received e.g. an ACK with that GUID has been returned.

As represented in once the per destination splitting has been performed the data for each destination is batched and sent out by a send mechanism of the send thread to the network transport .sub. that is on the socket. Note that in one preferred implementation because the send mechanism runs on a separate send thread which may also handle the pre transmission processing such as the splitting operation described above to perform the data transmission the data transfer is asynchronous and non blocking The send mechanism s sending of the data in the data item destination buffer is represented in by the circled numeral four . In addition to sending the data the send mechanism wraps the batched data in a message protocol that for each destination includes the GUID for the data item and information that allows the receiver to unwrap the data into corresponding data items.

As represented in by the circled letter A a receive mechanism e.g. a separate thread of the receiver subsystem .sub. receives the data off the socket and places the data in a currently active receiving element .sub. of a receive list . Preferably as represented in the receive mechanism comprises a separate receive thread so that the received data is immediately placed in the active receiving element .sub. of a receive list .

It should be noted that many other senders may be sending to the receiver subsystem and each sender may be sending multiple times to the same receiver. Thus as described above each received message needs to be tracked by its originating sender or origin as well as by the GUID that was generated at the sender when the transmission items were split into the per destination data items . To track the received data the origin and GUID are maintained in one list while the data e.g. buffer pointer and size is maintained in a parallel list .

As represented in and corresponding to the circled letter B in once a configurable amount of time has passed or other mechanism such as kept by another timer the current receive list element .sub. is closed off and a new one .sub. is started. When closed off the listed data items are passed by an internal transfer thread to the appropriate recipient application program for processing by its own thread and acknowledgements ACKS for each received transmission e.g. per origin GUID are sent back to their destination origins such as in a set of GUIDS. Note that like the original transmissions the acknowledgements GUIDs may be split and batched according to their origins by the internal transfer thread .

Returning to and corresponding to the circled numeral five in when an acknowledgement is sent back and received each acknowledgement s GUID is placed by an ACK handler e.g. a separate thread in a queue such as an active element .sub. of an ACK back list for later processing. Upon an event such as a configurable time limit being reached or size and or number of ACKs limit being reached or some other event such as a higher priority ACK being detected application or system shutdown being detected and so on the current active acknowledgement list element .sub. is closed off and a new acknowledgement list element .sub. started. A cleanup mechanism such as running as a separate cleanup thread uses the closed off ACK back list element .sub. to search back into the transmission tracking element s data structures generally to mark transmissions as successfully sent and or free up the data structures. The cleanup thread also may return to the application program to indicate the successful transmission whereby the program may free up its data. Any data that is not marked as acknowledged in the tracking component s data structures are subject to re sending after a configurable amount of time and or to be garbage collected.

Turning to an explanation of the operation of the present invention with particular reference to the flow diagrams of step of represents testing for whether data to transmit e.g. a transmission item has been submitted. If not step branches to step which resets the timer at step if no data has yet been queued e.g. so that the timer does not attempt to trigger a send when there is no data to send. Step loops back to step to await data. If there is some data in the queue at step step instead branches ahead to step to determine whether it is time to send that data. If not step loops back to step to handle more data and or trigger the send. Note that at least some of the steps of may be event driven rather than operate in a loop however a loop is shown in the example for purposes of clarity.

Returning to step if a new transmission item is submitted for queuing step instead branches to step where the transmission item is queued in the transmission item buffer. Step adds an entry to the transmission item list in the active transmission tracking element. Step then tests for whether the size limit has been reached and if so branches ahead to step to process the transmission items for sending as described above. If not other tests may be performed e.g. step represents a test as to whether the data indicates a high priority item whereby the data would be sent right away. Other tests such as an external event are not represented in for purposes of simplicity.

Eventually whether by size at step by priority at step by time at step or by some other event the send trigger is achieved and the process reaches step . Step represents the closing of the active transmission tracking element with step representing the opening of the new transmission tracking element and the resetting of the timer. Subsequent transmission requests will go into that new active element until it is likewise closed and a new one opened at the next triggered send.

Step represents the creation of the data item destination list accomplished by splitting the transmission items into data items by their destinations and adding entries to the data item destination list. Once split step sends the data to each destination. The process then loops back to step where the new active transmission element is used. Note that the filling may be via a separate thread or the send thread can handle both the sending and filling of the queue.

Turning to an explanation of how received data is handled once placed in the receive list it will be understood that the internal data transfer including the sending of acknowledgements essentially may follow the same queuing and batching logic as that of except that ACKs including GUIDs are extracted from the transmissions and queued rather than transmission items being submitted and handled. Thus will generally follow most of except that shows the buffer being filled by some thread not necessarily the send thread while is premised on a separate receive thread having filling the receive list as data is received.

Step of tests for whether there is any data in the receive queue and if not resets the timer at step until some data is detected. If there is data steps is instead executed which represents testing for whether it is time to handle the queued data and GUIDs and if not branching to step to test for a size and or GUID quantity limit having been reached. Step represents testing whether any of the data was high priority data for example in which case the data should be handled dequeued and receipt acknowledged without delay. Note that the time limits and sizes may be very different for received data e.g. as a machine may send many more transmissions than it receives or vice versa.

When triggered whether by time at step by size at step by priority at step or by some other event the process reaches step . Step represents the closing of the receive list element with step representing the opening of the new receive list element and the resetting of the timer. Step represents transferring the data e.g. buffer pointers and sizes to the recipient program or programs. Step represents splitting the acknowledgements GUIDS by their source and returning them to their corresponding originators.

When triggered whether by time at step by size quantity at step by priority at step or by some other event the process reaches step . Step represents the closing of the ACK back list element with step representing the opening of the new ACK back list element and the resetting of the timer. Step represents notifying the program or programs of the successful receipt of their transmission request while step represents using the GUIDs to search the transmission tracking element data structures so as to handle their acknowledgment and free them up when no longer needed. For example the per destination data item can be freed once its GUID was returned however the transmission item which may specify multiple recipients cannot be freed until each recipient has acknowledged the receipt of the data.

It should be noted that the various timers sizes priorities and so forth may be adjusted for each thread to tune the network machines as desired. The tuning may be for many machines or all machines in the network or per machine e.g. certain machines that send a lot of data may be given larger maximum sizes and or longer accumulation times. Further the tuning may be dynamic by communication from another machine e.g. so as to throttle a machine that is sending too many transmissions.

As can be seen from the foregoing detailed description there is provided a method and system for handling the transmission of data in a non blocking manner with asynchronous notifications returned to the senders to acknowledge that the data was successfully received. In general by queuing and then intelligently batching the transmissions into a lesser number of combined data transmissions the present invention scales to large numbers of networked machines. The method and system thus provide significant advantages and benefits needed in contemporary computing.

While the invention is susceptible to various modifications and alternative constructions certain illustrated embodiments thereof are shown in the drawings and have been described above in detail. It should be understood however that there is no intention to limit the invention to the specific forms disclosed but on the contrary the intention is to cover all modifications alternative constructions and equivalents falling within the spirit and scope of the invention.

