---

title: Extension of logical networks across layer 3 virtual private networks
abstract: A method of manages a set of managed forwarding elements that forward data between machines. The method configures (1) a first managed forwarding element to operate in a first network that uses first and second address spaces that at least partially overlap with each other, (2) a second managed forwarding element to operate in a second network that uses the first address space, and (3) a third managed forwarding element to operate in a third network that uses the second address space. A machine in the second network and a machine in the third network have an identical address that belongs to both the first and second address spaces. The method directs the first managed forwarding element to connect to the second and third managed forwarding elements in a manner that enables the first managed forwarding element to forward data from a machine in the first network to the machine in the second network via the second managed forwarding element.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09331938&OS=09331938&RS=09331938
owner: NICIRA, INC.
number: 09331938
owner_city: Palo Alto
owner_country: US
publication_date: 20130315
---
This application claims the benefit of U.S. Provisional Application 61 623 828 entitled Extension of Virtual Networks across Layer 3 Virtual Private Networks filed Apr. 13 2012. U.S. Application 61 623 828 is incorporated herein by reference.

Many current enterprises have large and sophisticated networks comprising switches hubs routers servers workstations and other networked devices which support a variety of connections applications and systems. The increased sophistication of computer networking including virtual machine migration dynamic workloads multi tenancy and customer specific quality of service and security configurations requires a better paradigm for network control. Networks have traditionally been managed through low level configuration of individual components. Network configurations often depend on the underlying network for example blocking a user s access with an access control list ACL entry requires knowing the user s current IP address. More complicated tasks require more extensive network knowledge forcing guest users port 80 traffic to traverse an HTTP proxy requires knowing the current network topology and the location of each guest. This process is of increased difficulty where the network forwarding elements are shared across multiple users.

In response there is a growing movement towards a new network control paradigm called Software Defined Networking SDN . In the SDN paradigm a network controller running on one or more servers in a network controls maintains and implements control logic that governs the forwarding behavior of shared network forwarding elements on a per user basis. Making network management decisions often requires knowledge of the network state. To facilitate management decision making the network controller creates and maintains a view of the network state and provides an application programming interface upon which management applications may access a view of the network state.

Some of the primary goals of maintaining large networks including both datacenters and enterprise networks are scalability mobility and multi tenancy. Many approaches taken to address one of these goals results in hampering at least one of the others. For instance one can easily provide network mobility for virtual machines within an L2 domain but L2 domains cannot scale to large sizes. Furthermore retaining tenant isolation greatly complicates mobility. As such improved solutions that can satisfy the scalability mobility and multi tenancy goals are needed.

Some embodiments of the invention provide a network controller that generates configuration data for configuring a set of managed forwarding elements operating in several different network sites connected through a wide area network WAN such that the machines in the different sites can share the same address spaces.

One of the use cases for network virtualization is to connect a customer s data center across a WAN to a multi tenant data center of a service provider SP . The service provider s data center is virtualized using an overlay of tunnels that interconnect forwarding elements within the data center typically virtual switches running on computers hosting one or more virtual machines that run a top of a hypervisor. A dedicated forwarding element referred to as a service node or pool node is then used to forward packets from a tunnel within the provider s data center onto a tunnel that leads to the customer site. At the customer site this tunnel is terminated on another forwarding element referred to as a gateway or an extender which forwards packets between the customer network and the tunnel.

In the current state of the art the tunnel from the forwarding element in the SP data center to the forwarding element in the customer data center can be any sort of IP tunnel GRE IPsec etc. but the customer s IP address must be unique. That is two different customers cannot use the same IP address for their extenders. In general for the SP data center to be able to route packets over the tunnels to customers each customer must have a public IP address on which the tunnel can terminate. This is a restriction that customers prefer to avoid.

Some embodiments of the invention use the capabilities of Layer 3 Virtual Private Networks as described in RFC 2547 and RFC 4364 to extend a virtualized data center network across the WAN using only the customer s private addressing scheme. Layer 3 Virtual Private Networks L3 VPNs provide a means for the sites of a customer to be interconnected over a service provider s network. The customers of L3 VPN services can use any addresses they want they are not required to have any public or globally unique addresses.

In the network control system of some embodiments an L3 VPN service is implemented using Provider Edge PE routers Provider P routers and Customer Edge CE routers. PE routers hold a number of Virtual Routing and Forwarding tables VRFs each of which holds routing information for a particular customer. A VRF is attached to one or more interfaces of the PE so that packets arriving on the interface s are forwarded using a routing table that is specific to the appropriate customer. Using this mechanism which is fully described in RFC 4364 the network control system of some embodiments can forward packets across the service provider backbone to the correct customer location based on the customer s IP addressing plan.

The network control system of some embodiments performs two key operations to extend a virtualized data center network across the WAN while using a customer s private addressing scheme. First as the system of some embodiments builds tunnels from a forwarding element the service node to a remote forwarding element the extender that has a non unique address the system uses some additional information e.g. customer identifier as well as the IP address of the tunnel endpoint to identify the remote switch. Second the system of some embodiments maps the tunneled packets to the correct virtual interface to hit the correct VRF in the outbound direction and maps the virtual interface to the correct customer context in the inbound direction.

Several problems arise when connecting a virtualized multi tenant data center network to an L3 VPN service. First the service node device has to be able to build tunnels to IP addresses that are not unique and to be able to differentiate among these tunnels when forwarding packets from a virtual network in the service provider data center to a customer site. In other words the service node has to be aware of the address spaces of the customers and should be able to relate these addresses to virtual networks in the data center. Second the service node has to be able to forward packets to the correct VRF in the PE that sits at the edge of the WAN. This is necessary to ensure that packets are correctly forwarded across the L3 VPN to the appropriate customer site. The solutions for these problems are further described below.

One problem related to uniquely identifying tunnels is a naming problem. RFC 4364 solves the problem of how to uniquely represent non unique addresses by prepending a customer specific identifier e.g. route distinguisher to the non unique customer addresses in order to create VPN addresses that are globally unique. The network control system of some embodiments does a similar thing for tunnels that originate in the SP data center. This system names the tunnels by a customer ID and the IP address of the tunnel endpoint. This system also enhances the service node to recognize that when it forwards a packet from a virtual network inside the data center to the WAN it must forward the packet to the tunnel that represents the correct location of the correct customer.

It is worth noting that there may be many virtual networks in the SP data center that map to the same customer tunnel. There may also be many tunnels for the same customer if that customer has many sites. In other words one customer has a one to many relationship with the virtual networks in the SP data center. The network control system of some embodiments described herein enables the customer to have a one to many relationship by uniquely identifying the tunnel using some other information in addition to the IP address of the tunnel s endpoints.

The second problem is getting the packets into the correct VRF. The network control system of some embodiments addresses this problem by using one virtualized physical link between the service node and the PE device. Different embodiments use different techniques to virtualize the link. For example the link could be an Ethernet link that supports VLAN tags and each VLAN tag can represent a virtual interface on the PE. Another example would be that the service node connects to the PE over an IP network and builds a GRE tunnel to each VRF. These are standard methods to connect virtual interfaces to VRFs on a PE. However it is novel that when the service node decides to forward a packet over a tunnel that leads to the WAN the service node needs to send the packet on the appropriate virtual link VLAN GRE tunnel etc. as well as sending on the tunnel that leads all the way across the WAN. In some embodiments several of these virtual interfaces link the service node to the PE while there is only one physical link between the service node and the PE. The service node applies the correct virtual interface header VLAN tag GRE header to ensure that the packets arrive at the correct VRF. Similarly in the reverse direction the PE puts the packets received from the WAN onto the correct virtual interface which enables the service node to determine which customer network the tunnel is associated with.

As a packet moves from a service node in a multi tenant data center to a remote extender in a customer site packet headers are applied to and stripped from the packet. What is entering the pool node is a packet with a payload which is just some data that needs to get out of the data center and off to the customer site. Typically the packet will travel over a tunnel to reach the service node in the first place. The service node applies the tunnel header which is addressed to the extender in the remote customer site and then applies the Virtual Interface header to direct the packet to the correct VRF.

The packet has the Virtual IF header removed at the PE and is looked up in the correct VRF. The VRF forwards the packet based on the IP address in the tunnel header. To convey this packet across the core of the L3 VPN the PE applies MPLS headers using standard techniques. The egress PE removes the MPLS labels and sends the packet to a CE that forwards the packet on to the extender using normal IP forwarding. The extender then forwards the packet appropriately according to the same techniques that would be used when the extender has a public address.

Some embodiments perform the following operations to configure the system at the start before any packets flow 1 The VPN is provisioned using standard techniques. This includes creating VRFs for the customer. 2 A virtual interface is configured on the PE and associated with the VRF. 3 The service node has to be configured with the mapping between the customer and virtual interface. 4 The service node and the extender need to learn each other s addresses so they can build the tunnel between them.

Once the system performs these four operations the service node has everything it needs to map virtualized networks in the data center to the correct tunnels and virtual interfaces so that customer traffic can flow between the SP data center and the WAN.

In some embodiments network controllers in a controller cluster control the extenders by configuring the extenders such that the extenders implement the virtualized network on the fly as the underlying physical network changes. In some embodiments the controllers configure an extender at a customer site through a service node set up for the customer. Alternatively or conjunctively the controllers in some embodiments configure the extender at the customer site using a daemon running in the extender. This daemon serves as a mechanism for the extender to communicate with the controller cluster. An example of such daemon is a proxy daemon running in the service node acting as the controllers.

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawing but rather are to be defined by the appended claims because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.

In the following detailed description of the invention numerous details examples and embodiments of the invention are set forth and described. However it will be clear and apparent to one skilled in the art that the invention is not limited to the embodiments set forth and that the invention may be practiced without some of the specific details and examples discussed.

Some embodiments of the invention provide a network controller that generates configuration data for configuring a set of managed forwarding elements operating in several different network sites connected through a wide area network WAN such that the machines in the different sites can share the same address spaces. In some embodiments managed forwarding elements are forwarding elements e.g. software and hardware switches software and hardware routers etc. that are managed e.g. configured by the network controllers. The managed forwarding elements are also referred to as managed switching elements in the present application.

The managed forwarding elements forward data e.g. data frames packets etc. in a managed network. In some embodiments the managed forwarding elements fall into different categories based on the functionality of the managed forwarding elements. For instance a managed forwarding element is an edge forwarding element when the managed forwarding element forwards data to and from machines that are sources and destinations of the data. A managed forwarding element is a pool node when the managed forwarding element does not directly interface with the machines that are sources and destinations of the data but facilitates data exchange between edge forwarding elements and or forwarding elements that are remotely located i.e. the forwarding elements that are in a site separated by a WAN from the site in which the pool node is located . The pool nodes and the edge forwarding elements are within the same managed network e.g. a data center .

Moreover a managed forwarding element is an extender when the managed forwarding element operates in another network and facilitates exchanges of data that originates from or is destined to the other network. The other network may be a network in a different geographical location another managed network an unmanaged network in the same data center a network in a different network zone etc. In some embodiments the network system includes a managed forwarding element that is used as a communication gateway for communicating network data between the two networks. In some embodiments the managed forwarding element is a part of the managed network while in other embodiments the managed forwarding element is part of the other network.

Pool nodes and extenders are also described in the U.S. Patent Publication No. 2013 0058250. In the present application pool nodes and extenders are also referred to as service nodes and gateways respectively.

The multi tenant site in some embodiments is a data center that serves several tenants. As shown the multi tenant site has two network controllers and four managed forwarding elements and seven tenant machines for tenants A and B. The network controllers and manage the managed forwarding elements by generating flow entries that define functionality of the managed forwarding elements and then sending the flow entries to the managed forwarding elements. In particular the network controller manages the managed forwarding elements and and the network controller manages the managed forwarding elements and .

The managed forwarding elements and function as edge forwarding elements based on the flow entries received from the network controllers that manage these three managed forwarding elements. That is the three managed forwarding elements directly interface with the tenant machines of the tenants A and B to forward data to and from the tenant machines.

The managed forwarding element functions as a pool node based on the flow entries received from the network controller . Specifically the managed forwarding element facilitates data exchange between the managed forwarding elements and and between the managed forwarding elements and . Moreover the managed forwarding element sends out the data from the tenant A s machines in the multi tenant site to the tenant A s machines in the tenant A s site and the data from the tenant B s machines in the multi tenant site to the tenant B s machines in the tenant B s site .

The tenant machines are machines of the tenants A and B. The machines of the same tenant send and receive network data between each other over the network. The machines are referred to as network hosts and are each assigned a network layer host address e.g. IP address . The machines may also be referred to as end systems because the machines are sources and destinations of data or endpoints of datapaths. In some embodiments each of the machines can be a desktop computer a laptop computer a smartphone a virtual machine VM running on a computing device a terminal or any other type of network host.

The tenant machines and of the tenant A are in one address space e.g. an IP prefix and the tenant machines and are in another address space. In some embodiments the network controller instances and configure the managed forwarding elements in such a way that the tenant machines of the tenants A and B can have at least partially overlapping address spaces as will be described further below by reference to . The multi tenant site will also be described in more details further below by reference to .

The tenant A s site of some embodiments is a private data center for the tenant A. As shown the site includes a managed forwarding element a forwarding element and two tenant A s machines and . The managed forwarding element functions as an extender and is managed by the network controller via the managed forwarding element in the multi tenant site . That is the managed forwarding element receives flow entries generated by the network controller from the managed forwarding element . The managed forwarding element forwards data 1 to and from the managed forwarding element in the multi tenant site and 2 to and from the forwarding element which is not managed by the network controllers of the multi tenant site . The forwarding element is an edge forwarding element that interfaces with the machines and to forward data 1 to and from the machines and of the tenant A and 2 to and from the managed forwarding element . The machines and are in the same address space as the machines and of the multi tenant site .

The tenant B s site is a private data center for the tenant B. As shown the site includes a managed forwarding element a forwarding element and two tenant B s machines and . The managed forwarding element functions as an extender configured by the network controller via the managed forwarding element in the multi tenant site . The forwarding element is not managed by the controller and interfaces with the machines and of the tenant B as shown. The machines and are in the same address space as the machines and of the multi tenant site .

The external network of some embodiments is used by the multi tenant site and the tenant sites and to communicate with each other. Specifically the external network utilizes Multiprotocol Label Switching MPLS Virtual Private Networks VPNs that enable the tenants machines of the tenant A in the tenant site and the tenants machines of the tenant B in the tenant site to have least partially overlapping address spaces. MPLS VPNs are described in detail in Rosen et al. BGP MPLS VPNs Network Working Group Informational RFC 2547 March 1999 hereinafter RFC 2547 available at http www.ietforg rfc rfc2547.txt and Rosen et al. BGP MPLS IP Virtual Private Networks VPNs Network Working Group Standards Track RFC 4364 February 2006 hereinafter RFC 4364 available at http www.ietf.org rfc rfc4364.txt. RFC 2547 and RFC 4364 are incorporated herein by reference.

One of the ordinary skill art will realize that the number of and the relationship between the network controllers the managed forwarding elements and the tenant machines in the multi tenant sites and the tenant sites shown in this figure and the figures below are exemplary and other combinations of network controllers managed forwarding elements and tenant machines are possible.

Several more detailed embodiments are described below. First Section I describes implementing the logical networks of some embodiments over several managed forwarding elements. Section II then describes extending the logical networks of some embodiments across layer 3 VPNs. Next Section III describes configuring service nodes and remote gateways by network controllers to effectuate the extension of the logical networks. Section IV follows with a description of a use case. Finally Section V describes an electronic system with which some embodiments of the invention are implemented.

The following section will describe implementation of logical forwarding elements of a tenant in a multi tenant site. In the present application forwarding elements and machines may be referred to as network elements. In addition a network that is managed by one or more network controllers may be referred to as a managed network in the present application. In some embodiments the managed network includes only managed forwarding elements e.g. forwarding elements that are controlled by one or more network controllers while in other embodiments the managed network includes managed forwarding elements as well as unmanaged forwarding elements e.g. forwarding elements that are not controlled by a network controller .

The physical implementation is the same as the multi tenant site illustrated in except that the physical implementation does not show the network controllers and . The managed forwarding element directly interfaces with the machines and of the tenant A and the machine of the tenant B and forwards data to and from these three machines. More specifically the managed forwarding element of some embodiments is configured to use a managed port not shown of the managed forwarding element for each of the machines to exchange data with the machine through the managed port. The managed forwarding element directly interfaces with the machine of the tenant B and forwards data to and from the machine . The managed forwarding element directly interfaces with the machine of the tenant A and the machines and of the tenant B and forwards data to and from these three machines.

The managed forwarding element exchanges data with the managed forwarding elements and over the connections established between the managed forwarding elements. In some embodiments these connections are tunnels that are defined using Generic Routing Encapsulation GRE IP Security IPSec Stateless Transport Tunneling STT or other tunneling protocols. In some embodiments the managed forwarding elements are software forwarding elements that run in a host e.g. a computing device a server etc. . The tunnels are established between the hosts that have software forwarding elements run in the host.

As mentioned above the managed forwarding element is a pool node that facilitates data exchange between the edge forwarding elements. For instance when data sent by the machine of the tenant B is destined to the machine of the tenant B the data is forwarded by the managed forwarding element and . This is because the managed forwarding element and that directly interface with the source and destination machines and do not have a connection established and thus need to use the connections to the pool node . In contrast the pool node does not get involved in forwarding data sent by the machine to the machine because the managed forwarding elements and that directly interface with the machines and respectively have a connection established between them as shown. The pool node also forwards data to and from the external network when the data is destined to or originates from the external network.

The logical implementation shows that the multi tenant site includes two logical forwarding elements and of the tenants A and B respectively. The logical forwarding element of the tenant A directly interfaces with the machines and of the tenant A and forwards data to and from these three machines. That is the logical forwarding element is configured to use a logical port not shown of the logical forwarding element for each of the machines and to exchange data with the machine. The logical forwarding element is also configured to use a logical port for the external network to send and receive data to and from the external network.

The network controllers and not shown in this figure configure the managed forwarding elements and to implement the logical forwarding element by mapping the logical ports of the logical forwarding element to the managed ports of the managed forwarding elements and . Specifically the logical port for the external network is mapped to the managed port for the external network of the managed forwarding element the logical ports for the machines and are mapped to the managed ports for the machines and respectively of the managed forwarding element and the logical port for the machine is mapped to the managed port for the machine of the managed forwarding element . Similarly the network controllers configure the managed forwarding elements and to implement the logical forwarding element by mapping the logical ports of the logical forwarding element to the managed ports of the managed forwarding elements and . In such manner the network controllers isolate the tenants A and B in the multi tenant site i.e. the data for one tenant is not forwarded to the other tenant s machines while the two tenants share the managed forwarding elements.

In some embodiments a packet includes a header and a payload. The header includes in some embodiments a set of fields that contains information used for forwarding the packet through a network. Forwarding elements may determine forwarding decisions based on the information contained in the header and may in some cases modify some or all of the header fields. As explained above some embodiments determine forwarding decisions based on flow entries in the forwarding elements forwarding tables.

In some embodiments the processing pipeline may be implemented by flow entries in the managed forwarding elements in the network. For instance some or all of the flow entries are defined such that the packet is processed against the flow entries based on the logical context tag in the packet s header. Therefore in some of these embodiments the managed forwarding elements are configured with such flow entries.

In the first stage of the processing pipeline a logical context lookup is performed on a packet to determine the logical context of the packet. In some embodiments the first stage is performed when the logical forwarding element receives the packet e.g. the packet is initially received by a managed forwarding element in the network that implements the logical forwarding element .

In some embodiments a logical context represents the state of the packet with respect to the logical forwarding element. For example some embodiments of the logical context may specify the logical forwarding element to which the packet belongs the logical port of the logical forwarding element through which the packet was received the logical port of the logical forwarding element through which the packet is to be transmitted the stage of the logical forwarding plane of the logical forwarding element the packet is at etc. Referring to as an example the logical context of some embodiments for packets sent from tenant A s machines specify that the packets are to be processed according to the logical forwarding element which is defined for the tenant A rather than the logical forwarding element which is defined for the tenant B .

Some embodiments determine the logical context of a packet based on the source MAC address or IP address of the packet i.e. the machine from which the packet was sent . Some embodiments perform the logical context lookup based on the source MAC address of the packet and the physical inport i.e. ingress port of the packet i.e. the port of the managed forwarding element through which the packet was received . Other embodiments may use other fields in the packet s header e.g. MPLS header VLAN id etc. for determining the logical context of the packet.

After the logical context of the packet is determined some embodiments store the information that represents the determined logical context in one or more fields of the packet s header. These fields may also be referred to as logical context or a logical context tag or a logical context ID. Furthermore the logical context tag may coincide with one or more known header fields e.g. the VLAN id field in some embodiments. As such these embodiments do not utilize the known header field or its accompanying features in the manner that the header field is defined to be used.

In some embodiments when the first hop managed forwarding element i.e. the managed forwarding element that has the physical ingress port for the packet determines the most of the logical context not all of the information that represents the determined logical context is stored in the packet s header. In these embodiments some information gets stored in registers of the first hop managed forwarding element rather than in the fields of the packet s header and some information e.g. determined logical egress port is stored in the packet s header. Therefore in such embodiments non first hop managed forwarding elements performs only part i.e. the third and fourth stages of the processing pipeline .

In the second stage of the processing pipeline logical forwarding lookups are performed on the packets to determine where to route the packet based on the logical forwarding element e.g. the logical port of the logical forwarding element through which to send the packet out through which the packet is being processed. In some embodiments the logical forwarding lookups include a logical ingress ACL lookup for determining access control when the logical forwarding element receives the packet a logical L2 lookup for determining where to route the packet through a layer 2 network and a logical egress ACL lookup for determining access control before the logical forwarding element routes the packet out of the logical forwarding element. Alternatively or in conjunction with the logical L2 lookup some embodiments of the logical forwarding lookups include a logical L3 lookup for determining where to route the packet through a layer three network. These logical lookups are performed based on the logical context tag of the packet in some of these embodiments.

In some embodiments the result of the logical forwarding lookups may include dropping the packet forwarding the packet to one or more logical egress ports of the logical forwarding element or forwarding the packet to a dispatch port of the logical forwarding element. When the logical forwarding lookups determines that the packet is to be routed to the dispatch port of the logical forwarding element some embodiments repeat the logical forwarding lookups until the packet is determined to be either dropped or forwarded to one or more logical egress ports.

Next the third stage of the processing pipeline performs a mapping lookup on the packet. In some embodiments the mapping lookup is a logical to physical mapping lookup that determines the physical port that corresponds to the logical egress port of the logical forwarding element. That is the mapping lookup determines one or more ports of one or more managed forwarding elements that correspond to the logical egress port of the logical forwarding element through which the packet is to be sent out. For instance if the packet is a broadcast packet or a multicast packet the third stage of some embodiments determines the ports of the managed forwarding elements that correspond to the logical egress ports of the logical forwarding element through which the packet is to be broadcasted or multicasted out i.e. the logical ports to which the intended recipients of the packet is coupled . If the packet is a unicast packet the third stage determines a port of a managed forwarding element that corresponds to the logical egress port of the logical forwarding element through which the packet is to be sent out i.e. the logical port to which the intended recipient of the packet is coupled . In some embodiments of the third stage the mapping lookups are performed based on the logical context tag of the packet.

At the fourth stage of the processing pipeline a physical lookup is performed. The physical lookup of some embodiments determines operations for forwarding the packet to the physical port s that corresponds to the logical egress port s that was determined in the third stage . For example the physical lookup of some embodiments determines one or more ports of the managed forwarding element on which the processing pipeline is being performed through which to send the packet out in order for the packet to reach the physical port s determined in the third stage . This way the managed forwarding elements can route the packet along the correct path in the network for the packet to reach the determined physical port s that corresponds to the logical egress port s .

Some embodiments remove the logical context tag after the fourth stage is completed in order to return the packet to its original state before the packet was processed by the processing pipeline .

As mentioned above in some embodiments the processing pipeline is performed by each managed forwarding element in the managed network that is used to implement the logical forwarding element. In some embodiments some of the managed forwarding elements perform only a portion of the processing pipeline . For example in some embodiments the managed forwarding element that initially receives the packet may perform the first fourth stages and the remaining managed forwarding elements that subsequently receive the packet only perform the first third and fourth stages and .

The following section will describe extending the logical networks defined for one or more tenants in a multi tenant site across a WAN and into the tenants private sites that may share overlapping address spaces.

Several problems arise when the multi tenant site is connected to the tenants sites because the machines of different tenants may share the same address space. First in some embodiments the pool node in the multi tenant site has to be able to establish VPN tunnels to addresses that are not unique. For instance the extenders and may have identical IP prefixes even though the extenders are in different tenant sites because the tenant sites share the same address space. This problem is resolved by MPLS VPN described in above incorporated RFC 4364. In order to make the addresses used by a tenant s site globally unique the external network of some embodiments that utilizes the MPLS VPN technology uses a tenant specific identifier e.g. a Route Distinguisher described in RFC 4364 and the IP addresses of the tunnel endpoints. This combination of the tenant specific identifier and the IP addresses allows the pool node to distinguish between the extenders and establish a tunnel to an extender in the intended tenant site. As shown the external network includes a provider edge PE router which includes two VPN Routing and Forwarding tables VRFs and . The provider that maintains the external network associates the VRFs and with the tenant specific identifiers of the tenants A and B sites and respectively so that the PE router can route the data to and from the sites and .

Having resolved the first problem gives a rise to a second problem which is connecting the pool node in the multi tenant site to a correct VRF in the PE router interfacing with the multi tenant site so that a particular tenant s data is forwarded to and from the end system in the particular tenant s site. The network controllers address this problem by establishing a virtualized physical link between the pool node and the PE router. Different embodiments use different techniques to virtualize the link. For instance in some embodiments the link is an Ethernet link that supports VLAN tags and each VLAN tag can represent a virtual interface VIF to the PE router. In other embodiments the network controllers configure the pool node to connect to the PE router over an IP network and to build a GRE tunnel to each VRF. As shown the managed forwarding element which is configured by the network controller to function as a pool node creates the VIFs and in the physical network interface to connect to the VRFs and respectively.

In some embodiments the network controllers address this second problem without virtualizing the physical link between the pool node and the PE router. Instead the network controllers configure the pool node to use one physical network interface per one VRF of the PE router to connect to the VRF.

As described above by reference to the logical forwarding element of the tenant A in the multi tenant site forwards data to and from the external network through the logical port to the external network. The network controller maps this logical port not shown to the VIF so that the data from the machines and can be forwarded to the machines and not shown in this figure in the tenant site and vice versa. Similarly the network controller maps the logical port not shown for the external network of the logical forwarding element to the VIF so that the data from the machines and can be forwarded to the machines and in the tenant site of the tenant B and vice versa. The data exchange over the VIFs and are further described below by reference to below.

In addition to the PE router that was illustrated in the external network illustrated in includes PE routers and that interface with the tenant sites and of the tenants A an B respectively. The VRFs in the PE routers and are not depicted in this figure for simplicity of the illustration. The tenant sites and in this figure have Customer Edge CE routers and respectively. CE routers are described in RFC 4364.

Forwarding of data e.g. a data packet for the tenant A in the outgoing direction will now be described. At the encircled the managed forwarding element receives the data packet . The data packet is from a tenant A s machine not shown in the multi tenant site and is destined to a tenant A s machine not shown in the tenant A s site . The data packet of some embodiments has header fields and the logical context. As mentioned above the logical context of some embodiments for packets sent from tenant A s machines specify that the packets are to be processed according to the logical forwarding element of the tenant A.

The managed forwarding element looks at the logical context of the data packet and determines that the data packet belongs to the tenant A. The logical context of the packet indicates that the logical egress port of the logical forwarding element is for a machine of the tenant A that is in the tenant A s site . Based on this information the managed forwarding element maps this logical egress port to a port of the managed forwarding element the extender at the tenant A s site . Therefore the managed forwarding element determines that the physical egress port of the managed forwarding element for this packet is the VIF . Likewise the physical egress port of the managed forwarding element for a packet from any machine of the tenant A in the multi tenant site is the VIF when the packet is destined to a tenant A s machine in the tenant A s site .

At the encircled the managed forwarding element then sends the data packet out of the VIF through a tunnel e.g. an IPsec tunnel established with the managed forwarding element which functions as the extender at the tenant A s site . At this point the data packet has additional headers for the tunnel and the VIF. As shown a tunnel header encapsulates the packet and the VIF header encapsulates the tunnel header and the packet . The VIF header includes an identifier for identifying the VIF e.g. a VLAN tag or a GRE tunnel header depending on the kind of VIF that the managed forwarding element is configured to create.

The packet then reaches the PE router . The PE router looks at the VIF header and determines that the VRF for the tenant A should be used. At the encircled the PE router removes the VIF header from the packet because the VIF header is only needed to get to the PE router . Being a PE router of a network that employs the MPLS VPN technology the PE router wraps the packet with an MPLS header . The MPLS header directs the packet from one forwarding element to the next forwarding element based on short path labels rather than long network addresses avoiding complex lookups and forwarding tables. These labels identify paths between the PE router to the PE router that interfaces with the tenant A s site . Accordingly the packet with the MPLS header gets forwarded by the forwarding elements not shown in the external network to the PE router .

At the encircled the PE router removes the MPLS header from the packet because the MPLS header is useful for the packet to reach the PE router . The PE router then forwards the packet to the CE router . At the encircled the CE router forwards the packet to the managed forwarding element using the address e.g. IP address specified in the tunnel header . This address is the address of the managed forwarding element or of the host in which the managed forwarding element runs . At the encircled the managed forwarding element the extender removes the tunnel header and the logical context and forwards the packet towards the destination machine not shown . At this point the packet still has other header s and will be forwarded based on the information included in the remaining headers.

Forwarding of data e.g. a data packet for the tenant A in the incoming direction will now be described. The packet then has an address an IP address of a tenant A s machine as the destination address of the packet. At the encircled the managed forwarding element receives the packet from the tenant A s machine in the tenant A s site . At the encircled the packet does not have the logical context yet. The managed forwarding element as the extender identifies the logical context based on the information included in the header of the packet and attaches the logical context to the packet at the encircled . Also at the encircled the managed forwarding element wraps the packet with the tunnel header to send the packet to the pool node through the tunnel that terminates at the pool node. The tunnel header of the packet being sent in the incoming direction has the address of the managed forwarding element as the endpoint of the tunnel. At the encircled the CE router then forwards the packet to the PE router according to the information included in the tunnel header .

The packet reaches the PE router . At the encircled the PE router looks at the destination address of the tunnel header and identifies the multi tenant site as the destination site because the destination address is of the multi tenant site . The PE router prepares the MPLS header so as to send the packet to the PE router because the PE router interfaces with the destination site the multi tenant site . The PE router then wraps the packet with the MPLS header . The packet with the MPLS header then gets forwarded by the forwarding elements not shown in the external network to the PE router .

Once the packet reaches the PE router the PE router examines the MPLS header which indicates that the packet has come from the tenant A s site . The PE router identifies that the VRF should be used because the VRF is associated with the tenant A. The VRF directs the PE router to send the packet to the virtualized physical link to the multi tenant site . At the encircled the PE router removes the MPLS header and attaches the VIF header to send the packet to the multi tenant site .

Once the packet reaches the PE router the PE router determines that the VRF should be used based on the examination of the MPLS header . The VRF directs the PE router to send the packet to the virtualized physical link to the multi tenant site . At the encircled the PE router removes the MPLS header and attaches the VIF header to send the packet to the multi tenant site .

The managed forwarding element receives the packet and identifies that the packet belongs to the tenant A because the packet comes through the VIF and has tenant A s logical context. The managed forwarding element also looks at the logical context of the packet and identifies the destination machine of the packet. At the encircled the managed forwarding element removes the VIF header and the tunnel header and sends the packet to the identified destination machine not shown of the packet.

It is to be noted that the VIF header may not be needed by a packet in both directions of the data exchange between the machine of the tenant A in the multi tenant site and the machine of the tenant A in the tenant A s site in some embodiments. In these embodiments the managed forwarding element uses physical network interfaces instead of creating virtualized links over a single physical link to the PE router .

In some embodiments the pool node in the multi tenant site does not establish a tunnel to the extender in the tenant s site. In such embodiments the PE router interfacing with the multi tenant site looks at the logical context of the packet traveling in the outgoing direction and identifies the destination address of the packet from the logical context because the logical context of the packet in some embodiments includes the destination address. Therefore the managed forwarding elements and do not wrap the packet with the tunnel header .

Moreover there may be other headers or header fields that are attached to and removed from the packet as the packet is forwarded in either of the outgoing and incoming directions e.g. to send packets from one forwarding element to another forwarding element in the external network . These headers or header fields are not depicted in for simplicity of illustration. Also the headers that are depicted in this figure may get modified as the packet travels in either direction but these modifications are not described nor depicted in the figure for simplicity of discussion and illustration.

The process begins by receiving at a packet from within the multi tenant site. For instance the pool node may receive the packet from a managed forwarding element that is an edge forwarding element interfacing with the source machine of the packet. The packet as received at the pool node has a logical context that the edge forwarding element has identified and attached to the packet.

Next the process determines at whether the packet s destination is within the multi tenant site. In some embodiments the process makes this determination based on the logical context of the packet because the logical context of the packet indicates the logical egress port of the logical forwarding element through which the packet should exit. The process identifies the physical port to which the logical egress port is mapped. When the physical port is of a managed forwarding element that is within the multi tenant site the process determines that the packet s destination is within the multi tenant site. Otherwise the process determines that the packet s destination is not within the multi tenant site.

When the process determines at that the packet s destination is not within the multi tenant site the process proceeds to which will be described further below. Otherwise the process forwards at the packet towards the destination of the packet within the multi tenant site.

When the process determines at that the packet s destination is not within the multi tenant site the process identifies at the tenant to which the packet belongs. In some embodiments the process identifies the tenant based on the logical context of the packet which indicates the tenant for which the logical forwarding element forwards the packet.

The process then identifies at a VIF through which to send the packet out to the PE router that interfaces with the multi tenant site. As mentioned above a VIF of the pool node is created to send a particular tenant s data to a particular VRF for the particular tenant in the PE router. Thus the process identifies the VIF through which to send the packet based on the identified at tenant.

Next the process forwards at the packet to the PE router through the identified at VIF. In some embodiments the process attaches a tunnel header to the packet to send the packet over the tunnel established between the pool node in the multi tenant site and the extender in the remote site of the tenant. The process also attaches the VIF header e.g. a VLAN tag a GRE tunnel header etc. to the outgoing packet. The process then ends.

The process begins by receiving at a packet from an external network. In some embodiments the process recognizes that the packet is an incoming packet when the packet is received through a virtual interface that the pool node has established to connect to a PE router that interfaces with the multi tenant site.

Next the process identifies at a tenant to which the incoming packet belongs. In some cases the packet has a logical context attached to the packet by the extender in the tenant s remote site. In these cases the process identifies the tenant based on the information included in the logical context after removing any additional encapsulations such as the VIF header and the tunnel header for the tunnel between the pool node and the extender. In other situations the process identifies and attaches a logical context when the packet does not originate from the tenant s remote site. In these situations the process identifies the tenant based on the information included in the header of the packet.

The process then identifies at the destination of the packet based on the logical context or the header of the packet. In some embodiments the process identifies the logical forwarding element of the identified at tenant and then identifies the logical egress port of the logical forwarding element. The process then identifies the physical port to which the identified logical egress port is mapped.

Next the process forwards at the packet towards the destination i.e. the edge forwarding element that has the physical port to which the logical port is mapped . For instance the process may forward the packet to the edge forwarding element or another pool node. The process then ends.

As shown the multi tenant site includes managed forwarding elements and . The managed forwarding element is configured to function as a pool node. The managed forwarding elements and implement two logical forwarding elements and . The machines of the tenant C with which the managed forwarding elements and directly interface are not depicted in this figure for simplicity of illustration.

The logical forwarding element of the tenant C shares the same address space e.g. an identical IP prefix with the remote site of the tenant C as shown. Likewise the logical forwarding element of the tenant C shares the same address space with the remote site of the tenant C.

The network controller of some embodiments creates a VIF in the physical network interface for reaching a VRF for the tenant C in a PE router that interfaces with the multi tenant site . The network controller maps a logical port of the logical forwarding element for the external network to the VIF because the logical forwarding element is of the tenant C and the VIF connects to the VRF which is for the tenant C. For the similar reason the network controller maps a logical port of the logical forwarding element for the external network to the VIF .

The network controller also configures the managed forwarding element to establish a tunnel to each of the remote sites and of the tenant C so that the tunnel headers for these tunnels can be used by the PE router to forward data from either of the logical forwarding element of the user to the correct remote site of the tenant C.

When receiving an outgoing packet that originates from a machine of the tenant C that interfaces with the logical forwarding element the managed forwarding element wraps the packet with a tunnel header for the tunnel established between the pool node the managed forwarding element of the multi tenant site and the extender the managed forwarding element of the tenant C s site . This is because the logical forwarding element and the tenant C s site share the same address space as indicated by the IP prefixes 2.1.1.1 24.

For the similar reason when receiving an outgoing packet that originates from a machine of the tenant C that interfaces with the logical forwarding element the managed forwarding element wraps the packet with a tunnel header for the tunnel established between the managed forwarding element and a managed forwarding element of the remote site . In this manner the VIF and thus the VRF forms a one to many relationship with the logical forwarding elements and of the tenant C.

When an outgoing packet originating from a machine of the tenant C at the multi tenant site reaches the PE router the PE router looks up the VRF using the address e.g. an IP address included in the tunnel header of the outgoing packet. The PE router prepares the MPLS header to attach to the packet based on the address included in the tunnel header of the outgoing packet. The PE router can identify one of the remote sites and as the destination site based on the address included in the tunnel header. The PE router attaches an MPLS header to the packet so that the forwarding elements not shown in the external network that employs the MPLS VPN technology to forward the packet to a PE router not shown that interfaces with the intended remote site of the tenant C.

Conversely when the incoming packet originating from a machine in either of the remote sites and of the tenant C reaches the managed forwarding element through the VIF the managed forwarding element identifies the intended logical forwarding element of the tenant C in the multi tenant site using the tunnel header. This is because this tunnel header identifies the tenant site of C that the packet came from.

As shown in some embodiments the remote site of the tenant C has two address spaces e.g. two IP prefixes 2.1.1.1 24 and 2.1.2.1 24 that the logical forwarding elements and in the multi tenant site also have. The relationship between the VRF in the PE router and the logical forwarding elements is still one to many. However because there is only one site of tenant C on the other side of the network and thus there is only one tunnel the tunnel header does not provide much information to distinguish between the data traffic to and from the logical forwarding elements and .

For an incoming packet originating from a machine not shown in the remote site the managed forwarding element that is configured to function as an extender in the tenant C s site identifies and attaches a logical context to the incoming packet. The managed forwarding element of some embodiments specifies in the logical context an identifier e.g. a VLAN tag for specifying which of the logical forwarding elements in the multi tenant site should handle the packet when the packet reaches the multi tenant site. When the packet reaches the managed forwarding element which is the pool node in the multi tenant site the managed forwarding element identifies the logical forwarding element to which to send the packet using the identifier in the logical context of the packet.

The process begins by receiving at a packet from within the multi tenant site. For instance the packet may come from a managed forwarding element that is an edge forwarding element interfacing with the source machine of the packet. The packet has a logical context that the edge forwarding element has identified and attached to the packet. The logical context indicates that the packet should exit the multi tenant site through a logical port of the logical forwarding element for the external network.

Next the process determines at whether the packet s destination is within the multi tenant site. In some embodiments the process looks at the logical context and makes this determination based on the logical context of the packet because the logical context of the packet indicates the logical egress port of the logical forwarding element through which the packet should exit. The process identifies the physical port to which the logical egress port is mapped. When the physical port is of a managed forwarding element that is within the multi tenant site the process determines that the packet s destination is within the multi tenant site. Otherwise the process determines that the packet s destination is not within the multi tenant site.

When the process determines at that the packet s destination is not within the multi tenant site the process proceeds to which will be described further below. Otherwise the process forwards at the packet towards the destination of the packet within the multi tenant site.

When the process determines at that the packet s destination is not within the multi tenant site the process identifies at the tenant to which the packet belongs. In some embodiments the process identifies the tenant based on the logical context of the packet which indicates the tenant for which the logical forwarding element forwards the packet.

The process then identifies at a VIF through which to send the packet out to the PE router that interfaces with the multi tenant site. As mentioned above a VIF of the pool node is created to send a particular tenant s data to a particular VRF for the particular tenant in the PE router. Thus the process identifies the VIF through which to send the packet based on the identified at tenant.

Next the process identifies at the address space to which the source machine of the packet belongs. In some embodiments the process identifies the address space using the logical context of the packet which indicates the address space e.g. an IP prefix to which the packet s source machine belongs. For some embodiments in which the identified at tenant has more than one remote site the process uses the identification of the address space i.e. the identification of logical forwarding element that handles the addresses in the address space to identify and attach a proper tunnel header to send the packet to the intended remote site of the tenant.

In other embodiments in which the identified at tenant has one remote site sharing the address spaces with all of the logical forwarding elements of the tenant in the multi tenant site the process puts the identification of the address space e.g. a VLAN tag of the tenant in the logical context of the packet so that the PE router interfacing with the multi tenant site can identify the intended remote site of the tenant based on the identification of the address space included in the logical context.

The process then forwards at the packet to the PE router through the identified at VIF. In some embodiments the process attaches a tunnel header to the packet to send the packet over the tunnel established between the pool node in the multi tenant site and the extender in the remote site of the tenant. The process also attaches the VIF header to the outgoing packet. The process then ends.

The process begins by receiving at a packet from an external network. In some embodiments the process recognizes that the packet is an incoming packet when the packet is received through a virtual interface that the pool node has established to connect to a PE router that interfaces with the multi tenant site.

Next the process identifies at a tenant to which the incoming packet belongs. In some cases the packet has a logical context attached to the packet by the extender in the tenant s remote site from which the packet originates. In these cases the process identifies the tenant based on the information included in the logical context after removing any additional encapsulations such as the VIF header and the tunnel header for the tunnel between the pool node and the extender. In other situations the process identifies and attaches a logical context when the packet does not originate from the tenant s remote site. In these situations the process identifies the tenant based on the information included in the header of the packet.

The process then identifies at the address space to which the source machine of the packet belongs based on the logical context. As mentioned above for some embodiments in which a tenant has more than one remote site the extender at the remote site from which the packet originates wraps the packet with a tunnel header for the tunnel established between the extender and the pool node. The process in these embodiments identifies the address space based on the tunnel header. For those embodiments in which the tenant has a single remote site that shares the address spaces with all of the logical forwarding elements of the tenant in the multi tenant site the extender specifies an identifier e.g. a VLAN tag in the logical context of the packet for identifying the address space to which the destination machine belongs. In these embodiments the process identifies the address space based on the identifier included in the logical context.

The process then identifies at the destination of the packet based on the logical context or the header of the packet. In some embodiments the process identifies the logical egress port of the identified at logical forwarding element. The process then identifies the physical port to which the identified logical egress port is mapped.

Next the process forwards at the packet towards the destination i.e. the edge forwarding element that has the physical port to which the logical port is mapped . For instance the process may forward the packet to the edge forwarding element or another pool node. The process then ends.

The following section will describe network controllers that configure service nodes and remote extenders to effectuate the extension of logical networks in a multi tenant site into tenants private sites that are remote to the multi tenant site.

This figure illustrates a multi tenant site and a remote site of a tenant. As shown the multi tenant site includes a network controller and a pool node . The remote site includes an extender . This figure also illustrates arrow headed lines and which conceptually indicates the paths of the configuration commands data traversing from the network controller to inside of the extender and inside of the pool node respectively.

The pool node includes an Open vSwitch OVS daemon a proxy daemon a pool node network stack the root bridge patch bridge and a set of NICs . The OVS daemon is also an application that runs in the pool node. The OVS daemon of some embodiments communicates with a network controller in order to process and forward packets that the pool node receives. For example the OVS daemon receives commands from the network controller regarding operations for processing and forwarding packets that the pool node receives. The OVS daemon of some embodiments communicates with the network controller through the OpenFlow protocol. The OpenFlow protocol is a communication protocol for controlling the forwarding plane e.g. forwarding tables of a forwarding element. For instance the OpenFlow protocol provides commands for adding flow entries to removing flow entries from and modifying flow entries in the forwarding element. In some embodiments another type of communication protocol is used.

As shown the OVS daemon includes an OpenFlow protocol module and a flow processor . The OpenFlow protocol module communicates with the network controller through the OpenFlow protocol. For example the OpenFlow protocol module receives configuration information from the network controller for configuring the pool node . Configuration information may include flows that specify rules e.g. flow entries for processing and forwarding packets. When the OpenFlow protocol module receives configuration information from the network controller the OpenFlow protocol module may translate the configuration information into information that the flow processor can understand. In some embodiments the OpenFlow protocol module is a library that the OVS daemon accesses for some or all of the functions described above.

The flow processor manages the rules for processing and forwarding packets. For instance the flow processor stores rules e.g. in a storage medium such as a disc drive that the flow processor receives from the OpenFlow protocol module which the OpenFlow protocol module receives from the network controller . In some embodiments the rules are stored as a set of flow tables that each includes a set of flow entries also referred to collectively as configured flow entries . The flow entries specify operations for processing and or forwarding network data e.g. packets based on forwarding criteria. In addition when the flow processor receives commands from the OpenFlow protocol module to remove rules the flow processor removes the rules.

The proxy daemon is an application that runs in the pool node . The proxy daemon functions as a proxy network controller cluster for the extenders in the remote sites. That is the proxy daemon receives commands from the network controller regarding operations for processing and forwarding packets that the extenders receive. The proxy daemon relays the commands to the extenders through the NICs using the pool node network stack . In some embodiments the proxy daemon communicates with the network controller and the extenders in the remote sites using the OpenFlow protocol. Since the proxy daemon operates like a network controller for the extenders at the remote sites the network controller which actually generates the commands does not have to directly interface with the extenders thereby hiding the IP address of the controller from the extenders.

In some embodiments each NIC in the set of NICs is typical network interface controllers for connecting a computing device to one or more networks and sending and receiving network data e.g. packets over such networks. In addition the set of NICs sends and receives network data from the pool node network stack .

In some embodiments the pool node network stack is an IP network stack that runs on the pool node . Also the pool node network stack processes and routes IP packets that are received from the patch bridge and the set of NICs by utilizing a set of forwarding tables not shown to forward the packets.

In some embodiments the patch bridge stores a set of rules e.g. flow entries that specify operations for processing and forwarding packets. The patch bridge communicates with the OVS daemon in order to process and forward packets that the patch bridge receives. For instance the patch bridge receives commands from the network controller via the OVS daemon related to processing and forwarding of packets that the pool node receives.

As mentioned above a pool node of some embodiments is responsible for processing packets that managed edge forwarding elements in the multi tenant site cannot process. In this example the patch bridge processes and forwards such packets. The patch bridge receives packets from the managed forwarding elements through the set of NICs and the pool node network stack . When the patch bridge receives a packet the patch bridge processes and forwards the packet according to the set of rules stored in the patch bridge . In some cases the patch bridge cannot process a packet e.g. the patch bridge does not have a rule to which the packet matches . In these cases the patch bridge sends the packet to the root bridge for processing.

The root bridge is responsible for a learning function. The root bridge of some embodiments stores a set of tables of learned MAC addresses. The root bridge learns MAC addresses in the typical manner that layer 2 switches learn MAC addresses. For instance when the root bridge does not know a MAC address i.e. a destination MAC address of a packet is not included in the set of tables of learned MAC addresses the root bridge floods all of the ports of the root bridge and records the MAC address of the packet that responds to the flood in the set of tables. Although illustrates a pool node that includes a root bridge some embodiments may not include a root bridge. In some of these embodiments the functions described above are implemented in the patch bridge of the pool node.

As shown in the right portion of the extender includes a kernel and a user space. The user space of the extender includes the OVS daemon . Other applications not shown may be included in the user space of the extender as well. The OVS daemon is an application that runs in the background of the user space of the extender . The OVS daemon of some embodiments communicates with the pool node specifically the proxy daemon of the pool node in order to process and route packets that the extender receives. The OVS daemon is similar to the OVS daemon otherwise.

The OVS daemon includes an OpenFlow protocol module and a flow processor . The OpenFlow protocol module communicates with the proxy daemon through the OpenFlow protocol. The flow processor manages the rules for processing and forwarding packets. For instance the flow processor stores rules e.g. in a storage medium such as a disc drive that the flow processor receives from the OpenFlow protocol module which in some cases the OpenFlow protocol module receives from the proxy daemon . In some embodiments the rules are stored as a set of flow tables that each includes a set of flow entries also referred to collectively as configured flow entries . As noted above flow entries specify operations for processing and or forwarding network data e.g. packets based on forwarding criteria. In addition when the flow processor receives commands from the OpenFlow protocol module to remove rules the flow processor removes the rules.

In some embodiments the flow processor supports different types of rules. For example the flow processor of such embodiments supports wildcard rules and exact match rules. In some embodiments an exact match rule is defined to match against every possible field of a particular set of protocol stacks. A wildcard rule is defined to match against a subset of the possible fields of the particular set of protocol stacks. As such different exact match rules and wildcard rules may be defined for different set of protocol stacks.

The flow processor handles packets for which an integration bridge does not have a matching rule. For example the flow processor receives packets from the integration bridge that does not match any of the rules stored in the integration bridge . In such cases the flow processor matches the packets against the rules stored in the flow processor which include wildcard rules as well as exact match rules.

In some embodiments the flow processor may not have a rule to which the packet matches. In such cases the flow process of some embodiments sends the packet to the proxy daemon through the OpenFlow protocol module . However in other cases the flow processor may have received from the proxy daemon a catchall rule that drops the packet when a rule to which the packet matches does not exist in the flow processor .

After the flow processor generates the exact match rule based on the wildcard rule to which the packet originally matched the flow processor sends the generated exact match rule and the packet to the integration bridge for the integration bridge to process. This way when the integration bridge receives a similar packet that matches the generated exact match rule the packet will be matched against the generated exact match rule in the integration bridge so the flow processor does not have to process the packet.

In some embodiments the OVS kernel module includes a PIF bridge for each NIC. For instance if the extender includes four NICs the OVS kernel module would include four PIF bridges for each of the four NICs in the extender . In other embodiments a PIF bridge in the OVS kernel module may interact with more than one NIC in the extender .

The PIF bridges and route network data between the extender network stack and network hosts external to the extender i.e. network data received through the NICs and . As shown the PIF bridge routes network data between the extender network stack and the NIC and the PIF bridge routes network data between the extender network stack and the NIC . The PIF bridges and of some embodiments perform standard layer 2 packet learning and forwarding.

In some embodiments the extender provides and controls the PIF bridges and . However the network controller may in some embodiments control the PIF bridges and via the proxy daemon and the OVS daemon in order to implement various functionalities e.g. quality of service QoS of the software forwarding element. More details on the flow processor and the OVS kernel module of an extender are described in the U.S. Patent Publication No. 2013 0058250 which is incorporated herein by reference.

The process begins by generating at configuration data e.g. flow entries for the pool node and the extender. In some embodiments the process generates the configuration data based on the information that is gathered from the pool node and the extender. For instance the process receives information about the host in which the extender runs and generates configuration data for the pool node for directing the pool node to establish a tunnel with the extender at the remote site. In some embodiments the configuration data are formatted to conform to certain communication protocol e.g. OpenFlow so that the pool node and the extender that support the protocol can understand and process the data. Formatting the configuration data are described in greater detail below in Subsection II.B.

Next the process sends at the generated data for the pool node to the pool node and the generated data for the extender to the pool node in order to send the configuration data for the extender to the extender via the pool node. In some embodiments the process sends the configuration data to the pool node without separating the data for the pool node from the data for the extender. In some such embodiments the process puts identifiers in the data that indicate the network elements that should take a particular piece of data. These identifiers are capable of specifying 1 whether the data is for a pool node or an extender and 2 which extender in which remote site should receive the data.

In other embodiments the process separates the data for the pool node from the data for the extender and sends the separated data to the pool node in separate communication channels. In some such embodiments the pool node runs two daemons e.g. the proxy daemon and the OVS daemon described above by reference to for receiving and processing the data for the pool node and the data for the extender. In these embodiments the network controller also puts identifiers in the data for an extender so that the pool node can determine the extender and the remote site to which the pool node should send the data. The pool node then configures the pool node based on the received data for the pool node and relays the received data for the extender to the extender. The process then ends.

Next the process determines at whether the received configuration data is for configuring the pool node or for configuring the extender. In some embodiments the network controller sends the configuration data without separating the data for the pool node from the data for the extender. In some such embodiments the network controller puts identifiers in the data that indicate the network elements that should take a particular piece of data. These identifiers are capable of specifying 1 whether the data is for a pool node or an extender and 2 which extender in which remote site should receive the data. In these embodiments the process determines whether the received configuration data is for a pool node or for an extender based on the identifiers included in the received configuration data.

In some embodiments the network controller separates the data for the pool node from the data for an extender and sends the separated data to the pool node in separate communication channels. In some such embodiments the pool node runs two daemons e.g. the proxy daemon and the OVS daemon described above by reference to for receiving and processing the data for the pool node and the data for the extender. In these embodiments the network controller also puts identifiers in the data for an extender so that the pool node can determine the extender to which the pool node should send the data. The process in these embodiments determines whether the received configuration data is for a pool node or for an extender based on the daemon that received the configuration data.

When the process determines at that the received configuration data is not for an extender the process proceeds to to configure the pool node based on the received configuration data. Otherwise the process proceeds to to send the configuration data to an extender. The process of some embodiments identifies the extender to which to send the configuration data based on the identifiers that the network controller has included in the configuration data. The process sends the configuration data to the identified extender. The process then ends.

In some embodiments a single layer of network controller either a single network controller or a network controller cluster communicates directly with the managed forwarding elements e.g. the edge forwarding elements the pool node s and the extender s . However in other embodiments several layers of network controllers process and generate flow entries in the network control system. For example in some embodiments each logical datapath set i.e. each logical forwarding element is assigned to a single logical higher level network controller. This logical controller receives logical control plane LCP data and converts the LCP data into logical forwarding plane LFP data. The logical controller also subsequently converts the LFP data into universal physical control plane UPCP data.

In some embodiments the UPCP data is published by the logical controller to a second level of network controller referred to as a physical controller . In some embodiments different physical controllers manage different physical forwarding elements e.g. edge forwarding elements pool nodes gateways etc. . Furthermore the physical controller of some embodiments converts the UPCP data into customized physical control plane CPCP data. In other embodiments however the physical controller passes the UPCP data to a conversion mechanism operating at the forwarding element itself referred to as a chassis controller .

The LCP data in some embodiments describes the logical network topology e.g. as a set of bindings that map addresses to logical ports . In some embodiments the LCP data is expressed as a set of database table records e.g. in the n Log language . An entry in the control plane describing the attachment of a particular virtual machine to the network might state that a particular MAC address or IP address is located at a particular logical port of a particular logical switch. In some embodiments the LFP data derived from the LCP data consists of flow entries described at a logical level. That is a flow entry might specify that if the destination of a packet matches a particular IP address to forward the packet to the logical port to which the IP address is bound.

The translation from LFP to physical control plane PCP data in some embodiments adds a layer to the flow entries that enables a managed forwarding element provisioned with the flow entries to convert packets received at a physical layer port e.g. a virtual interface into the logical domain and perform forwarding in this logical domain. That is while traffic packets are sent and received within the network at the physical layer the forwarding decisions are made according to the logical network topology entered by the user. The conversion from the LFP to the PCP enables this aspect of the network in some embodiments.

As mentioned the logical controller converts the LFP data into the UPCP which is subsequently converted to CPCP data. The UPCP data of some embodiments is a data plane that enables the control system of some embodiments to scale even when it contains a large number of managed forwarding elements e.g. thousands to implement a logical datapath set. The UPCP abstracts common characteristics of different managed forwarding elements in order to express PCP data without considering differences in the managed forwarding elements and or location specifics of the managed forwarding elements. The UPCP to CPCP translation involves a customization of various data in the flow entries. While the UPCP entries are applicable to any managed forwarding element because the entries include generic abstractions for any data that is different for different forwarding elements the CPCP entries include substituted data specific to the particular managed forwarding element to which the entry will be sent e.g. specific tunneling protocols virtual and physical interface etc. .

The API provides an interface for translating input into the control plane input tables . This API may be used by various types of management tools with which a user e.g. a network administrator for a particular tenant can view and or modify the state of a logical network in this case the logical network that spans both the data center and the tenant s remote site . In some embodiments the management tools provide a user interface such as a graphical user interface that allows a visual configuration of port bindings ACL rules etc. e.g. through a web browser . Alternatively or in conjunction with the graphical user interface some embodiments provide the user with a command line tool or other type of user interface.

Based on the information received through the API as well as updates to the network state received from the managed forwarding elements not shown the control application generates the input tables . The input tables represent the state of the logical forwarding elements managed by the user in some embodiments. As shown in this figure some of the input tables include the bindings of IP addresses with logical ports of the logical forwarding element. In some embodiments the input tables to the LCP to LFP conversion may include bindings of MAC addresses with logical ports for L2 logical forwarding as well as ACL rules set by the user. In this case the logical Port Z is associated with the remote site machines which include a set of IP addresses B. Because multiple different machines at the remote site are associated with a single port of the logical forwarding element the port is bound to a set of IP addresses.

The rules engine of some embodiments performs various combinations of database operations on different sets of input tables to populate and or modify different sets of output tables . As described in further detail in U.S. Patent Publication 2013 0058350 incorporated herein by reference in some embodiments the rules engine is an n Log table mapping engine that maps a first set of n Log tables into a second set of n Log tables. The output tables populated by the rules engine include logical forwarding plane lookups e.g. mapping the set of IP addresses to a destination output port .

The publisher is also described in further detail in U.S. Patent Publication 2013 0058350 and publishes or sends the output tables to the virtualization application in order for this application to use the output tables among its input tables. In some embodiments the publisher also outputs the tables to a data structure e.g. a relational database that stores network state information.

The virtualization application receives the output tables LFP data of the control application and converts this data to UPCP data. As shown the virtualization application includes a subscriber input tables a rules engine output tables and a publisher . The subscriber of some embodiments is responsible for retrieving tables published by the publisher . In some embodiments the subscriber retrieves these tables from the same data structure to which the publisher stores the table information. In other embodiments a change in the tables is detected by the conversion modules in order to initiate the processing.

The input tables include in some embodiments at least some of the output tables in addition to other tables. As shown in addition to the logical forwarding plane data generated by the control application the input tables include additional port binding information matching logical ports with the universally unique identifier UUID of particular source or destination managed forwarding elements .

In some embodiments the rules engine is the same as the rules engine . That is the control application and the virtualization application actually use the same rules engine in some embodiments. As indicated the rules engine performs various combinations of database operations on different sets of input tables to populate and or modify different sets of output tables . In some embodiments the rules engine is an n Log table mapping engine that maps a first set of n Log tables into a second set of n Log tables.

The output tables populated by the rules engine include different lookup entries for different managed forwarding elements. For instance in some embodiments that perform all logical processing at the first hop i.e. the edge forwarding element the physical control plane entries implementing the logical forwarding element will be sent to the edge forwarding elements that might receive a packet destined for one of the machines at the remote tenant site without logical context and need to be able to perform logical forwarding to send the packet to the remote tenant site. Thus the output tables include an entry mapping the set of IP addresses B to the logical egress port Z when the particular logical datapath set for the tenant is matched. In addition the UPCP will include entries for mapping the logical egress port to a physical port through which to send the packet with port abstractions so that the same entry can be sent to numerous edge forwarding elements .

The output tables also include entries for the non first hop forwarding elements such as the pool nodes and the remote gateways. In this case two UPCP entries are generated for the pool node in order to send packets to the extender. Specifically as shown the UPCP entries include an entry to send via a tunnel to Ext when the logical egress port matches Port Z and then if sending via a tunnel to Ext to send to a particular physical interface. Because these are UPCP entries the particular data about the tunnel and the physical interface are not filled in but are instead left as abstractions. A UUID in some embodiments discovered from the remote gateway is used in the input tables and then added to the flow entries in the output tables to identify the tunnel endpoint. Thus even if multiple extenders of multiple tenants have the same tunnel endpoint IP addresses the UUID serves to disambiguate the flows.

The publisher is similar to the publisher in some embodiments. The publisher publishes and or sends the output tables to the physical controllers. In some cases certain flow entries e.g. the entry shown for the edge forwarding elements may be sent to multiple different physical controllers while other entries are sent to only one physical controller. In some embodiments the publisher outputs the tables to a data structure e.g. a relational database that stores network state information.

The input tables include in some embodiments at least some of the output tables in addition to other tables. In addition to the UPCP data generated by the virtualization application the input tables include tunnel information that matches the UUID Ext to a tunnel IP a virtual interface in this case using the VLAN Q and a physical port P of the pool node. Because this conversion is performed at either the physical controller that manages the extender or at the chassis controller at the extender itself the input tables may not include the entry for performing logical forwarding. In addition this entry is not modified by the UPCP to CPCP conversion because no customization information e.g. physical ports tunnel endpoints etc. is required for the entry.

In some embodiments the rules engine is the same type of engine as that used by the control and virtualization applications at the logical controller. As indicated the rules engine performs various combinations of database operations on the different sets of input tables to populate and or modify different sets of output tables . In some embodiments the rules engine is an n Log table mapping engine that maps a first set of n Log tables into a second set of n Log tables.

The output tables populated by the rules engine include the customized physical control plane entries. As illustrated the physical control plane entries now include the customized information. Specifically the first entry indicates that if the egress context specifies the logical port Z to take the action of encapsulating the packet with the tunnel IP address and subsequently to add the VLAN tag Q. As described above the VLAN tag or e.g. GRE information or other virtual interface tagging enables packets for multiple different tenants to be sent to different VRFs at the same provider edge router. Furthermore the CPCP entries map the VLAN tag Q to a particular physical port of the pool node i.e. the physical interface virtualized by the VLANs.

One of ordinary skill in the art will recognize that the input and output tables shown in this figure are simplified conceptual representations of the actual tables which are generated in a database language appropriate for the rules engine e.g. n Log and may provide additional information to that shown. Furthermore different embodiments will use different sets of tables. For instance in addition to the entries for outgoing packets over the tunnel corresponding entries for incoming packets received over the tunnel and VLAN will be required at the pool node. In addition similar entries for establishing the tunnel though not the VLAN at the extender are required.

The following section will describe extending logical networks in a multi tenant site into another multi tenant site. illustrates how the data that originates from a machine of a particular tenant in a first multi tenant site is forwarded to a machine of the particular tenant in a second multi tenant site. Specifically this figures illustrates data exchange between a machine not shown of tenant E in a multi tenant site and a machine not shown of the tenant E in a multi tenant site in both directions of the exchange.

In addition to the two multi tenant sites and this figure illustrates an external network that employs the MPLS VPN technology. Also the top portion of the figure shows a data packet as the packet is forwarded through different parts of the network. The different parts of the network are depicted using encircled numbers .

Each of the multi tenant sites and is similar to the multi tenant site described above in that the managed forwarding elements in the multi tenant site or implement several logical forwarding elements. Moreover the machines of a tenant in both multi tenant sites are in the same address space. Because the data exchange is between two multi tenant sites rather than between a multi tenant site and a remote private site the tunnels are established between two pool nodes rather than between a pool node and an extender in some embodiments. Also each of the pool nodes creates VIFs for connecting to the VRFs for the tenants.

Forwarding of the packet for the tenant E from a source machine not shown in the multi tenant site to the destination machine in the multi tenant site will now be described. Because the forwarding of a packet in the opposite direction will show an identical sequence only the packet traversal in one direction will be described. At the encircled a managed forwarding element receives the data packet . The data packet of some embodiments has tenant E s context which includes the header fields and the logical context of the packet. The managed forwarding element determines that the data packet belongs to the tenant E based on the logical context of the packet. The managed forwarding element also determines that the logical egress port for this packet maps to a physical port of a managed forwarding element in the multi tenant site . The managed forwarding element thus identifies a VIF as the physical port through which the packet should be forwarded out.

At the encircled the managed forwarding element attaches a tunnel header and then a VIF header . A PE router that interfaces with the multi tenant site receives the packet and uses the VRF that is associated with the tenant E. At the encircled the PE router removes the VIF header and attaches an MPLS header so that the forwarding elements not shown in an external network forward the packet to a PE router that interfaces with the multi tenant site .

The PE router removes the MPLS header from the packet and looks at the tunnel header . Based on the information included in the tunnel header the PE router determines that a VRF of the PE router should be used to forward the packet. The VRF directs the PE router to forward the packet to a managed forwarding element which is configured to function as a pool node via a VIF of the managed forwarding element . Thus the PE router attaches a VIF header to the packet at the encircled .

The managed forwarding element receives the packet and identifies that the packet belongs to the tenant E because the packet comes through the VIF . The managed forwarding element also looks at the logical context of the packet and identifies the destination machine of the packet. At the encircled the managed forwarding element removes the VIF header and the tunnel header and sends the packet to the managed forwarding element because the destination machine not shown for the packet is directly interfacing with the managed forwarding element .

In some embodiments the pool nodes of the two multi tenant sites do not establish a tunnel between them. In some such embodiments the PE routers interfacing the multi tenant states will look at the logical context of the packet and identifies the destination of the packet from the logical context of the a packet.

With or without the tunnel between the two pool nodes and a logical forwarding element of the tenant E in the multi tenant site and a logical forwarding element of the tenant E in the multi tenant site are not different logical forwarding elements because both logical forwarding elements handle the same address space. In other words there effectively is one logical forwarding element for the tenant E that is implemented by the managed forwarding elements in both multiple tenant sites and .

In some embodiments a network controller at each multi tenant site configures the pool node in the multi tenant site. The controller does not have to configure a remote extender via the pool node but the network controller in some embodiments communicates with the network controller in the other multi tenant site in order to configure the pool nodes to effectuate the data exchange between the two multi tenant sites.

In some embodiments network controllers and do not use the links established between the two pool nodes and in the two multi tenant sites and for exchanging data traffic. Instead the network controllers and of some embodiments may open a direct communication channel to exchange configuration information.

Instead of having the two network controllers and communicate with each other horizontally some embodiments use a cloud management system as a single point of control to communicate with both of the network controllers and . In other embodiments a higher level network controller provides a higher level control policy and higher level logical datapath set to the network controllers and so that these two controllers implement the policy in their respective sites. Also any communication between the network controllers and takes place through the higher level controller . This high level controller may be operating in either of the two multi tenant sites and or in a third site. Alternatively or conjunctively an administrator for the tenant E may configure the pool nodes and using the network controllers and in some embodiments.

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives RAM chips hard drives EPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the system memory and the permanent storage device .

From these various memory units the processing unit s retrieve instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments.

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash drive etc. as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . From these various memory units the processing unit s retrieve instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices . The output devices display images generated by the electronic system. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD . Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself.

As used in this specification the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification the terms computer readable medium computer readable media and machine readable medium are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. In addition a number of the figures including conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. Furthermore the process could be implemented using several sub processes or as part of a larger macro process.

