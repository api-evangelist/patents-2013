---

title: Network interface card device pass-through with multiple nested hypervisors
abstract: In a data center computing system, multiple nested hypervisors are run, including an outer hypervisor and at least one inner hypervisor running as a virtual machine on top of the outer hypervisor. A guest operating system is run as a virtual machine in the innermost hypervisor. An emulated network interface card device is executed in all hypervisors. An extender component is executed in the outer hypervisor and an extender component is executed in the inner hypervisors such that the extender components in the outer hypervisor and in the inner hypervisors are architecturally cascaded. An interface for the guest operating system is assigned to the emulated network interface card device in each of the outer hypervisor and the inner hypervisors to enable network communications to bypass the outer hypervisor and the inner hypervisors.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09176767&OS=09176767&RS=09176767
owner: Cisco Technology, Inc.
number: 09176767
owner_city: San Jose
owner_country: US
publication_date: 20130411
---
An inner hypervisor running as Virtual Machine VM on top of an outer hypervisor is a powerful kind of software container for a guest Operating System OS running within the inner hypervisor. One use case of this is when the enterprise Information Technology IT administrator wants to reduce the operational cost of the VM by running the VM in the cloud while maintaining the service in the VM available all the time. A solution is for the enterprise IT administrator to live migrate a VM from a server on premises to a server off premises leased from a cloud data center provider. Moreover the server on premises may run a given hypervisor brand whereas the server in the cloud may run a different hypervisor brand.

An instance of the source hypervisor brand used on premises is created as an inner hypervisor VM on the destination hypervisor in the cloud data center forming two nested hypervisor instances . The original VM may then be live migrated from the on premise server to the inner hypervisor VM in the cloud datacenter. This solution is practical because running a generic VM on two nested hypervisors is supported to various degrees in commercially available hypervisors.

In a data center computing system multiple nested hypervisors are run including an outer hypervisor and at least one inner hypervisor running as a virtual machine on top of the outer hypervisor. A guest operating system is run as a virtual machine in the inner hypervisor. An emulated network interface card device is executed in the inner hypervisor and an emulated network interface card device is executed in the outer hypervisor. An extender component is executed in the outer hypervisor and an extender component is executed in the inner hypervisor such that the extender components in the outer hypervisor and the inner hypervisor are architecturally cascaded. Using the extender components in the outer hypervisor and in the inner hypervisor an interface for the guest operating system is assigned to the emulated network interface card device in each of the outer hypervisor and the inner hypervisor to enable network communications between the guest operating system and a network interface card device while bypassing both the outer hypervisor and the inner hypervisor.

Presented herein are techniques to achieve bypass of multiple nested hypervisors in a manner that is completely transparent to entities outside of the virtualized computing environment. Passing a network interface card NIC through a hypervisor to a guest operating system OS is a powerful method to provide improved scalable and predictable system networking performance in terms of both throughput and latency.

There exist today methods to pass a NIC through a single hypervisor which meet enterprise grade standards of high performance networking safety manageability and support for VM migration. These methods involve switching the runtime operational mode of a NIC dynamically between emulation and pass through.

Passing a NIC through two or more nested hypervisors in a safe and manageable way would offer similar or more benefits. The terms bypass and pass through are used herein interchangeably to mean directing network traffic around or passed hypervisors e.g. tow or more nested hypervisors. Within the use cases described above more specific use cases include the following.

The IT administrator has configured the VM to use a NIC in pass through mode on premises because the VM needs high and predictable networking input output I O performance. After the VM has live migrated to the cloud data center the IT administrator wants the VM to continue to use the NIC in pass through mode and interact directly with hardware for network I O in order for the VM to maintain the level of networking I O performance. For example the IT administrator wants the application software running in the guest OS to maintain close to native networking performance similar to when the same guest OS is running on a bare metal server. By contrast if the inner hypervisor VM and or the outer hypervisor is running the NIC in emulation mode that introduces a significant amount of networking performance overhead and it is more difficult to predict because it is more subject to environmental conditions e.g. overall server utilization . As a result maintaining a service level agreement of the kind close to native networking performance after the VM is migrated to the cloud may become impractical.

The workload running in the VM is network bound. After the VM has live migrated to the cloud data center and it is running on the inner hypervisor VM a non trivial amount of the server central processing unit CPU processing power maybe used to run the software device emulation and the software networking stack of both nested hypervisors. This is usually the case when a practical network intensive workload runs in a VM. The service provider may want the VM to use a NIC in pass through mode and interact directly with hardware for network I O in order to free up the CPU processing power on the server otherwise used to run both hypervisors device emulation and networking stack software. That allows a higher VM per server consolidation ratio and in turn lower per VM total cost of ownership for the service provider.

However today passing a NIC through two nested hypervisors is not possible even when the two nested hypervisors are of the same brand. State of the art implementation techniques used to pass a NIC through a single hypervisor cannot be readily used to pass a NIC through multiple nested hypervisors because they have the following drawbacks.

Each hypervisor has its own state of a guest OS interface including network interface identity number a name and metadata and the interface association to a virtual switch within the hypervisor. A guest OS network interface must be connected to the virtual switch in the inner hypervisor which is connected to a NIC in the inner hypervisor. The interface of the virtual switch in the inner hypervisor must be connected to a virtual NIC in the outer hypervisor and so on. As result between the guest OS and the physical network there could be either two independent virtual switches when the interface is in emulation mode or no virtual switch when the interface is in pass through mode. That ties a NIC transition between emulation and pass through mode with virtual network topology change and or re configuration events which creates network operations challenges.

Furthermore each hypervisor flavor brand or type has its own virtual network connectivity implementation often proprietary such as VMware vSphere vSwitch Microsoft Hyper V Virtual Switch Xen Open vSwitch. Furthermore some hypervisors have multiple flavors of network connectivity elements. For example KVM Linux has bridge or media access control MacVTap. MacVTap is a device driver meant to simplify virtualized bridged networking. In each hypervisor flavor the implementation of pass through is very much tied with the base networking connectivity elements. For example VMware VM Direct IO with VMotion is tied to vSphere Distributed Virtual Switch DVS . The proliferation of virtual switches implementations causes challenges in coordinating a NIC transition between pass through and emulation mode in multiple nested hypervisor instances of different types.

Hypervisor implementations of pass through are converging to the Single Root IO Virtualization SR IOV hardware NIC device model. However each hypervisor implementation is tied to a different and mostly proprietary para virtualized NIC device model presented to the guest OS vmxent3 in VMware vSphere hypervisor the synthetic NIC in Microsoft Hyper V VFIO in Linux KVM. These variations make it difficult to use the current hypervisor s form of pass through to achieve multiple hypervisors pass through.

Reference is now made to . is a software functional diagram of a system that is configured to achieve bypass i.e. pass through of multiple nested hypervisors in a manner that is completely transparent to entities outside of the virtualized computing environment. Using these techniques that are described in more detail below there is no need to sustain the system states in which only one hypervisor either the inner or the outer has a NIC in pass through mode. Two macro system states are sufficient either the NIC operates in emulation mode in both hypervisors or the NIC operates in pass through mode in both hypervisors.

The system comprises a cloud data center that is accessible over a network by one or more servers M . Server has a virtual machine VM running thereon as an example. The cloud data center includes nested hypervisors. There is an outer hypervisor installed on a cloud service provider host of the cloud data center and one or more nested inner hypervisors N each running as a VM within its outer hypervisor The hypervisors N are not peers. Instead the outer hypervisor and the inner hypervisors N all are nested. In other words hypervisor is the outer hypervisor to hypervisor the N 1 th hypervisor N 1 is the outer hypervisor of the Nth hypervisor N and the Nth hypervisor N is the inner hypervisor of the N 1 th hypervisor N 1 . The first hypervisor outer hypervisor is the outermost hypervisor and the Nth hypervisor N is the innermost. In the example of the inner hypervisor serves as software container for a guest operating system OS running within the inner hypervisor . While not shown as such in for simplicity there may be a guest OS running in any of the nested inner hypervisors N .

To enable network communications outside of the cloud data center there is a physical network interface card NIC device that communicates with the network . The physical NIC device supports pass through. Within the outer hypervisor there is a physical NIC driver to enable operations of the physical NIC device and an emulated NIC device to enable network communications with the guest OS running within the inner hypervisor . Similarly in the inner hypervisor there is an emulated NIC driver and an emulated NIC device . The emulated NIC driver runs operations of the emulated NIC device in much the same way as the physical NIC driver runs operations of the physical NIC device . Another term for emulated NIC device is virtual NIC VNIC or VIC.

The outer hypervisor has an extender software component and the inner hypervisor has a similar extender component . The extender components and may be a VM Fabric Extender VM FEX driver software component offered by Cisco Systems Inc. or a Port Extender PE component compliant with the IEEE 802.1BR standard. The extender components and VM FEXs or PEs may be architecturally cascaded. Therefore the FEXes or PEs one in each nested hypervisor instance can be conveniently connected once as cascaded. The advantage of using multiple cascaded extender components within the nested hypervisors is that the extender components do not need to be configured because they are self configuring. Similarly VM NIC or physical NIC connections to each extender component need not be configured because they are self configuring. By contrast if multiple virtual switch components were used within the hypervisors then each virtual switch would need to be configured and each VM NIC or physical NIC connection to each virtual switch would need to be configured as well.

More generally the extender component whether VM FEX or PE allows for a one to one connection between the physical NIC device and the component above e.g. the guest OS . From a user perspective there is no need for network configuration whereas a virtual switch must be configured and handles many to many connections.

VM FEX is a networking and network services software component offered by Cisco Systems Inc. that delivers architectural flexibility and consistent networking across physical virtual and cloud environment enabling convergence network scale virtualization awareness and intelligence. VM FEX collapses virtual and physical networking into a single infrastructure. Data center administrators can provision configure manage monitor and diagnose virtual machine network traffic and bare metal network traffic within a unified infrastructure. The VM FEX software extends a fabric extender technology to the virtual machine with several following capabilities providing a dedicated interface on the parent switch for each VM sending all virtual machine traffic directly to the dedicated interface on the switch eliminating a software based switch in the hypervisor.

VM FEX eliminates the need for a virtual switch within a hypervisor by providing individual virtual machine virtual ports on the physical network switch. Virtual machine I O is sent directly to the upstream physical network switch which takes full responsibility for virtual machine switching and policy enforcement. This approach leads to consistent treatment for all network traffic virtual or physical. VM FEX consolidates virtual and physical switching layers into a single layer and reduces the number of network management points by an order of magnitude. VM FEX is a pre standard implementation of IEEE 802.1BR.

IEEE 802.1BR introduces a new software device called a Port Extender. One or more port extenders can be attached to a controlling switch. A controlling switch and its set of port extenders form a single extended switch. An extended switch is managed as a single entity through the controlling switch. Adding a port extender to a controlling switch is like adding a line card to a traditional switch.

When multiple PEs are cascaded such as with multiple nested hypervisors as shown in only one component executes virtual interface discovery protocol VDP . The peer is always the first hop from the external network switch i.e. the physical NIC device which in this case is the outer hypervisor . One implementation is one in which the component farthest from the switch executes VDP. Cascaded PEs in between such component and the physical switch rely on VDP messages between such component and the switch. In this case the component executing VDP would be in the inner hypervisor . In other words the VDP component is terminated on top of the extender component of the innermost hypervisor when there are multiple nested hypervisors . This is shown at reference numeral in on top of the extender component in the inner hypervisor in an example in which there is only one inner hypervisor . More generally however any of the nested hypervisors could execute VDP.

The host physical NIC device the outer hypervisor or a combination of the two run the complementary network protocols related to VM FEX or IEEE 802.1BR in concert with the peer networking switch connected to the host. The network interface of the guest OS gets assigned to a remotely managed network port of switch and in turn to an emulated NIC device within each hypervisor and as part of the VM FEX or PE component operations within the hypervisors.

In addition to the use of VM FEX or PE components in the outer and inner hypervisors two further features are supported to allow for bypassing the NIC device through two or more hypervisors. First an extension to each hypervisor flavor type of emulated NIC device model is used to provide the pass through related control operations as opposed to emulating the complete NIC hardware behavior in pass through mode . Second a delegation model application programming interface API is used where the inner hypervisor emulated NIC device propagates pass through related operations to the outer hypervisor emulated NIC device . In other words a chain of responsibility design pattern is employed in order to link together the set of emulated NIC device instances one in each hypervisor which are assigned to the top level or innermost guest OS emulated NIC device.

The pass through from the guest OS in the inner hypervisor to the physical NIC device is shown at reference numeral in and more generally from a guest OS not shown in hypervisor N to physical NIC device bypassing multiple nested hypervisors as shown at reference numeral . This pass through or bypass is made through two or more nested hypervisors which is not heretofore known to be achievable.

Reference is now made to . is a hardware diagram showing an example of the physical equipment devices on which the software components shown in may reside. To this end a memory is provided that stores the software instructions code for the outer hypervisor physical NIC driver emulated NIC device VDP end point extender component inner hypervisor guest OS emulated NIC driver emulated NIC device and extender component and VDP component . One or more processing units are coupled to the memory to execute the software instructions stored in the memory . For example there are one or more physical central processing units CPUs N a memory management unit MMU and an I O MMU IOMMU coupled to the memory and to a bus . The physical NIC device is coupled to the bus . Any one or more of the CPUs N may access and execute the software instructions stored in memory .

Memory may comprise read only memory ROM random access memory RAM magnetic disk storage media devices optical storage media devices flash memory devices electrical optical or other physical tangible memory storage devices. The CPUs N are for example a microprocessor or microcontroller that executes instructions stored in the memory . Thus in general the memory may comprise one or more tangible non transitory computer readable storage media e.g. a memory device encoded with software comprising computer executable instructions and when the software is executed by one or more of the CPUs N it is operable to perform the operations described herein.

Turning now to a high level description of the device model operations for the emulated NIC device in the outer hypervisor and emulated NIC device driver in the inner hypervisor . The emulated NIC device in the outer hypervisor is extended to provide just only the pass through related control operations to the inner hypervisor . The pass through related control operations are APIs provided by the outer hypervisor to the inner hypervisor . The inner hypervisor accesses the extended pass through related control operations using emulated NIC device driver as shown at reference numeral . The extended operations device model extensions include allocating configuring and freeing a device Virtual Function VF discovering hardware capabilities of the VF such as Receive Side Scaling RSS discovering HW resources of the VF such as number of queues and interrupts resources configuring the hardware resources and features of a VF setting up and tearing down interrupt resources quiesce ing the device saving and restoring hardware state and accumulating statistics.

To summarize the emulated NIC device in all nested hypervisors are executed such that the emulated NIC device in the outer hypervisor provides just pass through related control operations to the inner hypervisor and the inner hypervisor accesses the pass through related control operations using the NIC device driver. Moreover the emulated NIC device is executed in all nested hypervisors such that the emulated NIC devices in all nested hypervisors operate in an emulation mode or the emulated NIC devices in all nested hypervisors operate in a pass through mode.

As it is known in the art the SR IOV functionality is comprised of a Physical Function PF driver and a VF driver. The PF driver of a SR IOV device is used to manage the Physical Function PF of an SR IOV capable device. A Peripheral Control Interface PCI Function that supports the SR IOV capabilities is defined in the SR IOV specification. A PF contains the SR IOV capability structure and is used to manage the SR IOV functionality. PFs are fully featured PCI Express PCIe functions which can be discovered managed and manipulated like any other PCIe device. A PCI function that supports the SR IOV capabilities as defined by the SR IOV specification. A PF contains the SR IOV capability structure and manages the SR IOV functionality. PFs can be used to configure and control a PCIe device.

By contrast a VF is associated with a PF. A VF is a lightweight PCIe function that shares one or more physical resources with the Physical Function and with other VFs that are associated with the same PF. Unlike a PF a VF can only configure its own behavior.

The emulated NIC device in the outer hypervisor instead of emulating the hardware behaviors corresponding to the above operations forwards the operations to the associated underlying physical NIC device driver via API calls. The physical NIC device driver provides appropriate returns to each operation to the outer hypervisor which forwards back the result of each operation to the emulated NIC device and in turn to the inner hypervisor . For example this behavior could be a new capability of the emulated device perhaps a sub set of SR IOV capabilities.

Turning now to a flow chart is shown for operations of the MMU and IOMMU . At the outer hypervisor programs appropriate memory mappings in the MMU and in the IOMMU to the inner guest OS . This could be done via APIs between outer and inner hypervisors known as hyper calls . Alternatively the outer hypervisor could emulate a virtual IOMMU device to the inner hypervisor .

When the inner hypervisor programs memory mappings in the MMU and IOMMU then at the outer hypervisor intercepts the memory mapping actions and it knows the characteristics of the inner hypervisor perceived address space. Therefore the outer hypervisor programs the hardware MMU and IOMMU with mappings from the innermost guest OS physical memory addresses to the actual corresponding machine memory addresses.

At the outer hypervisor sets up hardware interrupts to be delivered from the real hardware device physical NIC device to the inner hypervisor emulated NIC device . This setup can be achieved via APIs between outer and inner hypervisors hyper calls . Alternatively the outer hypervisor could emulate PCI interrupt functionality to the inner hypervisor.

With reference now to a flow chart is shown depicting the emergent system behavior during a NIC device transition from emulation to pass through mode. At the inner hypervisor initiates the switch of the emulated NIC device from emulation to pass through mode. At the same time the outer hypervisor attempts also to switch the bound VF device in pass through mode by forwarding the API calls to the physical NIC device driver associated with the physical NIC device . At the switch to pass through mode is complete when both the nested hypervisors and have completed their own transitions to pass through mode. In other words the physical NIC device manages operations in the pass through mode when transition to pass through mode is complete for the outer hypervisor and the inner hypervisor .

Once the switch to pass through mode is complete the system relies on the physical NIC device VF behavior in pass though mode. Packet I O happens directly between the VF of the physical NIC device and the guest OS . This is shown at reference numeral in and more generally at reference numeral for I O bypass between physical NIC device and a guest OS in hypervisor N for bypass of multiple nested hypervisors.

To summarize the operations depicted in the inner hypervisor transitions from emulation mode to pass through mode of the NIC device substantially simultaneously with the outer hypervisor transitioning from emulation mode to pass through mode by forwarding API calls to a NIC device driver associated with the NIC device. Once transition to pass through is complete for the inner hypervisor and the outer hypervisor the NIC device manages operations in pass through mode.

Turning now to a flow chart is shown for the operations associated with achieving the packet I O shown in . At the guest OS directly manipulates the VF memory mapped resources including the rings for packet transmission packet reception and completions events the interrupt acknowledge and mask registers the memory of buffer payload etc. At memory accesses to these resources from the guest OS are translated by the MMU whereas memory accesses from the physical NIC device to these resources are translated by the IOMMU .

There is no hypervisor software intervention in the packet I O path. Therefore the packet I O path is very fast. Interrupt event rendezvous happens directly between the VF of the physical NIC device and the inner guest OS with minimal intervention of both hypervisors. When the VF raises an interrupt event the outer hypervisor forwards the event to the inner hypervisor VM virtual CPU vCPU which in turn forwards the event to the inner guest OS vCPU on behalf of its virtual NIC vNIC i.e. the emulated NIC device .

In summary presented herein are techniques for using an extender component such as VM FEX or 802.1BR PE in hypervisors to facilitate passing a physical NIC device through multiple hypervisors. A simple extension to the para virtualized devices models of current hypervisors provides a practical way to achieve multiple hypervisor pass through with support from currently available hardware.

These techniques afford higher and more predictable performance of VM networking I O through multiple nested hypervisors because there is no hypervisor software intervention in the packet I O path. As result a networking workload running in the VM can potentially benefit from close to native networking performance similar to when the same guest OS is running on a bare metal server. By contrast state of the art nested hypervisor implementations with support for VM migration introduce per packet software intervention of the networking stack and of the device emulation of both the nested hypervisors. Such intervention is a source of major system performance overhead when a network bound workload runs in the VM.

In addition the solution proposed herein has advantages from the point of view of practical implementation in commercial hypervisors and deployment in realistic use cases. Transitions between emulation and pass through mode do not cause any network reconfiguration or topology change. The system relies on a simple repetitive and standard network connectivity extender component within each hypervisor i.e. VM FEX or PE . Each software component is extended in particular each variation of emulated NIC device just enough to support a minimum set of mandatory operations to achieve a generic NIC pass through model. This solution also supports passing a NIC through more than two nested hypervisor containers without requiring additional software or hardware components to be implemented. The physical NIC device hardware support necessary to pass a device through one hypervisor is sufficient. Furthermore this solution supports naturally the migration of the inner most VM.

