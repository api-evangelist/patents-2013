---

title: Systems and methods for integrating user personas with content during video conferencing
abstract: A system and method is disclosed for extracting a user persona from a video and embedding that persona into a background feed that may have other content, such as text, graphics, or additional video content. The extracted video and background feed are combined to create a composite video that comprises the display in a videoconference. Embodiments cause the user persona to be embedded at preset positions, or in preset formats, or both, depending on the configuration, position, or motion of the user's body.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09055186&OS=09055186&RS=09055186
owner: PERSONIFY, INC
number: 09055186
owner_city: Chicago
owner_country: US
publication_date: 20130723
---
Embodiments of the present invention relate to the field of video processing and more specifically towards systems and methods for integrating user personas with other display content during video conferencing.

Conventional video conferencing techniques typically employ a camera mounted at one location and directed at a user. The camera acquires an image of the user and background that is then rendered on the video display of another user. The rendered image typically depicts the user miscellaneous objects and background that are within the field of view of the acquiring camera. For example the camera may be mounted on the top edge of a video display within a conference room with the user positioned to view the video display. The camera field of view may encompass the user and in addition a conference table chairs and artwork on the wall behind the user i.e. anything else within the field of view. In this typical technique the image of the entire field of view is transmitted to the video display of a second user. Thus much of the video display of the second user is filled with irrelevant distracting unappealing or otherwise undesired information. Such information may diminish the efficiency efficacy or simply the esthetic of the video conference. Additionally typical video conferencing techniques do not incorporate the user with virtual content being presented. And the traditional capture of the user and surrounding environment would be unnatural when juxtaposed against virtual content within a composite video. Such a display would be a significant departure from the familiar experience of a face to face interaction with a presenter discussing content on a whiteboard or projected on a screen. Also typical techniques require that the user manipulate content using the keyboard.

The systems and methods disclosed herein disclose using depth location and configuration information relating to a foreground video to control or modify how the foreground video is combined with a background feed to create a composite video. Some embodiments comprise controlling the embedding of an extracted video persona into a background feed which may be e.g. a background video a desktop slides images or any application window.

In an embodiment a foreground video is created from an extracted persona of a user from a first video. A background feed is received as well as preset conditions and directions that correspond to the preset conditions and which direct the embedding of the foreground video. With this information it is determined whether the foreground video exhibits a preset condition. Should the foreground video exhibit a preset condition then the foreground video is embedded into the background feed based on the embedding directions that correspond to the preset condition.

In an embodiment a preset condition relates to the distance the user moves from an initial position. Should the user remain within a first distance from the initial position then the foreground video when embedded moves distances that are similar to the distances the user moves. Should the user move beyond the first distance then the foreground video moves further than the user by a multiplying factor.

In an embodiment a preset condition relates to the area in which a user moves. Should the user venture outside a threshold area i.e. beyond a boundary the foreground video is made transparent. In an embodiment there is a border beyond the boundary and if the user is within that border the foreground video is made partially transparent where the degree of transparency is based on where the user is within the border. For example the user is rendered more transparent the further the user moves from the threshold.

In an embodiment the preset condition relates to the posture or other configuration of the user. For example if the user is standing then the foreground video is embedded so that it is prominent within the composite video perhaps in the center. But if the user is sitting then the foreground video is embedded less conspicuously perhaps in a lower corner and perhaps only the head and shoulders are displayed.

In the following description numerous details and alternatives are set forth for the purpose of explanation. However one of ordinary skill in the art will realize that embodiments can be practiced without the use of these specific details. In other instances well known structures and devices are shown in block diagram form to avoid obscuring the embodiments with unnecessary detail.

Integrating the user persona with the presented content and reducing the need to rely on keyboard control of content can improve the effect of the video conference. Accordingly it is highly desirable to have systems and methods for integrating user personas with content during video conferencing. The inventive systems and methods within may extract the persona of the user from the field of view of the acquiring camera and incorporate that user persona or image into content on the video display of the second user. Methods for extracting a persona from a video were published in application Ser. No. 13 076 264 filed Mar. 20 2011 by Minh N. Do et al. and Ser. No. 13 083 470 filed Apr. 8 2011 by Quang H. Nguyen et al. each of which is incorporated herein in its entirety by reference.

Extracting a user persona to create a foreground video will now be described regarding and . illustrates an example video . In general the example video comprises a background portion and a foreground portion . For example the background portion may comprise a wall outdoor scene or any other background scene and the foreground portion may comprise a human user or presenter. However the foreground portion may comprise any identifiable object or entity. Thus the example video may be divided into at least two portions a background and a foreground . For example if the video comprises a user speaking in a room then the user may comprise the foreground portion and a wall of the room may comprise the background portion .

Regarding embedding a foreground video comprising the image of a user presenter extracted from a video in an embodiment a metric for quantifying the user position is called the user s center of mass c.m. . In this embodiment computing the center of mass of a user provides not the center of the user s actual mass but the center of the image of the user s torso position. This facilitates mapping the movement of the user s torso to movement of the user s image within virtual content. It follows from the concept that human perception does not consider mere arm movements to indicate the overall movement of a person. A goal of this mapping is improving user image stability within the virtual content which is enhanced by avoiding excessive jitter in the detected center of mass. And since smoothness is a desired quality in an input device enhanced image stability facilitates using the center of mass as an input to a user interface.

An embodiment of a method for computing the center of mass is based on the median of the pixel s x value i.e. the horizontal offset in frame for each pixel in the user image. The more obvious mean is projected to be less stable during standard body motions i.e. employing the mean has may allow significant changes to result from stretching out the arm or hand gestures.

In an embodiment the center of mass calculation can be optimized to improve stability by weighting pixels higher during the median computation if those pixels are located at the bottom of the image. The weighting criteria can vary in different embodiments. Weighting pixels that appear in a lower fraction of the image such as the bottom third or bottom half based on vertical offset of the user image in the frame i.e. the pixel y value has been successful in practice. Stability improves because the bottom half of the body tends to be more stable during standing position with upper body movements more common.

There exist additional factors to be considered in optimizing an embodiment. Regarding Detected To Have Stopped this may be determined by detecting no change in the position of the user center of mass for an arbitrary number of frames. In an embodiment a counter is incremented by one 1 for each frame that the user center of mass moves and decreased by ten 10 for each frame that the user center of mass does not move with a starting point of 150 . Thus if the user spends 1 s moving 30 frames then whether the user has stopped is decided in 0.1 s 3 frames . The initial count guarantees an initial wait of at least 0.5 s 15 frames . This can be generalized.

Another consideration is how to adjust for dead zone lateral movement e.g. a slow drift of the user to one side. In an embodiment small shifts in user position within the dead zone cause Cx to shift over a number of frames to the new position of the user as measured by for example a new center of mass . Thus small shifts of Cx are adjusted for even when the user does not move beyond r1 or r2. The purpose of shifting Cx in the absence of movements beyond r1 and r2 is to retain image stability. Since within the dead zone changes in user movement are typically mapped to equal changes in the user image within virtual content the mapping within the dead zone is probably the least jerky and most stable. Maintaining the user in the center of the dead zone potentially reduces the number of user movements that extend beyond r1 or r2 and because they extend beyond r1 and r2 result in more exaggerated and potentially jerky movements of the user image within the virtual content.

In an additional consideration the width of the dead zone about Cx i.e. r1 and the associated distance r2 may be made dependent on the distance of the user from the camera the z value . In an embodiment the dependence is set to make the dead zone width r1 to reduce linearly with depth z value . Thus when the user is farther away from the camera the dead zone is reduced and when the user is nearer the camera the dead zone is increased. The rationale for this relationship is that small physical movements can cause bigger changes in user center of mass when the user is closer to the camera. This reduces the need to increase the user movement pursuant to Use Smooth Curve or Use Second Relationship when mapping to image movement within virtual content.

Now regarding which illustrates a top view of an example setup comprising thresholds for displaying the foreground video with the background feed in accordance with some embodiments at least one of which provides the ability to fade the foreground video in and out based on the position of the subject of the foreground video. In the foreground video of the user presenter may be embedded into the background feed which is perhaps being shown on display based on a control input. For example the foreground video may be embedded into the background feed based upon a gesture from the user presenter a mouse click a remote control input an input into a smartphone application or a keyboard stroke. In some embodiments the foreground video may be embedded into the background feed based upon a user presenter threshold . For example the user presenter threshold may comprise a predefined distance from the camera as measured along a line parallel to the center of the camera s field of view such that if the user presenter meets or exceeds the threshold limit then the foreground video comprising the user presenter may be embedded into the background feed. For example user presenter threshold may comprise a distance of 2 feet from the camera . Thus if user presenter is at or beyond 2 feet from camera then the foreground video comprising the user presenter may be embedded into the background feed . However if the user presenter is 1 foot away from the camera then the user presenter is under the user presenter threshold and the foreground video will not be embedded into the background feed. Given this scenario the user presenter as shown in being beyond threshold would be embedded in the background display .

Pursuant to an embodiment the camera may receive a depth image comprising the user presenter . The camera and or related hardware or software may continuously monitor the depth image comprising the user presenter . If the user presenter meets a defined threshold e.g. a distance from the camera then the systems and methods disclosed herein may embed a video of the user presenter e.g. the foreground video into the background feed e.g. a presentation slide . However if the user presenter does not meet the defined threshold then the systems and methods disclosed herein may not embed a video of the user presenter into the background feed . Moreover since the camera may continuously monitor or receive the depth images the foreground video may be repeatedly embedded or removed from the background feed depending on the movement of the user presenter . Restated the systems and methods disclosed herein may detect a user presenter moving forward or backwards and use such user movement to embed a foreground video comprising the user presenter into a second video image presentation slide or any other image or video. And depth information may be obtained from a variety of cameras e.g. infrared cameras structured light and time of flight cameras and stereo cameras.

User presenter threshold need not be a straight line. Should it be desired the same depth information that provides for a straight user presenter threshold could provide for a curved user presenter threshold which may be a radius distance from the camera or other chosen point. In addition thresholds and need not be limited to defining a distance that user presenter must meet or exceed to be embedded. In an alternative thresholds and may define a distance that user presenter must be at or within that distance from camera to become embedded i.e with this scenario the user presenter as shown in being beyond threshold would not be embedded in the background display .

Although the above example discusses using depth image of the user presenter to embed or not embed the user presenter into the background feed the camera may also detect the user presenter moving from side to side i.e. not changing in depth relative to the camera to control whether or not to embed the foreground video into the background feed . For example the user presenter moving to the right may indicate that the foreground video should be embedded into the background feed and the user presenter moving to the left may indicate that the foreground video should not be embedded into the background feed.

Regarding side to side movement illustrates an embodiment that employs a side threshold that functions much as user presenter threshold did. In some embodiments the foreground video may be embedded into the background feed based upon a user presenter lateral threshold . For example the user presenter lateral threshold may comprise a predefined lateral movement relative to camera such that if the user presenter is at or within lateral threshold then the foreground video comprising the user presenter may be embedded into the background feed. For example the user presenter threshold may comprise a distance of 5 feet from the camera measured laterally from a line along the center of the camera s field of view not shown . Thus if the user presenter is at or within the 5 feet user presenter lateral threshold then the foreground video comprising the user presenter may be embedded into the background feed . However if the user presenter is 16 feet away from the camera then the user presenter is beyond the user presenter lateral threshold and the foreground video will not be embedded into the background feed. Thus in this embodiment the user presenter as shown in being within lateral threshold would be embedded in the background display .

User presenter lateral threshold need not be a line parallel to the center of the camera s field of view. Should it be desired the same depth information that provides for a user presenter lateral threshold could provide for an angled user presenter lateral threshold which may be a straight line offset an arbitrary angle from the center of the camera s field of view as shown in . In addition threshold and need not be limited to defining a distance that user presenter must be at or within to be embedded. Thresholds and may alternatively define a distance that user presenter must be beyond the center of the field of view of camera to become embedded i.e in this embodiment the user presenter as shown in being within threshold would not be embedded in the background display .

Still regarding the selective embedding of a foreground video of a user presenter in an embodiment the foreground video of user presenter is rendered partially or completely transparent before being embedded in the background display . Thus using standard transparency features provided by known video rendering software the foreground video can be rendered transparent before being embedded in the background display and with depth and lateral distance information the degree of transparency may be determined by where the user presenter is located. This embodiment provides the ability to fade the foreground video of user presenter in and out of background display based on position. So continuing with this embodiment borders are defined relative to camera such as depth border between user presenter threshold and second user presenter threshold or lateral border between lateral threshold and second lateral threshold or both. Also for each border a fade profile is chosen. In an embodiment the fade profile may be a linear increase in transparency such that the foreground video is at 0 transparency with user presenter at the threshold and increases linearly to 100 with user presenter at the other side of the threshold. For example with and depth border as the user presenter approaches and attains threshold the foreground video is incorporated fully within background display with 0 transparency. Once it is determined that user presenter has crossed over threshold and into border then the foreground video is rendered increasingly transparent as user presenter travels increasingly further into border . Upon user presenter reaching second threshold the foreground video image of user presenter will be rendered 100 transparent. The fade profile may also be other than linear and could encompass stepped or discontinuous profiles or curved profiles that increasingly or decreasingly fade the foreground video as user presenter passes through the border and that fade profiles need not result in 100 transparency of the foreground video. Borders similar to borders and could be associated with angular lateral threshold or curved user threshold . Regarding a border associated with angular lateral threshold such a border may be defined angularly itself and incorporated to account for the known decreasing performance of a camera as the user presenter moving laterally approaches an edge of optimum performance such as an edge defined by the camera s field of view. Similarly regarding a border associated with user presenter threshold such a border may be defined by a radius and incorporated to account for the known decreasing performance of a camera as the user presenter moving forward approaches an edge of optimum performance such as an edge defined by the camera s focal distance. In an embodiment a border is defined to account for the minimum or maximum range of the apparatus providing depth information. Borders could also be defined that partially or completely surround user presenter . An advantage of being able to fade the foreground video is that a user presenter may move to within inches of camera perhaps to make a keystroke on a PC without causing a close up of user presenter to become embedded into background display .

As seen in which is a flow diagram illustrating an example embodiment of a method for embedding a foreground video at block depth and color camera information of a video is received. In some embodiments the depth information may relate to the distance to points in an image scene from a specific point. For example the depth information may comprise the distance of each pixel of an image frame of a video from a sensor. The color information may comprise color pixel information of an image frame of a video. The depth and color camera information may be received from a three dimensional 3D camera depth camera z camera range camera or from a plurality of sources. For example the color information may be received from a color camera and the depth information may be received from a depth camera. In some embodiments the color information and depth information may be received from a single camera. For example the color information may be received from an red blue green RGB sensor on a camera and the depth information may be received from an infrared IR sensor comprised within the same camera. Generally the method receives depth and color information of a video.

At block threshold and border information is received. In some embodiments the threshold and border information may define a square stage where the foreground video created from a user presenter located within the stage is not rendered transparent i.e. remains opaque as normal . In these embodiments the threshold and border information define a border about the stage where the foreground video created from a user presenter located within the border is rendered partially or completely transparent and the degree of transparency determined by the position of the user presenter within the border and fade profile information which is also received with threshold and border information. As discussed the fade profile could call for the foreground video to be rendered transparent with the degree of transparency increasing linearly with the position of the user presenter within the border. But the fade profile could also be a higher order curved function step function or other arbitrary user choice. In addition the size shape and location of the thresholds and borders could be based on factors ranging from the performance characteristics of the camera e.g. field of view optimum focal distances to the limits of the user s imagination.

At block the background portion and the foreground portion of the video are identified. For example given an example of a human presenter in front of a wall the foreground portion may comprise the human presenter and the background portion may comprise the wall. Thus at block the depth information and or color information is used to identify the background portion and the foreground portion of a video. At block the background portion of the video is subtracted or removed to create a foreground video. For example in the previous example of a human presenter in front of a wall the image of the wall may be removed and the resulting video may only comprise the foreground portion e.g. the human presenter .

At block a background feed is received. In some embodiments the background feed may comprise an image or a slide from a presentation or a series of images or slides from a presentation. For example the background feed may comprise a series of presentation slides for which a human presenter will discuss. In the same or alternative embodiments the background feed may comprise an image or view of a shared computer desktop a pre recorded video stream live recording video stream and or a 3D virtual scene. Generally the background feed may comprise any image video or combination of at least one image or at least one video.

At block the position of the foreground video is determined. With the foreground video from block and the depth and color information from block the location of the foreground video relative to the camera is determined. In an embodiment the center of mass c.m. of the foreground video may be computed as discussed earlier.

At block it is determined whether the foreground video is within all thresholds that is it is determined whether the foreground video is to be rendered without any transparency. The foreground video position from block is compared to the threshold information received during block . If the foreground video is not within all thresholds then in block it is determined whether the foreground video is within a border comparing foreground video position information from block and border information from block . If the foreground video is also not within a border then in block the background feed is displayed without embedding the foreground video. Returning to block if the comparison determines that the foreground video is within all thresholds then in block the foreground video is embedded into the background feed to create a composite video. Subsequently in block that composite video is displayed. Returning now to block if the comparison determines that the foreground video is within a border then in block the foreground video is rendered transparent according to the foreground video position from block and the fade profile from block . Subsequently the modified foreground video is embedded into the background feed in block and the composite video displayed in block .

In an embodiment between and or between blocks and or between both sets an additional block not shown may be inserted that requires an additional control input to be received before the foreground video is embedded into the background feed. The control input may comprise an action to embed the foreground video into the background feed. If it is determined that the foreground video should not be embedded then the foreground video is not embedded into the background feed. Thus in some embodiments the background feed may be displayed without the embedding or insertion of the previously identified foreground video as is the case in block but where the foreground video is otherwise within the thresholds or a border. In some embodiments the control input may provide instruction as to where to embed the foreground video within the background feed.

Now regarding and which illustrate an embodiment of a method for embedding a foreground video into a background feed. In this embodiment the foreground video comprising user presenter is embedded into background feed and the location and other aspects of the embedding such as size and coloring may be controlled by the posture or shape assumed by the user presenter. This is advantageous because it provides the ability to change the rendering of user presenter simply by sitting or standing which may improve the overall effect of the video by not requiring user presenter manipulate controls that might detract from any message user presenter is attempting to convey. As illustrated by user presenter is embedded within background feed which contains background elements . User presenter is standing and is embedded near the center of background feed . User presenter is also rendered opaque so as to appear to be in front of background elements .

However in this embodiment as depicted in when user presenter assumes a seated position user presenter is embedded within background feed differently. The assumption of a seated position causes user presenter to be embedded in a less prominent location within background feed which is depicted as a corner of background feed . In user presenter is embedded in the left corner of background feed and is also rendered as being seated and approximately the same scale as in . However in an embodiment upon assuming a seated position user presenter could also be rendered differently such as significantly smaller only from the shoulders up in a different corner in black and white or partially transparent etc. In these embodiments the ability to render user presenter in a less prominent manner focuses a viewer s attention on background elements which as depicted in are no longer obscured by user presenter . In addition the rendering of user presenter in could be based on the size of user presenter and how much of a specified region within background feed user presenter occupies. For example assume a target is set that calls for a percentage of a region not shown in the lower left corner of background feed to be comprised of user presenter should user presenter assume a seated position. If as shown in user presenter is an adult then once user presenter assumes a seated position it may be that user presenter as shown in occupies more or less than the targeted percentage of the region. In such a case user presenter as rendered in may be changed in scale or portions of user presenter may be cropped or otherwise rendered transparent before being embedded within background feed . Similarly should user presenter be a child or otherwise diminutive adult then user presenter as rendered in may be increased in scale before being embedded. In this regard a similar target could is set that calls for a percentage of a region not shown in the lower center of background feed to be comprised of user presenter should user presenter be standing. If as shown in user presenter is an adult then if user presenter assumes a standing position it may be that user presenter as shown in occupies more than the targeted percentage of the region. In such a case user presenter may be changed in scale or portions of user presenter may be cropped or otherwise rendered transparent before being embedded within background feed .

At block the background portion and the foreground portion of the video are identified. For example given an example of a human presenter in front of a wall the foreground portion may comprise the human presenter and the background portion may comprise the wall. Thus at block the depth information and or color information is used to identify the background portion and the foreground portion of a video. At block the background portion of the video is subtracted or removed to create a foreground video. For example in the previous example of a human presenter in front of a wall the image of the wall may be removed and the resulting video may only comprise the foreground portion e.g. the human presenter .

At block a background feed is received. In some embodiments the background feed may comprise an image or a slide from a presentation or a series of images or slides from a presentation. For example the background feed may comprise a series of presentation slides which a human presenter will discuss. In the same or alternative embodiments the background feed may comprise an image or view of a shared computer desktop a pre recorded video stream live recording video stream and or a 3D virtual scene. Generally the background feed may comprise any image video or combination of at least one image or at least one video.

At block user preset positions are received. Thus as described with reference to and the preset position for the user when standing may be centered within the background feed though other positions within the background feed may be chosen arbitrarily. And when the user assumes a seated position the preset position may be seated at the lower left corner of the background feed. It is also at this block that information may be received regarding other qualities of the embedded user presenter such as scale cropping transparency color level or even video update rate. An added advantage is that such changes may reduce the apparent size or motion of the user presenter and perhaps the corresponding bandwidth requirement. And this information regarding other qualities could apply to the embedding of the standing position or the seated position or both. Note the potential positions that the user presenter may assume are not limited to standing or seated but rather these are illustrative of the ability of embodiments to direct the embedding of the foreground video differently based on changes in the physical configuration of the user presenter. For example an additional such change could be the folding of arms across the chest such that an elbow is detected.

At block it is determined whether the foreground video contains a user. In some embodiments a motion detection of connected components is performed. This motion detection determines if a component is moving between depth image frames. A moving component may then be determined to be a person e.g. a user . In this manner a user may be detected at block . In some embodiments a camera may provide an infrared intensity image and the difference between the infrared intensity or depth value of the current image frame and a previous image frame may be calculated. If a pixel s infrared intensity increases by a significant amount and the pixel s value is below a specific threshold then the pixel may be marked as moving. In embodiments where the camera does not provide an infrared intensity image a pixel may be considered to be moving if its depth value decreases by a specific amount and the pixel depth value is below a specific threshold. Each component comprising a moving pixel may be further examined. If the number of moving pixels in a single component is above a predefined minimum amount and the percentage of moving pixels is not small relative to all pixels of the component then the component may be tagged as being in motion and as such may comprise a user.

Continuing with block in some embodiments a connected component in motion must have a user head detected in order for the connected component to be categorized as a user. For example for an unknown component user tracking may be performed and comprise checking the unknown component to determine whether the unknown component should be a foreground component or if it is a part of an existing foreground component e.g. the unknown component is a user . If the unknown component is not part of an existing user then the unknown component may be a new user and thus is analyzed through additional processes. Similar processes are performed for a background component.

The performance of the user tracking at block may further comprise processing checks on foreground or user components. For example if a foreground or user component is far from a user s center of mass then it may be re categorized as an unknown component. If a user component is close to another user s center of mass then it may be removed from the current user and into the second user s history. In some embodiments the user s information may be updated based on the current frame. For example information related to a user s center of mass dimensions and motion may be updated. Thus the positioning and placement of a user may be detected such that a user s gestures and configuration may be determined. In this manner a history of various characteristics of a user are recorded and updated. Further details concerning detecting a user can be found in published application Ser. No. 13 076 264 filed Mar. 20 2011 by Minh N. Do et al. and Ser. No. 13 083 470 filed Apr. 8 2011 by Quang H. Nguyen et al. each of which was previously incorporated herein in its entirety by reference.

If at block it is determined that the foreground video does not contain a user then at block it is determined whether to insert the foreground video. At block and as described with reference with the foreground video may be embedded into the background feed based on a control input. For example the foreground video may be embedded into the background feed based upon a gesture from the user presenter a mouse click a remote control input an input into a smartphone application or a keyboard stroke. If at block it is not chosen to insert the foreground video then at block the background feed is displayed. However if at block it is chosen to insert the foreground video then at block the foreground video is embedded into the background feed to create a composite video. Subsequently in block that composite video is displayed.

Returning to block if it is determined that the foreground video does contain a user then at block the orientation or configuration of the user is determined. In some embodiments the user configuration may comprise a user s head and hands. To do so the user s torso and neck may first be located by segmenting the user component into a plurality of horizontal slices and then moving upward until the width of the horizontal slices begins to diverge from the average width by a set amount. After finding the user s torso and neck the user s head is identified by examining an area above the identified neck. Once the user s head is found then the user s hands may be identified by performing a skeletonization of the user component. In some embodiments the user s hands may be assumed to be the furthest points to the left and the right of the user s torso. The skeletonization of the user component could also include identification of the user s legs and feet with the user s feet assumed to be the furthest points below the user s head. In such embodiments the configuration of the user as sitting or standing could be determined based on the distance between the user s head and feet and the subsequent changes to that dimension that are caused by sitting or standing. Given the presence of a line separating the user s feet and legs and extending toward the user s head the user could also be assumed to be wearing trousers or having exposed legs. The act of sitting would cause the length of that line to diminish substantially and embodiments could employ that change to determine that the user has assumed a seated position. Conversely the act of standing could cause that line to elongate which would be an indication of a standing user. In an embodiment determining the user position could be based on a combination of these indications as well as information previously provided that concerns the history of the user movement and is used to help interpret current user configuration information. In an embodiment the outcome of block is that a user position variable gets a value that signifies whether the user is standing or seated.

Once the user configuration is determined in block that user configuration is compared to the user preset position information received in block . Should the user configuration match a particular preset position then in block the foreground video is modified to render the user pursuant to the preset position that matched the user s configuration as described with reference to block and also and . In an embodiment should the user configuration not match a particular preset position then the foreground video remains as it was before block i.e. it is not modified. Subsequent to block in block it is determined whether to insert the foreground video into the background feed the outcomes of which have been described previously.

As seen in a camera is connected to a computer . The camera may comprise a three dimensional 3D camera depth camera z camera range camera. In some embodiments the camera may be comprised of a color or RGB camera and a depth camera or may comprise of a single camera with an RGB sensor and depth sensor. As such the camera receives color information and depth information. The received color information may comprise information related to the color of each pixel of a video. In some embodiments the color information is received from a Red Green Blue RGB sensor . As such the RGB sensor may capture the color pixel information in a scene of a captured video image. The camera may further comprise an infrared sensor and an infrared illuminator . In some embodiments the infrared illuminator may shine an infrared light through a lens of the camera onto a scene. As the scene is illuminated by the infrared light the infrared light will bounce or reflect back to the camera . The reflected infrared light is received by the infrared sensor . The reflected light received by the infrared sensor results in depth information of the scene of the camera . As such objects within the scene or view of the camera may be illuminated by infrared light from the infrared illuminator . The infrared light will reflect off of objects within the scene or view of the camera and the reflected infrared light will be directed towards the camera . The infrared sensor may receive the reflected infrared light and determine a depth or distance of the objects within the scene or view of the camera based on the reflected infrared light.

In some embodiments the camera may further comprise a synchronization module to temporally synchronize the information from the RGB sensor infrared sensor and infrared illuminator . The synchronization module may be hardware and or software embedded into the camera . In some embodiments the camera may further comprise a 3D application programming interface API for providing an input output IO structure and interface to communicate the color and depth information to a computer system . The computer system may process the received color and depth information and comprise and perform the systems and methods disclosed herein. In some embodiments the computer system may display the foreground video embedded into the background feed onto a display screen .

Any node of the network may comprise a general purpose processor a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof capable to perform the functions described herein. A general purpose processor may be a microprocessor but in the alternative the processor may be any conventional processor controller microcontroller or state machine. A processor may also be implemented as a combination of computing devices e.g. a combination of a DSP and a microprocessor a plurality of microprocessors one or more microprocessors in conjunction with a DSP core or any other such configuration etc. .

In alternative embodiments a node may comprise a machine in the form of a virtual machine VM a virtual server a virtual client a virtual desktop a virtual volume a network router a network switch a network bridge a personal digital assistant PDA a cellular telephone a web appliance or any machine capable of executing a sequence of instructions that specify actions to be taken by that machine. Any node of the network may communicate cooperatively with another node on the network. In some embodiments any node of the network may communicate cooperatively with every other node of the network. Further any node or group of nodes on the network may comprise one or more computer systems e.g. a client computer system a server computer system and or may comprise one or more embedded computer systems a massively parallel computer system and or a cloud computer system.

The computer system includes a processor e.g. a processor core a microprocessor a computing device etc. a main memory and a static memory which communicate with each other via a bus . The machine may further include a display unit that may comprise a touch screen or a liquid crystal display LCD or a light emitting diode LED display or a cathode ray tube CRT . As shown the computer system also includes a human input output I O device e.g. a keyboard an alphanumeric keypad etc. a pointing device e.g. a mouse a touch screen etc. a drive unit e.g. a disk drive unit a CD DVD drive a tangible computer readable removable media drive an SSD storage device etc. a signal generation device e.g. a speaker an audio output etc. and a network interface device e.g. an Ethernet interface a wired network interface a wireless network interface a propagated signal interface etc. .

The drive unit includes a machine readable medium on which is stored a set of instructions i.e. software firmware middleware etc. embodying any one or all of the methodologies described above. The set of instructions is also shown to reside completely or at least partially within the main memory and or within the processor . The set of instructions may further be transmitted or received via the network interface device over the network bus .

It is to be understood that embodiments may be used as or to support a set of instructions executed upon some form of processing core such as the CPU of a computer or otherwise implemented or realized upon or within a machine or computer readable medium. A machine readable medium includes any mechanism for storing information in a form readable by a machine e.g. a computer . For example a machine readable medium includes read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices electrical optical or acoustical or any other type of media suitable for storing information.

Although the present embodiment has been described in terms of specific exemplary embodiments it will be appreciated that various modifications and alterations might be made by those skilled in the art without departing from the spirit and scope of the invention. The previous description of the disclosed embodiments is provided to enable any person skilled in the art to make or use the present invention. Various modifications to these embodiments will be readily apparent to those skilled in the art and the generic principles defined herein may be applied to other embodiments without departing from the spirit or scope of the invention. Thus the present invention is not intended to be limited to the embodiments shown herein but is to be accorded the widest scope consistent with the principles and novel features disclosed herein.

