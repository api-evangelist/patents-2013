---

title: Object removal using lidar-based classification
abstract: In scenarios involving the capturing of an environment, it may be desirable to remove temporary objects (e.g., vehicles depicted in captured images of a street) in furtherance of individual privacy and/or an unobstructed rendering of the environment. However, techniques involving the evaluation of visual images to identify and remove objects may be imprecise, e.g., failing to identify and remove some objects while incorrectly omitting portions of the images that do not depict such objects. However, such capturing scenarios often involve capturing a lidar point cloud, which may identify the presence and shapes of objects with higher precision. The lidar data may also enable a movement classification of respective objects differentiating moving and stationary objects, which may facilitate an accurate removal of the objects from the rendering of the environment (e.g., identifying the object in a first image may guide the identification of the object in sequentially adjacent images).
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09523772&OS=09523772&RS=09523772
owner: Microsoft Technology Licensing, LLC
number: 09523772
owner_city: Redmond
owner_country: US
publication_date: 20130614
---
Within the field of computing many scenarios involve the capturing and rendering of a representation of an environment such as a portion of a street the interior of a room or a clearing in a natural setting. As a first example a set of images may be captured by a spherical lens camera and stitched together to form a visual rendering. As a second example the geometry of objects within the environment may be detected and evaluated in order to render a three dimensional reconstruction of the environment.

In these and other scenarios the portions of the capturing of the environment may be occluded by objects that are present within the environment. For example a capturing of a set of images depicting the setting and buildings along a street may be occluded by objects such as vehicles pedestrians animals and street signs. While such objects may be present in the scene in a static or transient manner it may be undesirable to present such objects as part of the scene. Therefore in such scenarios image processing techniques may be utilized to detect the portions of the respective images depicting such objects and to remove such objects from the rendering of the environment. For example image recognition techniques may be applied to the respective images to identify the presence and location of depicted objects such as vehicles and people e.g. based on a visual estimation of the size shape and color of the objects utilizing imaging properties such as scale shadowing and parallax and to refrain from including those portions of the images in the rendering of the environment.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key factors or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

While the removal of occluding objects from a rendering of an environment may be desirable it may be difficult to achieve the removal through image processing techniques due to the limitations in the precision of image processing techniques. For example automated image techniques for identifying the presence of individuals in an image may be skewed by properties such as visual distortion glare and shadows and may therefore result in false negatives e.g. failing to identify a present individual and rendering the depiction of part or all of the individual into the scene and or false positives e.g. incorrectly identifying a portion of an image as depicting an individual and therefore removing the individual from the image .

However in some scenarios laser imaging lidar data may be accessible that provides a supplementary set of information about the objects present in an environment. For example some image capturing vehicles are also equipped with a lidar emitter that emits a low powered visible spectrum laser at a specific wavelength and a lidar detector that detects light at the specific wavelength representing a reflection off of nearby objects. The resulting lidar point cloud is often utilized e.g. for navigation and or calibration of the vehicle and cameras. However lidar data may also be capable of identifying the objects present in the environment and more specifically classifying the respective objects according to a movement classification e.g. moving foreground stationary background stationary and fixed ground stationary . These types of object identification and movement classification may guide the omission of the objects from the rendering of the environment. For example identifying an object in a first image of an environment using the lidar data and movement classification may facilitate the identification of the same object in sequentially adjacent images in an image sequence of the environment e.g. images chronologically preceding and following the first image .

To the accomplishment of the foregoing and related ends the following description and annexed drawings set forth certain illustrative aspects and implementations. These are indicative of but a few of the various ways in which one or more aspects may be employed. Other aspects advantages and novel features of the disclosure will become apparent from the following detailed description when considered in conjunction with the annexed drawings.

The claimed subject matter is now described with reference to the drawings wherein like reference numerals are used to refer to like elements throughout. In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the claimed subject matter. It may be evident however that the claimed subject matter may be practiced without these specific details. In other instances structures and devices are shown in block diagram form in order to facilitate describing the claimed subject matter.

Within the field of machine vision many scenarios involve an automated evaluation of images of an environment to detect the objects present in the environment and depicted in the images and more particularly to identify the position size orientation velocity and or acceleration of the objects. As a first example the evaluation may involve vehicles in a transit environment including automobiles bicycles and pedestrians in a roadway as well as signs trees and buildings in order to facilitate obstacle avoidance. As a second example a physical object tracking system may evaluate the motion of an object within an environment in order to interact with it e.g. to catch a ball or other thrown object . As a third example a human actor present in a motion capture environment may be recorded while performing various actions in order to render animated personalities with human like movement. In various scenarios the analysis may be performed in realtime or near realtime e.g. to facilitate a device or individual in interacting with the other present objects while in other scenarios the analysis may be performed retrospectively e.g. to identify the movement of objects that were present at the time of the capturing . These and other scenarios often involve the capturing and evaluation of a set of visible light images e.g. with a still or motion camera and the application of visual processing techniques to human viewable images. For example machine vision techniques may attempt to evaluate from the contents of the image the type color size shape orientation position speed and acceleration of an object based on visual cues such as shadowing from light sources perspective relative sizes and parallax effects.

In these and other scenarios it may be desirable to remove part or all of the objects detected in the environment . As a first example the objects may be associated with individuals and it may be desirable to remove identifying indicators of the individuals who were present when the environment was captured e.g. by removing an entire object present in the environment such as a depiction of an individual and or by removing only a personally identifying portion of an object such as the face of the individual or a license plate of a vehicle . As a second example it may be desirable to generate a rendering of the environment that is not obscured by the objects temporarily present in the environment at the time of capturing. As a third example it may be desirable to depict the movement of the detected objects within the environment which may involve generating a static three dimensional reconstruction of the environment omitting all of the objects and then to add animation of the objects through the environment and or to generate a more accurate three dimensional model of the moving objects for various applications including sharpened visualization further classification of the object e.g. identifying the make and model of a moving vehicle and movement tracking.

However in these scenarios the achievable precision in the identification of the movement of the objects from an inspection of visual images may be limited. For example techniques such as perspective and parallax may provide only general estimates particularly for objects that are distant from the camera lens and or may be distorted by visual artifacts such as glare and shadows. As a result such evaluative techniques may produce estimates with low precision and or a high degree of error and may be inadequate for particular uses. As a first example the image processing techniques may fail to recognize some objects or portions thereof i.e. false negatives and may therefore fail to omit the objects from the rendering of the environment . As a second example the image processing techniques may incorrectly identify a portion of an image as depicting an individual i.e. false positives and may omit portions of the rendering of the environment that are not associated with objects . For example a visual image may capture a billboard depiction of a vehicle or a stone sculpture of an individual. Image processing techniques may incorrectly identify these portions of the environment as depicting actual vehicles or individuals and may remove them from an image based rendering of the environment thus removing valuable information about the environment in the absence of a significant motivation of privacy preservation and or removal of obscuring objects within the environment i.e. it may be desirable to include these objects as significant features of the environment .

Many scenarios involving the evaluation of object movement may be achieved through devices such as objects that also have access to data from a laser imaging lidar capturing device which may emit a set of focused low power beams of light of a specified wavelength and may detect and record the reflection of such wavelengths of light from various objects. The detected lidar data may be used to generate a lidar point cloud representing the lidar points of light reflected from the object and returning to the detector thus indicating specific points of the objects present in the environment . By capturing and evaluating lidar data over time such a device may build up a representation of the relative positions of objects around the lidar detector e.g. the locations of other objects with respect to the object operating the camera . These representations may be used while generating reconstructions of the environment to omit the depictions of the objects .

In order to classify respective objects such as objects as moving or stationary and optionally in order to identify other properties such as position and velocity techniques may be utilized to translate the lidar points of the respective lidar point clouds to three dimensional space. presents an illustration of an exemplary scenario featuring a translation of a set of lidar point clouds to classify the objects depicted therein. In this exemplary scenario for respective lidar point clouds the lidar points are mapped to a voxel in a three dimensional voxel space . Next the voxels of the three dimensional voxel space may be evaluated to detect one or more voxel clusters of voxels e.g. voxels that are occupied by one or more lidar points in the lidar point cloud and that share an adjacency with other occupied voxels of the three dimensional voxel space such as within a specified number of voxels of another occupied voxel resulting in the identification of one or more objects within an object space corresponding to the three dimensional voxel space . Next for the respective lidar points in the lidar point cloud the lidar point may be associated with a selected object . The movement of the lidar points may then be classified according to the selected object e.g. the objects may be identified as moving or stationary with the object in the three dimensional voxel space . According to the classified movements of the lidar points associated with the object e.g. added for the object spaces at respective time points a projection of the lidar points and an evaluation of the movements of the lidar points associated with respective objects the movement of the respective objects may be classified. For example and as depicted in the projection of the lidar points associated with the first object after projection in view of the three dimensional voxel space appear to be moving with respect to the lidar detector and may result in a classification of the object as a moving object while the lidar points associated with the second object after projection in view of the three dimensional voxel space appear to be stationary after adjusting for the movement of the lidar detector and may result in a classification of the object as a stationary object.

These and other techniques for evaluating a lidar point cloud to detect and classify a set of objects in an environment may facilitate the process of generating a rendering of the environment omitting such objects . presents an illustration of an exemplary scenario featuring an omission of such objects from a rendering of an environment . In this exemplary scenario a representation of the environment is captured from a capture perspective e.g. a position within the environment which may include both the environment and the objects present therein including vehicles individuals signs and buildings . Some objects such as the signs and buildings may be regarded as part of the environment that are to be included in the rendering of the environment e.g. as fixed ground objects and background objects while other objects may be regarded as transients to be removed from the rendering of the environment e.g. as moving objects and stationary foreground objects . Moreover some objects may include only an object portion of the object that is to be omitted. For example rather than omitting an entire individual or vehicle it may be desirable to omit only an object portion of the object that may be associated with a particular individual such as the individual s face or a license plate of a vehicle.

In order to generate a rendering of the environment satisfying these considerations the representation of the environment including the lidar point cloud captured by a lidar detector may be evaluated to identify the objects in the environment and a movement classification of such objects . A rendering of the environment assembled from the capturing e.g. a stitched together image assembled from a set of panoramic and or spherical images may therefore present a spherical view from the capture perspective that omits any portions of the capturing depicting the objects detected within the environment and according to the movement classification . For example the rendering may exclude all objects that are classified to be moving. Objects that are classified as stationary may further be evaluated to distinguish stationary foreground objects e.g. objects that are within a particular range of the capture perspective from fixed ground objects such as signs and or background objects such as buildings . As a result the rendering of the environment may contain omitted portions e.g. spots in the rendering that have been blurred blackened or replaced with a depiction of the environment that is not obscured by an object . Additionally it may be desirable to omit only an object portion of an object such as the license plate of the vehicle. In this manner various techniques may be applied to utilize a lidar point cloud including as but one example the evaluation of the lidar point cloud in the exemplary scenario of in the omission of objects in a rendering of an environment in view of the classification of the objects according to the lidar point cloud in accordance with the techniques presented herein.

Still another embodiment involves a computer readable medium comprising processor executable instructions configured to apply the techniques presented herein. Such computer readable media may include e.g. computer readable storage devices involving a tangible device such as a memory semiconductor e.g. a semiconductor utilizing static random access memory SRAM dynamic random access memory DRAM and or synchronous dynamic random access memory SDRAM technologies a platter of a hard disk drive a flash memory device or a magnetic or optical disc such as a CD R DVD R or floppy disc encoding a set of computer readable instructions that when executed by a processor of a device cause the device to implement the techniques presented herein. Such computer readable media may also include as a class of technologies that are distinct from computer readable storage devices various types of communications media such as a signal that may be propagated through various physical phenomena e.g. an electromagnetic signal a sound wave signal or an optical signal and in various wired scenarios e.g. via an Ethernet or fiber optic cable and or wireless scenarios e.g. a wireless local area network WLAN such as WiFi a personal area network PAN such as Bluetooth or a cellular or radio network and which encodes a set of computer readable instructions that when executed by a processor of a device cause the device to implement the techniques presented herein.

An exemplary computer readable medium that may be devised in these ways is illustrated in wherein the implementation comprises a computer readable storage device e.g. a CD R DVD R or a platter of a hard disk drive on which is encoded computer readable data . This computer readable data in turn comprises a set of computer instructions configured to operate according to the principles set forth herein. In one such embodiment the processor executable instructions may be configured to perform a method of rendering an environment omitting a set of objects such as the exemplary method of . In another such embodiment the processor executable instructions may be configured to implement a system for rendering an environment omitting a set of objects such as the exemplary system of . Some embodiments of this computer readable medium may comprise a computer readable storage device e.g. a hard disk drive an optical disc or a flash memory device that is configured to store processor executable instructions configured in this manner. Many such computer readable media may be devised by those of ordinary skill in the art that are configured to operate in accordance with the techniques presented herein.

The techniques discussed herein may be devised with variations in many aspects and some variations may present additional advantages and or reduce disadvantages with respect to other variations of these and other techniques. Moreover some variations may be implemented in combination and some combinations may feature additional advantages and or reduced disadvantages through synergistic cooperation. The variations may be incorporated in various embodiments e.g. the exemplary method of and the exemplary system of to confer individual and or synergistic advantages upon such embodiments.

A first aspect that may vary among embodiments of these techniques relates to the scenarios wherein such techniques may be utilized.

As a first variation of this first aspect the techniques presented herein may be utilized to evaluate many types of objects including objects traveling in an environment such as automobiles and bicycles traveling on a roadway or airplanes traveling in an airspace and individuals moving in an area such as a motion capture environment .

As a second variation of this first aspect the techniques presented herein may be utilized with many types of lidar signals including visible near infrared or infrared near ultraviolet or ultraviolet light. Various wavelengths of lidar signals may present various properties that may be advantageous in different scenarios such as passage through various media e.g. water or air of varying humidity sensitivity to various forms of interference and achievable resolution.

As a third variation of this first aspect the techniques presented herein may be utilized with various types of lidar emitters and or lidar detectors such as various types of lasers and photometric detectors. Additionally such equipment may be utilized in the performance of other techniques e.g. lidar equipment provided for range detection in vehicle navigation systems may also be suitable for the classification of moving and stationary objects and may be applied to both sets of techniques concurrently or in sequence. Those of ordinary skill in the art may devise a broad variety of such scenarios for the identification and movement classification of objects according to the techniques presented herein.

A second aspect that may vary among embodiments of these techniques relates to the manner of evaluating the lidar point cloud to identify the objects and the movement classification thereof.

As a first variation of this second aspect the particular techniques illustrated in the exemplary scenarios of may be utilized to evaluate the lidar point cloud and to detect and classify objects associated with respective lidar points . However it may be appreciated that these exemplary scenarios present only one such technique for evaluating a lidar point cloud and that other evaluative techniques may be utilized that add to remove from and or alter these techniques. As a first example the mapping of lidar points to objects may involve a mapping to a three dimensional voxel space as illustrated in the exemplary scenario of . Alternatively such mapping may include a two dimensional mapping to two dimensional voxels e.g. a two dimensional grid representing an aerial view of the environment or a proximity calculation that identifies clusters of proximate lidar points that appear to move together in the environment over time. As a second such example identifying the movement classification of the objects may be based on the movement classification of the individual lidar points associated with the object such as in the exemplary scenario of or may involve calculating an average movement of the lidar points associated with the object and or may involve identifying regions of the three dimensional voxel space having lidar points that appear to be moving in a comparatively similar direction.

As a second variation of this second aspect the identification and or movement classification of objects may be achieved by algorithms devised and encoded by humans. Alternatively or additionally such identification may be achieved in whole or in part by a machine learning technique. For example a device may comprise a movement classifier that is trained and configured to select a movement classification of an object in an environment using the lidar point cloud such as an artificial neural network or a genetically evolved algorithm. The techniques presented herein may involve selecting the movement classification of the respective objects by invoking the movement classifier.

As a third variation of this second aspect many techniques may be used to facilitate the identification of objects in the environment along with the evaluation the lidar point cloud . As a first such example a device may have access to at least one image of the environment and the detection of the objects movement classification of the objects and or the rendering of the environment by focusing an image portion of the image depicting the object using the movement classification of the object . For example the precise information about the position orientation shape shape and or velocity of the object in the environment may enable the identification of a specific portion of the image that is associated with the area of the lidar points associated with the object and thus likely depicting the object in the environment. Such focusing may involve e.g. trimming the portion of the image to the boundaries of the object matching the lidar points and or selecting a focal distance of the image to sharpen the selected portion of the image. As a further variation the image may be focused on at least one selected object portion of an object such as an object portion of the object that may be personally identifying of an individual such as a face of an individual or a license plate of a vehicle. As one such example evaluating the lidar point cloud may enable a determination of the orientation of the object and a determination of the position of the object portion of the object e.g. detecting the orientation of a vehicle may enable a deduction of the location of the license plate on the vehicle such as a particular flat rectangle on a bumper of the vehicle and or an area at a certain height above ground level . Alternatively or additionally the evaluation of the lidar point cloud may enable a focusing of an image portion of the image that depicts the selected object portion of the object using the movement classification of the object . This type of focusing may enable e.g. the generation of a rendering of the environment omitting the image portion depicting the selected object portion of the object .

As a fourth variation of this second aspect in some scenarios at least one object in the environment may be visually associated with at least one character such as recognizable letters numbers symbols and or pictograms. In such scenarios the identification of objects and or object portions in respective images may involve the recognition of characters through an optical character recognizer. For example identifying the object in the environment may further involve applying an optical character recognizer to one or more images of the environment to detect the at least one character and associating the character with the object in the environment . This variation may be advantageous e.g. for automatically detecting symbols on a personally identifying license plate of a vehicle and may be used in conjunction with the evaluation of the lidar point cloud .

As a fifth variation of this second aspect the evaluation of the objects may include many types of movement classification . For example respective objects may be classified as having a movement classification selected from a movement classification set comprising a moving object a stationary foreground object a stationary background object and a fixed ground object. These movement classifications may facilitate determinations in which objects to omit from the rendering of the environment e.g. moving objects and stationary foreground objects may be presumed to be transient with respect to the environment and may be omitted while stationary background objects and fixed ground objects may be presumed to be integral to the environment and may not be omitted.

As a sixth variation of this second aspect the identification of objects in a first capturing of the environment using the lidar point cloud may facilitate the identification of objects in a second capturing of the environment . presents a first exemplary scenario featuring one such facilitated identification of objects . In this first exemplary scenario an environment is captured from a first perspective having a first viewing angle and may include an object such as a vehicle. When the object has been recognized in the capturing from the first perspective according to the lidar points of the lidar point cloud a representation portion of a first environment representation of the environment from the first perspective e.g. captured concurrently with the detection of the lidar point cloud by the lidar detector may be identified as depicting the object . Additionally this identification may facilitate an identification of the object in while evaluating a second environment representation captured from a second perspective form a different viewing angle . For example a device may upon identifying the object in the environment from a first environment representation identify a position of the object in the environment according to the first perspective e.g. determining the location of the first perspective with respect to the environment determining the relative position of the object with respect to the lidar detector and deducing the position of the object with respect to the environment . As one such example the position of the object with respect to the environment may be determined as the location of the voxel cluster of the object in the three dimensional voxel space . Conversely the second viewing angle of the second perspective may be mapped to a particular section of the three dimensional voxel space and areas of the three dimensional voxel space including the voxel cluster of the object may be mapped to portions of the second image captured from the second perspective . In this manner even before evaluating the second image it may be possible for the device to determine where in the second image the object is likely to appear. Such variations may be utilized e.g. to guide the identification process e.g. focusing the evaluation of the second environment representation on a selected portion to inform and or verify an image evaluation process or even to select the environment representation portion of the environment representation without applying any image evaluation techniques.

A third aspect that may vary among embodiments of these techniques involves the uses of the object identification and movement classification in accordance with the techniques presented herein.

As a first variation of this third aspect the omission of the objects may be in furtherance of various scenarios. As a first such example the omission of the objects from the rendering of the environment may be performed e.g. to preserve the privacy of the individuals . As a second such example the omission of the objects may be performed to obscure the identity of the environment e.g. removing any object may be distinctively identify the environment as compared with any other environment of similar appearance . As a third such example the omission of the objects may be performed to provide a rendering of the environment that is not obscured by the objects e.g. generating an empty environment as if the objects had not been present which may be achievable by substituting an image portion of an image capturing of the environment with a second portion of the environment that corresponds to the same view but that is not obscured. As a fourth such example the omission of the objects may enable the insertion of other objects or even of the same objects at different time points e.g. animation of the objects moving through the rendering of the static environment as a modeled or computer generated depiction of such motion may be more desirable than rendering the environment to include the motion of the object captured during the capturing .

As a second variation of this third aspect the omission of the objects from the rendering of the environment may be achieved in various ways. As a first such example for scenarios involving a capturing of at least one image of the environment the objects may be omitted by blurring at least an image portion of at least one image depicting the at least an object portion of the at least one object . Alternatively or additionally the omission may be achieved by blackening or whitening the image portion or substituting another image portion for the object portion e.g. pasting an image of a second object over the depiction of the first object in the environment . As another such example if the object comprises an individual and the capturing includes at least one personal identifier such as a recognizable feature of the individual the device may remove at least one of the recognizable features of the individual from the rendering of the environment . As another example if the object comprises a vehicle that is associated with an individual and if the capturing of the environment includes a personal identifier comprising a vehicle identifier attached to the vehicle the device may remove the vehicle identifier of the vehicle from the rendering of the environment . As yet another example the device may have access to at least one background portion of the rendering of the environment corresponding to a representation portion of the capturing that has been obscured by an object e.g. a second image of the environment from the same capture perspective that is not obscured by the object and a device may replace the obscured portion of the capturing with the background portion while generating the rendering of the environment .

As a third variation of this third aspect a device may in addition to omitting the object from the rendering of the environment apply the information extracted from the evaluation of the lidar point cloud to achieve other features. As a first such example a device may upon receiving a request to generate a second rendering of the environment that includes the objects insert the objects into the rendering of the environment to generate the second rendering . That is having removed the objects from the rendering of the environment the device may fulfill a request to reinsert the objects e.g. as a differential depiction of a populated vs. empty environment . The insertion may also present different depictions of the objects than the portions of the capturing removed from the rendering such as stylized iconified and or clarified depictions of the objects . As a second such example a device may for respective objects that are moving in the environment according to the movement classification estimate a movement vector of the object at one or more time points . Additionally the device may generate within the rendering a depiction of the object moving through the environment . For example having extracted a static empty representation of the environment insert an animation of the moving objects to depict action within the environment over time. As a third such example the information may be used to select an object type of the respective objects e.g. the evaluation of the lidar point cloud may inform and facilitate an object recognition technique . These and other uses of the information generated by the evaluation of the lidar point cloud may be devised and applied to a variety of scenarios by those of ordinary skill in the art in accordance with the techniques presented herein.

Although not required embodiments are described in the general context of computer readable instructions being executed by one or more computing devices. Computer readable instructions may be distributed via computer readable media discussed below . Computer readable instructions may be implemented as program modules such as functions objects Application Programming Interfaces APIs data structures and the like that perform particular tasks or implement particular abstract data types. Typically the functionality of the computer readable instructions may be combined or distributed as desired in various environments.

In other embodiments device may include additional features and or functionality. For example device may also include additional storage e.g. removable and or non removable including but not limited to magnetic storage optical storage and the like. Such additional storage is illustrated in by storage . In one embodiment computer readable instructions to implement one or more embodiments provided herein may be in storage . Storage may also store other computer readable instructions to implement an operating system an application program and the like. Computer readable instructions may be loaded in memory for execution by processing unit for example.

The term computer readable media as used herein includes computer readable storage devices. Such computer readable storage devices may be volatile and or nonvolatile removable and or non removable and may involve various types of physical devices storing computer readable instructions or other data. Memory and storage are examples of computer storage media. Computer storage storage devices include but are not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM Digital Versatile Disks DVDs or other optical storage magnetic cassettes magnetic tape and magnetic disk storage or other magnetic storage devices.

Device may also include communication connection s that allows device to communicate with other devices. Communication connection s may include but is not limited to a modem a Network Interface Card NIC an integrated network interface a radio frequency transmitter receiver an infrared port a USB connection or other interfaces for connecting computing device to other computing devices. Communication connection s may include a wired connection or a wireless connection. Communication connection s may transmit and or receive communication media.

The term computer readable media may include communication media. Communication media typically embodies computer readable instructions or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal may include a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.

Device may include input device s such as keyboard mouse pen voice input device touch input device infrared cameras video input devices and or any other input device. Output device s such as one or more displays speakers printers and or any other output device may also be included in device . Input device s and output device s may be connected to device via a wired connection wireless connection or any combination thereof. In one embodiment an input device or an output device from another computing device may be used as input device s or output device s for computing device .

Components of computing device may be connected by various interconnects such as a bus. Such interconnects may include a Peripheral Component Interconnect PCI such as PCI Express a Universal Serial Bus USB Firewire IEEE 1394 an optical bus structure and the like. In another embodiment components of computing device may be interconnected by a network. For example memory may be comprised of multiple physical memory units located in different physical locations interconnected by a network.

Those skilled in the art will realize that storage devices utilized to store computer readable instructions may be distributed across a network. For example a computing device accessible via network may store computer readable instructions to implement one or more embodiments provided herein. Computing device may access computing device and download a part or all of the computer readable instructions for execution. Alternatively computing device may download pieces of the computer readable instructions as needed or some instructions may be executed at computing device and some at computing device .

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

As used in this application the terms component module system interface and the like are generally intended to refer to a computer related entity either hardware a combination of hardware and software software or software in execution. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer. By way of illustration both an application running on a controller and the controller can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers.

Furthermore the claimed subject matter may be implemented as a method apparatus or article of manufacture using standard programming and or engineering techniques to produce software firmware hardware or any combination thereof to control a computer to implement the disclosed subject matter. The term article of manufacture as used herein is intended to encompass a computer program accessible from any computer readable device carrier or media. Of course those skilled in the art will recognize many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.

Various operations of embodiments are provided herein. In one embodiment one or more of the operations described may constitute computer readable instructions stored on one or more computer readable media which if executed by a computing device will cause the computing device to perform the operations described. The order in which some or all of the operations are described should not be construed as to imply that these operations are necessarily order dependent. Alternative ordering will be appreciated by one skilled in the art having the benefit of this description. Further it will be understood that not all operations are necessarily present in each embodiment provided herein.

Moreover the word exemplary is used herein to mean serving as an example instance or illustration. Any aspect or design described herein as exemplary is not necessarily to be construed as advantageous over other aspects or designs. Rather use of the word exemplary is intended to present concepts in a concrete fashion. As used in this application the term or is intended to mean an inclusive or rather than an exclusive or . That is unless specified otherwise or clear from context X employs A or B is intended to mean any of the natural inclusive permutations. That is if X employs A X employs B or X employs both A and B then X employs A or B is satisfied under any of the foregoing instances. In addition the articles a and an as used in this application and the appended claims may generally be construed to mean one or more unless specified otherwise or clear from context to be directed to a singular form.

Also although the disclosure has been shown and described with respect to one or more implementations equivalent alterations and modifications will occur to others skilled in the art based upon a reading and understanding of this specification and the annexed drawings. The disclosure includes all such modifications and alterations and is limited only by the scope of the following claims. In particular regard to the various functions performed by the above described components e.g. elements resources etc. the terms used to describe such components are intended to correspond unless otherwise indicated to any component which performs the specified function of the described component e.g. that is functionally equivalent even though not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary implementations of the disclosure. In addition while a particular feature of the disclosure may have been disclosed with respect to only one of several implementations such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application. Furthermore to the extent that the terms includes having has with or variants thereof are used in either the detailed description or the claims such terms are intended to be inclusive in a manner similar to the term comprising. 

