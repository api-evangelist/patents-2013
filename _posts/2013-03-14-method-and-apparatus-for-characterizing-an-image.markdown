---

title: Method and apparatus for characterizing an image
abstract: The present invention relates to a method and system for characterizing an image. The characterization may then be used to conduct a search for similar images, for example using a learning system trained using previously characterized images. A face may be identified within the image and a subsection extracted from said image which does not contain said face. At least one fixed size patch is taken from said extracted subsection; and input into said learning network to characterize said image.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08873838&OS=08873838&RS=08873838
owner: Google Inc.
number: 08873838
owner_city: Mountain View
owner_country: US
publication_date: 20130314
---
The present invention relates to a method and system for characterizing an image. The characterization may then be used to conduct a search for similar images for example using a learning system trained using previously characterized images.

The present invention seeks to provide a more efficient method for training a learning system to identify patterns or other characteristics within an image and once trained to search for related images.

According to a first aspect of the invention there is provided a computer implemented method for characterizing an image using a learning network the method comprising 

By removing the face the characterization is focused on a more pertinent portion of the image. By using a fixed size patch the characterization is more uniform across images of different sizes. Both of these features should improve the efficiency of the characterization of the image. Another advantage of using a fixed size patch which is smaller than the subsection is that more than one patch may be taken from each subsection. Thus various different patches may be used to characterize the image. This is helpful as explained in more detail when training a learning system. Further features may be included to provide further improvements.

For example any background within the image may be removed before extracting said subsection from said image. Again this focuses the subsection on a more pertinent portion of the image. The background may be extracted by applying a mask e.g. using any known method including the Watershed method. The face may also be identified using any known method e.g. by detecting Haar wavelets.

A size of said face may also be computed and the subsection may be rescaled based on said calculated face size. This ensures that all input patches have a similar of the same scale which addresses the problem explained above in relation to to

Thus according to another aspect of the invention there is provided a computer implemented method for extracting a subsection from an image the method comprising 

The identified face may be analyzed to determine a skin tone for the face. This may be done using known techniques. The skin tone may also be analyzed by cropping the image to a bounding area which encompasses the face and determining a skin tone area within said bounding area. The skin tone area is determined by detecting a largest area within said bounding area which comprises only pixels which are likely to be skin. This may be done by comparing all colour pixels in the bounding area with a predefined set of values which have been determined to be a good match for skin. Once the skin tone area largest area is identified further processing may be applied to identify other areas of skin within the whole original image. They may then be removed from said image before extracting said subsection from said image. For example a skin tone mask may be used to remove any patches from said image having a similar color to said determined skin tone. This may be done by clustering the pixels within the skin tone area and comparing a value for each cluster with a value for each pixel in the whole image.

It will be appreciated that where skin tone is removed using a mask and the background is removed using a mask the background mask and skin tone mask may be combined. It will be appreciated that removing both the skin areas and the background means that the selected subsection is likely to focus on the main characteristic of the image e.g. a striped pattern for an image of a model wearing a striped skirt. Removing the skin tone may inadvertently remove skin color from the subsection e.g. flecks of skin tone within a skirt. Accordingly a median filter or other similar mechanism may be applied to replace any speckles of skin tone.

The image may be grayscaled i.e. color may be removed before any processing is carried out. Alternatively the grayscaling step may be applied to said extracted subsection.

The extracted subsection may be the largest section of the image with no skin or background. The largest section may be determined using dynamic programming.

Each said patch has a fixed size for example 32 32 pixels. As indicated above a plurality of fixed size patches may be taken from within said extracted subsection and each of said plurality of patches may be input into said learning system. This is particularly helpful when training the learning system. Training said learning system may involved using a training set of images each having known values for said characteristic wherein said training comprises repeating said extracting taking and inputting steps for each image within said training set. By using a plurality of patches from within the same subsection the training set is increased in size dramatically. Several patches for example between 100 300 preferably 200 may be taken from each extracted section. The patches may also be extracted at different scales and or a number of linear distortions for example mirroring jittering flipping zoom in by a small random factor etc may be applied to artificially increase the size of the dataset.

Said learning system may be a neural network for example a convolutional neural network or an auto encoder. The neural network preferably comprises an output layer which contains one node neuron per category of characterization. Various categories may be included such as categories for pattern such as horizontal stripes flowers polka dots plain etc. The output layer is a set of numbers which preferably sum to one and which represent the extent to which each of these categories is contained in the patch. As is well known in the art a neural network consists of an interconnected group of artificial neurons. In most cases a neural network is an adaptive system that changes its structure during a learning phase. Neural networks are used to model complex relationships between inputs and outputs i.e. to take an input image and output a characterization e.g. pattern for said input image.

Said neural network may comprise at least one non linear layer e.g. a said non linear layer which applies a tanh non linearity. Use of a non linearity means that even a two layer network can theoretically represent any continuous function and thus the space of mappings which the network can learn is increased.

Said neural network may comprise at least a first layer comprising a plurality of units e.g. filters and at least one subsequent layer which is connected to said first layer and which comprises a plurality of units e.g. tanh units and a connectivity table having the connections between said units in said first layer and said units in said subsequent layer is defined when said network is created. The number of connections may be reduced by using a smaller number e.g. k of units from the first layer with the smaller number of units being selected at random and the selection being fixed on creation of the network.

Said neural network may comprise at least one downsampling layer whereby the number of outputs from the previous layer which connects to the downsampling layer is reduced. There are many techniques for reducing the dimension for example by applying a max pooling operation to each input.

The neural network may further comprise a multi layer perceptron MLP layer or other similar learning layer. An MLP typically consists of multiple layers of nodes in a directed graph with each layer fully connected to the next one. Except for the input nodes each node is a neuron or processing element with a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training the network. MLP may be useful because it may be able to distinguish data that is not linearly separable.

Once trained the learning system may be used to search for images within a database which have been characterized and which are potentially matches to a search query inputted by a user.

Thus according to another aspect of the invention there is provided a computer implemented method for searching for at least one image which matches a query image the method comprising 

inputting said at least one fixed size patch into said learning network to characterize said query image 

comparing said characterization of said query image with a characterization of a plurality of images within a search database and

The characterization of the query is thus done using the techniques described above. Similarly the characterization of each image within the database may have been done in a similar manner. Accordingly the features described above in relation to characterization of an image and the nature of the learning network apply equally to this embodiment.

Additional information may be stored in the database alongside the characterization of the image for example the type of clothing and or personal characteristics e.g. age gender of any model wearing the clothing. Accordingly the method may further comprise determining a location of said face relative to said extracted subsection within said query image and filtering said outputted images based on said determined location. Similarly the method may further comprise determining personal characteristics of said face within said query image and filtering said outputted images based on said determined personal characteristics.

The image may also be input to a color analysis module to extract a color histogram. The color histogram may be compared with color histograms for each of the images within the database and the output results may be filtered based on the color histogram.

In each embodiment the characterization of the image may comprise generating a pattern category histogram.

According to other aspects of the invention there are also provided systems for implementing the computer implemented methods above. Thus the invention may provide a computer for characterizing an image using a learning network the computer comprising 

There may also be a query server for searching for at least one image which matches a query image the server comprising 

The invention further provides processor control code to implement the above described systems and methods for example on a general purpose computer system or on a digital signal processor DSP . The code may be provided on a non transitory carrier such as a disk CD or DVD ROM programmed memory such as non volatile memory e.g. Flash or read only memory Firmware . Code and or data to implement embodiments of the invention may comprise source object or executable code in a conventional programming language interpreted or compiled such as C or assembly code. As the skilled person will appreciate such code and or data may be distributed between a plurality of coupled components in communication with one another.

As set out above to are each images of a person wearing striped clothing. It is clear that the stripes in and have approximately the same width and that the stripes in the clothing of are narrower. However it is difficult to determine the size of the stripes in the striped pattern of relative to the patterns in the other Figures because of the different scale.

At step S any faces in the image are identified. There are many standard face detection algorithms such as the Haar like features used in OpenCV which are described in Rapid Object Detection using a Boosted Cascade of Simple Features by Viola et al published in Conference on Computer Vision and Pattern Recognition 2001. The result of the face identification is shown in . If there are no faces the method proceeds to step S. If at least one face is identified the following steps are applied for each face.

At step S the range of colors that make up the face are identified to determine the skin tone for the model wearing the clothing. This may be done in many ways for example as shown in the flowchart of . This method is an effective way to find the skin tones contained in an image regardless of lighting image quality etc. The first step S is to detect a face in a grayscale version of the input e.g. using Haar wavelets as described above . The image is then cropped to a bounding polygon around the face S . The polygon is typically rectangular. At step S a subset of the color pixels in this bounding rectangle that are very likely to be skin is detected. This is preferably done by comparing the pixels against a pre defined set of hue and saturation values which have been heuristically pre determined to be a good match for skin tones. For example techniques such as those used in A survey of skin color modeling and detection methods by Kakumanu et al published in the Journal of Pattern Recognition in 2007 P1106 1122 and Skin Color Analysis by Sherrah et al published in September 2007 http ist.ksc.kwansei.ac.jp kono Lecture CV Protected Rejume Skin.pdf .

Once the skin tones have been identified step S finds the largest polygon within the cropped polygon i.e. within the face bounding polygon that contains only these skin tones. Again the polygon is typically rectangular. The largest polygon may be found in any known way for example using the dynamic programming technique described below. Selecting such a polygon favors pixels that are in the desired range and are also spatially coherent. This means that areas of skin rather than dots of skin like hue in the hair or background is more likely to be selected.

Further processing is then applied to the skin tone polygon to better understand the skin tone of the person within the image and to identify other areas of skin e.g. arms or legs. Again there are various techniques which include using color models such as HSL and HSV which are the two most common cylindrical co ordinate representations of points in an RGB colour model. HSL stands for hue saturation and lightness and is often also called HLS. HSV stands for hue saturation and value and is also often called HSB B for brightness . Thus in one embodiment all the pixels within the skin tone polygon are clustered into a few clusters for example in HSV space S . Any clustering algorithm may be used for example k means with for example k 4 in other words clustering the colors in the rectangle into four clusters.

Once the clusters are determined the mean color distance MCD between HSV values for the pixels in each cluster is determined S . This determines the size of each cluster in other words how much variance there is in the colors of the pixels in that cluster. If the variance is zero or very close to zero a heuristically determined non zero minimal value is used instead. Using the values for each cluster at step S all the pixels in the original whole image are now considered in relation to each cluster centroid in HSV space . At step S we thus find all the pixels in the whole image having a value in HSV space that is closer to the cluster than the MCD. We don t need to extend the range because the purpose is not to find all skin pixels but simply some. In this way we can determine S the positioning of the limbs which constrains where the torso body can be and where we crop out the clothes.

Returning now to at step S the identified skin tone is used to modify the original mask to remove any parts of the image having the same color as the skin tone as well as the background. The result is shown in and illustrates that the mask has removed the background together with skin color from the edges of the clothing e.g. the arms face etc and also from within the clothing which leaves speckles across the clothing. Accordingly at step S a median filter is applied to the resulting mask to remove any specks from the clothing. Thus as shown in the clothing no longer has specks of black i.e. mask across it.

The largest section of the image with no skin or background is then identified at step S. Typically such a section will lie below the face and between the limbs. The largest section is shown in . The image of is also extracted using a similar process. There are various methods for identifying this section for example by moving and resizing a rectangle around the image until no skin or background is included therein. Alternatively dynamic programming which is a standard approach to solving complex problems may be used. The basic principle is to divide the problem into different parts subproblems then combine the solutions of the subproblems to reach an overall solution. Dynamic programming is a well known technique which is described for example in The Theory of Dynamic Programming by Richard Bellman published in 1954 in the Bulletin of the American Mathematical Society.

The extracted section is output at step S. The extracted section is typically rectangular but may also be another shape. However each extracted section needs to have the same shape.

If no face is detected in the image an alternative algorithm as shown in steps S to S is used to create the extracted section. The first step S is to use a standard edge detection method to find the most likely edges of the clothing and or person wearing the clothing. Once the edges normally the vertical edges are identified the left and right boundaries of the object and the maximum and median widths between the boundaries are identified S . This area is broken down into areas of homogeneous colour and pattern and the largest such region is identified or potentially the largest two or three contiguous regions are identified and joined together S . The color and pattern comparison means that the identified area is more likely to be clothing than background. As large an area as possible is used to ensure that as much of the key pattern is captured as possible. Finally at step S a centrally located section is selected from within the identified area. The centrally located section has a smaller width and height than the identified area for example perhaps 60 . Taking such a central section reduces the chance of including parts of the background and or person wearing the clothes. This section is then output at step S.

After any rescaling a patch also termed sub patch is extracted at random from the extracted section S . The advantage of taking a patch from the extracted section is that the patch will have a uniform or fixed size for example 32 32 pixels. An example of a patch is shown in . The patch is then input into a neural network S which is explained in more detail below. The output from the neural network summarizes the pattern contained in the patch S . This output is added to a histogram or other feature record which represents the pattern for the whole image S . The next step is to determine whether or not to process any further patches. Several patches for example between 100 300 preferably 200 are taken from each extracted section. If there are more patches to process the system conducts steps S to S again. When the requisite number of patches have been added to the overall record histogram the record is output.

The image is input to an interface of the server such as an API Application Programming Interface . The image is then passed to a patch extractor which extracts the extracted section and the patches subpatches from within the extracted section as described above. The patch extractor may be implemented in hardware or a combination of hardware and software e.g. as a module executed by a programmable processor of a machine . Each patch is then input to a neural network or similar learning module for pattern classification which is output as a pattern category histogram. The patch is preferably gray scaled to remove color before being input to the neural network . For example this may be done by using just the Y component in YUV color space. The YUV model defines a color space in terms of one luma Y and two chrominance UV components. Thus Y is roughly the luminance.

The patch extractor also has an input to a color analysis module . The color analysis module may be implemented in hardware or a combination of software and hardware like the patch extractor . The input to the color analysis module is preferably the extracted section rather than the individual patches which are fed to the neural network. The color analysis module then extracts a color histogram for the extracted section which is representative of the clothing within the image. The color histogram may be created using standard techniques for example by examining individual pixels.

As is well known a histogram consists of tabular frequencies typically shown as adjacent rectangles erected over discrete intervals. The area of each rectangle is normally equal to the frequency of the observations in the interval. The height of a rectangle is equal to the frequency density of the interval. The total area of the histogram is equal to the number of data.

Both histograms may then be input to a similarity engine . The similarity engine module may be implemented in hardware or a combination of software and hardware like the patch extractor . If both color and pattern are being used to determine matching results the similarity engine compares both histograms with all the histograms of the images within the image database . Alternatively only color or pattern may be used for determining matching images. There are various known methods for ensuring that the comparison process is done efficiently and not necessarily by comparing every single image in the database. For example one known method is LSH locality sensitive hashing for example as described in Near Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions by Alexandr Andoni and Piotr Indyk . Communications of the ACM vol. 51 no. 1 2008 pp. 117 122 . The similarity engine thus determines a set of similar images which are then output to a user. For example and are output in this example and thus the color histogram may not have been used. If the color histogram had been used may have been output rather than . Similarly if the gender of the model had been used to filter results would not have been output. When the images are displayed to the user the user can then for example click on one of those images to restart the search iterative search or could visit the page that the item is originally from to find out more about it or to buy it.

The server for conducting the search can be implemented using standard hardware. The hardware components of any server typically include a central processing unit CPU an Input Output I O Controller a system power and clock source display driver RAM ROM and a hard disk drive. A network interface provides connection to a computer network such as Ethernet TCP IP or other popular protocol network interfaces. The functionality may be embodied in software residing in computer readable media such as the hard drive RAM or ROM . A typical software hierarchy for the system can include a BIOS Basic Input Output System which is a set of low level computer hardware instructions usually stored in ROM for communications between an operating system device driver s and hardware. Device drivers are hardware specific code used to communicate between the operating system and hardware peripherals. Applications are software applications written typically in C C Java assembler or equivalent which implement the desired functionality running on top of and thus dependent on the operating system for interaction with other software code and hardware. The operating system loads after BIOS initializes and controls and runs the hardware. Examples of operating systems include Linux Solaris Unix OSX Windows XP and equivalents.

As shown in the example of the learning system has at least two convolutional layers each of which comprises a set of convolutional filters a tanh layer which reduces the input values into a smaller range and a max pooling layer which reduces the dimension of the input data . A connectivity table between the units in two consecutive layers is chosen when the system is initialized created. If the units in each layer were fully connected there would be m n connections where m are the number of units in the first layer and n are the number of units in the next layer. The number of connections between layers may be reduced by using a smaller number of units from the first layer e.g. k where k may be 3 rather than 128 . Thus each unit in the next layer is connected with k units in the previous layer with the k units being selected at random. These selections need to be fixed at network creation time otherwise the signals received by units would not be consistent. Thus the connectivity between units in two consecutive layers is fixed at start up.

A patch is input to the first convolutional layer marked as filter . Each patch is a fixed size e.g. 32 32 pixels and as explained in it is preferably a gray scale image. The filter comprises a fixed number of filters in this example sixteen each having a kernel also termed weight matrix of the same size in this example 5 5. Each kernel of each filter is preferably different. The patch is input once to each filter to be convolved therewith. The convolution process adds bias at every point and results in the same number of maps or first modified patches as the number of filters in this case 16 . Moreover since the filters all have the same size the filtered patches all have the same fixed size in this case 28 28 pixels for an input patch of 32 32 pixels . Inputting the input patch to the first convolutional layer results in n filtered patches . . . . The general aim of applying the first set of convolutional filters is to highlight any first order pattern within the input patch.

Each filtered patch is then passed through a tanh nonlinearity in a tanh layer . Each of the n filtered patches . . . is transformed to an non linear filtered patch . . . . Thus there is only one connection between each filter in the convolutional filter layer and each tanh unit in the tanh layer. As set out above this connectivity is defined at the outset.

It will be appreciated that a tanh non linearity is just one of many options but is a standard choice in the literature. Use of a non linearity increases the space of mappings which the network can learn. In theory ten linear layers are equivalent to a single layer because the composition of linear functions is a linear function. Thus a network with linear layers can only learn linear functions. However when introducing a differentiable non linearity such as the hyperbolic tangent tanh or the logistic sigmoid even a two layer network can theoretically represent any continuous function with arbitrary precision. That is a huge increase in capacity. Intuitively the difference between the two spaces of functions is that between the space of all lines in 2D and the space of any function one can draw without lifting the pen off the paper. Tanh is a useful non linearity because its outputs are centered at 0 and hence the representations it produces are somewhat 0 meaned. That makes learning easier in higher layers.

Each output from the tanh layer is then downsampled i.e. reduced in size in the pooling layer . Each of the n non linear filtered patch . . . is then transformed to a reduced non linear filtered patch . . . . Thus there is only one connection between each tanh unit in the tanh layer and each reduction unit. The reduction unit for example applies a max pooling operation. Max pooling is just one type of pooling but it has been shown in several empirical studies to be superior to other types of pooling such as L2 or sum mean pooling. When reducing the dimensions of an image some things have to be sacrificed. Max pooling sacrifices exact position information within small neighborhoods in order to emphasize strong activations of filters. In our case we can ignore small translations in images since they don t change the class. Thus the average or the norm of filter activations are less important than the maximum value of these activations on the original input patch.

One example of max pooling can be done over a grid of 2 2 pixels using a step of 2 in both directions . The step size just means how far we move the window that we re looking at each step. This is illustrated in to which shows that in this case the step size is the same size as the window. Thus each pixel is only included once. However the method can also be done such that the step size is smaller so we look at overlapping areas.

In this example max pooling is used to reduce each non linear filtered patch of 28 28 pixels to a reduced non linear filtered patch of size 14 14. The value of each entry in the 14 14 grid is the maximum activation between the 4 pixels in the corresponding 2 2 cell of the filtered patch of 28 28 pixels after the tanh nonlinearity has been applied .

It will be appreciated that the sizes of the patches are for illustration and different sizes may be used. Both the pooling and non linearity layers are also optional but result in improved performance. For example omitting the pooling layer results in a larger data set and the neural network will take longer to learn the data. Similarly as explained above the non linearity layer increases the space of mappings but could be omitted. Moreover in the example above we have only single connections between the filters tanh units and reduction units. Accordingly the number of outputs is the same as the number of inputs i.e. n 16 . The number of connections could be increased but this would increase complexity.

As set out above the aim of the first set of convolutional filters is to highlight any first order patterns. The learning system also comprises a second set of convolutional filters which act on the outputs of the first convolutional layer after optional non linearity and pooling has taken place to highlight any second order pattern.

The second convolutional layer marked filter also comprises a fixed number m of filters in this case 64. Each filter has a kernel of the same size in this example 5 5 with each filter typically being different. In contrast to the previous convolutional layer Each of the 64 filters has k in this case four inputs. The four inputs correspond to four of the sixteen outputs from the previous layer i.e. four of the reduced non linear filtered patches of size 14 14 . The four inputs are selected at random from the group of sixteen at startup but are fixed thereafter so that each filter continues to receive inputs from the same four channels. Each filter is convolved with the respective four inputs and the convolution process adds bias at every point. The result is 256 maps or second modified patches all of the same fixed size in this case 10 10 for an original input patch of 32 32 pixels .

Each reduced non linear filtered patch . . . is thus convolved with a number of filters which ranges between 1 and m. Accordingly the reduced non linear filtered patch is transformed into two filtered patches . Each of filtered patches is created using a different filter and thus the outputs may be different. Similarly reduced non linear filtered patch is transformed into three filtered patches . The system is constrained so that overall the number of filtered patches is 256 i.e. k m 4 64 .

As before each filtered patch . . . is then passed through a tanh nonlinearity in a tanh layer to create a corresponding number of non linear filtered patches . . . . Each output from the tanh layer is then downsampled in the pooling layer in a similar method to that described above. This results in 256 outputs i.e. 256 reduced non linear filtered patches . . . of size 5 5.

The next layer is the learning section of the neural network e.g. an MLP multi layer perceptron which processes the 6400 outputs 256 5 5 to perform supervised learning. The learning section thus has 6400 input units which are connected to 128 tanh units in a final tanh layer . The 128 tanh units are in turn connected to 22 output linear units such that each output of the tanh layer is passed to every node in a second learning section.

Thus in the embodiment shown there are two convolutional sections which process the data before it is fed to two learning sections.

In one particular example the full architecture was trained with stochastic gradient descent SGD using mini batches of size 10 and a learning rate of 0.001. The training to testing ratio used was 5 1. i.e. 83 training 17 testing . A successful result was achieved using 238 000 patches from 1046 images. It will be appreciated that a different number of patches may also give a successful result. The patches were extracted at three different scales and a number of linear distortions were applied to artificially increase the size of the dataset for example mirroring jittering flipping zoom in by a small random factor etc. The patches were labelled by splitting in 22 categories for pattern. The test dataset consisted of about 58 000 patches similarly labeled. Training was done in epochs with each epoch taking roughly 30 60 minutes 30 minutes with 4 cores on a single server 60 minutes with 1 core . The training set of 238 000 patches took approximately 50 epochs i.e. around 24 hours to train the whole network. During the training period the network learnt the weights using back propagation.

No doubt many other effective alternatives will occur to the skilled person. It will be understood that the invention is not limited to the described embodiments and encompasses modifications apparent to those skilled in the art lying within the spirit and scope of the claims appended hereto.

