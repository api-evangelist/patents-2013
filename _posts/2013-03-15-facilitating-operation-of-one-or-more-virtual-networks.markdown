---

title: Facilitating operation of one or more virtual networks
abstract: Techniques for facilitating the operation of one or more virtual networks are described. In some examples, a system may include a first controller node device configured to control operation of a first set of elements in the one or more virtual networks, wherein the first set of elements includes a first server device. The system may also include a second controller node device configured to control operation of a second set of elements in the one or more virtual networks, wherein the second set of elements includes the second server device. The first controller node device and the second controller node device are peers according to a peering protocol by which the first controller node device and the second controller node device exchange information relating to the operation of the first set of elements and the second set of elements.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08755377&OS=08755377&RS=08755377
owner: Juniper Networks, Inc.
number: 08755377
owner_city: Sunnyvale
owner_country: US
publication_date: 20130315
---
This application claims the benefit of U.S. Provisional Application No. 61 722 696 filed Nov. 5 2012 U.S. Provisional Application No. 61 721 979 filed Nov. 2 2012 U.S. Provisional Application No. 61 721 994 filed Nov. 2 2012 U.S. Provisional Application No. 61 718 633 filed Oct. 25 2012 U.S. Provisional Application No. 61 656 468 filed Jun. 6 2012 U.S. Provisional Application No. 61 656 469 filed Jun. 6 2012 and U.S. Provisional Application No. 61 656 471 filed Jun. 6 2012 the entire content of each of which being incorporated herein by reference.

Techniques of this disclosure relate generally to computer networks and more particularly to virtual networks.

In a typical cloud data center environment there is a large collection of interconnected servers that provide computing and or storage capacity to run various applications. For example a data center may comprise a facility that hosts applications and services for subscribers i.e. customers of data center. The data center may for example host all of the infrastructure equipment such as networking and storage systems redundant power supplies and environmental controls. In a typical data center clusters of storage systems and application servers are interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers. More sophisticated data centers provide infrastructure spread throughout the world with subscriber support equipment located in various physical hosting facilities.

In general techniques for facilitating the operation of one or more virtual networks using a distributed virtual network controller are described. The one or more virtual networks may include at least a server device that is connected to an Internet Protocol network through at least a switch device. The system may include a first controller node device configured to control operation of a first set of elements in the one or more virtual networks. The system may further include a second controller node device configured to control operation of a second set of elements in the one or more virtual networks. The first controller node device and the second controller node device may be peered using a peering protocol and may be peers according to the peering protocol. The system may include hardware and software associated with one or more of the first controller node device and the second controller node device.

In some examples a system includes a first set of elements and a second set of elements that implement one or more virtual networks. The system also includes a first server device and a second server device each connected to a network by a switch fabric. The system also includes a first controller node device configured to control operation of the first set of elements in the one or more virtual networks wherein the first set of elements includes the first server device. The system further includes a second controller node device configured to control operation of the second set of elements in the one or more virtual networks wherein the second set of elements includes the second server device wherein the first controller node device and the second controller node device are peers according to a peering protocol by which the first controller node device and the second controller node device exchange information relating to the operation of the first set of elements and the second set of elements.

In some examples a method for facilitating operation of one or more virtual networks the one or more virtual networks including a first server device and a second server device each connected to a network by a switch fabric includes using a first controller node device to control operation of a first set of elements in the one or more virtual networks wherein the first set of elements includes the first server device. The method also includes using a second controller node device to control operation of a second set of elements in the one or more virtual networks wherein the second set of elements includes the second server device. The method further includes peering the first controller node device and the second controller node device using a peering protocol to exchange information between the first controller node device and the second controller node device relating to the operation of the first set of elements and the second set of elements.

In some examples a virtual network controller node device includes one or more processors and a control plane virtual machine executed by the processors to communicate with a plurality of virtual network switches using an eXtensible Messaging and Presence Protocol XMPP . The virtual network controller node device also includes a configuration virtual machine to store and manage a configuration database that includes configuration information for the virtual network switches. The virtual network controller node device also includes an analytics virtual machine to store and manage an analytics database that includes logging information for the virtual network switches wherein the configuration virtual machine and analytics virtual machine communicate with the control plane virtual machine using an Interface for Metadata Access Points protocol wherein the control plane virtual machine configures the virtual network switches by using XMPP to send route data and configuration information to the virtual network switches and wherein the control plane virtual machine uses XMPP to receive logging information for the virtual network switches and routes the logging information to the analytics virtual machine for storage to the analytics database.

In some examples a network system comprises a switch fabric comprising a plurality of switches and a distributed controller having a set of controller node devices in peer communication with each other in accordance with a peering protocol wherein each of the controller node devices configures and manages an overlay network within the plurality of switches. The network system also comprises a plurality of servers interconnected by the switch fabric wherein each of the servers comprises an operating environment executing one or more virtual machines in communication via the overlay networks and wherein the servers comprises a set of virtual switches that extends the overlay network as virtual networks to the operating environment of the virtual machines.

The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features objects and advantages of the invention will be apparent from the description and drawings and from the claims.

In some examples data center may represent one of many geographically distributed network data centers. As illustrated in the example of data center may be a facility that provides network services for customers . Customers may be collective entities such as enterprises and governments or individuals. For example a network data center may host web services for several enterprises and end users. Other exemplary services may include data storage virtual private networks traffic engineering file service data mining scientific or super computing and so on. In some embodiments data center may be individual network servers network peers or otherwise.

In this example data center includes set of storage systems and application servers A X herein servers interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers. Switch fabric is provided by a set of interconnected top of rack TOR switches A BN TOR switches coupled to a distribution layer of chassis switches . Although not shown data center may also include for example one or more non edge switches routers hubs gateways security devices such as firewalls intrusion detection and or intrusion prevention devices servers computer terminals laptops printers databases wireless mobile devices such as cellular phones or personal digital assistants wireless access points bridges cable modems application accelerators or other network devices.

In this example TOR switches and chassis switches provide servers with redundant multi homed connectivity to IP fabric and service provider network . Chassis switches aggregates traffic flows and provides high speed connectivity between TOR switches . TOR switches A and B may be network devices that provide layer 2 MAC address and or layer 3 IP address routing and or switching functionality. TOR switches and chassis switches may each include one or more processors and a memory and that are capable of executing one or more software processes. Chassis switches are coupled to IP fabric which performs layer 3 routing to route network traffic between data center and customers using service provider network .

Virtual network controller VNC provides a logically centralized controller for facilitating operation of one or more virtual networks within data center in accordance with one or more embodiments of this disclosure. In some examples virtual network controller may operate in response to configuration input received from network administrator .

Each virtual switch may execute within a hypervisor a host operating system or other component of each of servers . In some instances any of virtual switches may be present in a campus access switch or Wi Fi access point WAP . In the example of virtual switch executes within hypervisor also often referred to as a virtual machine manager VMM which provides a virtualization platform that allows multiple operating systems to concurrently run on one of host servers . In the example of virtual switch A manages virtual networks each of which provides a network environment for execution of one or more virtual machines VMs on top of the virtualization platform provided by hypervisor . Each VM is associated with one of the virtual subnets VN VN managed by the hypervisor .

In general each VM may be any type of software application and may be assigned a virtual address for use within a corresponding virtual network where each of the virtual networks may be a different virtual subnet provided by virtual switch A. A VM may be assigned its own virtual layer three L3 IP address for example for sending and receiving communications but may be unaware of an IP address of the physical server A on which the virtual machine is executing. In this way a virtual address is an address for an application that differs from the logical address for the underlying physical computer system i.e. server A in the example of .

In one implementation each of servers includes a virtual network agent VN agent A X VN agents that controls the overlay of virtual networks and that coordinates the routing of data packets within server . In general each VN agent communicates with virtual network controller which generates commands to control routing of packets through data center . VN agents may operate as a proxy for control plane messages between virtual machines and virtual network controller . For example a VM may request to send a message using its virtual address via the VN agent A and VN agent A may in turn send the message and request that a response to the message be received for the virtual address of the VM that originated the first message. In some cases a VM may invoke a procedure or function call presented by an application programming interface of VN agent A and the VN agent A may handle encapsulation of the message as well including addressing.

In one example network packets e.g. layer three L3 IP packets or layer two L2 Ethernet packets generated or consumed by the instances of applications executed by virtual machines within the virtual network domain may be encapsulated in another packet e.g. another IP or Ethernet packet that is transported by the physical network. The packet transported in a virtual network may be referred to herein as an inner packet while the physical network packet may be referred to herein as an outer packet. Encapsulation and or de capsulation of virtual network packets within physical network packets may be performed within virtual switches e.g. within the hypervisor or the host operating system running on each of servers . As another example encapsulation and de capsulation functions may be performed at the edge of switch fabric at a first hop TOR switch that is one hop removed from the application instance that originated the packet. This functionality is referred to herein as tunneling and may be used within data center to create one or more overlay networks. Other example tunneling protocols may be used including IP over GRE VxLAN MPLS over GRE etc.

As noted above virtual network controller provides a logically centralized controller for facilitating operation of one or more virtual networks within data center . Virtual network controller may for example maintain a routing information base e.g. on or more routing tables that store routing information for the physical network as well as the overlay network of data center . Similarly switches and virtual switches maintain routing information such as one or more routing and or forwarding tables. In one example implementation virtual switch A of hypervisor implements a network forwarding table NFT for each virtual network . In general each NFT stores forwarding information for the corresponding virtual network and identifies where data packets are to be forwarded and whether the packets are to be encapsulated in a tunneling protocol such as with one or more outer IP addresses.

The routing information may for example map packet key information e.g. destination IP information and other select information from packet headers to one or more specific next hops within the networks provided by virtual switches and switch fabric . In some case the next hops may be chained next hop that specify a set of operations to be performed on each packet when forwarding the packet such as may be used for flooding next hops and multicasting replication. In some cases virtual network controller maintains the routing information in the form of a radix tree having leaf nodes that represent destinations within the network. U.S. Pat. No. 7 184 437 provides details on an exemplary embodiment of a router that utilizes a radix tree for route resolution the contents of U.S. Pat. No. 7 184 437 being incorporated herein by reference in its entirety.

As shown in each virtual network provides a communication framework for encapsulated packet communications for the overlay network established through switch fabric . In this way network packets associated with any of virtual machines may be transported as encapsulated packet communications via the overlay network. In addition in the example of each virtual switch includes a default network forwarding table NFTand provides a default route that allows packet to be forwarded to virtual subnet VN without encapsulation i.e. non encapsulated packet communications per the routing rules of the physical network of data center . In this way subnet VN and virtual default network forwarding table NFTprovide a mechanism for bypassing the overlay network and sending non encapsulated packet communications to switch fabric .

Moreover virtual network controller and virtual switches may communicate using virtual subnet VN in accordance with default network forwarding table NFTduring discovery and initialization of the overlay network and during conditions where a failed link has temporarily halted communication via the overlay network. Once connectivity with the virtual network controller is established the virtual network controller updates its local routing table to take into account new information about any failed links and directs virtual switches to update their local network forwarding tables . For example virtual network controller may output commands to virtual network agents to update one or more NFTs to direct virtual switches to change the tunneling encapsulation so as to re route communications within the overlay network for example to avoid a failed link.

When link failure is detected a virtual network agent local to the failed link e.g. VN Agent A may immediately change the encapsulation of network packet to redirect traffic within the overlay network and notifies virtual network controller of the routing change. In turn virtual network controller updates its routing information any may issues messages to other virtual network agents to update local routing information stored by the virtual network agents within network forwarding tables .

In this example chassis switch CH which may be any of chassis switches of is coupled to Top of Rack TOR switches A B TORs by chassis link A and chassis link B respectively chassis links . TORs may in some examples be any of TORs of . In the example of TORs are also coupled to servers A B servers by TOR links A D TOR links . Servers may be any of servers . Here servers communicate with both TORs and can physically reside in either associated rack. TORs each communicate with a number of network switches including chassis switch A.

Chassis switch A has a processor A in communication with an interface for communication with a network as shown as well as a bus that connects a memory not shown to processor A. The memory may store a number of software modules. These modules include software that controls network routing such as an Open Shortest Path First OSPF module not shown containing instructions for operating the chassis switch A in compliance with the OSPF protocol. Chassis switch A maintains routing table RT table A containing routing information for packets which describes a topology of a network. Routing table A may be for example a table of packet destination Internet protocol IP addresses and the corresponding next hop e.g. expressed as a link to a network component.

TORs each have a respective processor B C an interface in communication with chassis switch A and a memory not shown . Each memory contains software modules including an OSPF module and routing table B C as described above.

TORs and chassis switch A may exchange routing information specifying available routes such as by using a link state routing protocol such as OSPF or IS IS. TORs may be configured as owners of different routing subnets. For example TOR A is configured as the owner of Subnet 1 which is the subnet 10.10.10.0 24 in the example of and TOR A is configured as the owner of Subnet 2 which is the subnet 10.10.11.0 24 in the example of . As owners of their respective Subnets TORs locally store the individual routes for their subnets and need not broadcast all route advertisements up to chassis switch A. Instead in general TORs will only advertise their subnet addresses to chassis switch A.

Chassis switch A maintains a routing table RT table A which includes routes expressed as subnets reachable by TORs based on route advertisements received from TORs . In the example of RT table A stores routes indicating that traffic destined for addresses within the subnet 10.10.11.0 24 can be forwarded on link B to TOR B and traffic destined for addresses within the subnet 10.10.10.0 24 can be forwarded on link A to TOR A.

In typical operation chassis switch A receives Internet Protocol IP packets through its network interface reads the packets destination IP address looks up these addresses on routing table A to determine the corresponding destination component and forwards the packets accordingly. For example if the destination IP address of a received packet is 10.10.0.0 i.e. the address of the subnet of TOR A the routing table of chassis switch A indicates that the packet is to be sent to TOR A via link A and chassis switch A transmits the packet accordingly ultimately for forwarding to a specific one of the servers .

Similarly each of TORs receives Internet Protocol IP packets through its network interface reads the packets destination IP address looks up these addresses on its routing table to determine the corresponding destination component and forwards the packets according to the result of the lookup.

Virtual network controller VNC of illustrates a distributed implementation of a VNC that includes multiple VNC nodes A N collectively VNC nodes to execute the functionality of a data center VNC including managing the operation of virtual switches for one or more virtual networks implemented within the data center. Each of VNC nodes may represent a different server of the data center e.g. any of servers of or alternatively on a server or controller coupled to the IP fabric by e.g. an edge router of a service provider network or a customer edge device of the data center network. In some instances some of VNC nodes may execute as separate virtual machines on the same server.

Each of VNC nodes may control a different non overlapping set of data center elements such as servers individual virtual switches executing within servers individual interfaces associated with virtual switches chassis switches TOR switches and or communication links. VNC nodes peer with one another using peering links to exchange information for distributed databases including distributed databases A K collectively distributed databases and routing information e.g. routes for routing information bases A N collectively RIBs . Peering links may represent peering links for a routing protocol such as a Border Gateway Protocol BGP implementation or another peering protocol by which VNC nodes may coordinate to share information according to a peering relationship.

VNC nodes of VNC include respective RIBs each having e.g. one or more routing tables that store routing information for the physical network and or one or more overlay networks of the data center controlled by VNC . In some instances one of RIBs e.g. RIB A may store the complete routing table for any of the virtual networks operating within the data center and controlled by the corresponding VNC node e.g. VNC node A .

In general distributed databases define the configuration or describe the operation of virtual networks by the data center controlled by distributed VNC . For instance distributes databases may include databases that describe a configuration of one or more virtual networks the hardware software configurations and capabilities of data center servers performance or diagnostic information for one or more virtual networks and or the underlying physical network the topology of the underlying physical network including server chassis switch TOR switch interfaces and interconnecting links and so on. Distributed databases may each be implemented using e.g. a distributed hash table DHT to provide a lookup service for key value pairs of the distributed database stored by different VNC nodes . Distributed databases may be implemented stored using computer readable media of or associated with VNC nodes .

As illustrated in the example of distributed virtual network controller VNC includes one or more virtual network controller VNC nodes A N collectively VNC nodes . Each of VNC nodes may represent any of VNC nodes of virtual network controller of . VNC nodes that peer with one another according to a peering protocol operating over network . Network may represent an example instance of switch fabric and or IP fabric of . In the illustrated example VNC nodes peer with one another using a Border Gateway Protocol BGP implementation an example of a peering protocol. In this sense VNC nodes A and N may represent a first controller node device and a second controller node device peered using a peering protocol. VNC nodes include respective network discovery modules A N to discover network elements of network .

VNC nodes provide to one another using the peering protocol information related to respective elements of the virtual network managed at least in part by the VNC nodes . For example VNC node A may manage a first set of one or more servers operating as virtual network switches for the virtual network. VNC node A may send information relating to the management or operation of the first set of servers to VNC node N by BGP A. Other elements managed by VNC nodes may include network controllers and or appliances network infrastructure devices e.g. L2 or L3 switches communication links firewalls and VNC nodes for example. Because VNC nodes have a peer relationship rather than a master slave relationship information may be sufficiently easily shared between the VNC nodes . In addition hardware and or software of VNC nodes may be sufficiently easily replaced providing satisfactory resource fungibility. Further distributed VNC may enable may enable horizontally scalable configuration and management which may give a single system view of the one or more virtual networks.

Each of VNC nodes may include substantially similar analogous components for performing substantially similar analogous functionality said functionality being described hereinafter primarily with respect to VNC node A. VNC node A may include an analytics database A for storing diagnostic information related to a first set of elements managed by VNC node A. Analytics database A may include The distributed virtual network controller may include a horizontally scalable network analytics database which may represent a fully integrated analytics collector configured to troubleshoot visualize and analyze distributed VNC and the one or more virtual networks. VNC node A may share at least some diagnostic information related to VNC node A and or one or more of the first set of elements managed by VNC node A and stored in analytics database as well as to receive at least some diagnostic information related to any of the elements managed by others of VNC nodes . Analytics database A may represent a distributed hash table DHT for instance or any suitable data structure for storing diagnostic information for network elements in a distributed manner in cooperation with others of VNC nodes . Analytics databases A N collectively analytics databases may represent at least in part one of distributed databases of distributed virtual network controller of .

VNC node A may include a configuration database A for storing configuration information related to a first set of elements managed by VNC node A. Control plane components of VNC node A may store configuration information to configuration database A using interface A which may represent an Interface for Metadata Access Points IF MAP protocol implementation. VNC node A may share at least some configuration information related to one or more of the first set of elements managed by VNC node A and stored in configuration database A including e.g. VNC node A as well as to receive at least some configuration information related to any of the elements managed by others of VNC nodes . Configuration database A may represent a distributed hash table DHT for instance or any suitable data structure for storing configuration information for network elements in a distributed manner in cooperation with others of VNC nodes . Configuration databases A N collectively configuration databases may represent at least in part one of distributed databases of distributed virtual network controller of . Configuration databases may store respective RIBs of . Portions of RIBs may be stored by control plane VMs to facilitate operation of network discovery modules and BGPs .

Virtual network controller may perform any one or more of the illustrated virtual network controller operations represented by modules which may include orchestration user interface VNC global load balancing and one or more applications . VNC executes orchestration module to facilitate the operation of one or more virtual networks in response to a dynamic demand environment by e.g. spawning removing virtual machines in data center servers adjusting computing capabilities allocating network storage resources and modifying a virtual topology connecting virtual switches of a virtual network. VNC global load balancing executed by VNC supports load balancing of analytics configuration communication tasks e.g. among VNC nodes . Applications may represent one or more network applications executed by VNC nodes to e.g. change topology of physical and or virtual networks add services or affect packet forwarding. In some instances a centralized network management system or other controller executes modules and communicates using a northbound interface of VNC nodes to perform orchestration configure VNC nodes perform VNC global load balancing and execute or VNC nodes with virtual network applications .

User interface includes an interface usable to an administrator or software agent to control the operation of VNC nodes . For instance user interface may include methods by which an administrator may modify e.g. configuration database A of VNC node A. Administration of the one or more virtual networks operated by VNC may proceed by uniform user interface that provides a single point of administration which may reduce an administration cost of the one or more virtual networks.

VNC node A may include a control plane virtual machine VM A that executes control plane protocols to facilitate the distributed VNC techniques described herein. Control plane VM A may in some instances represent a native process. In the illustrated example control VM A executes BGP A to provide information related to the first set of elements managed by VNC node A to e.g. control plane virtual machine N of VNC node N. Control plane VM A may use an open standards based protocol e.g. BGP based L3VPN to distribute information about its virtual network s with other control plane instances and or other third party networking equipment s . Given the peering based model according to one or more aspects described herein different control plane instances e.g. different instances of control plane VMs A N may execute different software versions. In one or more aspects e.g. control plane VM A may include a type of software of a particular version and the control plane VM N may include a different version of the same type of software. The peering configuration of the control node devices may enable use of different software versions for the control plane VMs A N. The execution of multiple control plane VMs by respective VNC nodes may prevent the emergence of a single point of failure.

Control plane VM A communicates with virtual network switches e.g. illustrated VM switch executed by server using a communication protocol operating over network . Virtual network switches facilitate overlay networks in the one or more virtual networks. In the illustrated example control plane VM A uses Extensible Messaging and Presence Protocol XMPP A to communicate with at least virtual network switch by XMPP interface A. Virtual network route data statistics collection logs and configuration information may in accordance with XMPP A be sent as XML documents for communication between control plane VM A and the virtual network switches. Control plane VM A may in turn route data to other XMPP servers such as an analytics collector e.g. analytics VM A or may retrieve configuration information on behalf of one or more virtual network switches. Control plane VM A may further execute a communication interface A for communicating with configuration virtual machine VM A associated with configuration database A. Communication interface A may represent an IF MAP interface. Server may represent an example instance of any of servers of or servers of with virtual network switch representing any of virtual switches and virtual network switch agent representing any of virtual network agents of for example.

VNC node A may further include configuration VM A to store configuration information for the first set of element to and manage configuration database A. Configuration VM A although described as a virtual machine may in some aspects represent a native process executing on an operating system of VNC node A. Configuration VM A and control plane VM A may communicate using IF MAP by communication interface A and using XMPP by communication interface A. In some aspects configuration VM A may include a horizontally scalable multi tenant IF MAP server and a distributed hash table DHT based IF MAP database represented by configuration database A. In some aspects configuration VM A may include a configuration translator which may translate a user friendly higher level virtual network configuration to a standards based protocol configuration e.g. a BGP L3VPN configuration which may be stored using configuration database A. Communication interface may include an IF MAP interface for communicating with other network elements. The use of the IF MAP may make the storage and management of virtual network configurations very flexible and extensible given that the IF MAP schema can be dynamically updated. Advantageously aspects of virtual network controller may be flexible for new applications .

VNC node A may further include an analytics virtual machine VM A to store diagnostic information and or visibility information related to at least the first set of elements managed by VNC node A. Control plane VM and analytics VM may communicate using an XMPP implementation by communication interface A. Analytics VM A although described as a virtual machine may in some aspects represent a native process executing on an operating system of VNC node A.

Analytics VM A may include analytics database A which may represent an instance of a distributed database that stores visibility data for virtual networks such as one of distributed database of distributed virtual network controller of . Visibility information may describe visibility of both distributed VNC and of customer networks. Analytics database A of analytics VM A may include an XMPP interface on a first southbound side and a REST JASON XMPP interface on a northbound second side by communication interface A.

Virtual network switch may implement the layer 3 forwarding and policy enforcement point for one or more end points and or one or more hosts. The one or more end points or one and or one or more hosts may be classified into a virtual network due to configuration from control plane VM A. Control plane VM A may also distribute virtual to physical mapping for each end point to all other end points as routes. These routes may give the next hop mapping virtual IP to physical IP and encapsulation technique used e.g. one of IPinIP NVGRE VXLAN etc. . Virtual network switch may be agnostic to actual tunneling encapsulation used. Virtual network switch may also trap interesting layer 2 L2 packets broadcast packets and or implement proxy for the packets e.g. using one of Address Resolution Protocol ARP Dynamic Host Configuration Protocol DHCP Domain Name Service DNS multicast DNS mDNS etc.

In some cases different VNC nodes may be provided by different suppliers. However the peering configuration of VNC nodes may enable use of different hardware and or software provided by different suppliers for implementing the VNC nodes of distributed VNC . A system operating according to the techniques described above may provide logical view of network topology to end host irrespective of physical network topology access type and or location. Distributed VNC may provide programmatic ways for network operators and or applications to change topology to affect packet forwarding and or to add services as well as horizontal scaling of network services e.g. firewall without changing the end host view of the network.

As shown in the specific example of computing device includes one or more processors one or more communication units one or more input devices one or more output devices and one or more storage devices . Computing device in the specific example of further includes operating system virtualization module and one or more applications A N collectively applications . Each of components and may be interconnected physically communicatively and or operatively for inter component communications. As one example in components and may be coupled by one or more communication channels . In some examples communication channels may include a system bus network connection interprocess communication data structure or any other channel for communicating data. Virtualization module and applications as well as operating system may also communicate information with one another as well as with other components in computing device .

Processors in one example are configured to implement functionality and or process instructions for execution within computing device . For example processors may be capable of processing instructions stored in storage devices . Examples of processors may include any one or more of a microprocessor a controller a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or equivalent discrete or integrated logic circuitry.

One or more storage devices may be configured to store information within computing device during operation. Storage devices in some examples are described as a computer readable storage medium. In some examples storage devices are a temporary memory meaning that a primary purpose of storage devices is not long term storage. Storage devices in some examples are described as a volatile memory meaning that storage devices do not maintain stored contents when the computer is turned off. Examples of volatile memories include random access memories RAM dynamic random access memories DRAM static random access memories SRAM and other forms of volatile memories known in the art. In some examples storage devices are used to store program instructions for execution by processors . Storage devices in one example are used by software or applications running on computing device e.g. operating system virtualization module and the like to temporarily store information during program execution.

Storage devices in some examples also include one or more computer readable storage media. Storage devices may be configured to store larger amounts of information than volatile memory. Storage devices may further be configured for long term storage of information. In some examples storage devices include non volatile storage elements. Examples of such non volatile storage elements include magnetic hard discs tape cartridges or cassettes optical discs floppy discs flash memories or forms of electrically programmable memories EPROM or electrically erasable and programmable memories EEPROM .

Computing device in some examples also includes one or more communication units . Computing device in one example utilizes communication units to communicate with external devices. Communication units may communicate in some examples by sending data packets over one or more networks such as one or more wireless networks via inbound and outbound links. Communication units may include one or more network interface cards IFCs such as an Ethernet card an optical transceiver a radio frequency transceiver or any other type of device that can send and receive information. Other examples of such network interfaces may include Bluetooth 3G and Wi Fi radio components.

Computing device in one example also includes one or more input devices . Input devices in some examples are configured to receive input from a user through tactile audio or video feedback. Examples of input devices include a presence sensitive display a mouse a keyboard a voice responsive system video camera microphone or any other type of device for detecting a command from a user. In some examples a presence sensitive display includes a touch sensitive screen.

One or more output devices may also be included in computing device . Output devices in some examples are configured to provide output to a user using tactile audio or video stimuli. Output devices in one example include a presence sensitive display a sound card a video graphics adapter card or any other type of device for converting a signal into an appropriate form understandable to humans or machines. Additional examples of output devices include a speaker a cathode ray tube CRT monitor a liquid crystal display LCD or any other type of device that can generate intelligible output to a user.

Computing device may include operating system . Operating system in some examples controls the operation of components of computing device . For example operating system in one example facilitates the communication of modules applications with processors communication units input devices output devices and storage devices . Applications may each include program instructions and or data that are executable by computing device . As one example application A may include instructions that cause computing device to perform one or more of the operations and actions described in the present disclosure.

In accordance with techniques of the present disclosure computing device may further include virtual switch and virtual network agent which may be executed by executed on virtualization module operating as a hypervisor or on a native operating system of computing device . Virtual switch and virtual switch agent may execute virtual switch and virtual network switch agent of respectively. Virtual switch may implement the layer 3 forwarding and policy enforcement point for one or more end points and or one or more hosts e.g. VMs executing on computing device . The one or more end points or one and or one or more hosts may be classified into a virtual network due to configuration information received from a virtual network controller such as VNC of .

The techniques described herein may be implemented in hardware software firmware or any combination thereof. Various features described as modules units or components may be implemented together in an integrated logic device or separately as discrete but interoperable logic devices or other hardware devices. In some cases various features of electronic circuitry may be implemented as one or more integrated circuit devices such as an integrated circuit chip or chipset.

If implemented in hardware this disclosure may be directed to an apparatus such a processor or an integrated circuit device such as an integrated circuit chip or chipset. Alternatively or additionally if implemented in software or firmware the techniques may be realized at least in part by a computer readable data storage medium comprising instructions that when executed cause a processor to perform one or more of the methods described above. For example the computer readable data storage medium may store such instructions for execution by a processor.

A computer readable medium may form part of a computer program product which may include packaging materials. A computer readable medium may comprise a computer data storage medium such as random access memory RAM read only memory ROM non volatile random access memory NVRAM electrically erasable programmable read only memory EEPROM Flash memory magnetic or optical data storage media and the like. In some examples an article of manufacture may comprise one or more computer readable storage media.

In some examples the computer readable storage media may comprise non transitory media. The term non transitory may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM or cache .

The code or instructions may be software and or firmware executed by processing circuitry including one or more processors such as one or more digital signal processors DSPs general purpose microprocessors application specific integrated circuits ASICs field programmable gate arrays FPGAs or other equivalent integrated or discrete logic circuitry. Accordingly the term processor as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition in some aspects functionality described in this disclosure may be provided within software modules or hardware modules.

Although the terms first second third etc. may be used herein to describe various signals elements components regions layers and or sections these signals elements components regions layers and or sections should not be limited by these terms. These terms may be used to distinguish one signal element component region layer or section from another signal region layer or section. Thus a first signal element component region layer or section discussed below may be termed a second signal element component region layer or section without departing from the teachings of the present invention. The description of an element as first does not imply that second or other elements are needed. The terms first second third etc. may also be used herein to differentiate different categories of elements. For conciseness the terms first second third etc. may represent first category second category third category etc. respectively.

Various embodiments have been described. These and other embodiments are within the scope of the following examples.

