---

title: Method and apparatus of adaptive textual prediction of voice data
abstract: Typical textual prediction of voice data employs a predefined implementation arrangement of a single or multiple prediction sources. Using a predefined implementation arrangement of the prediction sources may not provide a good prediction performance in a consistent manner with variations in voice data quality. Prediction performance may be improved by employing adaptive textual prediction. According to at least one embodiment determining a configuration of a plurality of prediction sources, used for textual interpretation of the voice data, is determined based at least in part on one or more features associated with the voice data or one or more a-priori interpretations of the voice data. A textual output prediction of the voice data is then generated using the plurality of prediction sources according to the determined configuration. Employing an adaptive configuration of the text prediction sources facilitates providing more accurate text transcripts of the voice data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09099091&OS=09099091&RS=09099091
owner: Nuance Communications, Inc.
number: 09099091
owner_city: Burlington
owner_country: US
publication_date: 20130122
---
Speech to text transcription is commonly used in many applications. The transcription is usually performed by a human agent. However the use of human agents to transcribe voice data to text is costly and sometimes the transcription quality is less than satisfactory. With significant advances in speech recognition and language modeling tools machine based solutions for speech to text transcription are becoming a reality. Such solutions may be used in combination with a human agent or separately.

According to at least one embodiment a computerized method or a corresponding apparatus for performing adaptive textual prediction of voice data comprise determining a configuration of a plurality of prediction sources used for textual interpretation of the voice data based at least in part on one or more features associated with the voice data or one or more a priori interpretations of the voice data and generating a textual output prediction of the voice data using the plurality of prediction sources according to the configuration determined.

The method further comprises extracting the one or more features associated with the voice data or the one or more a priori interpretations of the voice data. The one or more features include a signal to noise ratio associated with the voice data complexity measure of a lattice representing at least one a priori interpretation of the voice data or an a priori interpretation of the voice message generated by a human agent. The multiple prediction sources include a language model module lattice decoder module or a human agent. The textual output prediction may be provided to a human agent to facilitate generating a final transcript of the voice data. Alternatively the textual output prediction may be used as the final transcript of the voice data.

In determining the configuration of a plurality of prediction sources an order according to which the multiple prediction sources are to be applied is determined weightings associated with the multiple prediction sources are determined or a subset of the plurality of prediction sources for use in generating the textual output prediction is determined. A representation of the determined configuration may be sent to another device or stored in a database. A database storing a representation of a previous configuration of the plurality of prediction sources may be updated based on the configuration determined. A representation of the determined configuration includes an indication of an order according to which the multiple prediction sources being applied indication of weightings associated with the multiple prediction sources or indication of a subset of the plurality of prediction sources for use in generating the textual output prediction.

In transcribing voice data into text the use of human agents alone may be costly and of poor quality sometimes. Agents transcribing hours long voice data may be under strict time constraints. The voice data may not always have good audio quality. Such factors may result in unsatisfactory transcription results. To address the issues of cost and quality in speech to text transcription computer based text prediction tools are employed.

Speech recognition applications known in the art may not provide text transcripts corresponding to input speech signals. Instead an output of a speech recognition application may be in the form of statistical data illustrating different potential interpretations of a respective input speech signal. In addition speech recognition applications may process a speech signal on a per utterance or per phoneme basis and may not consider linguistic rules or the context of the speech or conversation associated with the input speech signal. Therefore the output of a speech recognition application is usually fed to a text prediction source to generate a text prediction corresponding to the input speech signal. A single source or multiple text prediction sources may be applied to a prior text interpretation e.g. output of a speech recognition application or a transcript by an agent of a speech signal. While the use of multiple prediction sources usually results in better performance than using a single prediction source a single arrangement of how such multiple prediction sources are employed may not provide equally good performance under different conditions. In the following different embodiments of adaptation of multiple text prediction sources are described.

In generating the output text prediction the adaptive text prediction module is configured to employ multiple text prediction sources or tools to the a priori text interpretations. According to at least one example embodiment the multiple prediction sources are employed according to adaptable configuration s . Specifically the adaptive text prediction module includes an adaptation module configured to determine a configuration of the multiple prediction sources based on features associated with the voice data the text data generated by the speech recognition module the text transcript or a combination thereof. The adaptive text prediction module also includes an execution module configured to execute the determined configuration of the multiple text prediction sources.

The features may be provided to the adaptation module by a feature extraction module . The feature extraction module extracts the features from voice data text data generated by the speech recognition module text transcript provided by the first agent or a combination thereof. Examples of the features include for example signal to noise ratio of the voice data characteristics of the speech recognition module output a measure of the accuracy or quality of text transcript or the like.

Based on the received features the adaptation module determines the configuration of the multiple prediction sources. According to one scenario the adaptation module may analyze the features to generate further parameters for use in determining the configuration of the multiple prediction sources. Alternatively the adaptation module may map the received features to a particular configuration based on for example a mapping table. According to yet another scenario the adaptation module may rank or assign a priority value to each of the multiple text prediction sources based on the received features and then determine a configuration based on the ranking or priority values assigned to each text prediction source. The ranking or priority values may be indicative of which text prediction source is to be employed the order with which a text prediction source is applied a weighting to be assigned to the output of a text prediction source or the like.

According to at least one example embodiment the adaptive text prediction module is coupled to a database . The database may store configuration parameters associated with each configuration implementations of different configurations pointers or application programming interfaces APIs for implementations of the different configurations or the like. The database may alternatively or in addition store APIs or implementations of the multiple text prediction sources. As such a particular configuration may be implemented on the fly by the adaptive text prediction module using the stored APIs or implementations of the multiple text prediction sources.

Upon determining a configuration of the multiple text prediction sources to be employed the adaptation module may inform or instruct an execution module about the determined configuration. The execution module is configured to receive the text data the text transcript or both. The execution module then applies the determined configuration of the plurality of text prediction sources to one or more of the received a priori text interpretations e.g. and . Instructions from the adaptation module may further include an indication of the a priori text interpretation s to be used. Alternatively such indication may be inherent in the configuration determined or selected by the adaptation module. The execution module may further be configured to retrieve the pointer s API s or implementation s of the selected configuration or of the respective multiple text predictions from the database .

By executing the selected configuration of the multiple text prediction sources the execution module generates an output text prediction . The output text prediction may be used as a final text transcript of the voice data . Alternatively the output text prediction may be presented to a second agent through an agent device to facilitate the generation of the final transcript of the voice data by the second agent. In other words the second agent may be provided with the voice data audio and the output text prediction . According to an example scenario the output text prediction may be used for example as part of an interactive tool which provides or displays prediction s of a next word as the second agent types the text transcript of the voice data. According to another scenario the output text prediction may be presented to the second agent as a document to be reviewed and edited by the second agent while listening to the voice data audio.

In a sequential configuration such as of the multiple text prediction sources the order of the different text prediction sources is important. For example in the configuration the LM module is applied first and the lattice decoder module is applied second. In an alternative sequential configuration the lattice decoder module is applied first followed by the LM module . The order of the multiple text prediction sources or the corresponding configuration is determined by the adaptation module based on features such as the complexity of the lattice the signal to noise ratio of the voice data the text transcript provided by the first agent or a combination thereof. With regard to the complexity of the lattice the more uncertainty is associated with the output of the speech recognition module the more complex the lattice is and the simpler the lattice is the more reliable the output of the speech recognition module is. In other words the complexity of the lattice may be viewed as a measure of the reliability of the lattice.

According to at least one example embodiment if the lattice is determined e.g. based on a complexity measure to be simple the lattice decoder module is applied in the beginning of a sequential configuration. Considering a configuration having a LM module and a lattice decoder module for example the order of the LM module and the lattice decoder module would be reversed compared to the configuration in when the lattice is found to be relatively simple. According to one scenario a complexity measure of the lattice is the difference between the likelihood or probability scores associated with the best interpretation e.g. path with highest probability and the second best interpretation e.g. path with second highest probability in the lattice. The larger the difference between such scores the simpler the lattice is and vise versa. According to another scenario an alternative complexity measure of the lattice may be the total number of paths in the lattice . A person of ordinary skill in the relative art should appreciate that other complexity measures may be used.

The adaptation module may also use the signal to noise ratio of the voice data to determine the configuration of the text prediction sources or the order of the text prediction sources within the configuration. A high signal to noise ratio of the voice data may lead to reliable performance by the speech recognition module and thus may be indicative of a reliable lattice . As such in the case of a high signal to noise ratio the lattice decoder module precedes the LM module in the determined configuration. If the signal to noise is low the LM module precedes the lattice decoder module in the configuration determined by the adaptation module . In addition a high signal to noise ratio may also be indicative of reliability of the text transcript generated by the first agent if available.

The weighting module uses features provided by the feature extraction module to determine the weights to be applied to each of the text predictions e.g. and or transcripts e.g. provided to the weighting module . Such features include for example the signal to noise ratio of the voice data the complexity of the lattice a measure of the accuracy or quality of the text transcript or the like. The weighting module may further use other criteria in applying weighting or assigning scores to each of the text predictions e.g. and or transcripts e.g. provided to the weighting module . For example each of the text prediction sources e.g. and may generate more than one text prediction e.g. and . In such case the weighting module may assign high scores or apply large weighting to a text prediction or a portion of a text prediction that is the output of more than one text prediction source. For example a text prediction or a sentence therein that appears in the output of the LM module and the lattice decoder module is assigned a relatively higher score than another text prediction or a sentence therein that appears in the output of a single text prediction source among and . The weighting module may process text predictions or portions of a text prediction when applying weightings or assigning priority scores.

In the case of a high signal to noise ratio of the voice data text prediction s generated by the lattice decoder module is are assigned higher priority scores or larger weightings than text prediction s generated by the LM module . In the case of a low signal to noise ratio of the voice data however text prediction s generated by the LM module is are assigned higher priority scores or larger weightings than text prediction s generated by the lattice decoder module . Text prediction s generated by the lattice decoder module may also be assigned relatively high score s or relatively large weighting s if the lattice has low complexity. However if the lattice is determined to be complex based on a complexity measure the text prediction s generated by the lattice decoder module is assigned relatively low score s or relatively small weighting s . The transcript if provided to the weighting module may also be assigned a score or weighting. The weightings or scores are provided by the adaptation module to the weighting module

The table in illustrates examples scores or weightings used in the case of high signal to noise ratio of the voice data . The first column corresponds to the case where no text transcript is provided by the first agent and the second column corresponds to the case where a text transcript is provided by the first agent. Relatively high scores are assigned in this case to outputs from both the LM module and the lattice decoder module

It should be understood that the example embodiments described above may be implemented in many different ways. In some instances the various methods and machines described herein may each be implemented by a physical virtual or hybrid general purpose or application specific computer having a central processor memory disk or other mass storage communication interface s input output I O device s and other peripherals. The general purpose or application specific computer is transformed into the machines that execute the methods described above for example by loading software instructions into a data processor and then causing execution of the instructions to carry out the functions described herein.

As is known in the art such a computer may contain a system bus where a bus is a set of hardware lines used for data transfer among the components of a computer or processing system. The bus or busses are essentially shared conduit s that connect different elements of the computer system e.g. processor disk storage memory input output ports network ports etc. that enables the transfer of information between the elements. One or more central processor units are attached to the system bus and provide for the execution of computer instructions. Also attached to the system bus are typically I O device interfaces for connecting various input and output devices e.g. keyboard mouse displays printers speakers etc. to the computer. Network interface s allow the computer to connect to various other devices attached to a network. Memory provides volatile storage for computer software instructions and data used to implement an embodiment. Disk or other mass storage provides non volatile storage for computer software instructions and data used to implement for example the various procedures described herein.

Embodiments may therefore typically be implemented in hardware firmware software or any combination thereof.

In certain embodiments the procedures devices and processes described herein constitute a computer program product including a computer readable medium e.g. a removable storage medium such as one or more DVD ROM s CD ROM s diskettes tapes etc. that provides at least a portion of the software instructions for the system. Such a computer program product can be installed by any suitable software installation procedure as is well known in the art. In another embodiment at least a portion of the software instructions may also be downloaded over a cable communication and or wireless connection.

Embodiments may also be implemented as instructions stored on a non transitory machine readable medium which may be read and executed by one or more processors. A non transient machine readable medium may include any mechanism for storing or transmitting information in a form readable by a machine e.g. a computing device. For example a non transient machine readable medium may include read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices and others.

Further firmware software routines or instructions may be described herein as performing certain actions and or functions of the data processors. However it should be appreciated that such descriptions contained herein are merely for convenience and that such actions in fact result from computing devices processors controllers or other devices executing the firmware software routines instructions etc.

It also should be understood that the flow diagrams block diagrams and network diagrams may include more or fewer elements be arranged differently or be represented differently. But it further should be understood that certain implementations may dictate the block and network diagrams and the number of block and network diagrams illustrating the execution of the embodiments be implemented in a particular way.

Accordingly further embodiments may also be implemented in a variety of computer architectures physical virtual cloud computers and or some combination thereof and thus the data processors described herein are intended for purposes of illustration only and not as a limitation of the embodiments.

While this invention has been particularly shown and described with references to example embodiments thereof it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the scope of the invention encompassed by the appended claims.

