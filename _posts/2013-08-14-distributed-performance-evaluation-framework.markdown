---

title: Distributed performance evaluation framework
abstract: A distributed performance evaluation framework provides functionality for evaluating the availability, reliability, and scalability of a network-based service. The framework includes a control and reporting tier and a load-generating tier. The control and reporting tier provides a control interface through which a request to evaluate the performance of a service may be received. In response to receiving such a request, the control and reporting tier creates a load-generating job for use by the load-generating tier. Load-generating instances in the load-generating tier are configured to perform the load-generating job by executing instances of a load-generating plug-in configured to generate requests to the service. The load-generating instances also periodically provide data regarding the status of each load-generating job to the control and reporting tier.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09455887&OS=09455887&RS=09455887
owner: Amazon Technologies, Inc.
number: 09455887
owner_city: Seattle
owner_country: US
publication_date: 20130814
---
Prior to the deployment of a network based service it is important for the developer of such a service to have a high level of confidence that the service can withstand the network traffic likely to be received by the service when placed into production. For a variety of reasons however it can be complex and expensive to simulate the network traffic likely to be received by a production network based service. This is especially true for large scale network based services that are architected to handle extremely high volumes of requests. Moreover systems might not scale according to predictive models when under the load of actual requests. As a result it may be difficult for a developer to test the availability reliability and scalability of a network based service prior to deployment.

The following detailed description is directed to concepts and technologies for providing a distributed performance evaluation framework. Utilizing an implementation of the technologies described herein a developer can quickly and easily evaluate the availability reliability and scalability of a network based service. Additionally by utilizing infrastructure on demand to perform the performance evaluation process disclosed herein even large scale network based services can be stress tested at a cost that is likely to be lower than previous solutions. Moreover such testing can be performed regardless of the protocol or protocols utilized by the network based service. The testing processes described herein might be utilized to provision computing resources in advance of the deployment to an actual production workload.

According to one particular embodiment the distributed performance evaluation framework which may be referred to herein as the framework includes a control and reporting tier and a separate load generating tier. As will be described in greater detail below the control and reporting tier and the load generating tier may be implemented in a distributed fashion such that the operation of each tier is scalable and such that the failure of components in one tier will not cause components in the other tier to also fail. The control and reporting tier and the load generating tier may be executed on the same or different hardware or virtual machine instances.

In one implementation one or more hardware computing systems or virtual machine instances are utilized to implement the control and reporting tier. The control and reporting tier provides a control interface such as a user interface UI or an application programming interface API such as a Web service API through which a request to evaluate the performance of a service which may be referred to herein as the service under test or SUT may be received. The request is expressed independently of the computing systems i.e. the load generating tier that are utilized to generate a load for evaluating the performance of the service. In this way the requestor does not need to be concerned about the configuration and or operation of the underlying computing systems that are utilized to test the service.

In one implementation a request includes a load generating plug in or data identifying the location of such a plug in. The request might also include load generating data or data identifying the location of the load generating data. The request might also identify a desired number of requests per time period e.g. requests per second that should be submitted to the SUT as well as data describing the condition or conditions under which the testing should be stopped e.g. a specified time period has elapsed or a specified number of requests have been submitted to the SUT . As will be described in greater detail below the load generating plug in is configured to communicate with the SUT using a protocol supported by the SUT. The load generating plug in is also configured to use the load generating data to generate requests to the SUT in some embodiments. It should be appreciated that in other embodiments the load generating plug in may be configured to generate requests to the SUT without the use of load generating data.

In response to receiving a request to evaluate the performance of a SUT such as the request described above the control and reporting tier is configured to create a load generating job for use by the load generating tier. In particular a load generating job may be created that identifies the load generating plug in the load generating data the desired number of requests per time period and the condition or conditions under which the test should be discontinued. The load generating job may then be placed on a job queue for consumption by one or more load generating instances. Mechanisms other than a queue might also be utilized to provide load generating jobs to components in the load generating tier.

Load generating instances in the load generating tier are configured to execute instances of the load generating plug in to generate requests to the SUT. The load generating instances may be implemented as hardware devices or might be implemented as virtual machine instances. When implemented as virtual machine instances the load generating instances might be provided by a service provider operated network based distributed computing environment which may be referred to herein as a service provider network . Using facilities provided by such a service provider network virtual machine instances can be provisioned and utilized as needed to generate requests to the SUT. The virtual machine instances may then be de provisioned once testing has been completed. By using computing resources for testing the SUT on an as needed basis the cost of evaluating the performance of a network based service can be minimized. Additional details regarding the use of computing resources provided by a service provider network to evaluate the performance of a network based service will be provided below.

The load generating instances operating in the load generating tier might also be configured to periodically examine the job queue to determine if a load generating job is available. If a load generating job is available a load generating instance with available processing capacity will dequeue the load generating job from the job queue. The load generating instance will then execute instances of the load generating plug in to generate requests to the SUT. As discussed briefly above the load generating plug in utilizes the load generating data to generate the requests to the SUT. The load generating instance also determines if it is supplying the desired number of requests per time period specified by the load generating job to the SUT. If the load generating instance is supplying the desired number of requests per time period specified by the load generating job to the SUT the load generating instance will continue executing the instances of the load generating plug ins to generate requests to the SUT until the specified condition or conditions for the load generating job to be stopped have been reached.

For various reasons a single load generating instance may be unable to supply the desired number of requests per time period specified by the load generating job to the SUT. For example and without limitation a single load generating instance may be unable to supply the desired number of requests per time period due to limited computational power memory or network bandwidth.

If a load generating instance is not supplying the desired number of requests per time period specified by the load generating job to the SUT the load generating instance will enable other load generating instances to also submit requests to the SUT. For example in one embodiment a load generating instance may place a load generating job on the job queue for the unmet portion of the desired number of requests. Another load generating instance may then retrieve the load generating job from the job queue and begin submitting requests to the same SUT using the load generating plug in in the manner described above. Load generating instances may be added in this manner until the desired number of requests per time period are being submitted to the SUT. Once the specified condition or conditions for stopping the evaluation of the SUT has been reached execution of the instances of the load generating plug in are stopped.

It should be appreciated that when a load generating instance places a job on the job queue for the unmet portion of a desired number of requests as described above it also reduces its own target number of requests per time period by the same amount. This prevents a situation where a resource bottleneck is removed on the original load generating instance and the combined load generating resources generate more requests per time period than intended as a result. For example one load generating instance may be only able to generate six requests per second out of a requested ten due to a bad network connection or other problem. The load generating instance therefore reduces its target to six requests per second and places a job on the job queue asking another load generating instance to generate four requests per second. In this example the bad network connection may be repaired later but the first load generating instance correctly continues to generate only six requests per second keeping to a total of ten requests per second.

In some embodiments the load generating instances are also configured to periodically provide data regarding the status of each load generating job to the control and reporting tier. For example in one embodiment the load generating instances write job status information to a database table referred to herein as the job status table indicating that a job is processing running is stopping has been stopped is deleting or is complete. The control and reporting tier might provide an interface such as a UI or an API through which the job status information may be retrieved. Mechanisms other than a database table may be utilized to communicate information regarding the status of load generating jobs from the load generating tier to the control and reporting tier. Additional details regarding the various components and processes described above for providing a distributed performance evaluation framework will be presented below with regard to .

It should be appreciated that the subject matter presented herein may be implemented as a computer process a computer controlled apparatus a computing system or an article of manufacture such as a computer readable storage medium. While the subject matter described herein is presented in the general context of program modules that execute on one or more computing devices those skilled in the art will recognize that other implementations may be performed in combination with other types of program modules. Generally program modules include routines programs components data structures and other types of structures that perform particular tasks or implement particular abstract data types.

Those skilled in the art will also appreciate that aspects of the subject matter described herein may be practiced on or in conjunction with other computer system configurations beyond those described herein including multiprocessor systems microprocessor based or programmable consumer electronics minicomputers mainframe computers handheld computers personal digital assistants e readers cellular telephone devices special purposed hardware devices network appliances and the like. As mentioned briefly above the embodiments described herein may be practiced in distributed computing environments where tasks may be performed by remote computing devices that are linked through a communications network. In a distributed computing environment program modules may be located in both local and remote memory storage devices.

In the following detailed description references are made to the accompanying drawings that form a part hereof and that show by way of illustration specific embodiments or examples. The drawings herein are not drawn to scale. Like numerals represent like elements throughout the several figures which may be referred to herein as a FIG. or FIGS. .

The control and reporting tier may be configured to provide a control interface such as a UI or an API such as a Web service API which may be referred to herein as a load generating Web service API through which a request not shown in to evaluate the performance of a SUT may be received. In response to receiving such a request the control interface is configured to transmit a control command to the load generating tier instructing the load generating tier to perform the requested evaluation of the SUT . In one embodiment for instance the control interface creates a load generating job not shown in for use by the load generating tier .

As mentioned above the load generating tier includes one or more load generating instances A C which might be referred to herein singularly as as load generating instance or collectively as the load generating instances . As discussed briefly above the load generating instances may be implemented as hardware devices or might be implemented as virtual machine instances. When implemented as virtual machine instances the load generating instances might be provided by a service provider network not shown in . In this way the load generating instances can be created and utilized as needed to generate requests to the SUT. The load generating instances may then be de provisioned once testing has been completed.

The load generating instances receive load generating jobs from the control interface and utilize the load generating jobs to generate requests A C which might be referred to herein singularly as a request or collectively as the requests to the SUT . The requests might be requests for the SUT to perform some type of processing to provide some type of data in response to the request such as a text image or video file or other types of requests. As will be described in greater detail below the load generating instances may be configured to communicate with one another to ensure that a desired number of requests are being provided to the SUT . Moreover additional load generating instances may be added to the load generating tier until the desired number of requests per time period are being provided to the SUT . The load generating instances will continue submitting requests to the SUT until one or more conditions specified by a load generating job have occurred or a request to stop testing has been received. The load generating instances will then stop submitting requests to the SUT and may be de provisioned in certain embodiments. Additional details regarding this process will be provided below.

The control interface might also be configured to receive and provide other types of control commands to the load generating tier . For example the control interface might receive control commands for modifying stopping or deleting load generating jobs. Other types of control commands might also be received and provided to the load generating tier . In response to receiving such control commands the load generating instances in the load generating tier will take appropriate action such as modifying stopping or deleting a load generating job.

As also mentioned briefly above the load generating instances in the load generating tier might also be configured to periodically provide status information regarding the status of each load generating job to the control and reporting tier . For example the load generating instances might be configured to provide status information to the control and reporting tier indicating that a job is processing running is stopping has been stopped is deleting or is complete. The control interface in the control and reporting tier might also provide a reporting interface such as a UI or an API through which the job status information may be retrieved.

As shown in the SUT is configured in some embodiments to provide feedback to the load generating tier and or the control and reporting tier . For instance the SUT might provide feedback to the load generating tier and or the control and reporting tier indicating the state of its performance. As a specific example the SUT might provide feedback indicating that it has exceeded a certain performance threshold e.g. percentage of memory or central processing unit CPU utilization . Various actions might then be performed in response to receiving the feedback . For example the submission of requests to the SUT might be stopped. Alternately the volume of requests being submitted to the SUT might be modified based upon the provided feedback . Other actions might also be taken based upon the feedback provided by the SUT . In some implementations the feedback might also indicate that the SUT has scaled its capacity in response to the demand placed upon it by the load generating tier . The load generating tier and or the control and reporting tier might take various actions based upon receiving this type of feedback from the SUT .

Additional details regarding the configuration and operation of the control and reporting tier will be provided below with regard to . Additional details regarding the configuration and operation of the load generating tier will be provided below with regard to . Additional details regarding the configuration and operation of the load generating instances will be provided below with regard to . Additional details regarding a service provider network that may be utilized to implement components in the control and reporting tier components in the load generating tier and or the SUT will be described in greater detail below with regard to .

In some implementations the request also specifies a desired number of requests per time period e.g. requests per second or other time period that the load generating instances are to submit to the SUT . The request might also specify a condition or conditions under which the testing should be stopped. For example the request might specify that the requests should be submitted to the SUT for a specified period of time or until a certain number of requests have been submitted to the SUT . The request might also include other job parameters in other embodiments. For example and without limitation the other job parameters might specify a desired amount of stress or resource usage that should be applied to the SUT . In this example the control and reporting tier might monitor feedback from the SUT indicating the current resource utilization and or stress level of the SUT . The control and reporting tire might discontinue testing when the desired stress level or resource utilization level has been reached.

In some implementations the request specifies a rate of change for the requests . For example the request might specify that the requests be gradually ramped up from zero requests per minute to one thousand requests per minute over the course of an hour. The request might further specify that the number of requests then stay constant for some period of time. The request might then specify that the number of requests being submitted to the SUT be ramped back down to zero over the course of some other time period. In this way a distribution might be specified for the number of requests to be submitted to the SUT over the course of a period of time. In this regard it should be appreciated that the volume of requests to be delivered to the SUT might also be specified in other ways in other embodiments.

In response to receiving a request to evaluate the performance of the SUT a component in the control and reporting tier such as the control interface is configured to create a load generating job for use by the load generating tier . The load generating job may identify the location of the load generating plug in the location of the load generating data the desired number of requests per time period and the condition or conditions under which the load generating job should be discontinued. The load generating job might also include one or more other job parameters .

A component within the control and reporting tier such as the control interface may then place the load generating job on a job queue not shown in in the load generating tier for consumption by one or more of the load generating instances . Mechanisms other than a queue might also be utilized to provide load generating jobs to components in the load generating tier . Additional details regarding this process will be provided below with regard to .

The load generating instances operating in the load generating tier are also configured to periodically examine the job queue to determine if a load generating job is available on the queue . If a load generating job is on the job queue an existing load generating instance with available processing capacity will dequeue the load generating job from the job queue . The load generating instance will then execute instances of the load generating plug in identified by the dequeued load generating job to generate requests to the SUT . Each load generating instance might include framework components for hosting and executing the load generating plug in .

In the example shown in for instance the load generating instance A has dequeued the load generating job from the job queue . The load generating instance A then executes instances of the load generating plug in specified by the load generating job to generate the requests B to the SUT . As mentioned above the load generating plug in is configured to communicate with the SUT using a protocol supported by the SUT . The load generating plug in generates the requests using the load generating data specified in the load generating job .

As discussed briefly above the load generating instances may be implemented as hardware devices or might be implemented as virtual machine instances. When implemented as virtual machine instances the load generating instances might be provided by a service provider network. Using facilities provided by a service provider network virtual machine instances can be instantiated and utilized as needed to generate requests to the SUT . The virtual machine instances may then be de provisioned once testing has been completed. Additional details regarding a service provider network configured to provide this functionality will be provided below with regard to .

Each load generating instance is also configured to determine if it is supplying the desired number of requests per time period specified by the load generating job to the SUT . If a load generating instance is supplying the desired number of requests per time period specified by a load generating job to the SUT the load generating instance will continue executing the instances of the load generating plug in to generate the requests to the SUT until the specified conditions for the load generating job to be stopped have been reached or until a control command is received indicating that the job should be stopped.

If a load generating instance is not supplying the desired number of requests per time period specified by a load generating job to the SUT the load generating instance will enable other load generating instances to also submit requests to the SUT . For example in one embodiment a load generating instance may place a new load generating job on the job queue for the unmet portion of the desired number of requests . Another load generating instance may then retrieve the load generating job from the job queue and begin submitting requests to the same SUT using the load generating plug in in the manner described above.

In the example shown in for instance the load generating instance A has determined that it is not supplying the desired number of requests specified in the load generating job to the SUT . As a result the load generating instance A has placed a new load generating job A on the job queue for the unmet number of requests . In turn another load generating instance B has retrieved the load generating job A from the job queue . The load generating instance B then utilizes the load generating plug in specified by the load generating job A to submit requests A to the SUT . Load generating instances may be added in this manner until the desired number of requests per time period are being submitted to the SUT . Once the specified condition or conditions for stopping the evaluation of the SUT has been reached or a control command to stop execution has been received execution of the instances of the load generating plug in are stopped.

As discussed briefly above the load generating instances might also be configured to periodically provide job status data regarding the status of each load generating job that is being executed to the control and reporting tier . For example in one embodiment the load generating instances write data to a job status table indicating that a load generating job is processing running is stopping has been stopped is deleting or is complete. The control and reporting tier might provide a reporting interface such as a UI or an API through which the job status information contained in the job status table may be retrieved. Mechanisms other than a database table may be utilized to communicate information regarding the status of load generating jobs from the load generating tier to the control and reporting tier.

As shown in each load generating instance might also maintain an in memory job queue . When one of the load generating threads dequeues a load generating job from the job queue a number of jobs are placed on the in memory task queue equal to the number of desired requests per time period specified by the load generating job . Instances of the load generating plug in are then executed. The instances of the load generating plug in then dequeue jobs from the in memory task queue and for each job submit a request to the SUT . Once a request has been completed an instance of the load generating plug in dequeues another job from the in memory task queue .

In some embodiments the load generating job specifies the maximum number of load generating threads that may be executed by a load generating instance at a time. In this embodiment instances of the load generating plug in may be created up to the maximum number of threads specified by the load generating job and utilized in the manner described above. It should be appreciated that the software architecture shown in is merely illustrative and that each load generating instance might be configured differently in other embodiments. Additional details regarding the operation of the load generating instances will be provided below with regard to .

It should be appreciated that in some implementations the same load generating instance might be utilized to generate requests on behalf of different users or entities. In these implementations mechanisms might be implemented at each load generating instance to ensure that the data associated with each user is inaccessible to other users. For example partitioning schemes encryption mechanisms and other technologies might be utilized to ensure that one user of a load generating instance cannot view or otherwise access the data of another user.

It should also be appreciated that the technologies described herein might also be utilized with other types of systems and technologies for introducing failures into a computing network. For example another system might be utilized in conjunction with the technologies disclosed herein that injects network packet loss latency timeouts and other types of network failures. In this way the operation of an SUT under different types of request loads and simulated network problems can be evaluated.

The routine begins at operation where a user authors a load generating plug in . As discussed above the load generating plug in is configured to communicate with a SUT using an appropriate protocol. The load generating plug in may be authored in virtually any declarative or scripted programming language. For example in some embodiments the load generating plug in is authored using JAVA. The load generating plug in might also be authored using other languages in other embodiments.

From operation the routine then proceeds to operation where a user authors the load generating data . As discussed above the load generating plug in utilizes the load generating data to create the requests for submission to the SUT . From operation the routine proceeds to operation .

At operation the control interface described above may be utilized to define a request to evaluate the performance of the SUT . As described above with regard to the request might be defined through a UI or an API provided by the control interface . As also mentioned above the request might include data identifying a network location of the location of a load generating plug in or the load generating plug in itself. The request might also include the load generating data or data identifying the location of the load generating data . The request might also specify a desired number of requests per time period that the load generating instances are to submit to the SUT and a condition or conditions under which the testing should be stopped.

From operation the routine proceeds to operation where the control interface or another component within the control and reporting tier creates a load generating job based upon the received request . The routine then continues to operation where the control interface or another component within the control and reporting tier places the newly created load generating job on the job queue . The load generating instances may then dequeue the load generating job and process the job in the manner described herein. From operation the routine proceeds to operation where it ends.

At operation the load generating instance examines the job queue to determine if a load generating job is available. If there are no load generating jobs on the job queue the routine proceeds from operation to operation described below. If there is at least one load generating job on the job queue the routine proceeds from operation to operation .

At operation the load generating instance dequeues a load generating job from the job queue . The load generating instance might also update an entry in the job status table for the job. For example the load generating instance might update an entry in the job status table to indicate that the job has been taken off of the job queue . The routine then proceeds from operation to operation where the load generating instance executes the load generating plug in specified by the dequeued load generating job to generate the requests to the SUT . As discussed above the load generating instance might execute a number of load generating threads to execute the instances of the load generating plug in . The load generating threads may be executed separately from the reporting monitoring threads .

From operation the routine proceeds to operation where the load generating instance may update an entry in the job status table for the job. For example the load generating instance might update an entry in the job status table to indicate the number of requests that are being submitted to the SUT . As mentioned above the load generating instance might execute one or more reporting monitoring threads to update the job status table . The reporting monitoring threads are executed separately from the load generating threads responsible for generating the requests to the SUT in one embodiment.

From operation the routine proceeds to operation where the load generating instance determines if the desired number of requests per time period are being delivered to the SUT . If the desired number of requests per time period are being delivered to the SUT the routine proceeds from operation to operation described below. If the desired number of requests per time period are not being delivered to the SUT the routine proceeds from operation to operation .

At operation the load generating instance creates a new load generating job on the job queue for the unmet number of requests per time period. As discussed above the load generating instance also reduces its own target number of requests per time period by the same amount. In this way another load generating instance can dequeue the job and assist in providing the desired number of requests per time period to the SUT. From operation the routine proceeds to operation .

At operation the load generating instance determines whether a load generating job has been completed. For example and as described above a load generating job might be completed after a certain amount of time has elapsed after a certain number of requests have been delivered to the SUT or after another condition or conditions have occurred. If a load generating job has completed the routine proceeds from operation to operation described below. If a load generating job has not completed the routine proceeds from operation to operation .

At operation the load generating instance determines if a control command has been received indicating that a load generating job should be stopped. As mentioned above the control interface might receive control commands by way of a UI or an API. The control commands may then be provided to the load generating tier . For example the control interface might update an entry in the job status table indicating that a particular job should be stopped. If such a command has not been received the routine proceeds from operation to operation described above.

If a request to stop a load generating job has been received the routine proceeds from operation to operation . At operation the load generating instance stops the execution of the load generating plug ins . The load generating instance might also update the job status table to indicate that that the job is stopping and ultimately that the job has been stopped. From operation the routine proceeds back to operation where the processing described above may be repeated.

The UI shown in includes a number of rows each of which corresponds to a load processing job . The UI also includes a number of columns corresponding to job status information for each of the load processing jobs . For instance in the illustrative UI shown in columns have been provided for displaying the status of each load processing job the name of the host i.e. load processing instance upon which each job is executing the desired number of requests per time period e.g. requests per second the actual number of requests per time period that are being provided to the SUT the number of errors encountered and the number of commands that have been executed. It should be appreciated that the content shown in the illustrated columns is merely illustrative and that other columns providing other types of information might also be presented. Additionally in other embodiments functionality may be provided for allowing a user to customize the columns that are displayed in the UI .

The UI shown in might also provide functionality for modifying a load producing job . For instance a user might be permitted to select and modify the desired requests per time period for each load generating job reflected in the UI . In the example shown in for instance a mouse cursor has been utilized to select the field . The value shown in the field may then be modified. The modified value may then be propagated to the job queue for consumption by the load generating instances . The UI might also be configured to allow a user to change other aspects of a load generating job .

The UI shown in allows a user to create a new load generating job . The UI might be presented by the control interface in response to receiving a request to create a new load generating job in one embodiment. The UI includes a number of fields A C respectively that allow a user to specify the desired number of requests per time period the location of a load generating plug in and the location of the load generating data . The UI might also provide fields not shown in for allowing a user to specify the conditions under which a job should be stopped the number of load generating threads that may be utilized by the job and potentially other types of data. Once the user has provided values for the fields A C the user may select the UI button . In response to such a selection a new load processing job will be created and processed in the manner described above. It should be appreciated that the UIs and shown in are merely illustrative and that many different arrangements of UI controls may be utilized to provide the functionality disclosed herein.

Each type of computing resource provided by the service provider network may be general purpose or may be available in a number of specific configurations. For example data processing resources may be available as virtual machine instances in a number of different configurations. The virtual machine instances may be configured to execute applications including Web servers application servers media servers database servers and other types of applications such as those described above. Data storage resources may include file storage devices block storage devices and the like. The service provider network might also provide other types of services such as a distributed queue service for implementing the job queue or database services for implementing a database for providing the job status table . A service provider might also offer other types of resources for purchase and use by customers.

The computing resources provided by the service provider network are enabled in one implementation by one or more data centers A N which may be referred herein singularly as a data center or in the plural as the data centers . The data centers are facilities utilized to house and operate computer systems and associated components. The data centers typically include redundant and backup power communications cooling and security systems. The data centers might also be located in geographically disparate locations. One illustrative configuration for a data center that implements aspects of functionality disclosed herein for providing a distributed performance evaluation framework will be described below with regard to .

The customers and other users of the service provider network may access the computing resources provided by the service provider network over a WAN . For example a user of the distributed performance evaluation framework disclosed herein might utilize a Web browser application executing on the customer computing system to submit requests to evaluate the performance of a SUT to the control interface . Similarly a user of the distributed performance evaluation framework disclosed herein might utilize a Web browser application executing on the customer computing system obtain the status information describing the status of the load generating jobs executing on the load generating instances . Although a WAN is illustrated in it should be appreciated that a local area network LAN the Internet or any other networking topology known in the art that connects the data centers to remote customers and other users may be utilized. It should also be appreciated that combinations of such networks might also be utilized.

The service provider operating the service provider network might also charge a fee for the use of computing resources to a customer that creates and uses the resources. The fee charged for a particular resource might be based upon the type and or configuration of the resource. The fee charged for a particular resource might also be based upon the amount of time the resource is utilized. For example in the case of a data processing resource like a virtual machine instance the fee for use of the resource might be charged based upon the amount of time the resource is utilized. In the case of a data storage resource the fee might be computed based upon the amount of data stored and or the amount of data transferred into or out of the resource. The fees for other types of resources might also be based upon other considerations. A service provider might also utilize various purchasing models to determine the amount to charge a customer for use of resources provided by the service provider. In some embodiments the functionality disclosed herein for evaluating the performance of an SUT is provided for free. A charge might be incurred however for the use of the virtual machine instances that implement the load generating instances . A charge might also be made for the use of the resources that implement the job status table and the job queue .

The various resources described above might also be provisioned and de provisioned as needed in an automated fashion. For example a new virtual machine instance might be provisioned in response to determining that a load generating instance is not capable of providing a desired number of requests to a SUT . In this case a provisioning component not shown in or one or more other components within the service provider network might create the new instance of the virtual machine in the service provider network . The new virtual machine instance may then be utilized to implement a load generating instance in the manner described above. In a similar fashion virtual machine instances might be de provisioned after the testing of a SUT has been completed. In this way a customer utilizing the framework described herein to evaluate the performance of a SUT is charged only for the resources actually utilized during testing. It should be appreciated that computing resources other than virtual machine instances utilized to evaluate the performance of a SUT might be provisioned and de provisioned in a similar fashion.

The server computers may be standard tower or rack mount server computers configured appropriately for providing the computing resources described herein. For example in one implementation the server computers are configured to provide computing resources such as data processing resources data storage resources database resources networking resources and potentially others. Some of the servers might also be configured to execute a resource manager capable of instantiating and or managing the resources. In the case of virtual machine instances for example the resource manager might be a hypervisor or another type of program configured to enable the execution of multiple virtual machine instances on a single server computer . The data center shown in might also include at least one management server computer F that is reserved for executing various software components for managing the operation of the data center the server computers and the computing resources described above. Other configurations might also be utilized.

In the example data center shown in an appropriate LAN is utilized to interconnect the server computers A F. The LAN is also connected to the WAN illustrated in . It should be appreciated that the configuration and network topology illustrated in has been greatly simplified and that many more computing systems networks and networking devices may be utilized to interconnect the various computing systems disclosed herein and to provide the functionality described above. Appropriate load balancing devices or software modules might also be utilized for balancing a load between each of the data centers A N between each of the server computers A F in each data center and potentially between computing resources in each of the data centers . It should be appreciated that the data center described with respect to is merely illustrative and that other implementations might be utilized.

The computer includes a baseboard or motherboard which is a printed circuit board to which a multitude of components or devices may be connected by way of a system bus or other electrical communication paths. In one illustrative embodiment one or more central processing units CPUs operate in conjunction with a chipset . The CPUs may be standard programmable processors that perform arithmetic and logical operations necessary for the operation of the computer . The CPUs may be configured with one or more processing cores.

The CPUs perform operations by transitioning from one discrete physical state to the next through the manipulation of switching elements that differentiate between and change these states. Switching elements may generally include electronic circuits that maintain one of two binary states such as flip flops and electronic circuits that provide an output state based on the logical combination of the states of one or more other switching elements such as logic gates. These basic switching elements may be combined to create more complex logic circuits including registers adders subtractors arithmetic logic units floating point units and the like.

The chipset provides an interface between the CPUs and the remainder of the components and devices on the baseboard . The chipset may provide an interface to a random access memory RAM used as the main memory in the computer . The chipset may further provide an interface to a computer readable storage medium such as a read only memory ROM or non volatile RAM NVRAM for storing basic routines that help to startup the computer and to transfer information between the various components and devices. The ROM or NVRAM may also store other software components necessary for the operation of the computer in accordance with the embodiments described herein.

The computer may operate in a networked environment using logical connections to remote computing devices and computer systems through a network such as the local area network . The chipset may include functionality for providing network connectivity through a NIC such as a gigabit Ethernet adapter. The NIC is capable of connecting the computer to other computing devices over the network . It should be appreciated that multiple NICs may be present in the computer connecting the computer to other types of networks and remote computer systems.

The computer may be connected to a mass storage device that provides non volatile storage for the computer. The mass storage device may store system programs application programs other program modules and data which have been described in greater detail herein. The mass storage device may be connected to the computer through a storage controller connected to the chipset . The mass storage device may consist of one or more physical storage units. The storage controller may interface with the physical storage units through a serial attached SCSI SAS interface a serial advanced technology attachment SATA interface a fiber channel FC interface or other type of interface for physically connecting and transferring data between computers and physical storage units.

The computer may store data on the mass storage device by transforming the physical state of the physical storage units to reflect the information being stored. The specific transformation of physical state may depend on various factors in different implementations of this description. Examples of such factors may include but are not limited to the technology used to implement the physical storage units whether the mass storage device is characterized as primary or secondary storage and the like.

For example the computer may store information to the mass storage device by issuing instructions through the storage controller to alter the magnetic characteristics of a particular location within a magnetic disk drive unit the reflective or refractive characteristics of a particular location in an optical storage unit or the electrical characteristics of a particular capacitor transistor or other discrete component in a solid state storage unit. Other transformations of physical media are possible without departing from the scope and spirit of the present description with the foregoing examples provided only to facilitate this description. The computer may further read information from the mass storage device by detecting the physical states or characteristics of one or more particular locations within the physical storage units.

In addition to the mass storage device described above the computer may have access to other computer readable storage media to store and retrieve information such as program modules data structures or other data. It should be appreciated by those skilled in the art that computer readable storage media can be any available media that provides for the storage of non transitory data and that may be accessed by the computer .

By way of example computer readable storage media may include volatile and non volatile removable and non removable media implemented in any method or technology. Computer readable storage media includes RAM ROM erasable programmable ROM EPROM electrically erasable programmable ROM EEPROM flash memory or other solid state memory technology compact disc ROM CD ROM digital versatile disk DVD high definition DVD HD DVD BLU RAY or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium that can be used to store the desired information in a non transitory fashion.

The mass storage device may store an operating system utilized to control the operation of the computer . According to one embodiment the operating system comprises the LINUX operating system. According to another embodiment the operating system comprises the WINDOWS SERVER operating system from MICROSOFT Corporation. According to further embodiments the operating system may comprise the UNIX or SOLARIS operating systems. It should be appreciated that other operating systems may also be utilized. The mass storage device may store other system or application programs and data utilized by the computer such as the load generating plug in the load generating data and or any of the other software components and data described above. The mass storage device might also store other programs and data not specifically identified herein.

In one embodiment the mass storage device or other computer readable storage media is encoded with computer executable instructions which when loaded into the computer transform the computer from a general purpose computing system into a special purpose computer capable of implementing the embodiments described herein. These computer executable instructions transform the computer by specifying how the CPUs transition between states as described above. According to one embodiment the computer has access to computer readable storage media storing computer executable instructions which when executed by the computer perform the various routines described above with regard to . The computer might also include computer readable storage media for performing any of the other computer implemented operations described herein.

The computer may also include one or more input output controllers for receiving and processing input from a number of input devices such as a keyboard a mouse a touchpad a touch screen an electronic stylus or other type of input device. Similarly the input output controller may provide output to a display such as a computer monitor a flat panel display a digital projector a printer a plotter or other type of output device. It will be appreciated that the computer may not include all of the components shown in may include other components that are not explicitly shown in or may utilize an architecture completely different than that shown in .

Based on the foregoing it should be appreciated that technologies for providing a distributed performance evaluation framework have been presented herein. Moreover although the subject matter presented herein has been described in language specific to computer structural features methodological acts and computer readable media it is to be understood that the invention defined in the appended claims is not necessarily limited to the specific features acts or media described herein. Rather the specific features acts and mediums are disclosed as example forms of implementing the claims.

The subject matter described above is provided by way of illustration only and should not be construed as limiting. Furthermore the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure. Various modifications and changes may be made to the subject matter described herein without following the example embodiments and applications illustrated and described and without departing from the true spirit and scope of the present invention which is set forth in the following claims.

