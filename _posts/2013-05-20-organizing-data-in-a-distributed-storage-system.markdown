---

title: Organizing data in a distributed storage system
abstract: A distributed storage system is provided. The distributed storage system includes multiple front-end servers and zones for managing data for clients. Data within the distributed storage system is associated with a plurality of accounts and divided into a plurality of groups, each group including a plurality of splits, each split being associated with a respective account, and each group having multiple tablets and each tablet managed by a respective tablet server of the distributed storage system. Data associated with different accounts may be replicated within the distributed storage system using different data replication policies. There is no limit to the amount of data for an account by adding new splits to the distributed storage system. In response to a client request for a particular account's data, a front-end server communicates such request to a particular zone that has the client-requested data and returns the client-requested data to the requesting client.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09069835&OS=09069835&RS=09069835
owner: GOOGLE INC.
number: 09069835
owner_city: Mountain View
owner_country: US
publication_date: 20130520
---
This application claims priority to U.S. Provisional Application Ser. No. 61 649 806 filed May 21 2012 entitled Organizing Data in a Distributed Storage System which is incorporated by reference herein in its entirety.

The disclosed implementations relate generally to the management of a distributed storage system and in particular to system and method for organizing a large volume of data in a distributed storage system.

Internet has become a popular venue for people across the globe to store and exchange information. As the amount of data managed by the Internet rapidly increases both individually and collectively and the service associated with the data gets more complicated it is becoming a constant challenge for an Internet based service provider to mange such a large volume of data and render the associated service efficiently in response to different data access requests by users from anywhere in the world.

In accordance with some embodiments described below a method for adding data to a distributed storage system that includes a plurality of computer servers each computer server including one or more processors and memory is provided. The data within the distributed storage system is associated with a plurality of accounts and divided into a plurality of groups each group including a plurality of splits and each split being associated with a respective account. The method includes identifying a first split associated with an account wherein the first split is a member of a first group of splits and the first split has a data range parameter indicating that more data can be added to the first split and a split size indicating an actual amount of data in the first split in response to determining that the split size of the first split reaches a predefined limit updating the data range parameter in the first split so that no data can be added to the first split creating a second split for the account wherein the second split includes a data range parameter indicating that more data can be added to the second split adding the second split to a second group of splits and replicating the second group of splits within the distributed storage system in accordance with a data replication policy associated with the account.

In accordance with some embodiments described below a distributed storage system includes a plurality of zones each zone including a plurality of computer servers and each computer server including one or more processors and memory a plurality of network links that connects two respective zones wherein data within the distributed storage system is associated with a plurality of accounts and divided into a plurality of groups each group including a plurality of splits and each split being associated with a respective account and a plurality of program modules wherein the plurality of program modules are stored in the memory of a respective computer server and to be executed by the one or more processors of the respective computer server. The plurality of program modules include instructions for identifying a first split associated with an account wherein the first split is a member of a first group of splits and the first split has a data range parameter indicating that more data can be added to the first split and a split size indicating an actual amount of data in the first split in response to determining that the split size of the first split reaches a predefined limit updating the data range parameter in the first split so that no data can be added to the first split creating a second split for the account wherein the second split includes a data range parameter indicating that more data can be added to the second split adding the second split to a second group of splits and replicating the second group of splits within the distributed storage system in accordance with a data replication policy associated with the account.

In accordance with some embodiments described below a non transitory computer readable storage medium stores one or more program modules configured for execution by a computer server that includes one or more processors and memory and is part of a distributed storage system wherein the distributed storage system is configured for dividing data associated with a plurality of accounts into a plurality of groups each group including a plurality of splits and each split being associated with a respective account. The one or more programs include instructions for identifying a first split associated with an account wherein the first split is a member of a first group of splits and the first split has a data range parameter indicating that more data can be added to the first split and a split size indicating an actual amount of data in the first split in response to determining that the split size of the first split reaches a predefined limit updating the data range parameter in the first split so that no data can be added to the first split creating a second split for the account wherein the second split includes a data range parameter indicating that more data can be added to the second split adding the second split to a second group of splits and replicating the second group of splits within the distributed storage system in accordance with a data replication policy associated with the account.

In accordance with some embodiments described below a method for replicating data within a distributed storage system that includes a plurality of computer servers each computer server including one or more processors and memory is provided. The data within the distributed storage system is associated with a plurality of accounts and divided into a plurality of groups each group including a plurality of splits and each split being associated with a respective account. The method includes dividing data associated with a first account and data associated with a second account into a first set of splits and a second set of splits respectively wherein the first account and the second account have different data replication policies adding each of the first set of splits and the second set of splits to a respective group of splits wherein there is no group including a member of the first set of splits and a member of the second set of splits creating a predefined number of replicas for each group of splits that includes a member of the first set of splits and allocating the replicas within the distributed storage system in accordance with a data replication policy associated with the first account and creating a predefined number of replicas for each group of splits that includes a member of the second set of splits and allocating the replicas within the distributed storage system in accordance with a data replication policy associated with the second account.

In accordance with some embodiments described below a distributed storage system includes a plurality of zones each zone including a plurality of computer servers and each computer server including one or more processors and memory a plurality of network links that connects two respective zones wherein data within the distributed storage system is associated with a plurality of accounts and divided into a plurality of groups each group including a plurality of splits and each split being associated with a respective account and a plurality of program modules wherein the plurality of program modules are stored in the memory of a respective computer server and to be executed by the one or more processors of the respective computer server. The plurality of program modules include instructions for dividing data associated with a first account and data associated with a second account into a first set of splits and a second set of splits respectively wherein the first account and the second account have different data replication policies adding each of the first set of splits and the second set of splits to a respective group of splits wherein there is no group including a member of the first set of splits and a member of the second set of splits creating a predefined number of replicas for each group of splits that includes a member of the first set of splits and allocating the replicas within the distributed storage system in accordance with a data replication policy associated with the first account and creating a predefined number of replicas for each group of splits that includes a member of the second set of splits and allocating the replicas within the distributed storage system in accordance with a data replication policy associated with the second account.

In accordance with some embodiments described below a non transitory computer readable storage medium stores one or more program modules configured for execution by a computer server that includes one or more processors and memory and is part of a distributed storage system wherein the distributed storage system is configured for dividing data associated with a plurality of accounts into a plurality of groups each group including a plurality of splits and each split being associated with a respective account. The one or more programs include instructions for dividing data associated with a first account and data associated with a second account into a first set of splits and a second set of splits respectively wherein the first account and the second account have different data replication policies adding each of the first set of splits and the second set of splits to a respective group of splits wherein there is no group including a member of the first set of splits and a member of the second set of splits creating a predefined number of replicas for each group of splits that includes a member of the first set of splits and allocating the replicas within the distributed storage system in accordance with a data replication policy associated with the first account and creating a predefined number of replicas for each group of splits that includes a member of the second set of splits and allocating the replicas within the distributed storage system in accordance with a data replication policy associated with the second account.

In accordance with some embodiments described below a method for locating data within a distributed storage system that includes a plurality of computer servers each computer server including one or more processors and memory is provided. The data within the distributed storage system is associated with a plurality of accounts and divided into a plurality of groups each group including a plurality of splits each split being associated with a respective account and each group having multiple tablets and each tablet managed by a respective tablet server of the distributed storage system. The method includes receiving a data access request from a client wherein the data access request includes a logical identifier of data associated with an account identifying one or more split identifiers associated with the logical identifier in accordance with the data access request wherein each split identifier identifies a split associated with the account for each of the identified one or more split identifiers identifying a group identifier wherein the group identifier corresponds to a group of splits selecting one of the multiple tablets associated with the identified group based on information about the tablet servers managing the multiple tablets communicating with the tablet server that manages the selected tablet for the split corresponding to the split identifier and receiving the split from the tablet server and forwarding the splits from the respective tablet servers to the requesting client.

In accordance with some embodiments described below a distributed storage system includes a plurality of zones each zone including a plurality of computer servers and each computer server including one or more processors and memory a plurality of network links that connects two respective zones wherein data within the distributed storage system is associated with a plurality of accounts and divided into a plurality of groups each group including a plurality of splits each split being associated with a respective account and each group having multiple tablets and each tablet managed by a respective tablet server of the distributed storage system and a plurality of program modules wherein the plurality of program modules are stored in the memory of a respective computer server and to be executed by the one or more processors of the respective computer server. The plurality of program modules include instructions for receiving a data access request from a client wherein the data access request includes a logical identifier of data associated with an account identifying one or more split identifiers associated with the logical identifier in accordance with the data access request wherein each split identifier identifies a split associated with the account for each of the identified one or more split identifiers identifying a group identifier wherein the group identifier corresponds to a group of splits selecting one of the multiple tablets associated with the identified group based on information about the tablet servers managing the multiple tablets communicating with the tablet server that manages the selected tablet for the split corresponding to the split identifier and receiving the split from the tablet server and forwarding the splits from the respective tablet servers to the requesting client.

In accordance with some embodiments described below a non transitory computer readable storage medium stores one or more program modules configured for execution by a computer server that includes one or more processors and memory and is part of a distributed storage system wherein the distributed storage system is configured for dividing data associated with a plurality of accounts into a plurality of groups each group including a plurality of splits each split being associated with a respective account and each group having multiple tablets and each tablet managed by a respective tablet server of the distributed storage system. The one or more programs include instructions for receiving a data access request from a client wherein the data access request includes a logical identifier of data associated with an account identifying one or more split identifiers associated with the logical identifier in accordance with the data access request wherein each split identifier identifies a split associated with the account for each of the identified one or more split identifiers identifying a group identifier wherein the group identifier corresponds to a group of splits selecting one of the multiple tablets associated with the identified group based on information about the tablet servers managing the multiple tablets communicating with the tablet server that manages the selected tablet for the split corresponding to the split identifier and receiving the split from the tablet server and forwarding the splits from the respective tablet servers to the requesting client.

In some embodiments the zone master monitors the performance of the tablet servers by periodically e.g. after every 10 seconds communicating with the tablet servers . A tablet server reports to the zone master its current status information including its CPU and memory usage etc. as well as other information used for determining the association between a set of tablets and the tablet server. Based on such information the zone master determines whether or not to assign a tablet in the data store to a corresponding tablet server. For example the zone master may identify some tablets associated with one tablet server which is deemed to be overloaded and assign the identified tablets to another tablet server In addition the zone master publishes the updated tablet to tablet server map through the location proxy servers When another entity e.g. a front end server or a tablet server wants to learn which tablet server is responsible for managing a particular tablet the entity can query one of the location proxy servers by providing a tablet ID and receiving a corresponding tablet server ID associated with the tablet ID. After identifying a tablet server for a particular tablet through the location lookup service the entity can communicate with the identified tablet server for any read write access requests directed at the tablet.

When the client wants to access data associated with a customer it submits a data access request to a front end server . In some embodiments different front end servers shown in are responsible for providing data related services to different clients. For example some of the front end servers are configured for handling data access requests from clients for access email service data and some other front end servers are configured for handling data access requests from clients for access advertising service data. In some embodiments data associated with an online service application is further partitioned into multiple portions and each front end server is configured for handling a subset of the data access requests for one or more portions of the data. In some embodiments the front end servers of the distributed storage system are located at different geographical locations to provide services to nearby clients that submit data access requests associated with different online services. As shown in a client submits a data access request by invoking an application programming interface API accepted by the front end server . The data access request includes identification information of the one or more customers. In some embodiments the data associated with a customer corresponds to a row in a data table and the row of customer data is further divided into multiple columns. The data access request may include a row identifier and possibly one or more column identifiers if the client is interested in accessing data in the one or more columns associated with each customer. Upon receipt of the data access request the front end server needs to determine where the client requested data is located in the distributed storage system such as information about the zone and tablet s that have the client requested data.

As shown in different components in the distributed storage system are organized into two domains based on their respective roles i the logical domain including the front end servers and ii the physical domain including the zones . The front end servers handle data access requests from the external clients and use data structures such as the directory map and the group map to free the clients from understanding how data is physically stored in the distributed storage system . In some embodiments each customer of the on line advertising service is associated with one directory in the directory map . Based on the logical directory identifier provided by a client the front end server identifies a particular entry in the director map which corresponds to the particular customer.

To improve the scalability of the distributed storage system data associated with a customer is divided into multiple segments which are referred to as splits each split having a unique split ID in the directory map . As shown in each directory e.g. directory includes one or more splits and . The number of splits associated with a directory is determined by the size of data associated with a corresponding customer. The more data the customer has the more splits the data may be divided into. When the size of data within a split reaches a predefined threshold no more data is added to the split and a new split is generated for hosting new data associated with the account. In some embodiments there is no limit on the size of data for an account. In other embodiments the size of data for an account is set to a predetermined limit. The predetermined limit may be determined by the distributed storage system e.g. a global limit of the size of data that is applied to all accounts the application for which the data for the account is associated e.g. a web mail application may impose a limit of the size of data for its accounts that are different than an advertisement application and or may be increased if an end user purchases more storage space for an account. Note that the client i.e. the on line advertising service provider does not need to know which split s has the client requested data. Instead the client specifies the requested data in a request using a format defined by the client associated online service provider and the front end server translates the client request into a set of split IDs that identify the splits including the client requested data.

To improve the reliability and efficiency of the distributed storage system data associated with a customer is replicated into multiple copies and stored in different tablets of one or more zones. In other words the basic data unit for a particular customer in a front end server is a split and the basic data unit in a zone is a tablet. As shown in a group in the group map is defined to associate a split in the directory map with a plurality of tablets in a particular zone. In this example the split is a data structure associating the split ID with a group ID which corresponds to an entry in the group map . The split also includes a split size indicating the actual amount of data currently within this split range indicator . As will be described below the data range indicator is used for indicating whether the split has space for more data or not. When the split runs out of space a new split e.g. split will be created for hosting new data associated with the account. In this example the split is associated with the group not the group . Note that different splits associated with an account may belong to the same group of splits or different groups of splits. Each group includes a plurality e.g. hundreds or even thousands of splits associated with different accounts and has a predefined group limit. The exact association between a split and a group is dynamically determined based in part on the remaining capacity of a particular group. In some embodiments the front end server tries to add different splits associated with the same account to the same group because these splits are likely to be accessed by a client at the same time and it is probably more convenient for them to be within the same group and therefore the same set of tablets which are replicas of the group. If the group e.g. group runs out of space the front end server may identify another group e.g. group for the split . In some embodiments the data replication policy is defined for each account the group is chosen for the split because it has the same number of tablets as the group . In other words splits associated with different accounts that have different data replication policies should be added to different groups with different numbers of tablets.

In accordance with a data replication policy provided by the client a predefined number of instances of the group are generated in the distributed storage system each instance of the group being referred to as a tablet. As shown in the group has a group ID which is the same as the group ID in the split a group size and a list of tablet IDs e.g. tablet IDs of the group. Splits associated with different directories e.g. directory and directory both belong to the group suggesting that the two accounts corresponding to the two directories have the same data replication policy. In response to a client request the front end server first identifies one or more split IDs in the directory map which are associated with a customer identified by the client request and then identifies a group and an associated list of tablet IDs for each split ID. depict an embodiment in which different splits as identified by the split IDs and associated with one customer are assigned to different groups the group and the group . This situation happens when the size of a group reaches a predefined group limit such that it is less efficient to keep all the data associated with one customer e.g. multiple splits in one group and therefore one tablet 

After the front end server identifies the tablet IDs in a group of splits that includes the client requested data the process of accessing the client requested data is shifted from the logical domain to the physical domain i.e. a zone that includes the tablets associated with the identified tablet IDs. In some embodiments a tablet ID or includes a respective zone ID embedded therein. Therefore after identifying the tablet IDs the front end server also knows which zone has the client requested data. As noted above each zone includes one or more location proxy servers that provide the location look up service for identifying a particular tablet server for each tablet. Based on the zone ID included in a tablet ID the front end server submits a query to a respective location proxy server at a particular zone identified by the zone ID the query including one or more tablet IDs . The location proxy server then returns one or more tablet server IDs each tablet server ID identifying a respective tablet server e.g. the tablet server or the tablet server that has been chosen by the zone master for managing the data access requests to the particular tablet. Upon receipt of the one or more tablet server IDs the front end sever submits a request to a corresponding tablet server the request including identification of one or more splits e.g. splits and within the tablet e.g. the tablet . In response to the request each tablet server identifies a corresponding tablet in the data store and performs the operations to the identified tablet accordingly.

Referring back to a tablet may include splits corresponding to multiple groups of splits . By including as many groups as possible within a tablet the total number of tablets can be reduced which makes it easier to manage the transactions directed to different groups that are associated with the same tablet. For example the reduction of tablets will consume less messages resources for maintaining the relationship between different tablets. There are fewer entities in the distributed storage system to deal with. With the reduction of tablets transactions on a set of neighboring directories in the directory map will likely be localized to a single set of tablets at a particular zone and will therefore be executed as a transaction at a single site e.g. a tablet server instead of transactions at multiple sites that might involve multiple tablet servers. In some embodiments the reduction of tablets makes it more likely that a child director is placed in the same group as its parent directory. By doing so a front end server can direct a call to the child directory to the location of the parent directory which reduces the size of the required location cache at the front end server and allows clients to start up faster since they will have to load fewer locations. In some embodiments the location of the parent directory does not correspond to the location of location of the child directory. In this case if the client has a big payload it may first send a verification message to the tablet server that manages the tablet including the parent directory and determine if the child directory is co located in the same tablet. Only after receiving a positive confirmation will the client send its payload to the tablet server. All these performance benefits can significantly reduce the cost of introducing a new directory to the distributed storage system and make it more likely that clients will map their structures to directories naturally instead of trying to form larger directories.

As shown in a tablet server further includes tablet metadata associated with tablets managed by the tablet server. In this example the tablet metadata includes a directory to group map and group state information . The directory to group map locates a particular group within a tablet for a particular directory in the directory map. The group state information includes the state information for a particular group replica such as the log data view information the list of group replicas etc. Given a directory associated with a tablet the tablet server can scan the directory to group map for an entry that has the same directory name. Once an entry is identified the tablet server can access the corresponding group state information using a group ID within this entry. In some embodiments the tablet server supports the removal of a range of directories from a tablet by eliminating data associated with each directory within the range from the tablet when removing a directory replica from the tablet.

In some embodiments one group in the distributed storage system may be split into two or more groups for several reasons. For example a tablet containing the group is overloaded or too large. In this case splitting the group may help reduce the load of the tablet if e.g. this group contributes to a significant fraction of the load or size of the tablet. Sometimes access patterns for some directories within a group are very different from access patterns for the rest of the group. For example a group may be split into two if most directories in the group are accessed from US but a particular set of directories in the group are mostly accessed from Europe.

As shown in the directory set is associated with the group which is a member of the tablet . It is assumed that the directory set which is a subset of the directory set before the movement will be moved to another group. To do so a new group is created within each of the same set of tablets that include the group by making a replica of the group . After the creation the new group can be changed via the normal replica addition or removal. In this example the group split is implemented as a single site transaction on the group and the transaction update is applied at every replica of the group . As shown in at the completion of the transaction a new group is created within the tablet and the old group is updated to the group to reflect the group split such that each directory in the new directory set which corresponds to the old directory set is associated with the new group . The metadata associated with the new group is populated such that it inherits at least a portion of the metadata associated with the group indicating that the relationship between the two groups and . By doing so the metadata associated with the group e.g. the list of tablet IDs does not change. It should be noted that splitting a group does not actually move any user data associated with the group. This makes the split fast because the transaction cost is independent of the sizes of the directories being moved.

Note that splitting the group within the tablet does not affect any load at the tablet because the new group is within the same tablet as the original group. In order to move away some load from the tablet the tablet needs to move some group to another tablet. As shown in the tablet includes two groups and each group corresponding to a respective set of directories. In some embodiments one group within a tablet is moved to another tablet within the same zone by having the two tablets sharing some data with each other. In some other embodiments the target tablet e.g. the tablet reads directly from the source tablet e.g. the tablet without going through a tablet server that is responsible for managing the tablet . As such the movement of groups between two tablets does not add additional load to the tablet server. In conjunction with the movement of a group from one tablet to another tablet the mapping between a directory e.g. a member in the directory set and a source tablet e.g. the tablet is updated to redirect to a target tablet e.g. the tablet .

By allowing each account to have its own data replication policy the distribute storage system offers both flexibility and scalability to different types of online service applications that use the distributed storage system for storing their data. For example an account that needs frequent access to its data from one or more geographical locations may specify such need in its data replication policy so that the distributed storage system may create more replicas for the data associated with the account at a zone close to the data accessing locations and reduce the latency required for a client to access such data.

As shown in the distributed storage system creates in a predefined number e.g. three of replicas for each group of splits e.g. the group that includes a member of the first set of splits and allocates the three replicas in a zone e.g. the zone in accordance with the data replication policy associated with the first account. In this example the zone includes three tablets each being one replica of the group and each tablet includes a copy of the first set of splits associated with the first account . As described above each tablet in a zone is assigned to a tablet server for managing data access requests directed to the tablet. In this case the three tablets are managed by two tablet servers and . In other words tablets associated with the same group of splits may be managed by the same tablet server or different tablet servers depending on the load balance of the respective tablet servers in a zone. Similarly the distributed storage system creates in a predefined number e.g. two of replicas for each group of splits e.g. the group that includes a member of the second set of splits and allocates the replicas in a zone e.g. the zone in accordance with the data replication policy associated with the second account. In this example the zone includes two tablets each being a replica of the group and each tablet includes a copy of the second set of splits associated with the second account and is managed by a respective tablet server or . It should be noted that the data replication policy of a group of splits is driven by the data replication policies of the different splits in the group which are driven by the data replication policies of the accounts associated with the different splits. The distributed storage system is responsible for putting those splits having the same or similar data replication policies into the same group to improve the system s efficiency. In some embodiments the enforcement of the account level or directory level data replication policy is determined by the distributed storage system based on the availability of resources at different zones. In other words it is possible that the distributed storage system may not always store splits associated with a particular account strictly in accordance with the account s data replication policy. For example the splits may be initially stored in a zone different from a zone defined by the account s data replication policy and then moved to the zone. In this case the distributed storage system allows an account to specify its desired placement of the splits associated with the account in the directory map and will try to satisfy such requirement whenever it is possible. In some embodiments an account may change its data replication policy from time to time. For example an email account user may temporarily move from North America to Europe. When the email application detects such movement it. may notify the distributed storage system to move tablets associated with the email account from a data center in North America to a data center in Europe to provide a better service to the end user.

In some embodiments each account has only one split whose data range parameter has a value of . As shown in the distributed storage system determines whether the split associated with the account reaches its limit according to a predefined schedule e.g. periodically . If not no the distributed storage system stops checking this account and proceeds to check another account. Otherwise yes the distributed storage system will take further actions by creating a new split for the account.

As shown in when the existing split runs out of space the distributed storage system updates the data range parameter in the split from to ABC. Note that the expression ABC is an expression that corresponds to the actual upper limit of the data within the split . From this parameter the distributed storage system can tell what data is within each split. In response to a client request for a particular piece of data the distributed storage system can use the data range parameter to determine which split or splits have the client requested data. By doing so the distributed storage system also marks the first split as not accepting any new data. The distributed storage system then creates a second split e.g. the split in for the account. As shown in the second split includes a data range parameter that has a value of indicating that more data can be added to the second split and a split size parameter that grows as more and more data is added to the second split . Moreover the distributed storage system selects a second group of splits e.g. the group in for the second split and adds the second split to a second group of splits. It should be noted that the second group may be the same group that includes the split or a different one. Finally the distributed storage system replicates the second group of splits in a particular zone e.g. the zone in in accordance with a data replication policy associated with the account. In this example the second group has two tablets in the zone which are managed by two respective tablet servers and . In some embodiments the creation of a new split for an account may be triggered by the movement of data within the distributed storage system e.g. in response to a change of the data replication policy. In either case the creation of new splits for the account ensures that the client can add more data to this account without disrupting the service associated with the account.

In response the front end server in identities one or more split identifiers associated with the logical identifier in accordance with the data access request. As shown in the front end server identified two splits within the directory map they are the split and the split . Each split s metadata includes a split ID that identifies a split associated with the account. For each of the identified one or more split identifiers the front end server identifies a group identifier e.g. the group IDs and in each group identifier corresponds to a group of splits. In the example shown in the group ID corresponds to the group of splits that includes a split corresponding to the split in the director map and the group ID corresponds to the group of splits that includes a split corresponding to the split in the directory map . The distributed storage system selects one of the multiple tablets associated with each identified group based on information about the tablet servers managing the multiple tablets. For example the zone includes two tablets associated with each of the two identified groups and . In particular the tablet is identified for the group and the tablet includes a split that corresponds to the split in the directory map . The tablet is identified for the group and the tablet includes a split that corresponds to the split in the directory map . For each selected tablet there is a corresponding tablet server in the zone that is responsible for managing data access requests to the tablet. In this example the tablet server is responsible for managing the tablet and the tablet server is responsible for managing the tablet . After identifying each tablet server the front end server communicates with the tablet server that manages the selected tablet for the split corresponding to the split identifier for the split associated with the client request and receives the split from the tablet server. After receiving the splits from different tablet servers the front end server forwards the splits from the respective tablet servers to the requesting client in satisfying the client s data access request. It should be noted that the client access request may be a read only request or a read and write request. Since each group of splits includes multiple tablets any data update to one split within the group should be replicated within each tablet associated with the group.

Reference has been made in detail to implementations examples of which are illustrated in the accompanying drawings. While particular implementations are described it will be understood it is not intended to limit the invention to these particular implementations. On the contrary the invention includes alternatives modifications and equivalents that are within the spirit and scope of the appended claims. Numerous specific details are set forth in order to provide a thorough understanding of the subject matter presented herein. But it will be apparent to one of ordinary skill in the art that the subject matter may be practiced without these specific details. In other instances well known methods procedures components and circuits have not been described in detail so as not to unnecessarily obscure aspects of the implementations.

Although the terms first second etc. may be used herein to describe various elements these elements should not be limited by these terms. These terms are only used to distinguish one element from another. For example first ranking criteria could be termed second ranking criteria and similarly second ranking criteria could be termed first ranking criteria without departing from the scope of the present invention. First ranking criteria and second ranking criteria are both ranking criteria but they are not the same ranking criteria.

The terminology used in the description of the invention herein is for the purpose of describing particular implementations only and is not intended to be limiting of the invention. As used in the description of the invention and the appended claims the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will also be understood that the term and or as used herein refers to and encompasses any and all possible combinations of one or more of the associated listed items. It will be further understood that the terms includes including comprises and or comprising when used in this specification specify the presence of stated features operations elements and or components but do not preclude the presence or addition of one or more other features operations elements components and or groups thereof.

As used herein the term if may be construed to mean when or upon or in response to determining or in accordance with a determination or in response to detecting that a stated condition precedent is true depending on the context. Similarly the phrase if it is determined that a stated condition precedent is true or if a stated condition precedent is true or when a stated condition precedent is true may be construed to mean upon determining or in response to determining or in accordance with a determination or upon detecting or in response to detecting that the stated condition precedent is true depending on the context.

Although some of the various drawings illustrate a number of logical stages in a particular order stages that are not order dependent may be reordered and other stages may be combined or broken out. While some reordering or other groupings are specifically mentioned others will be obvious to those of ordinary skill in the art and so do not present an exhaustive list of alternatives. For example it is possible for a front end server to return a split associated with an account to a client in its native format used by the distributed storage system and the client then converts the raw split into a format defined by the client. Moreover it should be recognized that the stages could be implemented in hardware firmware software or any combination thereof.

The foregoing description for purpose of explanation has been described with reference to specific implementations. However the illustrative discussions above are not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The implementations were chosen and described in order to best explain principles of the invention and its practical applications to thereby enable others skilled in the art to best utilize the invention and various implementations with various modifications as are suited to the particular use contemplated. Implementations include alternatives modifications and equivalents that are within the spirit and scope of the appended claims. Numerous specific details are set forth in order to provide a thorough understanding of the subject matter presented herein. But it will be apparent to one of ordinary skill in the art that the subject matter may be practiced without these specific details. In other instances well known methods procedures components and circuits have not been described in detail so as not to unnecessarily obscure aspects of the implementations.

