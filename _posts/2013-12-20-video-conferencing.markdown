---

title: Video conferencing
abstract: A video conferencing system is provided, in which at least two cameras are used to capture images of people at a first location participating in a video conference. One or more active speakers are identified among the people at the location, and one of the at least two cameras is automatically selected based on a position or positions of the one or more active speakers. Images from the selected camera are provided to a person at a second location participating the video conference.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09179098&OS=09179098&RS=09179098
owner: 
number: 09179098
owner_city: 
owner_country: 
publication_date: 20131220
---
This application is a continuation of and claims the benefit of U.S. patent application Ser. No. 13 649 751 filed on Oct. 11 2012 to be issued as U.S. Pat. No. 8 614 735 which is a continuation of and claims the benefit of U.S. patent application Ser. No. 11 966 674 filed on Dec. 28 2007 which claims priority to U.S. Provisional Patent Application 60 877 288 filed on Dec. 28 2006. The above applications are incorporated by reference in their entirety.

Video conferencing allows groups of people separated by large distances to have conferences and meetings. In some examples two parties of a video conference each uses a video conferencing system that includes a camera for capturing images of local participants and a display for showing images of remote participants and optionally the local participants of the video conference. The participants may manually control the cameras to adjust zoom and viewing angle in order to clearly show the faces of the speakers during the conference. In some examples a video conferencing system may include an array of microphones to detect sound sources using triangulation and automatically direct the camera to zoom in on the speaker.

In one aspect in general a method of conducting a video conference is provided in which at least two cameras are used to capture images of people at a first location participating in a video conference one or more active speakers are identified among the people at the location one of the at least two cameras is automatically selected based on a position or positions of the one or more active speakers and images from the selected camera are provided to a person at a second location participating the video conference.

Implementations may include one or more of the following features. The images are optically or digitally zoomed based on the one or more active speakers. Identifying one or more active speakers includes identifying people who have spoken within a predetermined period of time. A database of one or more active speakers is maintained adding a person who starts to speak to the database and removing a person who has not spoken for a predetermined period of time from the database. A user interface is provided to allow adjustment of the duration of the predetermined period of time.

In another aspect in general at least two cameras are used to capture images of people at a location one or more active speakers at the location are identified one of at least two cameras are automatically selected based on a position or positions of the one or more active speakers and images from the selected camera are provided.

Implementations may include one or more of the following features. Identifying one or more active speakers includes identifying people who have spoken within a predetermined period of time. A database of one or more active speakers is maintained a person who starts to speak is added to the database and a person who has not spoken for a predetermined period of time is removed from the database. The database is periodically updated and the selection of camera is automatically adjusted based on the updated database. Maintaining the database of one or more active speakers includes storing information about when each speaker starts and ends speaking. Maintaining the database of one or more active speakers includes storing information about a coordinate of each active speaker.

Selecting one of at least two cameras includes selecting one of the cameras having a smallest view offset angle with respect to the one or more active speakers. The images are sent to a remote party who is conducting a video conference with the people at the location. The position or positions of the one or more active speakers are determined. Determining positions of the active speakers includes determining positions of the active speakers by triangulation. Determining positions of the active speakers by triangulation includes triangulation based on signals from a microphone array. The camera is automatically zoomed to more clearly show the one or more active speakers. A zoom value is determined based on a distance or distances between the camera and the one or more active speakers. Determining the zoom value includes determining a zoom value to provide a first margin between a reference point of a left most active speaker and a left border of the image and a second margin between a reference point of a right most active speaker and a right border of the image. A zoom value is determined based on a distance between the camera and a closest of the one or more active speakers. A viewing angle of the camera is automatically adjusted to more clearly show the one or more active speakers.

In another aspect in general a log of one or more active speakers at a location is maintained a zoom factor and a viewing direction of a camera are automatically determined based on a position or positions of the one or more active speakers such that the active speakers are within a viewing range of the camera and images of the one or more active speakers are provided.

Implementations may include one or more of the following features. The one or more active speakers are periodically identified and the log is updated to include the identified one or more active speakers. Updating the log includes adding a person who starts to speak to the log and removing a person who has not spoken for a predetermined period of time from the log. Determining the zoom factor and viewing direction of the camera includes determining a zoom factor and viewing direction to provide a first margin between a reference point of a left most active speaker and left borders of the images and a second margin between a reference point of a right most active speaker and right borders of the images.

In another aspect in general active speakers in a room are identified the room having cameras for capturing images of people in the room a subset of less than all of the cameras in the room is selected and images from the selected subset of cameras are provided to show the active speakers. Identifying active speakers includes identifying people in the room who have spoken within a predetermined period of time.

In another aspect in general a video conferencing system is provided. At least two cameras capture images of people at a first location participating a video conference a speaker identifier identifies one or more active speakers at the first location a data processor selects one of the at least two cameras based on a position or positions of the one or more active speakers and provides images from the selected camera to show the one or more active speakers and a communication interface sends the images to a person at a second location participating the video conference.

Implementations may include one or more of the following features. A microphone array identifies speakers in the room based on triangulation. A storage stores information about speakers who have spoken within a predetermined period of time.

In another aspect in general at least two cameras capture images of people at a location and a data processor selects one of the at least two cameras based on a position or positions of one or more active speakers at the location and provide images from the selected camera to show the one or more active speakers.

Implementations may include one or more of the following features. A speaker identifier identifies active speakers in the room. The speaker identifier identifies active speakers in the room by identifying one or more people who have spoken within a predetermined period of time. The speaker identifier includes a microphone array that enables determination of positions of the active speakers by triangulation. A storage stores coordinates of the active speakers in the room and time points when each active speaker started and ended talking. The data processor selects one of the at least two cameras by selecting the camera having a smallest view offset angle with respect to the active speakers. The data processor executes a video conferencing process to send the image to a remote party who is video conferencing with the people at the location. The data processor controls a zoom factor and a viewing direction of the camera such that the active speakers are within a viewing range of the camera.

In another aspect in general at least two cameras capture images of people in a room a speaker identifier identifies active speakers in the room and a data processor selects a subset of the at least two cameras and provide at least one image from the subset of the at least two cameras to show the active speakers.

These and other aspects and features and combinations of them may be expressed as methods apparatus systems means for performing functions computer program products and in other ways.

The apparatuses and methods can have one or more of the following advantages. Interpersonal dynamics can be displayed with the video conferencing system. By using multiple cameras the viewing angle of the people speaking can be improved when there are several participants in the video conference most or all of the speakers do not have to turn their heads significantly in order to face one of the cameras. The system can automatically choose a camera and its viewing direction and zoom factor based on the person or persons speaking so that if there are two or more people conducting a conversation images of the two or more people can all be captured by the camera. Effectiveness of video conferences can be increased. Use of digital zooming can reduce mechanical complexity and reduce the response delay caused by motion controllers.

Other features and advantages of the invention are apparent from the following description and from the claims.

Referring to an example of a conversation sensitive video conferencing system includes multiple video cameras to capture a video or images of participants of a video conference from various viewing angles. The video conferencing system automatically selects one of the cameras to capture images of people who have spoken within a predetermined period of time. When two or more people are in a conversation or discussion the viewing direction and zoom factor of the selected camera are automatically adjusted so that images captured by the selected camera show most or all of the people actively participating in the conversation. When additional people join in the conversation or when some people drop out of the conversation the choice of camera and the viewing direction and zoom factor of the selected camera are automatically re adjusted so that images captured by the selected camera show most or all of the people currently participating in the conversation.

An advantage of the conversation sensitive video conferencing system is that remote participants of the video conference can see more clearly the people who are actively participating in the conversation. If only one camera were used it may be difficult to provide a good viewing angle for all or most of the participants. Some participants may have their backs toward the camera and would have to turn their heads significantly in order to face the camera. If the viewing angle and zoom factor of the camera were fixed during the conference so that the camera capture images showing all of the people in the conference room the faces of some of the people may be small and it may be difficult for the remote participants to see clearly the people who are speaking.

Another advantage of the conversation sensitive video conferencing system is that it is not necessary to manually select one of the cameras or adjust the viewing angle and zoom factor to capture images of people actively participating in the conversation. Participants of the video conference can focus on the discussion rather than being distracted by the need for constant adjustment of the cameras.

Each of the video cameras is capable of capturing a video that includes a sequence of images. In this description the images captured by the camera can be either still images or a sequence of images that form a video.

Referring to an example of a conversation sensitive video conferencing system can be used to show interpersonal dynamics among local participants of a video conference. The system includes a camera assembly having multiple cameras e.g. and collectively referenced as that capture images of the people participating in the video conference from various viewing angles. A video conferencing transceiver VCT controls the camera assembly to select one of the cameras to capture images of people who have spoken within a predetermined period of time. When two or more people are actively participating in a discussion the video conferencing transceiver automatically adjusts the viewing direction and zoom factor of the selected camera so that images captured by the selected camera show most or all of the people actively participating in the discussion. This is better than displaying images showing all participants of the video conference where each participant s face may be small and not clearly visible or displaying images of individual speakers where the images switch from one speaker to another.

A conference display is provided so that local participants of the video conference can see the images are captured by the selected camera as well as images of the remote participants of the video conference. The video conferencing transceiver is connected to the remote site through for example a broadband connection . A user keypad is provided to allow local participants of the vide conference to control the video conferencing transceiver to various system settings and parameters.

In some implementations the system includes a programming interface to allow configurations of the video conferencing transceiver to be updated using for example a personal computer. A speaker location detector determines the locations of speakers. The speaker location detector may include for example an array of microphones to detect utterances from a speaker and determine the location of the speaker based on triangulation.

The camera assembly sends audio signals picked up by the microphones to the video conferencing transceiver through signal lines . The camera assembly sends video signals to the video conferencing transceiver through signal lines which can be e.g. IEEE 1394 cables. The signal lines also transmit control signals from the video conferencing transceiver to the camera assembly . The camera assembly sends control signals for generating chirp signals for use in self calibration to the video conferencing transceiver through signal lines . The video conferencing transceiver transmits VGA signals to the programming interface and receives mouse data and keyboard data from the programming interface . The video conferencing transceiver sends video signals and audio signals to the conference display through a VGA cable and an audio cable respectively.

Referring to in some implementations each video camera in the video camera assembly includes a sensor for capturing images a camera microphone for capturing audio signals and an input output interface e.g. an IEEE 1394 interface for interfacing with the video conferencing transceiver . The sensor can be e.g. a charge coupled device CCD sensor or a complimentary metal oxide semiconductor CMOS sensor. The video camera is coupled to a chirp generator that is used to generate chirp signals for use in self calibrating the system .

Referring to in some implementations the video conferencing transceiver includes a motherboard hosting a central processing unit CPU memory devices and a chipset that controls various input output devices and storage devices. The CPU can be any type of microprocessor or microcontroller. The memory devices can be for example dynamic random access memory DRAM Flash memory or other types of memory. The motherboard includes a line out port for outputting conference audio signals to the conference display . A microphone input port is provided to receive audio signals from the microphones of the speaker location detector .

An IEEE 1394 controller is provided to process signals sent through the IEEE 1394 bus . Sound cards are provided to process audio signals from the video camera microphones . A network interface is provided to connect to the broadband connection . A video card is provided to generate video signals that are sent to the video conferencing display and to the programming interface . A hard drive and an optical disc drive provide mass storage capability. For example the hard drive can store software programs used to control the system and data generated when running the system .

Optionally the motherboard includes circuitry for processing calibration control signals for chirp generators.

Referring to in some implementations the memory devices store various information including programming instructions for controlling the system and constant values that are used by the programming instructions . The memory devices store variable values and stacks of data that are generated and used during operation of the system . The memory includes a region for storing output data and a region for storing text to be displayed on the conference display .

A first video scratch memory is provided to store image data from a first video camera and a second video scratch memory is provided to store image data from a second video camera . If more video cameras are used additional scratch memory can be provided for each video camera . Each video scratch memory corresponds to a window showing images captured by a corresponding video camera . The camera that is chosen has its window moved to the front of the display screen and the camera not chosen has its window sent to the back of the display screen. By switching between the video scratch memory and the system can quickly switch from images from one video camera to images from another video camera

Referring to in some implementations the conversation sensitive video conferencing system executes software programs written in object oriented programming in which various software classes are defined. For example a Camera class a Controller class an Auditor class a Talker class a Meeting Focus class a Communicator class and a Display class are defined. In this description the same reference number is used for a class and objects that belong to the class. For example the reference number is used for both the Camera class and a Camera object belonging to the Camera class.

A Camera object which is an instance of the Camera class can be used for controlling various aspects of one of the cameras . The Camera object can call LibDC 1394 functions to control the video camera to capture video. The LibDC 1394 is a library that provides a high level programming interface for controlling IEEE 1394 based cameras that conform to the 1394 based Digital Camera Specifications. The library allows control of the camera including turning the camera on or off and have continuous live feed. The Camera object can also call Java media framework JMF application programming interface API .

For example the Camera object can have methods or interfaces such as StartCapture and StopCapture which can be used for starting or stopping the recording of video data.

The Camera object has a number of parameters that can be adjusted such as current angle and zoom factor. Adjusting the current angle and zoom factor of the Camera object causes the current angle and zoom factor of the corresponding camera to be adjusted.

When the video conferencing system is turned on a Controller object which is an instance of the Controller class starts up a user interface to allow the user to control the video conferencing system.

For example the Controller object can have two methods including StartConference and StopConference methods which can be used to start or stop the system when either a local user or a remote user initializes or ends a conference.

An Auditor object which is an instance of the Auditor class can be used to monitor audio signals. For example there can be three Auditors each representing one of the microphones . If the speaker position detector includes more than three microphones an additional Auditor object can be provided for each additional microphone .

The Auditor object can have a Listen method for reading audio data from a circular buffer and determining whether the audio data represents background noise or speaker utterance. When a speaker utterance is detected referred to as an onset the Auditor object informs a Talker object described below which then correlates the signals from the three Auditor objects to determine the position of the speaker.

The Talker object which is an instance of the Talker class can receive information provided by the Auditor objects and can have a AcknowledgeTalker method that is used to calculate the location of the sound source e.g. location of the speaker using correlation and triangulation. The Talker object also checks to see if the speaker stops speaking as this information is useful to a conversation sensing algorithm in determining which persons are still participating in a conversation.

A Meeting Focus object which is an instance of the Meeting Focus class implements a conversation sensing algorithm that determines which persons referred to as speakers or talkers are participating in a conversation and constructs a talker map that includes information about the speakers the locations of the speakers and the start and end times of talk for each speaker. For example the talker map can be implemented as a database or a table.

For example the Meeting Focus object can have two methods including AddTalker and RemoveTalker methods that are used to add or remove speakers from the talker map.

The Meeting Focus object determines the best view of the speaker s taking into account variables such as the camera resolution dimensions of the window showing the images of the speakers percentage of entire screen being occupied by the images and centroid of images etc. The Meeting Focus object sends this information to a Display object described below .

A Communicator object which is an instance of the Communicator class controls the communications infrastructure of the system . If a connection with a remote party is made or broken the Communicator object informs the Controller object .

For example the Communicator object can have three methods including StartComm method for initiating the communication interfaces a MakeConnection method for establishing a connection with a remote party and a BreakConnection method for ending the connection with the remote party.

In some implementations a digital zoom is used. A Display object which is an instance of the Display class calculates a zoom factor based on requested coordinates and the percentage of the screen that the images is to be displayed which are provided by the Meeting Focus object .

For example the Display object can have three methods including a DisplayDefault method for displaying images using default parameters a DisplayScene method for display images that mainly show the active participants of a conversation and a StopDisplay method that stops display images that mainly show the active participants of a conversation.

The Controller invokes a StartConference method which initiates a self calibration process. The Controller starts buzzers by making a call to the Camera objects . During the self calibration process the acoustics characteristics of the video conference room is analyzed and the locations of the cameras are determined. Upon the completion of the self calibration process the video conference can begin.

When the Controller finishes self calibration the Controller invokes a StartCapture method of the Camera object to initialize each of the video cameras . The Controller passes a Cam parameter to the StartCapture method to indicate which camera is to start capturing images. The Camera object invokes a StreamIn method to start a video daemon to enable video data to be written into the memory . The Camera object passes an Addr parameter to the StreamIn method to indicate the address of the memory to which the video data is to be written. The video input daemon sends a CamReady acknowledgement signal to the Controller when the camera is ready to stream video data to the memory .

The Controller invokes a DisplayDefault method of the Display object to display images captured by the camera using default display settings. The Controller passes a Cam parameter to the DisplayDefault method to indicate that the images from the camera represented by the Cam parameter is to be displayed.

The Display invokes a StreamOut method to start a video output daemon so that a video with default settings is shown on the conference display and sent to the remote users of the video conference.

The Controller invokes a Listen method of an Auditor object . When the Auditor hears an onset the Auditor invokes an AcknowledgeTalker method of a Talker object to calculate the location of the audio source i.e. the location of a speaker who just started speaking through trigonometry and calculus algorithms. The Auditor passes a t parameter to the AcknowledgeTalker method to indicate the time of onset.

The Talker invokes an AddTalker method of a Meeting Focus object to add the new speaker to the talker map. The Talker passes a Location parameter to the AddTalker method to indicate the location of the new speaker. The Meeting Focus identifies the correct camera that can capture images that include the active speakers in the talker map. The Meeting Focus object determines a center of the image and the size of the image to display.

The Meeting Focus object invokes a DisplayScene method of the Display object which determines how pixel values are interpolated in order to achieve a certain digital zoom. The Meeting Focus object passes Cam Center and Size parameters to the DisplayScene method where Cam represents the selected camera Center represents the location of a center of the portion of image captured by the video camera to be displayed and Size represents a size of the portion of image to be displayed. The DisplayScene method causes an image to be shown on the conference display in which the image has the proper zoom and is centered near the centroid of the active speakers.

When there are more than one speaker in the talker map the Meeting Focus object determines a centroid of the speaker locations and adjusts the image so that the centroid falls near the center of the image. The Meeting Focus object also determines a zoom factor that affects the percentage of the screen that is occupied by the speaker s . For example if there is only one speaker a zoom factor may be selected so that the speaker occupies 30 to 50 of the width of the image. When there are two speakers a zoom factor may be selected so that the speakers occupy 50 to 70 of the width of the image when there are three or more speakers a zoom factor may be selected so that the speakers occupy 70 to 90 of the width of the image etc. The percentage values above are examples only other values may also be used.

The zoom factor that results in the speaker s occupying a particular percentage of the image can be determined based on a function of the distance s between the camera and the speaker s . The zoom factor can also be looked up from a table that has different zoom factors for different camera speaker distances and different percentage values representing the percentage that the speakers occupy the image .

When the Talker determines that a speaker has stopped talking for a predetermined period of time the Talker invokes a RemoveTalker method of the Meeting Focus object to remove the speaker from the talker map. The Meeting Focus object selects a camera that can capture images that include the remaining speakers in the talker map. The Meeting Focus object determines the centroid of the speaker locations and the percentage of screen that is occupied by the remaining speakers. The Meeting Focus object invokes the DisplayScene method to show images having the proper zoom and centered near the centroid of the remaining active speakers.

The Auditor continues to listen to the signals from the microphones of the speaker location detector . When the Auditor hears an onset the Auditor invokes an AcknowledgeTalker method repeating the steps described above for adding a speaker to the talker map and later removing the speaker from the talker map when the speaker ceases speaking. Whenever a speaker is added to or removed from the talker map the selection of camera and the viewing direction and zoom factor of the selected camera are adjusted to properly show the current participants of the conversation or discussion.

The video conference can be ended by invoking a StopConference method of the Controller object . The StopConference method can be invoked by the local user or by the remote party hanging up the call which invokes a BreakConnnection method of the Communicator . When the StopConference method is invoked the Controller terminates the Listen method and invokes a StopCapture method of the Camera object to cause the camera to stop capturing images. The Controller invokes a StopDisplay method of the Display object to end the video output to the conference display .

The methods associated with the Camera object include StartCapture and StopCapture . The StartCapture method causes the sensor of the camera to start operating and invokes a StreamIn Addr method that causes video data to be streamed into the memory starting at address Addr.

The StopCapture method ends the StreamIn Addr method to end the streaming of video data to the memory .

The methods associated with the Controller object include Main StartConference and StopConference methods. The Main method is used to set up a window for showing the video images of the video conference invoke an InitializeDisplay method to initialize the display and wait for input from the user or the communicator . If an input is received the Main method invokes the MakeConnection method to cause a connection to be established between the local participants and the remote participants.

The StartConference method can be initialized by either a local user or by the Communicator . StartConference invokes a StartCapture n method for each video camera n and wait until a CamReady flag is received from each video camera n indicating that the video camera n is ready. The StartConference method invokes the DisplayDefault method to cause images from the video camera to be shown on the display . The StartConference method invokes the Listen m method for each microphone m to listen to audio signals from the microphone m.

The StopConference method terminates the Listen i method for each microphone i invokes StopCapture j method for each camera j and invokes StopDisplay k method to stop images from the camera k from being shown on the display .

The methods associated with the Auditor object include the Listen method . The Listen mic method starts receiving audio data associated with the microphone mic through the audio card . The Listen method reads segments of the audio data based on a sliding time window from the memory and determines whether the audio signal is noise or speech by checking for correlation. If the audio signal is not noise the begin time t of the speech is determined. The Listen method then invokes the AcknowledgeTalker t method to cause a new speaker to be added to the talker map.

Referring to the AcknowledgeTalker t method implements a process to determine the location of the audio source e.g. location of the speaker . The AcknowledgeTalker method analyzes audio signals from different microphones and determines whether there is a correlation between the audio signals from the different microphones . Because the distances between the speaker and the various microphones may be different the same audio signal may be picked up by different microphones at different times. A time delay estimation technique can be used to determine the location of the speaker.

The process finds the onset detected by the first microphone at time t1. To determine whether there is correlation between the audio signal from the first microphone and the audio signal from the second microphone the process reads a segment of the audio signal from a first microphone starting at t1 and a segment of the audio signal from the second microphone starting at t2 using a sliding time window and calculates a correlation coefficient r of the two segments of audio signals . The sliding time window is adjusted e.g. by incrementing t2 until a correlation is found in which the correlation coefficient r is greater than a threshold . For example if an audio segment starting at t1 from the first microphone correlates to an audio segment starting from time t2 from the second microphone the process determines that the time t2 associated with the second microphone has been found .

The process above is repeated to find the correlation between the audio signal from the first microphone and the audio signal from the third microphone . The location x y of the audio source i.e. the speaker is determined using geometric and trigonometry formulas . The process records time points t1 t2 and t3 in variables and invoke the AddTalker method to add the speaker to the talker map. The process continuously check the correlations among the audio signals from the microphones 1 2 and 3. When the audio signals no longer correlate to one another the process invokes the RemoveTalker method to remove the speaker from the talker map.

The methods associated with the Meeting Focus object include the AddTalker Location method and the RemoveTalker Location method .

Referring to the AddTalker Location method implements a process to add a speaker to the talker map. The process records the location of the speaker in the talker map . A source map is read to determine if there are other speakers . The source map includes a log of all the people who have spoken. The talker map includes a list of the people who are currently actively participating in a conversation. The process determines if a conversation or discussion is taking place .

If there is only one active speaker which indicates no conversation is taking place the process selects a camera and determines the centroid and percentage of window to show the speaker at the location . If there are more than one active speaker which indicates that a conversation is taking place the process reads the location s of the other participant s selects a camera and determines the centroid and percentage of window to best show images that include the locations of all the active speakers . The process invokes the DisplayScene cam center size method to cause the image to be shown on the conference display .

Referring to the RemoveTalker location method implements a process that includes re selecting the camera and recalculating the center and size of image based on the locations of the remaining active speakers . The process updates the talker map and invokes the DisplayScene method to cause an updated image to be shown on the conference display .

The methods associated with the Communicator include the MakeConnection method and the BreakConnection method . The MakeConnection method checks to see if a connection is established with a remote site. If there is a connection the MakeConnection method invokes the StartConference method . If there is no connection the MakeConnection method checks the connection again after a period of time.

The methods associated with the Display include the DisplayScene Cam x y percentage of display method and the StopDisplay method . The DisplayScene method reads data from memory that is written by the video input daemon . The DisplayScene method selects a camera determines the dimensions of the digital zoom and implements the calculated dimensions. A filter is used to smooth out the images to reduce block artifacts in the images.

The StopDisplay method ends the StreamOut Addr method to stop the video output daemon form streaming video to the memory .

Referring to in some implementations the cameras do not change viewing directions and zooming and panning are achieved by digital cropping and enlargement of portions of the images. In this case a camera can be selected from among the plurality of cameras in the camera assembly by choosing the camera with the optimal viewing angle. In some implementations the camera having the optimal viewing angle can be selected by finding the camera with the smallest view offset angle. In this case the speaker will be closer to the center of the image captured by the camera.

In the example of a first camera and a second camera are facing each other so a viewing direction of the first camera aligns with a viewing direction of the second camera . The viewing directions of the cameras can be different. A speaker at location P has a view offset angle ViewOff1 with respect to the view direction of the first camera and a view offset angle ViewOff2 with respect to the view direction of the second camera . In this example because ViewOff1 is smaller than ViewOff2 the first camera is selected as the camera for capturing images of the speaker .

In some implementations digital zooming is used in which images from the video camera are cropped and enlarged to achieve a zoom effect. When there is a single speaker the cropped image has the speaker at the center of the image. The size of the cropped image frame is adjusted e.g. enlarged to fit the correct zoom factor. For example if the zoom factor is 2 the cropped image frame has a width and length that is one half of the original image so that when the cropped image is enlarged by 2 the enlarged image has the same size as the original image thereby achieving digital zooming. The position of the cropped image is selected to accurately display the chosen person e.g. so that the speaker is at the middle of the cropped image.

Referring to a source map is stored in the memory to manage information about a continuous list of talkers. The source map keeps track of past talkers and allows the Talker object to determine the current active participants of a conversation so that the correct participants can be shown on the display and to the remote participants. To establish the source map a SourceMapEntry class is used. A SourceMapEntry object includes the x y coordinates of the speaker s location and the times and that the speaker starts or stops talking.

The action angle1 angle2 time and x y coordinates are components of the SourceMapEntry class. The action parameter can be stop or start indicating whether the source has stopped or started talking The angle n n 1 2 3 . . . parameter represents the offset angle for the specific camera n to the talker the angle that the optical axis of the camera n would need to turn to be pointing toward the talker . In this example two cameras were used so there were two angle parameters for each SourceMapEntry. If more cameras are used more angle parameters can be used for the additional cameras.

Referring to a conversation recognition algorithm is used to determine the participants of an active conversation and to establish a talker map including a list of the participants of an active conversation. There can be various conversational dynamics that could happen during a conference. For example one person can be making a speech two people can be talking to each other in a discussion or one person can be the main speaker but taking questions from others. One way to capture these scenarios is that every time a speaker starts talking the system checks to see who has talked within a certain time. By default the camera that is the best choice for the new speaker will be chosen because he or she will be the one speaking and it is most important to see his or her motions. An appropriate zoom is chosen based on an evaluation of recent speakers in the source map specifically determining the leftmost and rightmost past speakers in relation to the chosen camera.

In some implementations if a speaker talks for more than a certain length of time and no other person has spoken during that period the system resets the scene and focuses on the sole speaker.

Referring to a conversation sensitive zoom process is used to select a camera and control the viewing angle and zoom factor of the selected camera to ensure that the leftmost and rightmost active speakers are included in the images displayed. The conversation sensitive zoom process includes the following steps 

Step 2 Read the entries in the source map e.g. by calling the SourceMapEntry and identify the entries having time stamps that are recent within a predetermined time period.

Step 3 Determine the leftmost speaker and the rightmost speaker based on the chosen camera and the particular recent entries from the source map .

Step 4 Calculate horizontal and vertical offsets as a fraction of the total field as signified by the left edge leftmost the right edge rightmost and the total field total angle .

In step 2 the system re centers the window by calculating the horizontal and vertical position offsets xPos and yPos based on zoom size using the equations 

The images shown in are somewhat grainy. The quality of the images can be improved by using video cameras having higher pixel resolutions. Alternatively optical zoom can also be used to improve image quality.

Various modifications can be made to the system . In some examples a delayed automatic zooming is used so that when a speaker ceases talking the camera zoom does not change immediately. Rather the scene changes after a few seconds to provide come sense of continuity to the viewer. This way if a speaker pauses for a few seconds and resumes talking the camera view angle will not change.

In some examples the system provides an advanced features section in the graphical user interface to allow users to configure the behavior of the system to suit their preferences. For example the user can adjust the duration of time that the system will consider a speaker to be active if the speaker has spoken within the duration of time the zoom factors for various settings the margins d at the left and right image borders and the time delay between detection of changes in the talker map and adjustment of the display view.

In some examples when the camera viewing angle or zoom factor are changed there is a smooth transition from one scene to the next. The video appears to have continuous panning and zooming when speakers join in or drop out of a conversation. This prevents distracting quick flashes.

Other implementations and applications are also within the scope of the following claims. For example the model numbers and parameters of the components in the video conferencing systems can be different from those described above. The video camera can be color or gray scale it can include e.g. a charge coupled device CCD or a CMOS image sensor. The cameras can be controlled using multiple embedded microcontrollers or a centralized computer. The system can provide a GUI to allow users to adjust parameter values inside of text boxes. A self calibration system that used beeps from the cameras to calibrate the microphone array can be used. The camera viewing angle can be adjusted mechanically and optical zooming can be used.

More than one camera can be selected. For example in a large conference room where speakers engaged in a discussion may be seated far apart it may not be practical to show an image that includes all of the active participants of the discussion. Two or more cameras can be selected to show clusters of active participants and the images can be shown on multiple windows. For example if a first group of two people at one end of a long conference table and a second group of three people at the other end of the long conference table were engaged in a discussion a video showing the first group of two people can be shown in one window and another video showing the second group of three people can be shown in another window.

