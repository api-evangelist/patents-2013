---

title: Distributed demand matrix computations
abstract: In one embodiment, a method includes receiving a packet at a first network device, logging the packet into a demand corresponding to a cell of a demand matrix, and storing the demand in a demand database at the first network device. The demand database includes a plurality of demands computed for a specified time period and corresponding to cells of the demand matrix associated with traffic entering a network at the first network device. Demands corresponding to cells of the demand matrix associated with traffic entering the network at a second network device are computed and stored at the second network device. An apparatus and logic are also disclosed herein.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09106510&OS=09106510&RS=09106510
owner: Cisco Technology, Inc.
number: 09106510
owner_city: San Jose
owner_country: US
publication_date: 20130129
---
The present application claims priority from U.S. Provisional Application No. 61 621 811 entitled NETWORK AVAILABILITY ANALYTICS filed on Apr. 9 2012. The contents of this provisional application are incorporated herein by reference in its entirety.

The present disclosure relates generally to communication networks and more particularly to computing traffic demand matrices.

A traffic matrix also referred to as a demand matrix or traffic demand matrix describes traffic flows that enter traverse and leave a network. Each demand is a single traffic matrix entry that identifies the amount of traffic that enters the network at one point and leaves the network at another point. While traffic matrices are essential to network operators they often have accuracy issues.

Corresponding reference characters indicate corresponding parts throughout the several views of the drawings.

In one embodiment a method generally comprises receiving a packet at a first network device logging the packet into a demand corresponding to a cell of a demand matrix and storing the demand in a demand database at the first network device. The demand database includes a plurality of demands computed for a specified time period and corresponding to cells of the demand matrix associated with traffic entering a network at the first network device. Demands corresponding to cells of the demand matrix associated with traffic entering the network at a second network device are computed and stored at the second network device.

In another embodiment an apparatus generally comprises a processor for logging a received packet into a demand corresponding to a cell of a demand matrix and storing the demand in a demand database. The demand database comprises a plurality of demands computed for a specified time period and corresponding to cells of the demand matrix associated with traffic entering a network at the apparatus. The apparatus further includes memory for storing the demand database. Demands corresponding to cells of the demand matrix associated with traffic entering the network at a network device are computed and stored at the network device.

In yet another embodiment logic is encoded on one or more tangible computer readable media for execution and when executed operable to log a packet received at a network device into a demand corresponding to a cell of a demand matrix store the demand in a demand database comprising a plurality of demands corresponding to cells of the demand matrix associated with traffic entering a network at the network device receive demands corresponding to cells of the demand matrix associated with traffic entering the network at other network devices and generate the demand matrix.

The following description is presented to enable one of ordinary skill in the art to make and use the embodiments. Descriptions of specific embodiments and applications are provided only as examples and various modifications will be readily apparent to those skilled in the art. The general principles described herein may be applied to other applications without departing from the scope of the embodiments. Thus the embodiments are not to be limited to those shown but are to be accorded the widest scope consistent with the principles and features described herein. For purpose of clarity details relating to technical material that is known in the technical fields related to the embodiments have not been described in detail.

A demand is generally defined as all packets exiting the network at a specific edge node. In a network of N edge nodes a demand matrix is made of N 2 cells such as X Y which denotes the number of packets and bytes that ingressed the network at X and egressed from the network at Y. A demand can be further defined in terms of the origin AS Autonomous System neighbor ingress AS neighbor egress AS destination AS incoming interface at the collecting router DSCP Differentiated Services Code Point class and the like.

A drawback with conventional techniques used to generate traffic matrices is the poor scaling of centralized solutions based around network traffic flow collectors in which routers collect flow information and export raw or aggregated data. Network traffic flow collection software for traffic monitoring e.g. NetFlow or hardware traffic probes can be installed around the perimeter of a network to provide detailed traffic matrix information. However an approach based purely on NetFlow or hardware probing is not appropriate for all network operators. In addition to processing all of the NetFlow records from their router clients collectors also need to maintain the specific BGP Border Gateway Protocol tables of each individual router client.

Conventional systems either require a full mesh of TE Traffic Engineering tunnels from PE provider edge to PE which does not scale well and incurs high costs or a centralized network traffic flow analyzer to continuously maintain the individual BGP table of each PE node. Another problem is the difficulty in correlating asynchronous flow export measurements with a network wide synchronized demand matrix. Combining data into a single demand matrix can be challenging because the data collected may not cover the same time period. The difficulty in gathering accurate consistent timely and complete measurement data often results in erroneous demand matrices.

The embodiments described herein can be used to generate accurate demand matrices without the need for centralized computation. The embodiments provide a scalable lightweight always on accurate solution to derive demand matrices. As described in detail below each participating distributed demand matrix DDM router uses an accounting technique to derive local elements of the demand matrix. Edge routers compute their share of the demand matrix so that the computation effort is distributed to each router. The distributed workload provides scalability benefits. Accuracy is provided by precise accounting. Furthermore there is no need to simulate the BGP view of each individual router on a centralized collector which is error prone. Timing rules ensure that all cells of a given demand matrix relate to the same time period. In one embodiment a pull model is used for classic applications requiring a demand matrix. In another embodiment the DDM routers advertise their local elements of the demand matrix so that all of the routers have the complete demand matrix. By providing routers with information on topology and global traffic rather than topology and local traffic new possibilities open up in the space of traffic engineering resiliency and agile IP optical topologies.

Referring now to the drawings and first to an example of a network in which embodiments described herein may be implemented is shown. For simplification only a small number of nodes are shown. The embodiments operate in the context of a data communication network including multiple network devices. The network may include any number of network devices in communication via any number of nodes e.g. routers switches or other network devices which facilitate passage of data within the network.

The network shown in the example of includes four network devices A B C D connected by links and located along an edge of the network. The network devices may be routers or other network devices configured to perform routing functions. The routers may be located for example along an edge of a service provider network or any other network. Each edge device may be in communication with one or more Autonomous System AS customer network data center or any other network or network device. The routers are part of a perimeter around the network which includes all of the provider edge routers aggregation routers broadband routers or other routers forming a continuous perimeter around the network.

A demand represents traffic flowing between any two edge devices in the network . There may be any number of traffic flows across the network. A demand matrix traffic matrix traffic demand matrix describes the aggregation of flows at the edge of the network. A demand is a single element cell of the demand matrix associated with a source and destination in the network. Each demand is thus a single entry in the demand matrix that identifies the amount of traffic that enters the network at one point and leaves the network at another point. For example traffic may enter the network at router A and traverse the network along a path until it reaches router C where it leaves the network. In order to construct the demand matrix information is collected about all of the traffic and how it enters and exits the network. Traffic measurements are mapped to individual demands.

Each row of the above demand matrix includes cells corresponding to traffic received at one of the edge devices . For example the first row comprises cells corresponding to traffic received at router A that will egress the network at router A AA router B AB router C AC and router D AD . Similarly the second row comprises cells corresponding to traffic received at router B that will egress the network at router A BA router B BB router C BC and router D BD and the third and fourth rows comprise the cells corresponding to traffic entering the network at routers C and D respectively. Each router computes its own row local demands for the demand matrix. The local demands from each of the routers can be collected at one or more of the routers or another device in the network and the complete demand matrix generated as described below.

Each DDM router includes a DDM module configured to compute all of the X Y cells of the demand matrix where X is the router at which the DDM module is located. Each DDM module is also configured for synchronized periodic measurement archive. This allows the distributed measurements from different routers to be combined. Each router further includes a demand database maintained by the DDM module . DDM measurements from each linecard are stored in the database for a specified time period. The demands in the database may be retrieved for use by capacity planning applications or other operational applications running on a centralized server for example. Demands from the database may also be advertised to other routers participating in the DDM process.

It is to be understood that the network shown in and described above is only an example and that the embodiments described herein may be implemented in networks having any number or type of network devices or topology. For example there may be any number of intermediate or core nodes located within the network between the edge devices . Also each edge device may be in communication with any number of other networks or network devices in one or more Autonomous System AS for example.

Memory may be a volatile memory or non volatile storage which stores various applications operating systems modules and data for execution and use by the processor . For example memory may include one or more of the DDM components implemented in software demand database for storing demand measurements or cells of a demand matrix and FIB Forwarding Information Base . The DDM module may comprise software or code stored in memory .

Logic may be encoded in one or more tangible media for execution by the processor . For example the processor may execute codes stored in a computer readable medium such as memory . The computer readable medium may be for example electronic e.g. RAM random access memory ROM read only memory EPROM erasable programmable read only memory magnetic optical e.g. CD DVD electromagnetic semiconductor technology or any other suitable medium.

The network interfaces may comprise any number of interfaces linecards ports for receiving data or transmitting data to other devices. The network interfaces may include for example an Ethernet interface for connection to a computer or network.

It is to be understood that the network device shown in and described above is only an example and that different configurations of network devices may be used. For example the network device may further include any suitable combination of hardware software algorithms processors devices components or elements operable to facilitate the capabilities described herein.

The database at a first network device e.g. router A in comprises a plurality of demands computed and stored for a first time period T and corresponding to cells of the demand matrix associated with traffic entering the network at the first network device. For example the demand database for router A includes demands traffic measurements for cells AA AB AC and AD of the demand matrix. The demands for each of these cells are computed and stored periodically at time period T. Demands corresponding to cells of the demand matrix associated with traffic entering the network at a second network device e.g. router B in are computed and stored at the second network device at a second time period T . For example demands corresponding to cells BA BB BC and BD are stored at router B periodically at time period T.

One of the first and second time periods is preferably a multiple of the other time period. For example T may be equal to 1 T 2 T 3 T . . . etc. Demands are also computed and stored at routers C and D in at specified time periods. All of the time periods used at each edge device have a common multiple. This timing rule makes it possible to combine the distributed measurements of different routers as described further below.

As discussed above the DDM process includes accounting synchronized periodic measurement archive DDM database maintenance and collection of DDM measurements. Each of these functions is described in detail below.

Accounting is distributed to each linecard of each router . Upon receiving a packet the linecard logs it into a demand either on the basis of a demand marker inserted in an FIB Forwarding Information Base structure on a per prefix basis which allows for accurate accounting and does not require any extra function other than a conventional FIB lookup or on the basis of NetFlow aggregation policies which then require NetFlow processing and potentially sampling for performance reasons.

The demand marker inserted in the FIB is preferably computed automatically by the router . One embodiment operates to allocate one demand marker per IGP node router ID. This tracks the number of packets and bytes egressing the network at each of the remote locations. A preferred embodiment involves allocating a marker per tuple such as ingress interface at the counting router neighboring ingress AS DSCP class egress IGP node and neighboring AS at the egress router. A further refinement includes allocating multiple demand markers per FIB entry. For example a second marker may be used to track the final destination AS. This scales better since it does not require further increasing the number of entries in the tuple defining the first marker. The embodiments may be implemented in the BGP routing process wherein it binds each BGP route to a demand marker. The resulting markers are then inserted in the FIB updates together with their related routes. This can be viewed as an extension of a BGP Policy Accounting BPA process with a much higher number of markers an automated generation of markers and the ability to have multiple markers per BGP route.

In one embodiment each participating router runs a DDM process on a route processor that leverages an NTP Network Time Protocol synchronized clock. Each router may be configured to archive measurements every time period T referenced to the top of the hour. T may be for example 1 2 3 5 6 10 12 20 30 or 60 minutes. In one example if a router is configured with T 15 minutes it will archive the DDM cell measurements for each of its linecards at 12 h00 12 h15 12 h30 12 h45 13 h00 13 h15 . . . etc. This simplifies the combining of distributed measurements of different routers.

In one example all four routers in run DDM with T 15 minutes. In this case the cells collected at 12 h15 by router A can easily be combined with the cells of routers B C and D collected at the same time to derive a clear demand matrix where all of the cells relate to the time period 12 h00 12 h15.

This scheme also allows for routers to store demand measurements at different time periods e.g. to accommodate different CPU performance or software release and still eases the recombination into a global demand matrix that relates to a well defined and common time period. For example router A may be running with a time period of T 5 minutes while the other routers run at T 15 minutes. A demand matrix can still be derived every 15 minutes where all the cells relate to the same 15 minute time period. For example the AC cell for 12 h00 12 h15 is the sum of the AC cells for 12 h00 12 h05 12 h05 12 h10 and 12 h10 12 h15.

In one embodiment T is restricted to 1 2 3 5 6 10 12 20 30 or 60 minutes to ensure that at least a network wide demand matrix can be derived every 60 minutes. Other sets of various time periods may also be used following the same multiplicity principle.

Each router configured for DDM runs a DDM process that maintains DDM database . In one embodiment the database is indexed according to time with one entry every time period T. At the end of every time period the DDM process collects the DDM measurements from each linecard at the router and stores the values in the database entry for that time period.

For example if router A is running at T 15 minutes at 12 h30 router A s DDM process collects the local DDM measurements AA AB AC AD and inserts the measurements in the local DDM database under the time entry 12 h15 12 h30. This scales very well even at the scale of a very large service provider. For example 5000 pairs of counters byte count packet count every 15 minutes results in 3.8 Mbyte per day.

The timing rules described above ensure that it is straightforward to recompose the different cells into matrices that relate to a specific time period. While a router can operate at a fine periodicity T 15 minutes for the purpose of maintaining its local DDM database it can send its BGP DDM updates at a lower frequency e.g. every 60 minutes . The multiple timing assumption described above is leveraged.

In one example the embodiments described herein are used with an application such as capacity planning or operational reporting. The application may run on a centralized server and therefore need to retrieve the DDM measurements via a pull model. In one embodiment a YANG data model is used and NETCONF access is leveraged. YANG is a data modeling language used to model configuration and state data manipulated by Network Configuration Protocol NETCONF NETCONF remote procedure calls and NETCONF notifications. XML Extensible Markup Language and XMPP Extensible Messaging and Presence Protocol may also be used.

Another use for the DDM embodiments is to let each router participating in DDM learn the measurements of the other routers. This provides resiliency agile topology and traffic engineering. With regard to resiliency in conventional networks the need to protect a link and the selection of backup path is done without any consideration for the real traffic going through the link. If a router has the demand matrix the router can compute which demands flow through its links and which need to be protected and therefore select different protection techniques accordingly. For example a partial LFA Loop Free Alternate coverage may be found if there is an LFA for 99.99 of the traffic but only LFA for 82 of the routes. With the distributed demand matrix the routers can compute resiliency based on real traffic impacted or protected rather than just route topology.

By leveraging IP optical integration routers have the ability to automate the addition of waves such as to provide a much more agile IP topology responding to demand matrix changes . The decision logic is incomplete as long as the routers do not have a view of the demand matrix. The embodiments provide DDM information for use with tools such as GMPLS Generalized Multi Protocol Label Switching UNI User Network Interface integration and DWDM Dense Wavelength Division Multiplexing agility.

In another example the availability of the demand matrix at a head end network device offers tools to improve the efficiency of the head end based computation as it can take into consideration the need of other head ends.

In one embodiment a BGP LS extension is used to disseminate the cells measured by each DDM router. Each router advertises these demands through an extension for example of IETF draft gredler bgp te 01 Advertising Link Sate Information in BGP H. Gredler et al. Jul. 11 2011 so that all routers are aware of all the demands in the network. For example router A is able to locally compute local demands A A A B A C and A D using the accounting scheme described above. Router A advertises these four cells via BGP updates using this new extension. Routers B C and D also advertise their DDM information. Thus all four of the routers A B C D learn the sixteen cells of the demand matrix four locally and twelve via BGP advertisements . Third party applications can listen to these BGP updates and learn dynamically the demand matrix and its evolution throughout the day. The BGP distribution extension acts as an API Application Programming Interface into the distributed demand matrix computation mechanism.

Although the method and apparatus have been described in accordance with the embodiments shown one of ordinary skill in the art will readily recognize that there could be variations made without departing from the scope of the embodiments. Accordingly it is intended that all matter contained in the above description and shown in the accompanying drawings shall be interpreted as illustrative and not in a limiting sense.

