---

title: Mitigating an impact of a datacenter thermal event
abstract: A ranking service can retrieve metrics from a metrics data store and use the metrics to determine a priority order in which to power down resources in a data center. Metrics from the data store can include a number of instances running on a host, a length of time that an instance has been operational, a type of instance, an amount of CPU use on a host, etc. The ranking service can also obtain other parameters from other sources. The parameters can include whether redundant or failover instances exist, the importance of the instances, whether the customer itself is considered important, other generic parameters from the customer account, a customer provided ranking of instances, etc.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09146814&OS=09146814&RS=09146814
owner: Amazon Technologies, Inc.
number: 09146814
owner_city: Seattle
owner_country: US
publication_date: 20130826
---
Network based services exist that allow customers to purchase and utilize instances of computing resources such as virtual machine instances on a permanent or as needed basis. In addition to virtual machine instances these services typically allow customers to purchase and utilize instances of other types of computing resources for use with the virtual machine instances. For example customers might be permitted to purchase and utilize instances of data storage resources instances of networking resources and instances of other types of resources.

Managing network based services such as those described above can be extremely complex. At least some of this complexity is attributable to the large number of instances of computing resources and other types of resources that typically exist in such a service at any given time. For example some network based services might utilize dozens of data centers around the world hundreds of thousands or even millions of server computers along with large numbers of networking components software programs and other types of resources.

In some instances a thermal event can occur in a data center. The thermal event generally means that a temperature in a data center is excessive or can become excessive for running of server computers and other electronic components. One simple example that can cause a thermal event is when one or more air conditioning units fail. In the case of a thermal event electronic devices can start to overheat and randomly shut down. Once the thermal event is over the devices can randomly start back up which can quickly increase temperature causing another thermal event.

In some implementations a resource monitoring component executes within or in conjunction with the distributed environment and collects data regarding the state of the resources . For example the resource monitoring component can collect data that describes the usage characteristics of resources . The usage characteristics can include CPU load memory usage number of instances on hosts temperature within a host temperature of a zone in the data center an amount of time that an instance has been operational redundancy of instances instance types etc.

In some embodiments once the data is obtained the resource monitoring component can store the data in a metrics data store . The data store or the resource monitoring component can allow the collected data to be made available for consumption and use by other components. For example in some embodiments the resource monitoring component is configured to expose an application programming interface API or another mechanism through which interested parties can request and receive the data collected for a particular resource . It should be appreciated that while the data is discussed herein primarily in the context of data describing the operational state of a resource the metrics stored in the data store can include other information about a resource such as information describing the configuration of the resource and other aspects of a resource such as its location in the data center a length of time that the resource has been operational etc. In this way the resource monitoring component can be utilized to obtain virtually any type of information about a resource in the distributed environment .

A ranking service can retrieve metrics from the metrics data store and use the metrics to determine a priority order in which to power down the resources in the case of an emergency such as a thermal event. Metrics from the data store can include a number of instances running on a host a length of time that an instance has been operational a type of instance an amount of CPU use on a host etc. Together such metrics can be considered operational parameters as they are associated with the operation of a host server computer. The ranking service can also obtain other parameters or metrics from other sources as described herein. The parameters can include whether redundant or failover instances exist the importance of the instances in the overall customer structure whether the customer itself is considered important other generic parameters from the customer account a customer provided ranking of instances etc. The ranking service can further receive weighting information which can be used to weight an impact of the parameters on an overall ranking order. The weighting information can be retrieved from a policy store which can be accessed by an administrator through a portal.

The ranking service can be coupled to a power down up service . Generally the ranking service provides an overall cost of powering down a hardware resource. The power down up service uses the cost information from the ranking service together with a calculation of a benefit derived from powering down a hardware resource in order to make a final determination an order in which to power down the resources. For example the power down up service can receive information regarding heat generated by a host and a location of a host. Such information can be supplied through a database having fields populated by an administrator of the data centers . Additionally the power down up service can pull temperature data and location within the data center from which the temperature data was acquired. Other information can also be obtained such as data from an HVAC system or building management service describing an operation of a cooling system and its location. The power down up service can use the temperature data or the HVAC data to detect a thermal event. For example if the temperature data exceeds a predetermined threshold then the power down up service can take corrective action. By correlating the location of the host with the location data associated with the temperature or cooling system the power down up service can make an assessment of whether powering down a host server computer will have sufficient impact on mitigation of the thermal event. Additionally having information regarding the heat generated by each host the power down up service can make a determination of the overall impact of powering down a host server computer. Although not shown other parameters can be used in both the power down up service and the ranking service . For example if the host server computer executes management functions for the data center it can be considered an important host server computer. Thus a host server computer that executes the ranking service or the power down up service can be considered important services that are only shut down after other less important options are exhausted. Additionally some of the parameters illustrated can be ignored or eliminated.

Once the power down up service determines which hardware resources to power down it can communicate with the hardware resources directly through established protocols. After powering down one or more host server computers or other hardware resources the power down up service can wait a predetermined period of time and then receive additional temperature data or HVAC data which can be pulled and stored by the resource monitoring component at periodic intervals. Using this information the power down up service can determine whether powering down one or more host server computers has had an impact on the thermal event. Thus a feedback loop is established wherein the power down up service iteratively repeats powering down hardware resources until the thermal event has passed.

The instances of resources provided by the distributed environment are enabled in one implementation by one or more data centers A N which may be referred to herein singularly as a data center or collectively as the data centers . The data centers are facilities utilized to house and operate computer systems and associated components. The data centers typically include redundant and backup power communications cooling and security systems. The data centers might also be located in geographically disparate locations. A client device not shown can supply rankings to software instances executing in the data centers which can be stored in database . Alternatively or in addition the customer can provide metadata including a structure of the instances which can include dependencies between the instances autoscaling information failover etc. Additionally the client device can provide weighting information stored in a policy document .

A resource monitoring component can be coupled to one or more databases and to a metrics data store . In this embodiment the resource monitoring component is positioned outside of the data centers so as to collect data in parallel from the data centers. As previously described the resource monitoring component can be positioned within one or more of the data centers . A ranking service can be coupled to the metrics data store to collect metrics data. The metrics data can include information about instances executing on the host server computers within the data centers . For example the information can include a number of instances running on the host a length of time that instances have been executing instance types etc. The metrics data can also include other information about the hardware resources such as operational data. For example an amount of CPU use on a host can be included as one parameter of the operational data. Other parameters can also be used. In an alternative embodiment not shown the ranking service can be coupled directly to the resource monitoring component so as to eliminate the need for the metrics data store. The ranking service can use the metrics data together with weightings from the policy document to generate a priority order in which hardware resources in the distributed environment should be powered down.

A power down up service can retrieve temperature data or HVAC data from the metrics data store in order to determine if a thermal event is occurring. If a thermal event is detected the power down up service can use ranking information from the ranking service regarding which host server computers to power down in a priority order. The power down up service can also receive benefit parameters indicating heat generated by different host server computers location of the host server computers etc. Such benefit information can be retrieved from a database not shown that can be populated by a network administrator. Additionally the power down up service can retrieve weighting information from the policy document if desired. Using predetermined algorithms programmed into the power down up service it can determine a priority order in which to power down hardware resources such as host server computers. The power down up service can then communicate through standard protocols to the hardware resources in the distributed environment in order to implement the power down.

The ranking service and power down service can be positioned within each data center such that each data center can manage power down operations independently of the other data centers.

The data center may be part of a larger computing system operated by a compute service that includes several data centers across any number of geographical areas. The various data centers may communicate via a network which can be the network or another network. The network may be a wide area network WAN a collection of networks operated by distinct entities such as the Internet or some other network. The compute service can provide computing and storage capacity to a single operator such as a single enterprise e.g. a company or university . The computing services may include web hosting data backup and mirroring disaster prevention and the like. In another embodiment the compute service provides such computing services and storage capacity to a variety of independent customers such as a number of different business entities. In yet another embodiment the compute service can provide computing services and storage capacity to users in the general public.

Customers may access the services on demand or on a subscription basis. In some embodiments the customers of the compute service may specify or select a particular computing device hardware and software configuration to use. Customers may then connect to a different physical computing device which satisfies the chosen hardware configuration each time the customer initiates a computing session. Virtual machine images of the chosen software configuration may be dynamically loaded or instantiated on a computing device as part of a computing session initialization process.

As illustrated in the datacenter may include any number of rooms in which computing devices and other datacenter components that provide the services described above or which support components which provide the services are physically located. The datacenter may also include a cooling system a power system and the network . For example the datacenter typically has a power system that connects to a power source such as the local power grid. The power system may include a power generator for backup or as a primary power source. The power system provides power to the various datacenter components including the cooling system the network and also the rooms .

The various components of the datacenter may emit heat that can be harmful to the function of the components themselves and to other components nearby. Therefore the data center may include a cooling system such as an air conditioner that regulates the temperate of the datacenter and its various rooms and components. In some embodiments a more powerful or more efficient cooling system may be provided instead of or in addition to an air conditioner. For example some datacenters may include a cooling loop that circulates chilled water throughout the datacenter and various rooms thereof and a condenser or evaporative waterfall to cool the water after it has absorbed heat from the datacenter components.

The network can be provided by a number of components such as routers switches hubs and the like. The network components may communicate via cables or wirelessly. In some embodiments there may be several core switches and or routers with which the network components of the various rooms communicate to provide redundancy and fault tolerance.

Broadly described as shown by the hierarchy in the data center includes rooms which in turn include racks . The racks include servers and or network components . A room of the data center can encapsulate a number of data center components and further hierarchical levels. For example a room may include any number of racks of computing devices a cooling system component such as any number of computer room air conditioning CRAC units any number of power system components such as power distribution units PDUs and any number of network components in communication with the network of the data center .

The PDUs may include one or more room level PDUs which each serve power to several racks . In such cases the room level PDUs may connect to rack level PDUs via cables and power whips. The rack level PDUs can then distribute power to the devices of the rack . In addition the room level PDUs can provide power to the CRAC unit and the network components .

The network components include room level switches and or routers which facilitate communication between the computing devices housed in the racks and the network of the data center . For example a room level switch may facilitate communication between computing devices on separate racks within the same room. Additionally the room level switch may in combination with the core routers of the data center facilitate communication between computing devices in different rooms or even different data centers and other computing devices outside the network computing provider environment.

The rack may be any frame or enclosure capable of mounting one or more servers or other computing devices. For example the rack can be a four post server rack a server cabinet an open frame two post rack a portable rack a LAN rack combinations of the same or the like. In some embodiments the computing devices mounted on the rack may be networking components such as switches or routers instead of or in addition to servers. For example the data center room can have in addition to racks which contain servers one or more racks which may contain any number of switches. In some embodiments a data center room may contain only one rack or may contain zero racks . For example a data center room may have servers embodied as one or more large scale computing devices such as computing appliances or midrange computers which may not be grouped together physically in a rack .

The rack may also encapsulate a number of data center components and additional hierarchical levels such as PDUs servers and network components . For example the rack may include any number of PDUs and other datacenter components such as power whips and the like for providing power from the room level PDUs to the servers and network components mounted in or associated with the rack . The network components of the rack can include top of rack TOR switches which provide network connectivity between the room level network components and the servers . The network components can also be powered by the rack level PDUs .

Each server can comprise additional data center components each of which may be monitored such as a processing unit a network interface computer readable medium drive and a memory. The memory generally includes RAM ROM and or other persistent or non transitory memory and may contain a hypervisor for managing the operation and lifetime of one or more virtual machine VM instances. In some embodiments the VM instances are also data center components.

As described above servers can be configured to host VMs at the request of customers of the network computing provider operating the data center . For example a business entity may rent computing and storage capacity from the network computing provider and may choose a VM configuration or have a VM machine image customized for their needs. A single server may at any time have one two or possibly many more VMs operating on behalf of customers actively processing data responding the customer requests and the like. In some embodiments the VM s on a given server may be operating on behalf of one two or possibly many different customers. In some embodiments the server need not host VMs and therefore the server may not have a hypervisor or VMs in memory.

The server which launches the VM for the customer may receive power through a power cable from a rack level PDU of the rack on which the server is located. The rack level PDU may in turn receive power through one or more power whips or cables from a room level PDU . The power may pass through any number of PDUs in between the rack level PDU and room level PDU . The room level PDU can draw power from the power system of the data center . The power may come from another PDU or directly from an on site generator or power source or from a link to the local power grid outside of the data center . One or more battery backup units BBUs can be provided for use in a power failure. A BBU can be dedicated to a rack of datacenter components a single datacenter component e.g. connected to or associated with the PDU or more than one datacenter component which can be located on one or more racks.

Each datacenter component involved in the illustrative communication described above can generate heat as it transfers power or communications or performs other computing operations. Heat can cause the data center components to become damaged or otherwise malfunction and similarly impact nearby components such as wiring servers network components PDUs etc. In order to dissipate the heat a room level component of the data center cooling system may be used such as a CRAC . In some embodiments rack level cooling units may also be implemented including fans pipes carrying chilled water and the like. Either rack level or room level cooling components and systems may be connected to a datacenter cooling system such as a chiller loop. As will be appreciated the cooling components of the data center may also be coupled to the power system of the data center as described above with respect the servers i.e. fans compressors and pumps typically require electrical power to operate . Sensors can be used to measure temperature and to determine the amount of heat that needs to be removed from a room and or a rack and or data center components. The sensors can also be coupled to an HVAC system for the building or other building management services that can detect that the cooling system is malfunctioning or powered down such that it is not operational.

The particular illustrated compute service includes a plurality of server computers A D. While only four server computers are shown any number can be used and large centers can include thousands of server computers. The server computers A D can provide computing resources for executing software instances A D. In one embodiment the instances A D are virtual machines. As known in the art a virtual machine is an instance of a software implementation of a machine i.e. a computer that executes applications like a physical machine. In the example of virtual machine each of the servers A D can be configured to execute an instance manager capable of executing the instances. The instance manager can be a hypervisor or another type of program configured to enable the execution of multiple instances on a single server. Additionally each of the instances can be configured to execute one or more applications.

It should be appreciated that although the embodiments disclosed herein are described primarily in the context of virtual machines other types of instances can be utilized with the concepts and technologies disclosed herein. For instance the technologies disclosed herein can be utilized with storage resources data communications resources and with other types of computing resources. The embodiments disclosed herein might also execute all or a portion of an application directly on a computer system without utilizing virtual machine instances.

One or more server computers can be reserved for executing software components for managing the operation of the server computers and the instances . For example the server computer can execute a management component . A customer can access the management component to configure various aspects of the operation of the instances purchased by the customer. For example the customer can purchase rent or lease instances and make changes to the configuration of the instances. The customer can also specify settings regarding how the purchased instances are to be scaled in response to demand. The management component can further include the policy document . An auto scaling component can scale the instances based upon rules defined by the customer. In one embodiment the auto scaling component allows a customer to specify scale up rules for use in determining when new instances should be instantiated and scale down rules for use in determining when existing instances should be terminated. The auto scaling component can consist of a number of subcomponents executing on different server computers or other computing devices. The auto scaling component can monitor available computing resources over an internal management network and modify resources available based on need.

A deployment component can be used to assist customers in the deployment of new instances of computing resources. The deployment component can have access to account information associated with the instances such as who is the owner of the account credit card information country of the owner etc. The deployment component can receive a configuration from a customer that includes data describing how new instances should be configured. For example the configuration can specify one or more applications to be installed in new instances provide scripts and or other types of code to be executed for configuring new instances provide cache logic specifying how an application cache should be prepared and other types of information. The deployment component can utilize the customer provided configuration and cache logic to configure prime and launch new instances . The configuration cache logic and other information may be specified by a customer using the management component or by providing this information directly to the deployment component .

Customer account information can include any desired information associated with a customer of the multi tenant environment. For example the customer account information can include a unique identifier for a customer a customer address billing information licensing information customization parameters for launching instances scheduling information auto scaling parameters previous IP addresses used to access the account etc.

A network can be utilized to interconnect the server computers A D and the server computer . The network can be a local area network LAN and can be connected to a Wide Area Network WAN so that end users can access the compute service . It should be appreciated that the network topology illustrated in has been simplified and that many more networks and networking devices can be utilized to interconnect the various computing systems disclosed herein.

A resource monitoring component can be connected through the local area network to the one or more server computers server computers or other server computers not shown. The resource monitoring component can gather metrics from any of the available server computers. The resource monitoring service can also use any available services offered by the compute service such as the customer account information . As described above a ranking service can communicate with the resource monitoring component or an associated database to acquire the metrics desired to rank the host server computers in terms of importance. A power down up service can communicate with the ranking service and other available storage or services in order to determine which of the server computers to power down or power up in a priority order.

Other general management services that may or may not be included in the compute service include an admission control e.g. one or more computers operating together as an admission control web service. The admission control can authenticate validate and unpack the API requests for service or storage of data within the compute service . The capacity tracker is responsible for determining how the servers need to be configured in order to meet the need for the different instance types by managing and configuring physical inventory in terms of forecasting provisioning and real time configuration and allocation of capacity. The capacity tracker maintains a pool of available inventory in a capacity pool database . The capacity tracker can also monitor capacity levels so as to know whether resources are readily available or limited. The instance manager controls launching and termination of instances in the network. When an instruction is received such as through an API request to launch an instance the instance manager pulls resources from the capacity pool and launches the instance on a decided upon host server computer. Similar to the instance manager are the storage manager and the network resource manager . The storage manager relates to initiation and termination of storage volumes while the network resource manager relates to initiation and termination of routers switches subnets etc. A network of partitions is described further in relation to and includes a physical layer upon which the instances are launched.

The resource monitoring component was described above in relation to and . In addition to what was already described the resource monitoring component can provide monitoring of the applications customers run on the compute service . System administrators can use the resource monitoring component to collect and track metrics and gain insight to how applications are running. For example the resource monitoring service can allow system wide visibility in which server computers are being utilized. Examples of information that can be obtained includes a length of time that an instance has been operational an amount of CPU use on a server computer a number of instances running on a host server computer temperature information such as those obtained from sensors of etc. The resource monitoring component can also be coupled to the instance manager which can alternatively provide information about the instances. For example the instance manager can provide additional metrics such as instance types executing on the host server computers. Metrics generated by the resource monitoring component can be stored in the metrics database . The ranking service can access the metrics in database to determine any desired metrics associated with instances running in the network of partitions . As indicated a policy store which stores policy information can also be available to the rankings service in order to provide weighting information to the ranking service. Furthermore customer provided rankings can be available to the ranking service . Other inputs to the ranking service can also be used based on the particular design implementation. A power down up service can be coupled to the ranking service to receive ranking information regarding which of the host partitions to power down first when there is a thermal event. The power down up service can also be coupled to a database in order to receive benefit parameters. The power down up service can further be coupled to the metrics database in order to obtain any desired data such as temperature data. Such temperature data can be used to determine if there is a thermal event. When a thermal event occurs the power down up service can communicate with hardware resources via connection to the network of partitions . Using established protocols e.g. SSH power down commands for example the power down up service can communicate with host server computers or other hardware resources within the network of partitions in order to power down them down in a systematic and ordered fashion. Using similar protocols the power down up component can power up the host server computers and other hardware resources in an intelligent manner.

Each host has underlying hardware including one or more CPUs memory storage devices etc. The hardware can also include one or more temperature sensors that provide an internal temperature within the host. Running a layer above the hardware is a hypervisor or kernel layer . The hypervisor or kernel layer can be classified as a type 1 or type 2 hypervisor. A type 1 hypervisor runs directly on the host hardware to control the hardware and to manage the guest operating systems. A type 2 hypervisor runs within a conventional operating system environment. Thus in a type 2 environment the hypervisor can be a distinct layer running above the operating system and the operating system interacts with the system hardware. Different types of hypervisors include Xen based Hyper V ESXi ESX Linux etc. but other hypervisors can be used. A management layer can be part of the hypervisor or separated therefrom and generally includes device drivers needed for accessing the hardware . The partitions are logical units of isolation by the hypervisor. Each partition can be allocated its own portion of the hardware layer s memory CPU allocation storage etc. Additionally each partition can include a virtual machine and its own guest operating system. As such each partition is an abstract portion of capacity designed to support its own virtual machine independent of the other partitions.

Any applications executing on the instances can be monitored using the management layer which can then pass the metrics to the resource monitoring component for storage in the metrics database . Additionally the management layer can pass to the monitoring component the number of instances that are running when they were launched the operating system being used the applications being run the temperature sensor information etc. All such metrics can be used for consumption by the resource monitoring component and stored in database . A separate management component not shown can also be used to capture the temperature sensor information and pass the same to the resource monitoring component for storage within the database. The management component can also monitor a status of the cooling system and report any malfunction to the resource monitoring component .

In process block a thermal event can be detected. Detection of a thermal event can be achieved by analyzing sensors in the data center such as shown in at at and or temperature sensors within the host server computers. In the case of temperature information it can be compared against one or more predetermined thresholds. For example if the temperature of a host server computer exceeds a manufacture recommended temperature then the server computer can be considered to have a thermal event. More typically a temperature of the entire data center or a zone of the data center is monitored to determine if it is above a desired limit which could impact multiple server computers. Alternatively if the thermal event relates to the cooling system an assessment can be made whether the problem with the cooling system can be considered a thermal event. The determination of when there is a thermal event can be within the control of the power down up service for purposes of clarity only the power down up service will be used below but it is understood that the other embodiments can equally be used. In some embodiments the power down up service can retrieve the predetermined threshold temperature information from the policy store see or the policy store can control a definition of what is considered a thermal event in terms of cooling system problems. In other embodiments a separate service not shown can determine that there is a thermal event and communicate the same to the power down up service for carrying out the necessary power down sequence.

In process block the assigned importance levels can be used to power down the hardware resources in a priority order. Generally less important hardware resources are powered down before more important hardware resources. Desirably the more important resources will not need to be powered down at all. To power down the resources established protocols can be used to communicate directly with the hardware resources. For example the power down up service can use established protocols e.g. Unix commands in order to power down devices in an orderly fashion.

In process block detection can be made that the thermal event has passed. While the power down up service shuts down select hardware resources in a data center it may interactively check for updated thermal data in order to determine if the thermal event has passed. For example powering down select host server computers should have an impact on the temperature in the data center. The temperature data from the temperature sensors in the data center can be periodically updated so that the power down up service can iteratively check an impact of the power down sequence. Once it is determined that sufficient hardware resources have been powered down to control the thermal event it can be determined that the thermal event has passed. For example a second threshold temperature can be used to determine that the thermal event has passed once the temperature is below that second threshold temperature. The second threshold temperature can be different than the first threshold temperature. Example temperatures can be 100 degrees for a first threshold to detect that a thermal event has occurred and 75 degrees for a second threshold temperature to determine that the thermal event has passed. When powering down different hardware resources a list can be generated in some embodiments indicating which hardware resources were powered down. Such a list can be used to power up the hardware resources as explained further below.

In process block the hardware resources can be powered up using a priority order. For example the power down up service can obtain the rankings from the metrics database to begin powering up hardware resources in a predetermined order. In some embodiments the power up order can differ from the power down order. For example different weightings can be used for each. Additionally the list of resources that were powered down can be used to determine which hardware resources to power back up in accordance with the rankings. Alternatively the power up commands can be sent in accordance with the rankings and those hardware resources already powered up can ignore the request while others can power up. Still further a separate service can monitor a power state of the hardware resources and report the same to the power down up service. The power down up service can iteratively monitor temperature of the data center to ensure that the temperature does not exceed a threshold temperature. Such a temperature can be yet a third threshold different than the first and second thresholds described above. As long as the temperature remains below the third threshold the power down up service can continue to power up additional hardware resources in accordance with the rankings.

In process block a cost benefit analysis can be calculated. For example an algorithm can be used to calculate the cost benefit analysis. An example algorithm can be as follows cost benefit rankingweight1 ranking parameter 1 rankingweight2 ranking parameter 2 . . . rankingweightN ranking parameter N benefitweight1 benefit parameter 1 benefitweight2 benefit parameter2 benefitweightM benefit parameter where N and M are any integer values. The benefit and ranking weight information can be obtained from the policy store e.g. in . In process block the cost benefit analysis can be used to determine whether powering down the hardware resource mitigates the thermal event with minimal impact to customers. For example a projection can be made on an amount of heat budget needs to be reduced as a projection due to a cooling system shutting down. The cost benefit analysis includes a determination of whether powering down a set of host server computers meets or exceeds the projection. If so then the power down up service can automatically power down the hardware resource using techniques already described above.

One or more of the process blocks of can be repeated and powered down individually based on a cost benefit assessment in order to mitigate the thermal event. Any of the costs parameters previously described can be used. For example the costs can be related to a number of instances executing on a hardware resource or a type of instances executing on a hardware resource. The cost can further be related to whether instances executing on the hardware resource are operationally important to the customer. Such information can be supplied by the customer in terms of dependencies associated with an instance or ranking information associated with an instance. The benefit can be associated with a location of the hardware resource relative to the thermal event or an amount of heat generated by the hardware resource.

With reference to the computing environment includes one or more processing units and memory . In this basic configuration is included within a dashed line. The processing units execute computer executable instructions. A processing unit can be a general purpose central processing unit CPU processor in an application specific integrated circuit ASIC or any other type of processor. In a multi processing system multiple processing units execute computer executable instructions to increase processing power. For example shows a central processing unit as well as a graphics processing unit or co processing unit . The tangible memory may be volatile memory e.g. registers cache RAM non volatile memory e.g. ROM EEPROM flash memory etc. or some combination of the two accessible by the processing unit s . The memory stores software implementing one or more innovations described herein in the form of computer executable instructions suitable for execution by the processing unit s .

A computing system may have additional features. For example the computing environment includes storage one or more input devices one or more output devices and one or more communication connections . An interconnection mechanism not shown such as a bus controller or network interconnects the components of the computing environment . Typically operating system software not shown provides an operating environment for other software executing in the computing environment and coordinates activities of the components of the computing environment .

The tangible storage may be removable or non removable and includes magnetic disks magnetic tapes or cassettes CD ROMs DVDs or any other medium which can be used to store information in a non transitory way and which can be accessed within the computing environment . The storage stores instructions for the software implementing one or more innovations described herein. For example either the ranking service and or the power down up service can be the software .

The input device s may be a touch input device such as a keyboard mouse pen or trackball a voice input device a scanning device or another device that provides input to the computing environment . The output device s may be a display printer speaker CD writer or another device that provides output from the computing environment .

The communication connection s enable communication over a communication medium to another computing entity. The communication medium conveys information such as computer executable instructions audio or video input or output or other data in a modulated data signal. A modulated data signal is a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media can use an electrical optical RF or other carrier.

Although the operations of some of the disclosed methods are described in a particular sequential order for convenient presentation it should be understood that this manner of description encompasses rearrangement unless a particular ordering is required by specific language set forth below. For example operations described sequentially may in some cases be rearranged or performed concurrently. Moreover for the sake of simplicity the attached figures may not show the various ways in which the disclosed methods can be used in conjunction with other methods.

Any of the disclosed methods can be implemented as computer executable instructions stored on one or more computer readable storage media e.g. one or more optical media discs volatile memory components such as DRAM or SRAM or nonvolatile memory components such as flash memory or hard drives and executed on a computer e.g. any commercially available computer including smart phones or other mobile devices that include computing hardware . The term computer readable storage media does not include communication connections such as signals and carrier waves. Any of the computer executable instructions for implementing the disclosed techniques as well as any data created and used during implementation of the disclosed embodiments can be stored on one or more computer readable storage media. The computer executable instructions can be part of for example a dedicated software application or a software application that is accessed or downloaded via a web browser or other software application such as a remote computing application . Such software can be executed for example on a single local computer e.g. any suitable commercially available computer or in a network environment e.g. via the Internet a wide area network a local area network a client server network such as a cloud computing network or other such network using one or more network computers.

For clarity only certain selected aspects of the software based implementations are described. Other details that are well known in the art are omitted. For example it should be understood that the disclosed technology is not limited to any specific computer language or program. For instance the disclosed technology can be implemented by software written in C Java Perl JavaScript Adobe Flash or any other suitable programming language. Likewise the disclosed technology is not limited to any particular computer or type of hardware. Certain details of suitable computers and hardware are well known and need not be set forth in detail in this disclosure.

It should also be well understood that any functionality described herein can be performed at least in part by one or more hardware logic components instead of software. For example and without limitation illustrative types of hardware logic components that can be used include Field programmable Gate Arrays FPGAs Program specific Integrated Circuits ASICs Program specific Standard Products ASSPs System on a chip systems SOCs Complex Programmable Logic Devices CPLDs etc.

Furthermore any of the software based embodiments comprising for example computer executable instructions for causing a computer to perform any of the disclosed methods can be uploaded downloaded or remotely accessed through a suitable communication means. Such suitable communication means include for example the Internet the World Wide Web an intranet software applications cable including fiber optic cable magnetic communications electromagnetic communications including RF microwave and infrared communications electronic communications or other such communication means.

The disclosed methods apparatus and systems should not be construed as limiting in any way. Instead the present disclosure is directed toward all novel and nonobvious features and aspects of the various disclosed embodiments alone and in various combinations and subcombinations with one another. The disclosed methods apparatus and systems are not limited to any specific aspect or feature or combination thereof nor do the disclosed embodiments require that any one or more specific advantages be present or problems be solved.

For example although embodiments show the power down up service as being separate from the ranking service they can be combined into a single service. Alternatively they can be further sub divided. Thus logical partitioning of such services is merely a design chose based on a particular implementation.

In view of the many possible embodiments to which the principles of the disclosed invention may be applied it should be recognized that the illustrated embodiments are only preferred examples of the invention and should not be taken as limiting the scope of the invention. Rather the scope of the invention is defined by the following claims. We therefore claim as our invention all that comes within the scope of these claims.

