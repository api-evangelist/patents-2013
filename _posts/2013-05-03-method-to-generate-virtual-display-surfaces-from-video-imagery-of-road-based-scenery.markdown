---

title: Method to generate virtual display surfaces from video imagery of road based scenery
abstract: Generating a virtual model of environment in front of a vehicle based on images captured using an image capturing. The Images captured on an image capturing device of a vehicle are processed to extract features of interest. Based on the extracted features, a virtual model of the environment is constructed. The virtual model includes one or more surfaces. Each of the surfaces may be used as a reference surface to attach and move graphical elements generated to implement augmented reality (AR). As the vehicle moves, the graphical elements move as if the graphical elements are affixed to the one of the surfaces. By presenting the graphical elements to move together with real objects in front of the vehicle, a driver perceives the graphical elements as being part of the actual environment and reduces distraction or confusion associated with the graphical elements.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09135754&OS=09135754&RS=09135754
owner: Honda Motor Co., Ltd.
number: 09135754
owner_city: Tokyo
owner_country: JP
publication_date: 20130503
---
This application claims priority under 35 U.S.C. 119 e to co pending U.S. Provisional Patent Application No. 61 643 792 filed on May 7 2012 which is incorporated by reference herein in its entirety.

The present disclosure is related to presenting graphical elements on a display device of a vehicle to implement augmented reality.

A driver of a vehicle is inundated with information from various sensory organs when driving a vehicle. In addition to sounds and vibrations from various sources the driver also receives a large amount of visual information while driving the vehicle. Sources of such visual information may include external stimuli such as people and buildings on the sides of the street advertising and infotainment applications in the dash displays. Such sources of visual information may often distract the driver and causing the driver to focus his attention away from the traveling direction of the vehicle.

Augmented reality systems in vehicles are being developed to reduce distractions to the driver while providing visual information to assist in various activities in the vehicles. Augmented reality systems in vehicles may use flat panel display devices e.g. liquid crystal display LCD installed in a dashboard or console of a vehicle or heads up display HUD that displays graphical elements on a windshield of a vehicle. The augmented reality systems can also enhance safety by enhancing the saliency of road hazards or suggesting safe actions for a particular driving context.

However augmented reality systems may themselves become a cause of distraction if information is not presented to the driver in an adequate manner. If the graphical elements are too vibrant inadequately animated or abruptly show up at critical moments images presented on the display devices in the vehicle may contribute to accidents or unsafe driving rather than enhancing safety. Hence the graphical elements presented on the vehicle should be designed to reduce the driver s distraction while providing various types of helpful visual information to the driver.

Embodiments relate to constructing a virtual model using visual features extracted from an image captured by an image capturing device e.g. a camera installed in a vehicle. The visual features can be used to determine the position of a road relative to the vehicle. The virtual model includes at least one surface which is referenced to configure and locate graphical elements on a display device. The graphical elements convey information to the driver.

In one embodiment the display device is a heads up display projecting the graphical elements on a windshield of the vehicle.

In one embodiment the virtual model represents a trench including a left wall a right wall a ground plane and a sky plane parallel to the ground plane.

In one embodiment the configured graphical elements move in conjunction with the at least one surface to generate an appearance that the configured graphical elements are fixed to the at least one surface.

In one embodiment the visual features are detected by cropping the captured image to extract a portion of image including outline of a road and detecting line features representing edges of the road from the cropped image.

In one embodiment inverse perspective mapping is performed on the cropped image to generate a bird s eye view image. A separable filter is used on the bird s eye view image to generate a filtered image. The line features are detected from the filtered image. Curve fitting is performed on the detected line features.

In one embodiment the location and heading of the vehicle are determined. Information is then received from an information source based on the location or heading of the vehicle.

In one embodiment a profile of a user and the location or heading of the vehicle are used to generate the graphical elements.

In one embodiment the location of the sun relative to the vehicle is determined based on a current time the heading of the vehicle and the location of the vehicle. Shadows of at least one of the configured graphical elements are generated based on the location of the sun relative to a position of the vehicle on the Earth s surface and the time of day. The generated shadows are displayed on the display device.

In one embodiment a pedestrian likely to cross the road in front of the vehicle is detected. A graphical element representing a crosswalk at a portion of the road across which the pedestrian is likely to cross the road is then generated.

The features and advantages described in the specification are not all inclusive and in particular many additional features and advantages will be apparent to one of ordinary skill in the art in view of the drawings specification and claims. Moreover it should be noted that the language used in the specification has been principally selected for readability and instructional purposes and may not have been selected to delineate or circumscribe the subject matter.

A preferred embodiment is now described with reference to the figures where like reference numbers indicate identical or functionally similar elements.

Reference in the specification to one embodiment or to an embodiment means that a particular feature structure or characteristic described in connection with the embodiments is included in at least one embodiment. The appearances of the phrase in one embodiment in various places in the specification are not necessarily all referring to the same embodiment.

Some portions of the detailed description that follows are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here and generally conceived to be a self consistent sequence of steps instructions leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical magnetic or optical signals capable of being stored transferred combined compared and otherwise manipulated. It is convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like. Furthermore it is also convenient at times to refer to certain arrangements of steps requiring physical manipulations of physical quantities as modules or code devices without loss of generality.

However all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussion it is appreciated that throughout the description discussions utilizing terms such as processing or computing or calculating or determining or displaying or determining or the like refer to the action and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical electronic quantities within the computer system memories or registers or other such information storage transmission or display devices.

Certain aspects of the embodiments include process steps and instructions described herein in the form of an algorithm. It should be noted that the process steps and instructions could be embodied in software firmware or hardware and when embodied in software could be downloaded to reside on and be operated from different platforms used by a variety of operating systems.

Embodiments also relate to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes or it may comprise a general purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium such as but is not limited to any type of disk including floppy disks optical disks CD ROMs magnetic optical disks read only memories ROMs random access memories RAMs EPROMs EEPROMs magnetic or optical cards application specific integrated circuits ASICs or any type of media suitable for storing electronic instructions and each coupled to a computer system bus. Furthermore the computers referred to in the specification may include a single processor or may be architectures employing multiple processor designs for increased computing capability.

The algorithms and displays presented herein are not inherently related to any particular computer or other apparatus. Various general purpose systems may also be used with programs in accordance with the teachings herein or it may prove convenient to construct more specialized apparatus to perform the required method steps. The required structure for a variety of these systems will appear from the description below. In addition the embodiments are not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings as described herein and any references below to specific languages are provided for disclosure of enablement and best mode.

In addition the language used in the specification has been principally selected for readability and instructional purposes and may not have been selected to delineate or circumscribe the subject matter. Accordingly the disclosure is intended to be illustrative but not limiting of the scope which is set forth in the following claims.

Embodiments relate to generating a virtual model of environment in front of a vehicle based on images captured using an image capturing device e.g. a camera . The Images captured on the image capturing device are processed to extract features of interest e.g. outline of a road . Based on the extracted features a virtual model of the environment is constructed. The virtual model includes one or more surfaces. Each of the surfaces may be used as a reference surface to attach and move graphical elements generated to implement augmented reality AR . As the vehicle moves the graphical elements move as if the graphical elements are affixed to one of the surfaces in the virtual model. By presenting the graphical elements to move together with real objects in front of the vehicle a driver perceives the graphical elements as being part of the actual environment and reduces distraction or confusion associated with the graphical elements.

The graphical elements described herein refer to visual image components displayed on a display device. The graphical elements may include but are not limited to text messages arrows icons animated objects three dimensional models and static or dynamic images. The graphical elements may also include static or dynamic advertisements.

Figure is a block diagram illustrating an augmented reality system according to one embodiment. The augmented reality system detects environment in front of a vehicle and positioning of the vehicle in the environment and displays virtual objects in a display device of the vehicle in a non distracting manner. The augmented reality system may include among other components an image capturing device an image processing module a positioning device a clock an information source an augmented reality processor and a heads up display HUD system .

The image capturing device is a device for capturing an image of environment in front of the vehicle. Preferably the images captured by the image capturing device closely resembles the images seen by a driver through a windshield. The image capturing device may be located at a location close to the driver to produce such images. The image capturing device may be an RGB camera a grayscale camera stereoscopic camera a depth camera or a combination of thereof. The image capturing device produces processed images that are sent to the image processing module for preprocessing.

The image processing module performs preprocessing on the captured images to facilitate extraction of features at the augmented reality processor . The preprocessing performed at the image processing module may include among others edge detection and noise filtering.

The positioning device is hardware software firmware or a combination thereof for detecting the location and heading of the vehicle. The positioning device may include for example a global positioning system GPS receiver a compass and an inertial sensor. The positioning device may also communicate with other sensors e.g. an odometer and a vehicle direction sensor to continue monitoring the location and heading of the vehicle even when GPS signals are not available. The positioning device generates positioning information including the location and the heading of the vehicle to the augmented reality processor .

A clock provides time information to the augmented reality processor . The time information indicates the current time and may be used to generate shadows of graphical elements as described below in detail with reference to .

The information source is a device for storing information to be displayed on the HUD system . The information source may include more than one vehicle component providing distinct information on the HUD systems . The information source may include for example an on board navigation system a telematics system a wireless communication system having Internet connectivity and an entertainment system in the vehicle. The information source sends data to augmented reality processor for processing and generation of graphical elements.

The augmented reality processor processes various types of information and generates graphical elements for display on the HUD system . To provide information to the driver in a seamless and non intrusive manner the augmented reality processor generates a virtual model of the environment and places the graphical elements on a virtual surface of the virtual model as described below in detail with reference to . The graphical elements generated by the augmented reality processor are sent to the HUD system as graphical data . Example components and functions of the components in the augmented reality processor are described below in detail with reference to .

The HUD system receives the graphical data from the augmented reality processor and projects the graphical elements indicated by the graphical data onto the windshield of the vehicle. In one embodiment the HUD system is implemented as a volumetric HUD that displays graphical elements at different focal depths. The HUD system may include among other components a projector for projecting rendered images and a windshield for reflecting the projected images towards the driver.

Although the embodiments are described herein primarily with reference to a HUD system display devices such as liquid crystal display LCD device organic light emitting diode OLED device plasma display device or other devices may also be used. These display devices may be installed in a dashboard of a vehicle or in a center console area of a vehicle to provide visual information to the driver. Also a display device projecting an image onto a transparent reflective surface may be used to render an image on a transparent surface.

The processor is a hardware component that executes instructions stored in memory . The input module is a hardware component for receiving information or data from other components e.g. the input processing module of the augmented reality system . The input module may implement communication protocols to ensure proper transmission of data from other components of the augmented reality system . The output module is a component for sending the graphical data to the HUD system . These components may be embodied in a single integrated circuit IC chip or multiple IC chips.

The memory is a non transitory computer readable storage medium that stores software components. The software components stored in the memory may include among other components an information processor image feature detectors an augmented reality AR model generator a pedestrian detector an environment state analyzer a graphical element generator and a graphical image renderer . The components may be divided into further software components or more than one of these components may be combined into a single larger software module.

The information processor determines information or messages to be presented to the driver in the form of graphical elements. To provide information relevant to the driver the information processor may receive a profile of the driver GPS information of the vehicle information from social networking services e.g. FACEBOOK or other online services and the information source . The profile of the drive may include for example age gender resident address log on information for social networking services and office address. The information or messages to be presented may include targeted advertisements street names names of points of interest POIs weather conditions status of phone calls contact information advertisements and SMS messages or emails received via a communication device. The information processor sends presentation information indicating the information or messages to the graphical element generator .

In one embodiment the information processor determines traffic rules associated with traffic signals or traffic signs detected by the vehicle. The information processor may generate and send graphical elements to indicate warnings to the driver based on traffic signals or traffic signs detected by processing the captured image .

The image feature detectors detect predetermined features in the processed images . The detected features may include visual features such as lines edges ridges and blobs of image pixels. Various computer vision algorithms may be used to detect and extract such features. Example algorithms for such purpose include separable filter algorithms and Hough transform algorithm. Environment image information including extracted visual features related to a road is sent to AR model generator pedestrian image including visual features related to pedestrians is sent to the pedestrian detector and traffic information including visual features related to traffic signals may be sent to the information processor .

In one embodiment the image feature detectors detects visual features relevant to various objections in the environment including but not limited to a road a pedestrian traffic signs and traffic signals. Visual features related to the road may include for example lines representing edges of the road blobs of continuous pixels in certain color space road markings or any fixed visible features on the road. Visual features related to a pedestrian may include blobs of pixels satisfying certain criteria. Visual features related to the traffic signs and traffic signals may include color information of pixels appearing in certain locations of the captured image.

The AR model generator receives the environment image information to construct a virtual model representing the environment in front of the vehicle. Line features in the processed images may be used to determine the configuration of the road as described below in detail with reference to . The configuration of the road may indicate for example the orientation and location of the road relative to the vehicle the width of the road and the depth of the road. In one embodiment the AR model generator generates a virtual model in the form of a trench as described below in detail with reference to . The AR model generator sends model information indicating the configuration of the virtual model to the graphical image renderer .

The pedestrian detector receives the pedestrian image from the image feature detectors to identify pedestrians in the processed image . The pedestrian detector also selects pedestrians matching certain criteria. The criteria may indicate any risks to the pedestrian or the vehicle such as likelihood of collision with the pedestrian and cues on whether a pedestrian is likely to start crossing the road e.g. a pedestrian facing the road and or moving towards the road . The location and other information about such selected pedestrians are sent to the graphical element generator as pedestrian information .

The environment state analyzer receives the time information and the positioning information to determine the orientation of the sun relative to the vehicle. The environment state analyzer analyzes the time information and the heading of the vehicle to determine the location of the sun relative to the vehicle. Then the orientation and length of a shadow to be included in the graphical elements can be determined to provide more realistic appearance of the graphical elements. The environment state analyzer generates state information as a result of the analysis and sends the state information to the graphical element generator .

The graphical element generator generates graphical elements based on the pedestrian information the state information the presentation information and or other information and data received from other components of the augmented reality system . The graphical elements may include for example icons indicating warning signals static or dynamic images and text messages. The generated graphical elements may also include a virtual crosswalk as described below in detail with reference to . In one embodiment the graphical elements may include shadows of the graphical elements based on the time of the day and the heading of the vehicle. The generated graphical elements are sent to the graphical image renderer .

The graphical image renderer configures the received graphical elements to be displayed on the HUD device system or other display devices based on the model information . At least some of the graphical elements are configured for placement on virtual surfaces of the virtual model. Specifically at least some of the graphical elements may be dynamically positioned and configured so that these graphical elements are displayed on the display devices as if these graphical elements are part of the real environment. The graphical image renderer includes the graphical elements and their configuration in the graphical data and sends the graphical data to the HUD system via the bus and the output module .

Using visual information available from images captured from an image capturing device e.g. RGB camera stereoscopic camera or depth camera enables expeditious construction of a virtual model of the environment. The visual information may be available from an image capturing device installed on a vehicle. Such visual information may be processed using computer vision algorithms to extract features relevant to construction of a virtual model without extensive computation delay of time or extensive amount of data from an external source compared to construction of a virtual model relying heavily on map data.

The image feature detectors first selects and crops the portion of the processed image for further processing. The portion of image for further processing is also referred to as region of interest ROI . The selected portion may extend vertically downwards to a certain distance from a vertical location near the horizon. The selected portion preferably excludes portions of the image that may interfere with accurate detection of features relevant to the road such as hood of vehicle. The vertical location and the distance of the portion may be preset or may change dynamically with the change of scenery.

After obtaining the edges of the road a curve fitting may be performed to represent the edges of the road. To form a virtual model the edges are fitted to spline curves. Other fitting mechanism may also be used. The area between the two fitted spline curves represents the road lying ahead of the vehicle.

By segmenting the road into multiple segments and constructing a virtual model for each segment the virtual models may represent more realistic version of the environment. As the vehicle moves forward segments move downward causing a new segment to appear at the top of the image and the segment closest to the bottom e.g. segment to disappear below the image. The segments may be updated with new configurations as newly captured images of the road are received and processed. After the configurations of each segments are determined the image feature detectors sends the information about the configurations in the environment image information and sends it to the AR model generator .

In one embodiment Kalman filter is applied to the parameters e.g. the width the orientation and the coordinates of the virtual model to achieve smooth transition or movement of virtual models as subsequent processed images are received and processed by the augmented reality processor .

In one embodiment the following assumption may be made to render a virtual model robustly based on the captured image i the road in front of the vehicle is flat ii there is no clockwise or counterclockwise roll of the camera and iii the image capturing device is placed at a fixed height from the ground. Additional assumptions may be added to facility the generation of the virtual model based on the captured image .

Graphical elements for display may be presented in various manners with reference to the surfaces of the virtual model . Examples of using the side surfaces as reference surfaces include i using side walls to which the graphical elements are plastered as illustrated by boxes ii hanging a signboard extending inwards or outwards from either side wall and iii placing a banner extending between the two side walls . The ceiling may also be used as a reference surface for placing a label or hanging a sign . An example of using the floor as a reference surface for the graphical elements includes placing floor label on the floor . Other graphical elements such as billboard or virtual character e.g. three dimensional animated character as illustrated in may also use the floor as the reference surface for placement.

The graphical elements move in the same direction and speed as the reference surfaces associated with the graphical elements to give appearance that the graphical elements are part of actual environment. In one embodiment graphical elements are displayed only for a virtual model corresponding to a segment of the road that is closest to the vehicle. In other embodiments the graphical elements are displayed for virtual models corresponding two or more segments of the road.

In one embodiment multiple virtual models are constructed based on the configuration of the road. For a straight road segment a straight trench virtual model as described above with reference to may be used. For a segment of a road including an intersection a virtual model representing two intersecting trenches may be used. By using more than one virtual model a more realistic virtual model resembling the actual real environment may be created.

After a virtual model is created parameters of the virtual model are sent to the graphical image renderer as the model information . The graphical image renderer receives the graphical elements and their configurations for presentation from the graphical element generator . Based on the virtual model the graphical image renderer generates image data with graphical elements placed and configured to be consistent with the virtual model.

In embodiments where the graphical elements are to be displayed on the HUD system the configurations of the graphical elements must be adjusted to account for the different in the location or perspective of the driver and the image capturing device . Because the location of the image capturing device and the driver s eye location are known adjustments of coordinate and change in perspective may be performed using for example OpenGL application programming interface API .

In other embodiment where the images are displayed on a display device installed in the dashboard or center console of the vehicle or the difference in the location of the driver and the image capturing device can be ignored the graphical elements can be overlaid on the image captured by the image capturing device directly for display without adjusting for the difference in the locations of the image capturing device and the driver.

Each of boxes are affixed to a left side wall or a right side wall of the virtual model to display advertisements or other information. The advertisements may be static images or dynamic moving pictures. A message and its shadow may also be shown on the road . The message is configured for placement on the floor of the virtual model corresponding to the environment in front of the vehicle. The message may indicate for example the name of a point of interest POI and warning signals on driving conditions. Shadows e.g. shadow are also displayed to enhance the realism of the augmented reality. The configuration of shadows may be determined using the positioning information and the time information as described below in detail.

As described above with reference to the graphical elements are moved and changed with the movement of the vehicle. Specifically the boxes move and change their dimensions as if the boxes are secured to fixed locations of side walls of the virtual model. On the other hand the message and its shadow move and changes their dimension as if they are secured to a floor of the virtual model.

In the example of the vehicle detected a stop signal by using captured images including a traffic signal or by receiving information via vehicle to infrastructure V2I communication. In response to detection of stop signal by the vehicle a stop sign is generated for display on the HUD system or other display devices in the vehicle. The stop sign is a graphical element and acting as if it is affixed to a ceiling of the virtual model. The stop sign may change to a go sign not shown when the traffic signal changes. Such a change may be prompted by the captured image indicating change of the traffic signal though image processing or information received via V2I communication.

While the vehicle is stopped graphical elements such as advertisement may be generated for presentation to the driver. The advertisement is configured to act as if it is displayed on a billboard secured to the floor of the virtual model. The advertisement may disappear once the vehicle starts moving or once the traffic signal changes. A shadow of the billboard may also be generated to add realism to the advertisement . By presenting the advertisement while the vehicle is stopped by a traffic signal distraction to the driver is reduced while keeping the driver entertained or informed.

Shadows of graphical elements may be generated by detecting the geographical location of the vehicle using real time GPS information received as part of the positioning information . Using the GPS information and time information the location of the sun relative to the vehicle can be determined in real time. Treating the sun as a direction light source and using surfaces of the virtual model as planes onto which shadows of the graphical elements are projected the shadows may be generated in an expedient manner.

Graphical elements and their configurations in are merely illustrative. Various other graphical elements of different configuration may be used to provide various types of visual information to the driver.

Various safety features may be implemented using the virtual model generated from the captured images. One of many such features is a virtual crosswalk generated to indicate a path that a pedestrian may take to cross a road. Pedestrians may sometime jaywalk a road creating safety hazard for vehicles. In one embodiment a pedestrian likely to jaywalk a road is automatically detected based on captured images and then a graphical element similar to a crosswalk is displayed to warn the driver.

In one embodiment the image capturing device includes a stereoscopic camera. By processing images captured by the stereoscopic camera the pose of a pedestrian and the distance to the pedestrian may be determined. The pedestrian detector determines whether a pedestrian is likely to cross the road based on actions such as facing the road walking towards the road holding up his arm or a combination of these actions. After the pedestrian detector determines that a pedestrian is likely to cross the road the pedestrian detector sends the pedestrian information indicating the location of the pedestrian to the graphical element generator .

Based on the pedestrian information the graphical element generator generates a graphical element representing a crosswalk for pedestrian as illustrated in . The crosswalk extends across road from the part of the road closest to the pedestrian . In addition the graphical element generator may generate a graphical element representing a warning signal to indicate that the pedestrian is likely to start crossing the road .

The distance to the pedestrian may be determined by processing the stereoscopic image captured by the image capturing device . As the vehicle comes closer to the pedestrian the graphical element changes to a stop signal as illustrated in to prompt higher cautionary measure from the driver.

The crosswalk is a graphical element affixed to the floor of the virtual model while the graphical elements are graphical elements affixed to the right side wall of the virtual model. Because these graphical elements move in conjunction with the change in scenery the graphical elements cause less distraction to the driver while alerting the driver of the possible risk factors.

The augmented reality processor extracts features relevant to a road from the captured image or preprocessed image . For this purpose the augmented reality processor may crop images and retain a portion of the captured image suitable for extracting the features. Further IPM algorithm may be applied to the cropped image to facilitate extraction of relevant features. The features may be extracted for example using a separable filter Hough transform or other applicable filters. In one embodiment the extracted features include lines corresponding to edges of the road.

Based on the extracted features a virtual model of the environment in front of the vehicle is constructed . The virtual model includes one or more surfaces to which the location and configuration of the graphical elements can be referenced. For example the graphical elements may be placed on the surface extended from a surface or placed on the surface.

The graphical elements are then generated for placement according to the virtual model. The graphical elements may be generated based on information from external sources or information generated in the vehicle e.g. pedestrian information . After the graphical elements are generated the configurations of the graphical elements are adjusted for placement with reference to surfaces in the virtual model. The configurations of the graphical elements may include tilting angles sizes and orientations of the graphical elements.

The generated graphical elements are then displayed on the display device e.g. the HUD system . Then the process returns to capturing a subsequent image and repeats the subsequent processes.

The steps and their sequences in are merely illustrative. For example a step of detecting the position of the vehicle may be added to generate shadows of graphical elements.

In one embodiment the positioning information including GPS information and the heading of the vehicle may be used to assist in generation of the virtual model. The GPS information and the heading of the vehicle may be used to determine the portion of road on which the vehicle is traveling in a map or a satellite image. Based on the map or the satellite image the orientation and width of the road may be verified against the bird s eye view e.g. or B . Adjustments may be made to the bird s eye view before constructing a virtual model of the environment.

In one embodiment instead of generating virtual models for the entire length of the road visible in the captured image only a portion of the road close to the vehicle may be used to generate a virtual model. Taking the example of a virtual model for only segment is generated. The segment may be continuously updated with the progress of time and vehicle.

In one embodiment color information of roads is used instead of or in addition to extracting line features in the captured image. Roads generally have grayish to black color. Using such characteristic of the road pixels in an image having color values within a predefined color space and contiguous region of the image may be selected to represent the road. Detection of roads using color information may be especially useful in intersections. The arrangements of pixels may then be analyzed to determine the configuration of the road ahead of the vehicle.

Although several embodiments are described above various modifications can be made within the scope of the present disclosure.

