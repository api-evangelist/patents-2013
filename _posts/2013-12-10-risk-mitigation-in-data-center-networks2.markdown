---

title: Risk mitigation in data center networks
abstract: A method employing resource orchestration algorithms may find a fewest number of working data centers (DCs) to guarantee K-connect survivability using an overlay network representing a physical optical network. The overlay network may not include certain topological features of the physical optical network. A risk-based algorithm may result in fewer working DCs for K-connect survivability. A delay-based algorithm may be more suitable for delay-sensitive cloud applications.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09565101&OS=09565101&RS=09565101
owner: Fujitsu Limited
number: 09565101
owner_city: Kawasaki
owner_country: JP
publication_date: 20131210
---
This application claims priority from U.S. Provisional Application No. 61 814 532 filed Apr. 22 2013 which is hereby incorporated by reference.

The present disclosure relates generally to data center networks and more particularly to risk mitigation in data center networks.

As more applications and workloads are moving to online network computing resources also generally referred to as the cloud geographically distributed data centers DCs are being deployed across wide area networks including optical networks. Such data centers may provide various instances of virtual machines VMs that may individually instantiate a computing environment such as a server operating system for example. Cloud applications may rely on distributed DCs for improved user experience. However some cloud service providers may not own optical network infrastructure and may count on network providers to optically interconnect distributed DCs. Some network providers may be unwilling and or unable to expose their full network topology information to cloud service providers.

Many cloud applications in distributed DCs are arranged in an aggregation communication pattern whereby an aggregation DC collects data processed at distributed DCs and outputs final results to users. Cloud applications can make physically dispersed VMs operate logically as one DC by collecting results from dispersed VMs at an aggregation DC. Other applications such as cloud search and data backup for example can allocate VMs close to data stored in distributed DCs and provide results at an aggregation DC for access by users. In certain instances complicated communication patterns can be constituted by scheduling a sequence of data aggregations.

Due to the reliance on distributed DCs and aggregation DCs survivability in the face of various risks such as network outages DC failure s and or equipment failure among other examples is becoming an important issue for cloud applications. Accordingly there is a need in the art for an overlay framework that enables cloud service providers to control cloud network connections and optimize resource orchestration yet enables network operators to offer network services while retaining detailed network topology information.

In one aspect a disclosed method for identifying a smallest M number of data centers DCs for K connect survivability includes generating a risk matrix associated with an aggregation DC included in an overlay network sorting the DC connection pairs according to a risk criteria and setting M equal to K 1 M K 1 . The risk matrix may indicate which of N number of DC connection pairs are associated with which of L number of shared risk groups SRGs in the overlay network. A DC connection pair may represent a connection in the overlay network to a DC from the aggregation DC. The method may include in iteration over each value of M evaluating in an increasing sorted order of the risk criteria a risk vector for each of the DC connection pairs to determine whether a DC connection pair is selected and when less than M number of DC connection pairs are selected incrementing M. The risk vector may be based on the risk matrix and or on previously selected DC connection pairs. The method may further include identifying the M number of DCs included in the M number of DC connection pairs selected. K may represent a minimum number of DCs that remain accessible to the aggregation DC. The overlay network may represent a physical network.

Additional disclosed aspects for identifying a smallest M number of data centers DCs for K connect survivability include an article of manufacture comprising a non transitory computer readable medium and computer executable instructions stored on the computer readable medium. A further aspect includes a management system comprising a memory a processor coupled to the memory and computer executable instructions stored on the memory.

The object and advantages of the embodiments will be realized and achieved at least by the elements features and combinations particularly pointed out in the claims. It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory and are not restrictive of the invention as claimed.

In the following description details are set forth by way of example to facilitate discussion of the disclosed subject matter. It should be apparent to a person of ordinary skill in the field however that the disclosed embodiments are exemplary and not exhaustive of all possible embodiments.

Throughout this disclosure a hyphenated form of a reference numeral refers to a specific instance of an element and the un hyphenated form of the reference numeral refers to the element generically or collectively. Thus as an example not shown in the drawings widget 12 1 refers to an instance of a widget class which may be referred to collectively as widgets 12 and any one of which may be referred to generically as a widget 12 . In the figures and the description like numerals are intended to represent like elements.

As will be described in further detail herein a K connect survivability concept is disclosed that may guarantee resource availability under a wide range of risks for cloud applications. Two resource orchestration schemes are disclosed that may implement the K connect survivability concept in an overlay framework for optical network virtualization. The resource orchestration schemes may identify a fewest number of data centers for guaranteeing K connect survivability where K represents a minimum number of DCs that remain accessible from an aggregation DC. The following parameters in Table 1 which are integers greater than zero are used herein with respect to K connect survivability.

A K connect survivability KCS may be defined by a scenario where at least K number of DCs out of M original working DCs are reachable from an aggregation DC DC for an arbitrary risk such as but not limited to network outages DC failure s and or other types of equipment failure also referred to herein collectively as risk events . For the purposes of the present disclosure it may be assumed that DCdoes not fail. A risk event may result in multiple failures that may occur at DC sites e.g. due to power outages natural disasters and or system maintenance or in networks due to fiber cuts . For cloud applications requesting a fixed number of VMs additional VMs can be allocated at the surviving K number of DCs in order to maintain the same number of virtual machines during a risk event scenario as during normal operation.

As will be described herein an overlay framework is presented that interconnects distributed data centers by virtualized optical networks. Survivable resource orchestration algorithms based on the network information provided by the virtualized optical networks such as shared risk groups SRG and delay are disclosed. The disclosed resource orchestration algorithms may find a fewest number of working DCs to ensure K connect survivability. The resource orchestration algorithms disclosed herein may provision the fewest number of working DCs based on SRG information provided for overlay networks where physical network topology may be unavailable and routing for connections may not be possible.

Turning now to the drawings illustrates an example embodiment of overlay framework which may be based on optical network virtualization. In overlay framework is shown including overlay network software defined network SDN application programming interfaces APIs and physical network . As shown overlay network may comprise connections between DCs where a bandwidth of connections may be adjustable using optical network virtualization. In an underlying optical network represented by physical network may be an optical transport network OTN and or a flexible optical data plane e.g. flexible transceivers configured to adjust the bandwidth of connections.

In overlay network is shown comprising virtualized DCs and connections . In certain embodiments DCs may correspond to physical DCs for example DC  may represent DC A DC  may represent DC F DC  may represent DC E and DC  may represent DC C while DC B and DC D may not be explicitly included in overlay network . In other embodiments DCs may include computing resources from one or more physical DCs and may represent virtualized DCs for example DC  may represent at least portions of DC A and DC B etc. It will be understood that other arrangements and configurations of mapping DCs in physical network to DCs in overlay network may be practiced in different embodiments. Furthermore connections may represent virtualized connections having a given capacity for transporting data. As shown connections and may represent low capacity connections connections and may represent mid capacity connections while connections and may represent high capacity connections. Although connections are shown in overlay network connecting two DCs connections may be physically implemented using various network topologies and may actually represent physical connections that include different nodes and or network segments. However to a cloud service provider using overlay network as an operational network platform the actual physical topology may remain hidden and or may change over time.

Cloud service providers may have a centralized controller not shown in the drawings that manages VMs at DCs interconnected by overlay network . The centralized controller also referred to herein simply as the controller may obtain network information such as delay and SRG of connections and may request the bandwidth of connections through network application programming interfaces APIs with the help of network control and management tools such as software defined networks SDN . As shown in overlay framework SDN APIs may represent software tools for enabling a user e.g. a cloud provider of overlay network to query network information. It is noted that overlay framework may enable network providers of physical network to keep detailed physical network topology information hidden while allowing cloud service providers to easily set up cloud services to perform resource orchestration and to flexibly increase or reduce the bandwidth of connections. The cloud providers may use SDN APIs to query certain specific attributes for DCs and or connections in overlay network without having knowledge of the specific network topology of physical network and or without direct interaction with hidden components in physical network such as intermediate network devices along connection paths that are not included in overlay network .

Turning now to example embodiments of aggregation requests and corresponding protection schemes are illustrated in diagram form. The controller may receive cloud requests and may perform resource orchestration. In one example embodiment. As shown in aggregation request may illustrate how aggregation DC handles a basic request for example via the controller by aggregating data from DC DC and DC . More complicated requests may be generated using a combination of basic requests and or sets of basic requests. A request may satisfy K connect survivability and may be associated with a given number of VMs V for risk events. When a risk event occurs a request with K connect survivability may allocate additional VMs at the surviving K DCs out of M number of working DCs in order to maintain V number of VMs. Assuming that each DC is allocated the same number of VMs for a request the total VMs for a request with K connect survivability may be given by V M K. Accordingly finding the fewest M number of DCs that satisfy K connect survivability results in the fewest VMs required for a request.

Guaranteeing K connect survivability may save network cost by jointly considering information from physical networks and DCs. In separate blind protection shows an example where sindicates risk i and network connections may be blindly protected by providing path disjoint connections dotted lines from aggregation DC . In K connect survivability for K 2 i.e. 2 connect survivability may be guaranteed by protecting against risk events at DCs separately from the network connections which may result in 6 connections in separate protection . In joint protection illustrates by using shared risk group SRG information from underlying optical networks how 2 connect survivability may be guaranteed by finding network connections and DCs that can be jointly protected. For example risks Sand Smay be joined in one SRG risk Sand Smay be joined in a second SRG and risks Sand Smay be joined in a third SRG. In joint protection significant savings in network resources may be achieved by having 3 connections representing a savings of 3 protection connections as compared to .

Using SDN APIs see a subset of DCs with minimum delay may be identifiable when multiple subsets of DCs that satisfy K connect survivability exist. A delay of a request may be given by a total delay of connections between the subset of DCs and the aggregation DC DC . It is noted that DCmay be allocated to a DC that is relatively near to users or relatively near to a particular subset of DCs depending on specific applications.

Based on the following problem description may be applied to the methods for K connect survivability based on aggregation and protection schemes .

As will now be described in further detail two heuristic algorithms are disclosed for solving the KCS problem in optically interconnected distributed DC networks. In both algorithms a risk matrix may be constructed for each aggregation DC. For each s the risk matrix records 1 if a pair p consisting of a connection Eand a DCis associated with risk s. In both algorithms described below Table 2 shows the values for parameters associated with an overlay network not shown that are assumed.

Table 3 shows an exemplary risk matrix constructed for an arbitrary DCcorresponding to the example of Table 2. The delay of pis dand the set of risks associated with pis the union of Aand A. The values in Table 3 may be queried for example using SDN APIs see based on a corresponding overlay network not shown .

For finding at least M number of working DCs M is incremented from K 1. For each M with DCas an aggregation DC sort pin an increasing order of delay. Risk vector pis incremented by 1 if a pis chosen and 1. A pcan be chosen if and only if risk vector p M K for all risks swith 1. If M pairs are found stop incrementing M. Finally the highest M represents the fewest number of working DCs that satisfies K connect survivability.

An example embodiment of algorithm A1 corresponding to the values in Table 2 and Table 3 above will now be described in detail. For delay based algorithm A1 it will be assumed that for pair p delay dincreases in a delay order given by p p p p which represents an order in which pairs pare selected for processing. Algorithm A1 is described below using pseudo code that roughly corresponds to instructions executable by a processor yet incorporates prosaic text for human readability and understanding.

Algorithm A1 begins at line A1 100 with a main loop that iterates over M with no pairs selected initially. At line A1 110 M is set to 3 for K 2 and thus M K 1. At line A1 120 evaluation of pairs begins with p based on the delay order. At line A1 130 risk vector p 1 is evaluated using the risk matrix given in Table 3 for palone because no pairs have yet been selected with the result that all values in risk vector p 1 are less than or equal to M K . At line A1 140 pis selected as the first pair. At line A1 150 evaluation of pairs continues with pbased on the delay order. At line A1 160 risk vector p 2 is evaluated using the risk matrix given in Table 3 for pand p because only phas yet been selected with the result that all values in risk vector p 2 are less than or equal to M K . At line A1 170 pis selected as the second pair. At line A1 180 evaluation of pairs continues with pbased on the delay order. At line A1 190 risk vector p 3 is evaluated using the risk matrix given in Table 3 for p p and p because both pand phave been selected with the result that all values in risk vector p 3 are not less than or equal to M K . At line A1 200 pis skipped. At line A1 210 evaluation of pairs continues with pbased on the delay order. At line A1 220 risk vector p 4 is evaluated using the risk matrix given in Table 3 for p p and p because both pand phave been selected with the result that all values in risk vector p 4 are less than or equal to M K . At line A1 230 pis selected as the third pair. At line A1 240 it is determined that M number of pairs have been selected thus M is not incremented and the main loop ends and Algorithm A1 ends having selected p p p for K connect survivability.

In the Delay Based Algorithm A1 it may be possible that pairs selected earlier are associated with many risks resulting in more working DCs for satisfying the K connect constraint. Hence Risk Based Algorithm A2 sorts ppairs in an increasing order of the total frequency of risks that are associated with p. The frequency of a risk is defined as the number of ppairs that are associated with the risk. Other steps in Risk Based Algorithm A2 may be similar to certain portions of the Delay Based Algorithm A1.

Based on the risk matrix generated in Table 3 the following frequency of risks may be established for each pair p 

Accordingly the risk order for Algorithm A2 is given by p p p p which represents an order in which pairs pare selected for processing.

Algorithm A2 Begins at Line A2 100 with a Main Loop that Iterates Over M with No Pairs Selected Initially. At Line A2 110 M is Set to 3 for K 2 and Thus M K 1. At Line A2 120 evaluation of pairs begins with p based on the risk order. At line A2 130 risk vector p 1 is evaluated using the risk matrix given in Table 3 for palone because no pairs have yet been selected with the result that all values in risk vector p 1 are less than or equal to M K . At line A2 140 pis selected as the first pair. At line A2 150 evaluation of pairs continues with pbased on the risk order. At line A2 160 risk vector p 2 is evaluated using the risk matrix given in Table 3 for pand p because only phas yet been selected with the result that all values in risk vector p 2 are less than or equal to M K . At line A2 170 pis selected as the second pair. At line A2 180 evaluation of pairs continues with pbased on the risk order. At line A2 190 risk vector p 3 is evaluated using the risk matrix given in Table 3 for p p and p because both pand phave been selected with the result that all values in risk vector p 3 are less than or equal to M K . At line A2 200 pis selected as the third pair. At line A2 210 it is determined that M number of pairs have been selected thus M is not incremented and the main loop ends and Algorithm A2 ends having selected p p p for K connect survivability.

Although both Algorithm A1 and A2 arrive at the same result in the example configuration described above the Algorithms A1 and A2 may differ in the order in which pairs pare evaluated and thus may differ in a number of evaluation iterations for a given configuration. It is noted that while additional iterations of the main loop to increment M are not described for descriptive clarity it will be understood that in larger network configurations subsequent iterations may be performed to find K connect survivability for larger values of M.

Turning now to selected elements of an embodiment of method for implementing K connect survivability as described herein is shown in flow chart format. In certain embodiments method may be implemented using KCS identification see . It is noted that certain operations depicted in method may be rearranged or omitted as desired.

Method may begin by generating operation a risk matrix associated with an aggregation DC included in an overlay network the risk matrix indicating which of N DC connection pairs are associated with which of L SRGs. The DC connection pairs may be sorted operation according to a risk criteria. The risk criteria may be risk based or may be delay based. Then method may let operation M K 1 and may initialize operation a risk vector with L zero values. Then a decision may be made operation whether M N. When the result of operation is YES method may end operation . When the result of operation is NO a risk vector may be evaluated operation in increasing order of the risk criteria for each of the DC connection pairs to determine whether a DC connection pair is selected the risk vector based on the risk matrix and on previously selected DC connection pairs. Then a decision may be made operation whether less than M DC connection pairs are selected. When the result of operation is YES M may be incremented operation and method may loop back to operation . When the result of operation is NO the M DCs included in the M DC connection pairs selected may be identified operation . Then method may end operation .

Turning now to selected elements of an embodiment of method for implementing K connect survivability as described herein is shown in flow chart format. In various embodiments method may be implemented using KCS identification see and may represent operation in method see . It is noted that certain operations depicted in method may be rearranged or omitted as desired. Method is described in an iterative loop context and in the description below the term next is used to designate iterative values used within the loop context.

Method may begin by advancing operation to evaluate a next DC connection pair. Method may then advance operation to evaluate a next SRG. Values in the risk matrix associated with the next SRG for the next DC connection pair and when present for all previously selected DC connection pairs may be summed operation to the risk vector. Then a decision may be made operation whether all L SRGs have been evaluated for the next DC connection pair. When the result of operation is NO method may loop back to operation . When the result of operation is YES a decision may be made operation whether all values in the risk vector are less than or equal to M K . When the result of operation is YES the next DC connection pair may be selected operation . When the result of operation is NO or after operation a decision may be made operation whether all N DC connection pairs have been evaluated. When the result of operation is NO method may loop back to operation . When the result of operation is YES method may continue to operation see .

Referring now to a block diagram of selected elements of an embodiment of management system is illustrated. In management system is represented as a computer system including physical and logical components for implementing K connect survivability as described herein and may accordingly include processor memory and network interface . Processor may represent one or more individual processing units and may execute program instructions interpret data and or process data stored by memory and or management system .

In memory may be communicatively coupled to processor and may comprise a system device or apparatus suitable to retain program instructions and or data for a period of time e.g. computer readable media . Memory may include various types components and devices such as random access memory RAM electrically erasable programmable read only memory EEPROM a PCMCIA card flash memory solid state disks hard disk drives magnetic tape libraries optical disk drives magneto optical disk drives compact disk drives compact disk arrays disk array controllers and or any suitable selection or array of volatile or non volatile memory. Non volatile memory refers to a memory that retains data after power is turned off. It is noted that memory may include different numbers of physical storage devices in various embodiments.

As shown in memory may include K connect survivability KCS identification which may represent respective sets of computer readable instructions that when executed by a processor such as processor may execute various algorithms for identifying DCs and or SRGs to satisfy K connect survivability including but not limited to Risk Based Algorithm A1 and or Delay Based Algorithm A2. Information storage may store various data and parameters such as data and parameters associated with KCS identification .

Turning now to simulation results of embodiments of selected methods for implementing K connect survivability are shown as data plots. In results of simulations of selected embodiments of the heuristic Algorithms A1 and A2 are shown for comparison. In the simulation results depicted in given a physical network having 75 DCs and 99 connections fully mesh connected overlay networks similar to overlay network see are generated with DCs located at randomly chosen nodes. The shortest paths for the connections in the overlay network are used with connection delays randomly assigned between 0.1 and 10 arbitrary time units while a total number of 60 SRG risks are used. The set of risks on each connection and each DC may be randomly chosen and a number of SRGs per connection or per DC notated by R is given in . For a cloud request an aggregation DC may be randomly assigned. The simulation results are averaged over a total of 10requests that are successfully allocated while an arbitrary amount of bandwidth is assumed for VMs that may be requested without any limitation from an underlying physical network infrastructure so that a fewest number of working DCs and the delay may be specifically evaluated.

In performance for increasing values of K is shown as plots for the average of the least values of M versus K for Algorithm A1 Delay Based and Algorithm A2 Risk Based . In performance for increasing values of K is shown as plots for the average delay per request versus K for Algorithm A1 Delay Based and Algorithm A2 Risk Based . show the least M and the average delay of requests as K increases where the total number of DCs in an overlay network is ten N 10 . shows that Risk Based Algorithm A1 may result in up to 12 fewer working DCs than Delay Based Algorithm A2. It is noted that in order to satisfy the increasing K connect constraint the fewest number of working DCs increases. When K is equal to 6 or 4 for R 1 or R 2 the fewest working DCs required has almost reached 9 out of 10 total DCs. Hence no solution may be found for higher K. shows that as K increases the average delay per request increases due to the requirement of more working DCs and the difference in delay reduces. When K is lower than N 2R which shows a high risk diversity of connections the delay of Risk Based Algorithm A1 may be higher than the delay of Delay Based Algorithm A2 even when Risk Based Algorithm A1 results in fewer working DCs because a chosen connection with lower total risk frequency may have longer delay. When K is higher than N 2R there may be limited risk diversity of connections and thus limited choices of sets of working DCs. Hence Risk Based Algorithm A1 may slightly outperform Delay Based Algorithm A2 with fewer working DCs and thus lower delay.

In performance for increasing values of N is shown as plots for the average of the least values of M versus N for Algorithm A1 Delay Based and Algorithm A2 Risk Based . In performance for increasing values of N is shown as plots for the average delay per request versus N for Algorithm A1 Delay Based and Algorithm A2 Risk Based . In K is fixed to be 4. Risk Based Algorithm A1 may result in fewer working DCs and higher delay of requests as N increases compared to Delay Based Algorithm A2. In for R 2 when N is lower than 16 there is limited diversity of connections N 2R 

While the subject of this specification has been described in connection with one or more exemplary embodiments it is not intended to limit any claims to the particular forms set forth. On the contrary any claims directed to the present disclosure are intended to cover such alternatives modifications and equivalents as may be included within their spirit and scope.

