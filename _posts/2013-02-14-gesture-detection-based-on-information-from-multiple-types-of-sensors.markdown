---

title: Gesture detection based on information from multiple types of sensors
abstract: A method includes receiving a first output from a first sensor of an electronic device and receiving a second output from a second sensor of the electronic device. The first sensor has a first sensor type and the second sensor has a second sensor type that is different from the first sensor type. The method also includes detecting a gesture based on the first output and the second output according to a complementary voting scheme that is at least partially based on gesture complexity.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09389690&OS=09389690&RS=09389690
owner: QUALCOMM Incorporated
number: 09389690
owner_city: San Diego
owner_country: US
publication_date: 20130214
---
The present application claims priority from commonly owned U.S. Provisional Patent Applications No. 61 605 636 filed on Mar. 1 2012 and No. 61 691 989 filed on Aug. 22 2012 the contents of which are expressly incorporated herein by reference in their entirety.

Advances in technology have resulted in smaller and more powerful computing devices. For example there currently exist a variety of portable personal computing devices including wireless computing devices such as portable wireless telephones personal digital assistants PDAs and paging devices that are small lightweight and easily carried by users. More specifically portable wireless telephones such as cellular telephones and Internet Protocol IP telephones can communicate voice and data packets over wireless networks. Further many such wireless telephones include other types of devices that are incorporated therein. For example a wireless telephone can also include a digital still camera a digital video camera a digital recorder and an audio file player.

Hands free operation of portable devices such as wireless telephones is becoming desirable in various use cases. For example in car hands free operation of a mobile phone may be desirable for safety reasons. Some electronic systems such as game consoles have gesture recognition capabilities. Gesture recognition is often performed using a single sensor e.g. either a camera or an infrared sensor .

Gesture recognition systems and methods suitable for use in mobile device applications are disclosed. Notably the disclosed techniques include leveraging multiple types of sensors to provide more robust gesture recognition. Exemplary applications of the disclosed systems and methods include but are not limited to in car gesture control for in car entertainment telephony navigation living room media control gaming in kitchen appliance control multimedia presentation control gym exercise equipment control etc. Moreover the disclosed systems and methods may be run on various platforms including mobile phones embedded devices netbook computers tablet computers laptop computers media centers set top boxes smart appliances game consoles etc.

By leveraging multiple sensor types the disclosed systems and methods may overcome disadvantages associated with single sensor type gesture recognition systems. For example consider an embodiment of the disclosed system that leverages both a camera and an ultrasound sensor for gesture recognition. Ultrasound may be used as a low power front end that wakes up the rest of the system when an object is detected proximate to the mobile device. The system may dynamically switch sensors on and off depending on lighting proximity conditions. When lighting conditions are poor and the camera is unreliable only ultrasound may be used. Conversely when a target gesture is far away from the mobile device or the gesture is a steady state or complex gesture ultrasound may be unreliable and therefore only the camera may be used. Various sensors or portions thereof may be selectively activated and deactivated based on lighting proximity conditions e.g. to conserve power .

When lighting proximity conditions enable the use of multiple types of sensors a complementary voting scheme may be applied to the sensor outputs to determine what gesture is detected. For example each sensor may output a detected gesture e.g. left right up down select etc. and a confidence level. When both types of sensors detect the same gesture the gesture may be output. When the sensors detect different gestures the gesture with the higher confidence may be output. When ultrasound picks up a simple gesture but the camera picks up a complex gesture the complex gesture may be output.

Each sensor may use information from other sensors to trigger self adjustment in an attempt to improve its own performance. In one implementation data may be exchanged via a common data model and or an application programming interface API .

In a particular embodiment an apparatus includes a first sensor configured to generate a first output and a camera configured to generate a second output. The apparatus also includes a processor and a gesture detection module executable by the processor to detect a gesture based on at least one of the first output and the second output. At least a portion of the camera is selectively deactivated based on a lighting level and a gesture range.

In another particular embodiment a method includes receiving a first output from a first sensor of an electronic device and receiving a second output from a second sensor of the electronic device. The first sensor has a first sensor type and the second sensor has a second sensor type that is different from the first sensor type. The method also includes detecting a gesture based on the first output and the second output according to a complementary voting scheme that is at least partially based on gesture complexity.

In another particular embodiment an apparatus includes an ultrasound sensor configured to generate a first output in accordance with a common data model and to provide the first output to an ultrasound processing path. The apparatus also includes a camera configured to generate a second output in accordance with the common data model and to provide the second output to an image processing path. The apparatus further includes a processor and a gesture detection module executable by the processor to detect a gesture based on at least one of the first output and the second output. The ultrasound processing path and the image processing path are configured to exchange data in accordance with the common data model.

In another particular embodiment an apparatus includes an ultrasound sensor configured to provide a first output to an ultrasound processing path and a camera configured to provide a second output to an image processing path. The apparatus also includes a processor and a gesture detection module executable by the processor to detect a gesture based on at least one of the first output and the second output. The ultrasound sensor and the camera are each configured to self adjust independent of the processor based on data exchanged between the ultrasound processing path and the image processing path.

Particular advantages provided by at least one of the disclosed embodiments include an ability to detect gestures based on applying a complementary voting scheme to the outputs of multiple types of sensors which may provide increased gesture recognition accuracy when compared to single sensor type systems. In addition sensors or portions thereof may be selectively activated and deactivated based on lighting proximity conditions e.g. to conserve power . Further sensors may self adjust based on output from other sensors to improve performance.

Other aspects advantages and features of the present disclosure will become apparent after review of the entire application including the following sections Brief Description of the Drawings Detailed Description and the Claims.

It should be noted that as used herein the term ultrasound sensor may identify a sensor that is capable of sensing only ultrasound signals i.e. a dedicated ultrasound sensor and may also identify a sensor that is capable of sensing other signals in addition to ultrasound signals i.e. a multi purpose sensor . For example the multi purpose sensor may also be operable to sense audio signals that are within the human range of hearing e.g. 20 Hz to 20 kHz and or other types of signals e.g. electromagnetic signals radio frequency RF signals etc. .

Each sensor of the electronic device may be coupled to a signal processing path illustrated in by an ultrasound processing control module and an image processing control module . Each of the processing control modules and may be configured to control the operation of and process the output generated by the corresponding respective sensors and . For example the ultrasound processing control module may process first output generated by the ultrasound sensor and the image processing control module may process second output generated by the camera .

The processing control modules and may be coupled to a gesture detection module . The gesture detection module may detect a gesture e.g. made by the hand based on the first output from the ultrasound sensor and or the second output from the camera . Each of the first output from the ultrasound sensor and the second output from the camera may include an identified gesture and a confidence score. To illustrate the identified gesture with respect to the hand may be left right up down away i.e. in a direction out of the plane of or towards i.e. in a direction into the plane of . In a particular embodiment the towards and or the away gestures may represent a select gesture e.g. during navigation of a menu or list . It should be noted that the gestures are for illustration only. Other types of gestures including complex and steady state gestures may also be detected. For example a steady state gesture in which the hand does not move thus making detection via the ultrasound sensor difficult may include the hand remaining motionless but having a particular number of fingers extended e.g. two fingers extended to indicate a selection of a particular menu item e.g. the second item from the top . Detectable gestures may also include periodic or repeated gestures e.g. the hand being waved back and forth .

In a particular embodiment all or a portion of the modules and or may be implemented using processor executable instructions that may be executed by a hardware processor as further described with reference to . In a particular embodiment all or a portion of the modules and or may be implemented using hardware such as via dedicated circuitry a controller one or more other hardware devices or any combination thereof.

In a particular embodiment the gesture detection module may apply a complementary voting scheme to determine what gesture is detected. The complementary voting scheme may be at least partly based on gesture complexity. For example when both the ultrasound sensor and the camera identify the same gesture the same gesture may be selected as the output gesture. When the ultrasound sensor and the camera identify different gestures the gesture having the higher confidence score may be selected as the output gesture. As another example when the ultrasound sensor and the camera identify gestures of varying complexity the more complex gesture may be selected as the output gesture. A particular example of a complementary voting scheme is further described with reference to .

In a particular embodiment the electronic device may include a light detector e.g. a light detector internal to the camera or a light detector external to the camera . The light detector or may determine an ambient lighting level. Based on the lighting level and or a gesture range e.g. how close the hand is to the electronic device as measured by the ultrasound sensor and or the camera portions of the ultrasound sensor and or the camera may be selectively deactivated. To illustrate when lighting conditions are poor and the light detector or determines a low lighting level all or a portion of the camera and the image processing control module may be deactivated to conserve power because the camera may not accurately detect motion in dark or near dark conditions. The light detector or may remain activated to detect changes in the lighting conditions.

As another example when the ultrasound sensor detects that the hand is located in a particular location the camera may utilize such information to identify an area of interest. A portion of a sensor array in the camera that corresponds to the area of interest may be activated while other portions or the remainder of the sensor array may be deactivated to conserve power.

To implement such functionality data may be exchanged between the ultrasound processing control module and the image processing control module illustrated as a data exchange . For example the ultrasound sensor and the camera may format their respective outputs in accordance with a common data model or an application programming interface API so that other sensors and processing control modules may successfully interpret the output. Data exchanged in accordance with the common data model may include output from the ultrasound sensor output from the camera data related to a range i.e. distance of the hand from the ultrasound sensor or the camera data related to a position of the hand relative to the ultrasound sensor or the camera data associated with an active configuration of the ultrasound sensor e.g. signal emission frequency signal emission periodicity signal emission direction etc. or the camera e.g. image capture mode focal length area of focus etc. or any combination thereof. Each of the signal processing control modules and may also be configured to detect false positives e.g. a detected gesture where no gesture actually exists based on data received from the other of the processing control modules and .

When data exchange between the processing control modules and is enabled each of the sensors and may self adjust based on information from the other of the sensors and . Moreover such self adjustment may be independent of a processor e.g. application processor of the electronic device . To illustrate the ultrasound sensor may self adjust an active configuration a signal emission frequency a signal emission direction or any combination thereof based on output from the camera . The camera may self adjust an active configuration a focal length an area of focus or any combination thereof based on output from the ultrasound sensor . Thus each of the sensors and may be configured to improve its own performance via self adjustment without being instructed to do so by a central processor or controller. A particular example of a framework to implement data exchange is illustrated in . An example of data exchange between a camera and an ultrasound sensor using a gesture daemon is illustrated in .

During operation the gesture detection module may identify gestures made by the hand based on outputs from one or more of the ultrasound sensor and the camera . For example when the lighting level determined by the light detector or is less than a lighting threshold e.g. it is too dark for the camera the camera may be deactivated and gesture detection may be performed based on output from the ultrasound sensor . When the lighting level is greater than or equal to the lighting threshold gesture detection may depend on a gesture range. To illustrate when the gesture range is less than a nearness threshold e.g. the hand is too close for the camera the camera may be deactivated and gesture detection may be performed based on output from the ultrasound sensor . When the gesture range is greater than a farness threshold e.g. the hand is too far for the ultrasound sensor the ultrasound sensor may be deactivated and gesture detection may be performed based on output from the camera . When the gesture range is between the nearness and farness thresholds gesture recognition may be performed based on output from both of the sensors and in accordance with a complementary voting scheme.

The system of may thus enable gesture detection based on information from multiple types of sensors. It will be appreciated that by leveraging different types of sensors the system of may provide greater gesture detection accuracy than single sensor type systems e.g. due to reduction of incorrect gestures and false positives . It will also be appreciated that the system of may conserve power by selectively deactivating sensors or portions thereof based on lighting and proximity information and may attempt to self improve performance based on data exchanged between sensor paths.

In a particular embodiment the modules and may include one or more sub modules. For example the ultrasound processing control module may include an ultrasound tracking module and an ultrasound gesture detection module . The ultrasound tracking module may be configured to track motion of one or more objects e.g. the hand of based on time of flight information associated with an emitted ultrasound signal and reflection s of the emitted ultrasound signal. The ultrasound gesture detection module may determine whether tracking data produced by the ultrasound tracking module represents a recognizable gesture and if so determine a confidence level associated with the recognizable gesture. Similarly the image processing control module may include a camera tracking module and a camera gesture detection module . The camera tracking module may output image tracking data. The camera gesture detection module may determine whether the image tracking data represents a recognizable gesture and if so determine a confidence level associated with the recognizable gesture. It should be noted that a set of gestures recognizable via the ultrasound sensor may be different than a set of gestures recognizable via the camera .

The gesture detection module may include a multi mode gesture discrimination module . For example the multi mode gesture discrimination module may apply a complementary voting scheme to the outputs of the ultrasound gesture detection module and the camera gesture detection module as further described with reference to . The gesture detection module may also include an extended gesture interface configured to output results of the multi mode gesture discrimination module in a standardized format to one or more applications such as an illustrative application .

In a particular embodiment the system may optionally include more than two types of sensors. For example the system may include a third sensor having a third sensor type that is different from ultrasound and camera. A signal path of the third sensor may include a third sensor tracking module and a third sensor gesture detection module and output of the third sensor gesture detection module may be provided to the complementary voting scheme applied by the multi mode gesture discrimination module . Data may be exchanged between the data path of the third sensor and data paths of other sensors indicated as another data exchange . The system of may thus provide an extensible framework to implement multi mode gesture recognition and data exchange.

The method may include receiving a first output from a first sensor of an electronic device at . The first sensor may have a first sensor type and the first output may identify a first gesture with a first confidence score. The method may also include receiving a second output from a second sensor of the electronic device at . The second sensor may have a second sensor type that is different from the first sensor type and the second output may identify a second gesture with a second confidence score. For example in gesture detection module may receive first output from the ultrasound sensor via the ultrasound processing control module and second output from the camera via the image processing control module .

The method may further include determining whether the first and second gestures are the same at . When it is determined that the first and second gestures are the same i.e. both sensors identified the same gesture the method may include selecting the same gesture as an output gesture at . When it is determined that the first and second gestured are different the method may include determining whether the first and second gestures vary in complexity at . When the first and second gestures vary in complexity e.g. one of the sensors identified a simple gesture but the other sensor identified a complex gesture the method may include selecting the more complex gesture as the output gesture at . To illustrate referring to when ultrasound detection identifies a simple up gesture but camera detection identifies a more complex two fingers extended gesture the gesture detection module may select the more complex two fingers extended gesture as the output gesture.

When it is determined that the first and second gestures have the same complexity the method may include determining whether the first confidence score is greater than the second confidence score at . When is determined that the first confidence score is greater than the second confidence score the method may include selecting the first gesture as the output gesture at . When is determined that the first confidence score is not greater than the second confidence score the method may include selecting the second gesture as the output gesture at .

The method of may thus enable detection of gestures based on applying a complementary voting scheme to the outputs of multiple types of sensors which may provide increased gesture recognition accuracy when compared to single sensor type systems.

The method may include performing ultrasound proximity detection at . Ultrasound proximity detection may represent a low power front end that selectively wakes up remaining portions of a device. A camera of the device may be off while performing the ultrasound proximity detection. For example in proximity detection may be performed via the ultrasound sensor while the camera is off. The method may also include determining whether a proximate object is detected or an incoming call is received at . Until the ultrasound sensor detects a proximate object or until an incoming call is received the method may iterate by returning to .

When a proximate object is detected or an incoming call is received the method may include enabling ultrasound based simple gesture recognition at . For example referring to such simple gesture recognition may be used to turn on a radio of the electronic device or to answer an incoming call. The method may include determining whether a lighting level is greater than or equal to a lighting threshold at . For example in the light detector or may determine a lighting level and the image processing control module may determine whether the lighting level is greater than or equal to a lighting threshold.

In response to determining the lighting level less than the lighting threshold the method may include performing ultrasound simple gesture recognition and disabling all or a portion of the camera at and outputting detected gesture s at . Illustrative examples of when the lighting level may be less than the lighting threshold include in car operation at night or while the car is in a tunnel.

In response to determining that the lighting level is greater than or equal to the lighting threshold the method may include performing ultrasound proximity detection at and determining a gesture range at . For example in a gesture range may be determined based on output from the ultrasound sensor and or the camera . When it is determined that the gesture range is too far for ultrasound e.g. greater than a farness threshold the method may include enabling the camera and performing camera based complex gesture recognition at and outputting detected gesture s at . The ultrasound sensor or a portion thereof may be disabled while the gesture range remains too far for ultrasound.

When it is determined that the gesture range is too close for the camera e.g. less than a nearness threshold the method may include disabling at least a portion of the camera and performing ultrasound based simple gesture recognition at and outputting detected gesture s at . When it is determined that the gesture range is acceptable for both ultrasound and the camera e.g. the gesture range is between the nearness and farness thresholds the method may include performing gesture recognition via application of a complementary voting scheme to the outputs of the ultrasound sensor and the camera at and outputting detected gesture s at .

The method of may thus enable in car hands free operation of an electronic device based on information from multiple types of sensors i.e. ultrasound and camera . However it should be noted that the scenario of in car hands free mobile phone operation is provided for illustration only. Other applications include but are not limited to in car gesture control for in car entertainment or navigation living room media control gaming in kitchen appliance control multimedia presentation control gym exercise equipment control etc. Moreover the systems and methods of the present disclosure may be run on platforms other than mobile phones such as embedded devices netbook computers tablet computers laptop computers media centers set top boxes smart appliances game consoles etc.

In particular embodiments the methods and of may be implemented by a field programmable gate array FPGA device an application specific integrated circuit ASIC a processing unit such as a central processing unit CPU a digital signal processor DSP a controller another hardware device a firmware device or any combination thereof. As an example the methods of FIG. and of can be performed by a processor that executes instructions such as described with respect to .

Referring to a block diagram of a particular illustrative embodiment of a wireless communication device is depicted and generally designated . In an illustrative embodiment all or a portion of the device may include be included within or otherwise be used to implement all or a portion of the electronic device of . The device includes a processor such as a digital signal processor DSP coupled to a memory . The memory may include instructions executable by the processor to perform the methods and process disclosed herein such as the method of and the method of .

The ultrasound sensor may be coupled to the processor via the ultrasound processing control module illustrated as being implemented via hardware as shown. The camera may be coupled to the processor via the image processing control module illustrated as being implemented via hardware as shown. Information from the modules and may be used by the gesture recognition module illustrated as being implemented via software executed by the processor to detect gestures. In a particular embodiment the gesture detection module may apply a complementary voting scheme to information received from the modules and . In a particular embodiment the modules and may be configured to exchange data in accordance with a common data model or an API illustrated as the data exchange . For example such data exchange may enable one or both of the ultrasound sensor and the camera to self adjust independent of the processor in an effort to improve performance. Based on data from the sensors and or all or a portion of the sensors and or the modules may be selectively deactivated to conserve power.

In a particular embodiment a device e.g. a mobile wireless device or component thereof may implement freeform gesture recognition. Freeform gesture recognition may be operable to recognize a particular set of gestures. For example freeform gesture recognition may be used to recognize left right and select gestures at an effective distance of up to about ten centimeters. In a particular embodiment freeform gesture recognition may involve use of a gesture library and a supporting ultrasound framework e.g. a library of functions modules and or algorithms running on or provided by a DSP .

Freeform gesture detection may be implemented using a plurality of processes that for ease of description are described herein as corresponding to separate functional blocks. The functional blocks may include a main gesture detection block a motion activity detection MAD block a full power proximity detection block an on off switch for full power mode and an interference detection and cancellation block. One or more of the functional blocks may be implemented using ultrasound. Ultrasound based gesture detection blocks may function similarly to sonar or radar. For example an ultrasound transmitter may transmit continuous broadband ultrasound signals. The ultrasound signals reflected from a user s hand may be detected by multiple spatially separated microphones. Time of flight and other timing features may be used to identify the hand gesture. In contrast to other systems no gesture training may be needed for gesture recognition. The MAD block may be used to detect object movement within a certain distance. A MAD flag may be used to indicate a change with respect to a captured snapshot of a background channel image e.g. via averaging frames . The snapshot may be used together with a proximity flag to switch a detection algorithm on off e.g. to reduce false positives . The range and sensitivity of the freeform gesture detection may be adjusted by control parameters. The interference detection block may detect offending e.g. interfering ultrasound sensor frequencies in a frequency domain and may cancel effects of such frequencies.

In a particular embodiment low power proximity sensing or detection may be implemented. For example low power proximity sensing may include transmitting low duty cycle ultrasound signals detecting a proximity event and waking up a full power mode gesture or hover detection module of a DSP. Particular examples of low power proximity detection are further described with reference to .

A mobile ultrasound framework may support switching between low power and full power modes. In some implementations analog microphone s may also be supported. In a particular embodiment multi modal e.g. multiple sensor type framework support may be included. The multi modal framework may enable independent gesture recognition systems e.g. camera ultrasound infrared etc. to share information e.g. in accordance with a common data model to improve gesture recognition accuracy. Hovering detection may also be implemented to detect slow moving finger coordinates at an effective distance of one to three centimeters from a screen.

In a particular embodiment low power proximity detection may detect and differentiate between the following conditions 1 when something covers a speaker microphone e.g. when the device is in a pocket or pushed tightly against an ear 2 when something is a short distance above a speaker microphone e.g. when a hand is waving a short distance away or when the device being pushed loosely against an ear and 3 when nothing is above the speaker microphone within a particular distance e.g. corresponding to an idle or no action period . illustrates a particular example of a low power proximity detection system and is generally designated . The system includes a high pass filter HPF an adaptive notch filter a multiplier a down sampler a transform module and a proximity detection module as shown. A signal received by the system may be a linear sweep or another wideband continuous ultrasound wave. FM demodulation down mixing may include multiplication of a transmitted signal frame and a received signal frame. Low pass filtering may remove a high frequency portion of the demodulated signal to generate a baseband signal. A fast Fourier transform FFT may be performed to generate phase delay infatuation and range or proximity measurement may be performed based on FFT peaks or spectrum pattern.

Various gesture detection functions modules and algorithms described herein may be implemented e.g. by a DSP in one or more gesture libraries. Applications including third party applications may call into the gesture libraries to incorporate gesture detection features. The described techniques may thus provide a gesture detection framework that application developers may leverage to enhance user experience. The gesture detection framework may also be used by mobile device vendors and operating systems to improve core user experience.

In a particular embodiment full power gesture detection may include near swipe gesture detection to detect simple swipe gestures near a device surface. For example user hands in different poses with arbitrary angles as well as finger swipes may be detected. Near swipe gestures may include left right and select gestures approximately five to ten centimeters from the device surface. To illustrate a left or right gesture may be represented by the user s hand or finger moving quickly from left to right right to left clockwise or counterclockwise in parallel or vertical to the device surface e.g. screen . A select or deselect gesture may be represented by the user s hand being extended away from the user s body and moving in a normal speed towards the screen followed by staying in a particular location for a particular period of time e.g. half a second and then moving away from the device. A configurable delay may be implemented before the next gesture is enabled. By using such near swipe gesture detection an application may obtain a detected gesture e.g. left right select via a key event or a touch event that is translated by framework software. An ultrasound framework may map specific user events received from a gesture library to a corresponding high level operating system HLOS input event or may output raw data via an API.

Full power gesture detection may also include support for audio and gesture concurrency in which audio may be played back from one or more speakers in stereo mode. Playback may be switched to a headset or to a high definition multimedia interface HDMI connection e.g. when the mobile device is connected to a television . Hot swapping back and forth may be supported. In a particular embodiment if a speakerphone supports audio and ultrasound signal output ultrasound signals may be output for gesture detection even while audio signals are being output. For example a user may listen to a song using one or more speakers of a mobile device while the same one or more speakers output ultrasound signals thereby enabling the user to control the music output via gestures e.g. rewind fast forward pause next song previous song raise volume lower volume etc. .

In a particular embodiment low power proximity detection may detect hands near a device surface covering the device or moving across the device surface. A proximal detection distance e.g. five to ten centimeters may act as a control parameter for the proximity detection. Examples of use for such proximity detection may include determining whether to turn on full power mode applications such as gesture hover camera gesture etc. In a particular embodiment proximity detection may activate a wake up event flag in response to detecting that a user s hand is covering the device has moved near a speaker microphone etc. By using such proximity detection an application may obtain proximity detection notification via a key event or a touch event translated by framework software. An ultrasound framework may map received user events and turn on full power gesture recognition in response to low power proximity detection.

In a particular embodiment a gesture detection framework may be robust enough to support certain user parameter values. For example a supported velocity parameter e.g. how fast a gesture can move but still be recognized may be 2 20 Hz. An operating distance parameter e.g. how far or close the user can be from a sensor may be 5 10 cm. A failure rate parameter may be 50 150 per thousand. A gesture intuitiveness parameter e.g. how easy it is for a user to learn how to perform a correct gesture may be that 90 of users are able to perform a correct gesture after reading setup instructions.

The framework may also be robust enough to support certain system parameter values. For example a 40 millisecond response time may be supported. The response time may correspond to how much time can elapse between a user initiating an action and an expected result being achieved e.g. a latency between completing a gesture and observing an effect . In a particular embodiment low power proximity detection may be performed at a DSP of a mobile device on a 25 duty cycle and full power gesture detection may be performed on a CPU of the mobile device where the CPU consumes approximately 60 times as much power as the DSP. In a particular embodiment the system may be designed such that transducers do not introduce high level nonlinear effects on a transmit path. Moreover gesture detection may work even in the presence of interference such as from wireless e.g. Institute of Electrical and Electronics IEEE 802.11 beacon signals and other devices e.g. when an ultrasound stylus is placed near the device or when multiple devices are operating in close proximity .

It should be noted that although ultrasound detection may operate in low light and no light scenarios certain types of lights may generate ultrasound band noise. Thus the system may function such that ultrasound noise emitted by such lights does not saturate the transmit path. In certain situations audio band noise wind noise and or turbulence may cause microphone saturation and generate a nonlinear effect. Thus the system may be robust enough to support gesture detection under such noise conditions. The system may also be robust enough to support various stabilities. For example although relative movement of the device and background may cause false detection the system may support gesture detection even when moderate amount of relative movement between a user s hand and the device exists.

In a particular embodiment a transition sequence between full power gesture detection and low power proximity detection maybe as follows. A gesture manager may be responsible for starting and stopping different gesture detection sources based on a mode of operation which may be determined at the application level and or the system level. The gesture manager may start ultrasound low power proximity detection. The ultrasound low power proximity detection may notify the gesture manager of a proximity event. The gesture manager may stop the ultrasound low power proximity detection and may start full power ultrasound camera gesture detection. The full power ultrasound camera gesture detection may notify the gesture manager of gesture events. When the full power ultrasound camera gesture detection times out the gesture manager may stop the full power ultrasound camera gesture detection and may restart the ultrasound low power proximity detection.

In a particular embodiment a switching mechanism between low power and full power modes may be implemented by an application processor e.g. CPU based on a state of the full power module s and other concurrent application events. For example a low power mode e.g. a low power proximity detection mode may be entered when a display screen is turned off. Low power mode may also be entered when full power gesture detection sends a switch off command. In response to the switch off command a device may wait for a period of time e.g. 1 2 minutes and then transition into the low power mode if the full power gesture detection has not sent a switch on command A low power proximity detection event may also trigger full power gesture hover camera detection.

In some implementations a delay may be associated with low power proximity detection. Thus several separate steps may be performed to complete a transition from the low power mode to the full power mode. Initially a hand may cover or move near the device to trigger the low power proximity detection. The full power mode may be entered with user feedback e.g. visual feedback . A follow on gesture may be detected in the full power mode. Alternately or in addition if a phone call is received the device may switch to the full power mode.

In accordance with a described embodiment an electronic device may transition out of a low power mode e.g. from a sleep mode to an active mode in response to detection of signal reflections having a particular characteristic when compared to a transmitted signal. For example as illustrated in an electronic device can be configured with a speaker and a microphone . When the electronic device is in a low power mode the speaker emits a signal. As the signal hits objects e.g. an illustrative object the signal is reflected and some of the reflections are detected by the microphone . In one implementation as shown in a signal with a frequency that varies linearly may be transmitted by the speaker and a reflection of the signal may be received at the microphone . The electronic device may determine e.g. via a low power ultrasound detection front end module or circuit whether the reflection indicates a request to transition out of the low power mode e.g. situations such as taking a phone out of a pocket flipping a phone from face down to face up etc. via a device wakeup module or circuit . To determine whether the reflection indicates a request to transition out of the low power mode a frequency difference between a frequency of the reflection and a frequency of the signal emitted from the speaker at the time the reflection is received may be calculated.

For example as illustrated in an ultrasound signal designated signal from speaker in may be emitted from a speaker of an electronic device e.g. the electronic device of at a first time e.g. t . A first reflection of the ultrasound signal e.g. designated rin may be received by a microphone at a second time e.g. t . A frequency of the first reflection at t may correspond to a frequency of the ultrasound signal at t. Upon receiving the first reflection the electronic device may begin a method to calculate the frequency difference of the first reflection. To begin the method the electronic device may multiply the frequency of the ultrasound signal at t and the frequency of the first reflection at t to produce a frequency difference and a frequency sum. The frequency difference may indicate a distance that an object that caused the first reflection is from the electronic device. The frequency difference and frequency sum are processed through a low pass filter to remove the frequency sum and isolate the frequency difference. The electronic device may perform a fast Fourier transform on the isolated frequency difference to generate an amplitude value e.g. the lower left quadrant of corresponding to the distance of the object from the electronic device. A second reflection of the ultrasound signal e.g. designated rin may be received by the microphone at a third time e.g. t . In response to receiving the second reflection the electronic device may perform the method steps again. As shown in after calculating the frequency differences of the first reflection and the second reflection a first peak and a second peak are identified e.g. p and p .

By sequentially performing the above described method using the first and second reflections numerous times a spectrum of peaks may be generated. The generated spectrum of peaks may be used to determine whether to transition the electronic device out of the low power mode. For example a particular spectrum may indicate whether the device is to transition out of the low power mode. The electronic device may include a memory to store one or more known spectrums e.g. a spectrum indicating that the electronic device is in a user s pocket the electronic device is face down on a table etc. . A first stored spectrum may correspond to a spectrum that is not to cause the electronic device to transition out of the low power mode e.g. the phone is in the user s pocket . The electronic device may transition out of the low power mode based on the generated spectrum.

In another implementation a waveform of the ultrasound signal may vary according to a cosine wave. The ultrasound signal may be transmitted by the speaker and a reflection of the signal may be received at the microphone. As explained above with reference to a waveform of the first reflection at a time t may correspond to the waveform of the ultrasound signal at t. The electronic device may transition out of the low power mode based on a phase difference between the waveform of the reflection at t and the waveform of the ultrasound signal at t.

When the spectrum does not match a negative example the electronic device may determine whether the spectrum indicates a signal with a repeated change of path e.g. a hand being waived in front of the electronic device at . If the spectrum indicates a repeated change of path the electronic device may transition out of the low power mode at .

When the spectrum does not indicate a signal with a repeated change of path the electronic device may determine whether the spectrum indicates a signal transition at . For example the electronic device may determine that a first spectrum indicates that the electronic device is in a pocket of a user. If a second spectrum is calculated that indicates the electronic device is not in the pocket of the user the electronic device may transition out of the lower power mode at . As another example if the electronic device is a phone a first spectrum may indicate that the phone has been placed on a desk with a display screen of the phone facing down towards the desk. If at a later time a second spectrum is calculated that indicates the phone has been picked up off the desk or that the display screen is no longer facing down towards the desk the electronic device may transition out of the lower power mode.

When the spectrum does not indicate a signal transition the electronic device may extract a characteristic of the spectrum e.g. energy indicated by the spectrum intensity of the spectrum etc. at and determine whether the characteristic exceeds a threshold value at . If the characteristic exceeds the threshold value the electronic device may transition out of the low power mode at . Otherwise the electronic device may update the negative examples at and wait a period of time at before transmitting a second ultrasound signal from the speaker.

Upon transitioning out of the low power mode at the electronic device may be configured to transition back into the low power mode if no activity e.g. positive feedback at at the electronic device is detected. Additionally if a false positive condition is detected the spectrum associated with the reflection that caused the device to transition out of the low power mode may be added to the stored negative examples e.g. because the spectrum was a false positive at .

Using ultrasound transmitters and microphones to identify when to transition an electronic device out of a low power mode may consume less power than other techniques e.g. optical infra red type systems and wake up on touch type systems . Additionally because many electronic devices are already equipped with a speaker and a microphone no extra transducers or sensors are needed.

In conjunction with the described embodiments an apparatus is disclosed that includes first sensor means for generating a first output and having a first sensor type. For example the first sensor means for generating may be the ultrasound sensor of the ultrasound processing control module of the ultrasound tracking module of the ultrasound gesture recognition module of one or more other devices or circuits to generate a first output and having a first sensor type or any combination thereof.

The apparatus may also include second sensor means for generating second output and having a second sensor type that is different from the first sensor type. For example the second sensor means for generating may be the camera of the image processing control module of the camera tracking module of the camera gesture recognition module of one or more other devices or circuits to generate a second output and having a second sensor type that is different from a first sensor type or any combination thereof.

The apparatus may further include means for detecting a gesture based on the first output and the second output according to a complementary voting scheme that is at least partially based on gesture complexity. For example the means for detecting may be the gesture recognition module of the multi mode gesture discrimination module of the extended gesture interface of one or more other devices or circuits to detect a gesture or any combination thereof.

The apparatus may include means for determining a lighting level. For example the means for determining the lighting level may be the light detector of the light detector of one or more other devices or circuits to determine a lighting level or any combination thereof. The apparatus may also include means for determining a gesture range. For example the means for determining the gesture range may be the ultrasound sensor of the camera of the ultrasound processing control module of the image processing control module of the ultrasound tracking module of the camera tracking module of one or more other devices or circuits to determine a gesture range or any combination thereof.

The apparatus may further include means for selectively deactivating the second sensor means based on the lighting level and the gesture range. For example the means for selectively deactivating may be a portion of the camera of the image processing control module of one or more other devices or circuits to selectively deactivate sensor means or any combination thereof.

Those of skill would further appreciate that the various illustrative logical blocks configurations modules circuits and algorithm steps described in connection with the embodiments disclosed herein may be implemented as electronic hardware computer software executed by a processing device such as a hardware processor or combinations of both. Various illustrative components blocks configurations modules circuits and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or executable software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application but such implementation decisions should not be interpreted as causing a departure from the scope of the present disclosure.

The steps of a method or algorithm described in connection with the embodiments disclosed herein may be embodied directly in hardware in a software module executed by a processor or in a combination of the two. A software module may reside in a non transitory storage medium such as random access memory RAM magnetoresistive random access memory MRAM spin torque transfer MRAM STT MRAM flash memory read only memory ROM programmable read only memory PROM erasable programmable read only memory EPROM electrically erasable programmable read only memory EEPROM registers hard disk a removable disk a compact disc read only memory CD ROM or any other form of storage medium known in the art. An exemplary storage medium is coupled to the processor such that the processor can read information from and write information to the storage medium. In the alternative the storage medium may be integral to the processor. The processor and the storage medium may reside in an application specific integrated circuit ASIC . The ASIC may reside in a computing device or a user terminal. In the alternative the processor and the storage medium may reside as discrete components in a computing device or a user terminal.

The previous description of the disclosed embodiments is provided to enable a person skilled in the art to make or use the disclosed embodiments. Various modifications to these embodiments will be readily apparent to those skilled in the art and the principles defined herein may be applied to other embodiments without departing from the scope of the disclosure. Thus the present disclosure is not intended to be limited to the embodiments shown herein but is to be accorded the widest scope possible consistent with the principles and novel features as defined by the following claims.

