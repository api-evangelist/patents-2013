---

title: Classification and non-parametric regression framework with reduction of trained models
abstract: A device receives selection of a classification and regression framework, and receives training data for the classification and regression framework. The device applies the training data to the classification and regression framework to generate a trained model, and monitors performance of the trained model. The device inspects a structure of the trained model, and reduces a size of the trained model. The device generates an object based on the trained model, and provides the object for display.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09501749&OS=09501749&RS=09501749
owner: The MathWorks, Inc.
number: 09501749
owner_city: Natick
owner_country: US
publication_date: 20130313
---
This application claims priority under 35 U.S.C. 119 based on U.S. Provisional Patent Application No. 61 610 657 filed Mar. 14 2012 the disclosure of which is incorporated by reference herein in its entirety.

The accompanying drawings which are incorporated in and constitute a part of this specification illustrate one or more implementations and together with the description explain these implementations. In the drawings 

The following detailed description refers to the accompanying drawings. The same reference numbers in different drawings may identify the same or similar elements.

Supervised learning algorithms perform the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions for a particular problem. Even if the hypothesis space contains hypotheses that are well suited for the particular problem it may still be difficult to find a suitable hypothesis. Ensembles combine multiple hypotheses to ideally form a better hypothesis. In other words an ensemble is a technique for combining many weak learners in an attempt to produce a strong learner. The term ensemble may refer to methods that generate multiple hypotheses using a same base learner.

An ensemble is a supervised learning algorithm because an ensemble can be trained and then used to make predictions. The trained ensemble therefore represents a single hypothesis. However this single hypothesis is not necessarily contained within a hypothesis space of models from which the single hypothesis is built. Thus ensembles may have more flexibility than functions represented by the ensembles.

Systems and or methods described herein may provide a classification and non parametric regression framework referred to herein as a classification regression framework or a framework that provides models for ensemble learning classification and or non parametric regression. The classification regression framework may provide a common application programming interface API so that a user of the framework may provide training data to the framework. The classification regression framework may utilize the training data to produce a final object for the user. The user may apply the final object to actual data in order to generate predicted results. The systems and or methods may enable the user to add one or more new models to the classification regression framework and thus extend the framework.

Full models may include a full parent class e.g. FullClassificationRegressionModel . The full parent class may include two child classes e.g. FullClassificationModel and FullRegressionModel . One child class e.g. FullClassificationModel may be derived from the child class e.g. ClassificationModel and the full parent class e.g. FullClassificationRegressionModel . Another child class e.g. FullRegressionModel may be derived from the child class e.g. RegressionModel and the full parent class e.g. FullClassificationRegressionModel . The classes of the framework may be abstract and may not be constructed directly. The classes of the framework may serve as base classes for extending the framework with new models. A full class may store information inherited from a respective compact class and may store information describing how the model is trained or fitted such as training data and model fit parameters selected by the user.

As further shown in the classification regression framework may receive a request e.g. from a user to add a new model to the framework. The classification regression framework may create based on the request a first class that computes predictions of the new model for unknown data. The classification regression framework may enable the user to create a second class that stores input parameters for the new model and may enable the user to create a third class that builds the new model based on training data and or class labels. The classification regression framework may add the new model to a set of models of the framework and may create a standalone model for the new model in the framework. The standalone model may be utilized for ensemble learning and or cross validation and may modify the framework to produce an extended framework as shown in .

The classification regression framework as further shown in may receive training data. The training data may include data used to discover potentially predictive relationships. In statistical modeling the training data may be used with a model in order to predict a response value from one or more predictors. The training data may be applied to the classification regression framework in order to generate a trained model. The classification regression framework may monitor performance of the trained model. The classification regression framework may inspect a structure of the trained model e.g. a decision tree structure and or fit information e.g. errors generated by the trained model and may reduce a size of the trained model e.g. via pruning .

The training data may or may not be re applied to the classification regression framework. After the training data is or is not re applied to the classification regression framework the framework may produce a final object based on the trained model as further shown in . In some implementations the final object may include a trained ensemble a trained classifier etc. As shown in the user may provide actual data to the final object and the final object may generate predicted results based on the actual data.

Such an arrangement may enable the user to quickly and easily create via a common API an object e.g. a trained ensemble a trained classifier etc. based on training data provided to the classification regression framework. The arrangement may also enable the user to add one or more new models to the classification regression framework via an established architecture. The added new model s may create an extended classification regression framework.

The terms code and program code as used herein are to be used interchangeably and are to be broadly interpreted to include text based code that may require further processing to execute e.g. C code Hardware Description Language HDL code very high speed integrated circuits VHSIC HDL VHDL code Verilog Java and or other types of hardware or software based code that may be compiled and or synthesized binary code that may be executed e.g. executable files that may directly be executed by an operating system bitstream files that can be used to configure a field programmable gate array FPGA Java byte code object files combined together with linker directives source code makefiles etc. text files that may be executed in conjunction with other executables e.g. Python text files a collection of dynamic link library DLL files with text based combining configuration information that connects pre compiled modules an extensible markup language XML file describing module linkage etc. etc. In one example code may include different combinations of the above identified classes e.g. text based code binary code text files etc. . Alternatively or additionally code may include a dynamically typed programming language e.g. the M language a MATLAB language a MATLAB compatible language a MATLAB like language etc. that can be used to express problems and or solutions in mathematical notations. Alternatively or additionally code may be of any type such as function script object etc. and a portion of code may include one or more characters lines etc. of the code.

User interfaces as described herein may include graphical user interfaces GUIs and or non graphical user interfaces such as text based interfaces. The user interfaces may provide information to users via customized interfaces e.g. proprietary interfaces and or other types of interfaces e.g. browser based interfaces etc. . The user interfaces may receive user inputs via one or more input devices may be user configurable e.g. a user may change the sizes of the user interfaces information displayed in the user interfaces color schemes used by the user interfaces positions of text images icons windows etc. in the user interfaces etc. and or may not be user configurable. Information associated with the user interfaces may be selected and or manipulated by a user of a technical computing environment TCE e.g. via a touch screen display a mouse a keyboard a keypad voice commands etc. .

Client device may include one or more computation devices such as for example a laptop computer a personal computer a tablet computer a desktop computer a workstation computer a smart phone a personal digital assistant PDA and or other computation devices. In some implementations client device may include a TCE described below.

Server device may include one or more server devices or other types of computation and communication devices. Server device may include a device that is capable of communicating with client device e.g. via network . In some implementations server device may include one or more laptop computers personal computers workstation computers servers central processing units CPUs graphical processing units GPUs application specific integrated circuits ASICs field programmable gate arrays FPGAs etc. In some implementations server device may include TCE and may perform some or all of the functionality described herein for client device . Alternatively server device may be omitted and client device may perform all of the functionality described herein for client device .

Network may include a network such as a local area network LAN a wide area network WAN a metropolitan area network MAN a telephone network such as the Public Switched Telephone Network PSTN or a cellular network an intranet the Internet or a combination of networks.

As indicated above TCE may be provided within a computer readable medium of client device . Alternatively or additionally TCE may be provided in another device e.g. server device that is accessible by client device . TCE may include hardware or a combination of hardware and software that provides a computing environment that allows users to perform tasks related to disciplines such as but not limited to mathematics science engineering medicine business etc. more efficiently than if the tasks were performed in another type of computing environment such as an environment that required the user to develop code in a conventional programming language such as C C Fortran Pascal etc. In some implementations TCE may include a dynamically typed programming language e.g. the M language a MATLAB language a MATLAB compatible language a MATLAB like language etc. that can be used to express problems and or solutions in mathematical notations.

For example TCE may use an array as a basic element where the array may not require dimensioning. These arrays may be used to support array based programming where an operation may apply to an entire set of values included in the arrays. Array based programming may allow array based operations to be treated as high level programming that may allow for example operations to be performed on entire aggregations of data without having to resort to explicit loops of individual non array operations. In addition TCE may be adapted to perform matrix and or vector formulations that can be used for data analysis data visualization application development simulation modeling algorithm development etc. These matrix and or vector formulations may be used in many areas such as statistics image processing signal processing control design life sciences modeling discrete event analysis and or design state based analysis and or design etc.

TCE may further provide mathematical functions and or graphical tools e.g. for creating plots surfaces images volumetric representations etc. . In some implementations TCE may provide these functions and or tools using toolboxes e.g. toolboxes for signal processing image processing data plotting parallel processing etc. . In some implementations TCE may provide these functions as block sets or in another way such as via a library etc.

TCE may be implemented as a text based environment e.g. MATLAB software Octave Python Comsol Script MATRIXx from National Instruments Mathematica from Wolfram Research Inc. Mathcad from Mathsoft Engineering Education Inc. Maple from Maplesoft Extend from Imagine That Inc. Scilab from The French Institution for Research in Computer Science and Control INRIA Virtuoso from Cadence Modelica or Dymola from Dynasim etc. a graphically based environment e.g. Simulink software Stateflow software SimEvents software Simscape software etc. by The MathWorks Inc. VisSim by Visual Solutions LabView by National Instruments Dymola by Dynasim SoftWIRE by Measurement Computing WiT by DALSA Coreco VEE Pro or SystemVue by Agilent Vision Program Manager from PPT Vision Khoros from Khoral Research Gedae by Gedae Inc. Scicos from INRIA Virtuoso from Cadence Rational Rose from IBM Rhapsody or Tau from Telelogic Ptolemy from the University of California at Berkeley aspects of a Unified Modeling Language UML or SysML environment etc. or another type of environment such as a hybrid environment that includes one or more of the above referenced text based environments and one or more of the above referenced graphically based environments.

TCE may include a programming language e.g. the MATLAB language that may be used to express problems and or solutions in mathematical notations. The programming language may be dynamically typed and or array based. In a dynamically typed array based computing language data may be contained in arrays and data types of the data may be determined e.g. assigned at program execution time.

For example suppose a program written in a dynamically typed array based computing language includes the following statements 

During run time when the statement A hello is executed the data type of variable A may be a string data type. Later when the statement A int32 1 2 is executed the data type of variable A may be a 1 by 2 array containing elements whose data type are 32 bit integers. Later when the statement A 1.1 2.2 3.3 is executed since the language is dynamically typed the data type of variable A may be changed from the above 1 by 2 array to a 1 by 3 array containing elements whose data types are floating point. As can be seen by this example data in a program written in a dynamically typed array based computing language may be contained in an array. Moreover the data type of the data may be determined during execution of the program. Thus in a dynamically type array based computing language data may be represented by arrays and data types of data may be determined at run time.

TCE may provide mathematical routines and a high level programming language suitable for non professional programmers and may provide graphical tools that may be used for creating plots surfaces images volumetric representations or other representations. TCE may provide these routines and or tools using toolboxes e.g. toolboxes for signal processing image processing data plotting parallel processing etc. . TCE may also provide these routines in other ways such as for example via a library local or remote database e.g. a database operating in a computing cloud remote procedure calls RPCs and or an application programming interface API . TCE may be configured to improve runtime performance when performing computing operations. For example TCE may include a just in time JIT compiler. In some implementations TCE may include the classification regression framework described herein.

Although shows example components of environment in some implementations environment may include fewer components different components differently arranged components or additional components than those depicted in . Alternatively or additionally one or more components of environment may perform one or more tasks described as being performed by one or more other components of environment .

Processing unit may include one or more processors microprocessors or other types of processing units that may interpret and execute instructions. Main memory may include one or more random access memories RAMs or other types of dynamic storage devices that may store information and or instructions for execution by processing unit . ROM may include one or more ROM devices or other types of static storage devices that may store static information and or instructions for use by processing unit . Storage device may include a magnetic and or optical recording medium and its corresponding drive.

Input device may include a mechanism that permits a user to input information to device such as a keyboard a camera an accelerometer a gyroscope a mouse a pen a microphone voice recognition and or biometric mechanisms a remote control a touch screen a neural interface etc. Output device may include a mechanism that outputs information to the user including a display a printer a speaker etc. Communication interface may include any transceiver like mechanism that enables device to communicate with other devices networks and or systems. For example communication interface may include mechanisms for communicating with another device or system via a network.

As described herein device may perform certain operations in response to processing unit executing software instructions contained in a computer readable medium such as main memory . A computer readable medium may be defined as a non transitory memory device. A memory device may include space within a single physical memory device or spread across multiple physical memory devices. The software instructions may be read into main memory from another computer readable medium such as storage device or from another device via communication interface . The software instructions contained in main memory may cause processing unit to perform processes described herein. Alternatively hardwired circuitry may be used in place of or in combination with software instructions to implement processes described herein. Thus implementations described herein are not limited to any specific combination of hardware circuitry and software.

Although shows example components of device in some implementations device may include fewer components different components differently arranged components or additional components than depicted in . Alternatively or additionally one or more components of device may perform one or more tasks described as being performed by one or more other components of device .

As shown in process may include receiving a request to add a new model to a classification regression framework block . For example client device TCE may provide for display a representation of the classification regression framework. In some implementations the representation of the classification regression framework may include a mechanism e.g. an icon a button a link etc. that may be selected by the user. When the mechanism is selected client device TCE may provide an interface that enables the user to provide a request for adding a new model e.g. a new weak learning model to the classification regression framework.

In some implementations the framework may provide a uniform interface for an individual classifier e.g. a single decision tree and or ensembles composed of many classifiers e.g. an ensemble of decision trees . The framework may grow the ensembles by boosting or bagging. Every classifier in the framework may be used as a standalone learning model and as a weak learner for ensembles. For example a single decision tree may be built by the framework based on training data X and Y and using the syntax tree ClassificationTree.fit X Y . An ensemble of decision trees may be grown by using the syntax ens fitensemble X Y ensalg nlearn Tree where ensalg may be one of several ensemble learning algorithms included in the framework and nlearn may be a desired number of trees in the ensemble. The same syntax may extend to every classifier included in the framework.

In some implementations if the user wants to integrate a new model e.g. Foo into the framework client device TCE may create several new classes as described below. Implementing the new classes may require considerably less effort than coding the new model from scratch. The framework may provide some or more all of the functionality for the new model by default. After the new classes are created the new model may handle various types of classification labels may recognize input arguments related to classification may be cross validated may be used for ensemble learning etc. The framework may provide such functionality for new model without requiring the user to code the functionality. Extending the framework with a new model may be thought of as filling slots in a template. After the user fills the slots the user may utilize use the entire functionality of the framework for the new model.

As further shown in process may include creating based on the request a first class that computes predictions of the new model for unknown data block . For example the user via client device TCE may create a first class that computes predictions of the new model for unknown data. In some implementations client device TCE may derive the first class e.g. CompactClassificationFoo from ClassificationModel of the framework. The first class may be considered compact since objects of the first class may not store training data. However the first class may store a trained configuration for the new model e.g. Foo . For example the first class may provide a hidden constructor and properties for storing the trained configuration of the new model. Parameters storing the trained configuration of the new model may be declared as properties of the first class.

In some implementations client device TCE may provide a hidden method e.g. CompactClassificationFoo score that generates an N by K matrix s for N observations and K classes. Client device TCE may compute the matrix s using the trained configuration. The matrix s may be used by the framework to predict class labels for observations in X. For example a higher score for an observation may indicate a higher probability that the observation is part of the first class. In some implementations client device TCE may provide model specific properties and or methods for the first class.

As further shown in process may include creating a second class that stores input parameters for the new model block . For example the user via client device TCE may create a second class that stores input parameters for the new model e.g. Foo . In some implementations client device TCE may derive the second class e.g. FooParams from the framework e.g. from ModelParams . The second class may store all input parameters needed to fully specify the new model before training. If the new model does not utilize input parameters client device TCE may still create the second class but the second class may not have any properties.

In some implementations client device TCE may declare the input parameters as properties with public syntax e.g. GetAccess and SetAccess and may enable the user to set as many parameters and or properties as desired. In some implementations client device TCE may provide an input parameter method that may be used by the framework to set default input parameters if the user does not provide the input parameters. The default input parameters may depend on an input matrix of predictors X true class labels Y observation weights W a data structure e.g. dataSummary with fields e.g. PredictorNames CategoricalPredictors and ResponseName a data structure e.g. classSummary with fields e.g. ClassNames NonzeroProbClasses Prior and Cost and a function handle e.g. scoreTransform for transforming classification scores produced by the new model from one range to another range.

Returning to process may include creating a third class that builds the new model based on training data and or class labels block . For example the user via client device TCE may create a third class that builds the new model e.g. Foo based on training data e.g. a matrix of predictors X and or true class labels Y . In some implementations client device TCE may derive the third class e.g. ClassificationFoo from FullClassificationModel of the framework and from the first class e.g. CompactClassificationFoo . The third class may be a full class as opposed to a compact class because objects of the third class may store training data X and Y. The third class may know how to train the new model based on the training data and may store the training data e.g. in a particular matrix of predictors X the true class labels Y and observation weights W . In some implementations the third class may include model specific properties and methods appropriate for a full object. These model specific properties and methods may rely on the training data absent in a compact object.

As further shown in process may include adding the new model to a set of models of the classification regression framework block . For example the user via client device TCE may add the new model e.g. Foo to a set of models of the classification regression framework. In some implementations client device TCE may add the new model to particular functions e.g. simpleModels.m classificationModels.m etc. of the framework. The functions may contain names of the models known to the framework. At this point the new model may be fitted using a fitted object e.g. ClassificationFoo.fit and may use the methods and properties of the fitted object.

Returning to process may include creating a standalone model for the new model in the classification regression framework block . For example client device TCE may create a standalone model based on the new model in the classification regression framework. In some implementations client device TCE may add the standalone model to a particular function e.g. weakLearners.m . This function may contain names of new models e.g. added by users that may be used for cross validation and ensemble learning.

As further shown in process may include providing the standalone model for ensemble learning and or cross validation block . For example client device TCE may provide the standalone model for display to the user and the user may utilize the standalone model for ensemble learning and or cross validation. In some implementations client device TCE may store the standalone model e.g. in storage device . In some implementations the standalone model may include the same architecture as existing classes in the classification regression framework.

While shows process as including a particular quantity and arrangement of blocks in some implementations process may include fewer blocks additional blocks or a different arrangement of blocks. Additionally or alternatively some of the blocks may be performed in parallel.

Based on the request to add the new model e.g. Foo client device TCE may create a first class that computes predictions of the new model for unknown data. In some implementations client device TCE may derive the first class e.g. CompactClassificationFoo from ClassificationModel of the framework. The first class may be considered compact since objects of the first class may not store training data. However the first class may store a trained configuration for the new model e.g. Foo . In some implementations client device TCE may provide a hidden constructor and properties for storing the trained configuration of the new model as shown by a user interface of . As shown the first class may include a data structure e.g. dataSummary with three fields e.g. PredictorNames variable names CategoricalPredictors indices of categorical variables and ResponseName name of the response variable .

The first class may include another data structure e.g. classSummary with four fields e.g. ClassNames class names NonzeroProbClasses subset of classes in ClassNames with non zero priors Prior class prior probabilities and Cost misclassification costs . The first class may also include a function handle e.g. scoreTransform for transforming output of CompactClassificationFoo predict. In some implementations dataSummary classSummary and scoreTransform may be provided by the framework without any user interaction. The first class may include a user definable set of parameters e.g. trainedFooConfig that store the trained configuration of the new model. The parameters storing the trained configuration of the new model may be declared as properties of the first class.

As further shown in the first class may provide a hidden method e.g. CompactClassificationFoo score that generates an N by K matrix s for N observations and K classes. Client device TCE may compute the matrix s using the trained configuration. The matrix s may be used by the framework to predict class labels for observations in X. For example a higher score for an observation may indicate a higher probability that the observation is part of the first class. In some implementations client device TCE may provide model specific properties and or methods for the first class.

In example further assume that the user via client device TCE creates a second class that stores input parameters for the new model e.g. Foo . In some implementations client device TCE may derive the second class e.g. FooParams from the framework e.g. from ModelParams . The second class may store all input parameters needed to fully specify the new model before training. If the new model does not utilize input parameters client device TCE may still create the second class but the second class may not have any properties.

In some implementations client device TCE may declare the input parameters as properties with public syntax e.g. GetAccess and SetAccess and may enable the user to set as many parameters and or properties as desired. For example as shown in a user interface of client device TCE may declare the input parameters as properties with the syntax 

As further shown in user interface of client device TCE may provide a protected constructor with the following syntax 

As still further shown in user interface of client device TCE may provide a static hidden method e.g. FooParams.make with the following syntax 

In example further assume that the user via client device TCE provides an input parameter method as shown in a user interface of which may be used by the framework to set default input parameters if the user does not provide the input parameters. The default input parameters may depend on an input matrix of predictors X true class labels Y observation weights W a data structure e.g. dataSummary with fields e.g. PredictorNames CategoricalPredictors and ResponseName a data structure e.g. classSummary with fields e.g. ClassNames NonzeroProbClasses Prior and Cost and a function handle e.g. scoreTransform for transforming classification scores produced by the new model from one range to another range.

In example assume that the user via client device TCE creates a third class that builds the new model e.g. Foo based on training data e.g. a matrix of predictors X and or true class labels Y . In some implementations client device TCE may derive the third class e.g. ClassificationFoo from FullClassificationModel of the framework and from the first class e.g. CompactClassificationFoo . The third class may be a full class as opposed to a compact class because objects of the third class may store training data X and Y. The third class may know how to train the new model based on the training data and may store the training data e.g. in a particular matrix of predictors X the true class labels Y and observation weights W .

As further shown in user interface of the third class may include a static method e.g. ClassificationFoo.fit with the syntax 

As still further shown in the third class may include a public compact method with the following syntax 

After the third class is created client device TCE may add the new model e.g. Foo and the first second and third classes to a set of models of the classification regression framework. In some implementations client device TCE may add the new model and the first third classes to particular functions e.g. simpleModels.m classificationModels.m etc. of the framework. Client device TCE may create a standalone model based on the new model and or the first third classes in the classification regression framework. In some implementations client device TCE may add the standalone model to a particular function e.g. weakLearners.m of the framework. After the standalone model is created client device TCE may provide a user interface for display to the user as shown in . For example user interface may include a representation e.g. a button an icon a link etc. of the standalone model e.g. Foo and an indication that the new model e.g. Foo has been created by the classification regression framework. The user may utilize the standalone model for ensemble learning and or cross validation. For example the user may identify a location of training data for the new model via user interface of .

As shown in process may include receiving selection of a classification regression framework block . For example client device TCE may provide for display a representation of the classification regression framework. In some implementations the representation of the classification regression framework may include a mechanism e.g. an icon a button a link etc. that may be selected by the user. When the mechanism is selected client device TCE may provide an interface that enables the user to provide training data to the classification regression framework.

As further shown in process may include receiving training data for the classification regression framework block . For example client device TCE may instruct the user to input training data for the classification regression framework. Based on the instruction the user may identify a location e.g. in storage device of the training data to provide to the framework. In some implementations the user may input e.g. via keyboard the training data to client device TCE may import the training data from a remote location e.g. from server device etc. and client device TCE may provide the training data to the framework.

As further shown in process may include applying the training data to the classification regression framework to generate a trained model block . For example client device TCE may apply the training data to the classification regression framework. The framework may utilize the training data to generate a trained model. In some implementations the framework may train a simple learner by calling a static fit method of a respective class. For example the framework may grow a decision tree for classification with the syntax obj ClassificationTree.fit X Y varargin where obj may include a full object to be used for operations on the trained model Xmay include a double or single matrix of predictors and Y may include a vector of response values. Static fit methods may return objects of the same type as a class owning the static method e.g. obj may have type ClassificationTree .

In some implementations the framework may grow a decision tree for regression with the syntax obj RegressionTree.fit X Y varargin where Y may be of numeric type and obj may be of type RegressionTree. If the trained model can be used for either classification or regression the framework may provide two classes for the trained model ClassificationXXX and RegressionXXX. Before the model is trained the framework may perform various checks and preparatory work on input data and model parameters.

In some implementations the framework may train an ensemble by calling a fitensemble function such as obj fitensemble X Y method nlearn learners varargin where obj may include a full object to be used for operations on the trained model X may include a double or single matrix of predictors Y may include a vector of response values method may include a name of the model to be trained learners may include a cell array of weak learner templates a single weak learner template a string with the name of the weak learner and nlearn may include a number of ensemble iterations to be performed. The number of weak learners grown by the ensemble may be determined as a product of nlearn and the length of learners.

The framework may train the ensemble by creating weak learner templates and growing an ensemble using the templates. For example static template methods may produce objects of class FitTemplate. The FitTemplate object may include a set of input parameters to be supplied to the weak learner constructor.

Returning to process may include monitoring performance of the trained model and inspecting a structure of the trained model and or fit information block . For example the framework may inspect performance of the trained model based on training cross validation out of bag independent etc. test data. In some implementations the framework may monitor performance of the trained model based on unseen data. In some implementations the framework may provide methods for monitoring a quality of the trained model based on training data and variants of the training data such as cross validated data and out of bag data.

In some implementations the framework may inspect a structure of the trained model e.g. a decision tree structure and or fit information e.g. errors produced by weak hypotheses . For example the framework may inspect full objects compact objects classification objects and regression objects associated with the trained model. Public properties of a compact object may be included as a subset of public properties of a respective full object. In some implementations the framework may inspect public properties of compact objects public properties of full objects public properties and methods for decision trees public properties and methods for ensembles public properties for cross validated models etc.

As further shown in process may include reducing a size of the trained model block . For example the framework may reduce a size of the trained model by shrinking or pruning the trained model. In some implementations the framework may provide pruning for regression ensembles by a lasso technique. The framework may add methods e.g. regularize shrink and cvshrink and a property Regularization to the class for full regression ensembles RegressionEnsemble . For example the framework may grow a regression ensemble and the Regularization property may be empty. The framework may execute the regularize method to fit for optimal learner weights by the lasso technique. Based on execution of the regularize method the framework may obtain a full regression ensemble and the Regularization property may be filled. The framework may execute the shrink method to reduce the ensemble size by removing learners with optimized weights below a certain threshold. Based on the execution of the shrink method the framework may obtain a compact regression ensemble. In some implementations the executions of the regularize method and the shrink method may be replaced with a single call to the shrink method. In this case the regularize method may be called by the shrink method internally but the optimized weights may not be stored. In some implementations the framework may add methods e.g. regularize shrink and cvshrink and a property Regularization to the class for classification ensembles.

Returning to process may include determining whether to resume training of the model block . If training is to be resumed on the model block YES process may return to block . For example the framework may determine whether to resume training on the trained model after reducing the size of the trained model. If the framework determines that training is to be resumed on the trained model the framework may re perform the functionality described above in connection with blocks . In some implementations the framework may train an ensemble and may grow a few weak learners. After growing the few weak learners the framework may stop may inspect the performance of the trained ensemble and may resume training of the ensemble if the performance does not satisfy particular criteria.

As further shown in if training is not to be resumed on the model block NO process may include producing a final object based on the trained model block . For example if the framework determines that training is not to be resumed on the trained model the framework may output a final object based on the trained model. In some implementations client device TCE may output a representation e.g. a button a block an icon a link etc. of the final object that when selected may cause client device TCE to provide access to the final object. The user may then utilize the final object e.g. to provide actual data to the final object .

Returning to process may include applying the final object to actual data to generate predicted results block . For example client device TCE may request that the user identify or provide actual data for the final object. The user may identify or provide the actual data to client device TCE and client device TCE may apply the final object to the actual data. The final object may process the actual data to produce predicted results. Client device TCE may provide the predicted results for display to the user. For example if the user inputs signaling information as the actual data the final object may generate predicted signals included in the signaling information and predicted background noise included in the signaling information. Thus the final object may provide a mechanism for the user to predict signals and or background noise in signaling information. In some implementations client device TCE may store the final object and or the predicted results e.g. in storage device .

While shows process as including a particular quantity and arrangement of blocks in some implementations process may include fewer blocks additional blocks or a different arrangement of blocks. Additionally or alternatively some of the blocks may be performed in parallel.

Client device TCE may apply the training data to the classification regression framework. In some implementations the classification regression framework may include the classes in as indicated by reference number . As shown the framework may include a predictor class e.g. Predictor a regression model class e.g. RegressionModel a classification model class e.g. ClassificationModel a full classification regression model class e.g. FullClassificationRegressionModel a full regression model class e.g. FullRegressionModel and a full classification model class e.g. FullClassificationModel .

The predictor class may include a base class of the framework and may include a hidden property e.g. Impl . The Impl property may include an object that implements a prediction part for the prediction class. Most models may be used for either classification or regression and the Impl property may be shared between the classification and regression parts. If a user wants to extend the framework with a new model the user may provide a class that implements the Impl property. For some models it may not be necessary to have a predictor class. For example a model may be used for classification only and nothing may be shared with a regression class. In that case the user may implement a compact class derived from ClassificationModel or RegressionModel directly without the predictor class.

The predictor class may be a super class that defines a base interface for all objects visible to the user. The Impl property may encapsulate code shared by classification and regression parts of a same algorithm. In some implementations the Impl property may be pushed down into concrete classes. The predictor class may include an abstract method e.g. predict that users may utilize to make predictions. The predict method may be provided by ClassificationModel predict and RegressionModel predict which may be implemented using ClassificationModel score and RegressionModel response respectively. ClassificationModel score and RegressionModel response may include abstract protected methods defined in concrete classes such as CompactClassificationEnsemble.

The abstract protected methods may compute raw predictions from a model. For classification the raw predictions may include an N by K matrix of scores for N observations and K classes. For regression the raw predictions may include a vector of size N. Declaring the abstract protected methods therefore may not impose any requirements on classes derived from ClassificationModel and RegressionModel. The predictor class may include additional properties e.g. PredictorNames CategoricalPredictors and ResponseName . The additional properties may include dependent properties bundled in a data structure e.g. DataSummary .

The regression model class may include a property e.g. ResponseTransform for transforming the raw predictions of the model to an expected range. In some implementations the regression model class may be set to an identity transformation. The regression model class may be implemented with the following syntax 

The regression model class may include a property e.g. loss . The loss property may include a sum of argument checking a call to predict and a call to a function handle that computes the loss e.g. funloss . The loss property may be implemented with the following syntax 

The classification model class may include a property e.g. ScoreTransform that may be set by the user to available transformations. The available transformations may depend on a range of predictions provided by the model and on a range desired by the user. The ScoreTransform may be set to an appropriate function by default. The user may modify the ScoreTransform when the user extends the framework with a new classifier for which a correct default transformation is not set by the framework or if the user wants to compare predictions from several models on a same scale. The following transformations e.g. numeric one to one transformations on two dimensional matrices may be available to the user doublelogit identity invlogit ismax logit sign symmetric symmetricismax symmetriclogit etc. The user may assign either a function handle or a string to the ScoreTransform.

The classification model class may include ClassificationModel predict according to the following syntax 

The classification model class may include a property e.g. loss . The loss property may be implemented with the following syntax 

where C may include an N by K matrix of true class membership and K may include a number of classes. The matrix C may be logical with one true and K 1 false per row. The classCount may be a utility for converting true labels Y into the matrix C and Sfit may include an N by K matrix of predicted scores. The framework may provide classification loss functions such as classification error exponential loss binomial deviance loss etc.

The classification model class may include a margin method and an edge method. Implementation of the margin and edge methods may be similar to that of the loss property. Basically the margin and edge methods may call predict and may apply some type of funloss like function.

In some implementations the classification model class may accept types such as char cellstr logical categorical numeric etc. To treat the types consistently the framework may utilize a class e.g. ClassLabel . The ClassLabel may build on functionality of nominal. The nominal may provide the user with a new way of organizing data. The ClassLabel may be an internal tool not intended for the user. The ClassLabel may permit a user to work with class labels without converting the class labels to a type understood by the framework understands and then back to a type selected by a user. The ClassLabel may store class labels internally and may returns the class labels to the user as a type originally supplied by the user.

The full classification regression model class may be a base class for full model classes in the framework. The full classification regression model class may store training data as a matrix X and weight vectors Y and W . The vector Y may be stored in the full classification regression model class as a property e.g. PrivY with GetAccess protected. For classification PrivY may be of type ClassLabel and for regression PrivY may be numeric. The full classification model class and the full regression model class may include a dependent Y property with the following access methods 

The full classification regression model class may include a ClassLabel labels that converts labels from an internal representation to a type originally supplied by the user. The full classification regression model class may include an object e.g. ModelParams of class ParamsHolder which may store all model specific parameters either supplied by the user or assumed by default. The full classification regression model class may include an abstract method e.g. compact that may be implemented in a concrete class. The full classification regression model class may include an abstract method e.g. crossval for cross validation which may return an object sub classed from a partitioned model.

The full classification regression model class may include a method e.g. FullClassificationRegressionModel prepareDataCR and two similar purpose methods in e.g. FullClassificationModel prepareData and FullRegressionModel prepareData that may be used for data validity checks and pre processing. The full classification regression model class may include data structure e.g. dataSummary with fields e.g. PredictorNames CategoricalPredictors and ResponseName . The full classification regression model class may include a data structure e.g. classSummary with fields e.g. ClassNames NonzeroProbClasses Prior and Cost .

The full classification regression model class may include an object e.g. ClassNames of type ClassLabel which may store a list of classes supplied by the user and or found in the input Y data. A subset e.g. NonzeroProbClasses of these classes may include non zero probabilities. A class may have zero probability because the user explicitly passes a zero prior value for this class or because the user assigns zero weights to all observations from the class. The full classification regression model class may include an abstract static fit method that is an interface for fitting any model in the framework.

The full regression model class may include a base class for all full regression models of the framework. The full regression model class may inherit predict loss and response from the regression model class. The full regression model class may include methods e.g. resubPredict and resubLoss for computing resubstitution predictions and loss according to the syntax 

The regression model class may provide one output from predict and loss but the implementations for resubPredict and resubLoss may be more general so that a derived class may return an arbitrary number of output arguments. The derived class may re implement predict and loss but may not need to re implement resubPredict and resubLoss. The full regression model class may include a concrete method e.g. FullRegressionModel crossval that returns an object of type RegressionPartitionedModel.

The full classification model class may include a base class for all full classification models of the framework. The full classification model may be implemented in a similar manner as the full regression model class.

In some implementations the framework may include a decision tree architecture as shown in . As shown decision tree architecture may include a compact tree implementation class e.g. CompactTreeImpl a compact classification tree class e.g. CompactClassificationTree a classification tree class e.g. ClassificationTree a compact regression tree class e.g. CompactRegressionTree a regression tree class e.g. RegressionTree the classification model class the regression model class the full classification model class and the full regression model class. The classification model class the regression model class the full classification model class and the full regression model class may include the features described above. Although not shown in decision tree architecture may also include the predictor class and the full classification regression model class described above.

The compact tree implementation class may include a shared implementation for classification and decision trees. This implementation class may include properties and methods used by both classification and regression trees. For example the compact tree implementation class may include a method e.g. view that calls either classregtree disp or classregtree view a method e.g. loss that calls classregtree test for loss values computed for a pruning sequence a method e.g. processSubtrees that processes sub tree arguments.

The compact classification tree class may include methods and properties that may be implemented as calls to a property e.g. Impl or Impl.Tree that is an instance of class CompactTreeImpl. The compact classification tree class may include a CompactClassificationTree score as well as a CompactRegressionTree response which return empty arrays. The compact classification tree class may compute model predictions with a method e.g. CompactClassificationTree predict that overrides ClassificationModel predict. The predict methods for decision trees may have special signatures. The predict method for classification may return class labels scores e.g. posterior class probabilities node numbers class numbers coded as integers etc. The predict method may accept subtrees as an optional argument which may permit users to request output for specific trees in a pruned sequence. If the user passes sub tree indices output arguments of the predict method may receive an extra dimension and may be filled for every sub tree. The compact classification tree class may include a method e.g. loss that dispatches either to a method of a parent class e.g. ClassificationModel loss or to classregtree test.

The classification tree class may inherit methods and properties from the compact classification tree class. The classification tree class may include a property e.g. ClassDecisionTree that implements a static fit method using a pattern provided in FitTemplate. The classification tree class may implement a compact method by constructing an instance of CompactClassificationTree from an instance of CompactTreeImpl. The classification tree class may utilize a method e.g. ClassificationTree cvloss to compute classification error by cross validation. For example a user may execute ClassificationTree cvloss to find an optimal pruning level by cross validation. The ClassificationTree cvloss method may belong in the full class since the method cross validates on stored X and Y data. The classification tree class may include a constructor e.g. ClassificationTree for growing an actual decision tree.

The compact regression tree class may include features similar to the features of the compact classification tree class described above. The regression tree class may include features similar to the features of the classification tree class.

In some implementations the framework may include an ensemble architecture as shown in . As shown ensemble architecture may include a compact ensemble implementation class e.g. CompactEnsembleImpl a compact ensemble class e.g. CompactEnsemble a compact classification ensemble class e.g. CompactClassificationEnsemble an ensemble class e.g. Ensemble a classification ensemble class e.g. ClassificationEnsemble the classification model class and the full classification model class. The classification model class and the full classification model class may include the features described above. Although not shown in ensemble architecture may also include the predictor class the regression model class the full regression model class and the full classification regression model class described above. Although not shown in ensemble architecture may also include a compact regression ensemble class e.g. CompactRegressionEnsemble and a regression ensemble class e.g. RegressionEnsemble .

The compact ensemble implementation class may include a cell array e.g. Trained of trained weak learners and an object e.g. Combiner used to combine predictions from the trained weak learners. The compact ensemble implementation class may include a method e.g. predictorImportance that sums estimates of predictor importance over the weak learners. This method may return a vector of predictor importance ranks with one rank per predictor.

The compact ensemble class may provide protected dependent properties wrapped to a property e.g. Impl . Declaring an abstract property e.g. Impl in the compact ensemble class may force concrete descendants of this class to be derived from another class with a concrete Impl. The compact ensemble class may include static hidden methods that may be used by ensembles to do work behind the scenes e.g. check a validity of input arguments aggregate predictions from weak learners into an overall ensemble prediction etc. .

The compact classification ensemble class may implement a score method and may not need to override ClassificationModel predict and ClassificationModel margin. The compact classification ensemble class may include methods e.g. loss and edge that allow optional arguments such as learners specific to ensembles. The compact classification ensemble class may include a call e.g. predictorImportance to CompactEnsembleImpl predictorImportance.

The ensemble class may include properties and methods common to full ensemble objects. The ensemble class may inherit the properties from the compact ensemble class and may include dependent properties obtained from the ModelParams property. The ensemble class may include a cell array e.g. Trainable of full weak learner objects. In some implementations the Trainable cell array may be filled for cross validated ensembles. In this case an instance of the classification ensemble class may store learners for cross validation folds such as one element of an array e.g. Trained and one element of the Trainable cell array per fold. The Trained array may store an array of trained ensembles e.g. compact classification ensemble objects and the Trainable cell array may store an array of trainable ensembles e.g. classification ensemble objects . The ensemble class may include an abstract method e.g. resume that provides an interface for accumulating more training cycles i.e. adding more weak learners to the ensemble . The ensemble class may also include a protected method e.g. fitWeakLearners that performs the actual training of the ensemble by looping through weak learner templates and applying the templates to properly modified training data.

In some implementations the ensemble class may be a class for full ensembles for classification and may implement fit compact and resume. The ensemble class may inherit methods such as predict resubPredict etc. from the full classification model class. The ensemble class may inherit methods such as edge and loss from the full classification model class and the compact classification ensemble class.

The ensemble class may implement a static fit using a pattern provided by FitTemplate. The static fit e.g. ClassificationEnsemble.fit may be hidden but may be called by the user. The ensemble class may adjust prior class probabilities for cost sensitive learning prior to growing weak classifiers. Regression classes not shown in may be symmetric to the classification classes. However a regression ensemble class may also include more methods e.g. regularize shrink and cvshrink than the classification ensemble class.

In the framework fitting may be implemented using a factory operating on learner templates. For example ClassificationTree fit may be implemented as 

An ensemble learner typically applies the same weak model with the same parameters over and over again to the modified e.g. re sampled reweighted etc. training data. Thus the framework may implement an object template for a model with its parameters before the model is applied to the data. The template may not store training data but may know how to construct a trained object for the model once the training data is supplied.

A factory pattern may mean that for every class e.g. ClassificationTree there needs to be another class e.g. ClassificationTreeTemplate that knows how to create the first class. In the framework a proliferation of classes may be avoided by introducing naming conventions for learner classes and classes methods used to process model parameters. For example the following code may show how FitTemplate.make prepares functions to be called when a simple learner object e.g. ClassificationTree needs to be constructed on input data 

In the framework every learner may include a respective Params class. The Params class for decision trees and ensembles may include an abstract class e.g. ModelParams that provides a common interface. The Params class may include a Type e.g. classification or regression and a Method that is a name of the model. The Params class may include a static method e.g. make that extracts arguments specific to a concrete model from varargin constructs a ModelParams object for the concrete model and returns all unrecognized arguments as extraArgs. The Params class may include a method e.g. fillDefaultArgs that is called on an instance of a ModelParams class by the constructor of the base class for full models. This method may locate empty properties and may fill them with reasonable non empty default values. The method may take training data as input because default values for these properties may depend on the training data.

The Params class may include objects e.g. Generator and Modifier used to generate training data for the next weak learner and modify this training data after this weak learner is trained. These objects may save parameters for data generation and modification either supplied by the user or assumed by default and may save information accumulated in the process of training weak learners.

In some implementations the framework may generate training data for weak learners using ensemble data generation classes. The ensemble data generation classes may include a base class e.g. Generator that saves training data supplied by the user as X Y and W. The ensemble data generation classes may include optional data e.g. FitData for every weak learner. The ensemble data generation classes may include an accumulated number T of data generation cycles. After training is completed the accumulated number may be equal to a number of grown weak learners e.g. NTrained . The ensemble data generation classes may include a logical matrix e.g. UseObsForIter of size N by T. An element i j of this matrix may be true if observation i is used for iteration j and may be false otherwise. This matrix may be used for computing out of bag and cross validated information.

The ensemble data generation classes may include a method e.g. generate that generates training data at every iteration and saves an updated generator state. The ensemble data generation classes may include a method e.g. update that updates the data X Y W and FitData to be used for a next generation cycle. The update method may be called every time after a weak learner is trained and may be used to update training data using output from a current weak learner.

The ensemble data generation classes may include a class e.g. BlankGenerator that is a simple concrete class with trivial implementations of the generate and update methods. The ensemble data generation classes may include a class e.g. Resampler that generates data by resampling. The ensemble data generation classes may include a class e.g. Partitioner partitions data into folds for cross validation. The Resampler and Partitioner classes may include static processArgs methods for processing input arguments related to data generation.

In some implementations the framework may modify the training data by weak learners. Modification of the training data by a weak learner may be needed for boosting. In the framework classes for data modification may be named after respective algorithms. The data modification classes may include an array e.g. FitInfo with a first dimension of length T . The FitInfo array may have many dimensions and may be used for storing information accumulated during training.

The data modification classes may include strings e.g. FitInfoDescription and ReasonForTermination for informing users about what is in the FitInfo array and why the ensemble algorithm has stopped respectively. The data modification classes may include a number T of learning iterations performed by a Modifier class and may be equal to NTrained. The data modification classes may include a rate e.g. LearnRate of shrinkage. The data modification classes may include a method e.g. modify that updates training data and a state of an algorithm at every learning iteration. The modify method may include a logical flag e.g. mustTerminate that instructs a routine calling Modifier modify that training must stop. The modify method may also include X Y W fitData and a compact object H for a weak hypothesis constructed at a last learning step.

The data modification classes may include a method e.g. makeCombiner that returns an object of type Combiner. This object may be used by a compact trained model to combine predictions from weak learners. The data modification classes may include a blank modifier class e.g. BlankModifier that provides implementations of two abstract methods and one abstract property. The blank modifier may be used for example for bagging. A bagged ensemble may be grown by resampling data at every learning step but no data modification may be performed.

In some implementations the framework may combine predictions from weak learners in an ensemble using a weighted sum or a weighted average. For ensembles the framework may monitor for example how classification error or other forms of loss depends on the ensemble size. For example for one hundred weak learners the framework may compute a classification error from learner then classification errors from learners 1 2 etc. until learners 1 100. The framework may aggregate predictions from individual learners into an overall prediction using static hidden method e.g. CompactEnsemble.aggregatePredict according to the following syntax 

where the for loop may execute over T trained weak learners. A static hidden method e.g. CompactEnsemble.predictOneWithCache may include the following syntax 

In some implementations the framework may grow ensembles by resampling. In addition to predict and resubPredict provided by a trained model ensembles grown by resampling by the framework may provide an out of bag predict method e.g. oobPredict an out of bag loss method e.g. oobloss an out of bag margin method e.g. oobmargin an out of bag edge method e.g. oobedge etc.

The framework may provide a bagged ensemble class e.g. BaggedEnsemble . Any ensemble grown by resampling may be derived from the bagged ensemble class and that bagged ensemble class may expose properties of a method e.g. Resampler to the user. Concrete properties of the Resampler may be exposed as dependent properties of the bagged ensemble as follows 

In some implementations the ensemble class and its derivatives e.g. ClassificationEnsemble and RegressionEnsemble may provide most of the functionality needed for cross validation. A cross validation model may be thought of as an ensemble with one learner per fold. A property e.g. UseObsForIter of the Generator method may keep record of what observations are used for what learners and this record may be used to compute cross validated information. A partitioned model class e.g. PartitionedModel may implement the Adapter pattern and may define an interface for cross validated models. The partitioned model class may enable the user to determine whether a model is a cross validated model and may expose relevant properties and methods of the ensemble and partitioned data to the user.

In some implementations the framework may include a classification partitioned model class and regression partitioned model class for simple non ensemble partitioned models. The classification partitioned model class may add classification specific properties and classification specific methods that are implemented as wrapped calls to respective methods of the ensemble class.

In example further assume that the framework inspects performance of the trained model based on training cross validation out of bag independent etc. test data. In some implementations the framework may inspect a structure of the trained model e.g. a decision tree structure and or fit information e.g. errors produced by weak hypotheses . For example the framework may inspect full objects compact objects classification objects and regression objects associated with the trained model. The framework may reduce a size of the trained model by shrinking or pruning the trained model. In some implementations the framework may provide pruning for regression ensembles by a lasso technique.

The framework may determine whether to resume training on the trained model after reducing the size of the trained model. For example if the framework determines that training is to be resumed on the trained model the framework may resume training of the ensemble if the performance does not satisfy particular criteria. If the framework determines that training is not to be resumed on the trained model the framework may output a final object based on the trained model. In some implementations client device TCE may output a representation of the final object that when selected may cause client device TCE to provide access to the final object as shown in a user interface of .

As further shown in client device TCE may request via user interface that the user identify or provide actual data for the final object. The user may identify or provide the actual data to client device TCE and client device TCE may apply the final object to the actual data. The final object may process the actual data to produce predicted results. Client device TCE may provide the predicted results for display to the user as shown in a user interface of .

Systems and or methods described herein may provide a classification regression framework that provides models for ensemble learning classification and or non parametric regression. The classification regression framework may provide a common API so that a user of the framework may provide training data to the framework. The classification regression framework may utilize the training data to produce a final object for the user. The user may apply the final object to actual data in order to generate predicted results. The systems and or methods may enable the user to add one or more new models to the classification regression framework and thus extend the framework.

The foregoing description of implementations provides illustration and description but is not intended to be exhaustive or to limit the implementations to the precise form disclosed. Modifications and variations are possible in light of the above teachings or may be acquired from practice of the implementations.

It will be apparent that example aspects as described above may be implemented in many different forms of software firmware and hardware in the implementations illustrated in the figures. The actual software code or specialized control hardware used to implement these aspects should not be construed as limiting. Thus the operation and behavior of the aspects were described without reference to the specific software code it being understood that software and control hardware could be designed to implement the aspects based on the description herein.

Further certain portions of the implementations may be implemented as a component that performs one or more functions. This component may include hardware such as a processor an ASIC or a FPGA or a combination of hardware and software.

Even though particular combinations of features are recited in the claims and or disclosed in the specification these combinations are not intended to limit the disclosure of the specification. In fact many of these features may be combined in ways not specifically recited in the claims and or disclosed in the specification. Although each dependent claim listed below may directly depend on only one other claim the disclosure of the specification includes each dependent claim in combination with every other claim in the claim set.

No element act or instruction used herein should be construed as critical or essential unless explicitly described as such. Also as used herein the articles a and an are intended to include one or more items and may be used interchangeably with one or more. Where only one item is intended the term one or similar language is used. Further the phrase based on is intended to mean based at least in part on unless explicitly stated otherwise.

