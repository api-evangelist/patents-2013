---

title: System, method, and computer program product for managing divergences and synchronization points during thread block execution by using a double sided queue for token storage
abstract: A system, method, and computer program product for ensuring forward progress of threads that implement divergent operations in a single-instruction, multiple data (SIMD) architecture is disclosed. The method includes the steps of allocating a queue data structure to a thread block including a plurality of threads, determining that a current instruction specifies a yield operation, pushing a token onto the second side of the queue data structure, disabling any active threads in the thread block, popping a next pending token from the first side of the queue data structure, and activating one or more threads in the thread block according to a mask included in the next pending token.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09459876&OS=09459876&RS=09459876
owner: NVIDIA Corporation
number: 09459876
owner_city: Santa Clara
owner_country: US
publication_date: 20130718
---
Threads i.e. an abstract construct of an instance of a program executing on a processor have a basic guarantee of forward progress. In other words if one thread becomes blocked e.g. due to resources being unavailable or the inability to acquire a semaphore then other threads continue to make forward progress until the thread becomes unblocked. The other threads will continue execution unless the other threads are also dependent on unavailable resources. The guarantee of forward progress is necessary to support patterns extremely common in procedural parallel programming such as locks. In single processors such as conventional CPUs threads are typically guaranteed forward progress by allocating each thread a number of cycles of the processing unit of the processor in a serialized or round robin fashion.

Unfortunately threads executing in a parallel processing architecture such as architectures common to today s graphics processing units GPUs may be executed concurrently and are not typically independent of other concurrently executing threads. When a particular thread becomes blocked a number of other concurrently executing threads may also become blocked as a result of current divergence mechanisms implemented in parallel processing architectures. Consequently parallel threads that implement locks or critical sections of code may deadlock unpredictably thereby failing to ensure forward progress of the threads. Thus there is a need for addressing this issue and or other issues associated with the prior art.

A system method and computer program product for ensuring forward progress of threads that implement divergent operations in a single instruction multiple data SIMD architecture is disclosed. The method includes the steps of allocating a queue data structure to a thread block including a plurality of threads determining that a current instruction specifies a yield operation pushing a token onto the second side of the queue data structure disabling any active threads in the thread block popping a next pending token from the first side of the queue data structure and activating one or more threads in the thread block according to a mask included in the next pending token.

Divergent threads may be tracked and managed via hardware and software using a data structure stored in a memory. Tokens are inserted into the data structure to track the divergent threads and extracted from the data structure to execute the divergent paths. Tokens are small packets of data that store state information related to a divergence control operation. A YIELD instruction is added to a processor s instruction set that causes the processor to disable the active threads being executed by the processor insert a token into the data structure and extract a token from the data structure. The extracted token causes the processor to activate a different set of threads to execute thus guaranteeing forward progress for different sets of threads in the thread block. In other words the YIELD instruction enables a program to yield execution time to one or more disabled threads.

At step the processor disables any active threads in the thread block. In one embodiment the processor clears all bits in the active mask associated with the thread block. At step the processor pops a next pending token from the first side of the data structure. In one embodiment the processor pops one or more tokens from the data structure in addition to the next pending token. At step the processor activates one or more threads in the thread block according to a mask included in the next pending token.

More illustrative information will now be set forth regarding various optional architectures and features with which the foregoing framework may or may not be implemented per the desires of the user. It should be strongly noted that the following information is set forth for illustrative purposes and should not be construed as limiting in any manner. Any of the following features may be optionally incorporated with or without the exclusion of other features described.

In one embodiment the PPU includes an input output I O unit configured to transmit and receive communications i.e. commands data etc. from a central processing unit CPU not shown over the system bus . The I O unit may implement a Peripheral Component Interconnect Express PCIe interface for communications over a PCIe bus. In alternative embodiments the I O unit may implement other types of well known bus interfaces.

The PPU also includes a host interface unit that decodes the commands and transmits the commands to the task management unit or other units of the PPU e.g. memory interface as the commands may specify. The host interface unit is configured to route communications between and among the various logical units of the PPU .

In one embodiment a program encoded as a command stream is written to a buffer by the CPU. The buffer is a region in memory e.g. memory or system memory that is accessible i.e. read write by both the CPU and the PPU . The CPU writes the command stream to the buffer and then transmits a pointer to the start of the command stream to the PPU . The host interface unit provides the task management unit TMU with pointers to one or more streams. The TMU selects one or more streams and is configured to organize the selected streams as a pool of pending grids. The pool of pending grids may include new grids that have not yet been selected for execution and grids that have been partially executed and have been suspended.

A work distribution unit that is coupled between the TMU and the SMs manages a pool of active grids selecting and dispatching active grids for execution by the SMs . Pending grids are transferred to the active grid pool by the TMU when a pending grid is eligible to execute i.e. has no unresolved data dependencies. An active grid is transferred to the pending pool when execution of the active grid is blocked by a dependency. When execution of a grid is completed the grid is removed from the active grid pool by the work distribution unit . In addition to receiving grids from the host interface unit and the work distribution unit the TMU also receives grids that are dynamically generated by the SMs during execution of a grid. These dynamically generated grids join the other pending grids in the pending grid pool.

In one embodiment the CPU executes a driver kernel that implements an application programming interface API that enables one or more applications executing on the CPU to schedule operations for execution on the PPU . An application may include instructions i.e. API calls that cause the driver kernel to generate one or more grids for execution. In one embodiment the PPU implements a SIMD Single Instruction Multiple Data architecture where each thread block i.e. warp in a grid is concurrently executed on a different data set by different threads in the thread block. The driver kernel defines thread blocks that are comprised of k related threads such that threads in the same thread block may exchange data through shared memory. In one embodiment a thread block comprises 32 related threads and a grid is an array of one or more thread blocks that execute the same stream and the different thread blocks may exchange data through global memory.

In one embodiment the PPU comprises X SMs X . For example the PPU may include 15 distinct SMs . Each SM is multi threaded and configured to execute a plurality of threads e.g. 32 threads from a particular thread block concurrently. Each of the SMs is connected to a level two L2 cache via a crossbar or other type of interconnect network . The L2 cache is connected to one or more memory interfaces . Memory interfaces implement 16 32 64 128 bit data buses or the like for high speed data transfer. In one embodiment the PPU comprises U memory interfaces U where each memory interface U is connected to a corresponding memory device U . For example PPU may be connected to up to 6 memory devices such as graphics double data rate version 5 synchronous dynamic random access memory GDDR5 SDRAM .

In one embodiment the PPU implements a multi level memory hierarchy. The memory is located off chip in SDRAM coupled to the PPU . Data from the memory may be fetched and stored in the L2 cache which is located on chip and is shared between the various SMs . In one embodiment each of the SMs also implements an L1 cache. The L1 cache is private memory that is dedicated to a particular SM . Each of the L1 caches is coupled to the shared L2 cache . Data from the L2 cache may be fetched and stored in each of the L1 caches for processing in the functional units of the SMs .

In one embodiment the PPU comprises a graphics processing unit GPU . The PPU is configured to receive commands that specify shader programs for processing graphics data. Graphics data may be defined as a set of primitives such as points lines triangles quads triangle strips and the like. Typically a primitive includes data that specifies a number of vertices for the primitive e.g. in a model space coordinate system as well as attributes associated with each vertex of the primitive. The PPU can be configured to process the graphics primitives to generate a frame buffer i.e. pixel data for each of the pixels of the display . The driver kernel implements a graphics processing pipeline such as the graphics processing pipeline defined by the OpenGL API.

An application writes model data for a scene i.e. a collection of vertices and attributes to memory. The model data defines each of the objects that may be visible on a display. The application then makes an API call to the driver kernel that requests the model data to be rendered and displayed. The driver kernel reads the model data and writes commands to the buffer to perform one or more operations to process the model data. The commands may encode different shader programs including one or more of a vertex shader hull shader geometry shader pixel shader etc. For example the TMU may configure one or more SMs to execute a vertex shader program that processes a number of vertices defined by the model data. In one embodiment the TMU may configure different SMs to execute different shader programs concurrently. For example a first subset of SMs may be configured to execute a vertex shader program while a second subset of SMs may be configured to execute a pixel shader program. The first subset of SMs processes vertex data to produce processed vertex data and writes the processed vertex data to the L2 cache and or the memory . After the processed vertex data is rasterized i.e. transformed from three dimensional data into two dimensional data in screen space to produce fragment data the second subset of SMs executes a pixel shader to produce processed fragment data which is then blended with other processed fragment data and written to the frame buffer in memory . The vertex shader program and pixel shader program may execute concurrently processing different data from the same scene in a pipelined fashion until all of the model data for the scene has been rendered to the frame buffer. Then the contents of the frame buffer are transmitted to a display controller for display on a display device.

The PPU may be included in a desktop computer a laptop computer a tablet computer a smart phone e.g. a wireless hand held device personal digital assistant PDA a digital camera a hand held electronic device and the like. In one embodiment the PPU is embodied on a single semiconductor substrate. In another embodiment the PPU is included in a system on a chip SoC along with one or more other logic units such as a reduced instruction set computer RISC CPU a memory management unit MMU a digital to analog converter DAC and the like.

In one embodiment the PPU may be included on a graphics card that includes one or more memory devices such as GDDR5 SDRAM. The graphics card may be configured to interface with a PCIe slot on a motherboard of a desktop computer that includes e.g. a northbridge chipset and a southbridge chipset. In yet another embodiment the PPU may be an integrated graphics processing unit iGPU included in the chipset i.e. Northbridge of the motherboard.

As described above the work distribution unit dispatches active grids for execution on one or more SMs of the PPU . The scheduler unit receives the grids from the work distribution unit and manages instruction scheduling for one or more thread blocks of each active grid. The scheduler unit schedules threads for execution in groups of parallel threads where each group is called a warp. In one embodiment each warp includes 32 threads. The scheduler unit may manage a plurality of different thread blocks allocating the thread blocks to warps for execution and then scheduling instructions from the plurality of different warps on the various functional units i.e. cores DPUs SFUs and LSUs during each clock cycle.

In one embodiment each scheduler unit includes one or more instruction dispatch units . Each dispatch unit is configured to transmit instructions to one or more of the functional units. In the embodiment shown in the scheduler unit includes two dispatch units that enable two different instructions from the same warp to be dispatched during each clock cycle. In alternative embodiments each scheduler unit may include a single dispatch unit or additional dispatch units .

Each SM includes a register file that provides a set of registers for the functional units of the SM . In one embodiment the register file is divided between each of the functional units such that each functional unit is allocated a dedicated portion of the register file . In another embodiment the register file is divided between the different warps being executed by the SM . The register file provides temporary storage for operands connected to the data paths of the functional units.

Each SM comprises L processing cores . In one embodiment the SM includes a large number e.g. 192 etc. of distinct processing cores . Each core is a fully pipelined single precision processing unit that includes a floating point arithmetic logic unit and an integer arithmetic logic unit. In one embodiment the floating point arithmetic logic units implement the IEEE 754 2008 standard for floating point arithmetic. Each SM also comprises M DPUs that implement double precision floating point arithmetic N SFUs that perform special functions e.g. copy rectangle pixel blending operations and the like and P LSUs that implement load and store operations between the shared memory L1 cache and the register file . In one embodiment the SM includes 64 DPUs 32 SFUs and 32 LSUs .

Each SM includes an interconnect network that connects each of the functional units to the register file and the shared memory L1 cache . In one embodiment the interconnect network is a crossbar that can be configured to connect any of the functional units to any of the registers in the register file or the memory locations in shared memory L1 cache .

In one embodiment the SM is implemented within a GPU. In such an embodiment the SM comprises J texture units . The texture units are configured to load texture maps i.e. a 2D array of texels from the memory and sample the texture maps to produce sampled texture values for use in shader programs. The texture units implement texture operations such as anti aliasing operations using mip maps i.e. texture maps of varying levels of detail . In one embodiment the SM includes 16 texture units .

The PPU described above may be configured to perform highly parallel computations much faster than conventional CPUs. Parallel computing has advantages in graphics processing data compression biometrics stream processing algorithms and the like.

In one embodiment a thread block is generated by the SM based on a pointer to a task metadata TMD data structure generated in a memory and passed to the PPU . The TMD data structure may specify information related to the one or more thread blocks to be executed by the PPU . The TMD data structure may also include a pointer to a base memory address for a program or kernel to be executed by each of the threads of the one or more thread blocks . When the SM launches the task associated with the TMD data structure the SM generates the one or more thread blocks for execution. Each thread block is executed concurrently on a number of functional units of the SM .

In one embodiment the PPU may be configured to resemble a SIMD architecture processing unit. In the SIMD architecture the PPU is configured to receive samples e.g. vertices fragments etc. that are assigned to a particular thread executing on an SM . A number of samples are received and assigned to the threads in a thread block . Then the samples are processed in parallel on multiple functional units of the SM with the same instructions being executed by each of the functional units on the different samples.

It will be appreciated that modern processors implement instruction sets that enable branching within programs executed by the processor. In one embodiment threads in a thread group may diverge based a conditional statement included in an instruction. For example an instruction in the program executed by the threads may specify a conditional branch instruction. The conditional branch instruction may be related to for example an IF statement in a high level programming language such as C or C . The conditional branch instruction may evaluate a condition or inverted condition for each of the threads in the thread block and then jump to a different location in the program based on the result of the evaluation of the condition. Because each of the threads is associated with different data some threads in the thread block may evaluate the condition as TRUE while other threads in the thread block may evaluate the condition as FALSE. In other words some threads may jump to an IF block of code to be executed when the condition is evaluated as TRUE and other threads may jump to an ELSE block of code to be executed when the condition is evaluated as FALSE. Because each of the threads in the thread block executes the same instruction across different data the processor is configured to disable certain threads from executing a particular instruction while other threads in the thread block are executing the instruction.

Because threads diverge assembly programmers or compilers may utilize special instructions that enable the threads of a program to be synchronized at certain points ensuring that all threads in the thread block have reached a particular point in the program before executing the next instruction. Modern processors may implement a mechanism for tracking thread divergence and synchronizing a plurality of divergent threads at specific points within the program.

As the threads of the thread block diverge tokens are pushed onto the CRS stack data structure that tracks the divergence of threads. These tokens may then be popped from the CRS stack data structure at a later point in the program to activate certain threads and jump to a specific location in the program based on the information contained in the tokens. This mechanism enables multiple nested divergences to be tracked in a reliable manner. The actual number of divergences and nested levels is limited by the size of the CRS stack data structure . The inclusion of instructions specifying divergence control operations that control the creation and unwinding of the tokens may be managed by a compiler implemented in a device driver for the processor . The driver may also be configured to manage the CRS stack data structure . In other words the driver may insert commands before or after certain instructions in the program that cause tokens to be pushed onto or popped from the CRS stack data structure and processed by the processor .

The processor is configured to maintain an active mask and a program counter for each thread block being executed by the processor . The active mask is a string of bits that indicates which threads in the thread group are currently active i.e. which threads in the thread block execute the instruction specified by the address pointed to by the program counter . Each bit in the active mask corresponds to a particular thread in the thread block. A bit in the active mask may be set to indicate that a corresponding thread is active. Consequently when all bits of the active mask are set the thread block is fully synchronized. The program counter indicates the address of the instruction currently being executed by the active threads in the thread block.

As the driver compiles the program the compiler may encounter one or more instructions that specify branch operations. When a branch operation is encountered the compiler inserts instructions in the program that cause tokens to be pushed or popped from the CRS stack data structure . Each token includes state information related to various threads in the thread block. The specific state information included in a token may depend on the type of branch operation encountered in the program.

For example if the compiler encounters an IF statement the compiler may include a set synchronization instruction before a conditional branch operation. The set synchronization instruction causes the processor to push a synchronization token onto the CRS stack data structure . The synchronization token includes a mask that indicates which threads were active prior to the conditional branch operation and an address for an instruction located at a convergence point after the conditional branch instruction. The synchronization token may also include a field that indicates the type of token such as 4 bits that indicate that the token is a synchronization token.

As the conditional branch operation is executed the processor may be configured to push a divergence token onto the CRS stack data structure . A divergence token includes a mask that indicates which threads in the active mask were disabled based on the evaluation of the conditional branch operation and an address to a location in the program associated with the path to be executed by the disabled threads. The processor disables a portion of the active threads based on the evaluation of the condition and executes the instructions on one of the paths of the branch for the remaining active threads. For example if the conditional branch operation is associated with an IF . . . ELSE statement then a first portion of the threads may evaluate the condition as true and execute the IF block of code and a second portion of the threads may evaluate the condition as false and execute the ELSE block of code. If the processor disables all active threads that evaluate the condition as FALSE then the IF block of code is selected as the taken path and the ELSE block of code is selected as the not taken path. Conversely if the processor disables all active threads that evaluate the condition as TRUE then the ELSE block of code is selected as the taken path and the IF block of code is selected as the not taken path. The processor executes the taken path associated with the conditional branch operation for the remaining active threads.

At the end of the taken path the compiler may include an EXIT instruction or a BREAK instruction that causes the processor to disable all of the active threads based on the completion of the taken path block of code. When all active threads have been disabled the processor is configured to pop one or more tokens from the top of the CRS stack data structure . In this case the top token would be a divergence token. The processor activates the threads in the mask of the divergence token which includes all threads associated with the not taken path of the conditional branch operation and updates the program counter with the address included in the divergence token which is the first instruction in the not taken path block of code.

The compiler may also include an instruction for a synchronization operation in the program before the convergence point after the conditional branch operation. The synchronization operation causes the processor to disable all active threads in order to synchronize the active threads with one or more inactive threads in the thread block. When all of the active threads have been disabled the processor pops one or more tokens from the CRS stack data structure . In this case the token is a synchronization token. The processor activates all threads indicated by the mask in the synchronization token and updates the program counter with the address included in the synchronization token.

The example described above is merely one example of the types of operations that may be effectuated using the CRS stack data structure . For example synchronization may be implemented around loops such as a WHILE statement a DO statement or a FOR statement in a high level programming language. Synchronization may also be implemented around function calls where different threads may return from the function at different points e.g. due to a break statement or loops within the function . It will be appreciated that compilers may not implement synchronization points based on high level programming instructions such as a WHILE statement for generating a loop structure. Instead compilers typically construct an abstracted control flow graph and perform an analysis on the control flow graph that reveals potentially multiple nested structures in the program where a divergence may occur. Notably a program using IF statements and GOTO statements may be synchronized just as well as a program using a WHILE statement.

A synchronization operation may be inserted into the program using a set synchronization instruction SSY and a corresponding synchronization instruction SYNC . The set synchronization instruction causes the processor to push a synchronization token onto the CRS stack data structure . The synchronization instruction causes the processor to disable all active threads and pop a token from the CRS stack data structure . The set synchronization instruction is inserted before a branch in the program where threads could diverge and the corresponding synchronization instruction is inserted at a later point in the program where the divergent threads are converged e.g. after the end of an IF statement or a loop .

It will be appreciated that a single unique semantic may be used for various instructions that cause the processor to disable the active threads and pop tokens from the CRS stack data structure . The semantic is parameterized on a reason for why the processor disables the active threads. All reasons for disabling the active threads may be considered equal can be reduced to a value e.g. 0 through 15 stored in 4 bits and have been associated with human friendly handles i.e. SSY BRK CONT LONGJMP RET etc. to aid in programming comprehension by a programmer. Compilers are generally configured to analyze the control flow graph for a program and select the correct reason for disabling the active threads in order to help the programmer during debugging. As used herein the semantic may be described throughout using the various human friendly handles for illustrative purposes.

Specific reasons for disabling a thread may include a break operation associated with a PBRK pre break instruction and a corresponding BRK break instruction a continuation operation associated with a PCONT pre continuation instruction and a corresponding CONT continuation instruction a branch operation associated with a BRA branch instruction or an indirect BRX indirect branch instruction a jump operation associated with a JMP jump instruction or an indirect JMX indirect jump instruction and an exit operation associated with a PEXIT pre exit instruction and a corresponding EXIT exit instruction .

Although the preceding operations are disclosed herein other systems may implement further operations to track and manage thread divergence in SIMD processors. Such other operations are contemplated as being within the scope of the present disclosure and may be included in the instruction set of the processor .

It will be appreciated that some common issues may arise in vector programs due to the divergence of various threads that may cause the processor to deadlock when certain programming techniques are employed. For example mutual exclusion mutex mechanisms for critical sections of code may be used to ensure that two threads do not attempt to use the same resource e.g. shared memory at the same time. Mutual exclusion on a multi processor system may utilize an atomic test and set instruction to test and set a value in a shared memory location. The atomic nature of the instruction ensures that only one thread can set the value at a time. Therefore if every thread in a thread block attempts to set a value at a memory location to a value associated with that particular thread the shared memory location will be updated by only one thread which indicates that that thread is allowed to use the shared resource at the exclusion of other threads in the thread block. Other mutual exclusion mechanisms such as compare and swap may also be implemented by a programmer and the instruction set of the processor .

An issue can arise when a thread obtains a mutex and is then deactivated due to the divergence of the threads based on a conditional branch operation. For example a particular thread may obtain a mutex prior to branching based on an IF statement. If the thread that acquired the mutex is then deactivated due to the thread evaluating the conditional statement that leads to the thread executing the not taken path for example then the other threads that are activated as part of the taken path may deadlock while waiting for the mutex to be released by a deactivated thread. Because the threads in the taken path are stalled the not taken path is never executed and the program deadlocks. The issue is that not all threads are guaranteed forward progress because the CRS stack data structure implementation serializes the execution of divergent threads in the SIMD architecture. Therefore a solution to this issue may guarantee that all threads in the thread block regardless of the particular path selected are guaranteed forward progress.

In one embodiment the queue data structure is stored in the memory associated with the PPU . Each thread block being executed by the PPU is associated with a corresponding queue data structure . In addition unless otherwise noted below the functionality of the prior art processor may be incorporated in whole or in part into the instruction set of the PPU where the instructions are configured to push tokens onto the queue data structure instead of the CRS stack data structure .

In one embodiment as shown in the queue data structure is an allocated portion of the memory . The device driver for the PPU may allocate one queue data structure for each thread block being executed by the PPU . The queue data structure is located at a base address in the memory . The queue data structure has a limited size defined as the difference between a maximum address and the base address . The queue data structure stores zero or more tokens in entries of the queue data structure . In one embodiment each token or entry is 64 bits wide including a 32 bit mask a 4 bit identifier and a 22 bit address. In other embodiments the additional six bits may be utilized for a longer address or a longer identifier field. The additional 6 bits may also be used to store additional state information for the thread block such as indicating which reasons threads in the thread block have been deactivated the reasons corresponding to the types of tokens pushed into the queue data structure such as synchronization divergence break etc. . In alternative embodiments tokens can be sized to store state information necessary to implement additional functionality associated with executing divergent threads. For example tokens may be 128 bits wide to store a 32 bit address a 64 bit mask and an 8 bit identifier along with any additional state information as required.

Tokens may be added to entries pointed to by the head pointer of the queue data structure . For example when a queue data structure is allocated by the device driver for the PPU the head pointer may be set to a mid point of the allocated space e.g. an offset 0x0080 from the base address in the case of a 256 entry queue data structure . As tokens are pushed onto the queue data structure the head pointer is incremented to point to the next entry in the queue data structure . As shown in a synchronization token a divergence token a call token a second synchronization token and a second divergence token have been pushed onto the queue data structure . The example arrangement of tokens as shown in may be generated by a program with an IF statement that includes a function call in the taken path block of code. The function may also include another nested IF statement. As the program is executed the PPU may push the tokens onto the queue data structure at a location pointed to by the head pointer . As active threads are disabled by the processor based on instructions inserted into the program by the compiler the tokens may be popped from the front of the queue data structure and unwound.

As discussed above pushing tokens onto the front of the queue data structure and popping tokens from the front of the queue data structure does not guarantee forward progress among the various threads of a thread block . In order to guarantee forward progress among the various groups of threads a special YIELD instruction is created and added to the instruction set of the PPU . The YIELD instruction causes the PPU to disable all active threads in a thread block . The YIELD instruction also causes the PPU to push a yield token onto the back of the queue data structure . The yield token includes a mask that is set equal to the current active mask when the YIELD instruction is executed and an address that corresponds to the instruction that immediately follows the yield instruction. Once all active threads have been disabled by the PPU the PPU pops a token from the front of the queue data structure . If the type of token matches a reason for disabling at least one thread in the thread block then the threads indicated by the mask in the token are enabled and the program counter is updated to match the address in the token . Otherwise the token is discarded and the next token is popped from the front of the queue data structure . A token is only discarded if none of the threads have been disabled according to a reason included in the token .

In one embodiment the YIELD instruction is added to any non deterministic loop within a program. In other words programs without loops will eventually end and upon exit each divergent branch would eventually be executed. Even some programs with loops are deterministic in that the loop is configured to run for a determined number of iterations e.g. a FOR loop such as for int i 0 i

In some embodiments the YIELD instruction causes the PPU to potentially disable the active threads . For example in one embodiment the YIELD instruction may be configured to disable the active threads within a loop every N iterations of the loop. In other words the YIELD instruction may be associated with a counter for each set of active threads that counts how many times the YIELD instruction has been executed for the set of active threads . The counter is initialized to zero and every time the YIELD instruction is executed for a particular set of active threads the counter is incremented by one. The PPU may then be configured to disable the active threads based on the execution of the YIELD instruction only when the counter is greater than or equal to N. If the PPU disables the active threads based on the YIELD instruction then the counter is reset to zero. In another embodiment the YIELD instruction may be configured to disable the active threads within a loop based on a random number generator. The YIELD instruction will disable the threads only if a random number generated by a random number generator is above or below a threshold value. In other words each time the YIELD instruction is executed by the PPU there is a chance that the threads will be disabled. The probability that the threads will be disabled is determined based on the threshold vale. Even a small chance that the threads will be disabled ensures that all threads are given a guarantee of forward progress as long as the random number generator is truly random.

The yield instruction breaks the serialized nature of storing tokens in the CRS stack data structure according to a LIFO manner of operation. In other words the yield tokens are pushed onto the back of the queue data structure and operate in a FIFO manner of operation in conjunction with the FIFO manner of operation for other divergence control operations. When a yield instruction is encountered in a program the active threads are disabled. The PPU looks at the tokens in the queue data structure for a match for the current yield instruction. A match is found when a token in the queue data structure is a yield token and has the same address as the address included in the yield instruction. If the PPU finds a match for the yield instruction then the PPU merges the current active mask with the mask in the token by performing a logical OR operation with the current active mask and the mask in the token. However if the PPU does not find a match for the yield instruction then the PPU pushes a yield token onto the queue data structure . Finally the PPU pops a token from the queue data structure and activates the threads specified by the token and updates the program counter with the address in the token. It will be appreciated that the purpose of the YIELD instruction is to enable threads to effectively opt out of the various synchronization operations included in the program. Threads that yield to other threads in the program will never take part in any pending synchronization operations because the tokens for those operations will be discarded. Instead the yield token merging process ensures that if threads yield at the same address then the threads can be opportunistically converged. Software may utilize the YIELD instruction to create a convergence barrier in the program which the hardware could not implement as configured.

In order to handle the case where a divergence control operation is encountered in the program that does not have a corresponding token in the queue data structure the instruction set of the PPU is changed such that the SYNC BRK and CONT instructions include an immediate address to jump to when a corresponding token is not found in the queue data structure . In other words if a matching token is not found then threads execute a simple branch to the address specified in the divergence control operation and tokens are popped from the queue data structure until a yield token is at the front of the queue data structure . This allows for synchronization points to become optional while the branch is not optional.

It will be appreciated that implementing a double ended queue data structure allows the queue data structure to fill in both directions. At some point in a program pushing tokens onto the queue data structure may cause the head pointer or the tail pointer to exceed one of the upper or lower bounds of the allocated space in the memory. In this case an exception may be thrown that causes the driver or other software processes executed by the PPU to adjust the entries in the queue data structure such that the entries in the queue data structure are approximately centered within the allocated space. In other words the driver may copy the tokens from one entry of the queue data structure into another entry of the queue data structure . The effect should be to approximately center the total number of entries within the allocated space of the queue data structure .

It will be appreciated that the YIELD instruction and the queue data structure mechanism described above is not limited to only the PPU . Other types of SIMD processors may also implement the YIELD instruction and a queue data structure in a memory associated with the SIMD processor. Such other implementations are contemplated as being within the scope of the present disclosure.

The mask is stored in bits 4 35 of the token . The address is stored in bits 36 57 of the token . The mask in conjunction with the address may be used to enable certain threads in a thread block and update the program counter to execute a particular section of code with a particular set of threads .

In one embodiment at least a portion of the plurality of reserved bits may be used to track the types of tokens in the queue data structure below the new token . For example four bits e.g. bits 58 61 of the token may be used to track the types of tokens in the queue data structure . Each bit of the four bits indicates whether a token of a particular type is currently included in an entry of the queue data structure below the token . A first bit e.g. bit 58 may indicate whether a synchronization token is included in the queue data structure below the token . A second bit e.g. bit 59 may indicate whether a divergence token is included in the queue data structure below the token . A third bit e.g. bit 60 may indicate whether a call token is included in the queue data structure below the token . A fourth bit e.g. bit 61 may indicate whether a yield token is included in the queue data structure below the token and so forth. When a token is pushed to the front of the queue data structure the bits may be logically ORed with the bits in the adjacent token and then a bit corresponding to the type of token at the top of the queue data structure is set if the bit is not set already. Thus the PPU may simply inspect the bits of the token at the front of the queue data structure to determine whether the queue data structure includes a token of a particular type below the token . Without these bits the PPU would potentially have to inspect each and every token included in the queue data structure to determine whether a token of a particular type was included in the queue data structure or not.

Returning to step if the divergence control operation is not one of a set synchronization operation a pre break operation or a pre continuation operation then the method proceeds to step where the PPU determines whether the divergence control operation is one of a branch operation an indirect branch operation a jump operation or an indirect jump operation. If the divergence control operation is one of a branch operation an indirect branch operation a jump operation or an indirect jump operation then the method proceeds to step where the PPU determines whether the threads of the thread block diverge. Each of the branch operation the indirect branch operation the jump operation or the indirect jump operation may be conditioned on the evaluation of a predicate for each of the active threads. Thus some threads may take the branch while other threads do not. If the threads diverge then at step the PPU pushes a token on the front of the queue data structure and the method proceeds to step . Returning to step if the threads do not diverge then the method skips straight to step where the PPU sets the active mask for the thread block . If the threads do not diverge then the active mask remains the same as the active mask prior to executing the divergence control operation. However if the threads do diverge then the active mask is changed to reflect the threads that are enabled in one path of the divergent operation. At step the PPU sets the program counter based on an address specified by the divergent control operation. After step the PPU continues execution of the next instruction specified by the program counter.

Returning to step if the divergence control operation is not one of a branch operation an indirect branch operation a jump operation or an indirect jump operation then the method proceeds to step where the PPU determines whether the divergence control operation is a yield operation. If the divergence control operation is a yield operation then the method proceeds to step where the PPU pushes a yield token on the back of the queue data structure before the method proceeds to step . Returning now to step if the divergence control operation is not a yield operation then the divergence control operation is a synchronization operation a break operation or a continuation operation and the method proceeds directly to step . At step the PPU disables all threads in the thread block .

At step the PPU determines whether there are any active threads . If there are no active threads then the method proceeds to step where the PPU determines whether there are any tokens in the queue data structure . If there are no tokens in the queue data structure then the PPU halts execution as the program is complete. However if there are tokens in the queue data structure then the method proceeds to step where the PPU pops a token from the front of the queue data structure . At step the PPU determines whether the token is associated with any threads in the thread block . The token is associated with at least one thread in the thread block when the thread is disabled for a reason that matches the type of token and the thread s corresponding bit is set in the token s mask. If there are no threads in the thread block that are associated with the token then the method returns to step . However if there are threads in the thread block that are associated with the token then the method proceeds to step where the PPU activates at least one thread in the thread block . The activated threads may be specified by the mask in the token .

Returning to step if there are active threads then the method proceeds to step and step where the active mask and program counter are set based on the mask and address included in the token popped from the front of the queue data structure . After step or step the PPU continues executing the next instruction in the program.

The system also includes input devices a graphics processor and a display i.e. a conventional CRT cathode ray tube LCD liquid crystal display LED light emitting diode plasma display or the like. User input may be received from the input devices e.g. keyboard mouse touchpad microphone and the like. In one embodiment the graphics processor may include a plurality of shader modules a rasterization module etc. Each of the foregoing modules may even be situated on a single semiconductor platform to form a graphics processing unit GPU .

In the present description a single semiconductor platform may refer to a sole unitary semiconductor based integrated circuit or chip. It should be noted that the term single semiconductor platform may also refer to multi chip modules with increased connectivity which simulate on chip operation and make substantial improvements over utilizing a conventional central processing unit CPU and bus implementation. Of course the various modules may also be situated separately or in various combinations of semiconductor platforms per the desires of the user.

The system may also include a secondary storage . The secondary storage includes for example a hard disk drive and or a removable storage drive representing a floppy disk drive a magnetic tape drive a compact disk drive digital versatile disk DVD drive recording device universal serial bus USB flash memory. The removable storage drive reads from and or writes to a removable storage unit in a well known manner.

Computer programs or computer control logic algorithms may be stored in the main memory and or the secondary storage . Such computer programs when executed enable the system to perform various functions. The memory the storage and or any other storage are possible examples of computer readable media.

In one embodiment the architecture and or functionality of the various previous figures may be implemented in the context of the central processor the graphics processor an integrated circuit not shown that is capable of at least a portion of the capabilities of both the central processor and the graphics processor a chipset i.e. a group of integrated circuits designed to work and sold as a unit for performing related functions etc. and or any other integrated circuit for that matter.

Still yet the architecture and or functionality of the various previous figures may be implemented in the context of a general computer system a circuit board system a game console system dedicated for entertainment purposes an application specific system and or any other desired system. For example the system may take the form of a desktop computer laptop computer server workstation game consoles embedded system and or any other type of logic. Still yet the system may take the form of various other devices including but not limited to a personal digital assistant PDA device a mobile phone device a television etc.

Further while not shown the system may be coupled to a network e.g. a telecommunications network local area network LAN wireless network wide area network WAN such as the Internet peer to peer network cable network or the like for communication purposes.

While various embodiments have been described above it should be understood that they have been presented by way of example only and not limitation. Thus the breadth and scope of a preferred embodiment should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the following claims and their equivalents.

