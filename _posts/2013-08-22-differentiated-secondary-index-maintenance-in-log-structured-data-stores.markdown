---

title: Differentiated secondary index maintenance in log structured data stores
abstract: There are provided a system and a computer program product for operating multi-node data stores. The system stores a data table in a first computing node and stores an index table in a second computing node. The index table provides keys used for accessing data in the first computing node and other multi-node data stores. The system performs operations that update or read the data table accessed from the first computing node and the index table accessed from the second computing node. The operations optimize between latency in updating or reading the data table and the index table and data consistency maintained between data entries in the data table and data entries pointed by indices in the index table.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09218385&OS=09218385&RS=09218385
owner: International Business Machines Corporation
number: 09218385
owner_city: Armonk
owner_country: US
publication_date: 20130822
---
This application is a continuation of U.S. patent application Ser. No. 13 837 812 filed Mar. 15 2013 the entire content and disclosure of which is incorporated herein by reference.

A data store refers to a data repository that uses a database e.g. IBM DB2 etc. to manage data stored in the data repository. A data store may include a plurality of computing nodes each of which uses a database management system to management data stored in a corresponding computing node. In recent years scale out data stores referred as NoSQL data stores are gaining attraction to support high traffic Internet applications e.g. yelp.com. A NoSQL data store is a class of database management systems that do not use a relational database model. A category of NoSQL data stores includes Log Structured Merge LSM Tree which is optimized for writing of data and MapReduce for scanning of data but is not optimized for point queries e.g. a query directed to a specific datum. Examples of NoSQL data stores include but are not limited to Dynamo BigTable PNUTS Cassandra HBase etc. These data stores usually provide limited APIs Application Programming Interfaces e.g. Create Read Update Delete operations compared to relational databases and focus on scalability and elasticity on commodity hardware. NoSQL data stores are particularly attractive for applications that perform relatively simple operations while needing low latency guarantees as data stored in NoSQL data stores scale to large sizes e.g. 1 Petabyte. While these NoSQL data stores are a great fit for high scale low complexity applications as these applications get more complex a lack of basic database functionality e.g. secondary indexes has become a stumbling factor.

There are provided a system and a computer program product for operating multi node data stores. The system stores at least one data table in at least one first computing node and stores at least one index table in at least one second computing node. The index table provides keys used for accessing data in said first computing node and other multi node data stores. The system performs operations that update or read the at least one data table accessed from the at least one first computing node and the at least one index table accessed from the at least one second computing node. The operations optimize between latency in updating or reading the at least one data table and the at least one index table and data consistency maintained between data entries in the at least one data table and data entries pointed by indices in the at least one index table.

In order to accelerate point queries in NoSQL data stores there is provided a secondary global index managed by a plurality of RegionServers. A global index effectively avoids broadcasting queries to every data partition. A server managed index is more robust and exposes less detail to clients. This secondary index can have multiple consistency levels that includes 1 fully synchronizing data consistency between a data table and a corresponding index table in the way that an index entry in an index table is immediately updated upon inserting a new data entry to a corresponding data table 2 an index entry in an index table is not updated upon an insertion of a corresponding new data entry into a corresponding data table that index entry in an index table is updated upon receiving a query directed to that a corresponding old index entry 3 an update to an index entry in an index table asynchronies with an insertion of a corresponding new data entry to a data table 4 during a session i.e. interactive information exchange including one or more messages in each direction data consistency between a data table and a corresponding index table is maintained in the way that an index entry in an index table is immediately updated upon a new data entry to a corresponding data table. By having a secondary index with multiple consistency levels a balance between accuracy of data and latency in retrieving data can be managed in different levels. Different consistency levels enable manageable tradeoffs between the accuracy and the latency which provide differentiated performance expectation for workloads. According to different workloads that balance can be changed e.g. sacrificing accuracy of data in order to speed up processing of data entry requests.

Astonishing growth and diversity in data have profoundly affected a way that data have been stored and served. Monolithic relational databases that are good at processing structured data and excel in ACID atomicity consistency isolation durability cannot handle high traffic Internet applications in an elastic manner. This is because relational databases put restrictions on data normalization and consistency and also the relational databases are not able to perform data sharing in an automatic way so that the relational databases are difficult to scale out. To address challenges in flexible schema and elasticity data stores dealing with columns key value pairs documents and graphs have emerged and collectively identified as NoSQL data stores. A NoSQL data store e.g. HBase that supports a convergence of transactional and analytics workloads is able to offer balanced read and write performance.

Consider a web application that manages social reviews e.g. yelp.com . Such an application tracks reviews posted by users about business e.g. restaurants bars clubs etc. A common query is to list all reviews by a certain criteria e.g. of a particular restaurant by a particular user. A schema in shows exemplary three tables Users table Products table and Reviews table . In any scale out data store i.e. a data store that can increase its data storage capacity as a demand for the data storage increases the tables can be partitioned across a cluster of computing nodes. A given table can be partitioned by using one attribute. While a query can be served using a full table scan e.g. scanning of tables this full table scan would incur an unacceptable latency to an interactive web application e.g. yelp.com.

In a distributed data store data are partitioned into regions e.g. countries or counties or states etc. From the perspective of indexes scope there are two types of indexes a local index and a global index. A local secondary index is an index on data in a given region and co located with a data region. In contrast a global index indexes all the data in the table and is potentially itself partitioned across all nodes. An advantage of a global index is in handling of highly selective queries i.e. queries whose results reside in only a small number of regions e.g. one or two regions. This is because a global index has an overall knowledge of data locations and sends queries only to those regions that actually contain required data. Drawback of the global index is that an update of the global index incurs remote procedure calls RPC and results in additional latency when data and index are not collocated. On the other hand a local index has an advantage of a fast index update e.g. within a couple of milliseconds because of its collocation with a data region. A drawback of the local index is that every query has to be broadcast to each region thus the query is costly especially for highly selective queries.

Consider a query for example Get all reviews by user u on the Reviews table shown in that includes a secondary index on the User table . With local indexes the query would need to be broadcasted to all the nodes in a cluster and every node would send results of probing the local index to a coordinator who would combine the results before sending them back to a client node which sent the query. With global indexes upon receiving the query a data store will be able to locate exactly a node or a small set of nodes that includes relevant values and return a pointer to an original row that includes data that client looks for. For an interactive application global indexes better support low latency queries at large volumes e.g. 1 million queries per day. If queries are uniformly distributed over a key space i.e. all the possible keys used in a data table that records data with global indexes a total throughput is limited by a throughput of a single node since every node has to participate in every query. For global indexes in the best case a total throughput of a data store can be the sum of throughputs of all the nodes in the data store. The worst case for the global indexes is when all the queries are directed to a single key whose index has to be served by a single node in which is the throughput of that single node determines the total throughput of a data store.

A central data structure used in NoSQL data stores for example BigTable is a Log Structured Merge LSM Tree. In contrast to B Trees i.e. a binary tree which can include a plurality of leaf nodes LSM Trees are well suited to high update ratio workloads e.g. frequent review updates in yelp.com because they only incur sequential I O for writes and incur random I O only for reads. 

However a design and implementation of secondary indexes in a distributed LSM is not trivial for the following reasons. Firstly workloads of Internet applications are becoming more write intensive with a proliferation of click streams GPS locations sensors events etc. LSM is designed to address this challenge by optimizing a latency of writes. In the meanwhile ad hoc queries are still important to make use of data stored in LSM. While indexes improve random read performance the indexes have inevitably negative impact on write performance. This is because that a benefit of using LSM to speed up write will be largely infringed if adding indexes brings overhead to writes e.g. a write operation to a data table may require more than one index update.

An issue i.e. index s overhead to data write applies to RDBMS Relational Database Management System and is not unique for LSM. However the impact on a write i.e. inserting a new data entry into a data table can be more serious in LSM stores given a nature of its underlying data structure in LSM there is no in place update and consequentially insert and update are indistinguishable. This characteristic results in that an index update needs to incur a data read operation into a data table in order to remove a stale index entry. That is to say a data read operation is added into a path of a data write operation. For example in HBase i.e. an implementation of LSM a data write operation takes a couple of milliseconds but a data read operation can take over ten milliseconds. NoSQL data stores including those LSM based data stores are partitioned and distributed to handle huge volume of data e.g. 10 petabytes. Presumably any index used in a NoSQL data store is to be large too.

LSM Tree prevails in workloads with a high rate of data record insertions and deletions. LSM Tree defers and batches data changes by cascading them from a memory to storage device e.g. a disk etc. in a batch manner. LSM Tree includes an in memory store e.g. in a form of a tree or map and multiple immutable data storages e.g. usually in a form of a B tree or its variants. For example LSM Tree in includes an in memory store e.g. mem store in and two disk stores C and C. Operations on LSM Tree are illustrated briefly in . In writing into LSM Tree is an insertion into a mem store . In practice a data write operation usually also involves an appending to a commit log for a durability purpose e.g. in order to recover from a mem store failure. Therefore a data write operation that includes a memory store operation and an optional sequential I O is fast e.g. less than a millisecond. When a storage capacity of a mem store reaches a certain volume its content is flushed i.e. pushed to a storage device e.g. a disk C . For example the mem store in is flushed i.e. pushsed into a new storage device e.g. disk C shown in . After the push the mem store becomes empty and denoted as a mem store in . With this procedure every write is virtually an appending an update or deletion to an existing data is achieved by adding a new version of the data or setting an invalidation bit in case of deletion into the mem store. By this means LSM Tree has no in place update and as a result is able to keep multiple versions of data. To retrieve either one or more versions of data the mem store and all disk stores need to be scanned shown in . Therefore a data read operation may include multiple random I O and is usually slow e.g. more than 10 millisecond. To alleviate this slow data read operation and consolidate multi versions of data into a single place multiple disk stores are periodically flushed merged and compacted. As an example storage devices C C and C are compacted into a storage device C as shown in .

Apache HBase is a LSM Tree data store. In HBase data is organized in tables. A table includes rows that are identified by a primary row key. Each row may include an arbitrary number of named columns with corresponding values and timestamps. Columns may be organized into column families and columns in a single family find them co located on a storage device e.g. a disk. Each column family may be partitioned and stored in multiple computing nodes and in each computing node each column may stored as a LSM tree.

put table key colname colvalue Insert a column value colvalue into a column corresponding to colname into a row corresponding to key in a table corresponding table. 

get table key colname Read a column value and its timestamp from an entry at a column corresponding to colname and at a row corresponding key in a table corresponding table. 

get table key colname minTime maxTime Read a column value and its timestamp from an entry at a column corresponding to colname and at a row corresponding key subject to the timestamp being between minTime i.e. a minimum time and maxTime i.e. a maximum time from a table corresponding table. 

delete table key colname Delete a column corresponding to colname from a row corresponding key in a table corresponding table. 

All these API methods operate on a table e.g. an index table or a data table. Although not shown there are also multi column versions of these methods. For example the multi column version of put operation allows many columns in a single row to be updated with a single put function call.

As shown in in each HBase table a key space is partitioned into Regions and each region is assigned to a RegionServer e.g. a server device which has its own DBMS Database Management System and manages data in the HBtable of its corresponding partitioned key space. A RegionServer is responsible for serving puts functions and gets functions for keys that fall in a corresponding Region. A RegionServer may serve more than one Region. A client node may store e.g. in a cache memory device not shown a copy of a key space partition map and can route a request for a given key to an appropriate RegionServer.

When a put i.e. data insertion request arrives at a region server the Region Server first assigns a timestamp to the put request and then writes this timestamp to a Write Ahead Log WAL not shown . This timestamp is local to a RegionServer and is a monotonically non decreasing 64 bit integer. A WAL is a file on HDFS and can guarantee durability i.e. a transaction corresponding to the put request is saved permanently in a backup storage device for example HDFS Hadoop Distributed File System . HDFS refers to a data storage system that stores huge data e.g. 40 petabytes and provides the data at high bandwidth 1 gigabyte sec etc. to applications. Once the request has been logged to the WAL the RegionServer applies an update to the MemStore Memtable shown in and also mem store shown in . Periodically contents of the Memtable are sorted and written to an on disk LSM component called HTable not shown . HTables are also written to HDFS and are therefore replicated and guaranteed to be durable. ZooKeeper manages the cluster of RegionServers.

In a data store availability availability of data and consistency consistency of data cannot be achieved at the same time if partition tolerance i.e. a tolerance to network partition is desired. Partition tolerance refers to that a system continuously functions though the system is partitioned across a plurality of computing nodes. A system which is tolerant to network partition sacrifices either availability or consistency. Latency of a new index entry insertion depends on how consistent an index to needs to be e.g. sync full or asyn simple. Inconsistency of an index can be tolerable for a certain workload e.g. data traffic to and from a website or URL Uniform Resource Locator yelp.com. A read operation of a data entry is more expensive than a write operation of a data entry in LSM data store because a multiple versions of a data entry may need to be found upon receiving a data read request of that data entry. An LSM data store does not currently distinguish an insertion of a data entry and an update of a data entry. A client node is not aware that there may exist multiple versions of a data entry until the client node receives a response to read a data entry from a data table.

An index can be considered as a replication of data it indexes. An index update is not always desirable in an Internet scale write intensive workload because of the following reasons 

1. All or nothing semantics restrict an availability of a system. In a lot of cases even when an index cannot be synchronized timely users still want a corresponding data entry to be written to a data table.

2. A protocol e.g. a known two phase commit and Paxos i.e. a set of protocols can provide a transaction guarantee to index updates however in a distributed environment this is expensive and results in delays.

3. A lot of workloads have a feature of write intensive and read from time to time for which there is no need to update index immediately upon an update of a corresponding data entry. Also sometimes it is acceptable that the index as a replication does not react to an update of a corresponding data.

This disclosure describes a novel approach to adding a secondary global index on a scale out data store based on Log Structured Merge LSM Trees. This disclosure proposes multiple different index update schemes sync full sync insert async simple and async session. These schemes are described in detail below herewith. These schemes offer different trade offs between performance e.g. a processing time to process a write or read request of a data entry etc. and data consistency e.g. data consistency between a data table and a corresponding index table whose indices point to data entries recorded in the data table by partially relaxing data consistency and data isolation while retaining atomicity and durability of individual operations.

 1 sync full implementing functions that fully synchronize data consistency between a data table and a corresponding index table in the way that an index entry in an index table is immediately updated upon a new data entry to a corresponding data table 2 sync insert implementing functions to prevent an old i.e. stale index entry in an index table from being deleted upon an insertion of a corresponding new data entry into a corresponding data table that old index entry in an index table is deleted upon receiving a query directed to the old index entry 3 async simple implementing functions that asynchronies an update to an index entry in an index table with an insertion of a corresponding new data entry to a data table 4 async session during a session i.e. interactive information exchange including one or more messages in each direction functions are implemented to maintain the data consistency between a data table and a corresponding index table in the way that an index entry is kept updated in a client session cache upon a new data entry to a corresponding data table.

Causal consistent once a new data entry is successfully inserted to a data table both data entry and its associated index entry are persisted in the data store i.e. the associated index entry is immediately inserted to an index table upon the insertion of the new data entry to the data table . This is the strongest consistency level in these different index update schemes and achieved by the sync full scheme to be described in detail below.

Causal consistent with repairing upon receiving a query i.e. a read request directed to a new data entry or its associated index entry an index is causal consistent when a client issues a read request to both an index table and a data table. This is achieved by the sync insert scheme to be described in detail below.

Eventually consistent once a new data entry is successfully inserted to a data table its associated index entry is not updated right away but will eventually be e.g. by running a background processing for updating the associated index entry. This is the weakest consistency level in these different index update schemes and achieved by the async simple scheme described in detail below.

Session consistent During a session the data consistency between a data table and a corresponding index table is maintained with a client session cache. This is achieved by the async session scheme described in detail below.

In a LSM data store update write insert and put are used interchangeably. In a LSM data store a deletion is achieved by indicating a corresponding data or index entry as invalid e.g. by using setting an invalid bit. In a LSM data store data is recorded in a data table in a form of a tuple where k represents a key v represents a new value of the key and t represents a timestamp which indicates a moment that the new value of the key is added to the data table. t can be omitted when it does not include a particular meaning in the context. In one embodiment an index table includes only keys.

In one embodiment the at least one index table includes at least one secondary index which points to a data entry recorded in the data table. In one embodiment the operations performed on the data table and the index table are initiated by at least one client node . An example of a client node is shown in described in detail below. A client node can initiate an index utility function call that includes but is not limited to creating of an index destroying of an index loading of a group of indices to the index table erasing all indices in the index table . The client node can further implement an API Application Programming Interface to obtain an index from the index table e.g. by using a get function call. The client node can further include a cache memory device that helps to implement async session scheme by enforcing a consistency during a session. The client node can further include another API that enables the client node to send a query to the data table and the index table . The client node may also manage the index table e.g. by using the index utility function call .

One or more coprocessors provide a means to extend HBase s function without intruding into its core code. Three coprocessors AsyncObserver SyncFullObserver and SyncInsertObserver are deployed in each index table. They implement functions that listen to and intercept each data entry made to the data table and act based on the schemes they implement. The SyncFullObserver implements sync full scheme. The SyncInsertObserver implements sync insert scheme. The AsyncObserver implements async simple and async session. Responsive to an index update scheme being selected e.g. by a user a corresponding observer e.g. AsyncObserver etc. is active.

Under sync full scheme the client node sends to a coprocessor i.e. a processor supplementing a primary processor called SyncFullObserver a request to insert a new data entry. Then the SyncFullObserver inserts a corresponding new data entry to the data table . The SyncFullObserver also synchronously inserts a new index entry into the index table upon inserting the corresponding new data entry to the data table . The SyncFullObserver synchronously deletes a corresponding stale index entry in the index table upon the insertion of the corresponding new index entry to the index table . Once the synchronous inserting and deleting are successfully performed in the index table and the data table the data consistency is maintained between the index table and the data table .

In one embodiment a latency of sync full scheme can be calculated according to equation 1 by sync full 1 where L sync full represents the latency of sync full scheme L P represents a time that takes to insert the new index entry to the index table L R represents a time that accesses the data table and reads the old value of the key k L D represents a time that takes to mark the stale index entry as invalid. L R would be much larger than both L P and L D .

In by running only steps and skipping steps the major component L R from the right hand side of equation 1 is removed. This scheme called Sync insert which runs method steps only in shortens index update latency significantly according to equation 2 sync insert 2 

At step the SyncInsertObserver compares the vagainst the up to date value of the index. At step the SyncInsertObserver determines that an up to date index entry is the v the value of the first key if the vis equal to the up to date value of the index. At step the SyncInsertObserver determines that the corresponding stale index entry is the v the value of the first key if the vis not equal to the up to date value of the index and the first timestamp is made prior to the up to date timestamp. The SyncInsertObserver invalidates the determined stale index entry e.g. by setting an invalid bit of the determined stale index entry.

Under Sync insert scheme the client node sends to the SyncInsertObserver a request to insert a new data entry. Then the SyncInsertObserver inserts a corresponding new data entry to the data table . The SyncInsertObserver invalidates a corresponding stale data entry in the data table upon the insertion of the new data entry into the data table . In order to invalidate one or more stale data entry corresponding to the new data entry the SyncInsertObserver identifies data entries which share a same key with the new data entry but which have timestamps older than a current timestamp of the new data entry. Then the SyncInsertObserver sets invalid bit s of the identified data entries. Under Sync insert scheme a new index entry and a corresponding stale index entry co exist in the index table until a query to the corresponding stale index entry is received at the index table .

Compared to sync full scheme sync insert scheme does not perform steps in during an index update but rather performs the additional steps in during an index read. Under sync insert scheme the SyncInsertObserver runs method steps in during an index read i.e. reading an index entry from the index table .

 Async simple scheme offers low latency on an index update because an index update is performed by a background processing e.g. UNIX or LINUX background processing. A data entry read latency of async simple scheme is same to a data entry read latency of the sync full scheme.

Under Async insert scheme the client node sends to a coprocessor called AsyncObserver a request to insert a new data entry. Then the AsyncObserver inserts a corresponding new data entry to the data table . The AsyncObserver asynchronously inserts a corresponding new index entry into the index table upon inserting the new data entry into the data table . The AsyncObserver asynchronously deletes a corresponding stale index entry in the index table upon inserting the new data entry into the data table . The index table is not updated right away when the new data entry is inserted to the data table . The background processing performs the asynchronous insertion of the new index entry to the at least one index table and performs the asynchronous deletion of the corresponding stale index entry in the at least one index table.

In one embodiment in order to update an index table the background processing dequeues an update represented by the added new data entry from the provided queue. The background processing reads and obtains an old value of the key k before the time t e.g. by using the method step described above. The background processing deletes the corresponding stale index entry which is represented by the old value of the key k the key k. The background processing inserts the new index entry into the at least one index table. The new index entry is represented by a tuple where v is the new value of the key k.

 Async session scheme can provide a way for application developers to hide a complexity of dealing with eventual consistency between a data entry and a corresponding index entry while still getting most of latency and throughput benefits of asynchronously maintained indexes. Consider two different users posting the following sequence of operations 

User 1 views reviews for a product A while User 2 is browsing reviews for a different product B. Next User 1 posts a review for a product A and requests to view reviews for product A again in order to possibly to see if his review gets listed correctly. If this request is served by looking up a secondary index on a product column in the Reviews table as shown in and if a corresponding index entry is maintained asynchronously it is possible that User 1 at time 3 does not see his own review appear when he lists all the reviews for the product A because the index has not yet been updated when User 1 s query was run. This is an undesirable effect and may lead to User 1 assuming that his posting was lost and resubmitting it only to find his review getting listed twice when he finally queries for it.

By using async session scheme User 1 would be guaranteed to see his review at time 3 when he lists all reviews for product A after posting his own review. However User 2 may not be guaranteed to see User 1 s third review of product A even though User 2 s query arrived at time 3 after the third review of User 1 had been posted.

Under async session scheme any index lookups are guaranteed to include updates to corresponding data that have been made in a current session. A session refers to interactive information exchange including one or more messages in each direction.

In a get session call a client library i.e. an application programming interface used for creating client applications creates a random session ID adds an entry in a data structure e.g. array etc. that tracks a corresponding live session and returns this ID to the application. For all operations during a session the client library maintains a set of local private in memory tables associated with the session ID and updates them every time a server device returns a success for a request. To service read requests which returns consistent data entries the client library first runs a regular read operation that gets results from an appropriate RegionServer but before returning to a client node it merges this read operation with contents of the private table in the client library. Under async session results from an index table which is read by a RegionServer and session cache are combined during an index read. The intuition behind this approach is to track a local state of each session that might not be synchronously maintained in an index table.

When an index table is updated the client library submits this request as a regular call but also requests that the server devices return an old value and a new timestamp assigned to the update. The library uses the old value and the new timestamp to identify corresponding stale index entries in the index table. The server device also generates a new index entry based on a new value of a data entry in the data table and inserts the new index entry into the index table.

On get requests results returned by the server device get combined with data in the data structures of the client library. Private data associated with a session is garbage collected when a client issues an end session call.

In one embodiment a maximum limit for a session duration is set to 30 minutes. If a session is inactive longer than this limit e.g. 30 minutes then data stored in the private table for this session is automatically garbage collected. An application that issues a request under this session ID after 30 minutes gets a session expiration error. An application may choose to deal with this error in one of several ways it can either show this to the user as a notification with a disclaimer that a new session is starting. Alternatively the application ignores the expiration error and re issue the request either with regular consistency e.g. by using sync full scheme rather than async session or with a new session.

At step the system identifies the index update preference from the retrieved index definition . At steps and if the index update preference is the sync insert scheme the system invokes the SyncInsertObserver in order to proceed a corresponding index update according to sync insert scheme. If the index update preference is the async session scheme or async simple scheme at steps the system inserts the corresponding index update request into the AUQ and invokes the AsyncObserver in order to proceed the index update according to async session scheme or async simple scheme.

At steps if the index update preference is the sync full scheme the system invokes the SyncFullObserver in order to proceed the corresponding index update according to sync full scheme. At step the system evaluates whether all the indexes updates are processed. If all the indexes are not processed the control returns to step . Otherwise the control goes to step in order to exit the method steps shown in .

In the event that any of the RegionServers fails HBase does the following a master node e.g. ZooKeeper in detects the failure through a timeout based mechanism e.g. a RegionServer does not send an expected status report within a specified time period. The master node then retrieves the WAL corresponding to the failed RegionServer. WAL is available on HDFS and splits it into separate logs one for each region. Then the master node re assigns all the Regions that were assigned to the failed RegionServer to currently available RegionServers. This re assignment is typically based on a current workload of the currently available RegionServers but any re assignment strategy can be used here. The currently available RegionServers replay the WAL corresponding to the regions they have been assigned and then regenerates writes that have been lost due to the failure. HTables corresponding to these assigned regions may be on remote RegionServers and therefore most write operations may involve reading non local data. This non local data will eventually move to a local RegionServer after HTables are compacted into a new HTable.

In an event that a HBase Master node fails a new Master node is elected by Zookeeper . In this disclosure in the event that a RegionServer fails the same protocol described above can be used. However if the WAL is checkpointed and rolled forward while there are still un processed updates in an AUQ e.g. AUQ shown in the server might be exposed to a vulnerability. If a RegionServer fails when there is un processed updates in an AUQ then data entry insertions to AUQ for which index updates had not yet been processed cannot be reconstructed from entries in the WAL. To guard against this possibility a modification to checkpoint code is made in order to lock the AUQ and block index updates that may add new entries to the AUQ while waiting until all un processed entries in the AUQ drains and then proceed with the checkpoint code to roll the WAL forward. Draining the AUQ can be accomplished without actually modifying any HBase code using a preFlush coprocessor hook i.e. a function call which is called before memstore is flushed into a storage device.

During replaying the WAL previous put operations i.e. insertion operation for which a failed RegionServer had already delivered an index update using the AUQ shown in may get re processed. As a result some of the index updates may get delivered more than once to an index table. This does not cause any problems since the index updates are idempotent inserting a column into a row with a same or older timestamp as an existing column has no effect. This is accomplished by ensuring that a timestamp is assigned to a put operation before it is written to the WAL.

Atomicity when a Base Put occurs there are three follow up operations shown in the right hand side of the equation 1. Each individual operation i.e. P an insertion of a new index entry to the index table R reading a value of a key from the data table and D a deletion of a stale index entry in the index table is atomic. This atomicity is offered by an atomicity of primitive HBase operations put read and delete respectively. If any of them fails the system e.g. the system shown in does not roll back the Base Put. Instead the system inserts the failed task s into the AUQ and the failed tasks will be retried by the background processing until eventually succeed. In other words if the Base Put succeeds the Index Put is also guaranteed to complete eventually. Under these different index update schemes there is no global transaction so the three operations are not guaranteed to occur at the same time. This may have implications on consistency and isolation but with an advantage of no global locking and coordination.

Consistency sync full scheme and sync insert scheme are causal consistent once all index update operations complete and a SUCCESS is returned to a client both a data entry and its associated index entry are maintained consistently in a data store. If any index operation fails a corresponding Base Put operation is still persisted and the failed operation is added to the AUQ. In this failed operation casual consistency is no longer guaranteed and AUQ is responsible for an eventual consistency between an index entry and a corresponding data entry i.e. that index points to that data entry. Async simple scheme and async session schemes provide eventual consistency and session consistency respectively.

Isolation In one embodiment Read Commit isolation is implemented allowing a transaction to obtain a lock a lock for reading of data or a lock for writing of data on a row of a data table that the transaction is accessing. During this transaction the row cannot be updated by other transactions until the lock is released. However during an index update other concurrent clients may see partial results a client may see a new data entry but not a corresponding index entry at a time point between a corresponding Base Put operation and P.

Durability Durability is guaranteed by the WAL and the AUQ. In sync full and sync insert schemes operations P R and Dare performed e.g. by a postPut function call i.e. a function that is called after inserting of a data entry to a data table that is triggered after Base Put operation. If any one of the operations fails the system adds the failed operation in the AUQ and the AUQ is responsible for the eventual processing of the failed task. If a RegionServer fails a recovery mechanism described above guarantees durability.

While the invention has been particularly shown and described with respect to illustrative and preformed embodiments thereof it will be understood by those skilled in the art that the foregoing and other changes in form and details may be made therein without departing from the spirit and scope of the invention which should be limited only by the scope of the appended claims.

Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with a system apparatus or device running an instruction.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with a system apparatus or device running an instruction.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable RF etc. or any suitable combination of the foregoing.

Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may run entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects of the present invention are described below with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which run via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks. These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which run on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of code which comprises one or more operable instructions for implementing the specified logical function s . It should also be noted that in some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be run substantially concurrently or the blocks may sometimes be run in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

