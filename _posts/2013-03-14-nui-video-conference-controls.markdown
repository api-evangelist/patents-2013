---

title: NUI video conference controls
abstract: A system and method providing gesture controlled video conferencing includes a local capture device detecting movements of a user in a local environment and an audio/visual display. A processor is coupled to the capture device and a remote capture device and a remote processor at a remote environment via a network. The local processor includes instructions to render a representation of the remote environment on the display responsive to the remote processor and remote capture device. The processor also tracks movements of a local user in a space proximate to the local capture device. Responsive to a user gesture detected at the local capture device, the audio or visual signals provided by the remote capture device are altered to change the representation of the remote location is altered locally.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09274606&OS=09274606&RS=09274606
owner: Microsoft Technology Licensing, LLC
number: 09274606
owner_city: Redmond
owner_country: US
publication_date: 20130314
---
In the past computing applications such as computer games and multimedia applications have used controllers remotes keyboards mice or the like to allow users to manipulate game characters or other aspects of an application. More recently computer games and multimedia applications have begun employing cameras and motion recognition to provide a human computer interface HCI . With HCI user gestures are detected interpreted and used to control aspects of an application.

Video conferencing between processing devices such as computers mobile phones and game consoles allow users a more intimated conferencing experience. However conferees are generally limited to experiencing that which is presented by those they are conferring with. A local conferee is presented with the view and sounds based on the settings and positioning defined by any remote conferees.

Technology is provided to enable a user experience interaction and navigation between a local conferee and a remote conferee using gesture based controls to improve a local user s experience of a remote conferee. A gesture controlled video conferencing apparatus includes a local capture device detecting movements of a user in a local environment and an audio visual display. A processor is coupled to the capture device and a remote capture device and a remote processor at a remote environment via a network. The local processor includes instructions to render a representation of the remote environment on the display responsive to the remote processor and remote capture device. The processor also tracks movements of a local user in a space proximate to the local capture device. Responsive to a user gesture detected at the local capture device the audio or visual signals provided by the remote capture device are altered to change the representation of the remote location is altered locally.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

Technology is provided to enable a user experience interaction and navigation between users participating in an audio visual teleconference. The technology enables a local user to adjust the local user s audio visual experience through gesture controls which implement changes in a remote user s processing device. Gesture controlled video conferencing utilizes a local capture device detecting movements of a local user in a local environment and an audio visual display. The local user may be in an audio visual conference with a remote user via a network. A representation of the remote environment with the remote user is provided on a local display and responsive to movements of a local user in a space proximate to the local capture device user gestures can alter audio or visual signals provided by the remote capture device to change the representation of the remote location on the local display. Audio and visual control gestures are provided.

As shown in the tracking system may include a computing environment . The computing environment may be a computer a gaming system or console or the like. According to one embodiment the computing environment may include hardware components and or software components such that the computing environment may be used to execute an operating system and applications such as gaming applications non gaming applications or the like. In one embodiment computing system may include a processor such as a standardized processor a specialized processor a microprocessor or the like that may execute instructions stored on a processor readable storage device for performing the processes described herein.

As shown in the tracking system may further include a capture device . The capture device may be for example a camera that may be used to visually monitor one or more users such as the user such that gestures performed by the one or more users may be captured analyzed and tracked to perform one or more controls or actions for the user interface of an operating system or application.

The capture device may be positioned on a three axis positioning motor allowing the capture device to move relative to a base element on which it is mounted.

According to one embodiment the tracking system may be connected to an audiovisual device such as a television a monitor a high definition television HDTV or the like that may provide game or application visuals and or audio to a user such as the user . For example the computing environment may include a video adapter such as a graphics card and or an audio adapter such as a sound card that may provide audiovisual signals associated with the game application non game application or the like. The audiovisual device may receive the audiovisual signals from the computing environment and may output the game or application visuals and or audio associated with the audiovisual signals to the user . According to one embodiment the audiovisual device may be connected to the computing environment via for example an S Video cable a coaxial cable an HDMI cable a DVI cable a VGA cable or the like.

As shown in the target recognition analysis and tracking system may be used to recognize analyze and or track one or more human targets such as the user . For example the user may be tracked using the capture device such that the movements of user may be interpreted as controls that may be used to affect an application or operating system being executed by computer environment .

Some movements may be interpreted as controls that may correspond to actions other than controlling a player avatar or other gaming object. Virtually any controllable aspect of an operating system and or application may be controlled by movements of the target such as the user . The player may use movements to select a game or other application from a main user interface. A full range of motion of the user may be available used and analyzed in any suitable manner to interact with an application or operating system.

In user is interacting with the tracking system to control the system user interface UI which in this particular example is displaying a list of menu items in the interface . The individual items may represent applications or other UI objects. A user may scroll left or right as seen from the user s point of view through the list to view other menu items not in the current display but also associated with the list select menu items to trigger an action such as opening an application represented by the menu item or further UI controls for that item. The user may also move backwards through the UI to a higher level menu item in the UI hierarchy.

The system may include gesture recognition so that a user may control an application or operating system executing on the computing environment which as discussed above may be a game console a computer or the like by performing one or more gestures. In one embodiment a gesture recognizer engine the architecture of which is described more fully below is used to determine from a skeletal model of a user when a particular gesture has been made by the user.

Generally as indicated in a user is confined to a physical space when using a capture device . The physically limited space is generally the best performing range of the capture device .

In the user performs a right handed gesture to scroll the list of menu items to the left as seen from the user s point of view. The user begins with his right hand in position as shown in then moves it to position toward the left side of his body. The list of menu items is in a first position in when the user begins the gesture with his hand at position . In the user has moved his hand to position causing the list of menu items to change by scrolling the list of menu items to the left. Menu item has been removed from the list as a result of scrolling to the left as defined in user s point of view . Each of items has moved one place to the left replacing the position of the immediately preceding item. Item has been added to the list as a result of scrolling from the right to the left.

As shown in the capture device may include an image camera component . According to one embodiment the image camera component may be a depth camera that may capture a depth image of a scene. The depth image may include a two dimensional 2 D pixel area of the captured scene where each pixel in the 2 D pixel area may represent a depth value such as a distance in for example centimeters millimeters or the like of an object in the captured scene from the camera.

As shown in the image camera component may include an IR light component a three dimensional 3 D camera and an RGB camera that may be used to capture the depth image of a capture area. For example in time of flight analysis the IR light component of the capture device may emit an infrared light onto the capture area and may then use sensors to detect the backscattered light from the surface of one or more targets and objects in the capture area using for example the 3 D camera and or the RGB camera . In some embodiments pulsed infrared light may be used such that the time between an outgoing light pulse and a corresponding incoming light pulse may be measured and used to determine a physical distance from the capture device to a particular location on the targets or objects in the capture area. Additionally the phase of the outgoing light wave may be compared to the phase of the incoming light wave to determine a phase shift. The phase shift may then be used to determine a physical distance from the capture device to a particular location on the targets or objects.

According to one embodiment time of flight analysis may be used to indirectly determine a physical distance from the capture device to a particular location on the targets or objects by analyzing the intensity of the reflected beam of light over time via various techniques including for example shuttered light pulse imaging.

In another example the capture device may use structured light to capture depth information. In such an analysis patterned light i.e. light displayed as a known pattern such as grid pattern or a stripe pattern may be projected onto the capture area via for example the IR light component . Upon striking the surface of one or more targets or objects in the capture area the pattern may become deformed in response. Such a deformation of the pattern may be captured by for example the 3 D camera and or the RGB camera and may then be analyzed to determine a physical distance from the capture device to a particular location on the targets or objects.

According to one embodiment the capture device may include two or more physically separated cameras that may view a capture area from different angles to obtain visual stereo data that may be resolved to generate depth information. Other types of depth image sensors can also be used to create a depth image.

The capture device may further include a microphone . The microphone may include a transducer or sensor that may receive and convert sound into an electrical signal. According to one embodiment the microphone may be used to reduce feedback between the capture device and the computing environment in the target recognition analysis and tracking system . Additionally the microphone may be used to receive audio signals that may also be provided by the user to control applications such as game applications non game applications or the like that may be executed by the computing environment .

In one embodiment the microphone comprises array of microphone with multiple elements for example four elements. The multiple elements of the microphone can be used in conjunction with beam forming techniques to achieve spatial selectivity

In one embodiment the capture device may further include a processor that may be in operative communication with the image camera component . The processor may include a standardized processor a specialized processor a microprocessor or the like that may execute instructions that may include instructions for storing profiles receiving the depth image determining whether a suitable target may be included in the depth image converting the suitable target into a skeletal representation or model of the target or any other suitable instruction.

Processor may include an imaging signal processor capable of adjusting color brightness hue sharpening and other elements of the captured digital image.

The capture device may further include a memory component that may store the instructions that may be executed by the processor images or frames of images captured by the 3 D camera or RGB camera user profiles or any other suitable information images or the like. According to one example the memory component may include random access memory RAM read only memory ROM cache Flash memory a hard disk or any other suitable storage component. As shown in the memory component may be a separate component in communication with the image capture component and the processor . In another embodiment the memory component may be integrated into the processor and or the image capture component . In one embodiment some or all of the components and of the capture device illustrated in are housed in a single housing.

The capture device may be in communication with the computing environment via a communication link . The communication link may be a wired connection including for example a USB connection a Firewire connection an Ethernet cable connection or the like and or a wireless connection such as a wireless 802.11b g a or n connection. The computing environment may provide a clock to the capture device that may be used to determine when to capture for example a scene via the communication link .

The capture device may provide the depth information and images captured by for example the 3 D camera and or the RGB camera including a skeletal model that may be generated by the capture device to the computing environment via the communication link . The computing environment may then use the skeletal model depth information and captured images to for example create a virtual screen adapt the user interface and control an application such as a game or word processor.

A motion tracking system uses the skeletal model and the depth information to provide a control output to an application on a processing device to which the capture device is coupled. The depth information may likewise be used by a gestures library structure data gesture recognition engine depth image processing and object reporting module and operating system . Depth image processing and object reporting module uses the depth images to track motion of objects such as the user and other objects. The depth image processing and object reporting module will report to operating system an identification of each object detected and the location of the object for each frame. Operating system will use that information to update the position or movement of an avatar or other images in the display or to perform an action on the provided user interface. To assist in the tracking of the objects depth image processing and object reporting module uses gestures library structure data and gesture recognition engine .

Structure data includes structural information about objects that may be tracked. For example a skeletal model of a human may be stored to help understand movements of the user and recognize body parts. Structural information about inanimate objects may also be stored to help recognize those objects and help understand movement.

Gestures library may include a collection of gesture filters each comprising information concerning a gesture that may be performed by the skeletal model as the user moves . A gesture recognition engine may compare the data captured by the cameras and device in the form of the skeletal model and movements associated with it to the gesture filters in the gesture library to identify when a user as represented by the skeletal model has performed one or more gestures. Those gestures may be associated with various controls of an application. Thus the computing system may use the gestures library to interpret movements of the skeletal model and to control operating system or an application not shown based on the movements.

More information about recognizer engine can be found in U.S. patent application Ser. No. 12 422 661 Gesture Recognizer System Architecture filed on Apr. 13 2009 incorporated herein by reference in its entirety. More information about recognizing gestures can be found in U.S. patent application Ser. No. 12 391 150 Standard Gestures filed on Feb. 23 2009 and U.S. patent application Ser. No. 12 474 655 Gesture Tool filed on May 29 2009 both of which are incorporated by reference herein in their entirety. More information about motion detection and tracking can be found in U.S. patent application Ser. No. 12 641 788 Motion Detection Using Depth Images filed on Dec. 18 2009 and U.S. patent application Ser. No. 12 475 308 Device for Identifying and Tracking Multiple Humans over Time both of which are incorporated herein by reference in their entirety.

A communication application may operate on the computing system to allow users to communicate via capture devices and communication systems which communicate with each other over a network illustrated and discussed below with respect to . Communication application may be any commercially available communication application instant messenger application or audio visual conferencing application. One example of one such application is Skype by Skype Communication SARL.

At step depth information corresponding to the visual image and depth image are determined. The visual image and depth image received at step can be analyzed to determine depth values for one or more targets within the image. Capture device may capture or observe a capture area that may include one or more targets. At step the capture device determines whether the depth image includes a human target. In one example each target in the depth image may be flood filled and compared to a pattern to determine whether the depth image includes a human target. In one example the edges of each target in the captured scene of the depth image may be determined. The depth image may include a two dimensional pixel area of the captured scene for which each pixel in the 2D pixel area may represent a depth value such as a length or distance for example as can be measured from the camera. The edges may be determined by comparing various depth values associated with for example adjacent or nearby pixels of the depth image. If the various depth values being compared are greater than a predetermined edge tolerance the pixels may define an edge. The capture device may organize the calculated depth information including the depth image into Z layers or layers that may be perpendicular to a Z axis extending from the camera along its line of sight to the viewer. The likely Z values of the Z layers may be flood filled based on the determined edges. For instance the pixels associated with the determined edges and the pixels of the area within the determined edges may be associated with each other to define a target or a physical object in the capture area.

At step the capture device scans the human target for one or more body parts. The human target can be scanned to provide measurements such as length width or the like that are associated with one or more body parts of a user such that an accurate model of the user may be generated based on these measurements. In one example the human target is isolated and a bit mask is created to scan for the one or more body parts. The bit mask may be created for example by flood filling the human target such that the human target is separated from other targets or objects in the capture area elements. At step a model of the human target is generated based on the scan performed at step . The bit mask may be analyzed for the one or more body parts to generate a model such as a skeletal model a mesh human model or the like of the human target. For example measurement values determined by the scanned bit mask may be used to define one or more joints in the skeletal model. The bitmask may include values of the human target along an X Y and Z axis. The one or more joints may be used to define one or more bones that may correspond to a body part of the human.

According to one embodiment to determine the location of the neck shoulders or the like of the human target a width of the bitmask for example at a position being scanned may be compared to a threshold value of a typical width associated with for example a neck shoulders or the like. In an alternative embodiment the distance from a previous position scanned and associated with a body part in a bitmask may be used to determine the location of the neck shoulders or the like.

In one embodiment to determine the location of the shoulders the width of the bitmask at the shoulder position may be compared to a threshold shoulder value. For example a distance between the two outer most Y values at the X value of the bitmask at the shoulder position may be compared to the threshold shoulder value of a typical distance between for example shoulders of a human. Thus according to an example embodiment the threshold shoulder value may be a typical width or range of widths associated with shoulders of a body model of a human.

In another embodiment to determine the location of the shoulders the bitmask may be parsed downward a certain distance from the head. For example the top of the bitmask that may be associated with the top of the head may have an X value associated therewith. A stored value associated with the typical distance from the top of the head to the top of the shoulders of a human body may then added to the X value of the top of the head to determine the X value of the shoulders. Thus in one embodiment a stored value may be added to the X value associated with the top of the head to determine the X value associated with the shoulders.

In one embodiment some body parts such as legs feet or the like may be calculated based on for example the location of other body parts. For example as described above the information such as the bits pixels or the like associated with the human target may be scanned to determine the locations of various body parts of the human target. Based on such locations subsequent body parts such as legs feet or the like may then be calculated for the human target.

According to one embodiment upon determining the values of for example a body part a data structure may be created that may include measurement values such as length width or the like of the body part associated with the scan of the bitmask of the human target. In one embodiment the data structure may include scan results averaged from a plurality depth images. For example the capture device may capture a capture area in frames each including a depth image. The depth image of each frame may be analyzed to determine whether a human target may be included as described above. If the depth image of a frame includes a human target a bitmask of the human target of the depth image associated with the frame may be scanned for one or more body parts. The determined value of a body part for each frame may then be averaged such that the data structure may include average measurement values such as length width or the like of the body part associated with the scans of each frame. In one embodiment the measurement values of the determined body parts may be adjusted such as scaled up scaled down or the like such that measurement values in the data structure more closely correspond to a typical model of a human body. Measurement values determined by the scanned bitmask may be used to define one or more joints in a skeletal model at step .

At step motion is captured from the depth images and visual images received from the capture device. In one embodiment capturing motion at step includes generating a motion capture file based on the skeletal mapping as will be described in more detail hereinafter. At the model created in step is tracked using skeletal mapping and to track user motion at . For example the skeletal model of the user may be adjusted and updated as the user moves in physical space in front of the camera within the field of view. Information from the capture device may be used to adjust the model so that the skeletal model accurately represents the user. In one example this is accomplished by one or more forces applied to one or more force receiving aspects of the skeletal model to adjust the skeletal model into a pose that more closely corresponds to the pose of the human target and physical space.

At step user motion is tracked. At step motion data is provided to an application such as a navigation system as described herein. Such motion data may further be evaluated to determine whether a user is performing a pre defined gesture. Step can be performed based on the UI context or contexts determined in step . For example a first set of gestures may be active when operating in a menu context while a different set of gestures may be active while operating in a game play context. Step can also include determining an active set of gestures. At step gesture recognition and control is performed. The tracking model and captured motion are passed through the filters for the active gesture set to determine whether any active gesture filters are satisfied. Any detected gestures are applied within the computing environment to control the user interface provided by computing environment . Step can further include determining whether any gestures are present and if so modifying the user interface action that is performed in response to gesture detection.

In one embodiment steps are performed by computing device . Furthermore although steps are described as being performed by capture device various ones of these steps may be performed by other components such as by computing environment . For example the capture device may provide the visual and or depth images to the computing environment which will in turn determine depth information detect the human target scan the target generate and track the model and capture motion of the human target.

Skeletal model includes joints n1 n18. Each of the joints n1 n18 may enable one or more body parts defined there between to move relative to one or more other body parts. A model representing a human target may include a plurality of rigid and or deformable body parts that may be defined by one or more structural members such as bones with the joints n1 n18 located at the intersection of adjacent bones. The joints n1 n18 may enable various body parts associated with the bones and joints n1 n18 to move independently of each other or relative to each other. For example the bone defined between the joints n7 and n11 corresponds to a forearm that may be moved independent of for example the bone defined between joints n15 and n17 that corresponds to a calf. It is to be understood that some bones may correspond to anatomical bones in a human target and or some bones may not have corresponding anatomical bones in the human target.

The bones and joints may collectively make up a skeletal model which may be a constituent element of the model. An axial roll angle may be used to define a rotational orientation of a limb relative to its parent limb and or the torso. For example if a skeletal model is illustrating an axial rotation of an arm a roll joint may be used to indicate the direction the associated wrist is pointing e.g. palm facing up . By examining an orientation of a limb relative to its parent limb and or the torso an axial roll angle may be determined. For example if examining a lower leg the orientation of the lower leg relative to the associated upper leg and hips may be examined in order to determine an axial roll angle.

In one alternative both local user and remote user can make gestures sounds and otherwise operate respective local system and remote system in a similar manner . User can make communicate by speaking gesturing and moving all of which are captured and transmitted by local capture device and to the remote location and displayed to user at the remote location. As illustrated in local computing system is coupled to a remote computing system via network . Each capture device may be coupled to the communication application described above and computing system includes an application programming interface API which may be accessed by the communication application to control aspects of the computing system or the capture device in a manner defined by both the application and the application programming interface.

As illustrated in when users participate in an audio visual conference there may be background audio or visual noise which is captured by each capture device. This noise can be distracting to the conference. This background information can take many forms including background audio noise and background visual noise. In addition other remote users or objects may be present which may or may not be of interest to user . Similarly there may be instances in the conference when it is desirable to focus or enhance the audio or video in a particular area of a remote location.

In accordance with the present technology a local user can use gesture controls to manipulate presentation of the remote location on local display allowing the local user a natural degree of control of the audiovisual conference using gesture based controls.

With reference to and gesture based user interaction with a local system during an audio visual conference with a remote user is illustrated. is a flow chart of detection of a local gesture and interaction with the video conferencing technology. may comprise one representation of one embodiment of step in .

In a local user initiate a gesture by for example raising the arm as indicated at which will in turn generate a user interface to allow for control of the representation of the remote location on the display . In this case as illustrated in user interface includes a selector a hand icon which tracks the motion of the hand of user when the hand is moved relative to the display and when positioned over region and for example held steady could be used to select region by the local user. The region is merely illustrative and selection may be of a larger or smaller region or may be of the remote user as a region. Once a region is selected the act of selecting the region may have a default action associated with it for example mute audio in this region or one or more additional gestures and or user interaction elements can be used to achieve processing desired by user . Illustrated in is an alternative volume slider which may appear once region is selected and allow the local user to adjust the audio output of user . The volume slider may for example allow the user to reduce the volume coming from this region. It should be recognized that nay number of simple or advanced audio or visual controls may be utilized in this embodiment.

With reference to when a user performs a gesture at step a determination is made that a control gesture has been initiated by a user. At step a user interface may be presented. User interface may be very basic or may be more complex as illustrated in the below figures. As noted above in some embodiments the user interface may be as basic as indicating where on the display a selection is being made. A basic user interface allows a selection icon such as the selector illustrated in to be presented on a display . This gives the user an idea of the relative position of the user s hand motion being made. At step a selection of a particular area of the display is received. The area may be the selection of a specific user or a region on the screen. Selection may occur by holding a user s hand steady over an area or by using another selection specific gesture. Selection may likewise be made of a region having some relationship to objects detected in the remote location. For example the remote system may have determined and indicated that the user is a human and can be selected as an object itself.

In the example illustrated in a region is selected based on a determination that an audio source or user is present within this region. In the user has selected the region and for example held his hand in place as illustrated in At step a determination is made as to whether or not a user has made a gesture which indicates an audio control is desired. An audio control may be represented by any number of gestures including a user s hand up flat wiping the hand across a selected region or any of a number of other types of gestures. If an audio control is selected a determination is made as to whether or not additional user interface controls should be presented to the user at step . Additional user interface controls may be for example the volume slider such as that illustrated in . Options may be provided to the user to allow the user to select additional interface controls. In an audio slider is presented just below region to allow user to change the audio output being received from this particular area of the display in the remote location. A user may grasp the slider or push the slider using a gesture to control the hand icon which has changed shape as illustrated in

Returning to at step if additional controls are required they are displayed to allow the user to provide control inputs. After control inputs are received at step or if no additional user interface controls were required at then an audio control signal based on the input from the local user is compiled at . If no audio control signal is necessary or in addition to the compilation of the audio control portion of the signal at then at a determination is made as to whether a video control gesture has been made. If no video control gesture has been made then at an output is made of the audio only portion of the control signal. If a video control gesture has been made at then at a determination is made as to whether additional video controls are required.

Returning to once a video control gesture is determined at the determination is made at as to whether or not additional video interface controls are desired at . If the additional video controls are required at then additional controls are displayed at and additional control inputs for the video portion of the control are received at . At the control video signal is compiled based on the input. If both audio and visual signals have been acquired then at a composite control signal is output to the remote device. As illustrated in the gesture which controls the screen output available to the user for video control may be similar to that illustrated above with respect to audio controls. It should be understood that any number of different types of gestures may be provided. It should also be understood that any different types of controls may be utilized to alter the video signal being transmitted from the remote location to the local location. Dials sliders abstract gestures or gestures mimicking interactions with the representation of the remote location on the display may all be user.

As noted above at steps and permissions may be set by the user or for example control application to determine whether control requests received from a remote user are allowed to alter local processing for the remote user.

In accordance with the present technology the control signal alters the processing output by the process memory and competing system at the remote location. This minimizes the amount of information which is transmitted from the remote location to the local location to be rendered on device . Because processing takes place at the remote location and because the actions of the user at the local location are gesture based a natural user interface is provided which allows an improved video conferencing solution using natural user interface controls. It should be further understood that the type of controls which may be provided by the audio system include enhancing the audio input from a particular region of the remote location using the directional array microphone blocking sounds from a particular area or more complicated digital signal processing techniques to enhance the sound coming from particular regions or block sounds coming from particular regions of the remote display. It should be further understood that although only one particular region is illustrated in the figures multiple regions of the remote display can be controlled. In addition it should be understood that for video processing any number of different types of video processing can occur including for example adjusting the pan tilt zoom game resolution frame rate or other video controls.

Still further although basic controls are illustrated in the figures more advanced controls can be provided.

CPU memory controller and various memory devices are interconnected via one or more buses not shown . The details of the bus that is used in this implementation are not particularly relevant to understanding the subject matter of interest being discussed herein. However it will be understood that such a bus might include one or more of serial and parallel buses a memory bus a peripheral bus and a processor or local bus using any of a variety of bus architectures. By way of example such architectures can include an Industry Standard Architecture ISA bus a Micro Channel Architecture MCA bus an Enhanced ISA EISA bus a Video Electronics Standards Association VESA local bus and a Peripheral Component Interconnects PCI bus also known as a Mezzanine bus.

In one implementation CPU memory controller ROM and RAM are integrated onto a common module . In this implementation ROM is configured as a flash ROM that is connected to memory controller via a PCI bus and a ROM bus neither of which are shown . RAM is configured as multiple Double Data Rate Synchronous Dynamic RAM DDR SDRAM modules that are independently controlled by memory controller via separate buses not shown . Hard disk drive and portable media drive are shown connected to the memory controller via the PCI bus and an AT Attachment ATA bus . However in other implementations dedicated data bus structures of different types can also be applied in the alternative.

A graphics processing unit and a video encoder form a video processing pipeline for high speed and high resolution e.g. High Definition graphics processing. Data are carried from graphics processing unit GPU to video encoder via a digital video bus not shown . Lightweight messages generated by the system applications e.g. pop ups are displayed by using a GPU interrupt to schedule code to render popup into an overlay. The amount of memory used for an overlay depends on the overlay area size and the overlay preferably scales with screen resolution. Where a full user interface is used by the concurrent system application it is preferable to use a resolution independent of application resolution. A scaler may be used to set this resolution such that the need to change frequency and cause a TV resync is eliminated.

An audio processing unit and an audio codec coder decoder form a corresponding audio processing pipeline for multi channel audio processing of various digital audio formats. Audio data are carried between audio processing unit and audio codec via a communication link not shown . The video and audio processing pipelines output data to an NV audio video port for transmission to a television or other display. In the illustrated implementation video and audio processing components are mounted on module .

In the implementation depicted in console includes a controller support subassembly for supporting four controllers . The controller support subassembly includes any hardware and software components needed to support wired and wireless operation with an external control device such as for example a media and game controller. A front panel I O subassembly supports the multiple functionalities of power button the eject button as well as any LEDs light emitting diodes or other indicators exposed on the outer surface of console . Subassemblies and are in communication with module via one or more cable assemblies . In other implementations console can include additional controller subassemblies. The illustrated implementation also shows an optical I O interface that is configured to send and receive signals that can be communicated to module .

MUs and are illustrated as being connectable to MU ports A and B respectively. Additional MUs e.g. MUs are illustrated as being connectable to controllers and i.e. two MUs for each controller. Controllers and can also be configured to receive MUs not shown . Each MU offers additional storage on which games game parameters and other data may be stored. In some implementations the other data can include any of a digital game component an executable gaming application an instruction set for expanding a gaming application and a media file. When inserted into console or a controller MU can be accessed by memory controller . A system power supply module provides power to the components of gaming console . A fan cools the circuitry within console . A microcontroller unit is also provided.

An application comprising machine instructions is stored on hard disk drive . When console is powered on various portions of application are loaded into RAM and or caches and for execution on CPU wherein application is one such example. Various applications can be stored on hard disk drive for execution on CPU .

Gaming and media console may be operated as a standalone system by simply connecting the system to monitor a television a video projector or other display device. In this standalone mode gaming and media console enables one or more players to play games or enjoy digital media e.g. by watching movies or listening to music. However with the integration of broadband connectivity made available through network interface gaming and media console may further be operated as a participant in a larger network gaming community.

The system described above can be used to add virtual images to a user s view such that the virtual images are mixed with real images that the user see. In one example the virtual images are added in a manner such that they appear to be part of the original scene. Examples of adding the virtual images can be found U.S. patent application Ser. No. 13 112 919 Event Augmentation With Real Time Information filed on May 20 2011 and U.S. patent application Ser. No. 12 905 952 Fusing Virtual Content Into Real Content filed on Oct. 15 2010 both applications are incorporated herein

Computing system comprises a computer which typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer and includes both volatile and nonvolatile media removable and non removable media. The system memory includes computer storage media in the form of volatile and or nonvolatile memory such as read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computer such as during start up is typically stored in ROM . RAM typically contains data and or program modules that are immediately accessible to and or presently being operated on by processing unit . By way of example and not limitation illustrates operating system application programs other program modules and program data .

The computer may also include other removable non removable volatile nonvolatile computer storage media. By way of example only illustrates a hard disk drive that reads from or writes to non removable nonvolatile magnetic media a magnetic disk drive that reads from or writes to a removable nonvolatile magnetic disk and an optical disk drive that reads from or writes to a removable nonvolatile optical disk such as a CD ROM or other optical media. Other removable non removable volatile nonvolatile computer storage media that can be used in the exemplary operating environment include but are not limited to magnetic tape cassettes flash memory cards digital versatile disks digital video tape solid state RAM solid state ROM and the like. The hard disk drive is typically connected to the system bus through an non removable memory interface such as interface and magnetic disk drive and optical disk drive are typically connected to the system bus by a removable memory interface such as interface .

The drives and their associated computer storage media discussed above and illustrated in provide storage of computer readable instructions data structures program modules and other data for the computer . In for example hard disk drive is illustrated as storing operating system application programs other program modules and program data . Note that these components can either be the same as or different from operating system application programs other program modules and program data . Operating system application programs other program modules and program data are given different numbers here to illustrate that at a minimum they are different copies. A user may enter commands and information into the computer through input devices such as a keyboard and pointing device commonly referred to as a mouse trackball or touch pad. Other input devices not shown may include a microphone joystick game pad satellite dish scanner or the like. These and other input devices are often connected to the processing unit through a user input interface that is coupled to the system bus but may be connected by other interface and bus structures such as a parallel port game port or a universal serial bus USB . The cameras and capture device may define additional input devices for the computing system that connect via user input interface . A monitor or other type of display device is also connected to the system bus via an interface such as a video interface . In addition to the monitor computers may also include other peripheral output devices such as speakers and printer which may be connected through a output peripheral interface . Capture Device may connect to computing system via output peripheral interface network interface or other interface.

The computer may operate in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be a personal computer a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer although only a memory storage device has been illustrated in . The logical connections depicted include a local area network LAN and a wide area network WAN but may also include other networks. Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet.

When used in a LAN networking environment the computer is connected to the LAN through a network interface or adapter . When used in a WAN networking environment the computer typically includes a modem or other means for establishing communications over the WAN such as the Internet. The modem which may be internal or external may be connected to the system bus via the user input interface or other appropriate mechanism. In a networked environment program modules depicted relative to the computer or portions thereof may be stored in the remote memory storage device. By way of example and not limitation illustrates application programs as residing on memory device . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

As explained above the capture device provides RGB images also known as color images and depth images to the computing system . The depth image may be a plurality of observed pixels where each observed pixel has an observed depth value. For example the depth image may include a two dimensional 2 D pixel area of the captured scene where each pixel in the 2 D pixel area may have a depth value such as a length or distance in for example centimeters millimeters or the like of an object in the captured scene from the capture device.

As mentioned above skeletal tracking ST techniques are often used to detect motion of a user or other user behaviors. However while useful for detecting certain types of user behaviors ST techniques have proven to be unreliable for detecting other types of user behavior. For example ST techniques are typically unreliable for detecting user behaviors where the user is laying or sitting on or near the floor. Certain embodiments described herein rely on depth images to detect user behaviors. Such user behaviors detected based on depth base images can be used in place of or to supplement ST techniques for detecting user behaviors. Accordingly before discussing such embodiments in additional detail it would first be useful to provide additional details of depth images.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

