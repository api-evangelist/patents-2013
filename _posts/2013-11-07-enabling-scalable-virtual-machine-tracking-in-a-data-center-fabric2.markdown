---

title: Enabling scalable virtual machine tracking in a data center fabric
abstract: A method is provided in one example embodiment and includes receiving at a first network element a packet from a second network element; processing the packet at the first network element to obtain information regarding an identity of a virtual machine (“VM”) hosted by the second network element contained within the packet; and storing at the first network element the identifying information. The identifying information stored at the first network element is accessible by at least one third network element. In some embodiments, the first network element comprises a physical switch and the second network element comprises a virtual switch.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09548922&OS=09548922&RS=09548922
owner: CISCO TECHNOLOGY, INC.
number: 09548922
owner_city: San Jose
owner_country: US
publication_date: 20131107
---
This disclosure relates in general to the field of computer networking and more particularly to techniques for enabling scalable tracking of virtual machines VMs in a data center fabric.

Currently for virtualized data centers visibility of orchestration engines such as vCloud Director Openstack and Cloupedia to name a few is limited to the hypervisors and virtual switch managers. Such orchestration engines provision virtual machines VMs and their resources both on the hypervisor and on the virtual switches. All of these VMs talk to the other VMs or to the infrastructure outside the data center via the data center network fabric. The network fabric comprises a collection of Top of Rack ToR switches referred to as leaf switches that in turn are connected by a set of spine stitches. The orchestration engines do not have visibility into the fabric similarly the switches in the fabric have no visibility into the server and their hypervisor infrastructure. Hence the switches cannot relay to the network or server administrators information regarding where a particular VM is physically attached to the network.

A crude way of tracking this information would be to dump the Address Resolution Protocol ARP and Media Access Control MAC tables and then attempt to correlate the information to form a table of directly attached local hosts. This is not only unwieldy but would make it very difficult for the administrators to correlate the information across various databases moreover the information would be provided on a per interface or per virtual Network Interface Card vNIC basis rather than a per end host VM basis.

A method is provided in one example embodiment and includes receiving at a first network element a packet from a second network element processing the packet at the first network element to obtain information regarding an identity of a virtual machine VM hosted by the second network element contained within the packet and storing at the first network element the identifying information. The identifying information stored at the first network element is accessible by at least one third network element. In some embodiments the first network element comprises a physical switch and the second network element comprises a virtual switch. The packet may be created by the second network element in response to a Virtual Station Interface Discovery and Configuration Protocol VDP event. In some embodiments the at least one third network element comprises a physical switch. The first network element may include a leaf switch and the at least one third network element may include a spine switch connected to the leaf switch. The packet may include a Type Length Value TLV element comprising the identifying information. In certain embodiments the identifying information may include a name of the VM in association with an identity of at least one network interface to which the VM is connected.

The following discussion references various embodiments. However it should be understood that the disclosure is not limited to specifically described embodiments. Instead any combination of the following features and elements whether related to different embodiments or not is contemplated to implement and practice the disclosure. Furthermore although embodiments may achieve advantages over other possible solutions and or over the prior art whether or not a particular advantage is achieved by a given embodiment is not limiting of the disclosure. Thus the following aspects features embodiments and advantages are merely illustrative and are not considered elements or limitations of the appended claims except where explicitly recited in a claim s . Likewise reference to the disclosure shall not be construed as a generalization of any subject matter disclosed herein and shall not be considered to be an element or limitation of the appended claims except where explicitly recited in a claim s .

As will be appreciated aspects of the present disclosure may be embodied as a system method or computer program product. Accordingly aspects of the present disclosure may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may generally be referred to herein as a module or system. Furthermore aspects of the present disclosure may take the form of a computer program product embodied in one or more non transitory computer readable medium s having computer readable program code encoded thereon.

Any combination of one or more non transitory computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device.

Computer program code for carrying out operations for aspects of the present disclosure may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages.

Aspects of the present disclosure are described below with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the disclosure. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

The flowchart and block diagrams in the figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present disclosure. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that in some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in a different order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

As noted above one manner in which to design a scalable data center fabric is to use what is commonly referred to as a fat tree or spine leaf architecture. Such an architecture includes two types of switches including a first type that that connects servers referred to as leaf switches or nodes and a second type that connects leaf switches nodes referred to as spine switches or nodes . Typically via protocols like Cisco Discovery Protocol CDP and Link Layer Discovery Protocol LLDP the ToR nodes are aware of their physical connectivity to other devices including physical servers however which VMs exist below a given leaf switch at any given time is known only via protocols such as Address Resolution Protocol ARP Dynamic Host Configuration Protocol DHCP and Neighbor Discovery ND for example based on network interface identifiers such as Internet Protocol IP address MAC address etc. What has been missing is a scalable solution that allows this information to be tracked directly from a data center s orchestration engine all the way to the physical network switches comprising the data center fabric. It will be noted that the embodiments described herein although described for the sake of example with reference to a fat tree architecture may be implemented in network architectures that are not implemented as a fat tree.

Embodiments described herein provide a scalable mechanism for tracking VMs in a data center fabric. Certain embodiments employ VSI Discovery and Configuration Protocol VDP to carry VM based identifiers such as the name of the VM and or the Universal Unique Identifier UUID associated with the VM from a virtual switch to which the VM is connected to the physical switch at the edge of the data center fabric where it is made available to the data center network manager DCNM in response to a query. This allows the data center fabric to be queried for the current location of a particular VM in terms of the leaf and port to which it is currently connected.

It will be recognized that tracking end hosts typically VMs in large data center deployments can be quite challenging especially with hundreds to thousands of VMs to track. While the different orchestration engines that deploy the VMs may be aware of the physical servers on which the VMs reside they do not have any visibility into the physical network infrastructure. From a data center troubleshooting and visibility point of view it is extremely important to know which VMs reside behind which physical switches or ToRs.

VDP is an IEEE 802.1Qbg standard that enables a virtual switch or virtual station to exchange information about its directly attached VMs with its adjacent bridge or switch i.e. ToR or leaf switch . The information exchanged may include information regarding events like events like VM coming up VM move VM going down etc. This allows the directly attached bridge or switch to provision network resources such as VLAN etc. for the VM so that is ready to handle subsequent traffic to and from the VM. It should be noted that VDP communicates on a per vNIC basis. As a result it shades the unique VM identity behind the vNIC. For example in the case that a VM comes up with multiple vNICs or a VM adds deletes multiple vNICs the standard VDP views and presents each of these as separate events to the user.

Most hypervisors employ a virtual switch vSwitch to connect VMs to the external network e.g. the network fabric . A vSwitch may also support connections between VMs on a single physical server. The IEEE 802.1Qbg standard referenced above has been responsible for creating an Edge Virtual Bridging EVB industry group to address various aspects of EVB including VSI Discovery. VSI is an internal point to point Ethernet LAN that connects a bridge port of a vSwitch to a vNIC. Each VSI carries a single MAC service instance. The term VSI may refer to the reference point at which the internal LAN attaches to the vNIC. VDP is used to discover and configure a VSI instance.

Referring again to two VMs A and B are connected to vSwitch . VM A has two vNICs vNIC A and B associated therewith. VM B has a single vNIC vNIC C associated therewith. In a standard VDP exchange between vSwitch and physical switch via the modules all that is presented to the physical switch is information regarding the three individual vNICs A C which is stored in a VDP table in a memory device internal to or accessible by physical switch . No information regarding the VMs A B connected to the three vNICs A C is provided to the physical switch . This presents obvious difficulties on the fabric network side when it comes to tracking back from a vNIC to the actual VM to which it is associated. Lack of VM identification information complicates network debuggging and troubleshooting for obvious reasons.

In one embodiment an extension mechanism provided in the aforementioned IEEE 802.1Qbg standard is utilized to make information regarding the VMs connected to the vNICs accessible to the physical switch and thereby to the entire network fabric. Once available this information may be used in a variety of ways and for a range of purposes including but not limited to debugging and troubleshooting.

Referring again to two VMs A and B are connected to vSwitch . VM A has two vNICs vNIC A and B associated therewith. VM B has a single vNIC vNIC C associated therewith. In an augmented VDP exchange between vSwitch and physical switch via the modules as described herein the physical switch is presented with information regarding the three individual vNICs A C as well as their associated VMs A B which is stored in an augmented VDP table in a memory device internal to or accessible by physical switch . In this manner the identity of the VM attached to each VSI vNIC is communicated to the physical switch .

The information can be presented in an organizationally unique format such as hierarchically. When a switch involved in the network fabric receives its information it makes the information available through any number of interfaces including for example a Command Line Interface CLI an XML Application Programming Interface API or a REST based interface so that a network manager such as Data Center Network Manager DCNM can query the information and display it to the data center administrator via a simple user friendly interface. As an example in the physical switch now receives a packet including the additional TLV indicating the VM identity. Based on this knowledge the physical switch can present the VM identity e.g. VM Name as well as the hierarchical relationship between a VM and its vNICs. It is possible that the VM Name may not be unique across tenants in which case the VM name can be translated to a UUID and then search can be made on the latter. Alternatively the VM name search can be performed in a more contextual manner by doing the search only within a specific desired tenant org. VDP on the leaf retains information about all the VMs including the VRFs and or tenants to which they belong which information can be leveraged for use by other elements.

In one example implementation various devices involved in implementing the embodiments described herein can include software for achieving the described functions. For example referring to the physical switch and the virtual switch may each include an augmented VDP module comprising software embodied in one or more tangible media for facilitating the activities described herein. Each of the switches may also include or have associated therewith a memory device or memory element for storing information to be used in achieving the functions as outlined herein including for example the VDP table . Additionally the switches may each include a processor capable of executing software or an algorithm such as embodied in modules to perform the functions as discussed in this Specification. These devices may further keep information in any suitable memory element random access memory RAM ROM EPROM EEPROM ASIC etc. software hardware or in any other suitable component device element or object where appropriate and based on particular needs. Any of the memory items discussed herein should be construed as being encompassed within the broad term memory element. Similarly any of the potential processing elements modules and machines described in this Specification should be construed as being encompassed within the broad term processor. Each of the network elements can also include suitable interfaces for receiving transmitting and or otherwise communicating data or information in a network environment.

Note that in certain example implementations the functions outlined herein and in may be implemented by logic encoded in one or more tangible media e.g. embedded logic provided in an application specific integrated circuit ASIC digital signal processor DSP instructions software potentially inclusive of object code and source code to be executed by a processor or other similar machine etc. . In some of these instances a memory element can store data used for the operations described herein. This includes the memory element being able to store software logic code or processor instructions that are executed to carry out the activities described in this Specification. A processor can execute any type of instructions associated with the data to achieve the operations detailed herein in this Specification. In one example the processor as shown in could transform an element or an article e.g. data from one state or thing to another state or thing. In another example the activities outlined herein may be implemented with fixed logic or programmable logic e.g. software computer instructions executed by a processor and the elements identified herein could be some type of a programmable processor programmable digital logic e.g. a field programmable gate array FPGA an erasable programmable read only memory EPROM an electrically erasable programmable ROM EEPROM or an ASIC that includes digital logic software code electronic instructions or any suitable combination thereof.

It should be noted that much of the infrastructure discussed herein can be provisioned as part of any type of network element. As used herein the term network element or network device can encompass computers servers network appliances hosts routers switches gateways bridges virtual equipment load balancers firewalls processors modules or any other suitable device component element or object operable to exchange information in a network environment. Moreover the network elements may include any suitable hardware software components modules interfaces or objects that facilitate the operations thereof. This may be inclusive of appropriate algorithms and communication protocols that allow for the effective exchange of data or information.

In one implementation network elements devices can include software to achieve or to foster the management activities discussed herein. This could include the implementation of instances of any of the components engines logic etc. shown in the FIGURES. Additionally each of these devices can have an internal structure e.g. a processor a memory element etc. to facilitate some of the operations described herein. In other embodiments these management activities may be executed externally to these devices or included in some other network element to achieve the intended functionality. Alternatively these network devices may include software or reciprocating software that can coordinate with other network elements in order to achieve the management activities described herein. In still other embodiments one or several devices may include any suitable algorithms hardware software components modules interfaces or objects that facilitate the operations thereof.

Note that with the example provided above as well as numerous other examples provided herein interaction may be described in terms of multiple components. However this has been done for purposes of clarity and example only. In certain cases it may be easier to describe one or more of the functionalities of a given set of flows by only referencing a limited number of network elements. It should be appreciated that topologies illustrated in and described with reference to the accompanying FIGURES and their teachings are readily scalable and can accommodate a large number of components as well as more complicated sophisticated arrangements and configurations. Accordingly the examples provided should not limit the scope or inhibit the broad teachings of the illustrated topologies as potentially applied to a myriad of other architectures.

It is also important to note that the steps in the preceding flow diagrams illustrate only some of the possible signaling scenarios and patterns that may be executed by or within communication systems shown in the FIGURES. Some of these steps may be deleted or removed where appropriate or these steps may be modified or changed considerably without departing from the scope of the present disclosure. In addition a number of these operations have been described as being executed concurrently with or in parallel to one or more additional operations. However the timing of these operations may be altered considerably. The preceding operational flows have been offered for purposes of example and discussion. Substantial flexibility is provided by communication systems shown in the FIGURES in that any suitable arrangements chronologies configurations and timing mechanisms may be provided without departing from the teachings of the present disclosure.

Although the present disclosure has been described in detail with reference to particular arrangements and configurations these example configurations and arrangements may be changed significantly without departing from the scope of the present disclosure. For example although the present disclosure has been described with reference to particular communication exchanges embodiments described herein may be applicable to other architectures.

Numerous other changes substitutions variations alterations and modifications may be ascertained to one skilled in the art and it is intended that the present disclosure encompass all such changes substitutions variations alterations and modifications as falling within the scope of the appended claims. In order to assist the United States Patent and Trademark Office USPTO and additionally any readers of any patent issued on this application in interpreting the claims appended hereto Applicant wishes to note that the Applicant a does not intend any of the appended claims to invoke paragraph six 6 of 35 U.S.C. section 112 as it exists on the date of the filing hereof unless the words means for or step for are specifically used in the particular claims and b does not intend by any statement in the specification to limit this disclosure in any way that is not otherwise reflected in the appended claims.

