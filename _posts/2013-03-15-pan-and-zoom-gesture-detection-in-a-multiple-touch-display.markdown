---

title: Pan and zoom gesture detection in a multiple touch display
abstract: Systems and methods of zooming and panning an image on a multi-touch enabled computing device are provided. The difference in the mean absolute deviation of consecutive move events is used to determine the scaling factor to apply, and the translation of centroids of consecutive move events is used to determine the pan gesture to apply.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09235338&OS=09235338&RS=09235338
owner: Amazon Technologies, Inc.
number: 09235338
owner_city: Reno
owner_country: US
publication_date: 20130315
---
People are increasingly relying on computing devices such as tablets and smartphones which utilize touch sensitive displays. These displays enable users to enter text select displayed items or otherwise interact with the devices by touching and performing various movements with respect to the display screen as opposed to other conventional input methods such as using a mouse or directional keys on a keyboard. Many mobile computing devices include touch sensitive displays that can detect multiple touches such as where a user uses two or more fingers to provide concurrent input. These multi touch enabled devices are programmed to interpret the movement of multiple touches over a period of time as user inputs to perform certain functions such as panning or zooming the image on the display.

Current touch based pan and zoom gesture detection methods rely on state based counting of touch points in order to distinguish and respond to user interaction. As touch event notifications are received by a software application on the computing device the application maintains a history of these events over time in order to respond appropriately to the user s input. This requires bookkeeping on the part of the application in order to track several different types of events and can result in a fragile decision tree. Accordingly there is a need for improved methods and systems for handling multi touch inputs.

In the following description reference is made to the accompanying drawings which illustrate several embodiments of the present invention. It is understood that other embodiments may be utilized and mechanical compositional structural and electrical operational changes may be made without departing from the spirit and scope of the present disclosure. The following detailed description is not to be taken in a limiting sense and the scope of the embodiments of the present invention is defined only by the claims of the issued patent.

Systems and methods in accordance with various embodiments of the present disclosure may overcome one or more of the aforementioned and other deficiencies experienced in conventional approaches to providing input to a computing device. In particular various approaches described herein enable a touch sensing computing device to interpret a series of touch points as user inputs for modifying an image being displayed. In accordance with certain embodiments the computing device does not need to keep track of finger down touch events or count how many fingers are being used at any moment. Instead the computing device can use the difference in the mean absolute deviation of consecutive move events to determine the how to scale an image. As a result an arbitrary number of touch points can be used to scale an image. In addition a user may execute both a panning function and a zooming function using a single gesture without picking up or putting down fingers.

In this example the portable computing device has a display e.g. a liquid crystal display LCD element operable to display image content to one or more users or viewers of the device. In at least some embodiments the display screen provides for touch or swipe based input using for example capacitive or resistive touch technology. Such a display element can be used to for example enable a user to provide input by touching an area of the display corresponding to an image of a button such as a right or left mouse button touch point etc.

The computing device includes a display element for displaying images using technologies such as for example electronic ink e ink organic light emitting diode OLED or liquid crystal display LCD . In other embodiments the display element may comprise an image projector such as for example a CRT projector LCD projector or LED projector. The image projector projects an image generated by the computing device onto a separate surface. In this embodiment the separate surface may not incorporate a touch sensitive surface such as a capacitive or resistive touch sensitive surface. In this case the touch points may be detected by for example image capture elements oriented so as to detect the user s movement relative to the projected image. These image capture elements may be incorporated in the computing device or may be separate devices wired or wirelessly coupled to the computing device .

The computing device may include an imaging element such as one or more cameras configured to capture an image of people or objects in the vicinity of the device . In at least some embodiments the computing device can use the image information detected by the imaging element to determine gestures or motions of the user which will enable the user to provide input through the portable device without having to actually contact and or move the portable device. The imaging element also can be used to determine the surroundings of the computing device . The imaging element can include any appropriate technology such as a CCD image capture element having a sufficient resolution focal range and viewable area to capture the desired images such as an image of the user when the user is operating the device .

The computing device may also include an audio element such as one or more audio speakers and or audio capture elements capable of capturing audio data. In some devices there may be only one microphone while in other devices there might be additional audio capture elements e.g. microphone on other sides and or corners of the device or in other appropriate locations. The microphones may be used to facilitate voice enabled functions such as voice recognition digital recording etc. The audio speakers may perform audio output. In some embodiments the audio speaker s may reside separately from the device.

The computing device may also include a positioning element such as motion position or orientation determining element that provides information such as a position direction motion or orientation of the device . This positioning element can include for example accelerometers inertial sensors electronic gyroscopes electronic compasses or GPS elements. Various types of motion or changes in orientation can be used to provide input to the device that can trigger at least one control signal for another device.

The computing device can include one or more input elements operable to receive inputs from a user. The input elements can include for example a push button touch pad touch screen wheel joystick keyboard mouse trackball keypad or any other such device or element whereby a user can provide inputs to the computing device . These input elements may be incorporated into the computing device or operably coupled to the computing device via wireless interface.

For computing devices with touch sensitive displays the input elements can include a touch sensor that operates in conjunction with the display element to permit users to interact with the image displayed by the display element using touch inputs e.g. with a finger or stylus . In computing devices with LCD displays the touch sensor may comprise a touchscreen panel positioned on top of an LCD panel to form the display . Alternatively the touch sensor may be integrated with the LCD panel.

In embodiments where the display element comprises a projector that projects an image onto a projection surface the touch sensor may comprise a touch sensing surface adjacent to the projection surface. Alternatively the touch sensor may comprise another form of sensor or detector for detecting a user s interaction with the projection surface. For example an imaging element such as a camera may be used to detect the position of the user s finger or hand relative to the projection surface.

The touch sensor may also include a touch controller which can be for example a low power microcontroller dedicated to sensing touches and or objects in proximity to the display . The touch controller is configured to analyze the changes in capacitance and or electric field in order to detect the presence and location of objects in proximity of the display .

The computing device may also include at least one communication interface comprising one or more wireless components operable to communicate with one or more separate devices within a communication range of the particular wireless protocol. The wireless protocol can be any appropriate protocol used to enable devices to communicate wirelessly such as Bluetooth cellular or IEEE 802.11. It should be understood that the computing device may also include one or more wired communications interfaces for coupling and communicating with other devices.

In accordance with some embodiments implementing a mutual capacitance touch sensitive display the columns of the grid are configured to be transmitters that transmit an electronic signal e.g. emit an electric field and the rows are configured as receivers that receive the electronic signal. When a conductive object such as a finger contacts or is in close proximity to the display the object reduces the amount of signal that the receiver is receiving. Based on such reduced signal being detected the touch controller can determine the location of the object on the screen at the intersection of the transmitter and receiver. Mutual capacitance thus enables the controller to determine the locations of multiple touches based on changes in capacitance at each intersection.

In accordance with some embodiments implementing a self capacitance touch sensitive display there are no transmitters or receivers. Instead each sensor line is treated as a conductive plate. In this mode the touch controller is capable of measuring the base self capacitance of each sensor line. When an object such as a finger touches one or more of the sensor lines or comes into close proximity with the sensor lines the capacitance of the object is added to the capacitance of the sensor line. The line thus sees an increase in capacitance which is detected by the touch controller. Based on the intersection of the lines which have seen an increase in capacitance the touch controller is able to determine the location of the object on the screen.

In various embodiments the plurality of sensors of the touch sensitive display can be contained in a single sensor layer or can be distributed between multiple layers of the display . For example in some embodiments the sensor rows may be contained in one layer while the sensor columns are contained in a separate sensor layer. In other embodiments both rows and columns are contained in the same layer.

Next a first event analysis is performed. The first event analysis comprises identifying the central tendency of the first set of touch points in step and identifying a dispersion value for the first set of touch points in step .

The central tendency can be any location within the coordinate space representing an average of the touch points in a set of touch points. In some embodiments the central tendency can be determined by calculating the centroid of the touch points in the set of touch points. The centroid is the arithmetic mean of all of the touch points in the set of touch points detected at a particular moment. For a two dimensional image the centroid can be defined by an X Y coordinate location within the display. The centroid C of a set of k points x x x . . . x may be calculated as follows 

The dispersion value can be any value representing the dispersion of touch points across the coordinate space . In some embodiments the dispersion value can be determined by calculating the mean absolute deviation of all of the touch points in the first set of touch points. The mean absolute deviation of a set of data can be determined by calculating the mean of the absolute deviations of the set of data about the data s mean. In a two dimensional coordinate space where each touch point has an X Y coordinate location the mean absolute deviation is calculated as a single value not a coordinate pair. In other embodiments the dispersion value can be determined by calculating the average or median absolute deviation of all of the touch points in the first set of touch points.

After a period of time has passed a second set of touch points is received in step . shows an example of a second set of touch points . This can represent the touch points that may be detected when a user first places two fingers onto touch points and respectively and then slides each of those fingers along paths respectively to arrive at the touch point locations 

The frequency with which each new set of touch points is received can vary in different embodiments. In various embodiments a new set of touch points may be received for example 60 times per second 20 times per second once per second or at any other desired frequency that is supported by the hardware and operating system of the computing device .

Next a second event analysis is performed. The second event analysis comprises identifying the central tendency of the second set of touch points in step and identifying a dispersion value for the second set of touch points in step . The determination of the central tendency in step and the dispersion value in step can be performed in the same way as in steps and respectively. In the centroid of the second set of touch points is shown and the mean absolute deviation of the second set of touch points is represented by circle centered at centroid and having a radius corresponding to the value of the mean absolute deviation of the second set of touch points. Because the touch points in the second set of touch points are located farther apart than the touch points in the first set of touch points the mean absolute deviation of the second set of touch points is larger represented by a circle having a larger radius.

In step the centroids of the first and second sets of touch points are compared. In step the dispersion values of the first and second sets of touch points are compared. In step a gesture is determined based on the compared centroids and dispersion values. In step the image displayed on the display is modified in accordance with the determined pan and or zoom gestures.

In step the translation from the first centroid to the second centroid may be used to define a pan gesture from the user. This pan gesture represents a direction and magnitude of image translation desired by the user. In response to receiving the pan gesture the application or operating system performs a pan operation to an image currently being displayed. The computing device can generate an image or other visual representation of data that fills a graphical space larger than the displayed element within the field of view of the display . The panning operation causes the computing device to translate the image across the field of view on the display to expose other portions of the image. Examples of panning include one dimensional panning such as the scrolling of a document up and down in a window or two dimensional such as the translation of an image across the user s field of view in the display . For example if the user is viewing a map image in a mapping application the pan gesture defines how the image of the map should be translated across the screen. In another embodiment if the user is viewing a document in a word processing application the pan gesture defines the direction and magnitude of scrolling of that document. The pan gesture may be interpreted in various ways depending on the application being used.

The comparison of dispersion values in step may be used to determine a zoom gesture. In response to receiving the zoom gesture the application or operating system performs a zoom operation to an image currently being displayed. The zoom operation causes the computing device to increase or decrease the size of an image within the field of view on the display . The zoom gesture may be determined by calculating a zoom scaling factor which determines the magnitude of a magnification or shrinking of an image displayed on the display . In one embodiment the zoom scaling factor may be calculated as a function of the ratio of the second mean absolute value to the first mean absolute value. A large zoom scaling factor can be interpreted as a user s request for a large zoom a small scaling factor can be interpreted as a user s request for a small zoom and a scaling factor of less than one can be interpreted as a user s request to shrink the image. In some embodiments the amount of zoom determined by the gesture is equal to the ratio of the second mean absolute value to the first mean absolute value. In other words the zoom scaling factor is applied on a 1 1 basis with the change in mean absolute value to determine the amount of zoom so if the ratio of the second mean absolute value to the first mean absolute value is 2 the determined gesture results in a 200 zoom. In other embodiments the zoom scaling factor may be adjusted by some amount to determine the amount of zoom applied and need not be applied on a 1 1 basis.

The zoom gesture may perform different functions depending on the application being used. For example in some applications the zoom gesture may cause the application to resize the text being displayed on the display where a zoom scaling factor greater than one causes an increase in font size and a zoom scaling factor less than one causes a decrease in text size. In other applications the zoom gesture may cause the application to resize images or other objects being displayed on the display .

In the embodiments described above with respect to only two touch points exist at each point in time. However the determination of the dispersion value and central tendencies of each set of touch points may be performed using any number of touch points. As a result it is not necessary for an application to maintain a record of all of the touch points at each point in time over a extended period of time. The determination of the user s gesture input can be made using only a comparison of the subsequent dispersion values and central tendencies. The exact locations of each of the touch points need not be stored and can be discarded after the dispersion value and central tendency is calculated for each set of touch points.

In addition if the user chooses to use more than two fingers or to use two palms to make a pan or zoom gesture the dispersion value and central tendency of those touch points can still be used to determine the user s desired gesture.

In many conventional touch screen devices the systems are programmed to interpret multi touch inputs as a single motion such as pan or a zoom but not both simultaneously. A single gesture such as the one depicted in is typically interpreted as either a pan or a zoom but not both. In these types of systems if the user begins moving his or her fingers across the screen in a way that is initially interpreted as a panning gesture then even if the user begins to spread his or her fingers apart in a way that would normally be interpreted as a zoom gesture the computing system will remain locked in a panning state until the user removes his or her fingers from the touch screen.

In accordance with some embodiments of the present invention the application may perform both a pan gesture and zoom gesture simultaneously. In some embodiments the comparison of the first and second centroids can be used to identify a reference point about which the image is scaled. In the embodiment shown in the reference point is the midpoint location between the first centroid and second centroid . The midpoint location can be calculated by identifying the average of the coordinates of the first centroid and second centroid or the midpoint of a line connecting the first centroid and second centroid . For applications where a zoom gesture is determined the reference point is set as the fixed point for determining how to magnify or shrink the image.

In other embodiments the location of the reference point can be determined in other ways and need not merely be the midpoint of the first and second centroids. For example the speed of the user s finger movement magnitude of centroid translation and magnitude of the scaling factor can also be used as factors in identifying the location of the reference point. If the user is scaling the image by a large amount relative to the magnitude of the pan gesture then the device may interpret this as being an indication that the user wishes to zoom the image about a reference point closer to the first centroid. On the other hand if the user is scaling the image by a small amount relative to the magnitude of the pan gesture then the device may interpret this as being an indication that the user wishes to zoom the image about a reference point closer to the second centroid.

In other embodiments where the device is configured to simultaneously perform a pan gesture and a zoom gesture it may be desirable to have minimum thresholds before initiating each of those gestures. For example if the magnitude of the translation from the first centroid to the second centroid is less than a predetermined minimum pan threshold then the device may interpret the translation as insignificant and will not interpret this as a pan gesture. Similarly if the difference between the first and second dispersion values is less than a predetermined minimum zoom threshold then the device may interpret the change in dispersion as insignificant and will not interpret this as a zoom gesture.

In yet other embodiments the determination of whether to interpret a series of touch events as either a pan gesture or a zoom gesture or both can be made by comparing the magnitude of the centroid translation to the magnitude of the change in dispersion values. In other words if the user s input reflects a very large translation relative to a small change in dispersion the device may interpret this as primarily a pan gesture and will not generate a zoom gesture despite the fact that there was a small change in dispersion value. Similarly if the user s input reflects a very small translation relative to a large change in dispersion the device may interpret this as primarily a zoom gesture and will not generate a pan gesture despite the fact that there was a small translation from the first to second centroid.

In yet other embodiments the device may be configured to disregard a determined gesture if it exceeds a certain maximum amount. This may be a desirable method of handling hardware and or software errors in the touch detection. For example if the magnitude of the translation from the first centroid to the second centroid exceeds a predetermined error threshold the device may disregard that translation and will not generate a corresponding pan gesture. Similarly if the difference between the first and second dispersion values is greater than a predetermined error threshold the device may disregard that scaling and will not generate a corresponding zoom gesture. In some embodiments the error thresholds may be configurable. For example the error threshold may be a function of the screen size of the display . If the screen of the device is very small then the user may be more likely to accidentally make a large swipe motion across the face of the display . In such a case it may be desirable for the error threshold to be lower to accommodate the increased likelihood that inadvertent touch inputs may be received and to avoid a large inadvertent translation or scaling of the image. In other embodiments the operating system or application may be configured to detect information regarding the hardware of the device and use that information to set the error thresholds. For example if the device has a display with a highly sensitive touch display then it may be desirable to set the error threshold higher.

It may also be desirable to implement a minimum threshold which must be exceeded before the determined gesture is executed. For example where there is signal noise generated in the touch sensor it may be desirable for the system to disregard any perceived touch inputs that do not meet a minimum threshold requirement. In some embodiments if the detected touch input indicates a movement of less than approximately 1 mm that input may be disregarded. The minimum threshold may vary or be adjusted based on user testing display size and or hardware specifications such as touch resolution or touch input.

The computing device illustrated in is one example of a system implementing aspects of the present invention. Various features and components of the device can vary in different embodiments. As shown in the device can include one or more image capture elements for purposes such as image and or video capture. The image capture elements can also be used for other purposes such as to determine motion and receive gesture input. While the portable computing device in this example includes one image capture element on the front of the device as shown in and one image capture element on the back of the device as shown in it should be understood that image capture elements could also or alternatively be placed on the sides or corners of the device and that there can be any number of image capture elements of similar or different types. Each image capture element may be for example a camera a charge coupled device CCD a motion detection sensor or an infrared sensor or can utilize another image capturing technology.

The computing device may also include a power supply such as for example a rechargeable battery operable to be recharged through conventional plug in approaches or through other approaches such as capacitive charging. Various other elements and or combinations are possible as well within the scope of various embodiments.

Embodiments of the present invention may provide various advantages not provided by prior art systems. The computing devices may be capable of simultaneously zooming and panning an image without the need for maintaining detailed records of every touch point detected. Accordingly the computing devices may distinguish and respond to user interaction without state based counting of individual touch points.

While the invention has been described in terms of particular embodiments and illustrative figures those of ordinary skill in the art will recognize that the invention is not limited to the embodiments or figures described. For example in many of the embodiments described above the touch points are detected by the operating system with the gesture determinations being made at the application level. In other embodiments the operating system may detect the touch points and determine the corresponding zoom and or pan gestures. These gesture events may then be passed to gesture listener objects at the application level.

Some portions of the detailed description which follows are presented in terms of procedures steps logic blocks processing other symbolic representations of operations on data bits that can be performed on computer memory. Each step may be performed by hardware software firmware or combinations thereof.

The program logic described indicates certain events occurring in a certain order. Those of ordinary skill in the art will recognize that the ordering of certain programming steps or program flow may be modified without affecting the overall operation performed by the preferred embodiment logic and such modifications are in accordance with the various embodiments of the invention. Additionally certain of the steps may be performed concurrently in a parallel process when possible as well as performed sequentially as described above.

Therefore it should be understood that the invention can be practiced with modification and alteration within the spirit and scope of the appended claims. The description is not intended to be exhaustive or to limit the invention to the precise form disclosed. It should be understood that the invention can be practiced with modification and alteration and that the invention be limited only by the claims and the equivalents thereof.

