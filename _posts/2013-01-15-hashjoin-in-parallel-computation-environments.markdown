---

title: Hash-join in parallel computation environments
abstract: According to some embodiments, a system and method for a parallel join of relational data tables may be provided by calculating, by a plurality of concurrently executing execution threads, hash values for join columns of a first input table and a second input table; storing the calculated hash values in a set of disjoint thread-local hash maps for each of the first input table and the second input table; merging the set of thread-local hash maps of the first input table, by a second plurality of execution threads operating concurrently, to produce a set of merged hash maps; comparing each entry of the merged hash maps to each entry of the set of thread-local hash maps for the second input table to determine whether there is a match, according to a join type; and generating an output table including matches as determined by the comparing.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09177025&OS=09177025&RS=09177025
owner: SAP SE
number: 09177025
owner_city: Walldorf
owner_country: DE
publication_date: 20130115
---
Some embodiments relate to a data structure. More specifically some embodiments provide a method and system for a data structure and use of same in providing a relational data join operation in parallel computing environments.

A number of presently developed and developing computer systems include multiple processors in an attempt to provide increased computing performance. Advances in computing performance including for example processing speed and throughput may be provided by parallel computing systems and devices as compared to single processing systems that sequentially process programs and instructions.

For parallel join processes a number of approaches have been proposed. However the previous approaches each include sequential operations and or synchronization operations such as locking to avoid inconsistencies or lapses in data coherency. Thus prior proposed solutions for parallel join operations and processes in parallel computation environments with shared memory either contain a sequential step s and or require some sort of synchronization on the data structures.

Accordingly a method and mechanism for efficiently processing join processes in parallel computation environments are provided by some embodiments herein.

In an effort to more fully and efficiently use the resources of a particular computing environment a data structure and techniques of using that data structure may be developed to fully capitalize on the design characteristics and capabilities of that particular computing environment. In some embodiments herein a data structure and techniques for using that data structure i.e. algorithms are provided for efficiently using the data structure disclosed herein in a parallel computing environment with shared memory.

As used herein the term parallel computation environment with shared memory refers to a system or device having more than one processing unit. The multiple processing units may be processors processor cores multi core processors etc. All of the processing units can access a main memory i.e. a shared memory architecture . All of the processing units can run or execute the same program s . As used herein a running program may be referred to as a thread. Memory may be organized in a hierarchy of multiple levels where faster but smaller memory units are located closer to the processing units. The smaller and faster memory units located nearer the processing units as compared to the main memory are referred to as cache.

Processing units and communicates with a shared memory via a system bus . System bus also provides a mechanism for the processing units to communicate with a storage device . Storage device may include any appropriate information storage device including combinations of magnetic storage devices e.g. a hard disk drive optical storage devices and or semiconductor memory devices for storing data and programs.

Storage device stores a program for controlling the processing units and and query engine application for executing queries. Processing units and may perform instructions of the program and thereby operate in accordance with any of the embodiments described herein. For example the processing units may concurrently execute a plurality of execution threads to build the index hash table data structures disclosed herein. Furthermore query engine may operate to execute a parallel join operation in accordance with aspects herein in cooperation with the processing units and by accessing database . Program and other instructions may be stored in a compressed uncompiled and or encrypted format. Program may also include other program elements such as an operating system a database management system and or device drivers used by the processing units and to interface with peripheral devices.

In some embodiments storage device includes a database to facilitate the execution of queries based on input table data. The database may include relational data tables data structures e.g. index hash tables rules and conditions for executing a query in a parallel computation environment such as that of .

In some embodiments the data structure s disclosed herein as being developed for use in parallel computing environments with shared memory is referred to as a parallel hash table. In some instances the parallel hash table may also be referred to as a parallel hash map. In general a hash table may be provided and used as index structures for data storage to enable fast data retrieval. The parallel hash table disclosed herein may be used in a parallel computation environment where multiple concurrently executing i.e. running threads insert and retrieve data in tables. Furthermore a hash join algorithm that uses the parallel hash tables herein is provided for computing a join in a parallel computation environment.

As shown in DBS is a server. DBS further includes a database management system DBMS . DBMS may comprise software e.g. programs instructions code applications services etc. that controls the organization of and access to database that stores data. Database may include an internal memory an external memory or other configurations of memory. Database may be capable of storing large amounts of data including relational data. The relational data may be stored in tables. In some embodiments a plurality of clients such as example client may communicate with DBS via a communication link e.g. a network and specified application programming interfaces APIs . In some embodiments the API language provided by DBS is SQL the Structured Query Language. Client may communicate with DBS using SQL to for example create and delete tables insert update and delete data and query data.

In general a user may submit a query from client in the form of a SQL query statement to DBS . DBMS may execute the query by evaluating the parameters of the query statement and accessing database as needed to produce a result . The result may be provided to client for storage and or presentation to the user.

One type of query is a join query. The join query may operate to combine fields from two tables by using values common to each table. As will be explained in greater detail below a parallel join algorithm process or operation may be used to compute SQL joins. In general with reference to some embodiments herein may include client wanting to join the data of two tables stored in database e.g. a user at client may desire to know all customers who bought a certain product . Client may connect to DBS and issue a SQL query statement that describes the join. DBMS may create a executable instance of the parallel join algorithm herein provide it with the information needed to run the parallel join algorithm e.g. the name of tables to access columns to join etc. and run the parallel join operation or algorithm. In the process of running the parallel join algorithm herein may create an index hash map to keep track of intermediate result data. An overall result comprising a result table may be computed based on the index hash map s containing the intermediate results. The overall parallel join result may be transmitted to client .

As an extension of DWHs may be built on top of DBSs. Thus a use case of a DWH may be similar in some respects to DBS of .

The computation environment of may include a plurality of processors that can operate concurrently in parallel and include a device or system similar to that described in . Additionally the computation environment of may have a memory that is shared amongst the plurality of processors for example like the system of . In order to fully capitalize on the parallel processing power of such a computation environment the data structures used by the system may be designed developed or adapted for being efficiently used in the parallel computing environment.

A hash table is a fundamental data structure in computer science that is used for mapping keys e.g. the names of people to the associated values of the keys e.g. the phone number of the people for fast data look up. A conventional hash table stores key value pairs. Conventional hash tables are designed for sequential processing.

However for parallel computation environments there exists a need for data structures particularly suitable for use in the parallel computing environment. In some embodiments herein the data structure of an index hash map is provided. In some aspects the index hash map provides a lock free cache efficient hash data structure developed to parallel computation environments with shared memory. In some embodiments the index hash map may be adapted to column stores.

In a departure from conventional hash tables that store key value pairs the index hash map herein does not store key value pairs. The index hash map herein generates key index pairs by mapping each distinct key to a unique integer. In some embodiments each time a new distinct key is inserted in the index hash map the index hash map increments an internal counter and assigns the value of the counter to the key to produce a key index pair. The counter may provide at any time the cardinality of an input set of keys that have thus far been inserted in the hash map. In some respects the key index mapping may be used to share a single hash map among different columns or value arrays . For example for processing a plurality of values distributed among different columns the associated index for the key has to be calculated just once. The use of key index pairs may facilitate bulk insertion in columnar storages. Inserting a set of key index pairs may entail inserting the keys in a hash map to obtain a mapping vector containing indexes. This mapping vector may be used to build a value array per value column.

Referring to input data is illustrated in including a key array . For each distinct key from key array the index hash map returns an index i.e. a unique integer as seen in . When all of the keys from a column for example have been inserted in the hash map the mapping vector of results. To achieve a maximum parallel processor utilization the index hash maps herein may be designed to avoid locking when being operated on by concurrently executing threads by producing wide data independence. In some embodiments index hash maps herein may be described by a framework defining a two step process. In a first step input data is split or separated into equal sized blocks and the blocks are assigned to worker execution threads. These worker execution threads may produce intermediate results by building relatively small local hash tables or hash maps. The local hash maps are private to the respective thread that produces it. Accordingly other threads may not see or access the local hash map produced by a given thread.

In a second step the local hash maps including the intermediate results may be merged to obtain a global result by concurrently executing merger threads. When accessing and processing the local hash maps each of the merger threads may only consider a dedicated range of hash values. The merger threads may process hash disjoint partitions of the local hash maps and produce disjoint result hash tables that may be concatenated to build an overall result.

The second step of the data structure framework herein is depicted in at S. At S the local hash maps are merged. The merging of the local hash maps produces a set of disjoint result hash tables or hash maps.

In some embodiments when accessing and processing the local hash maps each of the merger threads may only consider a dedicated range of hash values. From a logical perspective the local hash maps may be considered as being partitioned by their hash value. One implementation may use for example some first bits of the hash value to form a range of hash values. The same ranges are used for all local hash maps thus the partitions of the local hash maps are disjunctive. As an example if a value a is in range 5 of a local hash map then the value will be in the same range of other local hash maps. In this manner all identical values of all local hash maps may be merged into a single result hash map. Since the partitions are disjunctive the merged result hash maps may be created without a need for locks. Additionally further processing on the merged result hash maps may be performed without locks since any execution threads will be operating on disjunctive data.

In some embodiments the local index hash maps providing the intermediate results may be of a fixed size. Instead of resizing a local hash map the corresponding worker execution thread may replace its local hash map with a new hash map when a certain load factor is reached and place the current local hash map into a buffer containing hash maps that are ready to be merged. In some embodiments the size of the local hash maps may be sized such that the local hash maps fit in a cache e.g. L2 or L3 . The specific size of the cache may depend on the sizes of caches in a given CPU architecture.

In some aspects insertions and lookups of keys may largely take place in cache. In some embodiments over crowded areas within a local hash map may be avoided by maintaining statistical data regarding the local hash maps. The statistical data may indicate when the local hash map should be declared full independent of an actual load factor . In some aspects and embodiments the size of a buffer of a computing system and environment holding local hash maps ready to be merged is a tuning parameter wherein a smaller buffer may induce more merge operations while a larger buffer will necessarily require more memory.

In some embodiments a global result may be organized into bucketed index hash maps where each result hash map includes multiple fixed size physical memory blocks. In this configuration cache efficient merging may be realized as well as memory allocation being more efficient and sustainable since allocated blocks may be shared between queries. In some aspects when a certain load factor within a global result hash map is reached during a merge operation the hash map may be resized. Resizing a hash map may be accomplished by increasing its number of memory blocks. Resizing of a bucketed index hash map may entail repositioning the entries of the hash map. In some embodiments the maps hash function may be chosen such that its codomain increases by adding further least significant bits if needed during a resize operation. In an effort to avoid too many resize operations an estimate of a final target size of the map may be determined before an actual resizing of the hash map.

In some embodiments the index hash map framework discussed above may provide an infrastructure to implement parallelized query processing algorithms or operations. One embodiment of a parallelized query processing algorithm includes a hash based equi join as will be discussed in greater detail below.

In some embodiments a join algorithm herein is hash based. This hash based join algorithm may be used to combine two input tables. In accordance with some aspects and embodiments the input tables are hashed by multiple execution threads using the index hash table framework described hereinabove. Since the result tables of the index hash tables are disjoint all subsequent processing steps performed on the disjoint result tables can be executed in parallel by one thread per partition without a need to use locks.

In some embodiments the resulting table may not be constructed by copying all values to their final positions in the columns. Instead the resulting table may be a virtual table. The virtual table may hold references to the original columns and have a vector of all rows that match each other according to the join type being performed. Upon access to a row a call to do so may be routed transparently to the respective row of the original column. A benefit of the virtual result is that it is not necessary to copy the data.

In some embodiments the hash based join algorithm and methods herein use a data allocation and organization method that does not need to know the number of distinct values in advance.

In an effort to fully utilize the resources of parallel computing environments with shared memory a join operation should be computed and determined in parallel. In an instance the join is not computed in parallel the processing performance for the join would be bound by the speed of a single processing unit instead of being realized by the multiple processing units available in the parallel computing environment.

In two input tables are hashed using the index hash table framework. For purposes of clarity only one of the two input tables is depicted in since the hashing of the input tables is the same for each input table. As an initial step multiple concurrently running execution threads calculate hash values for the join columns of both tables. The join columns are the columns specified in a join statement e.g. a SQL join query statement . These hash values are inserted into thread local hash maps in accordance with the index hash table framework discussed above. One set of thread local hash maps are produced for the smaller table and one for the bigger table not shown . As illustrated in the input table is divided into partitions such as partitions and and a plurality of execution threads operate to produce disjoint local thread hash maps . The hash map for the second bigger input table is provided at and is similar to the hash maps . In addition to providing key index pairs the thread local hash maps also include the row number or row identifier of the original column corresponding to the value referenced by each hash map entry.

Proceeding with the flow of the join operation in the thread local hash maps of the smaller table are merged into one hash map per partition. This aspect may be accomplished in some embodiments by one thread per core operating to merge all partitions that belong to each other into a single hash map. Merged hash tables for the smaller input table are depicted at and . The merging of the thread local hash maps of the smaller table may be accomplished by a plurality of execution threads operating in parallel.

In the example of the set of hash maps of the bigger input table are not merged. Each entry of the bigger table hash maps are probed against or compared to the merged hash maps of the smaller table. If a match is found then both tables have rows to be joined with each other.

However while there is a match between the hash maps the matched rows each have a different identifier. Therefore the matched rows are aligned or otherwise reconciled so that all corresponding rows of both tables can be retrieved by using a single identifier. In an instance a value exists only in one of the two tables then it is kept only if one of the outer join types e.g. left outer right outer full outer are being performed.

Based on the row identifiers determined for the matches an output table may be generated. For example all matching rows are added to the output table . In an instance of outer joins the lines without matches but satisfying the outer join operation are added to the output table as well with the addition of a NULL indicator.

In some embodiments the hash maps of the bigger or second input table may be merged instead of the smaller first input table or in addition to the merging of the smaller first input table. Turning to a discussion of the data structures used in some embodiments herein it is again noted that each entry in a hash map refers to the row in the original columns where the corresponding value is stored i.e. the row number is stored with the hash map entries . However the number of distinct values in a column is unknown. Therefore it is not possible to pre allocate a reasonable amount of memory in advance. Moreover hash maps are merged and therefore the rows to which entries in the hash maps refer to have to be merged as well if two hash maps store the same value. A data structure capable of accommodating these concerns is illustrated in .

The data structure of includes an array as well as head and tail records and which are accessed through the index given by the hash map framework. shows a line through input table indicating that the input table is partitioned and each part is processed by one of the execution threads and . Each execution thread processes a not necessarily consecutive portion of input table and fills the corresponding part of array .

For each input table an integer array is allocated. Each field number of array corresponds to a row in input table . For each entry in a hash map head and tail values are stored that describe starting and end points in that array. As an example if the single value a is in row of an input table thread produces a stop indicator stored at field of the array. The position 5 is stored as head and tail values for the hash map value a . When all positions of value 5 are queried the tail is used to access the array. Position is the first matching row of the input table. Since the position 5 contains only the stop indicator it is known that only row has the value a .

In the instance more than one row of an input table has a certain value each field in the array having the particular value will store the row number of the next field containing that value. For example rows and operated on by thread store the value a in the input table . In data structure 15 will be stored as the tail value and 10 is stored as the head value. In the array field will store 10 and field will store a stop indicator. To retrieve all rows where value a is stored rows and can be retrieved by following the references in the array . provides an illustrative depiction of an example of two data structures and that store head and tail values. When two hash maps are merged and both store the same value the sequences in the array have to be merged as well. provides an illustrative depiction of a merged array according to some embodiments herein where the two sequences produced by the two threads and have been merged. The merging of the two data structures and results in the combined data structure with one head value and one tail value. Array reflects the merging. The head and tail values for the merged data structures are updated so that they point to the new head and tail of the combined sequences.

As described above including the discussion of the hash based parallel join algorithm herein includes the phases of 1 hashing the smaller table 2 hashing the bigger table 3 merging the hash maps of the smaller table 4 probing the entries of hash maps for the bigger table against the merged hash maps and 5 building the output table.

In some embodiments the two input tables may be hashed in any order. Therefore worker hasher threads can process parts of the input tables in any logical order to create the hash maps for the input tables. In some embodiments the hash maps of the bigger table may be created after a process of merging the hash maps for the smaller table. provides an illustrative depiction of a threading scenario . In this example parts of the smaller input table are hashed at parts of the larger input table are hashed at and then the hash maps of the smaller input table are merged at . Thereafter the merged hash maps of the smaller input table and the hash maps of the bigger input table are probed at followed by the building of the output table at .

In order to hash the two input tables execution threads pick up parts e.g. chunks of data and create hash maps. This results in two separate sets of hash maps one set for each input table. Each hash map is then partitioned. In some embodiments as many parts are created as cores are available in a parallel computation system e.g. system . After the partitioning of the hash tables all subsequent steps are then executed for all parts that belong to each other.

In some aspects a thread for a specific part considers all parts of all input hash maps merges the parts of the required hash maps probes the values and builds the output table. As an example consider a system with 32 processing cores. In this example 32 threads may be started to hash the input tables. When all input tables are hashed the thread for as an example part considers all hash maps of the smaller input tables and merges their part into a new hash map which is then dedicated for all values for part . Thread then considers part of each hash map of the bigger table. The part portions of the bigger input table are then probed against the part portions of the merged hash map. Then the thread builds the output of part .

In some embodiments each part i.e. partitioned portion has approximately the same size since the hash function may equally distribute values. However the sizes may still differ based on the input data.

The process steps of merging probing and building the output table are closely coupled since each process step cannot start before a previous step has finished. Therefore it may be advantageous to start directly merging the smaller table hash maps when the hash maps are built. If one part being hashed is smaller than the other parts being hashed in parallel the responsible thread may use the time freed by processing of the smaller part of the smaller table to hash the bigger table. This aspect is illustrated in where threading scenario illustrates the merging process step is executed after the hashing of the smaller table at and before the bigger table is hashed at . When all parts are merged at the threads each get part of the bigger table and start hashing them at until no portions of the bigger table remain un hashed. The threads then proceed with the probing phase at .

In both thread has the longest runtime. The processing finishes when all threads are done. The total runtime in is shorter than in . Thus modifying the order of executing the phases may have a positive impact on the overall runtime if the extra time for a bigger part is longer than the time that is required to hash a portion of the input table.

The earliest time permissible to start the merging of the smaller table hash tables is right after all of the hash maps for the smaller table are created. This is illustrated in at that includes a white gap i.e. no processing activity for thread . In such a case the gap in time in processing of the smaller table may be used for hashing a chunk of the bigger table. Since merging should start as early as possible the chunk size is reduced in this case.

The threading task distribution may be further optimized. For example the size of a part may be determined during the merge phase. If the part of the smaller table is bigger due to the distribution of values by the hash function it is likely that the parts of the bigger table will also be bigger. However even if this is not the case the probing and output phase may take longer due to the bigger merged tables. Therefore the amount of chunks or portions of the bigger table processed by the thread that handles the bigger parts can be reduced.

Each system described herein may be implemented by any number of devices in communication via any number of other public and or private networks. Two or more of the devices herein may be co located may be a single device or may be located remote from one another and may communicate with one another via any known manner of network s and or a dedicated connection. Moreover each device may comprise any number of hardware and or software elements suitable to provide the functions described herein as well as any other functions. Other topologies may be used in conjunction with other embodiments.

All systems and processes discussed herein may be embodied in program code stored on one or more computer readable media. Such media may include for example a floppy disk a CD ROM a DVD ROM magnetic tape and solid state Random Access Memory RAM or Read Only Memory ROM storage units. According to some embodiments a memory storage unit may be associated with access patterns and may be independent from the device e.g. magnetic optoelectronic semiconductor solid state etc. Moreover in memory technologies may be used such that databases etc. may be completely operated in RAM memory at a processor. Embodiments are therefore not limited to any specific combination of hardware and software.

Embodiments have been described herein solely for the purpose of illustration. Persons skilled in the art will recognize from this description that embodiments are not limited to those described but may be practiced with modifications and alterations limited only by the spirit and scope of the appended claims.

