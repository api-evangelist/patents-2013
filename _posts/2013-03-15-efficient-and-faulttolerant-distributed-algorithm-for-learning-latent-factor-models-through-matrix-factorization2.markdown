---

title: Efficient and fault-tolerant distributed algorithm for learning latent factor models through matrix factorization
abstract: A method for estimating model parameters. The method comprises receiving a data set related to a plurality of users and associated content, partitioning the data set into a plurality of sub data sets in accordance with the users so that data associated with each user are not partitioned into more than one sub data set, storing each of the sub data sets in a separate one of a plurality of user data storages, each of said data storages being coupled with a separate one of a plurality of estimators, storing content associated with the plurality of users in a content storage, where the content storage is coupled to the plurality of estimators so that the content in the content storage is shared by the estimators, and estimating, asynchronously by each estimator, one or more parameters associated with a model based on data from one of the sub data sets.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09535938&OS=09535938&RS=09535938
owner: EXCALIBUR IP, LLC
number: 09535938
owner_city: Sunnyvale
owner_country: US
publication_date: 20130315
---
This application is a National Stage of PCT US2013 032385 filed Mar. 15 2013 and designating the United States hereby expressly incorporated by reference in its entirety.

The present teaching relates to methods and systems for providing content. Specifically the present teaching relates to methods and systems for providing online content.

The Internet has made it possible for a user to electronically access virtually any content at anytime and from any location. With the explosion of information it has become more and more important to provide users with information that is relevant to the user and not just information in general. Further as users of today s society rely on the Internet as their source of information entertainment and or social connections e.g. news social interaction movies music etc it is critical to provide users with information they find valuable.

Efforts have been made to attempt to allow users to readily access relevant and on the point content. For example topical portals have been developed that are more subject matter oriented as compared to generic content gathering systems such as traditional search engines. Example topical portals include portals on finance sports news weather shopping music art film etc. Such topical portals allow users to access information related to subject matters that these portals are directed to. Users have to go to different portals to access content of certain subject matter which is not convenient and not user centric.

Another line of efforts in attempting to enable users to easily access relevant content is via personalization which aims at understanding each user s individual likings interests preferences so that an individualized user profile for each user can be set up and can be used to select content that matches a user s interests. The underlying goal is to meet the minds of users in terms of content consumption. User profiles traditionally are constructed based on users declared interests and or inferred from e.g. users demographics. There have also been systems that identify users interests based on observations made on users interactions with content. A typical example of such user interaction with content is click through rate CTR .

These traditional approaches have various shortcomings. For example users interests are profiled without any reference to a baseline so that the level of interest can be more accurately estimated. User interests are detected in isolated application settings so that user profiling in individual applications cannot capture a broad range of the overall interests of a user. Such traditional approach to user profiling lead to fragmented representation of user interests without a coherent understanding of the users preferences. Because profiles of the same user derived from different application settings are often grounded with respect to the specifics of the applications it is also difficult to integrate them to generate a more coherent profile that better represent the user s interests.

User activities directed to content are traditionally observed and used to estimate or infer users interests. CTR is the most commonly used measure to estimate users interests. However CTR is no longer adequate to capture users interests particularly given that different types of activities that a user may perform on different types of devices may also reflect or implicate user s interests. In addition user reactions to content usually represent users short term interests. Such observed short term interests when acquired piece meal as traditional approaches often do can only lead to reactive rather than proactive services to users. Although short term interests are important they are not adequate to enable understanding of the more persistent long term interests of a user which are crucial in terms of user retention. Most user interactions with content represent short term interests of the user so that relying on such short term interest behavior makes it difficult to expand the understanding of the increasing range of interests of the user. When this is in combination with the fact that such collected data is always the past behavior and collected passively it creates a personalization bubble making it difficult if not impossible to discover other interests of a user unless the user initiates some action to reveal new interests.

Yet another line of effort to allow users to access relevant content is to pooling content that may be interested by users in accordance with their interests. Given the explosion of information on the Internet it is not likely even if possible to evaluate all content accessible via the Internet whenever there is a need to select content relevant to a particular user. Thus realistically it is needed to identify a subset or a pool of the Internet content based on some criteria so that content can be selected from this pool and recommended to users based on their interests for consumption.

Conventional approaches to creating such a subset of content are application centric. Each application carves out its own subset of content in a manner that is specific to the application. For example Amazon.com may have a content pool related to products and information associated thereof created updated based on information related to its own users and or interests of such users exhibited when they interact with Amazon.com. Facebook also has its own subset of content generated in a manner not only specific to Facebook but also based on user interests exhibited while they are active on Facebook. As a user may be active in different applications e.g. Amazon.com and Facebook and with each application they likely exhibit only part of their overall interests in connection with the nature of the application. Given that each application can usually gain understanding at best of partial interests of users making it difficult to develop a subset of content that can be used to serve a broader range of users interests.

Another line of effort is directed to personalized content recommendation i.e. selecting content from a content pool based on the user s personalized profiles and recommending such identified content to the user. Conventional solutions focus on relevance i.e. the relevance between the content and the user. Although relevance is important there are other factors that also impact how recommendation content should be selected in order to satisfy a user s interests. Most content recommendation systems insert advertisement to content identified for a user for recommendation. Some traditional systems that are used to identify insertion advertisements match content with advertisement or user s query also content with advertisement without considering matching based on demographics of the user with features of the target audience defined by advertisers. Some traditional systems match user profiles with the specified demographics of the target audience defined by advertisers but without matching the content to be provided to the user and the advertisement. The reason is that content is often classified into taxonomy based on subject matters covered in the content yet advertisement taxonomy is often based on desired target audience groups. This makes it less effective in terms of selecting the most relevant advertisement to be inserted into content to be recommended to a specific user.

There is a need for improvements over the conventional approaches to personalizing content recommendation.

The teachings disclosed herein relate to methods systems and programming for providing personalized web page layouts.

In one embodiment a method is provided for estimating model parameters. The method comprises receiving a data set related to a plurality of users and associated content partitioning the data set into a plurality of sub data sets in accordance with the users so that data associated with each user are not partitioned into more than one sub data set storing each of the sub data sets in a separate one of a plurality of user data storages each of said data storages being coupled with a separate one of a plurality of estimators storing content associated with the plurality of users in a content storage where the content storage is coupled to the plurality of estimators so that the content in the content storage is shared by the estimators and estimating asynchronously by each estimator one or more parameters associated with a model based on data from one of the sub data sets.

In another embodiment a system is provided for estimating model parameters. The system comprises a modeling unit configured to receive a data set related to a plurality of users and associated content and partition the data set into a plurality of sub data sets in accordance with the users so that data associated with each user are not partitioned into more than one sub data set. The system further comprises a first one of a plurality of data storages configured to store each of the sub data sets each of said data storages being coupled with a separate one of a plurality of estimators. The system further includes a content storage configured to store content associated with the plurality of users where the content storage is coupled to the plurality of estimators so that the content in the content storage is shared by the estimators wherein each estimator is configure to asynchronously estimate one or more parameters associated with a model based on data from one of the sub data sets.

In yet another embodiment a non transitory computer readable medium is provided having recorded thereon information for estimating model parameters wherein the information when read by a computer causes the computer to perform a plurality of steps. The steps comprising receiving a data set related to a plurality of users and associated content partitioning the data set into a plurality of sub data sets in accordance with the users so that data associated with each user are not partitioned into more than one sub data set storing each of the sub data sets in a separate one of a plurality of user data storages each of said data storages being coupled with a separate one of a plurality of estimators storing content associated with the plurality of users in a content storage where the content storage is coupled to the plurality of estimators so that the content in the content storage is shared by the estimators and estimating asynchronously by each estimator one or more parameters associated with a model based on data from one of the sub data sets.

In the following detailed description numerous specific details are set forth by way of examples in order to provide a thorough understanding of the relevant teachings. However it should be apparent to those skilled in the art that the present teachings may be practiced without such details. In other instances well known methods procedures components and or circuitry have been described at a relatively high level without detail in order to avoid unnecessarily obscuring aspects of the present teachings.

The present teaching relates to personalizing on line content recommendations to a user. Particularly the present teaching relates to a system method and or programs for personalized content recommendation that addresses the shortcomings associated the conventional content recommendation solutions in personalization content pooling and recommending personalized content.

With regard to personalization the present teaching identifies a user s interests with respect to a universal interest space defined via known concept archives such as Wikipedia and or content taxonomy. Using such a universal interest space interests of users exhibited in different applications and via different platforms can be used to establish a general population s profile as a baseline against which individual user s interests and levels thereof can be determined. For example users active in a third party application such as Facebook or Twitter and the interests that such users exhibited in these third party applications can be all mapped to the universal interest space and then used to compute a baseline interest profile of the general population. Specifically each user s interests observed with respect to each document covering certain subject matters or concepts can be mapped to e.g. Wikipedia or certain content taxonomy. A high dimensional vector can be constructed based on the universal interest space in which each attribute of the vector corresponds to a concept in the universal space and the value of the attribute may corresponds to an evaluation of the user s interest in this particular concept. The general baseline interest profile can be derived based on all vectors represent the population. Each vector representing an individual can be normalized against the baseline interest profile so that the relative level of interests of the user with respect to the concepts in the universal interest space can be determined. This enables better understanding of the level of interests of the user in different subject matters with respect to a more general population and result in enhanced personalization for content recommendation. Rather than characterizing users interests merely according to proprietary content taxonomy as is often done in the prior art the present teaching leverages public concept archives such as Wikipedia or online encyclopedia to define a universal interest space in order to profile a user s interests in a more coherent manner. Such a high dimensional vector captures the entire interest space of every user making person to person comparison as to personal interests more effective. Profiling a user and in this manner also leads to efficient identification of users who share similar interests. In addition content may also be characterized in the same universal interest space e.g. a high dimensional vector against the concepts in the universal interest space can also be constructed with values in the vector indicating whether the content covers each of the concepts in the universal interest space. By characterizing users and content in the same space in a coherent way the affinity between a user and a piece of content can be determined via e.g. a dot product of the vector for the user and the vector for the content.

The present teaching also leverages short term interests to better understand long term interests of users. Short term interests can be observed via user online activities and used in online content recommendation the more persistent long term interests of a user can help to improve content recommendation quality in a more robust manner and hence user retention rate. The present teaching discloses discovery of long term interests as well as short term interests.

To improve personalization the present teaching also discloses ways to improve the ability to estimate a user s interest based on a variety of user activities. This is especially useful because meaningful user activities often occur in different settings on different devices and in different operation modes. Through such different user activities user engagement to content can be measured to infer users interests. Traditionally clicks and click through rate CTR have been used to estimate users intent and infer users interests. CTR is simply not adequate in today s world. Users may dwell on a certain portion of the content the dwelling may be for different lengths of time users may scroll along the content and may dwell on a specific portion of the content for some length of time users may scroll down at different speeds users may change such speed near certain portions of content users may skip certain portion of content etc. All such activities may have implications as to users engagement to content. Such engagement can be utilized to infer or estimate a user s interests. The present teaching leverages a variety of user activities that may occur across different device types in different settings to achieve better estimation of users engagement in order to enhance the ability of capturing a user s interests in a more reliable manner.

Another aspect of the present teaching with regard to personalization is its ability to explore unknown interests of a user by generating probing content. Traditionally user profiling is based on either user provided information e.g. declared interests or passively observed past information such as the content that the user has viewed reactions to such content etc. Such prior art schemes can lead to a personalization bubble where only interests that the user revealed can be used for content recommendation. Because of that the only user activities that can be observed are directed to such known interests impeding the ability to understand the overall interest of a user. This is especially so considering the fact that users often exhibit different interests mostly partial interests in different application settings. The present teaching discloses ways to generate probing content with concepts that is currently not recognized as one of the user s interests in order to explore the user s unknown interests. Such probing content is selected and recommended to the user and user activities directed to the probing content can then be analyzed to estimate whether the user has other interests. The selection of such probing content may be based on a user s current known interests by e.g. extrapolating the user s current interests. For example for some known interests of the user e.g. the short term interests at the moment some probing concepts in the universal interest space for which the user has not exhibited interests in the past may be selected according to some criteria e.g. within a certain distance from the user s current known interest in a taxonomy tree and content related to such probing concepts may then be selected and recommended to the user. Another way to identify probing concept corresponding to unknown interest of the user may be through the user s cohorts. For instance a user may share certain interests with his her cohorts but some members of the circle may have some interests that the user has never exhibited before. Such un shared interests with cohorts may be selected as probing unknown interests for the user and content related to such probing unknown interests may then be selected as probing content to be recommended to the user. In this manner the present teaching discloses a scheme by which a user s interests can be continually probed and understood to improve the quality of personalization. Such managed probing can also be combined with random selection of probing content to allow discovery of unknown interests of the user that are far removed from the user s current known interests.

A second aspect of recommending quality personalized content is to build a content pool with quality content that covers subject matters interesting to users. Content in the content pool can be rated in terms of the subject and or the performance of the content itself. For example content can be characterized in terms of concepts it discloses and such a characterization may be generated with respect to the universal interest space e.g. defined via concept archive s such as content taxonomy and or Wikipedia and or online encyclopedia as discussed above. For example each piece of content can be characterized via a high dimensional vector with each attribute of the vector corresponding to a concept in the interest universe and the value of the attribute indicates whether and or to what degree the content covers the concept. When a piece of content is characterized in the same universal interest space as that for user s profile the affinity between the content and a user profile can be efficiently determined.

Each piece of content in the content pool can also be individually characterized in terms of other criteria. For example performance related measures such as popularity of the content may be used to describe the content. Performance related characterizations of content may be used in both selecting content to be incorporated into the content pool as well as selecting content already in the content pool for recommendation of personalized content for specific users. Such performance oriented characterizations of each piece of content may change over time and can be assessed periodically and can be done based on users activities. Content pool also changes over time based on various reasons such as content performance change in users interests etc. Dynamically changed performance characterization of content in the content pool may also be evaluated periodically or dynamically based on performance measures of the content so that the content pool can be adjusted over time. i.e. by removing low performance content pieces adding new content with good performance or updating content.

To grow the content pool the present teaching discloses ways to continually discover both new content and new content sources from which interesting content may be accessed evaluated and incorporated into the content pool. New content may be discovered dynamically via accessing information from third party applications which users use and exhibit various interests. Examples of such third party applications include Facebook Twitter Microblogs or YouTube. New content may also be added to the content pool when some new interest or an increased level of interests in some subject matter emerges or is predicted based on the occurrence of certain spontaneous events. One example is the content about the life of Pope Benedict which in general may not be a topic of interests to most users but likely will be in light of the surprising announcement of Pope Benedict s resignation. Such dynamic adjustment to the content pool aims at covering a dynamic and likely growing range of interests of users including those that are e.g. exhibited by users in different settings or applications or predicted in light of context information. Such newly discovered content may then be evaluated before it can be selected to be added to the content pool.

Certain content in the content pool e.g. journals or news need to be updated over time. Conventional solutions usually update such content periodically based on a fixed schedule. The present teaching discloses the scheme of dynamically determining the pace of updating content in the content pool based on a variety of factors. Content update may be affected by context information. For example the frequency at which a piece of content scheduled to be updated may be every 2 hours but this frequency can be dynamically adjusted according to e.g. an explosive event such as an earthquake. As another example content from a social group on Facebook devoted to Catholicism may normally be updated daily. When Pope Benedict s resignation made the news the content from that social group may be updated every hour so that interested users can keep track of discussions from members of this social group. In addition whenever there are newly identified content sources it can be scheduled to update the content pool by e.g. crawling the content from the new sources processing the crawled content evaluating the crawled content and selecting quality new content to be incorporated into the content pool. Such a dynamically updated content pool aims at growing in compatible with the dynamically changing users interests in order to facilitate quality personalized content recommendation.

Another key to quality personalized content recommendation is the aspect of identifying quality content that meets the interests of a user for recommendation. Previous solutions often emphasize mere relevance of the content to the user when selecting content for recommendation. In addition traditional relevance based content recommendation was mostly based on short term interests of the user. This not only leads to a content recommendation bubble i.e. known short interests cause recommendations limited to the short term interests and reactions to such short term interests centric recommendations cycle back to the short term interests that start the process. This bubble makes it difficult to come out of the circle to recommend content that can serve not only the overall interests but also long term interests of users. The present teaching combines relevance with performance of the content so that not only relevant but also quality content can be selected and recommended to users in a multi stage ranking system.

In addition to identify recommended content that can serve a broad range of interests of a user the present teaching relies on both short term and long term interests of the user to identify user content affinity in order to select content that meets a broader range of users interests to be recommended to the user.

In content recommendation monetizing content such as advertisements are usually also selected as part of the recommended content to a user. Traditional approaches often select ads based on content in which the ads are to be inserted. Some traditional approaches also rely on user input such as queries to estimate what ads likely can maximize the economic return. These approaches select ads by matching the taxonomy of the query or the content retrieved based on the query with the content taxonomy of the ads. However content taxonomy is commonly known not to correspond with advertisement taxonomy which advertisers use to target at certain audience. As such selecting ads based on content taxonomy does not serve to maximize the economic return of the ads to be inserted into content and recommended to users. The present teaching discloses method and system to build a linkage between content taxonomy and advertisement taxonomy so that ads that are not only relevant to a user s interests but also the interests of advertisers can be selected. In this way the recommended content with ads to a user can both serve the user s interests and at the same time to allow the content operator to enhance monetization via ads.

Yet another aspect of personalized content recommendation of the present teaching relates to recommending probing content that is identified by extrapolating the currently known user interests. Traditional approaches rely on selecting either random content beyond the currently known user interests or content that has certain performance such as a high level of click activities. Random selection of probing content presents a low possibility to discover a user s unknown interests. Identifying probing content by choosing content for which a higher level of activities are observed is also problematic because there can be many pieces of content that a user may potentially be interested but there is a low level of activities associated therewith. The present teaching discloses ways to identify probing content by extrapolating the currently known interest with the flexibility of how far removed from the currently known interests. This approach also incorporates the mechanism to identify quality probing content so that there is an enhanced likelihood to discover a user s unknown interests. The focus of interests at any moment can be used as an anchor interest based on which probing interests which are not known to be interests of the user can be extrapolated from the anchor interests and probing content can be selected based on the probing interests and recommended to the user together with the content of the anchor interests. Probing interests content may also be determined based on other considerations such as locale time or device type. In this way the disclosed personalized content recommendation system can continually explore and discover unknown interests of a user to understand better the overall interests of the user in order to expand the scope of service.

Additional novel features will be set forth in part in the description which follows and in part will become apparent to those skilled in the art upon examination of the following and the accompanying drawings or may be learned by production or operation of the examples. The advantages of the present teachings may be realized and attained by practice or use of various aspects of the methodologies instrumentalities and combinations set forth in the detailed examples discussed below.

Knowledge archives may be an on line encyclopedia such as Wikipedia or indexing system such as an on line dictionary. On line concept archives may be used for its content as well as its categorization or indexing systems. Knowledge archives provide extensive classification system to assist with the classification of both the user s preferences as well as classification of content. Knowledge concept archives such as Wikipedia may have hundreds of thousands to millions of classifications and sub classifications. A classification is used to show the hierarchy of the category. Classifications serve two main purposes. First they help the system understand how one category relates to another category and second they help the system maneuver between higher levels on the hierarchy without having to move up and down the subcategories. The categories or classification structure found in knowledge archives is used for multidimensional content vectors as well as multidimensional user profile vectors which are utilized by personalized content recommendation module to match personalized content to a user . Third party platforms maybe any third party applications including but not limited to social networking sites like Facebook Twitter LinkedIn Google . It may include third party mail servers such as GMail or Bing Search. Third party platforms provide both a source of content as well as insight into a user s personal preferences and behaviors.

Advertisers are coupled with the ad content database as well as an ads classification system or ad. taxonomy intended for classified advertisement content. Advertisers may provide streaming content static content and sponsored content. Advertising content may be placed at any location on a personalized content page and may be presented both as part of a content stream as well as a standalone advertisement placed strategically around or within the content stream.

Personalized content recommendation module comprises applications content pool content pool generation update unit concept content analyzer content crawler unknown interest explorer user understanding unit user profiles content taxonomy context information analyzer user event analyzer third party interest analyzer social media content source identifier advertisement insertion unit and content advertisement taxonomy correlator . These components are connected to achieve personalization content pooling and recommending personalized content to a user. For example the content ranking unit works in connection with context information analyzer the unknown interest explorer and the ad insertion unit to generate personalized content to be recommended to a user with personalized ads or probing content inserted. To achieve personalization the user understanding unit works in connection with a variety of components to dynamically and continuously update the user profiles including content taxonomy the knowledge archives user event analyzer and the third party interest analyzer . Various components are connected to continuously maintain a content pool including the content pool generation update unit user event analyzer social media content source identifier content concept analyzer content crawler the content taxonomy as well as user profiles .

Personalized content recommendation module is triggered when user engages with system through applications . Applications may receive information in the form of a user id cookies log in information from user via some form of computing device. User may access system via a wired or wireless device and may be stationary or mobile. User may interface with the applications on a tablet a Smartphone a laptop a desktop or any other computing device which may be embedded in devices such as watches eyeglasses or vehicles. In addition to receiving insights from the user about what information the user might be interested applications provides information to user in the form of personalized content stream. User insights might be user search terms entered to the system declared interests user clicks on a particular article or subject user dwell time or scroll over of particular content user skips with respect to some content etc. User insights may be a user indication of a like a share or a forward action on a social networking site such as Facebook or even peripheral activities such as print or scan of certain content. All of these user insights or events are utilized by the personalized content recommendation module to locate and customize content to be presented to user . User insights received via applications are used to update personalized profiles for users which may be stored in user profiles . User profiles may be database or a series of databases used to store personalized user information on all the users of system . User profiles may be a flat or relational database and may be stored in one or more locations. Such user insights may also be used to determine how to dynamically update the content in the content pool .

A specific user event received via applications is passed along to user event analyzer which analyzes the user event information and feeds the analysis result with event data to the user understanding unit and or the content pool generation update unit . Based on such user event information the user understanding unit estimates short term interests of the user and or infer user s long term interests based on behaviors exhibited by user over long or repetitive periods. For example a long term interest may be a general interest in sports where as a short term interest may be related to a unique sports event such as the Super Bowl at a particular time. Over time a user s long term interest may be estimated by analyzing repeated user events. A user who during every engagement with system regularly selects content related to the stock market may be considered as having a long term interest in finances. In this case system accordingly may determine that personalized content for user should contain content related to finance. Contrastingly short term interest may be determined based on user events which may occur frequently over a short period but which is not something the user is interested in the long term. For example a short term interest may reflect the momentary interest of a user which may be triggered by something the user saw in the content but such an interest may not persist over time. Both short and long term interest are important in terms of identifying content that meets the desire of the user but need to be managed separately because of the difference in their nature as well as how they influence the user.

In some embodiments short term interests of a user may be analyzed to predict the user s long term interests. To retain a user it is important to understand the user s persistent or long term interests. By identifying user s short term interest and providing him her with a quality personalized experience system may convert an occasional user into a long term user. Additionally short term interest may trend into long term interest and vice versa. The user understanding unit provides the capability of estimating both short and long term interests.

The user understanding unit gathers user information from multiple sources including all the user s events and creates one or more multidimensional personalization vectors. In some embodiments the user understanding unit receives inferred characteristics about the user based on the user events such as the content he she views self declared interests attributes or characteristics user activities and or events from third party platforms. In an embodiment the user understanding unit receives inputs from social media content source identifier . Social media content source identifier relies on user s social media content to personalize the user s profile. By analyzing the user s social media pages likes shares etc social media content source identifier provides information for user understanding unit . The social media content source identifier is capable of recognizing new content sources by identifying e.g. quality curators on social media platforms such as Twitter Facebook or blogs and enables the personalized content recommendation module to discover new content sources from where quality content can be added to the content pool . The information generated by social media content source identifier may be sent to a content concept analyzer and then mapped to specific category or classification based on content taxonomy as well as a knowledge archives classification system.

The third party interest analyzer leverages information from other third party platforms about users active on such third party platforms their interests as well as content these third party users to enhance the performance of the user understanding unit . For example when information about a large user population can be accessed from one or more third party platforms the user understanding unit can rely on data about a large population to establish a baseline interest profile to make the estimation of the interests of individual users more precise and reliable e.g. by comparing interest data with respect to a particular user with the baseline interest profile which will capture the user s interests with a high level of certainty.

When new content is identified from content source or third party platforms it is processed and its concepts are analyzed. The concepts can be mapped to one or more categories in the content taxonomy and the knowledge archives . The content taxonomy is an organized structure of concepts or categories of concepts and it may contain a few hundred classifications of a few thousand. The knowledge archives may provide millions of concepts which may or may not be structures in a similar manner as the content taxonomy . Such content taxonomy and knowledge archives may serve as a universal interest space. Concepts estimated from the content can be mapped to a universal interest space and a high dimensional vector can be constructed for each piece of content and used to characterize the content. Similarly for each user a personal interest profile may also be constructed mapping the user s interests characterized as concepts to the universal interest space so that a high dimensional vector can be constructed with the user s interests levels populated in the vector.

Content pool may be a general content pool with content to be used to serve all users. The content pool may also be structured so that it may have personalized content pool for each user. In this case content in the content pool is generated and retained with respect to each individual user. The content pool may also be organized as a tiered system with both the general content pool and personalized individual content pools for different users. For example in each content pool for a user the content itself may not be physically present but is operational via links pointers or indices which provide references to where the actual content is stored in the general content pool.

Content pool is dynamically updated by content pool generation update module . Content in the content pool comes and go and decisions are made based on the dynamic information of the users the content itself as well as other types of information. For example when the performance of content deteriorates e.g. low level of interests exhibited from users the content pool generation update unit may decide to purge it from the content pool. When content becomes stale or outdated it may also be removed from the content pool. When there is a newly detected interest from a user the content pool generation update unit may fetch new content aligning with the newly discovered interests. User events may be an important source of making observations as to content performance and user interest dynamics. User activities are analyzed by the user event analyzer and such Information is sent to the content pool generation update unit . When fetching new content the content pool generation update unit invokes the content crawler to gather new content which is then analyzed by the content concept analyzer then evaluated by the content pool generation update unit as to its quality and performance before it is decided whether it will be included in the content pool or not. Content may be removed from content pool because it is no longer relevant because other users are not considering it to be of high quality or because it is no longer timely. As content is constantly changing and updating content pool is constantly changing and updating providing user with a potential source for high quality timely personalized content.

In addition to content personalized content recommendation module provides for targeted or personalized advertisement content from advertisers . Advertisement database houses advertising content to be inserted into a user s content stream. Advertising content from ad database is inserted into the content stream via Content ranking unit . The personalized selection of advertising content can be based on the user s profile. Content advertisement user taxonomy correlator may re project or map a separate advertisement taxonomy to the taxonomy associated with the user profiles . Content advertisement user taxonomy correlator may apply a straight mapping or may apply some intelligent algorithm to the re projection to determine which of the users may have a similar or related interest based on similar or overlapping taxonomy categories.

Content ranking unit generates the content stream to be recommended to user based on content selected from content pool based on the user s profile as well as advertisement selected by the advertisement insertion unit . The content to be recommended to the user may also be determined by the content ranking unit based on information from the context information analyzer . For example if a user is currently located in a beach town which differs from the zip code in the user s profile it can be inferred that the user may be on vacation. In this case information related to the locale where the user is currently in may be forwarded from the context information analyzer to the Content ranking unit so that it can select content that not only fit the user s interests but also is customized to the locale. Other context information include day time and device type. The context information can also include an event detected on the device that the user is currently using such as a browsing event of a website devoted to fishing. Based on such a detected event the momentary interest of the user may be estimated by the context information analyzer . which may then direct the Content ranking unit to gather content related to fishing amenities in the locale the user is in for recommendation.

The personalized content recommendation module can also be configured to allow probing content to be included in the content to be recommended to the user even though the probing content does not represent subject matter that matches the current known interests of the user. Such probing content is selected by the unknown interest explorer . Once the probing content is incorporated in the content to be recommended to the user information related to user activities directed to the probing content including no action is collected and analyzed by the user event analyzer which subsequently forwards the analysis result to long short term interest identifiers and . If an analysis of user activities directed to the probing content reveals that the user is or is not interested in the probing content the user understanding unit may then update the user profile associated with the probed user accordingly. This is how unknown interests may be discovered. In some embodiments the probing content is generated based on the current focus of user interest e.g. short term by extrapolating the current focus of interests. In some embodiments the probing content can be identified via a random selection from the general content either from the content pool or from the content sources so that an additional probing can be performed to discover unknown interests.

To identify personalized content for recommendation to a user the content ranking unit takes all these inputs and identify content based on a comparison between the user profile vector and the content vector in a multiphase ranking approach. The selection may also be filtered using context information. Advertisement to be inserted as well as possibly probing content can then be merged with the selected personalized content.

Once the user profiles and the content pool are created when the system detects the presence of a user at the context information such as locale day time may be obtained and analyzed at . illustrates exemplary types of context information. Based on the detected user s profile optionally context information personalized content is identified for recommendation. A high level exemplary flow for generating personalized content for recommendation is presented in . Such gathered personalized content may be ranked and filtered to achieve a reasonable size as to the amount of content for recommendation. Optionally not shown advertisement as well as probing content may also be incorporated in the personalized content. Such content is then recommended to the user at .

User reactions or activities with respect to the recommended content are monitored at and analyzed at . Such events or activities include clicks skips dwell time measured scroll location and speed position time sharing forwarding hovering motions such as shaking etc. It is understood that any other events or activities may be monitored and analyzed. For example when the user moves the mouse cursor over the content the title or summary of the content may be highlighted or slightly expanded. In anther example when a user interacts with a touch screen by her his finger s any known touch screen user gestures may be detected. In still another example eye tracking on the user device may be another user activity that is pertinent to user behaviors and can be detected. The analysis of such user events includes assessment of long term interests of the user and how such exhibited short term interests may influence the system s understanding of the user s long term interests. Information related to such assessment is then forwarded to the user understanding unit to guide how to update at the user s profile. At the same time based on the user s activities the portion of the recommended content that the user showed interests are assessed at and the result of the assessment is then used to update at the content pool. For example if the user shows interests on the probing content recommended it may be appropriate to update the content pool to ensure that content related to the newly discovered interest of the user will be included in the content pool.

The content concept analyzing control unit interfaces with the content crawler to obtain candidate content that is to be analyzed to determine whether the new content is to be added to the content pool. The content concept analyzing control unit also interfaces with the content concept analyzer see to get the content analyzed to extract concepts or subjects covered by the content. Based on the analysis of the new content a high dimensional vector for the content profile can be computed via e.g. by mapping the concepts extracted from the content to the universal interest space e.g. defined via Wikipedia or other content taxonomies. Such a content profile vector can be compared with user profiles to determine whether the content is of interest to users. In addition content is also evaluated in terms of its performance by the content performance estimator based on e.g. third party information such as activities of users from third party platforms so that the new content although not yet acted upon by users of the system can be assessed as to its performance. The content performance information may be stored together with the content s high dimensional vector related to the subject of the content in the content profile . The performance assessment is also sent to the content quality evaluation unit which e.g. will rank the content in a manner consistent with other pieces of content in the content pool. Based on such rankings the content selection unit then determines whether the new content is to be incorporated into the content pool .

To dynamically update the content pool the content pool generation update unit may keep a content log with respect to all content presently in the content pool and dynamically update the log when more information related to the performance of the content is received. When the user activity analyzer receives information related to user events it may log such events in the content log and perform analysis to estimate e.g. any change to the performance or popularity of the relevant content over time. The result from the user activity analyzer may also be utilized to update the content profiles e.g. when there is a change in performance. The content status evaluation unit monitors the content log and the content profile to dynamically determine how each piece of content in the content pool is to be updated. Depending on the status with respect to a piece of content the content status evaluation unit may decide to purge the content if its performance degrades below a certain level. It may also decide to purge a piece of content when the overall interest level of users of the system drops below a certain level. For content that requires update e.g. news or journals the content status evaluation unit may also control the frequency of the updates based on the dynamic information it receives. The content update control unit carries out the update jobs based on decisions from the content status evaluation unit and the frequency at which certain content needs to be updated. The content update control unit may also determine to add new content whenever there is peripheral information indicating the needs e.g. there is an explosive event and the content in the content pool on that subject matter is not adequate. In this case the content update control unit analyzes the peripheral information and if new content is needed it then sends a control signal to the content concept analyzing control unit so that it can interface with the content crawler to obtain new content.

In operation the baseline interest profile generator access information about a large user population including users interests and content they are interested in from one or more third party sources e.g. Facebook . Content from such sources is analyzed by the content concept analyzer which identifies the concepts from such content. When such concepts are received by the baseline interest profile generator it maps such concepts to the knowledge archives and content taxonomy and generate one or more high dimensional vectors which represent the baseline interest profile of the user population. Such generated baseline interest profile is stored at in the user understanding unit . When there is similar data from additional third party sources the baseline interest profile may be dynamically updated to reflect the baseline interest level of the growing population.

Once the baseline interest profile is established when the user profile generator receives user information or information related to estimated short term and long term interests of the same user it may then map the user s interests to the concepts defined by e.g. the knowledge archives or content taxonomy so that the user s interests are now mapped to the same space as the space in which the baseline interest profile is constructed. The user profile generator then compares the user s interest level with respect to each concept with that of a larger user population represented by the baseline interest profile to determine the level of interest of the user with respect to each concept in the universal interest space. This yields a high dimensional vector for each user. In combination with other additional information such as user demographics etc. a user profile can be generated and stored in .

User profiles are updated continuously based on newly received dynamic information. For example a user may declare additional interests and such information when received by the user profile generator may be used to update the corresponding user profile. In addition the user may be active in different applications and such activities may be observed and information related to them may be gathered to determine how they impact the existing user profile and when needed the user profile can be updated based on such new information. For instance events related to each user may be collected and received by the user intent interest estimator . Such events include that the user dwelled on some content of certain topic frequently that the user recently went to a beach town for surfing competition or that the user recently participated in discussions on gun control etc. Such information can be analyzed to infer the user intent Interests. When the user activities relate to reaction to content when the user is online such information may be used by the short term interest identifier to determine the user s short term interests. Similarly some information may be relevant to the user s long term interests. For example the number of requests from the user to search for content related to diet information may provide the basis to infer that the user is interested in content related to diet. In some situations estimating long term interest may be done by observing the frequency and regularity at which the user accesses certain type of information. For instance if the user repeatedly and regularly accesses content related to certain topic e.g. stocks such repetitive and regular activities of the user may be used to infer his her long term interests. The short term interest identifier may work in connection with the long term interest identifier to use observed short term interests to infer long term interests. Such estimated short long term interests are also sent to the user profile generator so that the personalization can be adapted to the changing dynamics.

More detailed disclosures of various aspects of the system particularly the personalized content recommendation module are covered in different U.S. patent applications as well as PCT applications entitled Method and System For User Profiling Via Mapping Third Party Interests To A Universal Interest Space Method and System for Multi Phase Ranking For Content Personalization Method and System for Measuring User Engagement Using Click Skip In Content Stream Method and System for Dynamic Discovery And Adaptive Crawling of Content From the Internet Method and System For Dynamic Discovery of Interesting URLs From Social Media Data Stream Method and System for Discovery of User Unknown Interests Method and System for Efficient Matching of User Profiles with Audience Segments Method and System For Mapping Short Term Ranking Optimization Objective to Long Term Engagement Social Media Based Content Selection System Method and System For Measuring User Engagement From Stream Depth Method and System For Measuring User Engagement Using Scroll Dwell Time Efficient and Fault Tolerant Distributed Algorithm for Learning Latent Factor Models through Matrix Factorization and Almost Online Large Scale Collaborative Based Recommendation System. The present teaching is particularly directed to efficient distributed fault tolerant learning as discussed in further detail below.

To this end illustrates an embodiment of a user understanding unit of implemented by way of a collaborative filtering CF system . The CF system determines user content item affiliations based on executing computer readable instructions for evaluating various types of user activity and user profile information via a CF modeling unit which for instance implements a latent factor model technique and outputs likely user content item pairs based on corresponding affiliation scores. The user content item pairs having an affiliation score that exceeds a predetermined threshold are added to the user content affiliation database and subsequently to the content database for serving to the user as suggested content of interest. The latent factor model executed by the CF modeling unit characterizes both content items and users based on multiple factor vectors inferred from ratings and or user activity patterns. In an embodiment the latent factor model is implemented via a matrix factorization approach.

The CF modeling unit receives a plurality of input signals including a long term user activity log comprising activity vector signals indicating user activity with respect to certain content. User activity with respect to a content item comprises user click data user skip data derived from user click data or received via direct signaling as well as user dwell data indicative of user inactivity with respect to a content item or part thereof and scroll data indicative of scroll direction scroll rate and scroll extent among other user activity information. The long term user activity log data includes activity data collected and buffered over a period of time exceeding a predetermined threshold such as approximately on the order of an hour. In one embodiment the long term activity log includes user activity data buffered over a six hour period.

The CF modeler also receives an input of user profile information including a declared interest vector for each user which includes multiple values indicative of the user s interest in various types of content based on the user s interest indications received from social media sources e.g. likes shares tweets re tweets and the like with respect to certain content items as well as static user profile data from electronic questionnaires forms and the like. In the context of evaluating user content item affiliations the CF modeler relies on content item taxonomy such as a Wikipedia Wiki content taxonomy indicating the content item s mapping to one or more content categories for example.

The integrated short long term modeling enhancer module implements parallel short term and long term model parameter learning processes in order to dynamically adjust CF model parameters of the CF modeling unit as discussed in further detail below. Thus in addition to the long term user activity log information the modeling enhancer also receives input of short term user activity . The short term user activity log includes user activity data having duration that is shorter than the duration of the long term user activity log . In an embodiment the duration of the short term user activity log is on the order of minutes such as five minutes of user activity data for example.

Users signed in authenticated or unregistered Id Categorical variables Gender geo information and age Creation timestamp.

Stories or pages Id Categorical variables a set of predetermined document property indicators content category vector in accordance with a predetermined taxonomy author publisher Publication timestamp.

Page features entities a set of predetermined document property indicators content category vector in accordance with a predetermined taxonomy authors and publishers.

Story was presented in a property e.g. HomeRun page and was clicked in case order of presentation of all stories involved is available the absence of clicks can be inferred .

In certain cases where the signals indicate only positive user page interactions negative user page interactions may be inferred and generated from the signals.

For example we may randomly pick a popular story that was likely viewed but not clicked by the user as a negative interaction.

In order not to increase model dimensions user page interaction sessions may be identified and corresponding implicit scores may be generated as a function of the above signals.

The weights can be optimized in an advance stage using bucket testing on real users click through rate CTR .

The following discussion describes an embodiment of a low ranked Matrix Factorization MF implementation of a collaborative filtering model algorithm also referred to as a latent factor model stored as computer readable instructions in non transient computer memory and executed via one or more computer processors of the CF modeling unit .

In an embodiment of a low ranked Matrix Factorization MF implementation of collaborative filtering also referred to as latent factor model each entity users users features stories and stories features is assigned an l dimensions latent factor vector LFV and a bias.

New entities parameters are initialized using a random generator of a zero mean Gaussian distribution with standard deviation .

c R i th user bias set of user features b R feature f i bias some of the users features biases may be forced to be zero 

To overcome data sparsity smoothing may be useful. As an illustrative example one embodiment uses a linear combination of features vectors of ages 20 21 and 22 to represent the age of 21 in a 21 year old user model.

Since stories have a short life span we may represent stories by a normalized sum of their viewers users LFVs. This story less model is coupled neatly with the short term learning process described below and is also related to the story cold start problem.

In an embodiment users are kept indefinitely in the model stored in a key value store. Pages are kept in memory for Thours e.g. 48 hours .

The short term based model parameter adjuster receives input of the short term user activity log data from the short term user activity buffering unit and executes an on going short term model learning process occurring every few minutes for example. The short term model learning process uses user page e.g. story or other content interactions accumulated over the short term user activity log period such as five minutes to perform an incremental update of values of those model parameters e.g. at least some bias and or LFV vector parameters values that correspond to the change in the user activity vectors received during the corresponding short term period.

The integrated CF model parameter modifier unit receives the adjusted CF modeling parameter information from the error based model parameter optimizer and short term based model parameter adjuster and forwards the adjusted CF modeling parameters to the CF modeling unit . In an embodiment each of the short term and long term user activity buffering units is controlled via a corresponding timer . In a further embodiment the timers and the buffering units are dynamically controlled via corresponding adjuster modules for instance based on feedback from the long term error based model parameter adjuster and or short term based model parameter adjuster .

An embodiment of the long term and short term model learning processes is described below in further detail.

For consistency we may use a simple algorithm that alternately throws inactive users and unpopular pages until convergence 

Validation set 20 of all interactions used to evaluate the new model and optimize the meta parameters

Other objectives such as log likelihood maximization or mean absolute error MAE may be considered as well

Regularization We use Tichonov quadratic regularization with parameter to reduce overfitting of the model to the training set

Boosting To boost recent interactions we may assign multiplicative weights to the summands of the original error expression a la.

where t is the i j training interaction time and tis the beginning time of the current long term learning process

Parallel implementation Due to the sheer complexity of the problem hundreds millions of users and tens of thousands of pages a grid implementation of the long term learning process is required. Since stochastic gradient decent SGD is not applicable for grid implementation other algorithms are considered

Block gradient decent BGD . Unlike SGD where every parameter is updated for every relevant training event applying BGD every parameter is updated once for all relevant training events at every sweep

Generate an intermediate model using estimated scores and errors calculated using the current model. Hence for each model parameter .

where k is the sweep number and is the initial training rate meta parameter which may be optimized TBD 

Model smoothing to smooth the evolving models between two consecutive long term learning processes we may use the following 

Train an intermediate new model tilde over than exponential smooth each parameter tilde over tilde over to generate the new model tilde over 

As an alternative to the exponential smoothing we may use Kalman filter to smooth the model parameters

Each learning phase covers period of Tminutes e.g. 60minutes and occur every Tminutes e.g. 5 minutes 

As an alternate embodiment of implementing a short term learning procedure a decay is employed in place of moving a window. Accordingly the users LFVs may be updated for the n 1 short term learning period as follows

In an embodiment LFVs resulting from the short term learning procedures are not used as the starting point of the next long term learning procedure.

In general Collaborating filtering CF has proved itself in numerous settings to be key technology for content recommendation systems RecSys . One of the popular approaches of CF RecSys is the matrix factorization MF approach. In its basic form CF MF model entities user and item are presented by vectors of K latent features usually 10

In a standard non parallel computing environment the learning process is typically done by an iterative stochastic gradient decent algorithm SGD . In the standard setting the SGD computation paradigm requires incrementally updating the user and item vectors with gradient increments for each user item interaction record. In order for the process to converge the algorithm scans the entire input user item interaction set S multiple times until the changes to the model become small enough.

However in a large scale setting running SGD on a single machine is infeasible since it is impossible to load the user and item models in the RAM of a single machine. The option of updating a remote scalable database upon each processed record is also unattractive since it entails a massive amount of direct access I O. In this context SGD can be approximated by batch gradient descent BGD . The algorithm breaks the computation into a series of small batches of user item interactions. The user and item models are frozen throughout the batch. The computation aggregates the cumulative increment and applies it to the model at the end of the batch. Thus SGD can be viewed as an extreme case of BGD with the batch size of 1 .

For very large datasets it is attractive to run BGD in parallel on multiple machines. Popular data processing computing platforms like map reduce MR or Spark can be adapted for this process. For example MR allows parallel processing of multiple partitions of the original dataset map followed by aggregation of the partial cumulative gradients reduce . In this context every input partition is a BGD split and multiple splits are processed independently based on the user U and item I models. Therefore one complete scan of the entire input stands for a single iteration of the matrix factorization MF learning algorithm.

However a straightforward implementation on top of MR one job per iteration entails a large performance overhead for three reasons 1 the intermediate results of each job are written to a distributed filesystem in multiple replicas and 2 the management overhead of a map reduce job execution and 3 the job s completion is driven by the slowest task. The execution can be optimized on a distributed machine learning oriented platform e.g. Spark that 1 spreads the intermediate results in RAM across multiple machines and 2 performs all computation in a single job. However this solution is still subject to slow task bottlenecks since it synchronizes all the tasks at the end of each iteration. This synchronization is not required for proper functionality.

Embodiments of system and methods described herein provide BGD computation on the MR infrastructure which reduces the execution latency by orders of magnitude. The algorithm described below exploits the MR framework as a container for reliable distributed execution. It embeds the iterative computation into a single MR job thus avoiding the overhead described above. The iterated gradient aggregation and write back happen in the mapper code and there is no synchronization among the multiple tasks.

We exploit the fact that the number of users significantly exceeds the number of items U I . Therefore we optimize the processing by partitioning the interaction set S by user id such that multiple mappers access non overlapping partitions of U and share the access to I. The present teaching s core advantage is therefore I O efficient isolated update of U combined with inexpensive shared update if I.

To avoid excessive I O the write back to U private storage is deferred to the end of the task whereas the write back to I shared storage happens asynchronously not necessarily on iteration boundary .

The described embodiments inherit the fault tolerance built into the MR platform and exploit the context provided by the infrastructure to overcome the transient failures.

An embodiment of the present teaching maintains a model for a universe of users U and a universe of items I . It is assumed that U I .

The following embodiments receive as input an interaction set S a set of pairs each capturing the event of user u consuming item i. S is Implemented as partitioned sequential file.

Preprocessing Joins U and S over the user id key in a MR job. Co partition the result set denoted S U with U.

A single mapper task t scans one partition of S U task s split and the whole I shared file . It stores the scanned data in its local RAM. In an embodiment the data structures are as follows 

The task locally performs the BGD iterations until the termination condition is satisfied. No communication with the other tasks happens. Upon convergence the mapper writes back Ut.

Periodically and asynchronously from the other tasks the mapper applies It to I such as through an atomic increment Application Programming Interface API which is present in multiple NoSQL databases. After the increment it re scans I to synchronize with the other task s modifications. The final increment happens upon the task s completion.

As shown in the user set U is partitioned into multiple sets U U U and so on. The mapper units execute an embodiment of a BGD in parallel on respective multiple machines or processors. Each mapper unit receives a partitioned set of user U content item I interactions and asynchronously estimates CF model parameters for the item I and one or more users in respective user sets U U U.

In an embodiment the input to each mapper unit is user item interaction data partitioned for each set of users. For example mapper unit receives input of user item interactions for a set of users U. The user item interaction data includes vectors indicating click activity of users accessing certain items of content. Each mapper unit e.g. a particular machine or processor asynchronously executes a BGD algorithm only for a corresponding subset of users e.g. users U in case of mapper unit . The mapper units generate separate latent model parameters for users U and items I. Each user and each item is modeled by a latent vector. Thus each mapper unit has two outputs 1 the latent vectors or of the users in the respective partition 2 the contribution I or e.g. an item vector model parameter change to the shared item model I . The contribution I to the shared item model I is applied to model multiple times asynchronously during the task s execution.

In an embodiment each mapper unit asynchronously writes to its respective user data repository after it is finished modeling all item user pairs for its respective set of users. Thus in the foregoing embodiment the mapper units update the local user model a matrix Ut which is written back upon the task s completion and b update the shared item model also a matrix I to is periodically added an increment It . After the update the task re scans I to its local cache. The advantage of updating I and re scanning it asynchronously is synchronizing with the partial computation results in the other tasks. This accelerates the model convergence. In an embodiment the foregoing asynchronous functionality replaces direct message exchange among the tasks and results in a computational performance boost in the context of using the BGD algorithm for CF model parameter learning.

In a further embodiment the mapper units provide fault tolerant task execution when a given mapper unit fails by re running the BGD model parameter learning algorithm. In particular the replacement mapper unit starts updating the model starting from the step at which the failure has happened. This can be achieved by checkpointing the minimal task state in the database such as by storing the number of times that the I database was updated. Then the backup task skips these first updates and proceeds regularly from that point.

The computer for example includes COM ports connected to and from a network connected thereto to facilitate data communications. The computer also includes a central processing unit CPU in the form of one or more processors for executing program instructions. The exemplary computer platform includes an internal communication bus program storage and data storage of different forms e.g. disk read only memory ROM or random access memory RAM for various data files to be processed and or communicated by the computer as well as possibly program instructions to be executed by the CPU. The computer also includes an I O component supporting input output flows between the computer and other components therein such as user interface elements . The computer may also receive programming and data via network communications.

Hence at least some aspects of the methods of the methods described herein may be embodied in programming. Program aspects of the technology may be thought of as products or articles of manufacture typically in the form of executable code and or associated data that is carried on embodied in or physically stored on a type of machine readable medium. Tangible non transitory storage type media include any or all of the memory or other storage for the computers processors or the like or associated modules thereof such as various semiconductor memories tape drives disk drives and the like which may provide storage at any time for the software programming.

All or portions of the software may at times be communicated through a network such as the Internet or various other telecommunication networks. Such communications for example may enable loading of the software from one computer or processor into another. Thus another type of media that may bear the software elements includes optical electrical and electromagnetic waves such as used across physical interfaces between local devices through wired and optical landline networks and over various air links. The physical elements that carry such waves such as wired or wireless links optical links or the like also may be considered as media bearing the software. As used herein unless restricted to tangible storage media terms such as computer or machine readable medium refer to any medium that participates in providing instructions to a processor for execution.

Hence a machine readable medium may take many forms including but not limited to a tangible storage medium a carrier wave medium or physical transmission medium. Non volatile storage media include for example optical or magnetic disks such as any of the storage devices in any computer s or the like which may be used to implement the system or any of its components as shown in the drawings. Volatile storage media include dynamic memory such as a main memory of such a computer platform. Tangible transmission media include coaxial cables copper wire and fiber optics including the wires that form a bus within a computer system. Carrier wave transmission media can take the form of electric or electromagnetic signals or acoustic or light waves such as those generated during radio frequency RF and infrared IR data communications. Common forms of computer readable media therefore include for example a floppy disk a flexible disk hard disk magnetic tape any other magnetic medium a CD ROM DVD or DVD ROM any other optical medium punch cards paper tape any other physical storage medium with patterns of holes a RAM a PROM and EPROM a FLASH EPROM any other memory chip or cartridge a carrier wave transporting data or instructions cables or links transporting such a carrier wave or any other medium from which a computer can read programming code and or data. Many of these forms of computer readable media may be involved in carrying one or more sequences of one or more instructions to a processor for execution.

Those skilled in the art will recognize that the present teachings are amenable to a variety of modifications and or enhancements. For example although the implementation of various components described above may be embodied in a hardware device it can also be implemented as a software only solution e.g. an installation on an existing server. In addition the ad modality selection server and its components as disclosed herein can be implemented as a firmware firmware software combination firmware hardware combination or a hardware firmware software combination.

While the foregoing has described what are considered to be the best mode and or other examples it is understood that various modifications may be made therein and that the subject matter disclosed herein may be implemented in various forms and examples and that the teachings may be applied in numerous applications only some of which have been described herein. It is intended by the following claims to claim any and all applications modifications and variations that fall within the true scope of the present teachings.

