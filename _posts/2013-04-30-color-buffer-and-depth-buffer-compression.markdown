---

title: Color buffer and depth buffer compression
abstract: In an example, a method of coding graphics data comprising a plurality of pixels includes performing, by a graphics processing unit (GPU), multi-sample anti-aliasing to generate one or more sample values for each pixel of the plurality of pixels. The method may also include determining whether pixels comprise edge pixels, where the determination comprises identifying, for each pixel, differing sample values. The method may also include encoding the pixels based on the edge pixel determination.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09330475&OS=09330475&RS=09330475
owner: QUALCOMM Incorporated
number: 09330475
owner_city: San Diego
owner_country: US
publication_date: 20130430
---
This application claims the benefit of U.S. Provisional Application No. 61 641 257 filed 1 May 2012 the entire contents of which is hereby incorporated by reference.

A device that provides content for visual presentation generally includes a graphics processing unit GPU . The GPU renders pixels that are representative of the content on a display. The GPU generates one or more pixel values for each pixel on the display and performs graphics processing on the pixel values for each pixel on the display to render each pixel for presentation.

Aspects of this disclosure generally relate to compressing graphics data which may be referred to as coding graphics data. For example a pixel of graphics data may be composed of one or more samples e.g. one or more color samples one or more depth samples and the like with each sample contributing to the pixel value. Aspects of this disclosure generally relate to compressing the sample values associated with a pixel to reduce the amount of data that is needed to represent pixel values. Aspects of this disclosure may also relate to identifying pixels located at the edges of objects which may have differing sample values. In some examples sample deltas may be generated relative to a base value when compressing the samples associated with edge pixels.

In an example a method of encoding graphics data comprising a plurality of pixels includes determining by a graphics processing unit GPU one or more sample values for each pixel of the plurality of pixels determining whether each pixel comprises an edge pixel wherein the determination comprises identifying for each pixel differing sample values of the one or more sample values for the respective pixel and encoding the pixels based on the edge pixel determination.

In another example an apparatus for encoding graphics data comprising a plurality of pixels includes one or more processors configured to determine one or more sample values for each pixel of the plurality of pixels determine whether each pixel comprises an edge pixel wherein the determination comprises identifying for each pixel differing sample values of the one or more sample values for the respective pixel and encode the pixels based on the edge pixel determination.

In another example an apparatus for encoding graphics data comprising a plurality of pixels includes means for determining one or more sample values for each pixel of the plurality of pixels means for determining whether each pixel comprises an edge pixel wherein the determination comprises identifying for each pixel differing sample values of the one or more sample values for the respective pixel and means for encoding the pixels based on the edge pixel determination.

In another example a non transitory computer readable medium having instructions stored thereon that when executed cause one or more processors to determine one or more sample values for each pixel of the plurality of pixels determine whether each pixel comprises an edge pixel wherein the determination comprises identifying for each pixel differing sample values of the one or more sample values for the respective pixel and encode the pixels based on the edge pixel determination.

The details of one or more examples of the disclosure are set forth in the accompanying drawings and the description below. Other features objects and advantages will be apparent from the description and drawings and from the claims.

In the example of computing device includes a central processing unit CPU having CPU memory a graphics processing unit GPU having GPU memory and coding unit a display unit a display buffer unit a user interface unit and a storage unit . In addition storage unit may store GPU driver having compiler GPU program and locally compiled GPU program .

Examples of CPU include but are not limited to a digital signal processor DSP general purpose microprocessor application specific integrated circuit ASIC field programmable logic array FPGA or other equivalent integrated or discrete logic circuitry. Although CPU and GPU are illustrated as separate units in the example of in some examples CPU and GPU may be integrated into a single unit. CPU may execute one or more applications. Examples of the applications may include web browsers e mail applications spreadsheets video games audio and or video capture playback or editing applications or other applications that initiate the generation for image data to be presented via display unit .

In the example shown in CPU includes CPU memory . CPU memory may represent on chip storage or memory used in executing machine or object code. CPU memory may each comprise a hardware memory register capable of storing a fixed number of digital bits. CPU may be able to read values from or write values to local CPU memory more quickly than reading values from or writing values to storage unit which may be system memory that is accessed e.g. over a system bus.

GPU represents one or more dedicated processors for performing graphical operations. That is for example GPU may be a dedicated hardware unit having fixed function and programmable components for rendering graphics and executing GPU applications. GPU may also include a DSP a general purpose microprocessor an ASIC an FPGA or other equivalent integrated or discrete logic circuitry.

GPU also includes GPU memory which may represent on chip storage or memory used in executing machine or object code. GPU memory may comprise one or more hardware memory registers capable of storing a fixed number of digital bits. GPU may be able to read values from or write values to local GPU memory more quickly than reading values from or writing values to storage unit which may be accessed e.g. over a system bus.

Display unit represents a unit capable of displaying video data images text or any other type of data for consumption by a viewer. Display unit may include a liquid crystal display LCD a light emitting diode LED display an organic LED OLED an active matrix OLED AMOLED display or the like.

Display buffer unit represents a memory or storage device dedicated to storing data for presentation of imagery such as photos or video frames for display unit . Display buffer unit may represent a two dimensional buffer that includes a plurality of storage locations. The number of storage locations within display buffer unit may be substantially similar to the number of pixels to be displayed on display unit . For example if display unit is configured to include 640 480 pixels display buffer unit may include 640 480 storage locations. Display buffer unit may store the final pixel values for each of the pixels processed by GPU . Display unit may retrieve the final pixel values from display buffer unit and display the final image based on the pixel values stored in display buffer unit .

User interface unit represents a unit with which a user may interact or otherwise interface to communicate with other units of computing device such as CPU . Examples of user interface unit include but are not limited to a trackball a mouse a keyboard and other types of input devices. User interface unit may also be a touch screen and may be incorporated as a part of display unit .

Storage unit may comprise one or more computer readable storage media. Examples of storage unit include but are not limited to a random access memory RAM a read only memory ROM an electrically erasable programmable read only memory EEPROM CD ROM or other optical disk storage magnetic disk storage or other magnetic storage devices flash memory or any other medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer or a processor.

In some example implementations storage unit may include instructions that cause CPU and or GPU to perform the functions ascribed to CPU and GPU in this disclosure. Storage unit may in some examples be considered as a non transitory storage medium. The term non transitory may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. However the term non transitory should not be interpreted to mean that storage unit is non movable. As one example storage unit may be removed from computing device and moved to another device. As another example a storage unit substantially similar to storage unit may be inserted into computing device . In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM .

Storage unit stores a GPU driver and compiler GPU program and locally compiled GPU program . GPU driver represents a computer program or executable code that provides an interface to access GPU . CPU executes GPU driver or portions thereof to interface with GPU and for this reason GPU driver is shown in the example of as a dash lined box labeled GPU driver within CPU . GPU driver is accessible to programs or other executables executed by CPU including GPU program .

GPU program may include code written in a high level HL programming language e.g. using an application programming interface API . Examples of APIs include Open Computing Language OpenCL Open Graphics Library OpenGL and DirectX as developed by Microsoft Inc. In general an API includes a predetermined standardized set of commands that are executed by associated hardware. API commands allow a user to instruct hardware components of a GPU to execute commands without user knowledge as to the specifics of the hardware components.

GPU program may invoke or otherwise include one or more functions provided by GPU driver . CPU generally executes the program in which GPU program is embedded and upon encountering GPU program passes GPU program to GPU driver e.g. in the form of a command stream . CPU executes GPU driver in this context to process GPU program . That is for example GPU driver may process GPU program by compiling GPU program into object or machine code executable by GPU . This object code is shown in the example of as locally compiled GPU program .

In some examples compiler may operate in real time or near real time to compile GPU program during the execution of the program in which GPU program is embedded. For example compiler generally represents a module that reduces HL instructions defined in accordance with a HL programming language to low level LL instructions of a LL programming language. After compilation these LL instructions are capable of being executed by specific types of processors or other types of hardware such as FPGAs ASICs and the like including e.g. CPU and GPU .

LL programming languages are considered low level in the sense that they provide little abstraction or a lower level of abstraction from an instruction set architecture of a processor or the other types of hardware. LL languages generally refer to assembly and or machine languages. Assembly languages are a slightly higher LL language than machine languages but generally assembly languages can be converted into machine languages without the use of a compiler or other translation module. Machine languages represent any language that defines instructions that are similar if not the same as those natively executed by the underlying hardware e.g. a processor such as the x86 machine code where the x86 refers to an instruction set architecture of an x86 processor developed by Intel Corporation as one example.

In any case compiler may translate HL instructions defined in accordance with a HL programming language into LL instructions supported by the underlying hardware. Compiler removes the abstraction associated with HL programming languages and APIs such that the software defined in accordance with these HL programming languages is capable of being more directly executed by the actual underlying hardware.

In the example of compiler may receive GPU program from CPU when executing HL code that includes GPU program . Compiler may compile GPU program to generate locally compiled GPU program that conforms to a LL programming language. Compiler then outputs locally compiled GPU program that includes the LL instructions.

GPU generally receives locally compiled GPU program as shown by the dashed lined box labeled locally compiled GPU program within GPU whereupon in some instances GPU renders an image and outputs the rendered portions of the image to display buffer unit . For example GPU may generate a number of primitives to be displayed at display unit . Primitives may include one or more of a line including curves splines etc. a point a circle an ellipse a polygon where typically a polygon is defined as a collection of one or more triangles or any other two dimensional 2D primitive. The term primitive may also refer to three dimensional 3D primitives such as cubes cylinders sphere cone pyramid torus or the like. Generally the term primitive refers to any basic geometric shape or element capable of being rendered by GPU for display as an image or frame in the context of video data via display unit .

GPU may transform primitives and other state data e.g. that defines a color texture lighting camera configuration or other aspect of the primitives into a so called world space by applying one or more model transforms which may also be specified in the state data . Once transformed GPU may apply a view transform for the active camera which again may also be specified in the state data defining the camera to transform the coordinates of the primitives and lights into the camera or eye space. GPU may also perform vertex shading to render the appearance of the primitives in view of any active lights. GPU may perform vertex shading in one or more of the above model world or view space although it is commonly performed in the world space .

Once the primitives are shaded GPU may perform projections to project the image into a unit cube with extreme points as one example at 1 1 1 and 1 1 1 . This unit cube is commonly referred to as a canonical view volume. After transforming the model from the eye space to the canonical view volume GPU may perform clipping to remove any primitives that do not at least partially reside within the view volume. In other words GPU may remove any primitives that are not within the frame of the camera. GPU may then map the coordinates of the primitives from the view volume to the screen space effectively reducing the 3D coordinates of the primitives to the 2D coordinates of the screen.

Given the transformed and projected vertices defining the primitives with their associated shading data GPU may then rasterize the primitives. For example GPU may compute and set colors for the pixels of the screen covered by the primitives. During rasterization GPU may apply any textures associated with the primitives where textures may comprise state data . GPU may also perform a Z buffer algorithm also referred to as a depth test during rasterization to determine whether any of the primitives and or objects are occluded by any other objects. The Z buffer algorithm sorts primitives according to their depth so that GPU knows the order in which to draw each primitive to the screen. GPU outputs rendered pixels to display buffer unit .

In some examples aliasing occurs because real world objects have continuous smooth curves and lines while a typical display may be configured to output discrete points of light i.e. pixels. Pixels are uniformly colored and always of the same shape which may result in lines of a rendered image becoming jagged in appearance. With multi sample anti aliasing MSAA multiple samples may be generated for a single pixel. The samples may then be combined e.g. averaged to determine a final pixel value.

Accordingly GPU may render an image at a higher resolution than the resolution being displayed. GPU may then down sample the image to the appropriate size prior to display. The result may be smoother transitions from one line of pixels to another along the edges of objects. MSAA may be performed using a factor of 4 8 16 or other values.

As described herein a pixel may refer to a single fragment that is visible in an image. A sample may refer to a single value that contributes to a pixel. For example a sample may include a color value e.g. a single RGBA value that contributes to one pixel in the final image. In four sample MSAA mode four Red Green Blue Alpha RGBA samples may contribute to each pixel. In other examples a sample may include a depth value e.g. a z value . In addition an edge may refer to one or more points e.g. one or more pixels of an image having brightness color or depth discontinuities between sample values contributing to the points. For example in a four sample MSAA mode if all four samples are not equal the corresponding pixel may be classified as an edge pixel.

A block may refer to an N N group of pixels. A block cache may refer to GPU memory e.g. on chip storage that stores a set of uncompressed blocks. Block encoding may be computed when a block is written to system memory from the cache. Block decoding may be computed when a compressed block is read in to the cache from system memory. In some examples Red Blue RB or Red Green Blue RGB color writes may go to and RB color reads or RGB may come from the block cache.

If a pixel is covered by a single primitive all samples e.g. MSAA samples inside the pixel may share the same color and or depth information. Accordingly there may be 4 duplicate information at the sample level. It is also possible that four samples from multiple primitives share the same color and or depth data. For example pixels may have similar colors and or depths inside a relatively small block e.g. a 4 4 block of pixels . Aspects of this disclosure relate to compressing such color and depth data.

Display buffer unit may temporarily store the rendered pixels of the rendered image until the entire image is rendered. Display buffer unit may be considered as an image frame buffer in this context. Display buffer unit may then transmit the rendered image to be displayed on display unit . Display buffer unit may include storage for a variety of data including color data e.g. Red Green Blue Alpha RGBA or data in any other color space and depth data e.g. z values indicative of pixel depth relative to pixels of other primitives .

In any event as noted above GPU may read data from and write data to memory external to GPU e.g. display buffer unit and or storage unit referred to generally as system memory when rendering pixel data. The amount of data that needs to be transferred between system memory and GPU during rendering of a single image may be significant and may limit rendering performance. For example with respect to mode rendering immediate mode and or retained mode rendering GPU may render a single pixel of a final image multiple times. The bandwidth between local GPU memory and system memory may be further strained when GPU uses MSAA techniques due to the additional samples associated with MSAA. Accordingly when draw calls are used to directly render pixels to system memory sufficient memory bandwidth may not be available to transfer color and depth data between system memory and local GPU memory .

In addition typical video and or image compression algorithms may not be optimized to perform compression of 3D color buffer rendering as the memory access patterns may be random unit access sizes may be small and the compression for graphics rendering may need to be lossless. Further typical video and or image compression techniques may not be applicable to computer generated graphics as the concept of MSAA is not generally applicable in the video image domain.

Aspects of this disclosure generally relate to compressing graphics data which may reduce the amount of data that needs to be transferred between GPU memory and system memory i.e. display unit buffer and or storage unit . For example according to aspects of this disclosure GPU may compress sample values associated with a pixel to reduce the amount of data that is needed to represent pixel values.

In the example shown in coding unit may be responsible for coding graphics data. For example coding unit may encode rendered graphics data when writing the graphics data to display unit buffer and or storage unit as coded data . Likewise coding unit may decode the coded data when reading coded data from display unit buffer and or storage unit . While coding unit is shown in the example of as being included in GPU in other examples coding unit may be incorporated in any other component of computing device .

In some examples coding unit may code compress edge pixels differently than non edge pixels. For example as noted above samples for non edge pixels may share the same color depth and or other samples. Accordingly GPU may represent all samples with the same single sample value. However as noted above edge pixels do not have equal sample values. Selecting a representative sample value for all of the samples may result in lossy compression.

According to aspects of this disclosure coding unit may code non edge pixels by combining all of the sample values for a pixel. For example a non edge pixel sampled using a four sample MSAA scheme may have four equal color samples. Accordingly coding unit may combine the samples into a single sample value.

Coding unit may detect an edge pixel by identifying samples associated with a pixel that have differing values e.g. at least one color brightness and or depth value that is not equal to the other samples associated with the other samples of the pixel . Coding unit may detect an edge pixel from a rasterizer or render backend as shown in below . After detecting an edge pixel coding unit may determine a base value for the edge pixel and may only store bits for the samples that are different than the base value which may be referred to as bit plane based compression. In this way coding unit may retain all of the information for the samples while still compressing the amount of data required to represent the pixel.

Assume for purposes of explanation that coding unit is configured to code 4 4 blocks of pixels when writing such blocks from GPU memory to display unit buffer and or storage unit . Assume also that each pixel is sampled using a four sample MSAA scheme. After detecting edge pixels coding unit may generate an edge mask for the block that identifies the locations of the edge pixels. For example coding unit may generate a 16 bit edge mask that identifies the locations of edge pixels in the 4 4 block e.g. 1 bit per pixel where 0 represents a non edge pixel and 1 represents an edge pixel or vice versa .

When coding color data coding unit may also determine a base color for the block. In an example in which an RGBA8888 format is used for color samples the base color may be represented using one 32 bit value. Coding unit may determine the base color for the block by combining the color samples. For example coding unit may perform an AND operation to combine samples or determine an arithmetic average mean median or the like.

Coding unit may then determine delta values for each pixel in the block where the delta values represent differences between the actual sample colors and the base color. For example coding unit may determine one delta for each non edge pixel which is equal to the difference between the base color and the color of the samples which is the same for non edge pixels . In addition coding unit may determine delta values for each sample value associated with edge pixels. Accordingly in a 4 MSAA scheme coding unit may determine four delta values for each edge pixel.

Coding unit may then determine a location mask for the block that indicates the location of each sample of the block that is not equal to the base color samples for which there is a delta that is greater than zero . Accordingly for the 4 4 block with 4 MSAA discussed above coding unit may generate a 32 bit value that includes a bit that is set to one for each sample having a different color than the base color and a bit that is set to zero for each sample having the same color as the base color or vice versa .

With respect to decoding coding unit may decode blocks when reading the blocks from display unit buffer and or storage unit . According to aspects of this disclosure coding unit may decode a coded block by scanning pixels in edge mask order. If a pixel is an edge pixel the four next sample deltas may correspond to the current pixel e.g. for every delta color channel . If a pixel is not an edge pixel coding unit may use one delta to replicate the color for all four samples of the pixel. Deltas may be per color channel and the edge mask may be the same for all color channels of the pixel. Thus for every sample the resulting color is a combination of the base color and the delta. A delta bit count per color channel may be computed from the color mask. Each delta bit location may also be read from the color mask.

According to aspects of this disclosure the same compression techniques described above may be applied to color and depth data thereby providing hardware cost reductions e.g. reductions to computational load and or memory buss requirements . For example in addition to coding color data coding unit may also encode decode depth data e.g. such as z values . That is coding unit may code non edge pixels by combining all of the depth samples values for a pixel. For edge pixels coding unit may determine a base depth value and may only store bits for the samples that are different than the base value as described above with respect to the color compression example.

In some examples RGBA channels may have different characteristics at a block level. Accordingly adaptive memory footprints for RGBA channels may be beneficial. For example for 4 4 block having an RGBA8888 color format and 4 MSAA a block size may be 4 4 4 32 bits 2048 bits. In this example a 512 bit compressed block generated according to the techniques of this disclosure is one fourth of the original memory footprint. A fixed set of pre determined block sizes can be used. Aspects of this disclosure may be implemented to compress a large variety of block sizes.

Aspects of this disclosure may be applied to floating point and or integer render targets and depth buffers. As noted above the techniques of this disclosure may generally be lossless and may be applied in various MSAA schemes e.g. 1 2 4 8 16 and the like . The techniques of this disclosure may in some examples decrease memory consumption rates by a factor of two and up to six or more in some applications. Conserving memory bandwidth may also reduce power consumption due to power savings from components responsible for data transfer.

It should be understood that computing device is provided as merely an example and other computing devices performing the techniques of this disclosure may be arranged differently. For example while display buffer unit is shown and described separately from storage unit in other examples display unit buffer and storage unit may be incorporated into the same component.

Moreover it should be understood that computing device may include additional modules or units not shown in for purposes of clarity. For example computing device may include a transceiver module for transmitting and receiving data and may include circuitry to allow wireless or wired communication between computing device and another device or a network. Computing device may also include a speaker and a microphone neither of which are shown in to effectuate telephonic communications in examples where computing device is a mobile wireless telephone such as a smartphone or a speaker where computing device is a media player or tablet computer. In some instances user interface unit and display unit may be external to computing device in examples where computing device is a desktop computer or other device that is equipped to interface with an external user interface or display.

Graphics processing pipeline generally includes programmable stages e.g. illustrated in by blocks with rounded corners and fixed function stages e.g. illustrated in by blocks with squared corners . For example graphics rendering operations associated with certain stages of graphics rendering pipeline are generally performed by a programmable shader processor while other graphics rendering operations associated with other stages of graphics rendering pipeline are generally preformed by non programmable fixed function hardware units associated with GPU . Graphics rendering stages performed by shading units may generally be referred to as programmable stages while stages performed by fixed function units may generally be referred to as fixed function stages.

Input assembler stage is shown in the example of as a fixed function stage and is generally responsible for supplying graphics data triangles lines and points to graphics processing pipeline . For example input assembler stage may collect vertex data for high order surfaces primitives and the like and output vertex data and attributes to vertex shader stage . Accordingly input assembler stage may read vertices from an off chip memory such as storage unit using fixed function operations. Input assembler stage may then create pipeline work items from these vertices while also generating vertex identifiers VertexIDs instance identifiers InstanceIDs which are made available to the vertex shader and primitive identifiers PrimitiveIDs which are available to the geometry shader and pixel shader . Input assembler stage may automatically generate VertexIDs InstanceIDs and PrimitiveIDs upon reading the vertices.

Vertex shader stage may process the received vertex data and attributes. For example vertex shader stage may perform per vertex processing such as transformations skinning vertex displacement and calculating per vertex material attributes. In some examples vertex shader stage may generate texture coordinates vertex color vertex lighting fog factors and the like. Vertex shader stage generally takes a single input vertex and outputs a single processed output vertex.

Hull shader stage receives primitives from vertex shader stage and is responsible for carrying out at least two actions. First hull shader stage is typically responsible for determining a set of tessellation factors. Hull shader stage may generate tessellation factors once per primitive. The tessellation factors may be used by tessellator stage to determine how finely to tessellate a given primitive e.g. split the primitive into smaller parts . Hull shader stage is also responsible for generating control points that will later be used by domain shader stage . That is for example hull shader stage is responsible for generating control points that will be used by domain shader stage to create actual tessellated vertices which are eventually used in rendering.

When tessellator stage receives data from hull shader stage tessellator stage uses one of several algorithms to determine an appropriate sampling pattern for the current primitive type. For example in general tessellator stage converts a requested amount of tessellation as determined by hull shader stage into a group of coordinate points within a current domain. That is depending on the tessellation factors from hull shader stage as well as the particular configuration of the tessellator stage tessellator stage determines which points in a current primitive need to be sampled in order to tessellate the input primitive into smaller parts. The output of tessellator stage may be a set of domain points which may include barycentric coordinates.

Domain shader stage takes the domain points in addition to control points produced by hull shader stage and uses the domain points to create new vertices. Domain shader stage can use the complete list of control points generated for the current primitive textures procedural algorithms or anything else to convert the barycentric location for each tessellated point into the output geometry that is passed on to the next stage in the pipeline.

Geometry shader stage may receive a primitive defined by the vertex data e.g. three vertices for a triangle two vertices for a line or a single vertex for a point and further process the primitive. For example geometry shader stage may perform per primitive processing such as silhouette edge detection and shadow volume extrusion among other possible processing operations. Accordingly geometry shader stage may receive one primitive as an input which may include one or more vertices and outputs zero one or multiple primitives which again may include one or more vertices . The output primitive may contain more data than may be possible without geometry shader stage . The total amount of output data may be equal to the vertex size multiplied by the vertex count and may be limited per invocation. The stream output from geometry shader stage may allow primitives reaching this stage to be stored to the off chip memory such as storage unit . The stream output is typically tied to geometry shader stage and the stream output and geometry shader stage may be programmed together e.g. using an API .

Rasterizer stage is typically a fixed function stage that is responsible for clipping primitives and preparing primitives for pixel shader stage . For example rasterizer stage may perform clipping including custom clip boundaries perspective divide viewport scissor selection and implementation render target selection and primitive setup. In this way rasterizer stage may generate a number of fragments for shading by pixel shader stage .

Pixel shader stage receives fragments from rasterizer stage and generates per pixel data such as color. Pixel shader stage may also perform per pixel processing such as texture blending and lighting model computation. Accordingly pixel shader stage may receive one pixel as an input and may output one pixel at the same relative position or a zero value for the pixel .

Output merger stage is generally responsible for combining various types of output data such as pixel shader values depth and stencil information to generate a final result. For example output merger stage may perform fixed function blend depth and or stencil operations for a render target pixel position .

In some examples according to aspects of this disclosure GPU may compress shaded pixels from stream out e.g. following geometry shader stage or output merger stage . For example GPU may write shaded pixels from output merger stage to an on chip memory cache such as GPU memory . GPU may perform the techniques of this disclosure to compress the shaded pixel data when transferring the pixel data from on chip memory to memory external to GPU . In some examples GPU may perform compression on a block by block basis e.g. 4 4 blocks 8 8 blocks 16 16 blocks and the like .

In the example shown in pixel A C and D are non edge pixels with sample values of 7 0000111 value for purposes of illustration only . Pixel B however is an edge pixel having one sample with a value of 7 0000111 and three samples with a value of 5 00000101 again value for purposes of illustration only .

Accordingly as shown in GPU may identify pixels A C and D as non edge pixels and may compress the sample values to a single sample value 7 . Due to the differing sample values GPU may determine that pixel B is an edge pixel and may not compress the sample values into a single value.

GPU may generate an edge mask to indicate which pixels are edge pixels. In the example shown in the edge mask may be 0100 e.g. in raster order with the 1 indicating B as the edge pixel . As noted above with respect to GPU may also generate a base value. For example GPU may generate a base value using a bitwise AND operation that may be equal to 7 5 5 7 5 7 7 5 00000101 . In other examples GPU may generate the base value using an alternative method e.g. arithmetic average mean median and the like .

In some examples GPU may indicate the delta values using a difference mask. For example in the example shown in GPU may perform a bitwise XOR operation that may be equal to 7 5 5 7 5 7 7 2 00000010 . GPU may also generate a location mask to indicate the location of each of the samples with which the actual sample value differs from the base value. In some examples GPU may set one location bit for each non edge pixel because all samples are the same and four location bits for each edge pixel one bit for each sample . Accordingly in the example shown in the location mask may be equal to 1 0 0 1 0 1 1 which yields 1001011 raster order . That is with respect to 1 relates to pixel A 0 0 1 0 relates to pixel B 1 relates to pixel C and 1 relates to pixel D.

According to aspects of this disclosure instead of storing 8 bits per pixel bpp for a single color e.g. 4 samples 4 samples 8 bits of color 128b a compressed block memory footprint may be approximately 1.7 bpp e.g. 8 bit base color 8 bit difference mask 4 bit edge mask 7 bits of location mask deltas 1 4 1 1 1 27 bits per 2 2 block of pixels . Hence 27 bits per block divided by 16 MSAA samples is equal to approximately 1.68 bpp.

With respect to the example shown above it is assumed that the format is an RGBA8888 format and that 4 MSAA is used. According to this example the compressed block stores 32b base color 8b per channel 32b difference mask 8b per channel 16b edge mask 1b per pixel and variable size location masks per encoded sample e.g. 0 8b per sample with a different bit for each color channel . Thus the compressed block memory footprint is roughly 96 bits plus delta bits. The delta memory footprint may depend on block content. While the example described above is described with respect to a specific format and aliasing scheme it should be understood that the techniques of this disclosure may be applied to other formats and other anti aliasing techniques.

In addition while the example shown in is described with respect to color values the same techniques may be performed for depth values. For example with respect to z values each pixel may have four associated z values. GPU may perform depth compression in the same manner as color compression described above. That is GPU may generate a base depth an edge mask e.g. indicating which pixels are edge pixels a difference mask e.g. indicating differences between the base depth and actual depths and a location mask e.g. indicating the locations for samples having non zero delta values .

In the example of GPU renders pixels values to a cache . The cache may be included for example in GPU memory such that the cache is an on chip memory. GPU may determine whether the cache is full or that the data written to the cache is otherwise ready to be written out from on chip memory .

If the cache is full the yes branch of step GPU may encode color values for the pixels in the cache . For example for non edge pixels GPU may compress the sample values to generate a single value for all samples. For edge pixels GPU may not compress the sample values. In any case GPU may determine a base value for the samples and determine the differences between the base value and the actual sample values.

In addition GPU may encode depth values for the pixels in the cache . For example as described above GPU may encode depth values in the same way as color values. That is for non edge pixels GPU may compress the sample values to generate a single value for all samples. For edge pixels GPU may not compress the sample values. In any case GPU may determine a base depth value for the samples and determine the differences between the base depth value and the actual sample values.

After coding the color and depth values GPU may write the encoded graphics data from the cache to system memory . For example GPU may write the encoded graphics data from the cache to display unit buffer and or storage unit . It should be understood that the steps shown in are provided as merely one example. That is the steps shown in need not necessarily be performed in the order shown and fewer additional or alternative steps may be performed.

In the example shown in GPU may determine whether pixels are edge pixels . For example with respect to encoding color GPU may determine whether pixels are edge pixels by identifying differing color samples for a pixel. With respect to depth GPU may determine whether pixels are edge pixels by identifying differing depth samples for a pixel. In any case for 4 MSAA if all four sample values of a pixel are equal GPU may identify the pixel as a non edge pixel. Alternatively if one or more of the sample values of a pixel are different GPU may identify the pixel as an edge pixel.

GPU may generate an edge mask to indicate the locations of edge pixels in a block of pixels . For example GPU may generate a bit string that indicates the position of each edge pixel in a block of pixels. GPU may also generate a base value for the block . That is when encoding color GPU may generate the base color for the block. When encoding depth information GPU may generate a base depth value.

After generating the base value GPU may generate delta values that indicate a difference between the base value and the actual value for each of the samples in the block . In some examples GPU may generate a single delta value for non edge pixels which represents the difference between the base value and a combined sample value because all samples are equal in value . GPU may generate delta values for each sample of edge pixels which also represent the difference between the base value and the sample values.

GPU may also generate a location mask to indicate which sample values are associated with the generated deltas . For example GPU may generate a bit string indicating the location of each sample value that differs from the base value. In some examples non edge pixels can be represented with a single value regardless of the number of samples associated with the pixel as described above.

It should be understood that the steps shown in are provided as merely one example. That is the steps shown in need not necessarily be performed in the order shown and fewer additional or alternative steps may be performed.

GPU may retrieve pixel values from system memory . For example GPU may read pixel values from display unit buffer and or storage unit . GPU may also determine whether the pixel values being read are encoded values . That is in some examples GPU may be preconfigured to perform encoding and decoding of graphics data any time such data is transferred between local GPU memory such as GPU memory and system memory such as display unit buffer and or storage unit . In such examples the determining step may not be carried out. In other examples however GPU may be configurable and may not always perform encoding decoding functions e.g. depending on system resources . In such examples GPU may determine whether data being read from system memory has been previously encoded.

In any case if the pixel values are encoded the yes branch of step GPU may decode the color values for the pixels . For example GPU may determine a base value determine which samples are different than the base value and combine received delta values with the base value to generate the decoded pixel values. As noted above the base value edge mask indicating samples that are different than the base value and delta mask may be provided with the coded data. GPU may also decode depth values for the pixels in the same way . After decoding the color and or depth values GPU may write the pixel values to a cache such as GPU memory .

It should be understood that the steps shown in are provided as merely one example. That is the steps shown in need not necessarily be performed in the order shown and fewer additional or alternative steps may be performed. For example while illustrates decoding both color and depth values in some examples only color or depth values may be decoded.

In the example of GPU may determine a base value for a block of pixels . In some examples GPU may receive an indication of the base value. For example as noted above with respect to GPU may determine the base value at the time of encoding. Accordingly the base value may be included with the retrieved data. GPU may also determine delta values for pixels of the block . In the same way as the base value the delta values for the block may be included with retrieved data.

The manner in which the received base value and received delta values are combined may depend on whether pixels are edge pixels or non edge pixels. Accordingly GPU may determine whether a pixel is an edge pixel or non edge pixel for example using a received edge mask .

GPU may scan pixels in edge mask order. For edge pixels the yes branch of step GPU may combine the next four for 4 MSAA delta values with the base value to generate the actual separate sample values e.g. 4 actual sample values for the pixel . If a pixel is not an edge pixel the no branch of step GPU may combine the next single delta value with the base value to generate a combined sample value for the pixel .

The steps described with respect to may be performed for decoding color and or depth values. It should be understood that the steps shown in are provided as merely one example. That is the steps shown in need not necessarily be performed in the order shown and fewer additional or alternative steps may be performed.

It should also be understood that depending on the example certain acts or events of any of the methods described herein can be performed in a different sequence may be added merged or left out all together e.g. not all described acts or events are necessary for the practice of the method . Moreover in certain examples acts or events may be performed concurrently e.g. through multi threaded processing interrupt processing or multiple processors rather than sequentially.

In one or more examples the functions described may be implemented in hardware software firmware or any combination thereof. If implemented in software the functions may be stored as one or more instructions or code on an article of manufacture comprising a non transitory computer readable medium. Computer readable media may include computer data storage media. Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions code and or data structures for implementation of the techniques described in this disclosure. By way of example and not limitation such computer readable media can comprise RAM ROM EEPROM CD ROM or other optical disk storage magnetic disk storage or other magnetic storage devices flash memory or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be accessed by a computer. Disk and disc as used herein includes compact disc CD laser disc optical disc digital versatile disc DVD floppy disk and blu ray disc where disks usually reproduce data magnetically while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer readable media.

The code may be executed by one or more processors such as one or more DSPs general purpose microprocessors ASICs FPGAs or other equivalent integrated or discrete logic circuitry. In addition in some aspects the functionality described herein may be provided within dedicated hardware and or software modules. Also the techniques could be fully implemented in one or more circuits or logic elements.

The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses including a wireless handset an integrated circuit IC or a set of ICs e.g. a chip set . Various components modules or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques but do not necessarily require realization by different hardware units. Rather as described above various units may be combined in a codec hardware unit or provided by a collection of interoperative hardware units including one or more processors as described above in conjunction with suitable software and or firmware.

Various examples have been described. These and other examples are within the scope of the following claims.

