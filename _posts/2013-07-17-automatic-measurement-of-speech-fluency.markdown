---

title: Automatic measurement of speech fluency
abstract: Techniques are described for automatically measuring fluency of a patient's speech based on prosodic characteristics thereof. The prosodic characteristics may include statistics regarding silent pauses, filled pauses, repetitions, or fundamental frequency of the patient's speech. The statistics may include a count, average number of occurrences, duration, average duration, frequency of occurrence, standard deviation, or other statistics. In one embodiment, a method includes receiving an audio sample that includes speech of a patient, analyzing the audio sample to identify prosodic characteristics of the speech of the patient, and automatically measuring fluency of the speech of the patient based on the prosodic characteristics. These techniques may present several advantages, such as objectively measuring fluency of a patient's speech without requiring a manual transcription or other manual intervention in the analysis process.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09230539&OS=09230539&RS=09230539
owner: Regents of the University of Minnesota
number: 09230539
owner_city: Minneapolis
owner_country: US
publication_date: 20130717
---
This application is a continuation of U.S. patent application Ser. No. 12 652 535 filed Jan. 5 2010 which claims the benefit of U.S. Provisional Application No. 61 142 788 filed Jan. 6 2009 each of which is hereby incorporated by reference in its respective entirety.

Researchers and others have used computers to receive speech in a variety of contexts. For example computers have been programmed to receive a person s speech and transcribe the speech into an electronic document i.e. speech to text . Speech to text programs often require that the speaker read one or more prepared documents to the computer. The computer then aligns the spoken portion with the text to develop a model of the speaker s voice. When a new speaker uses the program the new speaker must read the prepared documents before the speech to text program will work effectively.

Another instance of computerized speech recognition involves a human operator who transcribes a speaker s dialog. The operator then inputs the transcription and a recording of the audio to a computer that then processes the recorded audio in light of the transcription. The speaker may therefore speak spontaneously but human intervention is required in the form of the transcription.

In general computerized speech recognition programs utilize a statistical model of a language such as the English language based on common words of that language. Computerized speech recognition programs are often constructed to recognize particular words of the language and to ignore other sounds or parts of speech. In this manner these recognition programs accept recognized words and reject other sounds such as mumbled speech or a cough or other non linguistic sound. These speech recognition programs discard disfluencies such as silent pauses filled pauses e.g. umm or ahh and false starts to create a text document that does not include such disfluencies.

In general techniques are described for automating speech analysis. In one embodiment an analysis system collects data from a patient s speech regarding prosodic characteristics of the patient s speech. The patient may have a speech impairment such as aphasia caused by fronto temporal dementia FTD Alzheimer s disease or a precursor thereof schizophrenia epilepsy autism spectrum disorders including Asperger s syndrome side effects caused by medication or other impairments. The data may be automatically analyzed to determine for example fluency of a patient s speech. The system analyzes a recorded audio sample of the patient s speech to automatically identify locations of phonemes and disfluencies of the audio sample. The system then identifies for example silent pauses filled pauses repetition of phonemes or intonation to measure fluency of the patient s speech.

In one embodiment a method includes receiving an audio sample that includes speech of a patient analyzing the audio sample to identify prosodic characteristics of the speech of the patient and automatically measuring fluency of the speech of the patient based on the prosodic characteristics.

In another embodiment a system includes a speech analyzer to analyze an audio sample including speech of a patient to automatically measure fluency of the speech of the patient and model data that includes at least one model of a characteristic of a language spoken by the patient.

In another embodiment a computer readable medium contains instructions. The computer readable medium may be a computer readable storage medium. The instructions may be executed e.g. by a programmable processor to cause the processor to receive an audio sample that includes speech of a patient analyze the audio sample to identify prosodic characteristics of the speech of the patient and measure fluency of the speech of the patient based on the prosodic characteristics.

In another embodiment a method includes providing an audio sample that includes speech of a patient to a speech analysis device wherein the speech analysis device analyzes the audio sample to identify prosodic characteristics of the speech of the patient and automatically measures fluency of the speech of the patient based on the prosodic characteristics and receiving an indication of the fluency of the speech of the patient from the speech analysis device.

The techniques described herein may present several advantages. For example the techniques may provide for automatic analysis of fluency of a patient s speech. Thus a patient s speech may be analyzed without for example a manual transcription thereof. The techniques therefore provide for automatic analysis of spontaneous speech from the patient. The prosodic characteristics e.g. silent pauses filled pauses repetition fundamental frequency etc. of the patient s speech may also be measured objectively and automatically.

The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features objects and advantages of the invention will be apparent from the description and drawings and from the claims.

In one embodiment speech analyzer may be a module of a computing device such as a server computer or a workstation computer. In another embodiment speech analyzer may be a module of a stand alone analysis device. All or part of speech analyzer may be implemented in hardware software firmware or any combination thereof. Speech analyzer receives and analyzes audio to produce speech analysis . In particular speech analyzer identifies various elements of audio relating to fluency of speech therein such as identifying silent pauses filled pauses false starts repetitions intonation or other speech elements related to fluency.

Speech analyzer operates to automatically analyze speech of audio . That is speech analyzer in one embodiment does not require the assistance of a human user to identify elements of speech of audio . Instead speech analyzer identifies phonemes and locations of phonemes within audio without the aid of a human made transcription of audio . In this manner speech analyzer automatically produces speech analysis that includes a measurement of a patient s speech fluency from prosodic characteristics of the patient s speech.

In the example of speech analyzer includes recognition engine and repetition detector . An example embodiment of recognition engine is described in greater detail with respect to . In general recognition engine identifies elements of the patient s speech of audio such as phonemes and pauses. Repetition detector identifies repeated sounds or words of the patient s speech of audio . In one embodiment repetition detector may also identify a fundamental frequency of the patient s speech. In another embodiment speech analyzer may include an additional module for identifying intonation of the patient s speech of audio . Repetition detector may use the identified fundamental frequency to hypothesize as to the patient s intonation. Repetition detector is discussed in greater detail with respect to . Recognition engine and repetition detector each receive audio in a raw form to perform their respective analyses as discussed in greater detail below.

Recognition engine sends output to analysis module and repetition detector sends output to analysis module . Analysis module measures fluency of the patient s speech from the results received from recognition engine and repetition detector . In one embodiment analysis module normalizes the various measurements e.g. the number of pauses the number of repetitions etc. to the duration of audio . In another embodiment analysis module may normalize the measurements by calculating a standard score a z score that represents the proportion of the difference between each individual measurement e.g. a specific silent pause duration and the speaker s or the sample s mean for that measurement e.g. mean silent pause duration to the standard deviation of the mean. The resulting measurement is computed as the mean of individual z scores. That is analysis module may use the standard z score formula z score measurement sample mean sample standard deviation . Analysis module may also perform normalization using other available methods for example methods discussed in C. Wightman and M. Ostendorf Automatic recognition of prosodic phrases in Toronto Canada 1991 and C. Wightman and M. Ostendorf Automatic labeling of prosodic patterns vol. 2 pp. 469 81 1994 both of which are incorporate herein by reference.

Analysis module generates speech analysis as output which analysis module may output in one or more various formats such as for example a flat file a report data saved to a database data sent to other modules for further analysis or reporting as discussed with respect to for example or in other formats. Speech analysis may include for example a number of silent pauses a number of filled pauses a total number of pauses silent and filled a mean or median length of pauses silent filled or both a standard deviation for the length of pauses a ratio of total pause time to audio time i.e. the total time of audio a number of repetitions and or other values. These measurements may be normalized to the duration of audio for the purposes of analysis or diagnosis. Speech analysis may also include analyzing the distribution of prosodic events e.g. silent and filled pauses and other speech events as a time series to construct models of the observed variation in the fluency of speech. The time series models may be periodic e.g. a sinusoidal model globally predictive of alternating periods of hesitant and fluent speech at regular intervals or aperiodic e.g. a Markov model able to predict the fluency of a given segment of speech based on the observed fluency in the local immediately surrounding context . Such analysis may be helpful in identifying and measuring the deviation of the model parameters established in a population with a cognitive disorder from a healthy population.

Systems and devices embodying the techniques of this disclosure such as system may be used in a variety of contexts. In one example system may be used as part of a clinical study to determine the effectiveness of various therapies such as drug therapies in treating disorders that affect a patient s speech fluency. In such examples a first group of patients is administered a controlled drug dosage and a second group of patients is administered a placebo. Therapists or clinicians unaware of which group has been administered the placebo and which has been administered the drug observe patients of both groups using system and record speech fluency for the patients. An indication that the group of patients who were administered the drug had a higher speech fluency by a statistically significant margin than the group of patients who were administered the placebo would tend to indicate that the drug is effective at treating the disease.

In another example system may be used to monitor an individual patient or a group of patients over time. A clinician may obtain a baseline speech fluency value for a patient at an initial meeting and observe the patient or patients over time to determine the effectiveness of one or more therapies and or to monitor the progression of an ailment. The clinician may also use the baseline speech fluency value to prescribe an initial drug dosage to the patient and then monitor the patient s speech fluency over time to determine whether to increase or decrease the dosage or whether to prescribe a different drug to the patient. The clinician may also use the speech fluency values over time to prescribe therapy regimens such as for example particular days on which to take particular dosages of drug drug infusion patterns e.g. times of day when a patient should take the drug other activities to be performed in combination with taking the drug or other elements of the therapy regimen.

In still another example system may be used to validate or calibrate other systems. For example system may be used in combination with an electroencephalogram EEG or magnetoencephalogram MEG . In some cases an EEG or MEG reports variations in high frequency signals for one or more patients relative to a group of healthy patients and a clinician may not be able to readily determine the origin of the variation. The clinician may utilize system along with the EEG or MEG to provide a possible origin for the signal variation in the patient. That is the clinician may observe a correlation between the variation in the signals and patients who have a relatively lower speech fluency as reported by system . In this manner the clinician may use the output of system to provide a basis for further researching the variation in the patient s brainwave activity.

Similarly system may be used in conjunction with another diagnostic tool such as a variable N gram overlap test. The N gram overlap test generally includes having a patient read a paragraph or observe a picture removing the paragraph or picture from the patient s possession after a period of time and asking the patient to recite the paragraph or describe the picture. Each N gram overlap with the original paragraph or expected description of the picture where an N gram typically includes one two three or more words is tallied to produce a score for each N gram. In general more overlap and higher scores in the higher N gram categories indicate better recall of the paragraph or picture which may indicate that the patient is relatively more healthy. System may be used during the variable N gram overlap test or as another test performed before or after the variable N gram overlap test to provide additional data for assessing the patient s condition.

Clinicians have also observed that younger children with the autism spectrum disorder tend to have distinct intonation and speech patterns. One potentially valuable area of research includes the development of speech models that include prosodic characteristics such as intonation for younger children who are healthy and speech models for children with various disorders such as autism spectrum disorders. Accordingly clinicians may develop speech models of children with autism spectrum disorders and then use system to diagnose autism spectrum disorders in children at an early age.

The system of may present several advantages. For example the system of may automatically analyze fluency of a patient s speech. Thus speech analyzer may analyze speech of audio without for example a manual transcription thereof. Audio may also include spontaneous natural speech from a patient. Accordingly the prosodic characteristics e.g. silent pauses filled pauses repetition fundamental frequency etc. of the patient s speech may be measured objectively and automatically.

Audio tokenizer produces individual sound units i.e. tokens from audio . A token may include for example a particular phoneme silence or white noise that is not a phoneme. In one embodiment audio tokenizer may label each token as a phoneme silence or white noise. In one embodiment audio tokenizer identifies a length of each token. Audio tokenizer retrieves data from acoustic model to distinguish phonemes from white noise and silence. Acoustic model in one embodiment is a model of a general language such as the English language. Acoustic model is developed from natural dialog which may include disfluencies such as for example silent pauses filled pauses false starts or other disfluencies. In general audio tokenizer refers to acoustic model to identify where individual phonemes begin and end. Audio tokenizer outputs individual tokens such as phoneme tokens silence tokens and white noise tokens to audio parser . In one embodiment audio tokenizer may discard white noise tokens.

Audio parser identifies the phonemes of phoneme tokens located by audio tokenizer . In one embodiment audio parser distinguishes phonemes from non speech sounds such as white noise. Audio parser retrieves data from phoneme model to particularly identify each phoneme token. Phoneme model may include a various number of phonemes. For example in one embodiment phoneme model may include two classes of phonemes vowels and consonants. In another embodiment phoneme model may include a plurality of phonemes each corresponding to phonemes of the language that was used to develop acoustic model . For example phoneme model may include a model of over phonemes of the modeled language.

Audio parser in one embodiment identifies all phonemes of audio even if a phonemic sequence including a particular phoneme does not amount to a meaningful morpheme. Under certain circumstances a patient may produce a sequence of phonemes that do not form a morpheme i.e. a sound unit with an identifiable meaning. A patient may produce such a sequence of phonemes as a result of various conditions such as in response to chemical treatments neurological conditions impediments or for other reasons. In the example embodiment of audio parser identifies the phonemes in order to fully analyze the patient s speech regardless of whether the phoneme is part of a meaningful morpheme.

In the example of audio parser generates output . Output includes a representation of audio in the form of a tagged list of symbols. In one embodiment recognition engine may store output to a database. In another embodiment recognition engine may store output as a flat file. In another embodiment recognition engine may send output to analysis module . In another embodiment recognition engine may further include an analysis module to perform analysis of output . In general output is a representation of audio . For example output may be written in a markup language. Output includes symbols to identify particular phonemes of audio symbols to identify silence of audio and symbols e.g. tags to demarcate where the phonemes and silences occur. Output may further include information regarding the length of each sound or silence. In the example of recognition engine sends output to analysis module .

Referring again to analysis module upon receiving output analyze output to identify for example silent pauses filled pauses false starts or other measurements. For example analysis module may calculate a number of silent pauses a number of filled pauses a mean or median duration of pauses silent filled or both silent pause frequency hesitation frequency hesitation duration false start frequency filled pause frequency utterance duration a standard deviation in the length of pauses a ratio of total pause time to audio time i.e. the total time of audio rhythmic phrase length or other calculations.

Analysis module may calculate hesitation frequency and duration by determining a ratio of pauses to words i.e. non silent non filled pause segments . Analysis module may calculate filled pause frequency and duration according to a count and mean duration of nasalized umm and non nasalized ahh filled pause segments. Analysis module may identify false start frequency by counting the number of instances of initial word fragments. Analysis module may calculate a total disfluency rate as for example a combined count of filled pauses false starts and repetitions. Analysis module may also determine initial pauses as the duration of a silent pause before speech initiation.

In one embodiment analysis module may require a silence to exceed a minimum duration before classifying the silence as a silent pause. For example in one embodiment analysis module counts silences lasting more than 150 ms as silent pauses whereas analysis module classifies silences less than 150 ms as merely breaks between words.

To identify a silent pause analysis module may identify silence following a phoneme. To identify a filled pause analysis module may identify a phoneme or sequence of phonemes corresponding to a typical filled pause in the corresponding language that was used to construct acoustic model and phoneme model . This sequence of one or more phonemes may generally be referred to as a filled pause phoneme sequence. As one example analysis module may determine that an open back unrounded phoneme i.e. uhh or a similar phoneme e.g. ahh are filled pause phonemes when these phonemes exist between two silences. As another example analysis module may determine that an open mid back unrounded phoneme followed by a voiced bilabial nasal phoneme i.e. umm between two silences is a filled pause phoneme sequence. Analysis module may determine that other sounds constitute filled pauses as well depending upon the modeled language of acoustic model and phoneme model and the speech being analyzed.

In one embodiment analysis module measures prosodic characteristics in accordance with methods described by Colin Wightman and Mori Ostendorf ICA SSP1991 incorporated herein by reference. In one embodiment analysis module calculates 

Analysis module may calculate various statistics and measurements from output . For example analysis module may analyze output to identify a total number of pauses a number of silent pauses a number of filled pauses an average mean or median or both length of pauses silent filled or both a standard deviation of the length of pauses a number of false starts a duration of audio or other measurements. Analysis module may include any or all of these measurements in speech analysis . Analysis module may output speech analysis in the form of data sent to another module such as a diagnostic engine or report generator as discussed with respect to for example a flat file data stored to a relational database data transmitted over a network or other formats.

Repetition detector identifies a portion of audio to compare to other portions of audio . Spectrogram calculator calculates a spectrogram from the identified portion. Spectrogram calculator divides the identified portion into a number of frames e.g. 100 frames. Spectrogram calculator divides each frame into for example 128 bands each corresponding to frequencies of sound. The bands may be divided into a logarithmic scale such as decibels dB . Spectrogram calculator then determines an intensity value for each band of each frequency of the identified portion of audio . Spectrogram calculator stores the intensity values for the bands of the frames in a data structure such as a three dimensional matrix a tree a linked list or other suitable data structure.

Comparison module compares the spectrogram of the identified portion to another portion of audio . In general comparison module treats the identified portion as a sliding window comparing the identified portion to other portions of equal size of audio . To compare the identified portion with another portion spectrogram calculator calculates a spectrogram for the portion against which the identified portion is compared. Comparison module compares the spectrogram of the identified portion with the spectrogram of the portion for comparison.

In one embodiment comparison module calculates a vector for each of the frames of each spectrogram. To compare a vector of the identified portion to a vector of the portion for comparison comparison module calculates an angle between the two vectors such as by calculating a cosine between the two vectors. The calculated angle describes the similarity between the two vectors and accordingly the two compared frames. Comparison module performs this analysis for each frame of the identified portion and a corresponding frame of the portion for comparison. Comparison module may then calculate a value to determine how similar the identified portion overall is to the portion for comparison. For example comparison module may calculate an average e.g. mean of the cosine values aggregate the widths of the angles or use other calculations to determine a similarity between the identified portion and the portion used for comparison.

Comparison module determines whether the similarity between the identified portion and the compared portion represents a repetition of a sound or word. In one embodiment comparison module determines or is configured with a threshold for determining whether a similarity in the two portions corresponds to a repetition. When a value for the similarity exceeds the threshold comparison module determines that a repetition has occurred. When a repetition is detected comparison module sends a signal to analyzer to indicate the occurrence of a repetition.

Analyzer receives output from comparison module in the example of . In one embodiment analyzer maintains a counter to count the total number of repetitions that have occurred. Analyzer may also determine a repetition frequency i.e. how often repetitions occur in audio or other statistics regarding repetition.

In one embodiment analyzer uses the spectrogram calculated by spectrogram calculator to determine a fundamental frequency of the patient s speech. Analyzer then hypothesizes as to the patient s intonation for the identified portion of audio . For example analyzer may identify fluctuations in the fundamental frequency. Analyzer may include fundamental frequency and intonation analysis in output . In another embodiment another module may calculate the fundamental frequency of the patient s speech. For example spectrogram calculator may exist as a separate module that sends spectrograms to repetition detector and a fundamental frequency determination module.

After repetition detector has analyzed audio analyzer may send output to analysis module . In other embodiments analyzer may send output to a database write output as a flat file transmit output over a network or send output to other modules. Upon receiving output analysis module may include data of output in speech analysis . For example analysis module may include a number of repetitions of audio in speech analysis . Analysis module or analyzer may also calculate a repetition frequency as the number of instances where one two or three words are repeated for example. The repetition frequency may include a repetition of sounds words filled pauses or false starts. Analysis module may include the calculated repetition frequency in speech analysis . Analysis module may also use repetitions detected by repetition detector to determine homogeneity of the speech sample of audio .

Report generator also included in software modules receives output of speech analyzer and generates a report thereof to be returned to one of clinic computers . In one embodiment report generator may generate a report containing historical data for a corresponding patient retrieved from analysis data . Report generator may generate one or more reports that include one or more measures of disfluencies of a patient s speech. For example report generator may generate a report that includes a duration of the patient s speech a total number of pauses a number of silent pauses a number of filled pauses a frequency of pauses an average mean median or both duration of pauses a standard deviation for the duration of pauses a number of false starts a frequency of false starts a number of repetitions a frequency of repetitions or other measurements. Report generator may normalize these measures to the duration of the audio sample from which they were measured. When a particular patient has provided a number of audio samples report generator may generate a report indicating averages for these measures or a change over time in these measures e.g. an increasing trend in the number of pauses a decreasing trend in the number of repetitions etc. .

Diagnostic engine also included in software modules receives output of speech analyzer and generates one or more potential diagnoses based on the received output. The diagnosis may be returned to one of clinic computers . Diagnostic engine may base the diagnosis on current output of speech analyzer as well as historical data for a corresponding patient retrieved from analysis data . In generating a diagnosis diagnostic engine may compare a number of disfluencies of a particular patient to an average number of disfluencies for an average patient the average patient may be a patient without a speech impairment or a patient who has a speech impairment depending upon the diagnostic approach. In one embodiment diagnostic engine may normalize the number of disfluencies identified over the total duration of audio to better quantify and analyze the number of detected disfluencies. In one embodiment diagnostic engine may focus on certain categories of disfluencies more than others for particular diagnoses.

Although illustrated as server side software modules provided by analysis system software modules could readily be implemented as client side software modules executing on computing devices used by for example users of clinic computers A N clinic computers . Software modules could for example be implemented as Active X modules executed by a web browser executing on one or more of clinic computers .

Analysis system also includes web servers which provide access to communication software modules . Communication software modules include administration admin module record manager output manager and application programming interface API . Admin module presents an interface by which authorized users such as administrator configure analysis system . Administrator may for example manage accounts for users at clinics corresponding to clinic computers including setting access privileges creating new accounts removing old accounts or other administrative functions. Admin module may enable administrator to manage and store account information such as particular customer accounts user privileges and other data in configuration database config data .

Record manager allows users of clinic computers to define analyses reports patient records or other clinical data for use by a clinician or patient. Furthermore record manager allows a user to set various characteristics and properties for an analysis. Record manager may also keep track of billing records received payments or other information in for example config data . In one embodiment users of clinic computers may be billed based on a number of analyses sent to analysis system . In one embodiment users of clinic computers may be billed based on a duration of all audio sent to analysis system by the corresponding one of clinic computers . Users of clinic computers may also be billed based on other measurements.

Output manager controls aspects of report or analysis generation that is sent to clinic computers . After a user of one of clinic computers has uploaded an audio sample to analysis system via network output manager returns output from speech analyzer report generator diagnostic engine or other modules of analysis system to the corresponding one of clinic computers .

Application programming interface API provides the ability to establish direct connections with external computing devices allowing such devices to automatically control analysis system . A front end module such as a script or command line interface provided by the remote computing device for example may communicate with API directly bypassing the interfaces presented by other software modules . In this manner the front end module can automatically interact with analysis system and control output. As a result API can be useful when connecting to internal corporate systems to incorporate for example analysis information.

Analysis system also includes database servers that provide access to databases . Databases include configuration config data analysis data and model data . Config data as explained above stores records corresponding to users of clinic computers billing records billing reports access privileges client information patient information and other information regarding the configuration of analysis system . Analysis data stores present and historical analyses from various patients at clinics corresponding to clinic computers . Software modules may retrieve data from analysis data for example to compare a present analysis with historical analyses for a patient. Model data may store modeling data such as modeling data of acoustic model and phoneme model . Model data may store other models as well such as a language model for identifying particular morphemes or words of an audio sample or models corresponding to patients with speech impairments.

Speech analyzer passes audio as input to recognition engine . Speech analyzer may also pass audio as input to repetition detector as discussed with respect to for example . In any case recognition engine may first pass audio to audio tokenizer . Audio tokenizer analyzes audio to identify locations of phonemes non phonemic noise and silence . Audio tokenizer produces a token for each phoneme non phonemic noise and silence. For example audio tokenizer may analyze a first portion of audio to determine whether the first portion of audio corresponds to a phoneme white noise or silence. Audio tokenizer may then retrieve data from audio until a new category is reached. For example when audio tokenizer first perceives silence in audio audio tokenizer continues to retrieve data from audio until either a phoneme or white noise is detected. Audio tokenizer then classifies the silence as a silence token including a duration of the silence.

To detect boundaries of phonemes audio tokenizer may retrieve data from acoustic model . Audio tokenizer may further use data from acoustic model to distinguish phonemes from white noise i.e. non phonemic sounds. Upon detecting a particular phoneme audio tokenizer may continue to retrieve data from audio until either a silence or white noise is detected. Audio tokenizer may then classify the phoneme as a phoneme token including a duration of the phoneme.

After audio tokenizer has tokenized audio audio parser may parse the tokens to identify particular phonemes for each phoneme token . In one embodiment audio parser may instead call a function of audio tokenizer e.g. tokenizer.getNextToken In any case upon receiving a phoneme token audio parser identifies the particular phoneme of the phoneme token. To do so audio parser refers to phoneme model . In general a phoneme token is merely an expression that a phoneme is present whereas audio parser may particularly identify the phoneme of the token. For example if the phoneme token corresponds to an m sound of audio audio tokenizer may identify that a phoneme token exists in a particular location of audio whereas audio parser may particularly identify that phoneme token as m i.e. a voiced bilabial nasal phoneme.

Audio parser outputs a series of tagged symbols. For example for each phoneme audio parser may output a tag to indicate the beginning of the phoneme an identifier of the phoneme and a tag to indicate the end of the phoneme. Audio parser may further indicate in the beginning and ending tags the time corresponding to audio of the beginning and the ending of the phoneme. Likewise audio parser may include tags for the beginning and ending of each silence which may further indicate the duration of the silence. In one embodiment audio parser may discard white noise. In another embodiment audio parser may include tags to mark white noise and indicate white noise as such. In one embodiment audio parser may send output to a database. In another embodiment audio parser may send output to a flat file. In another embodiment audio parser may direct its output as input to a module such as analysis module . In another embodiment audio parser may return its output to recognition engine which may then direct the output to analysis module .

Analysis module may either retrieve or receive output from recognition engine to analyze the sequence of tagged symbols produced by recognition engine . Analysis module may analyze the sequence of tagged symbols to analyze various characteristics of the patient s speech. In one embodiment analysis module may analyze the patient s speech in accordance with the example method of as discussed below. In other embodiments analysis module may perform other analyses of the patient s speech. In general analysis module may automatically measure fluency of the patient s speech. That is analysis module may identify various characteristics of the patient s speech to determine a number of disfluencies such as silent pauses filled pauses hesitations false starts and other disfluencies.

Analysis module generates output in the form of speech analysis in the example of . Analysis module may output speech analysis to a database a flat file transmit speech analysis over a network pass speech analysis as input to a module or utilize speech analysis in some other manner. Analysis module may also receive input from repetition detector before generating speech analysis in one embodiment.

In the example of analysis module first determines a duration of audio . Analysis module may normalize analyses in accordance with the duration of audio . Analysis module may also calculate certain measurements in view of the duration of audio such as frequencies of pauses or false starts.

Analysis module also calculates a number of silent pauses of audio . Analysis module may examine the tagged sequence produced by audio parser to identify each tag corresponding to a silent pause. For each tag indicating a beginning of a silent pause analysis module may increment a silent pause counter. Analysis module also determines the duration of each detected silent pause . Analysis module may add the duration of each detected silent pause to a silent pause duration aggregator.

Analysis module also identifies filled pauses from the parsed output of audio parser . In general filled pauses are sounds people make during speech that indicate that further speech is forthcoming. Common examples of filled pauses among English language speakers are umm and ahh. To identify a filled pause analysis module may utilize a grammar when examining the tagged sequence produced by audio parser . The grammar may specify a filled pause as a phonetic sequence such as 

Analysis module then determines an average duration of pauses . Analysis module may determine a set of averages such as an average duration of silent pauses an average duration of filled pauses and an average duration of all pauses silent and filled . To find an average analysis module aggregates the durations of pauses and divides the aggregated duration by the number of pauses. Analysis module also calculates the standard deviation of the duration of pauses .

Repetition detector may then select a portion of audio to use as a control for a sliding window that slides down audio . This control portion will be compared with other portions of audio to determine how similar the control portion is to the compared portion. The control portion may include a sequence of phonemes before and after silence of audio . The control portion may represent a word a phonemic portion of a filled pause or a fixed length portion of audio .

Spectrogram calculator calculates a spectrogram for the control portion of audio . An example spectrogram is discussed with respect to . To create a spectrogram from the control portion spectrogram calculator first divides the control portion into a number of frames e.g. 100 frames. Spectrogram calculator then divides each frame into a set of 128 frequency bands. The bands may be linear logarithmic or some other division among the frequencies. Other embodiments may also use different numbers of bands and frames. In any case for each band of each frame spectrogram calculator determines a value from the control portion of audio corresponding to the frequency at that frame. Once spectrogram calculator has determined a value for each band of each frame spectrogram calculator may store the values in a data structure such as a three dimensional matrix. In other embodiments spectrogram calculator may store the values in a different data structure such as a linked list a tree or other suitable data structure.

Comparison module may then compare the spectrogram of the control portion to a comparison portion of audio . Spectrogram calculator calculates a spectrogram for the comparison portion of audio . Comparison module then compares the two spectrograms to determine a similarity value. In one embodiment for example comparison module treats each frame of the matrix as a vector i.e. an array of values. Comparison module then calculates a cosine angle between a vector of the control portion and a corresponding vector of the comparison portion for each of the vectors. Comparison module then computes an average of the cosine angles to determine a value for similarity between the control portion and the comparison portion.

Comparison module then compares the similarity value to the threshold . When comparison module determines that the similarity value is less than the threshold NO branch of comparison module determines that there is not a repetition so comparison module compares the control portion to another comparison portion of audio e.g. one or more frames downstream of audio . However when comparison module determines that the similarity value is greater than the threshold YES branch of analyzer increments a repetition counter . Comparison module then compares the control portion to another portion of audio past the comparison portion that was determined to be a repetition.

Comparison module compares portions of a spectrogram such as spectrogram to other portions thereof to identify repetitions. For example comparison module may identify region ranging from time 1.3 s to 1.65 s and compare region across time axis to other portions of spectrogram . Comparison module may compare region to region ranging from time 2.0 s to 2.35 s for example and determine that no repetition is detected between region and region in accordance with the techniques described herein. After comparing region to region comparison module compares region to a next region. In one embodiment the next region begins at the edge of the previously compared region e.g. 2.35 s the end of region . In another embodiment when no repetition is detected from the previously compared region the next region begins within the previously compared region e.g. 2.05 s within region . In this manner and in accordance with the techniques described herein a sample region is treated as a sliding window that slides along time axis and is compared against other regions of a spectrogram such as spectrogram .

The techniques described in this disclosure may be implemented at least in part in hardware software firmware or any combination thereof. For example various aspects of the described techniques may be implemented within one or more processors including one or more microprocessors digital signal processors DSPs application specific integrated circuits ASICs field programmable gate arrays FPGAs or any other equivalent integrated or discrete logic circuitry as well as any combinations of such components embodied in programmers such as physician or patient programmers stimulators image processing devices or other devices. The term processor or processing circuitry may generally refer to any of the foregoing logic circuitry alone or in combination with other logic circuitry or any other equivalent circuitry.

Such hardware software and firmware may be implemented within the same device or within separate devices to support the various operations and functions described in this disclosure. In addition any of the described units modules or components may be implemented together or separately as discrete but interoperable logic devices. Depiction of different features as modules or units is intended to highlight different functional aspects and does not necessarily imply that such modules or units must be realized by separate hardware or software components. Rather functionality associated with one or more modules or units may be performed by separate hardware or software components or integrated within common or separate hardware or software components.

The techniques described herein may also be embodied in a computer readable medium containing instructions. Instructions embedded in a computer readable medium may cause a programmable processor or other processor to perform the method e.g. when the instructions are executed. A computer readable medium may be a computer readable storage medium. Computer readable storage media may include for example random access memory RAM read only memory ROM programmable read only memory PROM erasable programmable read only memory EPROM electronically erasable programmable read only memory EEPROM flash memory a hard disk a CD ROM a floppy disk a cassette magnetic media optical media or other computer readable media.

