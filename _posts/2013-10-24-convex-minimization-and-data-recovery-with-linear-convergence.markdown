---

title: Convex minimization and data recovery with linear convergence
abstract: A convex minimization is formulated to robustly recover a subspace from a contaminated data set, partially sampled around it, and propose a fast iterative algorithm to achieve the corresponding minimum. This disclosure establishes exact recovery by this minimizer, quantifies the effect of noise and regularization, and explains how to take advantage of a known intrinsic dimension and establish linear convergence of the iterative algorithm. The minimizer is an M-estimator. The disclosure demonstrates its significance by adapting it to formulate a convex minimization equivalent to the non-convex total least squares (which is solved by PCA). The technique is compared with many other algorithms for robust PCA on synthetic and real data sets and state-of-the-art speed and accuracy is demonstrated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09251438&OS=09251438&RS=09251438
owner: Regents of the University of Minnesota
number: 09251438
owner_city: Minneapolis
owner_country: US
publication_date: 20131024
---
This application claims the benefit of U.S. Provisional Application 61 717 994 filed Oct. 24 2012 the entire content of which is incorporated herein by reference.

This invention was made with government support under DMS 0915064 and DMS 0956072 awarded by the National Science Foundation. The government has certain rights in the invention.

The invention relates to sensing systems and more specifically data analysis for sensing system and machine learning.

The most useful paradigm in data analysis and machine learning is arguably the modeling of data by a low dimensional subspace. The well known total least squares solves this modeling problem by finding the subspace minimizing the sum of squared errors of data points. This is practically done via principal components analysis PCA of the data matrix. Nevertheless this procedure is highly sensitive to outliers.

In general the invention techniques are described in which an iterative algorithm is applied to formulate a convex minimization to robustly recover a subspace from a contaminated data set partially sampled around it. This disclosure establishes exact recovery by this minimizer quantifies the effect of noise and regularization and explains how to take advantage of a known intrinsic dimension and establish linear convergence of the iterative algorithm. The minimizer is an M estimator. The disclosure demonstrates its significance by adapting it to formulate a convex minimization equivalent to the non convex total least squares which is solved by PCA . The technique is compared with many other algorithms for robust PCA on synthetic and real data sets and state of the art speed and accuracy is demonstrated.

In one embodiment a method comprises receiving with a device multidimensional data comprising a set of data points wherein the set of data points conforms to a plurality of dimensions D and includes outlier data points and iteratively processing the set of data points with the device to compute a reduced data set representative of the set of data points wherein the reduced data set conforms to a subspace having a reduced number of dimensions d less than the plurality of dimensions D of the set of data points. According to the method iteratively processing the set of data points to compute the reduced data set comprises determining for each iteration a scaled version of the set of data points by re computing a corresponding coefficient for each of the data points as a function of a proximity of the data point to a current estimate of the subspace and computing for each iteration an updated estimate of the subspace based on a minimization of a sum of least squares of the scaled version of the set of data points. The iteratively processing of the multidimensional data to compute the reduced data set has a processing complexity with linear convergence with respect to the number of iterations.

In another embodiment a device comprises a memory to store multidimensional data having a set of outliers. A processor is configured to execute program code to process the multidimensional data with the device to compute a reduced data set that is representative of the multidimensional data without the outliers. The program code iteratively processes the set of data points with the device to compute a reduced data set representative of the set of data points wherein the reduced data set conforms to a subspace having a reduced number of dimensions d less than the plurality of dimensions D of the set of data points. During this transformation of the data the program code determines for each iteration a scaled version of the set of data points by re computing a corresponding coefficient for each of the data points as a function of a proximity of the data point to a current estimate of the subspace and computes for each iteration an updated estimate of the subspace based on a minimization of a sum of least squares of the scaled version of the set of data points.

The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features objects and advantages of the invention will be apparent from the description and drawings and from the claims.

Estimator of processing unit process image data to compute low dimensional data model representative of image data . Low dimensional data model includes a reduced set of data points that conforms to a subspace having a reduced number of dimensions d less than the plurality of dimensions D of the set of data points. In this way estimator computes data model of image data by determining a low dimensional subspace representative of the image data. Moreover as described herein estimator computes data model as an estimate of the underlying subspace in a manner that is resistance to any outliers within image data .

As further described herein estimator provides an iterative approach in which a convex minimization of image data is formulated to robustly recover the subspace of image data where the image data is contaminated with outlier data points in the form of noise. In each iteration estimator applies a scaling procedure to scale each data point computes an updated estimate for the subspace based on the scaled version of the data points. Estimator determines for each iteration the scaled version of the set of data points by re computing coefficients for the data points as a function of a proximity of each of the data points to the current estimate of the subspace. During each iteration estimator computes the updated estimate of the subspace based on a summation of weighted least absolute squares of the scaled version of the set of data points.

In general image data may be represented as a data set X x for which the geometric median is the minimizer of the following function of 

There are several obstacles in developing robust and effective estimators for subspaces. For purposes of example estimators of linear subspaces are discussed and it is assumed that the data is centered at the origin. A main obstacle is due to the fact that the set of d dimensional linear subspaces in i.e. the Grassmannian G D d is not convex. Therefore a direct optimization on G D d or a union of G D d over different d s will not be convex even not geodesically convex and may result in several or many local minima. Another problem is that extensions of simple robust estimators of vectors to subspaces e.g. using 11 type averages can fail by a single far away outlier. For example one may extend the d dimensional geometric median minimizing 1 to the minimizer over L G D d of the function

A different convex relaxation is described herein that does not introduce arbitrary parameters and its implementation is significantly faster.

Described herein is a convex relaxation of the minimization of 2 . The original minimization is over all subspaces L or equivalently all orthogonal projectors P P . P with a D D matrix satisfying P P and P P where denotes the transpose can be identified. Since the latter set is not convex it is relaxed to include all symmetric matrices but avoid singularities by enforcing unit trace. That is the set follow set may be minimized over 

If the intrinsic dimension d is known or can be estimate from the data the subspace can be estimated by the span of the bottom d eigenvectors of circumflex over Q . This procedure is robust to sufficiently small levels of noise. This is referred to herein as the Geometric Median Subspace GMS algorithm and summarize in Algorithm 1.

We remark that circumflex over Q is semi definite positive we verify this later in Lemma 14 . We can thus restrict to contain only semi definite positive matrices and thus make it even closer to a set of orthogonal projectors. Theoretically it makes sense to require that the trace of the matrices in is D d since they are relaxed versions of projectors onto the orthogonal complement of a d dimensional subspace . However scaling of the trace in 3 results in scaling the minimizer of 4 by a constant which does not effect the subspace recovery procedure.

Equation 4 is an M estimator with residuals r Qx 1 i N and x x . Unlike 2 which can also be seen as a formal M estimator the estimator circumflex over Q is unique under a weak condition stated later.

The techniques described herein results in the following fundamental contributions to robust recovery of subspaces 

In order to study the robustness to outliers of our estimator for the underlying subspace formulate the exact subspace recovery problem is formulated. This problem assumes a fixed d dimensional linear subspace L inliers sampled from L and outliers sampled from its complement it asks to recover L as well as identify correctly inliers and outliers.

In the case of point estimators like the geometric median minimizing 1 robustness is commonly measured by the breakdown point of the estimator. Roughly speaking the breakdown point measures the proportion of arbitrarily large observations that is the proportion of outliers an estimator can handle before giving an arbitrarily large result. For example the breakdown point of the geometric median is 50 .

In the case of estimating subspaces this definition cannot be directly extended since the set of subspaces i.e. the Grassmannian or unions of it is compact so an arbitrarily large result that is a subspace with arbitrarily large distance from all other subspaces cannot be described. Furthermore given an arbitrarily large data point a subspace containing it can always be formed that is this point is not arbitrarily large with respect to this subspace. Instead the outliers are identified as the ones in the compliment of L and the techniques focus on the largest fraction of outliers or smallest fraction of inliers per outliers allowing exact recovery of L . Whenever an estimator can exactly recover a subspace under a given sampling scenario it is viewed herein as robust and its effectiveness can be measured by the largest fraction of outliers it can tolerate. However when an estimator cannot exactly recover a subspace one needs to bound from below the distance between the recovered subspace and the underlying subspace of the model. Alternatively one would need to point out at interesting scenarios where exact recovery cannot even occur in the limit when the number of points approaches infinity.

In one example in order to guarantee exact recovery of our estimator three kinds of restrictions are applied to the underlying data which are explained herein. First of all the inliers need to permeate through the whole underlying subspace L in particular they cannot concentrate on a lower dimensional subspace of L . Second of all outliers need to permeate throughout the whole complement of L . Some practical methods are also suggested to avoid this failure mode when d is unknown. Third of all the magnitude of outliers needs to be restricted. All data points may be initially scaled to the unit sphere in order to avoid extremely large outliers. However outliers concentrating along lines which may have an equivalent effect of a single arbitrarily large outlier still need to be avoided.

In the case where d is known a tight convex relaxation of the minimization of 31 over all projectors Pof rank d may be used. An optimizer refered to as the REAPER of the needle in haystack problem minimize the same function F Q see 4 over the set

The underlying subspace may be estimated by the bottom d eigenvectors of the REAPER. The new constraints in H result elegant conditions for exact recovery and tighter probabilistic theory due to the tighter relaxation . Since d is known any failure mode of GMS is avoided. The REAPER algorithm for computing the REAPER is based on the IRLS procedure described herein with additional constraints which complicate its analysis.

While the REAPER framework applies a tighter relaxation the GMS framework still has several advantages over the REAPER framework. First of all in various practical situations the dimension of the data is unknown and thus REAPER is inapplicable. On the other hand GMS can be used for dimension estimation demonstrated below. Second of all the GMS algorithm described herein is faster than REAPER the REAPER requires additional eigenvalue decomposition of a D D matrix at each iteration of the IRLS algorithm . Third of all when the failure mode of GMS is avoided the empirical performances of REAPER and GMS are usually comparable while GMS is faster . At last GMS and REAPER have different objectives with different consequences. REAPER aims to find a projector onto the underlying subspace. On the other hand GMS aims to find a generalized inverse covariance and is formally similar to other common M estimators. Therefore the eigenvalues and eigenvectors of the GMS estimator i.e. the generalized inverse covariance can be interpreted as robust eigenvalues and eigenvectors of the empirical covariance.

The formulation of the exact subspace recovery problem which is a robust measure for the performance of the estimator is repeated below. Image data conforms to in this example a linear subspace L G D d and comprises a data set x which contains inliers sampled from L and outliers sampled from L . Given the data set and no other information the objective of the exact subspace recovery problem is to exactly recover the underlying subspace L .

In the noisy case where inliers do not lie on L but perturbed by noise the techniques investigate near subspace recovery i.e. recovery up to an error depending on the underlying noise level. In this case additional information on the model may be needed. In some example implementations dimensionality d of the subspace may be know estimated from the data.

Another issue of exact subspace recovery is whether the subspace obtained by a proposed algorithm is unique. Many of the convex algorithms depend on convex l type methods that may not be strictly convex. But it may still happen that in the setting of pure inliers and outliers and under some conditions avoiding the three types of enemies the recovered subspace is unique even though it may be obtained by several non unique minimizers . Nevertheless uniqueness of our minimizer and not the recovered subspace is important for analyzing the numerical algorithm approximating it and for perturbation analysis e.g. when considering near recovery with noisy data .

Additional assumptions are introduced on the data to address the three types of enemies. The sets of exact inliers and outliers are denoted by and respectively i.e. L and L . The following two conditions simultaneously address both type 1 and type 3 enemies 

A lower bound on the common LHS of both 6 and 7 is designed to avoid type 1 enemies. This common LHS is a weak version of the permeance statistics which can be defined as follows 

Similarly to the permeance statistics it is zero if and only if all inliers are contained in a proper subspace of L . Indeed if all inliers lie in a subspace L L then this common LHS is zero with the minimizer Q P P . Similarly if it is zero then Qx 0 for any x and for some Q with kernel containing L . This is only possible when is contained in a proper subspace of L . Similarly to the permeance statistics if the inliers nicely permeate through L then this common LHS clearly obtain large values.

The upper bounds on the RHS s of 6 and 7 address two complementing type 3 enemies. If contains few data points of large magnitude which are orthogonal to L then the RHS of 6 may be too large and 6 may not hold. If on the other hand contains few data points with large magnitude and a small angle with L then the RHS of 7 will be large so that 7 may not hold. Conditions 6 and 7 thus complete each other.

The RHS of condition 7 is similar to the linear structure statistics for L which was defined in 3.3 of LMTZ2012. The linear structure statistics uses an laverage of dot products instead of the laverage used here and was applied in this context to R instead of L in Lerman et al. Similarly to the linear structure statistics the RHS of 7 is large when outliers either have large magnitude or they lie close to a line so that their combined contribution is similar to an outlier with a very large magnitude as exemplified in . The RHS of condition 7 is a very weak analog of the linear structure statics of L since it uses a minimum instead of a maximum. There are some significant outliers within L that will not be avoided by requiring 7 . For example if the codimension of L is larger than 1 and there is a single outlier with an arbitrary large magnitude orthogonal to L then the RHS of 7 is zero.

The next condition avoids type 2 enemies and also significant outliers within L i.e. type 3 enemies that were not avoided by condition 7 . This condition requires that any minimizer of the following oracle problem

The requirement that QP 0 is equivalent to the condition ker Q L and therefore the rank of the minimizer is at most D d. Enforcing the rank of the minimizer to be exactly D d restricts the distribution of the projection of onto L . In particular it avoids its concentration on lower dimensional subspaces and is thus suitable to avoid type 2 enemies. Indeed if all outliers are sampled fromtilde over L L then any Q H with ker Q tilde over L L satisfies F Q 0 and therefore it is a minimizer of the oracle problem 4 but it contradicts 9 .

This condition also avoids some type 3 enemies which were not handled by conditions 6 and 7 . For example any D d 1 outliers with large magnitude orthogonal to L will not be excluded by requiring 6 or 7 but will be avoided by 9 .

This condition is restrictive though especially in very high ambient dimensions. Indeed it does not hold when the number of outliers is smaller than D d since then the outliers are sampled from sometilde over L with dimtilde over L L 

This example demonstrates the violation of the conditions above for the examples depicted in . The actual calculations rely on the concepts described herein. For example in which represents a type 1 enemy both conditions 6 and 7 are violated. Indeed the common LHS of 6 and 7 is 5.69 whereas the RHS of 6 is 8.57 and the RHS of 7 is larger than 10.02 this lower bound is obtained by substituting v 0 1 0 in the RHS of 7 note that v is a unit vector in L .

In which represents a type 2 enemy condition 9 is violated. Indeed a solution circumflex over Q with rank circumflex over Q 1 D d 2 is obtained numerically. It can be proved in this case that circumflex over Q is the projector onto the orthogonal complement of the plane represented by the dashed rectangle.

In which represents a type 3 enemy both conditions 6 and 7 are violated. Indeed the common LHS of 6 and 7 is 1.56 and the RHS s of 6 and 7 are 5.66 and 4.24 respectively. However all points are normalized to lie on the unit circle this enemy can be overcome. Indeed for the normalized data the common LHS of 6 and 7 is 6 and the RHS s of 6 and 7 are 1.13 and 0.85 respectively.

In the example of which also represents a type 3 enemy both conditions 6 and 7 are violated. Indeed the LHS of 6 and 7 are 5.99 and the RHS s of 6 and 7 are 6.91 and 7.02 respectively.

This section discusses that that the minimizer of 4 solves the exact recovery problem under the above combinatorial conditions.

Theorem 1 implies that if 6 7 and 9 hold then ker circumflex over Q is unique. The uniqueness of is is guaranteed independently of the exact subspace recovery problem.

It is sufficient to guarantee exact recovery by requiring 9 and that for an arbitrarily chosen solution of 8 circumflex over Q the following two conditions are satisfied as long as they are well defined 

Conditions 11 and 12 can be verified when and L are known unlike 6 and 7 where Qcan be found by Algorithm 2. Furthermore 11 and 12 are weaker than 6 and 7 though they are more technically involved and harder to motivate.

In order to demonstrate the near tightness of 11 and 12 we formulate the following necessary conditions for the recovery of L as ker Q . For an arbitrarily chosen solution of 8 Q 

The conditions for exact recovery or the main two of them and the condition for uniqueness of the minimizer Q hold with high probability under basic probabilistic models. Such a probabilistic theory is cleaner when the outliers are sampled from a spherically symmetric distribution as demonstrate herein with two different models . One issue is that when the outliers are spherically symmetric then various non robust algorithms such as PCA can asymptotically approach exact recovery and nearly recover the underlying subspace with sufficiently large sample. It is shown herein how the theory presented can be slightly modified to establish exact recovery of the GMS algorithm in an asymmetric case where PCA cannot even nearly recover the underlying subspace.

First we assume a more general probabilistic model. We say that on is an Outliers Inliers Mixture OIM measure w.r.t. the fixed subspace L G D d if where 0 1 is a sub Gaussian probability measure and is a sub Gaussian probability measure on R representing outliers that can be decomposed to a product of two independent measures such that the supports of and are L and L respectively and is spherically symmetric with respect to rotations within L .

To provide cleaner probabilistic estimates we also invoke a needle haystack model that assumes that both and are the Gaussian distributions N 0 I D and N 0 PP d the factors 1 D and 1 d normalize the magnitude of outliers and inliers respectively so that their norms are comparable . While this assumes a fixed number of outliers and inliers independently sampled from and respectively here we independently sample from the mixture measure we refer tog as a needle haystack mixture measure.

In order to prove exact recovery under any of these models one needs to restrict the fraction of inliers per outliers or equivalently the ratio . We refer to this ratio as SNR signal to noise ratio since we may view the inliers as the pure signal and the outliers as some sort of noise . For the needle haystack model we require the following SNR 

We later explain how to get rid of the term . For the OIM model we assume the following more general condition 

For samples from an OIM measure satisfying 16 we can establish our modified conditions of unique exact recovery i.e. 11 12 and 9 with overwhelming probability in the following way we also guarantee the uniqueness of the minimizer Q .

Theorem 4 If X is an i.i.d. sample from an OIM measure satisfying 16 then conditions 11 12 and 9 hold with probability 1 C exp N C where C is a constant depending on and its parameters. Moreover 10 hold with probability 1 if there are at least 2D 1 outliers i.e. the number of points in X L is at least 2D 1 

Under the needle haystack model the SNR established by Theorem 4 is comparable to the best SNR among other convex exact recovery algorithms. However the probabilistic estimate under which this SNR holds is rather loose and thus its underlying constant C is not specified. Indeed the proof of Theorem 4 uses nets and union bounds arguments which are often not useful for deriving tight probabilistic estimates. One may thus view Theorem 4 as a near asymptotic statement.

The statement of Theorem 4 does not contradict our previous observation that the number of outliers should be larger than at least D d. Indeed the constant C is sufficiently large so that the corresponding probability is negative when the number of outliers is smaller than D d.

In the next theorem we assume only a needle haystack model and thus we can provide a stronger probabilistic estimate based on the concentration of measure phenomenon. However the SNR is worse than the one in Theorem 4 by a factor of order square root over D d . This is because we are unable to estimate Qof 8 by concentration of measure. Similarly in this theorem we do not estimate the probability of 9 which also involves Q . Nevertheless we observed in experiments that 9 holds with high probability for N 2 D d and the probability seems to go to 1 as N 2 D d and D d . Moreover one of the algorithms proposed below EGMS does not require condition 9 .

And 64 max 2 2 2 18 Then 6 and 7 hold with probability 1 e 2e e. A Special Case with Asymmetric Outliers

In the case of spherically symmetric outliers PCA cannot exactly recover the underlying subspace but it can asymptotically recover it. In particular with sufficiently large sample with spherically symmetric outliers PCA nearly recovers the underlying subspace. We thus slightly modify the two models described above so that the distribution of outliers is asymmetric and show that our combinatorial conditions for exact recovery still hold with overwhelming probability . On the other hand the subspace recovered by PCA when sampling data from these models is sufficiently far from the underlying subspace for any given sample size.

We first generalize Theorem 5 under a generalized needle haystack model Let N 0 D where is an arbitrary positive definite matrix not necessarily a scalar matrix as before and as before N 0 PP d . We claim that Theorem 5 still holds in this case if we replace in the RHS of 17 with square root over where denotes the largest eigenvalue of .

In order to generalize Theorem 4 for asymmetric outliers we assume that the outlier component of the OIM measure is a sub Gaussian distribution with an arbitrary positive definite covariance matrix . Following Coudron and Lerman 2012 we define the expected version of F Fand its oracle minimizer Qwhich is analogous to 8 the subscript I indicates integral 19 

We assume that Qis the unique minimizer in 20 we remark that the two subspaces criterion in 25 for the projection of onto L implies this assumption . Under these assumptions Theorem 4 still holds if we multiply the RHS of 16 by the ratio between the largest eigenvalue of P QPand the D d th eigenvalue of P QP.

We show that in the case of sufficiently small additive noise i.e. the inliers do not lie exactly on the subspace L but close to it the GMS algorithm nearly recovers the underlying subspace.

We use the following notation A and A denote the Frobenius and spectral norms of A respectively. Furthermore denotes the set of all positive semidefinite matrices in that is H Q H Q 0. We also define the following two constants

The sum in the RHS s of 21 and 22 is the following second directional derivative d dtF Q t when Qx 0 its ith term can be set to 0. It is interesting to note that both 21 and 22 express a Restricted Strong Convexity RSC parameter . The difference between and of 21 and 22 is due to the choice of either the Frobenius or the spectral norms respectively for measuring the size of .

Theorem 6 Assume that is a set of positive numbers X Xand Let F Q and F Q denote the corresponding versions of F Q w.r.t. the sets X and tilde over X and let circumflex over Q and tilde over Q denote their respective minimizers. Then we have

Moreover if tilde over L and circumflex over L are the subspaces spanned by the bottom d eigenvectors of circumflex over Q and tilde over Q respectively and vis the D d th eigengap of circumflex over Q then

We note that if and tilde over satisfy the conditions of Theorem 6 then given the perturbed data set tilde over and the dimension d Theorem 6 guarantees that GMS nearly recovers L . More interestingly the theorem also implies that we may properly estimate the dimension of the underlying subspace in this case. Such dimension estimation is demonstrated later in .

Theorem 6 is a perturbation result. We note that asymptotically the bounds on the recovery errors in 23 and 24 depend only on the empirical mean of and do not grow with N. To clarify this point we formulate the following proposition.

Proposition 7 If X is i.i.d. sampled from a bounded distribution and 0 and c 0 depending on such that

We first note that the bounds in Theorem 6 are generally not optimal. Indeed if O for all 1 i N then the error bounds in Theorem 6 are O square root over whereas we empirically noticed that these error bounds are O .

The dependence of the error on D which follows from the dependence of and on D is a difficult problem and strongly depends on the underlying distribution of and of the noise. For example in the very special case where the set is sampled from a subspace L Rof dimension D

For our practical algorithm it is advantageous to regularize the function F as follows see Theorems 11 and 12 below 

In order to address the regularization in our case and conclude that the GMS algorithm nearly recovers L for the regularized objective function we adopt a perturbation procedure. We denote by Qand Q the minimizers of F Q and F Q in H respectively. Furthermore let Land L denote the subspaces recovered by the bottom d eigenvectors of Qand Q respectively. Using the constants vand of Theorem 6 the difference between the two minimizers and subspaces can be controlled as follows.

A robust M estimator for the 0 centered covariance matrix minimizes the following function over all D D positive definite matrices for some choices of a function 

If we set x square root over x and A Qthen the objective function L A in 30 is Q N log Q . This energy function is formally similar to our energy function. Indeed using Lagrangian formulation the minimizer Q in 4 is also the minimizer of Q E log Q among all D D symmetric matrices or equivalently nonnegative symmetric matrices for some 0 the parameter only scales the minimizer and does not effect the recovered subspace . Therefore the two objective functions differ by their second terms. In the common M estimator with x square root over x and A Q it is log det Q or equivalently log Q where in our M estimator it is Q .

The common M estimator is designed for robust covariance estimation however we show here that in general it cannot exactly recover the underlying subspace. To make this statement more precise we recall the following uniqueness and existence conditions for the minimizer of 30 1 2 is positive continuous and non increasing. 2 Condition M u x x is strictly increasing. 3 Condition D For any linear subspace L 

For symmetric outliers the common M estimator can still asymptotically achieve exact recovery similarly to PCA . However for many scenarios of asymmetric outliers in particular the subspace recovered by the common M estimator is sufficiently far from the underlying subspace for any given sample size.

The total least squares subspace approximation is practically the minimization over L G D d of the function

We show here that Qcoincides with a scaled version of the empirical inverse covariance matrix. This clearly implies that the relaxed total least squared subspace coincides with the original one as the bottom eigenvectors of the inverse empirical covariance are the top eigenvectors of the empirical covariance . We require though that the data is of full rank so that the empirical inverse covariance is well defined. This requirement does not hold if the data points are contained within a lower dimensional subspace in particular if their number is smaller than the dimension. We can easily avoid this restriction by initial projection of the data points onto the span of eigenvectors of the covariance matrix with nonzero eigenvalues. That is by projecting the data onto the lowest dimensional subspace containing it without losing any information.

We view 4 as a robust version of 32 . Since we verified robustness of the subspace recovered by 4 and also showed that 32 yields the inverse covariance matrix we sometimes refer to the solution of 4 as a robust inverse covariance matrix though we have only verified robustness to subspace recovery . This idea helps us interpret our numerical procedure for minimizing 4 which we present below.

The procedure for minimizing formally follows from the simple fact that the directional derivative of F at Q in any direction Q Q where tilde over Q H is 0 i.e. 

Since F Q is symmetric and tilde over Q Q can be any symmetric matrix with trace 0 it is easy to note that 34 implies that F Q is a scalar matrix e.g. multiply it by a basis of symmetric matrices with trace 0 whose members have exactly 2 nonzero matrix elements . That is 

Indeed we can easily verify that 37 solves 6 furthermore 36 is a Lyapunov equation whose solution is unique. Since Q 1 we obtain that

Formula 38 is undefined whenever Qx 0 for some k N and 1 i N. In theory we address it as follows. Let I Q 1 i N Qx 0 L Q xand

Using this notation the iterative formula can be corrected as follows . 40 In practice we can avoid data points satisfying Qx for a sufficiently small parameter instead of Qx 0 . We follow a similar idea by replacing F with the regularized function Ffor a regularized parameter . In this case 40 obtains the following form 

The two iterative formulas i.e. 40 and 41 give rise to IRLS algorithms. For simplicity of notation we exemplify this idea with the formal expression in 38 . It iteratively finds the solution to the following weighted with weight 1 Qx least squares problem 

Formula 41 as well as 40 provides another interpretation for Q as robust inverse covariance. Indeed we note for example that the RHS of 41 is the scaled inverse of a weighted covariance matrix the scaling enforces the trace of the inverse to be 1 and the weights of xxare significantly larger when xis an inlier. In other words the weights apply a shrinkage procedure for outliers. Indeed since Qxapproaches Qxand the underlying subspace which contain the inliers is recovered by ker Q for an inlier xthe coefficient of xxapproaches 1 which is a very large number in practice we use 10 . On the other hand when xis sufficiently far from the underlying subspace the coefficient of xxis significantly smaller.

As another example implementation estimator may re computes the minimization matrix A as a normalized and weighted covariance i.e. not an inverse covariance having weighting coefficients that represent proximity to estimate of the subspace associated with minimization matrix A . In this case the minimization matrix A may be represented as follows 

The following theorem analyzes the convergence of the sequence proposed by 40 to the minimizer of 4 .

Theorem 11 Let X xbe a data set in satisfying 10 circumflex over Q the minimizer of 4 Qan arbitrary symmetric matrix with tr Q 1 and Qthe sequence obtained by iteratively applying 40 while initializing it with Q then Qconverges to a matrix tilde over Q . If tilde over Q 0 for all 1 i N then the sequence Qconverges linearly to tilde over Q and tilde over Q circumflex over Q .

The condition for the linear convergence to Q in Theorem 11 i.e. Qx 0 for all 1 i N usually does not occur for noiseless data. This phenomenon is common in IRLS algorithms whose objective functions are l type and are not twice differentiable at 0. Regularized IRLS algorithms often converge linearly to the minimizer of the regularized function. We demonstrate this principle in our case as follows.

Theorem 12 Let X xbe a data set in satisfying 10 Qan arbitrary symmetric matrix with tr Q 1 and Qthe sequence obtained by iteratively applying 41 while initializing it with Q . Then the sequence Qconverges linearly to the unique minimizer of F Q .

Following the theoretical discussion we prefer using the regularized version of the IRLS algorithm. We fix the regularization parameter to be smaller than the rounding error i.e. 10 so that the regularization is very close to the original problem even without regularization the iterative process is stable but may have few warnings on badly scaled or close to singular matrices . The idea of the algorithm is to iteratively apply 41 with an arbitrary initialization symmetric with trace 1 . We note that in theory F Q is non increasing. However empirically the sequence decreases when it is within the rounding error to the minimizer. Therefore we check F Q every four iterations and stop our algorithm when we detect an increase we noticed empirically that checking every four iterations instead of every iteration improves the accuracy of the algorithm . Algorithm 2 summarizes our practical procedure for minimizing 4 .

Each update of Algorithm 2 requires a complexity of order O N D due to the sum of N D D matrices. Therefore for niterations the total running time of Algorithm 2 is of order O n N D . In most of our numerical experiments nwas less than 40. The storage of this algorithm is O N D which amounts to storing . Thus Algorithm 2 has the same order of storage and complexity as PCA. In practice it might be a bit slower due to a larger constant for the actual complexity.

We view the GMS algorithm as a prototype for various subspace recovery algorithms. We discuss here modifications and extensions of this procedure in order to make it even more practical.

In theory the subspace recovery described here can work without knowing the dimension d. In the noiseless case one may use 5 to estimate the subspace as guaranteed by Theorem 1. In the case of small noise one can estimate d from the eigenvalues of Q and then apply the GMS algorithm. This strategy is theoretically justified by Theorems 1 and 6 as well as the discussion following 87 . The problem is that condition 9 for guaranteeing exact recovery by GMS is restrictive in particular it requires the number of outliers to be larger than at least D d according to our numerical experiments it is safe to use the lower bound 1.5 D d . For practitioners this is a failure mode of GMS especially when the dimension of the data set is large for example D N .

While this seems to be a strong restriction we remark that the problem of exact subspace recovery without knowledge of the intrinsic dimension is rather hard and some assumptions on data sets or some knowledge of data parameters would be expected. Other algorithms for this problem require estimates of unknown regularization parameters which often depend on various properties of the data in particular the unknown intrinsic dimension or strong assumptions on the underlying distribution of the outliers or corrupted elements.

We first note that if only conditions 6 and 7 hold then Theorem 1 still guarantees that the GMS algorithm outputs a subspace containing the underlying subspace. Using some information on the data one may recover the underlying subspace from the outputted subspace containing it even when dealing with the failure mode.

In the rest of this section we describe several practical solutions for dealing with the failure mode in particular with small number of outliers. We later demonstrate them numerically for artificial data and for real data.

Our first practical solution is to reduce the ambient dimension of the data. When the reduction is not too aggressive it can be performed via PCA. We also propose a robust dimensionality reduction which can be used instead. There are two problems with this strategy. First of all the reduced dimension is another parameter that requires tuning. Second of all some information may be lost by the dimensionality reduction and thus exact recovery of the underlying subspace is generally impossible.

A second practical solution is to add artificial outliers. The number of added outliers should not be too large otherwise 6 and 7 will be violated but they should sufficiently permeate through so that 9 holds. In practice the number of outliers can be 2D since empirically 9 holds with high probability when N 2 D d . To overcome the possible impact of outliers with arbitrarily large magnitude we project the data with artificial outliers onto the sphere. Furthermore if the original data matrix does not have full rank in particular if N

A third solution is to regularize our M estimator that is to minimize the following objective function with the regularization parameter 

Knowledge of the intrinsic dimension d can help improve the performance of GMS or suggest completely new variants especially as GMS always finds a subspace containing the underlying subspace . For example knowledge of d can be used to carefully estimate the parameter of 43 e.g. by finding yielding exactly a d dimensional subspace via a bisection procedure.

We formulate in Algorithm 3 the Extended Geometric Median Subspace EGMS algorithm for subspace recovery with known intrinsic dimension.

We justify this basic procedure in the noiseless case without requiring 9 as follows. Theorem 13 Assume that d D N d

The vectors obtained by EGMS at each iteration can be used to form robust principal components in reverse order even when Q is degenerate.

The computational complexity of GMS is of the same order as that of Algorithm 2 i.e. O n N D where nis the number of required iterations for Algorithm 2 . Indeed after obtaining Q computing L by its smallest d eigenvectors takes an order of O d D operations.

EGMS on the other hand repeats Algorithm 2 D d times therefore it adds an order of O D d n N D operations where ndenotes the total number of iterations for Algorithm 2. In implementation we can speed up the EGMS algorithm by excluding the span of some of the top eigenvectors of Q from L instead of excluding only the top eigenvector in the third step of Algorithm 2 . We demonstrate this modified procedure on artificial setting below.

Below we generate data from the following model. We randomly choose L G D d sample Ninliers from the d dimensional Multivariate Normal distribution N 0 I on L and add Noutliers sampled from a uniform distribution on 0 1 . The outliers are strongly asymmetric around the subspace to make the subspace recovery problem more difficult. In some experiments below additional Gaussian noise is considered. When referring to this synthetic data we only need to specify its parameters N N D d and possibly the standard deviation for the additive noise. For any subspace recovery algorithm or heuristics we denote by tilde over L its output i.e. the estimator for L and measure the corresponding recovery error by e P P .

We present two different artificial cases where in one of them condition 9 holds and in the other one it does not hold and test the practical solutions in the second case.

The two cases are the following instances of the synthetic model a N N D d 100 100 100 20 and b N N D d 100 20 100 20 . The GMS algorithm estimates the underlying subspace L given d 20 with recovery errors 2.1 10and 3.4 in cases a and b respectively. In case a there are sufficiently many outliers with respect to D d and the GMS algorithm is successful. We later show that the underlying dimension d 20 can be easily estimated by the eigenvalues of Q. In case b N 0.25 D d therefore condition 9 is violated and the GMS algorithm completely fails.

We demonstrate the success of the practical solutions in case b . We assume that the dimension d is known though we also estimate d correctly for the non regularized solutions. Therefore these solutions can be also applied without knowing the dimension. If we reduce the dimension of the data set in case b from D 100 to D 35 via PCA though one can also use EGMS then GMS with d 20 achieves a recovery error of 0.23 which indicates that GMS almost recovers the subspace correctly. We remark though that if we reduce the dimension to e.g. D 55 then the GMS algorithm will still fail. We also note that the recovery error is not as attractive as the ones below this observation probably indicates that some information was lost during the dimension reduction.

The GMS2 algorithm with d 20 recovers the underlying subspace in case b with error 1.2 10. This is the method we advocated for when possibly not knowing the intrinsic dimension.

The regularized minimization of 43 with 100 works well for case b . In fact it recovers the subspace as ker Q without using its underlying dimension with error 3.3 10. The only issue is how to determine the value of . We described above that if d is known then can be carefully estimated by the bisection method. This is true for this example in fact we initially chose this way.

We remark that the REAPER algorithm did not perform well for this particular data though in general it is a very successful solution. The recovery error of the direct REAPER algorithm was 3.725 and 3.394 for S REAPER and the error for its modified version via bisection relaxing the bound on the largest eigenvalue so that dim ker Q 20 was 3.734 and 3.175 for S REAPER .

At last we demonstrate the performance of EGMS and its faster heuristic with d 20. The recovery error of the original EGMS for case b is only 0.095. We suggested above a faster heuristic for EGMS which can be reformulated as follows In the third step of Algorithm 3 we replace u the top eigenvector of Q with U the subspace spanned by several top eigenvectors. In the noiseless case we could let U be the span of the nonzero eigenvectors of Q. This modification of EGMS for the noiseless case required only two repetitions of Algorithm 2 and its recovery error was 2.2 10. In real data sets with noise we need to determine the number of top eigenvectors spanning U which makes this modification of EGMS less automatic.

We test dimension estimation by eigenvalues of Q for cases a and b . The eigenvalues of Q obtained by Algorithm 2 for the two cases are shown in . That is in the starred points and the dotted point represent log scaled eigenvalues of the output of Algorithm 2 for cases a and b respectively corresponds to case b with dimension reduced to 35.

In the largest logarithmic eigengap i.e. the largest gap in logarithms of eigenvalues occurs at 80 so we can correctly estimate that d D 80 20 the eigenvalues are not zero since Algorithm 2 uses the regularized objective function . However in the largest eigengap occurs at 60 and thus mistakenly predicts d 40.

As we discussed above the dimension estimation fails here since condition 9 is not satisfied. However we have verified that if we try any of the solutions proposed above then we can correctly recover that d 20 by the logarithmic eigengap. For example in we demonstrate the logarithms of eigenvalues of circumflex over Q in case b after dimensionality reduction via PCA onto dimension D 35 and it is clear that the largest gap is at d 20 or D d 80 . We obtained similar graphs when using 2D artificial outliers more precisely the GMS2 algorithm without the final application of the GMS algorithm or the regularization of 43 with 100.

Throughout the paper we emphasized the subspace recovery problem but did not discuss at all the information that can be inferred from the eigenvectors of our robust PCA strategy. Since in standard PCA these vectors have significant importance we exemplify the information obtained from our robust PCA and compare it to that obtained from PCA and some other robust PCA algorithms.

We create a sample from a mixture of two Gaussian distributions with the same mean and same eigenvalues of the covariance matrices but different eigenvectors of the covariance matrices. The mixture percentages are 25 and 75 . We expect the eigenvectors of any good robust PCA algorithm robust to outliers as perceived in this paper to be close to that of the covariance of the mail component with 75 .

More precisely we sample 300 points from N 0 where is a 10 10 diagonal matrix with elements 1 2 2 . . . 2and 100 points from N 0 where U U where U is randomly chosen from the set of all orthogonal matrices in . The goal is to estimate the eigenvectors of i.e. the standard basis vectors in the presence of 25 outlier . Unlike the subspace recovery problem where we can expect to exactly recover a linear structure among many outliers here the covariance structure is more complex and we cannot exactly recover it with 25 outliers.

We estimated the eigenvectors of by the eigenvectors of circumflex over Q of Algorithm 2 in reverse order recall that circumflex over Q is a scaled and robust version of the inverse covariance . We refer to this procedure as EVs eigenvectors of circumflex over Q . We also estimated these eigenvectors by standard PCA LLD with 0.8 square root over D N and PCP with 1 square root over max D N . We repeated the random simulation with different samples for the random orthogonal matrix U 100 times reported in Table 2 the average angles between the estimated and actual top two eigenvectors of according to the different methods. We note that the EVs of circumflex over Q outperforms PCS LLD or OP and PCP in terms of estimation of the top two eigenvectors of . We remark though that PCP does not suit for robust estimation of the empirical covariance and thus the comparison is unfair for PCP.

When the covariance matrix and consequently also is degenerate circumflex over Q might be singular and therefore circumflex over Q cannot be directly used to robustly estimate eigenvectors of the covariance matrix. For this case EGMS Algorithm 3 can be used where the vector u obtained in the ith iteration of Algorithm 3 can be considered as the D i 1 st robust eigenvector that is we reverse the order again . To test the performance of this method we modify in the above model as follows diag 1 0.5 0.25 0 0 . . . 0 . We repeated the random simulations of this modified model 100 times and reported in Table 2 the average angles between the estimated and actual top two eigenvectors of according to the different methods. Here LLD did slightly better than EGMS and they both outperformed PCA and PCP .

We assume a synthetic data set sampled according to the model above with N N D d 250 250 100 10 . We use the GMS algorithm with d 10 and different values of the regularization parameter and record the recovery error in . is a graph of the recovery errors and the regularization parameters . For 10 10 log error log is constant. We thus empirically obtain that the error is of order O in this range. On the other hand 29 only obtained an order of O square root over . It is possible that methods similar to those of Coudron and Lerman 2012 can obtain sharper error bounds. We also expect that for sufficiently small here smaller than 10 the rounding error becomes dominant. On the other hand perturbation results are often not valid for sufficiently large here this is the case for 10 .

Using the synthetic data we compared the GMS algorithm with the following algorithms MDR Mean Absolute Deviation Roundin LLD Low Leverage Decomposition OP Outlier Pursuit PCP Principal Component Pursuit MKF Median K flats with K 1 HR PCA High dimensional Robust PCA a common M estimator and R PCA. We also record the output of standard PCA where we recover the subspace by the span of the top d eigenvectors. We ran the experiments on a computer with Intel Core 2 CPU at 2.66 GHz and 2 GB memory.

We remark that since the basic GMS algorithm already performed very well on these artificial instances.

For all of our experiments with synthetic data we could correctly estimate d by the largest logarithmic eigengap of the output of Algorithm 2. Nevertheless we used the knowledge of d for all algorithms for the sake of fair comparison.

For LLD. OP and PCP we estimated L by the span of the top d eigenvectors of the low rank matrix. Similarly for the common M estimator we used the span of the top d eigenvectors of the estimated covariance A. For the HR PCA algorithm we also used the true percentage of outliers 50 in our experiments . For LLD OP and PCP we set the mixture parameter as 0.8 square root over D N 0.8 square root over D n 1 square root over max D N respectively following the suggestions of McCoy and Tropp 2011 for LLD OP and Candes et al. 2011 for PCP . These choices of parameters are also used in experiments with real data sets.

For the common M estimator we used u x 2 max ln x x 10 and the algorithm discussed by Kent and Tyler 1991 . Considering the conditions discussed above we also tried other functions u x max x 10 had a significantly larger recovery error and u x max x 10 resulted in a similar recovery error as max ln x x 10 but a double running time.

We used the syntectic data with different values of N ND d . In some instances we also add noise from the Gaussian distribution N 0 I with 0.1 or 0.01. We repeated each experiment 20 times due to the random generation of data . We record in Table 4 the mean running time the mean recovery error and their standard deviations.

We remark that PCP is designed for uniformly corrupted coordinates of data instead of corrupted data points i.e. outliers therefore the comparison with PCP is somewhat unfair for this kind of data. On the other hand the applications are tailored for the PCP model though the other algorithms still apply successfully to them .

From Table 4 we can see that GMS is the fastest robust algorithm. Indeed its running time is comparable to that of PCA. We note that this is due to its linear convergence rate usually it converges in less than 40 iterations . The common M estimator is the closest algorithm in terms of running time to GMS since it also has the linear convergence rate. In contrast PCP OP and LLD need a longer running time since their convergence rates are much slower. Overall GMS performs best in terms of exact recovery. The PCP OP and LLD algorithms cannot approach exact recovery even by tuning the parameter . For example in the case where N N D d 125 125 10 5 with 0 we checked a geometric sequence of 101 values from 0.01 to 1 and the smallest recovery errors for LLD OP and PCP are 0.17 0.16 and 0.22 respectively. The common M estimator performed very well for many cases sometimes slightly better than GMS but its performance deteriorates as the density of outliers increases e.g. poor performance for the case where N N D d 125 125 10 5 . Indeed Theorem 9 indicates problems with the exact recovery of the common M estimator.

At last we note that the empirical recovery error of the GMS algorithm for noisy data sets is in the order of square root over where is the size of noise.

We apply our algorithm to face images. It has been shown that face images from the same person lie in a low dimensional linear subspace of dimension at most 9. However cast shadows specular reflections and saturations could possibly distort this low rank modeling. Therefore one can use a good robust PCA algorithm to remove these errors if one has many images from the same face.

We used the images of the first two persons in the extended Yale face database B where each of them has 65 images of size 192 168 under different illumination conditions. Therefore we represent each person by 65 vectors of length 32256. We applied GMS GMS2 and EGMS with d 9 and we also reduced the 65 32256 matrix to 65 65 in fact we only reduced the representation of the column space by rejecting left vectors with zero singular values. We also applied the GMS algorithm after initial dimensionality reduction via PCA to D 20. The running times of EGMS and GMS without dimensionality reduction are 13 and 0.16 seconds respectively on average for each face we used the same computer as above . On the other hand the running times of PCP and LLD are 193 and 2.7 seconds respectively. Moreover OP ran out of memory.

For background subtraction in surveillance videos we consider the following two videos Lobby in an office building with switching on off lights and Shopping center from http perception.i2r.a star.edu.sg bk model bk index.html . In the first video the resolution is 160 128 and we used 1546 frames from SwitchLight1000.bmp to SwitchLight2545.bmp . In the second video the resolution is 320 256 and we use 1000 frames from ShoppingMall1001.bmp to ShoppingMall2000.bmp . Therefore the data matrices are of size 1546 20480 and 1001 81920. We used a computer with Intel Core 2 Quad Q6600 2.4 GHz and 8 GB memory due to the large size of these data.

We applied GMS GMS2 and EGMS with d 3 and with initial dimensionality reduction to 200 to reduce running time. For this data we are unaware of a standard choice of d though we noticed empirically that the outputs of our algorithms as well as other algorithms are very stable to changes in d within the range 2 d 5. We obtain the foreground by the orthogonal projection to the recovered 3 dimensional subspace.

Initially estimator receives input data from a sensor such as image source . The input data comprises a set of data points that conforms to a plurality of dimensions D and includes outlier data points such as noise.

In some examples estimator pre processes the data points . For example as explained above estimator may insert a plurality of artificial outlier data points into the set of data point prior to iteratively processing the set of data points to compute the reduced data set. As another example as also explained herein estimator may normalize each of the data points to a unit sphere by dividing each of the data points to a corresponding Euclidean norm of the data point.

After optionally pre processing the set of data points estimator initializes a minimization matrix Q for estimating a subspace of dimension d . The minimization matrix Q or A for a non inverse covariance matrix initialized for use with any of the techniques described herein such as the GMS technique Algorithm 2 or the Extended GMS technique Algorithm 3 .

Next estimator iteratively re computes the minimization matrix Q as a normalized and weighted covariance having weighting coefficients that represent proximity to estimate of the subspace associated with minimization matrix Q . As explained above estimator may in some example implementations compute the minimization matrix Q in inverse form i.e. an inverse covariance matrix. In this way estimator determines for each iteration a scaled version of the set of data points by re computing a corresponding coefficient for each of the data points as a function of a proximity of the data point to a current estimate of the subspace and re computes for each iteration the minimization matrix A representing an updated estimate of the subspace based on a summation of weighted least absolute squares of the scaled version of the set of data points.

Upon terminating the iterative process YES of estimator determines the estimate of the subspace from the minimization matrix Q . For example when using Qin the form of an inverse covariance matrix estimator extracts as the subspace a bottom set of d eigenvectors from the computed minimizer Q. As another example when using Qin the form of a covariance matrix estimator extracts as the subspace a top set of d eigenvectors from the computed minimizer A.

Upon determining the reduced data set including the subspace data analysis system may take action. For example data analysis system may further process the reduced data set in view of the determined subspace to perform face recognition with respect to the image data received from the image source. As other examples data analysis system may further process the reduced data set in view of the determined subspace perform computer vision machine learning or other actions.

Here a computer includes a processor that is operable to execute program instructions or software causing the computer to perform various methods or tasks. Processor is coupled via bus to a memory which is used to store information such as program instructions and other data while the computer is in operation. A storage device such as a hard disk drive nonvolatile memory or other non transient storage device stores information such as program instructions data files of the multidimensional data and the reduced data set and other information. The computer also includes various input output elements including parallel or serial ports USB Firewire or IEEE 1394 Ethernet and other such ports to connect the computer to external device such a printer video camera surveillance equipment or the like. Other input output elements include wireless communication interfaces such as Bluetooth Wi Fi and cellular data networks.

The computer itself may be a traditional personal computer a smart phone a rack mount or business computer or server as shown in or any other type of computerized system such as an image capture or processing device. Other examples include appliances such as a set top box including a separate appliance or incorporated into another appliance such as a media player or television. The computer in a further example may include fewer than all elements listed above such as a thin client or mobile device having only some of the shown elements. In another example the computer is distributed among multiple computer systems such as a distributed server that has many computers working together to provide various functions.

The techniques described herein may be implemented in hardware software firmware or any combination thereof. Various features described as modules units or components may be implemented together in an integrated logic device or separately as discrete but interoperable logic devices or other hardware devices. In some cases various features of electronic circuitry may be implemented as one or more integrated circuit devices such as an integrated circuit chip or chipset.

If implemented in hardware this disclosure may be directed to an apparatus such a processor or an integrated circuit device such as an integrated circuit chip or chipset. Alternatively or additionally if implemented in software or firmware the techniques may be realized at least in part by a computer readable data storage medium comprising instructions that when executed cause one or more processors to perform one or more of the methods described above. For example the computer readable data storage medium may store such instructions for execution by a processor. Any combination of one or more computer readable medium s may be utilized.

A computer readable medium may form part of a computer program product which may include packaging materials. A computer readable medium may comprise a computer data storage medium such as random access memory RAM read only memory ROM non volatile random access memory NVRAM electrically erasable programmable read only memory EEPROM flash memory magnetic or optical data storage media and the like. In general a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device. Additional examples of computer readable medium include computer readable storage devices computer readable memory and tangible computer readable medium. In some examples an article of manufacture may comprise one or more computer readable storage media.

In some examples the computer readable storage media may comprise non transitory media. The term non transitory may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM or cache .

The code or instructions may be software and or firmware executed by processing circuitry including one or more processors such as one or more digital signal processors DSPs general purpose microprocessors application specific integrated circuits ASICs field programmable gate arrays FPGAs or other equivalent integrated or discrete logic circuitry. Accordingly the term processor as used herein may refer to any of the foregoing structure or any other processing circuitry suitable for implementation of the techniques described herein. In addition in some aspects functionality described in this disclosure may be provided within software modules or hardware modules.

An M estimator is described herein for the problems of exact and near subspace recovery. The recovery obtained by this estimator is quantified as well as its numerical approximation. Numerical experiments demonstrate state of the art speed and accuracy for the implementation on both synthetic and real data sets.

Although specific embodiments have been illustrated and described herein it will be appreciated by those of ordinary skill in the art that any arrangement that achieve the same purpose structure or function may be substituted for the specific embodiments shown. This application is intended to cover any adaptations or variations of the embodiments of the invention described herein. It is intended that this invention be limited only by the claims and the full scope of equivalents thereof.

