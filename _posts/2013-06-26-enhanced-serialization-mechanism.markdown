---

title: Enhanced serialization mechanism
abstract: The present disclosure discloses a method and network device for an enhanced serialization mechanism. Specifically, the disclosed system receives a plurality of packets from a plurality of transport layer flows corresponding to a security association. Also, the system designates one processor of a plurality of processors to be associated with the security association. Moreover, the system assigns a sequence number to each packet, and transmits the plurality of packets from the plurality of transport layer flows such that packets within the same transport layer flow are transmitted in order of their sequence numbers. However, at least two packets from two different transport layer flows may be transmitted out of incremental order of their sequence number.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09065701&OS=09065701&RS=09065701
owner: ARUBA NETWORKS, INC.
number: 09065701
owner_city: Sunnyvale
owner_country: US
publication_date: 20130626
---
This application claims the benefit of priority on U.S. Provisional Patent Application 61 732 829 filed Dec. 3 2012 the entire contents of which are incorporated by reference.

Related patent applications to the subject application include the following 1 U.S. Patent Application entitled System and Method for Achieving Enhanced Performance with Multiple Networking Central Processing Unit CPU Cores by Janakiraman et al. U.S. application Ser. No. 13 692 622 filed Dec. 3 2012 2 U.S. Patent Application entitled Ingress Traffic Classification and Prioritization with Dynamic Load Balancing by Janakiraman et al. U.S. application Ser. No. 13 692 608 filed Dec. 3 2012 3 U.S. Patent Application entitled Method and System for Maintaining Derived Data Sets by Gopalasetty et al. U.S. application Ser. No. 13 692 920 filed Dec. 3 2012 4 U.S. Patent Application entitled System and Method for Message handling in a Network Device by Palkar et al. U.S. application Ser. No. 13 918 732 filed Jun. 14 2013 5 U.S. Patent Application entitled Session Based Forwarding by Janakiraman et al. U.S. application Ser. No. 13 918 748 filed Jun. 14 2013 6 U.S. Patent Application entitled Rate Limiting Mechanism Based on Device Load Capacity or Traffic Content by Nambiar et al. U.S. application Ser. No. 13 918 760 filed Jun. 14 2013 7 U.S. Patent Application entitled Control Plane Protection for Various Tables Using Storm Prevention Entries by Janakiraman et al. U.S. patent application Ser. No. 13 918 770 filed Jun. 14 2013. The entire contents of the above applications are incorporated herein by reference.

The present disclosure relates to networking processing performance of a symmetric multiprocessing SMP network architecture. In particular the present disclosure relates to a system and method for an enhanced serialization mechanism.

A symmetric multiprocessing SMP architecture generally is a multiprocessor computer architecture where two or more identical processors can connect to a single shared main memory. In the case of multi core processors the SMP architecture can apply to the CPU cores.

In an SMP architecture multiple networking CPUs or CPU cores can receive and transmit network traffic. Generally network packets are received by NAE are passed to Packet ordering engine POE . POE in SMP system not only helps control the order in which the packets are transmitted out from the system but also helps control the order in which the packets are processed within the system.

Typically to perform load balancing the system will process a packet on a first come first serve round robin fashion. Thus when the POE receives one or more packets from the NAE before sending the packets to the CPU core for processing the POE will set the order of the packets according when each packet is received by POE. When POE receives processed packets from the CPU core POE will order the packets according to their set order. Thus POE will send out processed packets in exactly the same order as those packets are received.

In addition a conventional POE also allows for grouping packets from multiple flows by their corresponding flow identifiers. Thus the POE can map a particular L4 flow to a POE flow. A POE flow generally represents a queue. For example a POE may support 64K queues and 64K slots. A slot generally refers to a buffer or a packet descriptor. On the other hand while ordering received packets the POE can parse a packet and classify it as either a L3 or L4 flow and then map the packet to a queue in the POE based on the flow ID associated with the packet. Specifically the POE can extract a key from a received packet and configure a hashing algorithm which can be indexed to a range of flow identifiers. Therefore essentially the POE can map a packet to a flow. When the packet arrives at the CPU processing core the packet will include information about both its flow identifier and its the packet descriptors. Note that it is possible for the POE to use 64K queues with each queue corresponding to a slot. Also the POE may use one queue that uses all of the 64K slots. Alternatively the system may determine which ingress CPU which a received packet will be sent to based on the flow key without the use of the POE.

In a system where packets from the same flow are always sent to the same CPU core for processing the proper ordering of packets within the same transport layer flow is guaranteed. However it is difficult to guarantee correct ordering of packets from multiple transport layer flows because these packets may be forwarded to different CPU cores for processing and thus may experience various amount of delay which results in the packets to be transmitted out of the system out of order.

In the following description several specific details are presented to provide a thorough understanding. While the context of the disclosure is directed to SMP architecture performance enhancement one skilled in the relevant art will recognize however that the concepts and techniques disclosed herein can be practiced without one or more of the specific details or in combination with other components etc. In other instances well known implementations or operations are not shown or described in details to avoid obscuring aspects of various examples disclosed herein. It should be understood that this disclosure covers all modifications equivalents and alternatives falling within the spirit and scope of the present disclosure.

Embodiments of the present disclosure relate to networking processing performance. In particular the present disclosure relates to a system and method for enhanced serialization. Specifically the system achieves enhanced serialization by receiving a plurality of packets from a plurality of transport layer flows corresponding to a security association. Also the system designates one processor of a plurality of processors to be associated with the security association. Moreover the system assigns a sequence number to each packet and transmits the plurality of packets from the plurality of transport layer flows such that packets within the same transport layer flow are transmitted in order of their sequence numbers. However at least two packets from two different transport layer flows may be transmitted out of incremental order of their sequence number.

Specifically includes at least a control plane process two or more datapath processors a lockless shared memory accessible by the two or more datapath processors and a network interface .

Control plane process may be running on one or more CPU or CPU cores such as CP CPU CP CPU . . . CP CPU M . Furthermore control plane process typically handles network control or management traffic generated by and or terminated at network devices as opposed to data traffic generated and or terminated at client devices.

According to embodiments of the present disclosure datapath processors include a single exception handling CPU such as a slowpath SP processor and multiple fastpath FP processors or ASICs e.g. ASIC FP CPU ASIC FP CPU . . . ASIC FP CPU N . Only ASIC FP processors are able to receive data packets directly from network interface . Exception handling processor on the other hand only receives data packets from ASIC FP processors.

Lockless shared memory is a flat structure that is shared by all datapath processors and not tied to any particular CPU or CPUs. Any datapath processor can read any memory location within lockless shared memory . Therefore both the single exception handling processor and the multiple ASIC FP processors e.g. ASIC FP CPU ASIC FP CPU . . . ASIC FP CPU N have read access to lockless shared memory but besides write access to update timestamps by the multiple ASIC FP processors only the single exception handling processor has write access to lockless shared memory . More specifically any datapath processor can have access to any location in lockless shared memory in the disclosed system.

Also control plane process is communicatively coupled to exception handling CPU but not ASIC or fastpath FP processors e.g. ASIC FP CPU ASIC FP CPU . . . ASIC FP CPU N . Thus whenever control plane process needs information from datapath processors control plane process will communicate with exception handling processor .

In a typical FP processor s pipeline process as illustrated in for each ingress packet the packet will pass through one or more of the following processing steps a port lookup a VLAN lookup a port VLAN lookup a bridge lookup a firewall session lookup a route table or route cache lookup forwarding table lookup etc. In some embodiments before the packet is transmitted the packet will go through encapsulation and or encryption process. If it s a L2 packet the packet will go through bridge table lookup after encryption and continue with the process flow. On the other hand if it s a L3 packet the packet will go through firewall session lookup and continue with the process flow. In some embodiments for example when an ingress packet is received via a GRE tunnel the packet will go through a tunnel de capsulation and or decryption process. If it s a L2 packet the packet will go through bridge table lookup after encryption and continue with the process flow. Note that the L2 packets will be going through steps such as firewall session processing because the disclosed system is capable of performing L3 L4 policy enforcement Deep Packet Inspection DPI etc. for even L2 bridge traffic.

Thus the network forwarding process illustrated in includes at least a port lookup operation a virtual local area network VLAN lookup operation a port VLAN lookup operation a bridge lookup operation a firewall session lookup operation a route lookup operation a forward lookup operation an encapsulation operation an encryption operation a tunnel de capsulation operation a decryption operation and a transmit operation .

Packet ordering engine POE not only helps to control the order in which the packets are sent out from the disclosed system but also helps control the order in which the packets are processed inside the system.

In some embodiments to simplify processing the system processes packets of the same session in a one CPU to ensure that a session entry is being used and updated by only one CPU at any point of time. This implicitly decides the order in which the packets shall be handled.

On the other hand packets from two servers destined to the same client can be processed in different CPUs. When these packets need to be encrypted with the same key prior to being transmitted to the client the packets would have to egress the network device e.g. a network controller in the order in which the sequence counter got assigned to each of the packets.

Now consider two packets from each of two different flows destined to the same client A A and B B. As long as packet A is forwarded to the network from the disclosed system after packet A and packet B after packet B the order across the two flows has little significance on the network s functionality and or performance and thus giving the disclosed system flexibility in which the encryption sequence counters are assigned to these packets. The disclosed system only needs to inform the POE about the sequence in which the counters were assigned so that POE can release the packets to the network in the same order in which the counters were assigned.

Given that the ingress CPU assignment of the packets by design ensures the correct ordering within a flow clear text packets do not require explicit ordering of packets from hardware. The packets need be ordered only when they need to be encrypted by SAE because there is a possibility for packet reordering in the security engine SAE as well as in the ingress CPU that assigns the packet counter and sets up the packet for encryption.

In one embodiment Security Acceleration Engine SAE comprises multiple identical units e.g. up to 12 identical units to which the packets are allocated to in a round robin fashion. For a given cipher encryption time for a packet may be dependent on the size of the packet. Hence a small packet arriving at SAE after a large packet can be sent back out of order after encryption from SAE.

When an attempt to send a message fails the message gets retried from a virtual output queue in every CPU. Based on the credit availability the packets can be temporarily stuck in these virtual output queues for a short period of time. Moreover when packets get setup for encryption from different ingress CPUs based on the credit availability message to SAE can be sent out of order compared to the order in which the counters were assigned to the packet.

To avoid packet ordering issues under these circumstances POE can be setup with the ordering in which it should be transmitted when the counters are assigned to the packet. Thus after encryption when the POE receives the packet out of order from the SAE either due to the spraying in SAE or due to other reasons POE can wait for the packet in the head to arrive prior to transmitting the packets that are encrypted and are ready for transmission. Specifically as an illustrative example a POE can queue up to 64K packets at a time across 64K queues. Up to 6K of the packets can be in an on chip memory and additional packets will be stored into memory.

While the POE buffer in sending packets out of the CPU in the same order in which it was received when allowing the packet to be processed in parallel in CPU or any other subsystem in the SMP architecture.

Packet ordering engine POE in the SMP architecture provides hardware support to ensure transmission of packet in the order in which it was received based on a flow identifier. Particularly multi core multi thread processor variants in the disclosed system can support a plurality of flow identifiers e.g. up to 64K flow identifiers where only some of these flow identifiers normally less than one quarter of the flow identifiers e.g. 6K flow identifiers can be stored on the chip and the rest in the memory. The flow identifier for a queued packet can be determined in multiple ways.

Packet ordering is done based on two entities 1 a flow identifier e.g. 16 bit identifier and 2 a buffer handle e.g. 16 bit handle . The buffer handle is used to identify a specific packet stored at a certain location. The actual address or the content of the packet is not used by the POE. POE generally maintains multiple queues of packet handles based on their corresponding flow identifiers. Once the flow identifier and the buffer handle is assigned POE can control the order in which the packets are processed.

Security Acceleration Engine SAE is capable of supporting up to 40 Gbps of throughput. It supports all of the WLAN ciphers such as Advanced Encryption Standard Counter with Cipher Block Chaining Message Authentication Code AESCCM Advanced Encryption Standard Counter with Galois Counter Mode AESGCM and Rivest Cipher 4 RC4 along with other IPSec crypto ciphers such as Triple Data Encryption Standard 3DES .

In a general ciphering process a security key typically includes two parts a static key and a counter which can be 32 bit 64 bit or 128 bit based on different ciphering algorithms. The key may be negotiated by any security protocol such as IKE. IEEE 802.1x etc. The counter increments for each packet. Thus no same key is used to encrypt two packets because of the packet counter increment. As an example when a packet is transmitted from device A to device B the counter portion is carried in dear text in the packet. Further the key is already negotiated and thus both device A and device B have knowledge of the pre negotiated key.

Note that the counter is used for protecting against replay attacks. Thus the system must ensure that the counter values in the packets are in increasing order. Nevertheless if device A and device B communicates in layer 3 or layer 4it is possible that the packets are received in different ordering because different packets may be transmitted via different routes. In addition each security protocol typically defines an acceptance window. For example for IPSec size of the acceptance window is defined as 64. Thus for any received packet the system will accept the packet if the difference between the counter in the packet and the counter in a previously received packet causing a first known gap in the counter sequence is within the protocol defined acceptance window. Otherwise if the difference the counters are greater than the size of the acceptance window the received packet will be discarded. By contrast for IEEE 802.11 packets L2 encryption is used in one hop communications for example between an access point and a client. Thus the acceptance window size is 1. Therefore if any packet from a flow is received out of order the packet will be discarded.

Also SAE supports Advanced Encryption Standard in Counter with CBC MAC mode AES CCM in a single pass and provides 40 Gbps of raw throughput. As mentioned earlier SAE throughput is independent of the CPU clock frequency. The descriptor format for SAE in SMP architecture supports byte alignment for all address and length fields to allow for use without restriction under various circumstances like WLAN fragmentation and Aggregated MAC Service Date Unit AMSDU .

Furthermore when a descriptor is sent to SAE for encryption or decryption the return message generally is set to be received by a CPU with a cookie that was sent in the original request for software use. SAE allows specifying any feedback message and allows sending the response back to any subsystem in the chipset. It could be set to return the packet to the POE engine to serialize the packet and send it for transmission. The return descriptor format can be set to contain the encrypted packet along with the flow identifier and the POE handle to send the packet for transmission.

SAE internally is composed of multiple engines e.g. 12 engines which have corresponding security pipes e.g. 12 security pipes each with a capacity of approximately 4 Gbps for example. There are multiple Virtual Channels VCs such as 16 virtual channels for example which schedule the descriptors for cipher operations. SAE Engine select registers allows these channels to send descriptors to one or more of the Security Pipes. A majority of the virtual channels may be low priority virtual channels while the remainder are high priority virtual channels. As an illustrative embodiment out of 16 virtual channels 12 channels are low priority channels and the virtual channels through are high priority VCs. Messages in high priority VCs are scheduled for operations before any of the messages in 12 low priorities and sent to the security pipes.

Moreover packets from different flows such as flow and flow can be processed by different CPU processing cores. However packets from both flows need to be encrypted using the same Security Association SA . Therefore once each packet is assigned a SA sequence number the system needs to ensure that the packets are transmitted out in the order of the assigned SA sequence number.

One solution for ensuring the order of the SA sequence number is to use an atomic counter for the sequence number. The atomic counter is uniquely associated with the SA. Thus the atomic counter provides way of providing consistent counters in an SMP architecture by serially incrementing the SA sequence number value.

It is an important goal that the order in which the packets are sent to the counter shall be the same as the order in which the packets are sent by the POE to the SAE. Thus an alternative and enhanced way of achieving the above goal is for SAE to use the POE s assistance to perform the reordering of the packets.

Now assuming that forwarding processor and forwarding processor both perform the operations that a generic forwarding process does as described above. Furthermore assuming that both a first packet from flow is received by forwarding processor and a second packet from flow is received by forwarding processor at the same time point t. Both flow and flow are associated with the same client and thus same SA. The second packet however gets immediately assigned a first sequence number by the counter CPU at time point t. On the other hand the first packet gets assigned a second sequence number by the counter CPU at a later time point t. Thus the second sequence number assigned to the first packet is greater than the first sequence number assigned to the second packet in this case.

Moreover forwarding processor sends the first packet to SAE for encryption at time point t and the encrypted first packet is subsequently received by forwarding processor at time point t. By contrast forwarding processor sends the second packet to SAE at time point t which is later than time point t. The encrypted second packet is later received by forwarding processor at time point t which is later than time point tat which the first packet is returned to forwarding processor . Thus although the second sequence number assigned to the first packet is greater than the first sequence number assigned to the second packet here the first packet is received from SAE earlier than the second packet. Therefore there is no guarantee that a packet with an earlier assigned sequence number would be returned by SAE earlier and thus in the order of the sequence number.

Hence SAE may produce packets that are out of ordering as illustrated in . A number of reasons may lead to reordering of the packets for example various CPU processing times by different forwarding processors various SAE processing time due to different natures of the packets various delays in different queues in the system the encryption setup process at various forwarding processors may experience delays etc.

Likewise for flow a second packet from is received by NAE . The second packet is then forwarded from NAE to POE operation A . POE assigns a sequence number to the second packet based on its flow identifier and forwards the second packet to the corresponding ingress CPU A that is designated for processing flow operation B . Next ingress CPU A sends the second packet to anchor CPU operation C which uses an atomic counter to assign a POE sequence number and returns the second packet to POE operation D . The second packet having assigned a POE sequence number is then sent by POE to ingress CPU A for further processing operation E . Upon receiving the second packet ingress CPU A forwards the second packet to SAE for encryption operation F . SAE encrypts the second packet and send the encrypted packet back to POE operation G . Subsequently POE ensures that packets in flow are in correct order and sends the processed packet to NAE operation H . If POE detects that packets are received from SAE out of order POE will wait for the missing packets with earlier assigned sequence numbers to be returned from SAE prior to sending out received packets with later assigned sequence numbers.

In particular note that with the solution described herein the disclosed system designates one forwarding CPU as an anchor CPU for a specific security association SA where the anchor CPU is configured for handling serialization through assignment of sequence numbers. Note that the anchor CPU can be different from the designated ingress CPU. Also multiple ingress CPUs may be designated to the same anchor CPU. Because it is common for one anchor CPU to serve multiple ingress CPUs it is desirable to assign the anchor CPU minimal amount of work in order to achieve high system performance. Also it is contemplated that a hardware engine such as POE would handle the serialization without the need of the anchor CPU.

Accordingly for the anchor CPU to work effectively with the POE to ensure correct packet ordering across multiple flows associated with the same SA at least two things need to happen atomically. First the assignment of POE sequence number needs to be atomic. Second the anchor CPU shall inform the POE of the flow identifier and the buffer.

Note that besides executing the above operations at the anchor CPU the above two operations can be alternatively executed as a critical section at the ingress CPU without involvement of the anchor CPU. A critical section is a SMP technique which generally means that no two CPUs execute the same code. More specifically the process of a critical section involves obtaining a lock executing pieces of code and releasing the lock.

Assuming that each SA corresponds to a specific POE flow identifier because each end to end connection between a client and a server represents a layer 4 flow when one client has two concurrent connections to two different servers and thus corresponds to two different layer 4 flows the disclosed system will designate a single POE flow identifier to both layer 4 flows sharing the same SA. Note that whenever a new SA is created upon completion of authentication key negotiation e.g. completion of IEEE 802.1x transaction the disclosed system will assign a POE flow identifier to the SA.

When a packet is received by NAE the packet is forwarded from NAE to POE POE forwards the packet to its designated ingress CPU without any modification based on the flow identifier associated with the packet. Thus a first packet from flow will be forwarded to ingress CPU N and a second packet from flow will be forwarded to ingress CPU A. When the packet is received by the ingress CPU the ingress CPU will go through the forwarding pipeline process as described in . Moreover the ingress CPU knows which key shall be used to encrypt the packet by first determining the destination e.g. a client s IP or MAC address associated with the packet. Then based on the destination the ingress CPU can determine which SA the packet is associated with. Thus the ingress CPU can further determine based on the SA which flow the packet belongs to.

Because each SA corresponds to a specific anchor CPU in the message sent from ingress CPU e.g. ingress CPU A or ingress CPU N to anchor CPU the disclosed system will include but are not limited to the SA the POE flow identifier and a packet descriptor. In one embodiment the packet descriptor includes a start address of the buffer. Moreover the POE flow identifiers are stored in the queue.

Next upon receiving the packet from ingress CPU e.g. ingress CPU A or ingress CPU N anchor CPU increments its counter. Note that the increment of the counter by anchor CPU does not need to be an atomic operation because there is only one anchor CPU assigned for each POE flow. Thus in the example illustrated in packets from both flow and flow will be sent to the same anchor CPU corresponding to the same POE flow identifier despite that they may be sent from two different ingress CPUs corresponding to two different layer 4 flows. The need for atomic operation is eliminated because there is a single anchor CPU that can process only one packet at a time regardless of which layer 4 flow that the packet belongs to.

Subsequently anchor CPU sends the packet to POE with the POE flow identifier and the response message that is set up to be sent to the original ingress CPU e.g. ingress CPU A or N . Specifically the response message includes but is not limited to a counter e.g. POE sequence number SA information and a packet descriptor. Because anchor CPU is configured to perform only two operations anchor CPU can perform its functions very efficiently. Note that POE is not engaged in the assignment of any flow identifier.

When POE receives the packet POE performs an enqueue operation on the packet based on the flow identifier and assigns flow id POE addr to the packet. The POE address generally refers to one of the 64K slots that the disclosed system maintains.

Note that other orderings of the packets may exist as long as the packets within the same layer 4 flow are in incrementing orders in the queue of the POE flow. In other words the disclosed system allows any kind of reordering of the packets between different layer 4 flows. However the ordering of the packets within each layer 4 flows is the same in the POE queue. In some embodiments POE address generally refers to a link list address. Thus flow id POE addr corresponds to an element in a queue as illustrated in . Further flow id POE addr is associated with flow id and an address inside the queue e.g. POE addr which can be a link list address 

POE queues the packets in the ordering in which the packets are received from anchor CPU. Then POE sends the packets to their respective ingress CPUs which send the packets to SAE for encryption. Upon completion of the encryption operation SAE will return the packets to POE. POE will compare the returned packet with the head of the POE queue and determine whether the returned packet is identical to the head of the POE queue. Note that SAE will mark the status of the packet as being ready for transmission after SAE encrypts the packet. The disclosed system can transmit the processed packet to the network anytime when the packet is marked as ready for transmission if the packet is identical to the head of the queue. After a packet returned from SAE to POE is transmitted out to the network via NAE POE will remove the packet from the queue that it maintains such that the head of the POE queue is associated with the packet with the next POE sequence number. If however the packet received by POE from SAE is not the head of the queue then POE will queue the packet until it receives the processed packet which is marked as ready that matches the head of the POE queue.

In some embodiments to further enhance the capacity of the disclosed system multiple security associations SAs can correspond to the same POE flow while multiple layer 4 flows may correspond to each SA. Thus in these embodiments each element of the POE queue may be represented as flow id SA info POE addr .

Furthermore the disclosed system assigns a sequence number to each packet operation . In some embodiments assigning the sequence number is performed by the designated one processor. In other embodiments assigning the sequence number is executed as a critical section by a respective ingress processor among the plurality of processors. The critical section ensures that no two processors in the disclosed system will execute the code section simultaneously.

The designated one processor will then forward the packet with assigned sequence number to a packet ordering module. Subsequently the packet ordering module queues the plurality of packets by the assigned sequence number of each packet in a queue and sends the plurality of packets to a security processing module for encryption. When the packet ordering module receives a processed packet from the security processing module the packet ordering module determines whether the processed packet corresponds to a head element of the queue. If so the disclosed system transmits the processed packet and removes the head element from the queue. Otherwise the disclosed system delays the transmission of the processed packet until the processed packet corresponds to the head element of the queue. Accordingly the disclosed system transmits the plurality of packets from the plurality of transport layer flows such that packets within the same transport layer flow are transmitted in order of their sequence numbers operation .

It is important to note that two packets from two different transport layer flows may be transmitted out of incremental order of their sequence number. Also note that the plurality of packets may correspond to a queue that is uniquely identified by any one of a security association a priority queue and a basic service identifier BSSID .

Network interface can be any communication interface which includes but is not limited to a modem token ring interface Ethernet interface wireless IEEE 802.11 interface e.g. IEEE 802.11n IEEE 802.11ac etc. cellular wireless interface satellite transmission interface or any other interface for coupling network devices. In some embodiments network interface may be software defined and programmable for example via an Application Programming Interface API and thus allowing for remote control of the network device .

Shared memory can include storage components such as Dynamic Random Access Memory DRAM Static Random Access Memory SRAM etc. In some embodiments shared memory is a flat structure that is shared by all datapath processors including e.g. exception handling processor core ASIC fastpath processor core ASIC fastpath processor core . . . ASIC fastpath processor core etc. and not tied to any particular CPU or CPU cores. Any datapath processor can read any memory location within shared memory . Shared memory can be used to store various tables lists and or POE queues to facilitate enhanced serialization mechanism described herein. For example the tables may include but are not limited to a bridge table a session table a user table a station table a tunnel table a route table and or route cache etc. It is important to note that any datapath processor can have access to any location in lockless shared memory in network device .

Exception handling processor core typically includes a networking processor core that is capable of processing network data traffic. Exception handling processor core is a single designated CPU core per ASIC fastpast CPU core that typically handles table managements and exceptions such as missed entry in the tables. Note that exception handling processor core only receives data packets from one or more ASIC fastpath processor cores such as ASIC fastpath processor core ASIC fastpath processor core . . . ASIC fastpath processor core . In other words exception handling processor core does not receive data packets directly from any line cards or network interfaces. Only the plurality of fastpath processor cores can send data packets to exception handling processor core . Moreover exception handling processor core is the only processor core having the write access to entries in the tables stored within shared memory and thereby will not cause any data integrity issues even without a locking mechanism in place for shared memory .

ASICs fastpath processor cores also include networking processor cores that are capable of processing network data traffic. However by definition ASICs fastpath processor cores only performs fast packet processing. Thus ASICs fastpath processor cores do not block themselves and wait for other components or modules during the processing of network packets. Any packets requiring special handling or wait by a processor core will be handed over by ASIC fastpath processor cores to exception handling processor core .

Each of ASIC fastpath processor cores maintains one or more counters. The counters are defined as a regular data type for example unsigned integer unsigned long etc. in lieu of an atomic data type. When an ASIC fastpath processor core receives a packet it may increment or decrement the values of the counters to reflect network traffic information including but not limited to the number of received frames the number of received bytes error conditions and or error counts etc. A typical pipeline process at ASIC fastpath processor cores includes one or more of port lookup VLAN lookup port VLAN table lookup bridge table lookup firewall session table lookup route table lookup packet encapsulation packet encryption packet decryption tunnel de capsulation forwarding etc.

According to embodiments of the present disclosure network services provided by network device solely or in combination with other wireless network devices include but are not limited to an Institute of Electrical and Electronics Engineers IEEE 802.1x authentication to an internal and or external Remote Authentication Dial In User Service RADIUS server an MAC authentication to an internal and or external RADIUS server a built in Dynamic Host Configuration Protocol DHCP service to assign wireless client devices IP addresses an internal secured management interface Layer 3 forwarding Network Address Translation NAT service between the wireless network and a wired network coupled to the network device an internal and or external captive portal an external management system for managing the network devices in the wireless network etc.

The present disclosure may be realized in hardware software or a combination of hardware and software. The present disclosure may be realized in a centralized fashion in one computer system or in a distributed fashion where different elements are spread across several interconnected computer systems coupled to a network. A typical combination of hardware and software may be an access point with a computer program that when being loaded and executed controls the device such that it carries out the methods described herein.

The present disclosure also may be embedded in non transitory fashion in a computer readable storage medium e.g. a programmable circuit a semiconductor memory such as a volatile memory such as random access memory RAM or non volatile memory such as read only memory power backed RAM flash memory phase change memory or the like a hard disk drive an optical disc drive or any connector for receiving a portable memory device such as a Universal Serial Bus USB flash drive which comprises all the features enabling the implementation of the methods described herein and which when loaded in a computer system is able to carry out these methods. Computer program in the present context means any expression in any language code or notation of a set of instructions intended to cause a system having an information processing capability to perform a particular function either directly or after either or both of the following a conversion to another language code or notation b reproduction in a different material form.

As used herein digital device generally includes a device that is adapted to transmit and or receive signaling and to process information within such signaling such as a station e.g. any data processing equipment such as a computer cellular phone personal digital assistant tablet devices etc. an access point data transfer devices such as network switches routers controllers etc. or the like.

As used herein access point AP generally refers to receiving points for any known or convenient wireless access technology which may later become known. Specifically the term AP is not intended to be limited to IEEE 802.11 based APs. APs generally function as an electronic device that is adapted to allow wireless devices to connect to a wired network via various communications standards.

As used herein the term interconnect or used descriptively as interconnected is generally defined as a communication pathway established over an information carrying medium. The interconnect may be a wired interconnect wherein the medium is a physical medium e.g. electrical wire optical fiber cable bus traces etc. a wireless interconnect e.g. air in combination with wireless signaling technology or a combination of these technologies.

As used herein information is generally defined as data address control management e.g. statistics or any combination thereof. For transmission information may be transmitted as a message namely a collection of bits in a predetermined format. One type of message namely a wireless message includes a header and payload data having a predetermined number of bits of information. The wireless message may be placed in a format as one or more packets frames or cells.

As used herein wireless local area network WLAN generally refers to a communications network links two or more devices using some wireless distribution method for example spread spectrum or orthogonal frequency division multiplexing radio and usually providing a connection through an access point to the Internet and thus providing users with the mobility to move around within a local coverage area and still stay connected to the network.

As used herein the term mechanism generally refers to a component of a system or device to serve one or more functions including but not limited to software components electronic components electrical components mechanical components electro mechanical components etc.

As used herein the term embodiment generally refers an embodiment that serves to illustrate by way of example but not limitation.

It will be appreciated to those skilled in the art that the preceding examples and embodiments are exemplary and not limiting to the scope of the present disclosure. It is intended that all permutations enhancements equivalents and improvements thereto that are apparent to those skilled in the art upon a reading of the specification and a study of the drawings are included within the true spirit and scope of the present disclosure. It is therefore intended that the following appended claims include all such modifications permutations and equivalents as fall within the true spirit and scope of the present disclosure.

While the present disclosure has been described in terms of various embodiments the present disclosure should not be limited to only those embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Likewise where a reference to a standard is made in the present disclosure the reference is generally made to the current version of the standard as applicable to the disclosed technology area. However the described embodiments may be practiced under subsequent development of the standard within the spirit and scope of the description and appended claims. The description is thus to be regarded as illustrative rather than limiting.

