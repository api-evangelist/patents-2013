---

title: Receiving data streams in parallel and providing a first portion of data to a first state machine engine and a second portion to a second state machine
abstract: An apparatus can include a first state machine engine configured to receive a first portion of a data stream from a processor and a second state machine engine configured to receive a second portion of the data stream from the processor. The apparatus includes a buffer interface configured to enable data transfer between the first and second state machine engines. The buffer interface includes an interface data bus coupled to the first and second state machine engines. The buffer interface is configured to provide data between the first and second state machine engines.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09448965&OS=09448965&RS=09448965
owner: Micron Technology, Inc.
number: 09448965
owner_city: Boise
owner_country: US
publication_date: 20131028
---
This application is a non provisional of U.S. Patent Application Ser. No. 61 788 364 which was filed on Mar. 15 2013.

Embodiments of the invention relate generally to electronic devices and more specifically in certain embodiments to electronic devices with parallel devices for data analysis.

Complex data analysis e.g. pattern recognition can be inefficient to perform on a conventional von Neumann based computer. A biological brain in particular a human brain however is adept at performing complex data analysis. Current research suggests that a human brain performs data analysis using a series of hierarchically organized neuron layers in the neocortex. Neurons in the lower layers of the hierarchy analyze raw signals from for example sensory organs while neurons in higher layers analyze signal outputs from neurons in the lower levels. This hierarchical system in the neocortex possibly in combination with other areas of the brain accomplishes the complex data analysis that enables humans to perform high level functions such as spatial reasoning conscious thought and complex language.

In the field of computing pattern recognition tasks for example are increasingly challenging. Ever larger volumes of data are transmitted between computers and the number of patterns that users wish to detect is increasing. For example spam or malware are often detected by searching for patterns in a data stream e.g. particular phrases or pieces of code. The number of patterns increases with the variety of spam and malware as new patterns may be implemented to search for new variants. Searching a data stream for each of these patterns can form a computing bottleneck. Often as the data stream is received it is searched for each pattern one at a time. The delay before the system is ready to search the next portion of the data stream increases with the number of patterns. Thus pattern recognition may slow the receipt of data.

Hardware has been designed to search a data stream for patterns but this hardware often is unable to process adequate amounts of data in an amount of time given. Some devices configured to search a data stream do so by distributing the data stream among a plurality of circuits. The circuits each determine whether the data stream matches a portion of a pattern. Often a large number of circuits operate in parallel each searching the data stream at generally the same time. However there has not been a system that effectively allows for performing complex data analysis in a manner more comparable to that of a biological brain. Development of such a system is desirable.

Turning now to the figures illustrates an embodiment of a processor based system generally designated by reference numeral . The system e.g. data analysis system may be any of a variety of types such as a desktop computer laptop computer pager cellular phone personal organizer portable audio player control circuit camera etc. The system may also be a network node such as a router a server or a client e.g. one of the previously described types of computers . The system may be some other sort of electronic device such as a copier a scanner a printer a game console a television a set top video distribution or recording system a cable box a personal digital media player a factory automation system an automotive computer system or a medical device. The terms used to describe these various examples of systems like many of the other terms used herein may share some referents and as such should not be construed narrowly in virtue of the other items listed. 

In a typical processor based device such as the system a processor such as a microprocessor controls the processing of system functions and requests in the system . Further the processor may comprise a plurality of processors that share system control. The processor may be coupled directly or indirectly to each of the elements in the system such that the processor controls the system by executing instructions that may be stored within the system or external to the system .

In accordance with the embodiments described herein the system includes a state machine engine which may operate under control of the processor . As used herein the state machine engine refers to a single device e.g. single chip . The state machine engine may employ any automaton theory. For example the state machine engine may employ one of a number of state machine architectures including but not limited to Mealy architectures Moore architectures Finite State Machines FSMs Deterministic FSMs DFSMs Bit Parallel State Machines BPSMs etc. Though a variety of architectures may be used for discussion purposes the application refers to FSMs. However those skilled in the art will appreciate that the described techniques may be employed using any one of a variety of state machine architectures.

As discussed further below the state machine engine may include a number of e.g. one or more finite state machine FSM lattices e.g. core of the state machine engine . For purposes of this application the term lattice refers to an organized framework e.g. routing matrix routing network frame of elements e.g. Boolean cells counter cells state machine elements state transition elements . Furthermore the lattice may have any suitable shape structure or hierarchical organization e.g. grid cube spherical cascading . Each FSM lattice may implement multiple FSMs that each receive and analyze the same data in parallel. Further the FSM lattices may be arranged in groups e.g. clusters such that clusters of FSM lattices may analyze the same input data in parallel. Further clusters of FSM lattices of the state machine engine may be arranged in a hierarchical structure wherein outputs from state machine lattices on a lower level of the hierarchical structure may be used as inputs to state machine lattices on a higher level. By cascading clusters of parallel FSM lattices of the state machine engine in series through the hierarchical structure increasingly complex patterns may be analyzed e.g. evaluated searched etc. .

Further based on the hierarchical parallel configuration of the state machine engine the state machine engine can be employed for complex data analysis e.g. pattern recognition in systems that utilize high processing speeds. For instance embodiments described herein may be incorporated in systems with processing speeds of 1 GByte sec. Accordingly utilizing the state machine engine data from high speed memory devices or other external devices may be rapidly analyzed. The state machine engine may analyze a data stream according to several criteria e.g. search terms at about the same time e.g. during a single device cycle. Each of the FSM lattices within a cluster of FSMs on a level of the state machine engine may each receive the same search term from the data stream at about the same time and each of the parallel FSM lattices may determine whether the term advances the state machine engine to the next state in the processing criterion. The state machine engine may analyze terms according to a relatively large number of criteria e.g. more than 100 more than 1000 or more than 10 000. Because they operate in parallel they may apply the criteria to a data stream having a relatively high bandwidth e.g. a data stream of greater than or generally equal to 1 GByte sec without slowing the data stream.

In one embodiment the state machine engine may be configured to recognize e.g. detect a great number of patterns in a data stream. For instance the state machine engine may be utilized to detect a pattern in one or more of a variety of types of data streams that a user or other entity might wish to analyze. For example the state machine engine may be configured to analyze a stream of data received over a network such as packets received over the Internet or voice or data received over a cellular network. In one example the state machine engine may be configured to analyze a data stream for spam or malware. The data stream may be received as a serial data stream in which the data is received in an order that has meaning such as in a temporally lexically or semantically significant order. Alternatively the data stream may be received in parallel or out of order and then converted into a serial data stream e.g. by reordering packets received over the Internet. In some embodiments the data stream may present terms serially but the bits expressing each of the terms may be received in parallel. The data stream may be received from a source external to the system or may be formed by interrogating a memory device such as the memory and forming the data stream from data stored in the memory . In other examples the state machine engine may be configured to recognize a sequence of characters that spell a certain word a sequence of genetic base pairs that specify a gene a sequence of bits in a picture or video file that form a portion of an image a sequence of bits in an executable file that form a part of a program or a sequence of bits in an audio file that form a part of a song or a spoken phrase. The stream of data to be analyzed may include multiple bits of data in a binary format or other formats e.g. base ten ASCII etc. The stream may encode the data with a single digit or multiple digits e.g. several binary digits.

As will be appreciated the system may include memory . The memory may include volatile memory such as Dynamic Random Access Memory DRAM Static Random Access Memory SRAM Synchronous DRAM SDRAM Double Data Rate DRAM DDR SDRAM DDR2 SDRAM DDR3 SDRAM etc. The memory may also include non volatile memory such as read only memory ROM PC RAM silicon oxide nitride oxide silicon SONOS memory metal oxide nitride oxide silicon MONOS memory polysilicon floating gate based memory and or other types of flash memory of various architectures e.g. NAND memory NOR memory etc. to be used in conjunction with the volatile memory. The memory may include one or more memory devices such as DRAM devices that may provide data to be analyzed by the state machine engine . As used herein the term provide may generically refer to direct input insert send transfer transmit generate give output place write etc. Such devices may be referred to as or include solid state drives SSD s MultimediaMediaCards MMC s SecureDigital SD cards CompactFlash CF cards or any other suitable device. Further it should be appreciated that such devices may couple to the system via any suitable interface such as Universal Serial Bus USB Peripheral Component Interconnect PCI PCI Express PCI E Small Computer System Interface SCSI IEEE 1394 Firewire or any other suitable interface. To facilitate operation of the memory such as the flash memory devices the system may include a memory controller not illustrated . As will be appreciated the memory controller may be an independent device or it may be integral with the processor . Additionally the system may include an external storage such as a magnetic storage device. The external storage may also provide input data to the state machine engine .

The system may include a number of additional elements. For instance a compiler may be used to configure e.g. program the state machine engine as described in more detail with regard to . An input device may also be coupled to the processor to allow a user to input data into the system . For instance an input device may be used to input data into the memory for later analysis by the state machine engine . The input device may include buttons switching elements a keyboard a light pen a stylus a mouse and or a voice recognition system for instance. An output device such as a display may also be coupled to the processor . The display may include an LCD a CRT LEDs and or an audio display for example. They system may also include a network interface device such as a Network Interface Card NIC for interfacing with a network such as the Internet. As will be appreciated the system may include many other components depending on the application of the system .

The configurable elements can be configured e.g. programmed to implement many different functions. For instance the configurable elements may include state machine elements SMEs shown in that are hierarchically organized into rows shown in and blocks shown in . The SMEs may also be considered state transition elements STEs . To route signals between the hierarchically organized SMEs a hierarchy of configurable switching elements can be used including inter block switching elements shown in intra block switching elements shown in and intra row switching elements shown in .

As described below the switching elements may include routing structures and buffers. A SME can correspond to a state of a FSM implemented by the FSM lattice . The SMEs can be coupled together by using the configurable switching elements as described below. Accordingly a FSM can be implemented on the FSM lattice by configuring the SMEs to correspond to the functions of states and by selectively coupling together the SMEs to correspond to the transitions between states in the FSM.

In an example the input block the output block and or the programming interface can be implemented as registers such that writing to or reading from the registers provides data to or from the respective elements. Accordingly bits from the image stored in the registers corresponding to the programming interface can be loaded on the SMEs . Although illustrates a certain number of conductors e.g. wire trace between a block input block output block and an inter block switching element it should be understood that in other examples fewer or more conductors may be used.

In an example the row includes a first and second plurality of row interconnection conductors . In an example an input of a GOT can be coupled to one or more row interconnection conductors and an output can be coupled to one or more row interconnection conductor . In an example a first plurality of the row interconnection conductors can be coupled to each SME of each GOT within the row . A second plurality of the row interconnection conductors can be coupled to only one SME of each GOT within the row but cannot be coupled to the other SME of the GOT . In an example a first half of the second plurality of row interconnection conductors can couple to first half of the SMEs within a row one SME from each GOT and a second half of the second plurality of row interconnection conductors can couple to a second half of the SMEs within a row the other SME from each GOT as will be better illustrated with respect to . The limited connectivity between the second plurality of row interconnection conductors and the SMEs is referred to herein as parity . In an example the row can also include a special purpose element such as a counter a configurable Boolean logic element look up table RAM a field configurable gate array FPGA an application specific integrated circuit ASIC a configurable processor e.g. a microprocessor or other element for performing a special purpose function.

In an example the special purpose element comprises a counter also referred to herein as counter . In an example the counter comprises a 12 bit configurable down counter. The 12 bit configurable counter has a counting input a reset input and zero count output. The counting input when asserted decrements the value of the counter by one. The reset input when asserted causes the counter to load an initial value from an associated register. For the 12 bit counter up to a 12 bit number can be loaded in as the initial value. When the value of the counter is decremented to zero 0 the zero count output is asserted. The counter also has at least two modes pulse and hold. When the counter is set to pulse mode the zero count output is asserted when the counter reaches zero and the clock cycles. The zero count output is asserted during the next clock cycle of the counter . Resulting in the counter being offset in time from the clock cycle. At the next clock cycle the zero count output is no longer asserted. When the counter is set to hold mode the zero count output is asserted during the clock cycle when the counter decrements to zero and stays asserted until the counter is reset by the reset input being asserted.

In another example the special purpose element comprises Boolean logic. For example the Boolean logic may be used to perform logical functions such as AND OR NAND NOR Sum of Products SoP Negated Output Sum of Products NSoP Negated Output Product of Sume NPoS and Product of Sums PoS functions. This Boolean logic can be used to extract data from terminal state SMEs corresponding to terminal nodes of a FSM as discussed later herein in FSM lattice . The data extracted can be used to provide state data to other FSM lattices and or to provide configuring data used to reconfigure FSM lattice or to reconfigure another FSM lattice .

In an example a state machine element comprises a plurality of memory cells such as those often used in dynamic random access memory DRAM coupled in parallel to a detect line . One such memory cell comprises a memory cell that can be set to a data state such as one that corresponds to either a high or a low value e.g. a 1 or 0 . The output of the memory cell is coupled to the detect line and the input to the memory cell receives signals based on data on the data stream line . In an example an input at the input block is decoded to select one or more of the memory cells . The selected memory cell provides its stored data state as an output onto the detect line . For example the data received at the input block can be provided to a decoder not shown and the decoder can select one or more of the data stream lines . In an example the decoder can convert an 8 bit ACSII character to the corresponding 1 of 256 data stream lines .

A memory cell therefore outputs a high signal to the detect line when the memory cell is set to a high value and the data on the data stream line selects the memory cell . When the data on the data stream line selects the memory cell and the memory cell is set to a low value the memory cell outputs a low signal to the detect line . The outputs from the memory cells on the detect line are sensed by a detection cell .

In an example the signal on an input line sets the respective detection cell to either an active or inactive state. When set to the inactive state the detection cell outputs a low signal on the respective output regardless of the signal on the respective detect line . When set to an active state the detection cell outputs a high signal on the respective output line when a high signal is detected from one of the memory cells of the respective SME . When in the active state the detection cell outputs a low signal on the respective output line when the signals from all of the memory cells of the respective SME are low.

In an example an SME includes 256 memory cells and each memory cell is coupled to a different data stream line . Thus an SME can be programmed to output a high signal when a selected one or more of the data stream lines have a high signal thereon. For example the SME can have a first memory cell e.g. bit 0 set high and all other memory cells e.g. bits 1 255 set low. When the respective detection cell is in the active state the SME outputs a high signal on the output when the data stream line corresponding to bit 0 has a high signal thereon. In other examples the SME can be set to output a high signal when one of multiple data stream lines have a high signal thereon by setting the appropriate memory cells to a high value.

In an example a memory cell can be set to a high or low value by reading bits from an associated register. Accordingly the SMEs can be configured by storing an image created by the compiler into the registers and loading the bits in the registers into associated memory cells . In an example the image created by the compiler includes a binary image of high and low e.g. 1 and 0 bits. The image can configure the FSM lattice to implement a FSM by cascading the SMEs . For example a first SME can be set to an active state by setting the detection cell to the active state. The first SME can be set to output a high signal when the data stream line corresponding to bit 0 has a high signal thereon. The second SME can be initially set to an inactive state but can be set to when active output a high signal when the data stream line corresponding to bit 1 has a high signal thereon. The first SME and the second SME can be cascaded by setting the output of the first SME to couple to the input of the second SME . Thus when a high signal is sensed on the data stream line corresponding to bit 0 the first SME outputs a high signal on the output and sets the detection cell of the second SME to an active state. When a high signal is sensed on the data stream line corresponding to bit 1 the second SME outputs a high signal on the output to activate another SME or for output from the FSM lattice .

In an example a single FSM lattice is implemented on a single physical device however in other examples two or more FSM lattices can be implemented on a single physical device e.g. physical chip . In an example each FSM lattice can include a distinct data input block a distinct output block a distinct programming interface and a distinct set of configurable elements. Moreover each set of configurable elements can react e.g. output a high or low signal to data at their corresponding data input block . For example a first set of configurable elements corresponding to a first FSM lattice can react to the data at a first data input block corresponding to the first FSM lattice . A second set of configurable elements corresponding to a second FSM lattice can react to a second data input block corresponding to the second FSM lattice . Accordingly each FSM lattice includes a set of configurable elements wherein different sets of configurable elements can react to different input data. Similarly each FSM lattice and each corresponding set of configurable elements can provide a distinct output. In some examples an output block from a first FSM lattice can be coupled to an input block of a second FSM lattice such that input data for the second FSM lattice can include the output data from the first FSM lattice in a hierarchical arrangement of a series of FSM lattices .

In an example an image for loading onto the FSM lattice comprises a plurality of bits of data for configuring the configurable elements the configurable switching elements and the special purpose elements within the FSM lattice . In an example the image can be loaded onto the FSM lattice to configure the FSM lattice to provide a desired output based on certain inputs. The output block can provide outputs from the FSM lattice based on the reaction of the configurable elements to data at the data input block . An output from the output block can include a single bit indicating a match of a given pattern a word comprising a plurality of bits indicating matches and non matches to a plurality of patterns and a state vector corresponding to the state of all or certain configurable elements at a given moment. As described a number of FSM lattices may be included in a state machine engine such as state machine engine to perform data analysis such as pattern recognition e.g. speech recognition image recognition etc. signal processing imaging computer vision cryptography and others.

Each of the nodes can be in either an active or an inactive state. When in the inactive state a node does not react e.g. respond to input data. When in an active state a node can react to input data. An upstream node can react to the input data by activating a node that is downstream from the node when the input data matches criteria specified by an edge between the upstream node and the downstream node . For example a first node that specifies the character b will activate a second node connected to the first node by an edge when the first node is active and the character b is received as input data. As used herein upstream refers to a relationship between one or more nodes where a first node that is upstream of one or more other nodes or upstream of itself in the case of a loop or feedback configuration refers to the situation in which the first node can activate the one or more other nodes or can activate itself in the case of a loop . Similarly downstream refers to a relationship where a first node that is downstream of one or more other nodes or downstream of itself in the case of a loop can be activated by the one or more other nodes or can be activated by itself in the case of a loop . Accordingly the terms upstream and downstream are used herein to refer to relationships between one or more nodes but these terms do not preclude the use of loops or other non linear paths among the nodes.

In the diagram the root node can be initially activated and can activate downstream nodes when the input data matches an edge from the root node . Nodes can activate nodes when the input data matches an edge from the node . Nodes throughout the diagram can be activated in this manner as the input data is received. A terminal node corresponds to a match of a sequence of interest in the input data. Accordingly activation of a terminal node indicates that a sequence of interest has been received as the input data. In the context of the FSM lattice implementing a pattern recognition function arriving at a terminal node can indicate that a specific pattern of interest has been detected in the input data.

In an example each root node standard node and terminal node can correspond to a configurable element in the FSM lattice . Each edge can correspond to connections between the configurable elements. Thus a standard node that transitions to e.g. has an edge connecting to another standard node or a terminal node corresponds to a configurable element that transitions to e.g. provides an output to another configurable element. In some examples the root node does not have a corresponding configurable element.

As will be appreciated although the node is described as a root node and nodes are described as terminal nodes there may not necessarily be a particular start or root node and there may not necessarily be a particular end or output node. In other words any node may be a starting point and any node may provide output.

When the FSM lattice is programmed each of the configurable elements can also be in either an active or inactive state. A given configurable element when inactive does not react to the input data at a corresponding data input block . An active configurable element can react to the input data at the data input block and can activate a downstream configurable element when the input data matches the setting of the configurable element. When a configurable element corresponds to a terminal node the configurable element can be coupled to the output block to provide an indication of a match to an external device.

An image loaded onto the FSM lattice via the programming interface can configure the configurable elements and special purpose elements as well as the connections between the configurable elements and special purpose elements such that a desired FSM is implemented through the sequential activation of nodes based on reactions to the data at the data input block . In an example a configurable element remains active for a single data cycle e.g. a single character a set of characters a single clock cycle and then becomes inactive unless re activated by an upstream configurable element.

A terminal node can be considered to store a compressed history of past events. For example the one or more patterns of input data required to reach a terminal node can be represented by the activation of that terminal node . In an example the output provided by a terminal node is binary that is the output indicates whether the pattern of interest has been matched or not. The ratio of terminal nodes to standard nodes in a diagram may be quite small. In other words although there may be a high complexity in the FSM the output of the FSM may be small by comparison.

In an example the output of the FSM lattice can comprise a state vector. The state vector comprises the state e.g. activated or not activated of configurable elements of the FSM lattice . In another example the state vector can include the state of all or a subset of the configurable elements whether or not the configurable elements corresponds to a terminal node . In an example the state vector includes the states for the configurable elements corresponding to terminal nodes . Thus the output can include a collection of the indications provided by all terminal nodes of a diagram . The state vector can be represented as a word where the binary indication provided by each terminal node comprises one bit of the word. This encoding of the terminal nodes can provide an effective indication of the detection state e.g. whether and what sequences of interest have been detected for the FSM lattice .

As mentioned above the FSM lattice can be programmed to implement a pattern recognition function. For example the FSM lattice can be configured to recognize one or more data sequences e.g. signatures patterns in the input data. When a data sequence of interest is recognized by the FSM lattice an indication of that recognition can be provided at the output block . In an example the pattern recognition can recognize a string of symbols e.g. ASCII characters to for example identify malware or other data in network data.

The first FSM lattice A is configured to receive input data for example raw data at a data input block. The first FSM lattice A reacts to the input data as described above and provides an output at an output block. The output from the first FSM lattice A is sent to a data input block of the second FSM lattice B. The second FSM lattice B can then react based on the output provided by the first FSM lattice A and provide a corresponding output signal of the hierarchical structure . This hierarchical coupling of two FSM lattices A and B in series provides a means to provide data regarding past events in a compressed word from a first FSM lattice A to a second FSM lattice B. The data provided can effectively be a summary of complex events e.g. sequences of interest that were recorded by the first FSM lattice A.

The two level hierarchy of FSM lattices A B shown in allows two independent programs to operate based on the same data stream. The two stage hierarchy can be similar to visual recognition in a biological brain which is modeled as different regions. Under this model the regions are effectively different pattern recognition engines each performing a similar computational function pattern matching but using different programs signatures . By connecting multiple FSM lattices A B together increased knowledge about the data stream input may be obtained.

The first level of the hierarchy implemented by the first FSM lattice A can for example perform processing directly on a raw data stream. That is a raw data stream can be received at an input block of the first FSM lattice A and the configurable elements of the first FSM lattice A can react to the raw data stream. The second level implemented by the second FSM lattice B of the hierarchy can process the output from the first level. That is the second FSM lattice B receives the output from an output block of the first FSM lattice A at an input block of the second FSM lattice B and the configurable elements of the second FSM lattice B can react to the output of the first FSM lattice A. Accordingly in this example the second FSM lattice B does not receive the raw data stream as an input but rather receives the indications of patterns of interest that are matched by the raw data stream as determined by the first FSM lattice A. The second FSM lattice B can implement a FSM that recognizes patterns in the output data stream from the first FSM lattice A. It should be appreciated that the second FSM lattice B may receive inputs from multiple other FSM lattices in addition to receiving output from the FSM lattice A. Likewise the second FSM lattice B may receive inputs from other devices. The second FSM lattice B may combine these multiple inputs to produce outputs.

In an example the compiler includes an application programming interface API that allows software developers to create images for implementing FSMs on the FSM lattice . The compiler provides methods to convert an input set of regular expressions in the source code into an image that is configured to configure the FSM lattice . The compiler can be implemented by instructions for a computer having a von Neumann architecture. These instructions can cause a processor on the computer to implement the functions of the compiler . For example the instructions when executed by the processor can cause the processor to perform actions as described in blocks and on source code that is accessible to the processor .

In an example the source code describes search strings for identifying patterns of symbols within a group of symbols. To describe the search strings the source code can include a plurality of regular expressions regexs . A regex can be a string for describing a symbol search pattern. Regexes are widely used in various computer domains such as programming languages text editors network security and others. In an example the regular expressions supported by the compiler include criteria for the analysis of unstructured data. Unstructured data can include data that is free form and has no indexing applied to words within the data. Words can include any combination of bytes printable and non printable within the data. In an example the compiler can support multiple different source code languages for implementing regexes including Perl e.g. Perl compatible regular expressions PCRE PHP Java and .NET languages.

At block the compiler can parse the source code to form an arrangement of relationally connected operators where different types of operators correspond to different functions implemented by the source code e.g. different functions implemented by regexes in the source code . Parsing source code can create a generic representation of the source code. In an example the generic representation comprises an encoded representation of the regexs in the source code in the form of a tree graph known as a syntax tree. The examples described herein refer to the arrangement as a syntax tree also known as an abstract syntax tree in other examples however a concrete syntax tree or other arrangement can be used.

Since as mentioned above the compiler can support multiple languages of source code parsing converts the source code regardless of the language into a non language specific representation e.g. a syntax tree. Thus further processing blocks by the compiler can work from a common input structure regardless of the language of the source code.

As noted above the syntax tree includes a plurality of operators that are relationally connected. A syntax tree can include multiple different types of operators. That is different operators can correspond to different functions implemented by the regexes in the source code.

At block the syntax tree is converted into an automaton. An automaton comprises a software model of a FSM and can accordingly be classified as deterministic or non deterministic. A deterministic automaton has a single path of execution at a given time while a non deterministic automaton has multiple concurrent paths of execution. The automaton comprises a plurality of states. In order to convert the syntax tree into an automaton the operators and relationships between the operators in the syntax tree are converted into states with transitions between the states. In an example the automaton can be converted based partly on the hardware of the FSM lattice .

In an example input symbols for the automaton include the symbols of the alphabet the numerals 0 9 and other printable characters. In an example the input symbols are represented by the byte values 0 through 255 inclusive. In an example an automaton can be represented as a directed graph where the nodes of the graph correspond to the set of states. In an example a transition from state p to state q on an input symbol i.e. p is shown by a directed connection from node p to node q. In an example a reversal of an automaton produces a new automaton where each transition p q on some symbol is reversed q p on the same symbol. In a reversal start state becomes a final state and the final states become start states. In an example the language recognized e.g. matched by an automaton is the set of all possible character strings which when input sequentially into the automaton will reach a final state. Each string in the language recognized by the automaton traces a path from the start state to one or more final states.

At block after the automaton is constructed the automaton is optimized to reduce its complexity and size among other things. The automaton can be optimized by combining redundant states.

At block the optimized automaton is converted into a netlist. Converting the automaton into a netlist maps each state of the automaton to a hardware element e.g. SMEs other elements on the FSM lattice and determines the connections between the hardware elements.

At block the netlist is placed to select a specific hardware element of the target device e.g. SMEs special purpose elements corresponding to each node of the netlist. In an example placing selects each specific hardware element based on general input and output constraints for of the FSM lattice .

At block the placed netlist is routed to determine the settings for the configurable switching elements e.g. inter block switching elements intra block switching elements and intra row switching elements in order to couple the selected hardware elements together to achieve the connections describe by the netlist. In an example the settings for the configurable switching elements are determined by determining specific conductors of the FSM lattice that will be used to connect the selected hardware elements and the settings for the configurable switching elements. Routing can take into account more specific limitations of the connections between the hardware elements that placement at block . Accordingly routing may adjust the location of some of the hardware elements as determined by the global placement in order to make appropriate connections given the actual limitations of the conductors on the FSM lattice .

Once the netlist is placed and routed the placed and routed netlist can be converted into a plurality of bits for configuring a FSM lattice . The plurality of bits are referred to herein as an image e.g. binary image .

At block an image is published by the compiler . The image comprises a plurality of bits for configuring specific hardware elements of the FSM lattice . The bits can be loaded onto the FSM lattice to configure the state of SMEs the special purpose elements and the configurable switching elements such that the programmed FSM lattice implements a FSM having the functionality described by the source code. Placement block and routing block can map specific hardware elements at specific locations in the FSM lattice to specific states in the automaton. Accordingly the bits in the image can configure the specific hardware elements to implement the desired function s . In an example the image can be published by saving the machine code to a computer readable medium. In another example the image can be published by displaying the image on a display device. In still another example the image can be published by sending the image to another device such as a configuring device for loading the image onto the FSM lattice . In yet another example the image can be published by loading the image onto a FSM lattice e.g. the FSM lattice .

In an example an image can be loaded onto the FSM lattice by either directly loading the bit values from the image to the SMEs and other hardware elements or by loading the image into one or more registers and then writing the bit values from the registers to the SMEs and other hardware elements. In an example the hardware elements e.g. SMEs special purpose elements configurable switching elements of the FSM lattice are memory mapped such that a configuring device and or computer can load the image onto the FSM lattice by writing the image to one or more memory addresses.

Method examples described herein can be machine or computer implemented at least in part. Some examples can include a computer readable medium or machine readable medium encoded with instructions operable to configure an electronic device to perform methods as described in the above examples. An implementation of such methods can include code such as microcode assembly language code a higher level language code or the like. Such code can include computer readable instructions for performing various methods. The code may form portions of computer program products. Further the code may be tangibly stored on one or more volatile or non volatile computer readable media during execution or at other times. These computer readable media may include but are not limited to hard disks removable magnetic disks removable optical disks e.g. compact disks and digital video disks magnetic cassettes memory cards or sticks random access memories RAMs read only memories ROMs and the like.

Referring now to an embodiment of the state machine engine e.g. a single device on a single chip is illustrated. As previously described the state machine engine is configured to receive data from a source such as the memory over a data bus. In the illustrated embodiment data may be sent to the state machine engine through a bus interface such as a double data rate three DDR3 bus interface . The DDR3 bus interface may be capable of exchanging e.g. providing and receiving data at a rate greater than or equal to 1 GByte sec. Such a data exchange rate may be greater than a rate that data is analyzed by the state machine engine . As will be appreciated depending on the source of the data to be analyzed the bus interface may be any suitable bus interface for exchanging data to and from a data source to the state machine engine such as a NAND Flash interface peripheral component interconnect PCI interface gigabit media independent interface GMII etc. As previously described the state machine engine includes one or more FSM lattices configured to analyze data. Each FSM lattice may be divided into two half lattices. In the illustrated embodiment each half lattice may include 24K SMEs e.g. SMEs such that the lattice includes 48K SMEs. The lattice may comprise any desirable number of SMEs arranged as previously described with regard to . Further while only one FSM lattice is illustrated the state machine engine may include multiple FSM lattices as previously described.

Data to be analyzed may be received at the bus interface and provided to the FSM lattice through a number of buffers and buffer interfaces. In the illustrated embodiment the data path includes data buffers an instruction buffer process buffers and an inter rank IR bus and process buffer interface . The data buffers are configured to receive and temporarily store data to be analyzed. In one embodiment there are two data buffers data buffer A and data buffer B . Data may be stored in one of the two data buffers while data is being emptied from the other data buffer for analysis by the FSM lattice . The bus interface may be configured to provide data to be analyzed to the data buffers until the data buffers are full. After the data buffers are full the bus interface may be configured to be free to be used for other purposes e.g. to provide other data from a data stream until the data buffers are available to receive additional data to be analyzed . In the illustrated embodiment the data buffers may be 32 KBytes each while in other embodiments the data buffers may be any suitable size e.g. 4 KBytes 8 KBytes 16 KBytes 64 KBytes etc. . The instruction buffer is configured to receive instructions from the processor via the bus interface such as instructions that correspond to the data to be analyzed and instructions that correspond to configuring the state machine engine . The IR bus and process buffer interface may facilitate providing data to the process buffer . The IR bus and process buffer interface can be used to ensure that data is processed by the FSM lattice in order. The IR bus and process buffer interface may coordinate the exchange of data timing data packing instructions etc. such that data is received and analyzed correctly. Generally the IR bus and process buffer interface allows the use of multiple devices in a rank of devices. The multiple devices in the rank of devices share data such that all of the multiple devices receive all of the shared data in the correct order. For example multiple physical devices e.g. state machine engines chips separate devices may be arranged in a rank and may provide data to each other via the IR bus and process buffer interface . For purposes of this application the term rank refers to a set of state machine engines connected to the same chip select. In the illustrated embodiment the IR bus and process buffer interface may include an 8 bit data bus.

In the illustrated embodiment the state machine engine also includes a de compressor and a compressor to aid in providing data to and from the state machine engine . As may be appreciated the compressor and de compressor may use the same compression algorithms to simplify software and or hardware designs however the compressor and the de compressor may also use different algorithms. By compressing the data the bus interface e.g. DDR3 bus interface utilization time may be minimized. In the present embodiment the compressor may be used to compress state vector data configuration data e.g. programming data and match results data obtained after analysis by the FSM lattice . In one embodiment the compressor and de compressor may be disabled e.g. turned off such that data flowing to and or from the compressor and de compressor is not modified e.g. neither compressed nor de compressed .

The compressor and de compressor can also be configured to handle multiple sets of data and each set of data may be of varying lengths. By padding compressed data and including an indicator as to when each compressed region ends the compressor may improve the overall processing speed through the state machine engine .

The state machine engine includes a state vector system having a state vector cache memory a state vector memory buffer a state vector intermediate input buffer and a state vector intermediate output buffer . The state vector system may be used to store multiple state vectors of the FSM lattice to move state vectors onto or off of the state machine engine and to provide a state vector to the FSM lattice to restore the FSM lattice to a state corresponding to the provided state vector. For example each state vector may be temporarily stored in the state vector cache memory . That is the state of each SME may be stored such that the state may be restored and used in further analysis at a later time while freeing the SMEs for analysis of a new data set e.g. search term . Like a typical cache the state vector cache memory allows storage of state vectors for quick retrieval and use here by the FSM lattice for instance. In the illustrated embodiment the state vector cache memory may store up to 512 state vectors. Each state vector comprises the state e.g. activated or not activated of the SMEs of the FSM lattice and the dynamic e.g. current count of the counters .

As will be appreciated the state vector data may be exchanged between different state machine engines e.g. chips in a rank. The state vector data may be exchanged between the different state machine engines for various purposes such as to synchronize the state of the SMEs of the FSM lattices and the dynamic count of the counters to perform the same functions across multiple state machine engines to reproduce results across multiple state machine engines to cascade results across multiple state machine engines to store a history of states of the SMEs and the dynamic count of the counters used to analyze data that is cascaded through multiple state machine engines and so forth. Furthermore it should be noted that within a state machine engine the state vector data may be used to quickly restore the state vector. For example the state vector data may be used to restore the state of the SMEs and the dynamic count of the counters to an initialized state e.g. to search for a new search term to restore the state of the SMEs and the dynamic count of the counters to prior state e.g. to search for a previously searched search term and to change the state of the SMEs and the dynamic count of the counters to be configured for a cascading configuration e.g. to search for a search term in a cascading search . In certain embodiments the state vector data may be provided to the bus interface so that the state vector data may be provided to the processor e.g. for analysis of the state vector data reconfiguring the state vector data to apply modifications reconfiguring the state vector data to improve efficiency and so forth .

For example in certain embodiments the state machine engine may provide cached state vector data e.g. data stored by the state vector system from the FSM lattice to an external device. The external device may receive the state vector data modify the state vector data and provide the modified state vector data to the state machine engine for restoring the FSM lattice e.g. resetting initializing . Accordingly the external device may modify the state vector data so that the state machine engine may skip states e.g. jump around as desired.

The state vector cache memory may receive state vector data from any suitable device. For example the state vector cache memory may receive a state vector from the FSM lattice another FSM lattice e.g. via the IR bus and process buffer interface the de compressor and so forth. In the illustrated embodiment the state vector cache memory may receive state vectors from other devices via the state vector memory buffer . Furthermore the state vector cache memory may provide state vector data to any suitable device. For example the state vector cache memory may provide state vector data to the state vector memory buffer the state vector intermediate input buffer and the state vector intermediate output buffer .

Additional buffers such as the state vector memory buffer state vector intermediate input buffer and state vector intermediate output buffer may be utilized in conjunction with the state vector cache memory to accommodate rapid retrieval and storage of state vectors while processing separate data sets with interleaved packets through the state machine engine . In the illustrated embodiment each of the state vector memory buffer the state vector intermediate input buffer and the state vector intermediate output buffer may be configured to temporarily store one state vector. The state vector memory buffer may be used to receive state vector data from any suitable device and to provide state vector data to any suitable device. For example the state vector memory buffer may be used to receive a state vector from the FSM lattice another FSM lattice e.g. via the IR bus and process buffer interface the de compressor and the state vector cache memory . As another example the state vector memory buffer may be used to provide state vector data to the IR bus and process buffer interface e.g. for other FSM lattices the compressor and the state vector cache memory .

Likewise the state vector intermediate input buffer may be used to receive state vector data from any suitable device and to provide state vector data to any suitable device. For example the state vector intermediate input buffer may be used to receive a state vector from an FSM lattice e.g. via the IR bus and process buffer interface the de compressor and the state vector cache memory . As another example the state vector intermediate input buffer may be used to provide a state vector to the FSM lattice . Furthermore the state vector intermediate output buffer may be used to receive a state vector from any suitable device and to provide a state vector to any suitable device. For example the state vector intermediate output buffer may be used to receive a state vector from the FSM lattice and the state vector cache memory . As another example the state vector intermediate output buffer may be used to provide a state vector to an FSM lattice e.g. via the IR bus and process buffer interface and the compressor .

Once a result of interest is produced by the FSM lattice match results may be stored in a match results memory . For example a match vector indicating a match e.g. detection of a pattern of interest may be stored in the match results memory . The match result can then be sent to a match buffer for transmission over the bus interface to the processor for example. As previously described the match results may be compressed.

Additional registers and buffers may be provided in the state machine engine as well. For instance the state machine engine may include control and status registers . In addition restore and program buffers may be provided for use in configuring the SMEs of the FSM lattice initially or restoring the state of the SMEs in the FSM lattice during analysis. Similarly save and repair map buffers may also be provided for storage of save and repair maps for setup and usage.

As previously described data to be analyzed is received at the bus interface . The bus interface directs the data to a data buffer system of each state machine engine e.g. F0 F1 F2 F3 F4 F5 F6 F7 which includes the data buffers and the instruction buffer . The data buffers are configured to receive and temporarily store data to be analyzed. In the illustrated embodiment there are two data buffers e.g. data buffer A and data buffer B in each state machine engine . Data may be stored in one of the two data buffers while data is being emptied from the other data buffer for analysis by an FSM lattice . As previously discussed the instruction buffer is configured to receive instructions from the processor via the bus interface such as instructions that correspond to the data to be analyzed. From the data buffer system data to be analyzed and instructions that correspond to the data are provided to one or more of the FSM lattices via the IR bus and process buffer interface . In the present embodiment the physical FSM lattices are arranged into logical groups. Specifically the FSM lattices of the state machine engines F0 and F1 are arranged into a logical group A the FSM lattices of the state machine engines F2 and F3 are arranged into a logical group B the FSM lattices of the state machine engines F4 and F5 are arranged into a logical group C and the FSM lattices of the state machine engines F6 and F7 are arranged into a logical group D . In other embodiments the physical FSM lattices may be arranged into any suitable number of logical groups e.g. 1 2 3 4 5 6 7 8 . Furthermore as will be appreciated data may be exchanged between the state machine engines via the IR bus and process buffer interface . For example the IR bus and process buffer interface may be used to exchange data between any of the state machine engines e.g. F0 F1 F2 F3 F4 F5 F6 F7 . Although eight state machine engines are illustrated the rank of devices may have any suitable number of state machine engines e.g. 1 2 4 8 and so forth . As will be appreciated the IR bus and process buffer interface of each state machine engine may include inputs for receiving data e.g. from its own data buffer system and from the IR bus and process buffer interface of other state machine engines . Likewise the IR bus and process buffer interface of each state machine engine may include outputs for sending data e.g. to the FSM lattices and to the IR bus and process buffer interfaces of other state machine engines .

The bus interface may receive data to be analyzed in a format that is tailored for efficient use of the data. Specifically illustrate examples of how data may be assigned e.g. grouped by the processor into data blocks that are provided to the state machine engines via the bus interface .

Referring now to an example of data segments e.g. data sets search terms assigned by the processor into data blocks to be provided to the state machine engines is illustrated. In the present embodiment multiple data segments are assigned into a single data block. Each data block is assigned to be analyzed by a single logical group of FSM lattices e.g. on one or more state machine engines in a rank of state machine engines . For example a data stream e.g. a large amount of data to be sent by the processor to the state machine engines is assigned by the processor into a first data block that corresponds to data intended for the logical group A a second data block that corresponds to data intended for the logical group B a third data block that corresponds to data intended for the logical group C and a fourth data block that corresponds to data intended for the logical group D . Specifically the data stream is assembled by the processor from data segments and . As will be appreciated each of the data segments and may represent a data set to be analyzed by an FSM lattice . As will be appreciated the processor may assign data segments and to the data blocks and for any suitable reason. For example the processor may assign data segments to certain data blocks based on a length of each data set and or an order that data sets are to be analyzed in order to process the data sets efficiently.

The data segments and may be assigned into the data blocks and using any suitable manner. For example the data segments and may be assigned into data blocks and such that a number of bytes in the data blocks and is minimized. As another example the data segments and may be assigned into data blocks and such that certain data segments are grouped together.

As illustrated the first data block includes the data segment A the data segment F and the data segment I . The second data block includes the data segment B and the data segment K . Furthermore the third data block includes the data segment C the data segment E and the data segment G . The fourth data block includes the data segment D the data segment H and the data segment J .

As will be appreciated to process the data blocks efficiently the data blocks may all have an equal amount of data. Furthermore the data segments within the data blocks may start and or stop at predetermined intervals e.g. bytes words within the data blocks so that processing devices can determine when data segments start and stop. However the data segments may not have the correct amount of data to start and or stop at the predetermined intervals. Accordingly data padding may be inserted between certain data segments so that data starts and or stops within the data blocks at the predetermined intervals. In addition data padding may be added to the end of a data block so that all data blocks have an equal amount of data.

Referring now to an example of data padding inserted between the data segments of the data blocks and of is illustrated. For example in the first data block data padding may be inserted between the data segment A and the data segment F . Further data padding may be inserted between the data segment F and the data segment I . As another example in the second data block data padding may be inserted between the data segment B and the data segment K . In the third data block data padding may be inserted between the data segment C and the data segment E . Likewise data padding may be inserted between the data segment E and the data segment G . As another example in the fourth data block data padding may be inserted between the data segment D and the data segment H . In addition data padding may be inserted between the data segment H and the data segment J .

The data padding and may include any suitable number of bytes of data that are not to be analyzed e.g. invalid data junk data filler data garbage data etc. . In one embodiment the number of bytes used as data padding may be a number of bytes that when added to a number of bytes of the prior data segment reach a whole word boundary i.e. a number of bytes of the prior data segment plus the number of bytes used as data padding is equally divisible by the whole word boundary . For example a number of bytes of the data padding may be such that the combined number of bytes of the data padding and the data segment A i.e. the prior data segment is equally divisible e.g. no remainder by the whole word boundary. In the illustrated embodiment the whole word boundary may be eight bytes. In other embodiments the whole word boundary may be any suitable number of bytes or bits. As such in the illustrated embodiment if the data segment A were to include 63 bytes of data the data padding would include one byte of data e.g. to make 64 combined bytes of data between the data segment A and the data padding with 64 being equally divisible by eight bytes . As another example if the data segment A included 60 bytes of data e.g. which is not equally divisible by eight the data padding would include four bytes of data. As a further example if the data segment A included 64 bytes of data the data padding would include zero bytes of data or in other words the data padding would not be needed between the data segment A and the data segment F . As will be appreciated each data padding and may operate in a similar manner.

Referring now to an example of data padding inserted after data segments of the data blocks and of is illustrated. Specifically data padding may be inserted at the end of each data block and as needed to make the number of bytes in each data blocks and equal. Furthermore the data padding at the end of each data block and may be used so that each data block and reaches a whole word boundary as previously described. In the illustrated embodiment data padding is inserted after the data segment I data padding is inserted after the data segment G and data padding is inserted after the data segment J . Accordingly each of the data blocks and includes an equal number of bytes and each of the data blocks and reaches a whole word boundary.

It may be difficult for FSM lattices to distinguish data padding from valid data. Accordingly instructions may accompany the data blocks and so that data padding may be identified and disregarded by the FSM lattices during analysis of the valid data. Such instructions may be sent to the state machine engine by the processor via the bus interface and may be received stored and provided by the instruction buffer of the state machine engine . To produce the instructions the processor may logically divide the data stream into regions and . The end boundaries of the regions and may be formed such that each region ends when any data padding ends. For example the first region ends when the data padding ends. As another example the fifth region ends when the data padding ends.

The instructions that accompany the data blocks and may include a number of total bytes for each region and and a number of valid bytes e.g. the number of bytes excluding byte padding for each data block and within each region. For example the instructions may include a number of bytes that corresponds to the first region a number of bytes that corresponds to the valid bytes for the first data block within the first region a number of bytes that corresponds to the valid bytes for the second data block within the first region a number of bytes that corresponds to the valid bytes for the third data block within the first region and a number of bytes that corresponds to the valid bytes for the fourth data block within the first region . Note that in this example the number of bytes represented by and are equal because there is no padding following data segments A1 B1 and D1.

Likewise the instructions may include numbers of bytes and that correspond to the second region numbers of bytes and that correspond to the third region numbers of bytes and that correspond to the fourth region numbers of bytes and that correspond to the fifth region numbers of bytes and that correspond to the sixth region numbers of bytes and that correspond to the seventh region and numbers of bytes and that correspond to the eighth region . It should be noted that the instructions may include a number of valid bytes for each data segment in each region . Therefore for the seventh region the logical group A may include a number of valid bytes of zero. Furthermore for the eighth region the logical group A may include a number of valid bytes of zero. Accordingly using the instructions the FSM lattices may identify the data padding inserted with the data segments. Although one specific type of instructions has been presented herein it should be noted that the instructions included with the group of data blocks and may be any suitable group of instructions that allow the FSM lattices to distinguish valid data from data padding i.e. invalid data .

Referring now to an example of the data blocks and of organized by the processor for transmission to data buffer systems of the state machine engines is illustrated. Each of the data blocks and are arranged with rows of data having a number of bytes equivalent to a whole word length. In the illustrated embodiment the whole word length is eight bytes represented by a byte for each of state machine engines e.g. F0 F1 F2 F3 F4 F5 F6 and F7 . The first byte from each of the data segments begins at the right side of each data block and and increase toward the left side of each data block such that the first byte for the data segment A is in column F0 and the eighth byte for the data segment A is in column F7. As will be appreciated the column F0 represents data that will be initially stored in the data buffers of the F0 state machine engine the column F1 represents data that will be initially stored in the data buffers of the F1 state machine engine and so forth. Furthermore the data segments are placed in rows from top to bottom. As illustrated each combination of a data segment and data padding ends in column F7 i.e. they each extend for a whole word length . Furthermore each data block and is equal in size. As will be appreciated during operation the data blocks and may be provided from the processor to the state machine engines sequentially.

The data of a given block and is provided to and stored across the data buffer systems of all of the state machine engines in a rank of devices such that the data intended for the corresponding logical group or respectively is spread across the data buffer systems of the state machine engines of the rank. The data may be received and stored in this manner to enable data to be quickly provided over the bus interface to the data buffer systems . In certain embodiments the data buffers of the data buffer systems may be configured to latch data from the bus interface e.g. at predetermined intervals . In other embodiments the data buffers of the data buffer systems may receive a limited portion of data based on the connection between the data buffers and the bus interface . As explained in detail below the data stored in the data buffer systems is sorted out when the data is provided from the data buffer systems to the process buffers via the IR bus and process buffer interface .

The pins of the module are coupled to the DDR3 bus interface to facilitate data transfer between the processor and the module . Moreover the pins of the module are coupled to pins of the chips e.g. F0 F1 F2 F3 F4 F5 F6 and F7 having the state machine engines using routing lines of the printed circuit board. The DDR3 bus interface includes a DDR3 address bus to enable the processor to select a memory address of the data buffer system of each state machine engine . As illustrated the DDR3 address bus includes 16 address lines however in other embodiments the DDR3 address bus may include fewer or more than 16 address lines. Each address line of the DDR3 address bus is coupled to the data buffer system of each state machine engine . Accordingly the processor may select an address of the data buffer system of each state machine engine together.

As may be appreciated the DDR3 bus interface between the state machine engines and the processor may enable the state machine engines to receive data from the processor using all of the data lines from the processor . For example if the processor has 64 data lines and each state machine engine has eight data lines eight state machine engines may be coupled to the 64 data lines to receive data from all of the data lines of the processor . Accordingly the processor may provide data to the state machine engines quickly and using a standard interface. Moreover the state machine engines may be configured to coordinate together such that all of the data received collectively by the state machine engines is provided to all of the state machine engines in an orderly manner. In the illustrated embodiment the DDR3 bus interface includes a data bus having sets of data lines and coupled to each state machine engine . As illustrated separate data lines are coupled to each state machine engine . For example in certain embodiments the data bus includes 64 data lines and the module includes eight state machine engines . In such an embodiment eight data lines may be coupled and or solely dedicated to each of the eight state machine engines . Accordingly using the data bus and the DDR3 address bus a respective byte of data may be synchronously provided to a selected address of the data buffer system of each state machine engine . In certain embodiments fewer or more than eight data lines may couple the state machine engines to the data bus . Furthermore the DDR3 bus interface may include fewer or more than 64 data lines. The following table TABLE 1 illustrates various examples of configurations of DDR3 bus interfaces having a specified number of data lines and a number of data lines of the DDR3 bus interfaces that are coupled to the state machine engines .

As illustrated in TABLE 1 if the data bus includes eight data lines the eight data lines may be coupled to one state machine engine having an input data bus width of eight lines. As another example if the data bus includes 16 data lines the 16 data lines may be coupled to two state machine engines having an input data bus width of eight lines or one state machine engine having an input data bus width of 16 lines. As a further example if the data bus includes 32 data lines the 32 data lines may be coupled to four state machine engines having an input data bus width of eight lines or two state machine engine having an input data bus width of 16 lines. Moreover if the data bus includes 64 data lines the 64 data lines may be coupled to eight state machine engines having an input data bus width of eight lines or four state machine engine having an input data bus width of 16 lines.

The DDR3 bus interface includes other control lines and that facilitate data transfer between the processor and the module . The other control lines and may enable individual communication between the processor and a selected state machine engine and or may enable communication between the processor and the state machine engines collectively.

During operation the processor may provide data to the DDR3 bus interface . For example the processor may provide 64 bits of data at a time using a data bus having 64 data lines. Furthermore the state machine engines may each receive eight bits of data from eight data lines coupled to each state machine engine . Accordingly the processor may provide 64 bits of data at a time to the module . As discussed previously the state machine engines may be configured to receive data to be analyzed by other different state machine engines . As such the processor may provide data to the module in data blocks with each data block intended to be processed by one or more of the state machine engines of the module . In other words the processor may not sort and or pack the data blocks it provides. For example the processor may provide a number of consecutive bytes of data intended to be analyzed by the state machine engine F0 even though portions of the number of consecutive bytes of data will be received and stored by each of the state machine engines F0 F1 F2 F3 F4 F5 F6 and F7 . Thus the processor may provide the data to the module in a simplified and efficient manner and may enable the state machine engines to sort the data to be processed by selected state machine engines .

The processor may also provide instructions to the state machine engines to instruct the state machine engines that are to analyze each data block provided from the processor may provide instruction to the state machine engines to analyze data for a period of time e.g. predetermined period of time and may provide instructions to the state machine engines to instruct the state machine engines concerning a length of each data block provided from the processor . In certain embodiments the data buffer system may include certain memory locations to store instructions received from the processor . Accordingly the processor may select using the DDR3 address bus a predetermined address of the data buffer system that is dedicated to receiving instructions. The processor may then provide instructions to each of the state machine engines using the data bus and the DDR3 address bus .

The IR bus and process buffer interface is part of the module and includes connections e.g. electrical optical or another operable connection that interconnect the state machine engines . As illustrated the IR bus and process buffer interface includes an IR data bus and other control lines that may be part of an instruction bus. In the illustrated embodiment the IR data bus includes eight data lines that couple each of the state machine engines to one another. Specifically the IR data bus couples together the data buffer system of each state machine engine and the process buffers of each state machine engine . Moreover the IR data bus may be used to provide data received from the processor to other state machine engines for analysis of the data. The other control lines may be used to synchronize and or control data transfer between the state machine engines .

Each chip having a state machine engine may be grouped into a logical group to process data received from the processor . As may be appreciated the state machine engines of the module may include one or more logical groups. In the module there may be one logical group that includes all of the state machine engines F0 F1 F2 F3 F4 F5 F6 and F7 . Furthermore there may be two three four five six seven or eight logical groups. The logical groups may have any number of state machine engines and the logical groups do not have to be the same size. For example in one embodiment a first logical group may include state machine engines F0 and F1 and a second logical group may include state machine engines F2 F3 F4 F5 F6 and F7. By using logical groups that may include more than one state machine engine a single automata processor block that is too large to be programmed into a single state machine engine may be analyzed by the logical group. Furthermore logical groups enable multiple data blocks to be analyzed concurrently by distinct logical groups. Moreover logical groups enable data to be analyzed at high throughput speeds such as by using logical groups having the same size and or by parallel processing. Thus the architecture of the module provides flexibility and may enable a single state machine engine of the module to process data at rates up to 1.0 Gbps or more. Furthermore the architecture of the module may enable the module to process data in parallel in eight logical groups such as by using eight state machine engines thereby achieving data processing rates up to eight times the rate of a single state machine engine e.g. 8.0 Gbps or more .

Even though each state machine engine may be part of a logical group the state machine engines may not receive any indication that they are part of a particular logical group and or any indication about a number of state machine engines that are part of its logical group. However certain information may be provided to the state machine engines using instructions and or the other control lines and . For example the state machine engines may receive and or store an indication about a total bus width of the DDR3 bus interface a number of data lines coupled to the DDR3 bus interface a number of data lines of the IR data bus a device position on the DDR3 bus interface a device position on the IR data bus whether the state machine engine is a master device whether the state machine engine is a slave device a number of IR data bus cycles to perform a number of bytes that will be received and or a number of bytes to be analyzed e.g. valid data .

For example during initialization of the module the processor may provide data to each state machine engine to assign a number to each state machine engine e.g. 0 1 2 3 4 5 6 7 . In certain embodiments the state machine engine that receives a 0 may be state machine engine F0 and may be the master device and all other devices may be slave devices. In other embodiments the master device may be assigned any suitable value. In certain embodiments the master device may be configured to coordinate synchronization of the state machine engines . The processor may also provide data to each state machine engine to indicate a total number of state machine engines that are part of the module a logical group to which the state machine engine belongs and or a number of logical groups that are part of the module .

In one embodiment with one logical group the IR bus and process buffer interface enables each state machine engine in the rank e.g. F0 F1 F2 F3 F4 F5 F6 and F7 to analyze all of the bytes of data provided by the processor . Accordingly in such an embodiment the entire data stream may be processed in each of the state machine engines . In another embodiment having multiple logical groups the IR bus and process buffer interface enables each state machine engine in the rank to receive an assigned portion of the data stream in a timely manner by implementing a data slicing scheme. For example the processor may provide data intended for each logical group sequentially but the state machine engines may store the data in an offset manner in the data buffer system of each state machine engine so that the data may be provided to the IR data bus to efficiently provide the data to intended state machine engines .

In addition each of the state machine engines is configured to receive an input from and or to control a signal of the IRDV . For example in one embodiment the master device may be configured to control a signal of the IRDV while the slave devices may be configured to receive an input from the IRDV . In the illustrated embodiment the state machine engine F0 is configured to control a signal of the IRDV while the state machine engines F1 F2 and F3 are configured to receive an input from the IRDV . The state machine engine F0 may be configured to control the signal to a first indication e.g. logic high indicating that data provided to the state machine engines F1 F2 and F3 is valid. Moreover the state machine engine F0 may be configured to control the signal to a second indication e.g. logic low indicating that data provided to the state machine engines F1 F2 and F3 is not valid. Each of the state machine engines is configured to receive an input from and to provide an output to the IRDQS and the IRDQS . Moreover each of the state machine engines is configured to output data to the IRDQS and or the IRDQS while providing data to the IR data bus .

One embodiment of signals exchanged during such an operation is illustrated in . For example at a time the RSYNC transitions from a logic low to a logic high. Such a transition indicates that all of the state machine engines F0 F1 F2 and F3 have received an instruction e.g. from the processor that initiates synchronization of the state machine engines that all of the state machine engines F0 F1 F2 and F3 have paused operation and that all of the state machine engines F0 F1 F2 and F3 have stopped driving the signal of the RSYNC to a logic low. In this example the state machine engines F0 F1 F2 and F3 have encountered a data buffer processing instruction e.g. I DA.PROCESS I DA.PROCESS EoP I DA.PROCESS EoD I DB.PROCESS I DB.PROCESS EoP and I DB.PROCESS EoD . At a time the RSYNC transitions from a logic high to a logic low. Such a transition indicates that at least one of the state machine engines F0 F1 F2 and F3 has completed the data buffer processing instruction and that at least one of the state machine engines F0 F1 F2 and F3 has pulled the RSYNC to a logic low.

At a time the IRDV transitions from a logic low to a logic high. Such a transition is an indication from the master state machine engine F0 to all of the state machine engines F0 F1 F2 and F3 that valid data will begin to be provided to the IR data bus starting with the master device and continuing through each state machine engine in the rank in order e.g. F0 F1 F2 F3 . At a time the IRDV transitions from a logic high to a logic low. Such a transition is an indication from the master state machine engine F0 to all of the state machine engines F0 F1 F2 and F3 that valid data is no longer being provided to the IR data bus .

At a time the master state machine engine F0 provides data e.g. a first byte of data intended for logical group 0 onto the IR data bus and outputs a logic high onto the IRDQS and or the IRDQS . Then at a time the master state machine engine F0 provides data e.g. a second byte of data intended for logical group 1 onto the IR data bus and outputs a logic high onto the IRDQS and or the IRDQS . Moreover at a time the state machine engine F1 provides data e.g. a first byte of data intended for logical group 0 onto the IR data bus and outputs a logic high onto the IRDQS and or the IRDQS . Then at a time the state machine engine F1 provides data e.g. a second byte of data intended for logical group 1 onto the IR data bus and outputs a logic high onto the IRDQS and or the IRDQS .

Furthermore at a time the state machine engine F2 provides data e.g. a first byte of data intended for logical group 0 onto the IR data bus and outputs a logic high onto the IRDQS and or the IRDQS . Then at a time the state machine engine F2 provides data e.g. a second byte of data intended for logical group 1 onto the IR data bus and outputs a logic high onto the IRDQS and or the IRDQS . In addition at a time the state machine engine F3 provides data e.g. a first byte of data intended for logical group 0 onto the IR data bus and outputs a logic high onto the IRDQS and or the IRDQS . Then at a time the state machine engine F3 provides data e.g. a second byte of data intended for logical group 1 onto the IR data bus and outputs a logic high onto the IRDQS and or the IRDQS .

Accordingly each of the state machine engines provides data having a burst length of two bytes of data e.g. each state machine engine outputs two bytes of data before the next state machine engine in the rank provides data . In the illustrated embodiment the first byte of data is intended for the first logical group logical group 0 and the second byte of data is intended for the second logical group logical group 1 . As may be appreciated the burst length may vary in other embodiments. For example the burst length may be based on a number of logical groups equal to the number of logical groups and so forth. As data is provided to the IR data bus the state machine engines receive the data and store data that is intended for that state machine engine . For example because the state machine engines F0 and F1 are part of the logical group 0 the state machine engines F0 and F1 store every first byte provided by each of the state machine engines . Furthermore because the state machine engines F2 and F3 are part of the logical group 1 the state machine engines F2 and F3 store every second byte provided by each of the state machine engines . In certain embodiments each of the state machine engines is configured to store data based on the falling edge of the IRDQS and or the IRDQS .

In the illustrated embodiment each of the state machine engines provided only one data burst. However as may be appreciated the rotating cycle of each state machine engine providing data bursts may repeat any number of times before the RSYNC transitions from a logic high to a logic low at the time . In certain embodiments the cycle of each state machine engine providing data bursts may be repeated based on instructions provided to the state machine engines from the processor . For example the processor may provide an instruction to each of the state machine engines indicating a number of bytes that the respective state machine engine is to store CNTC from the IR data bus . Accordingly the cycle of each state machine engine providing data bursts may be repeated a number of times equal to the CNTC number. In certain embodiments the CNTC number may be equal to a length of the longest data group to be provided to a logical group thereby enabling sufficient cycles for each logical group to receive its intended data. Moreover the processor may provide an instruction to each of the state machine engines indicating a number of bytes that the respective state machine engine is to analyze CNTV . In certain embodiments if the CNTC number is greater than the CNTV number the state machine engine may consider the bytes received after the CNTV number up to the CNTC number as invalid data e.g. junk data garbage data etc. .

At a time the RSYNC transitions from a logic low to a logic high. Such a transition indicates that all of the state machine engines F0 F1 F2 and F3 have received an instruction e.g. from the processor that initiates synchronization of the state machine engines that all of the state machine engines F0 F1 F2 and F3 have paused operation and that all of the state machine engines F0 F1 F2 and F3 have stopped driving the signal of the RSYNC to a logic low. In this example the state machine engines F0 F1 F2 and F3 have encountered an M BAR instruction. The M BAR instruction is used to synchronize the state machine engines before additional instructions are executed by the state machine engines . At a time the RSYNC transitions from a logic high to a logic low. Such a transition indicates that at least one of the state machine engines F0 F1 F2 and F3 has pulled the RSYNC to a logic low and that the state machine engines are synchronized and may proceed with executing additional instructions.

At a time the RSYNC transitions from a logic low to a logic high. In this example the state machine engines F0 F1 F2 and F3 have encountered another data buffer processing instruction e.g. I DA.PROCESS I DA.PROCESS EoP I DA.PROCESS EoD I DB.PROCESS I DB.PROCESS EoP and I DB.PROCESS EoD . Accordingly at a time the IRDV transitions from a logic low to a logic high. Such a transition is an indication from the master state machine engine F0 to all of the state machine engines F0 F1 F2 and F3 that valid data will begin to be provided to the IR data bus starting with the master device and continuing through each state machine engine in the rank in order e.g. F0 F1 F2 F3 . At a time the master state machine engine F0 provides data e.g. a first byte of data intended for logical group 0 onto the IR data bus and outputs a logic high onto the IRDQS and or the IRDQS . Then at a time the master state machine engine F0 provides data e.g. a second byte of data intended for logical group 1 onto the IR data bus and outputs a logic high onto the IRDQS and or the IRDQS . Other state machine engines may thereafter provide data onto the IR data bus as described above in relation to times through .

During operation of the state machine engines data may be provided to the IR data bus in a synchronized manner using the RSYNC and the IRDV with all of the state machine engines being synchronized together. Moreover each of the state machine engines stores instructions indicating its position in the rank of devices that make up the module . As such each of the state machine engines is able to time data storage and data output to facilitate orderly data transfer using the IR data bus so that data is provided and stored at the right time. As may be appreciated errors may occur during the data transfer process. Accordingly the master state machine engine F0 may transition the IRDV to a logic low at any time to stop e.g. block stall delay etc. data transfer on the IR data bus until the error is resolved. Furthermore any of the state machine engines F1 F2 and F3 may direct the RSYNC to a logic low thereby providing an indication to the master state machine engine F0 that data transfer on the IR data bus should be stopped.

The table includes a write count column indicating a number of writes from the processor to the 64 data lines and byte number columns indicating individual byte numbers in numerical order based on when the bytes are received from the processor . The byte number columns include columns F0 F1 F2 F3 F4 F5 F6 and F7 indicating specific bytes that are stored in the data buffer system of a respective state machine engine F0 F1 F2 F3 F4 F5 F6 and F7 . The table also includes a data buffer address column indicating an address within each of the data buffers A and B of the data buffer system where the bytes indicated in the byte number columns are stored and a target column indicating data to be processed by a specific logical group of the state machine engines . For example during a first write from the processor to the 64 data lines e.g. 0 in the write count column the processor provides to the DDR3 address bus an address selection of a first address of the data buffer system e.g. processor write address 0 and the corresponding eight data bytes. This may result in a first data byte e.g. 0 being stored in a first state machine engine e.g. F0 at a first data buffer address e.g. 0 of the first state machine engine e.g. F0 a second data byte e.g. 1 being stored in a second state machine engine e.g. F1 at the same first data buffer address e.g. 0 of the second state machine engine e.g. F1 a third data byte e.g. 2 being stored in a third state machine engine e.g. F2 at the same first data buffer address e.g. 0 of the third state machine engine e.g. F2 a fourth data byte e.g. 3 being stored in a fourth state machine engine e.g. F3 at the same first data buffer address e.g. 0 of the fourth state machine engine e.g. F3 a fifth data byte e.g. 4 being stored in a fifth state machine engine e.g. F4 at the same first data buffer address e.g. 0 of the fifth state machine engine e.g. F4 a sixth data byte e.g. 5 being stored in a sixth state machine engine e.g. F5 at the same first data buffer address e.g. 0 of the sixth state machine engine e.g. F5 a seventh data byte e.g. 6 being stored in a seventh state machine engine e.g. F6 at the same first data buffer address e.g. 0 of the seventh state machine engine e.g. F6 and an eighth data byte e.g. 7 being stored in an eighth state machine engine e.g. F7 at the same first data buffer address e.g. 0 of the eighth state machine engine e.g. F7 .

As another example during a second write from the processor to the 64 data lines e.g. 1 in the write count column the processor provides to the DDR3 address bus an address selection of a second address of the data buffer system e.g. processor write address 1 and the corresponding eight data bytes. This may result in a first data byte e.g. 8 being stored in the first state machine engine e.g. F0 at a second data buffer address e.g. 8 of the first state machine engine e.g. F0 a second data byte e.g. 9 being stored in the second state machine engine e.g. F1 at the same second data buffer address e.g. 8 of the second state machine engine F1 a third data byte e.g. 10 being stored in the third state machine engine e.g. F2 at the same second data buffer address e.g. 8 of the third state machine engine e.g. F2 a fourth data byte e.g. 11 being stored in the fourth state machine engine e.g. F3 at the same second data buffer address e.g. 8 of the fourth state machine engine e.g. F3 a fifth data byte e.g. 12 being stored in the fifth state machine engine e.g. F4 at the same second data buffer address e.g. 8 of the fifth state machine engine e.g. F4 a sixth data byte e.g. 13 being stored in the sixth state machine engine e.g. F5 at the same second data buffer address e.g. 8 of the sixth state machine engine e.g. F5 a seventh data byte e.g. 14 being stored in the seventh state machine engine e.g. F6 at the same second data buffer address e.g. 8 of the seventh state machine engine e.g. F6 and an eighth data byte e.g. 15 being stored in the eighth state machine engine e.g. F7 at the same second data buffer address e.g. 8 of the eighth state machine engine e.g. F7 . As illustrated in the two examples the data buffer address changed by eight e.g. a number equal to the number of logical groups in the rank of module between the first write from the processor to the second write from the processor . Although the processor continues writing a lineal address block on successive processor writes to the data buffer system the data buffer address continues to be automatically incremented by eight until all data intended for the first logical group logical group 0 is provided to the module . Data is provided to the other logical groups in a similar manner as illustrated.

For example during a 513th write from the processor to the 64 data lines e.g. 512 in the write count column the processor provides to the DDR3 address bus an address selection of a third address of the data buffer system e.g. processor write address 512 and the corresponding eight data bytes. This may result in a first data byte e.g. 4096 being stored in the first state machine engine e.g. F0 at a third data buffer address 416 e.g. 1 of the first state machine engine e.g. F0 a second data byte e.g. 4097 being stored in the second state machine engine e.g. F1 at the same third data buffer address e.g. 1 of the second state machine engine e.g. F1 a third data byte e.g. 4098 being stored in the third state machine engine e.g. F2 at the same third data buffer address e.g. 1 of the third state machine engine e.g. F2 a fourth data byte e.g. 4099 being stored in the fourth state machine engine e.g. F3 at the same third data buffer address e.g. 1 of the fourth state machine engine e.g. F3 a fifth data byte e.g. 4100 being stored in the fifth state machine engine e.g. F4 at the same third data buffer address e.g. 1 of the fifth state machine engine e.g. F4 a sixth data byte e.g. 4101 being stored in the sixth state machine engine e.g. F5 at the same third data buffer address e.g. 1 of the sixth state machine engine e.g. F5 a seventh data byte e.g. 4102 being stored in the seventh state machine engine e.g. F6 at the same third data buffer address e.g. 1 of the seventh state machine engine e.g. F6 and an data eighth byte e.g. 4103 being stored in the eighth state machine engine e.g. F7 at the same third data buffer address e.g. 1 of the eighth state machine engine e.g. F7 .

It should be noted that the table indicates that all of the writes 0 through 511 from the write count column include data intended for the logical group 0 which includes the first state machine engine e.g. F0 . Furthermore the table indicates that all of the writes 512 through 1023 from the write count column include data intended for the logical group 1 which includes the second state machine engine e.g. F1 and so forth.

Accordingly the table illustrates that data is stored in the data buffer system in an offset manner and provided to the process buffers in a straight manner. For example during a first data burst onto the IR data bus the state machine engine F0 may provide bytes 0 4096 8192 12288 16384 20480 24576 and 28672 received from the processor e.g. the first byte for each of the logical groups . During a second burst onto the IR data bus the state machine engine F1 may provide bytes 1 4097 8193 12289 16385 20481 24577 and 28673 received from the processor e.g. the second byte for each of the logical groups and so forth. Each of the state machine engines are configured to store bytes from the data burst that correspond to that state machine engine s logical group. For example the state machine engine F0 is configured to store the first byte of each data burst provided to the IR data bus the state machine engine F1 is configured to store the second byte of each data burst provided to the IR data bus and so forth. Thus the data that was stored in an offset manner when stored by the data buffer system is provided to the state machine engines over the IR data bus so that each state machine engine may receive its intended data in a correct order for analysis.

As used herein the term apparatus may be a single module or a system including one or more modules. While the invention may be susceptible to various modifications and alternative forms specific embodiments have been shown by way of example in the drawings and have been described in detail herein. However it should be understood that the invention is not intended to be limited to the particular forms disclosed. Rather the invention is to cover all modifications equivalents and alternatives falling within the spirit and scope of the invention as defined by the following appended claims.

