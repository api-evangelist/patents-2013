---

title: Cloud snapshots
abstract: A computer implemented method, system, and program product for creating snapshots instructions at a replication site, the method comprising determining the differences between a first snapshot at a first time of a LUN and a second snapshot at a second time of the LUN, determining the differences between the second snapshot and a third snapshot at a third time of the LUN, creating a redacted snapshot of the second snapshot wherein the redacted snapshot contains pointers to the first snapshot for the common data in the first snapshot and the second snapshot; wherein the redacted snapshot contains pointers to the third snapshot for common data in the second snapshot and the third snapshot; and further wherein the redacted snapshot stored data that is not present in the first snapshot and is not present in the third snapshot.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09563517&OS=09563517&RS=09563517
owner: EMC IP Holding Company LLC
number: 09563517
owner_city: Hopkinton
owner_country: US
publication_date: 20131230
---
A portion of the disclosure of this patent document may contain command formats and other computer language listings all of which are subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever.

This Application is related to U.S. patent application Ser. No. 14 143 778 entitled CLOUD RESTORE filed on even date herewith the contents and teachings of which are incorporated herein by reference in their entirety.

Computer data is vital to today s organizations and a significant part of protection against disasters is focused on data protection. As solid state memory has advanced to the point where cost of memory has become a relatively insignificant factor organizations can afford to operate with systems that store and process terabytes of data.

Conventional data protection systems include tape backup drives for storing organizational production site data on a periodic basis. Such systems suffer from several drawbacks. First they require a system shutdown during backup since the data being backed up cannot be used during the backup operation. Second they limit the points in time to which the production site can recover. For example if data is backed up on a daily basis there may be several hours of lost data in the event of a disaster. Third the data recovery process itself takes a long time.

Another conventional data protection system uses data replication by creating a copy of the organization s production site data on a secondary backup storage system and updating the backup with changes. The backup storage system may be situated in the same physical location as the production storage system or in a physically remote location. Data replication systems generally operate either at the application level at the file system level at the hypervisor level or at the data block level.

Current data protection systems try to provide continuous data protection which enable the organization to roll back to any specified point in time within a recent history.

Continuous data protection systems aim to satisfy two conflicting objectives as best as possible namely i minimize the down time in which the organization production site data is unavailable during a recovery and ii enable recovery as close as possible to any specified point in time within a recent history.

Continuous data protection typically uses a technology referred to as journaling whereby a log is kept of changes made to the backup storage. During a recovery the journal entries serve as successive undo information enabling rollback of the backup storage to previous points in time. Journaling was first implemented in database systems and was later extended to broader data protection.

One challenge to continuous data protection is the ability of a backup site to keep pace with the data transactions of a production site without slowing down the production site. The overhead of journaling inherently requires several data transactions at the backup site for each data transaction at the production site. As such when data transactions occur at a high rate at the production site the backup site may not be able to finish backing up one data transaction before the next production site data transaction occurs. If the production site is not forced to slow down then necessarily a backlog of un logged data transactions may build up at the backup site. Without being able to satisfactorily adapt dynamically to changing data transaction rates a continuous data protection system chokes and eventually forces the production site to shut down.

A computer implemented method system and program product for creating snapshots instructions at a replication site the method comprising determining the differences between a first snapshot at a first time of a LUN and a second snapshot at a second time of the LUN determining the differences between the second snapshot and a third snapshot at a third time of the LUN creating a redacted snapshot of the second snapshot wherein the redacted snapshot contains pointers to the first snapshot for the common data in the first snapshot and the second snapshot wherein the redacted snapshot contains pointers to the third snapshot for common data in the second snapshot and the third snapshot and further wherein the redacted snapshot stored data that is not present in the first snapshot and is not present in the third snapshot.

In certain embodiments the current disclosure may enable shipping of a snapshot into object based storage. Generally object based storage and cloud object based storage such as services like EMC Atmos and Amazon S3 may provide inexpensive storage but such storage may have limited features and capabilities. Usually in object based storage it may be possible to create read and delete an object but there may be no way to modify an object. Typically object based stores may not allow storage of traditional snapshots in the object store. Generally a full logical unit may not be stored as a single object as there is no ability to modify it. Usually to update an object in object base storage the whole object would need to be re created. Conventionally a logical unit may not be broken into an infinite number of building blocks as this may require an object stored for each individual block and the overhead of creating and holding so many objects may be too high to reasonably manage.

In certain embodiments the current invention may enable a change in the format of block data to enable it to be stored in object storage. In certain embodiments a LU may be broken up into objects. In certain embodiments the objects may range in size between 1 MB LOMB. In some embodiments it may be efficient to manage snapshots at a 1 MB LOMB granularity. In other embodiments it may be more efficient to manage a snapshot at a granularity the array uses. In some embodiments if a block changed in a block of 1 MB creating snapshots with 1 MB granularity may require sending 1 MB of data to the cloud. In at least one embodiment this may be because there may not be a way to modify a stored object. In most embodiments the blocks may be encrypted and sent into the cloud. In at least some embodiments metadata describing the blocks may be sent to be stored in the cloud. In certain embodiments metadata describing the blocks may also include a SHA 1 has signature for the blocks.

In further embodiments it may be beneficial to keep differential changes to objects corresponding to block that have changed. In certain embodiments this may enable sending a change of for example 64 kb change object instead of 1 MB object to the cloud. In certain embodiments keeping a block and a differential may require reading the 1 MB and the 64 KB. In one embodiment two snapshots may consist of a block and a change block. In other embodiments snapshots referencing difference between snapshots may have pointers to the data in other snapshots. In further embodiments the current disclosure may enable bi directional snapshots which may represent deltas or pointers to future and past snapshots. In other embodiments the user may designate high importance snapshot that have no dependency on previous snapshots.

In some embodiments cloud based object storage may provide a replication back up to a production site. In certain embodiments as described herein a replication device may divide a LUN being replicated into a number of objects of different size. In most embodiments the objects may be encrypted and send to cloud based storage. In at least some embodiments metadata representing the LUN being replicated may be sent to the cloud. In further embodiments if the LUN that is being replicated to the cloud becomes corrupted it may be possible to verify or correct the LUN without bringing all the data from the cloud. In certain embodiments the LUN may be divided into a plurality of pieces corresponding to the object of the cloud. In certain embodiments a hash may be calculated for each piece of the LUN and compared to a corresponding hash for the piece of the LUN sent to the cloud. In most embodiments if the hashes are the same the data is not corrupted. In other embodiments if the hashes don t match the data may be corrupted and the replicated object may be restored from the cloud.

In a particular embodiment a volume may be accessed while it is being recovered from the cloud. In certain embodiments chunks of data being access may be retrieved from the cloud when accessed. In further embodiments the system may include the activity of each chunk in the production site metadata and the most active chunks may be retrieved first.

In some embodiments bi direction delta marking may be used to create snapshots. In a particular embodiment assume there are objects of 4 blocks. In this particular embodiment at time T1 the LUN being replicated has a b c d at time T2 a e c d and at time T3 a e f d. In this embodiment if there was a fully copy of the block at time T1 and time T2 then at time T2 the snapshot may contain pointers to the data of the snapshots at times T1 and T2. In this embodiment the snapshot may not need to keep any additional data other than metadata describing the snpashot.

In certain embodiments bidirectional snapshots may allow more important snapshots to have faster recovery. In most embodiments bidirectional snapshots may have more snapshots with significantly less storage overhead.

In some embodiments if there is a storage failure efficient data store from the cloud may be enabled. In many embodiments when there is a storage failure most of the data in the cloud may identical to the data on the storage array. In certain embodiments signatures corresponding to portions of the corrupted LUN may be compared to signatures of the data in the cloud. In some embodiments the signature metadata file may be restored from the cloud. In at least some embodiments in the background the slices for which the signatures are different than the ones on the storage array may be recovered. In certain embodiments the recovery may be instantaneous and corrupted data may be determined and recovered when accessed. In most embodiments corrupted data may be determined by a hash comparison.

A description of journaling and some techniques associated with journaling may be described in the patent titled METHODS AND APPARATUS FOR OPTIMAL JOURNALING FOR CONTINUOUS DATA REPLICATION and with U.S. Pat. No. 7 516 287 and METHODS AND APPARATUS FOR OPTIMAL JOURNALING FOR CONTINUOUS DATA REPLICATION and with U.S. Pat. No. 8 332 687 which are hereby incorporated by reference. A description of synchronous and asynchronous replication may be described in the patent titled DYNAMICALLY SWITCHING BETWEEN SYNCHRONOUS AND ASYNCHRONOUS REPLICATION and with U.S. Pat. No. 8 341 115 which is hereby incorporated by reference.

A discussion of image access may be found in U.S. patent application Ser. No. 12 969 903 entitled DYNAMIC LUN RESIZING IN A REPLICATION ENVIRONMENT filed on Dec. 16 2010 assigned to EMC Corp. which is hereby incorporated by reference.

Reference is now made to which is a simplified illustration of a data protection system in accordance with an embodiment of the present invention. Shown in are two sites Site I which is a production site on the right and Site II which is a backup site on the left. Under normal operation the production site is the source side of system and the backup site is the target side of the system. The backup site is responsible for replicating production site data. Additionally the backup site enables rollback of Site I data to an earlier pointing time which may be used in the event of data corruption of a disaster or alternatively in order to view or to access data from an earlier point in time.

During normal operations the direction of replicate data flow goes from source side to target side. It is possible however for a user to reverse the direction of replicate data flow in which case Site I starts to behave as a target backup site and Site II starts to behave as a source production site. Such change of replication direction is referred to as a failover . A failover may be performed in the event of a disaster at the production site or for other reasons. In some data architectures Site I or Site II behaves as a production site for a portion of stored data and behaves simultaneously as a backup site for another portion of stored data. In some data architectures a portion of stored data is replicated to a backup site and another portion is not.

The production site and the backup site may be remote from one another or they may both be situated at a common site local to one another. Local data protection has the advantage of minimizing data lag between target and source and remote data protection has the advantage is being robust in the event that a disaster occurs at the source side.

The source and target sides communicate via a wide area network WAN although other types of networks are also adaptable for use with the present invention.

In accordance with an embodiment of the present invention each side of system includes three major components coupled via a storage area network SAN namely i a storage system ii a host computer and iii a data protection appliance DPA . Specifically with reference to the source side SAN includes a source host computer a source storage system and a source DPA . Similarly the target side SAN includes a target host computer a target storage system and a target DPA .

Generally a SAN includes one or more devices referred to as nodes . A node in a SAN may be an initiator or a target or both. An initiator node is a device that is able to initiate requests to one or more other devices and a target node is a device that is able to reply to requests such as SCSI commands sent by an initiator node. A SAN may also include network switches such as fiber channel switches. The communication links between each host computer and its corresponding storage system may be any appropriate medium suitable for data transfer such as fiber communication channel links.

In an embodiment of the present invention the host communicates with its corresponding storage system using small computer system interface SCSI commands.

System includes source storage system and target storage system . Each storage system includes physical storage units for storing data such as disks or arrays of disks. Typically storage systems and are target nodes. In order to enable initiators to send requests to storage system storage system exposes one or more logical units LU to which commands are issued. Thus storage systems and are SAN entities that provide multiple logical units for access by multiple SAN initiators.

Logical units are a logical entity provided by a storage system for accessing data stored in the storage system. A logical unit is identified by a unique logical unit number LUN . In an embodiment of the present invention storage system exposes a logical unit designated as LU A and storage system exposes a logical unit designated as LU B.

In an embodiment of the present invention LU B is used for replicating LU A. As such LU B is generated as a copy of LU A. In one embodiment LU B is configured so that its size is identical to the size of LU A. Thus for LU A storage system serves as a backup for source side storage system . Alternatively as mentioned hereinabove some logical units of storage system may be used to back up logical units of storage system and other logical units of storage system may be used for other purposes. Moreover in certain embodiments of the present invention there is symmetric replication whereby some logical units of storage system are used for replicating logical units of storage system and other logical units of storage system are used for replicating other logical units of storage system .

System includes a source side host computer and a target side host computer . A host computer may be one computer or a plurality of computers or a network of distributed computers each computer may include inter alia a conventional CPU volatile and non volatile memory a data bus an I O interface a display interface and a network interface. Generally a host computer runs at least one data processing application such as a database application and an e mail server.

Generally an operating system of a host computer creates a host device for each logical unit exposed by a storage system in the host computer SAN. A host device is a logical entity in a host computer through which a host computer may access a logical unit. In an embodiment of the present invention host device identifies LU A and generates a corresponding host device designated as Device A through which it can access LU A. Similarly host computer identifies LU B and generates a corresponding device designated as Device B.

In an embodiment of the present invention in the course of continuous operation host computer is a SAN initiator that issues I O requests write read operations through host device to LU A using for example SCSI commands. Such requests are generally transmitted to LU A with an address that includes a specific device identifier an offset within the device and a data size. Offsets are generally aligned to 512 byte blocks. The average size of a write operation issued by host computer may be for example 10 kilobytes KB i.e. 20 blocks. For an I O rate of 50 megabytes MB per second this corresponds to approximately 5 000 write transactions per second.

System includes two data protection appliances a source side DPA and a target side DPA . A DPA performs various data protection services such as data replication of a storage system and journaling of I O requests issued by a host computer to source side storage system data. As explained in detail hereinbelow when acting as a target side DPA a DPA may also enable rollback of data to an earlier point in time and processing of rolled back data at the target site. Each DPA and is a computer that includes inter alia one or more conventional CPUs and internal memory.

For additional safety precaution each DPA is a cluster of such computers. Use of a cluster ensures that if a DPA computer is down then the DPA functionality switches over to another computer. The DPA computers within a DPA cluster communicate with one another using at least one communication link suitable for data transfer via fiber channel or IP based protocols or such other transfer protocol. One computer from the DPA cluster serves as the DPA leader. The DPA cluster leader coordinates between the computers in the cluster and may also perform other tasks that require coordination between the computers such as load balancing.

In the architecture illustrated in DPA and DPA are standalone devices integrated within a SAN. Alternatively each of DPA and DPA may be integrated into storage system and storage system respectively or integrated into host computer and host computer respectively. Both DPAs communicate with their respective host computers through communication lines such as fiber channels using for example SCSI commands.

In accordance with an embodiment of the present invention DPAs and are configured to act as initiators in the SAN i.e. they can issue I O requests using for example SCSI commands to access logical units on their respective storage systems. DPA and DPA are also configured with the necessary functionality to act as targets i.e. to reply to I O requests such as SCSI commands issued by other initiators in the SAN including inter alia their respective host computers and . Being target nodes DPA and DPA may dynamically expose or remove one or more logical units.

As described hereinabove Site I and Site II may each behave simultaneously as a production site and a backup site for different logical units. As such DPA and DPA may each behave as a source DPA for some logical units and as a target DPA for other logical units at the same time.

In accordance with an embodiment of the present invention host computer and host computer include protection agents and respectively. Protection agents and intercept SCSI commands issued by their respective host computers via host devices to logical units that are accessible to the host computers. In accordance with an embodiment of the present invention a data protection agent may act on an intercepted SCSI commands issued to a logical unit in one of the following ways 

A protection agent may handle different SCSI commands differently according to the type of the command. For example a SCSI command inquiring about the size of a certain logical unit may be sent directly to that logical unit while a SCSI write command may be split and sent first to a DPA associated with the agent. A protection agent may also change its behavior for handling SCSI commands for example as a result of an instruction received from the DPA.

Specifically the behavior of a protection agent for a certain host device generally corresponds to the behavior of its associated DPA with respect to the logical unit of the host device. When a DPA behaves as a source site DPA for a certain logical unit then during normal course of operation the associated protection agent splits I O requests issued by a host computer to the host device corresponding to that logical unit. Similarly when a DPA behaves as a target device for a certain logical unit then during normal course of operation the associated protection agent fails I O requests issued by host computer to the host device corresponding to that logical unit.

Communication between protection agents and their respective DPAs may use any protocol suitable for data transfer within a SAN such as fiber channel or SCSI over fiber channel. The communication may be direct or via a logical unit exposed by the DPA. In an embodiment of the present invention protection agents communicate with their respective DPAs by sending SCSI commands over fiber channel.

In an embodiment of the present invention protection agents and are drivers located in their respective host computers and . Alternatively a protection agent may also be located in a fiber channel switch or in any other device situated in a data path between a host computer and a storage system.

What follows is a detailed description of system behavior under normal production mode and under recovery mode.

In accordance with an embodiment of the present invention in production mode DPA acts as a source site DPA for LU A. Thus protection agent is configured to act as a source side protection agent i.e. as a splitter for host device A. Specifically protection agent replicates SCSI I O requests. A replicated SCSI I O request is sent to DPA . After receiving an acknowledgement from DPA protection agent then sends the SCSI I O request to LU A. Only after receiving a second acknowledgement from storage system may host computer initiate another I O request.

When DPA receives a replicated SCSI write request from data protection agent DPA transmits certain I O information characterizing the write request packaged as a write transaction over WAN to DPA on the target side for journaling and for incorporation within target storage system .

DPA may send its write transactions to DPA using a variety of modes of transmission including inter alia i a synchronous mode ii an asynchronous mode and iii a snapshot mode. In synchronous mode DPA sends each write transaction to DPA receives back an acknowledgement from DPA and in turns sends an acknowledgement back to protection agent . Protection agent waits until receipt of such acknowledgement before sending the SCSI write request to LU A.

In asynchronous mode DPA sends an acknowledgement to protection agent upon receipt of each I O request before receiving an acknowledgement back from DPA .

In snapshot mode DPA receives several I O requests and combines them into an aggregate snapshot of all write activity performed in the multiple I O requests and sends the snapshot to DPA for journaling and for incorporation in target storage system . In snapshot mode DPA also sends an acknowledgement to protection agent upon receipt of each I O request before receiving an acknowledgement back from DPA .

For the sake of clarity the ensuing discussion assumes that information is transmitted at write by write granularity.

While in production mode DPA receives replicated data of LU A from DPA and performs journaling and writing to storage system . When applying write operations to storage system DPA acts as an initiator and sends SCSI commands to LU B.

During a recovery mode DPA undoes the write transactions in the journal so as to restore storage system to the state it was at at an earlier time.

As described hereinabove in accordance with an embodiment of the present invention LU B is used as a backup of LU A. As such during normal production mode while data written to LU A by host computer is replicated from LU A to LU B host computer should not be sending I O requests to LU B. To prevent such I O requests from being sent protection agent acts as a target site protection agent for host Device B and fails I O requests sent from host computer to LU B through host Device B.

In accordance with an embodiment of the present invention target storage system exposes a logical unit referred to as a journal LU for maintaining a history of write transactions made to LU B referred to as a journal . Alternatively journal LU may be striped over several logical units or may reside within all of or a portion of another logical unit. DPA includes a journal processor for managing the journal.

Journal processor functions generally to manage the journal entries of LU B. Specifically journal processor i enters write transactions received by DPA from DPA into the journal by writing them into the journal LU ii applies the journal transactions to LU B and iii updates the journal entries in the journal LU with undo information and removes already applied transactions from the journal. As described below with reference to journal entries include four streams two of which are written when write transaction are entered into the journal and two of which are written when write transaction are applied and removed from the journal.

Reference is now made to which is a simplified illustration of a write transaction for a journal in accordance with an embodiment of the present invention. The journal may be used to provide an adaptor for access to storage at the state it was in at any specified point in time. Since the journal contains the undo information necessary to rollback storage system data that was stored in specific memory locations at the specified point in time may be obtained by undoing write transactions that occurred subsequent to such point in time.

Write transaction is transmitted from source side DPA to target side DPA . As shown in DPA records the write transaction in four streams. A first stream referred to as a DO stream includes new data for writing in LU B. A second stream referred to as an DO METADATA stream includes metadata for the write transaction such as an identifier a date time a write size a beginning address in LU B for writing the new data in and a pointer to the offset in the do stream where the corresponding data is located. Similarly a third stream referred to as an UNDO stream includes old data that was overwritten in LU B and a fourth stream referred to as an UNDO METADATA include an identifier a date time a write size a beginning address in LU B where data was to be overwritten and a pointer to the offset in the undo stream where the corresponding old data is located.

In practice each of the four streams holds a plurality of write transaction data. As write transactions are received dynamically by target DPA they are recorded at the end of the DO stream and the end of the DO METADATA stream prior to committing the transaction. During transaction application when the various write transactions are applied to LU B prior to writing the new DO data into addresses within the storage system the older data currently located in such addresses is recorded into the UNDO stream.

By recording old data a journal entry can be used to undo a write transaction. To undo a transaction old data is read from the UNDO stream in a reverse order from the most recent data to the oldest data for writing into addresses within LU B. Prior to writing the UNDO data into these addresses the newer data residing in such addresses is recorded in the DO stream.

The journal LU is partitioned into segments with a pre defined size such as 1 MB segments with each segment identified by a counter. The collection of such segments forms a segment pool for the four journaling streams described hereinabove. Each such stream is structured as an ordered list of segments into which the stream data is written and includes two pointers a beginning pointer that points to the first segment in the list and an end pointer that points to the last segment in the list.

According to a write direction for each stream write transaction data is appended to the stream either at the end for a forward direction or at the beginning for a backward direction. As each write transaction is received by DPA its size is checked to determine if it can fit within available segments. If not then one or more segments are chosen from the segment pool and appended to the stream s ordered list of segments.

Thereafter the DO data is written into the DO stream and the pointer to the appropriate first or last segment is updated. Freeing of segments in the ordered list is performed by simply changing the beginning or the end pointer. Freed segments are returned to the segment pool for re use.

A journal may be made of any number of streams including less than or more than 5 streams. Often based on the speed of the journaling and whether the back up is synchronous or a synchronous a fewer or greater number of streams may be used.

Refer now to the simplified example embodiments of which illustrate sending a LUN chunk into a cloud. On production site snapshot is taken of replicated LUN step . Snapshot is divided into snapshot LUN chunks step . A Hash such as hash which may be a SHA1 hash of each snapshot LUN chunk data is created step . The hash of each LUN chunk is stored on production site in local meta data file step . LUN chunks are encrypted by DPA step . LUN chunks are compressed by DPA step . DPA sends encrypted LUN chunks into cloud via communicative coupling step . LUN chunks arrive in cloud and are stored as replicated objects .

In certain embodiments a meta data file containing the list of the chunk objects and the hashes of the data of each chunk may be sent to the replication site at the cloud. In some embodiments the chunks may be sent using an API for example a REST based API.

Refer now to the simplified example embodiments of which illustrate shipping a snapshot into the cloud. In these example embodiments a first snapshot snapshot has been taken of replicated device at a first time. A second snapshot snapshot has been taken at a second time. Snapshot has been stored in cloud as replicated objects snapshot . A comparison is made between snapshot and snapshot to arrive at snapshot delta step . RPA encrypts snapshot delta and sends it to cloud via communicative coupling step .

In some embodiments the snapshot delta may be a list of the chunks that had data changed between the first and second snapshot. In certain embodiments the system may send the changed chunks a metadata file describing the list of objects that represent the chunks of the new snapshot and the hash of the value of each chunk. In other embodiments the changed chunks may be sent as differences from previous chunks i.e. only the locations changed in the chunk are shipped to the cloud. In certain embodiments to recover a chunk its reference chunk may need to be recovered as well.

Refer now to the simplified example embodiment of which illustrates chunk data and metadata about chunks. Chunk 1 data object chunk 2 object and chunk 3 object are referenced by version 1 metadata object as referring to a set of data at a particular time or version. Chunk 2 object contains the data for this chunk at the second point in time. Version 2 metadata object denotes what data corresponds to version 2 or the second point in time. In this embodiment chunk 1 data object and chunk 3 object have not changed between versions and version 2 metadata object refers to the same data as in the first version. In this embodiment chunk 2 object has changed and version 2 metadata object references chunk 2 object .

Refer now to the simplified example embodiment of which illustrates how snapshots may reference other snapshots. In the example embodiment of there are three types of snapshots. An I snapshot such as snapshot and is a full snapshot that contain all the data of the object of the snapshot. A P snapshot such as snapshots and is a snapshot that references data of a snapshot that occurred at a previous point in time. For example P snapshot references data in snapshot I . This means that all the data common in snapshots and is not duplicated in both snapshots. Rather the later snapshot P references back to data stored in snapshot . A B snapshot is a snapshot that is relative to a snapshot both before and after the snapshot was taken. For example B snapshot refers both backwards in time to snapshot I and forward in time to P snapshot . By referring backwards in time or both backwards and forwards in time to other snapshots less data needs to be stored than if every snapshot were a full snapshot.

In certain embodiments a P snapshot may be created by instead of shipping a full chunk if a single block is changed shipping a list of the blocks that changed from the reference chunk and the new data which may be compressed and encrypted. In most embodiments in order to recover a chunk which is in a P snapshot both the object describing the differences and the object pointed to by the object may need to be referenced. Referring back to the example embodiment of a chunk in snapshot may need to reference the object describing the chunk as well as the object describing the chunk in snapshot . Referring again to in the cast of snapshot which is a B snapshot both the respective chunks from snapshot and may need to be referenced.

In a first embodiment a B type snapshot may be created by refining granularity. In this embodiment an I snapshot may be sent to the cloud. such as snapshot . In this first embodiment a P snapshot may be sent to the cloud such as snapshot . In this embodiment the production site may refine the available point in time and send another bi directional snapshot such as snapshot to the cloud.

In a second embodiment a P snapshot may be changed to a B snapshot. In the second embodiment an I snapshot may be sent to the replica such as snapshot . In this embodiment a P snapshot such as may be sent to the replica. In this embodiment a P snapshot such as snapshot may be sent to the replica. In this embodiment the cloud may reorganized a P snapshot such as snapshot into a B snapshot and the P version of the snapshot may be erased thus saving space in the cloud.

In certain embodiments a B snapshot may include a full pointer to a chunk object in the respective P and I snapshots. In a particular embodiment take a volume with 3 chunks c1 c2 c3 at time t1. In this embodiment c2 and c3 change at time t2. In this embodiment the snapshot at t2 a P snapshot may include the original block c1 and the changed blocks c2 and c3. In this embodiment the snapshot at t1 a B snapshot may include c1 and c2 from the original snapshot and c3 from the P snapshot. In certain embodiments a B snapshot may include a differential chunk if the same chunk changed twice once before the B snapshot and once after the B snapshots.

In embodiment A the B snapshot includes a list of pointers to T0 and T2 and does not include any data chunks of its own.

In embodiment B a B snapshot a T1 will have a pointer to chunk 1 in T2 and chunk 3 T2 but for chunk 2 the differences between chunk 2 in T2 and chunk 2 in T2 is compressed.

In certain embodiments a chunk may be 1 MB or more. In most embodiments the differences between chunks may yield significant storage and processing efficiencies.

Refer now to the example embodiment of showing a transformation from a P snapshot to a B snapshot. On production site at time T2 there is snapshot and snapshot . At time T2 Snapshot has been transferred to cloud replication as replicated objects snapshot and snapshot has been transferred Snapshot . At time T2 in cloud replication snapshot is a P snapshot referencing data in replicated objects snapshot . Cloud replication represents the cloud at future time T3.

Refer now to the example embodiment of . In this example embodiment at time T3 snapshot has has been created and sent to cloud replication . Replication objects snapshot represents a full snapshot. Snapshot delta is adjusted to point to data on replicated objects snapshot . By doing this snapshot delta is able to have a smaller footprint and reduce the amount of storage necessary on cloud replication .

Refer now to the example embodiments of which illustrate recovery from a failure on the production site. Device has become corrupted and a user may want to restore the device to an earlier point in time stored at the cloud. In this embodiment all blocks in device may not be corrupted so it may be desired to restore the blocks which are corrupt. DPA reads metadata file for point in time to be restored from the cloud the file include the list of chunks and for each chunk the hash value of the data of the chunk step . DPA initiates a background process which starts restoring and or verifying the chunks. For each chunk the chunk data is read from the production storage and the hash is calculated step . DPA compares the hash or metadata from device and cloud step . If the hash or metadata is the same the location is verified as consistent step . If the hash or metadata is not the same the data is restored from cloud step . In the case data is restored from cloud DPA gets data from replicated object snapshot . DPA sends the data to replicated device .

In some embodiments the restore process may occur when user is accessing the LU. In certain embodiments if the user is accessing the LUN during a restore a splitter may redirect all reads and writes to the DPA. In some of embodiments when a read or a write arrives at the DPA the DPA may check if the chunk was already verified. In certain of these embodiments if the chunk is not yet verified the system may verify the chunks and notify the splitter to process with the read or the write IO. In some other embodiments if the chunk is has been verified the splitter may be notified to proceed with the IO.

Refer now to the example embodiments of which illustrate read write access to replicated device being restored by the cloud. Splitter directs IO to DPA step . A hash corresponding to the target of the IO is gotten step . A determination is made if the target chunk for the IO has been verified step . If the target chunk has not been verified it is verified and the data is read from cloud step . If the chunk has been verified the IO proceeds to replicated device step . In certain embodiments once all chunks have been verified the LUN may be moved to normal operation and IOs may no longer be intercepted by the splitter.

The methods and apparatus of this invention may take the form at least partially of program code i.e. instructions embodied in tangible media such as floppy diskettes CD ROMs hard drives random access or read only memory or any other machine readable storage medium. When the program code is loaded into and executed by a machine such as the computer of the machine becomes an apparatus for practicing the invention. When implemented on one or more general purpose processors the program code combines with such a processor to provide a unique apparatus that operates analogously to specific logic circuits. As such a general purpose digital machine can be transformed into a special purpose digital machine. shows Program Logic embodied on a computer readable medium as shown and wherein the Logic is encoded in computer executable code configured for carrying out the reservation service process of this invention and thereby forming a Computer Program Product . The logic may be the same logic on memory loaded on processor . The program logic may also be embodied in software modules as modules or as hardware modules.

The logic for carrying out the method may be embodied as part of the system described below which is useful for carrying out a method described with reference to embodiments shown in for example . For purposes of illustrating the present invention the invention is described as embodied in a specific configuration and using special logical arrangements but one skilled in the art will appreciate that the device is not limited to the specific configuration but rather only by the claims included with this specification.

Although the foregoing invention has been described in some detail for purposes of clarity of understanding it will be apparent that certain changes and modifications may be practiced within the scope of the appended claims. Accordingly the present implementations are to be considered as illustrative and not restrictive and the invention is not to be limited to the details given herein but may be modified within the scope and equivalents of the appended claims.

