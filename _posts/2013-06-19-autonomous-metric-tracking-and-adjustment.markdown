---

title: Autonomous metric tracking and adjustment
abstract: Example embodiments relate to autonomous metric tracking and adjustment. In some examples, a computing node may include a processor to run a main operating system and an application that runs on top of the main operating system. The computing node may include a hardware-level controller that dynamically adjusts individual hardware components of the computing node via control signals that do not pass through the main operating system. The adjustments may be based on a target metric from a scheduling service external to the computing node and individual performance metrics from the computing node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09143403&OS=09143403&RS=09143403
owner: Hewlett-Packard Development Company, L.P.
number: 09143403
owner_city: Houston
owner_country: US
publication_date: 20130619
---
A common goal of IT and application administrators of large infrastructures is to balance various performance goals for example such as maximizing the execution of tasks operations and minimizing operating costs e.g. energy power consumption etc. .

A distributed application is a software program that is composed of multiple individual parts running in a coordinated manner on multiple networked computing nodes. These parts may collectively contribute to a unified objective such as serving web pages running a distributed database and running a high performance computing HPC task.

As mentioned above various parts of a distributed application may run on multiple computing nodes. Each of these parts may be referred to generally as an application running on its respective computing node. In order to balance various performance goals with respect to the distributed application the computing nodes and related computing equipment it may be desirable to monitor various performance metrics related to the various parts of the distributed application and in response adjust various aspects of the computing nodes.

Various techniques for monitoring distributed applications may require a software program or a virtual machine e.g. referred to as an agent running on each computing node. For each computing node the agent runs on top of the main operating system OS of the computing node meaning that the agent accesses the resources e.g. CPU memory storage etc. of the computing node via the main OS. The agent communicates with an application e.g. part of a distributed application of the same computing node to read performance indicators and metrics about the application. The agent communicates with a centralized scheduling server or with other agents of other computing nodes in a peer to peer manner. The scheduling server may receive performance indicators and metrics from each computing node and may implement a control loop to send and update control signals to the agents of the various computing nodes based on the performance indicators. For each computing node the agent may modify hardware resources of its computing node e.g. CPU memory storage etc. in response to signals from the application and the scheduling server. However the agent modifies the hardware resources by communicating with the main OS. The agent may have supervisor level privileges allowing the agent to access and modify low level protected settings and resources of the computing node.

The techniques just described may have various disadvantages. For example the software agent may be intrusive and may drain resources that ideally may be reserved for the applications running on the computing node. First the software agent may use valuable network traffic that otherwise may be available to the application. The communication between the software agent and the scheduling server is frequent and the agent may send many different performance indicators or metrics to the scheduling server. For example in frequent intervals e.g. every second the agent may send metrics regarding CPU utilization network utilization application performance metrics and the like. Then at various intervals the software agent may receive control signals from the scheduling server regarding adjustments that should be made to various components of the computing node. All of this traffic to and from the agent travels via the same network as the network used by the application to perform various network tasks e.g. referred to as the application network . Thus this agent traffic adds noise and unpredictability to the application network which may reduce the network performance for the applications of the various computing nodes. Next the software agent may use valuable processing resources of the computing node that otherwise would be available to the application. Because the software agent and the application both sit on top of the main OS of the computing node each uses the main processor s of the computing node to perform tasks. The software agent may expend processing resources by collecting performance metrics from the application and by parsing and implementing control signals from the scheduling server. The software agent may also represent a software quality and support burden since the agent has to be ported and tested to a large variety of computing environments. The software agent may also be a potential security and data privacy concern because the agent needs to be able to access low level protected resources with supervisor level privileges.

The present disclosure describes autonomous metric tracking and adjustment. In various examples of the present disclosure a computing node may include a hardware level controller that may dynamically adjust individual hardware components via control signals that do not pass through the main operating system of the computing node. The adjustments may be based on a target metric from a scheduling service external to the computing node. The target metric may be used to indicate one or more performance goals perhaps a tradeoff or balance of performance goals e.g. quality of service and power energy . The hardware level controller may implement a control loop e.g. an autonomous closed control loop that monitors metrics of the computing node and automatically and dynamically adjusts various aspects e.g. hardware components of the computing node to track as closely as possible the target metric. Various descriptions below may describe the solutions of the present disclosure as they may apply to managing parts of a distributed application. However it should be understood that these descriptions may apply to various other scenarios where a computing node may manage its resources based on a target metric provided by an external service. Thus for example application of may in some scenarios not be part of a distributed application. Or for example the solutions provided herein may generally aim to monitor performance metrics of the computing node and adjust various aspects e.g. hardware components of the computing node. Thus the specific example of a distributed application may be used to explain the various features of the present disclosure and is not intended to be limiting.

The present disclosure may provide benefits over various techniques for monitoring distributed applications e.g. those described above. For example the autonomous metric tracking and adjustment described herein is implemented at the hardware level and thus does not require a software agent and does not execute commands via the main processor of the computing node. Additionally the hardware level controller may communicate with the scheduling server via a different network e.g. management network than the network used by applications to perform network related tasks e.g. application network . Thus the hardware level controller may use little to no processing power of the computing node and may use little to no network traffic on the application network which minimizes the impact on the applications of the computing node. Additionally because no software agent is required the solution of the present disclosure may require no porting and testing to a large variety of computing environments. Additionally because no software agent is required that needs to access low level protected resources the computing node may reduce the amount of software programs that may have supervisor level privileges.

Computing nodes may each be any computing device that is capable of running applications and communicating with various networks. Each computing node e.g. may include an application layer an OS layer and a hardware layer which may include hardware components and a hardware level controller . Applications may run at the application layer and may communicate with application network . Hardware level controller may communicate with management network and in turn with scheduling system to among other things provide an observed metric and receive a target metric as described in more detail below.

Scheduling system may be implemented as at least one computing device for example any computing device capable of communicating with computing nodes via at least one network e.g. . In some examples scheduling system may be implemented as more than one computing device for example computing devices that are in communication with each other e.g. via a network . In these examples the computing devices may operate together and provide a unified service to users. In these examples the computing devices of the scheduling system may be separate devices perhaps geographically separate. The term system may be used to refer to a computing environment that includes one computing device or more than one networked computing device.

Scheduling system may include a scheduling service that may communicate e.g. via network with computing nodes to among other things receive observed metrics from the various computing nodes and send target metrics to the various computing nodes as described in more detail below. Scheduling service may monitor the progress of a distributed application e.g. with parts running on various computing nodes such as computing node . Scheduling service may periodically receive observed metrics from the various computing nodes and may determine whether adjustments should be made based on the observed metrics. Scheduling service may periodically send target metrics to the various computing nodes which may be used by the computing nodes to adjust various aspects of the computing node.

Application may be but need not be a part of a distributed application. In this respect various computing nodes similar to computing node e.g. a cluster of computing nodes may run applications that are similar to application and these applications may operate together. A cluster of computing nodes may be shared by multiple applications. However various descriptions herein in order to provide a clear disclosure may describe a single application e.g. and a hardware level controller e.g. that is dedicated to that single application. It should be understood however that the techniques provided herein may accommodate multiple applications running on the same computing node. Application may perform various tasks by communicating with a network e.g. an application network similar to application network of for example.

Main OS controls the resources of computing node . Main OS may support and monitor applications e.g. . Main OS may detect allocate and tune e.g. via frequency and voltage tuners the hardware resources e.g. hardware components of the computing node. As can be seen in example hardware components e.g. in include processor components e.g. CPU CPU cores etc. memory components e.g. DRAM network components and storage components . Processor components may include a main processor or set of processors that perform the vast majority of all the operations and computations of the computing node . Such a main processor may run the main OS application and any other applications in the application layer and controller driver . As described in more detail below the main processor may not however run operations and computations of MC which may have its own processor .

Management controller MC also referred to as a baseboard management controller may be a specialized service processor that monitors the state e.g. physical state of various hardware components e.g. of computing node . MC may be located on the motherboard or main circuit board of the computing node. MC may include a number of sensors to monitor various hardware components e.g. . Sensors may measure various metrics e.g. internal physical variables of the hardware components such as temperature humidity power supply voltage fan speeds and communications parameters. MC may have access to various tuning controls of hardware components and thus may tune various hardware components. For example MC could turn off a CPU or a core tune a CPU or a core to run faster slower change bus core clock multipliers change DRAM timing change network link speed and the like. MC may include its own processor and may run its own software . Thus the operations and calculations performed by MC may not detract from the processing power of the main processor s e.g. in of the computing node which as described above run the main OS . In some examples MC is only accessible to the main OS via a controller driver e.g. as described in more detail below.

In some embodiments MC may be implemented external to computing node for example as a chassis management controller CMC . A CMC may operate in a manner similar to the MC described herein but it may control multiple computing nodes for example computing nodes that are physically aggregated together in a rack or chassis. In these examples the hardware level controller described herein and its various features may be implemented in the CMC or near the CMC e.g. external to computing node . Thus for example the hardware level controller may be implemented in a rack or chassis and may implement at least one inner control loop as described herein. In general the functionality of the MC and or hardware level controller may be similar whether the MC and or hardware level controller is implemented internal to computing node or external.

MC may include a network interface which play allow MC to access a management network e.g. management network of . In various existing computing nodes the management network may be used to allow communication between a system administrator and a management controller such that the system administrator may perform basic administrative tasks e.g. to power cycle certain hardware components on etc. . Additionally if certain hardware components monitored by the management controller exhibit metrics that stray outside of acceptable ranges the administrator may be notified via the management network. The management network for various reasons e.g. security is typically a separate network from the application network. As described in more detail below the present disclosure may take advantage of this existing management network and the fact that it is separate from the application network to implement a hardware level controller that causes minimal or no noise to the application network. Thus in the examples of the present disclosure network interface may receive a target metric from a scheduling service and may send an observed metric to the scheduling service.

Hardware level controller may communicate with MC . In some examples hardware level controller is included within MC . Hardware level controller may communicate with the network interface of MC for example to receive a target metric and send an observed metric as described more below. In this respect hardware level controller may communicate with a scheduling server via the management network to perform autonomous metric tracking and adjustment while causing minimal or no noise to the application network. Hardware level controller may communicate with the sensors of MC for example to receive various metrics e.g. internal physical variables of the hardware components e.g. of computing node . Hardware level controller may communicate with MC to in turn access various tuning controls of hardware components and thus tune various hardware components. Thus it can be seen that hardware level controller may extend the functionality of MC .

Hardware level controller may be implemented as one or more hardware devices including electronic circuitry for implementing the functionality described herein. In addition or as an alternative hardware level controller may be implemented as a series of Instructions encoded on a machine readable storage medium and executable by a processor of MC e.g. processor . Thus it can be seen that the operations and computations performed by hardware level controller may require minimal or no processing power of the main processor s e.g. in of computing node . Additionally hardware level controller may interfere minimally or not at all with the main OS and or application . In short hardware level controller may transparently perform its tasks e.g. monitoring application progress etc. while causing minimal to no performance overhead.

Hardware level controller may communicate with a controller driver to access main OS and application . As described in more detail below hardware level controller may communicate with controller driver to access various hardware and OS metrics and counters. Additionally controller driver may allow application to send information e.g. unit of work information such as number of web pages served or number of steps completed in a scientific simulation directly to hardware level controller . Hardware level controller may invoke controller driver periodically for example based on a sampling frequency established through the standard hardware monitoring parameters of MC . Main OS may also perform monitoring functions according to the same sampling frequency. For example main OS could monitor a number of executed instructions at a granularity of 10 000 so that every 10 000 instructions the relevant information e.g. various performance counters are available to hardware level controller . Alternatively or in addition every 10 000 instructions for example main OS could copy the relevant information e.g. values of various performance counters to a memory area e.g. in and then hardware level controller may access the memory area via MC .

Hardware level controller may track various performance metrics of computing node for example with the goal of tracking the performance or computing progress of application . Each hardware level controller in the various computing nodes may be responsible for tracking a part of a distributed application that is running on the computing node. As described in more detail below hardware level controller may autonomously track these performance metrics meaning that hardware level controller may automatically cause adjustments of various components of computing node e.g. by altering various tuning controls of hardware components based on performance metrics observed by the hardware level controller.

Hardware level controller may track the performance or computing progress e.g. rate of execution of tasks of application in various ways. For example hardware level controller may analyze indirect metrics and or direct metrics. Indirect metrics may be performance indicators and metrics from components other than application e.g. from main OS or hardware components that may indicate or suggest the computing progress of application . Direct metrics may be performance indicators that are sent directly from application to hardware level controller as described in more detail below.

Hardware level controller may receive indirect metrics from the various hardware components of computing node . For example hardware level controller may communicate with MC e.g. with sensors to receive performance metrics e.g. network traffic CPU utilization disk traffic etc. from the hardware components. Hardware level controller may then correlate such performance metrics with the progress of application . In other words hardware level controller may use the performance metrics to approximate the progress e.g. rate of execution of application . Hardware level controller may include logic lookup tables or other knowledge e.g. knowledge gained from experiments tests statistical analysis etc. that determines how various hardware performance metrics may correlate to progress of application . For example if application serves web pages and if hardware level controller is attempting to determine the progress of application in this task hardware level controller may know that a certain amount of network traffic e.g. X Kb may correspond to one served web page on average. Thus hardware level controller may receive network traffic metrics e.g. from network components and use those metrics to indirectly estimate the number of web pages that have been served by application . Hardware level controller may receive various other types of hardware metrics as well for example CPU voltage and frequency settings e.g. from core bus frequency multipliers DRAM timing training at power up e.g. from voltage regulator regime network link speed e.g. from and local storage subsystem hardware RAID parameters e.g. from . Hardware level controller may correlate one or more of these metrics with computing progress of application .

Hardware level controller may also receive indirect metrics from main OS for example by communicating with controller driver . Main OS may communicate with application and may maintain various performance counters. Hardware level controller may have access to various performance metrics and counters maintained by main OS . Similar to the hardware metrics described above these OS metrics may be used by hardware level controller to approximate the progress of application .

Hardware level controller may receive direct metrics e.g. application level metrics counting events etc. from application for example via controller driver . Hardware level controller may then use the direct metrics to compute the progress e.g. rate of execution of application . In order to provide direct metrics programmers of application may insert specific programming calls or annotations into the code of application . These programming calls may be able to communicate with controller driver and in turn with hardware level controller . These programming calls may be provided by an application programming interface API software development kit SDK or the like. Each programming call may indicate when at least one event of application has occurred. Such programming calls may provide more accurate information about the computing progress of application than do the indirect metrics described above. In some examples these direct metrics may augment the indirect metrics and hardware level controller may analyze direct and indirect metrics. As one example of direct metrics and continuing with the web page serving example from above a programmer of application may insert a programming call into the code of application to signal each time a new web page has been generated. As another example in a database application a programming call may signal when a transaction has been committed. As yet another example in a high performance computing HPC application a programming call may signal when the next time step iteration of a physics simulation has completed. By allowing programmers to provide application level information directly to hardware level controller applications may be more precisely monitored and hardware level controller may better optimize tune various components of computing node .

Hardware level controller may use a control loop e.g. a closed control loop to adjust tune various components of a computing node based on various performance metrics of the computing node and a target metric received from an external scheduling service. A closed control loop may use control theory which deals with the behavior of dynamic systems with inputs. A general closed loop system may receive an external input from an external system called a reference. Then the closed loop e.g. via a controller may attempt to alter various control signals settings and the like of the system in order to cause at least one output variable of the system to track the reference as closely as possible over time. In general the closed loop may determine proper corrective action for the system based on the reference and various internal system metrics.

In the example of target metric may be a reference signal coming from the external scheduling service and hardware level controller via control module may attempt to alter various control signals e.g. of the computing node in order to cause at least one output variable e.g. observed metric to track the reference target metric as closely as possible over time. In general hardware level controller may determine proper corrective action for the computing node based on target metric and various internal performance metrics e.g. of the computing node. As can be seen in the output e.g. observed metric of hardware level controller is fed back into control module which allows control module to autonomously and dynamically compensate for changes to the output.

The present disclosure describes a solution that may include two control loops from the perspective of each computing node. A first outer control loop may be implemented in the scheduling service . This control loop may receive as input various observed metrics e.g. from various computing nodes and may based on these inputs periodically determine performance goals e.g. target metrics such as for the various computing nodes. This control loop may execute less frequently than the inner control loops in the computing nodes discussed more below . A second inner control loop may be implemented in the computing node. Thus each computing node in the network setup e.g. of may implement an inner control loop. The inner control loop may execute more frequently than the outer control loop. Thus a majority of the corrective action adaptation of the computing nodes may be determined in the computing nodes and it may be said that the metric tracking and adjustment described herein is autonomous. The inner control loop may be implemented in the hardware level controller as described in more detail herein. Thus the inner control loop is executed at the hardware level and not at the application level as may be the case with various software agents described above.

Control module may receive an input e.g. target metric from external scheduling service . Target metric may be depending on a determination made by the scheduling service e.g. directed by an administrator various types of performance targets goals. For example target metric may be a power energy target a target that specifies a number of executed tasks a target that specifies usage of various resources e.g. a quality of service metric or the like. Some target metrics may indicate that certain metrics should be optimized. Other target metrics may serve a more managerial purpose for example indicating a tradeoff between various metrics e.g. a performance level to track at minimum power or a power level to track at maximum performance .

In some scenarios it may not be optimal for the distributed application as a whole for each node to optimize a certain metric. Instead it may be better for the distributed application as a whole if individual computing nodes execute tradeoffs between various metrics. As a specific example in various network setups the speed of global computations of the distributed application may be dependent on the slowest computing node and thus instructing one computing node to perform certain tasks faster may not be beneficial if another slower node is already slowing down the global computations. In such a case computing nodes other than the slowest node may be instructed to track some other metric e.g. power . In this respect it can be seen that a scheduling service may manage the various computing nodes with various target metrics in order to achieve a global goal e.g. total cost of ownership overall throughput of a computation profitability of an infrastructure or the like . The scheduling server may implement a variety of higher level heuristics which may be for example dependent on a business scenario. For example in a high performance computing HPC environment the scheduling server might aim to maximize an application s throughput while keeping the power consumption within a certain cap. As another example in a commercial environment the scheduling server might aim to minimize TCO total cost of ownership without violating certain service level agreements and so on. An administrator may be able to configure set these target metrics e.g. that are sent to various computing nodes. Thus the present disclosure may offer flexibility in setting and configuring performance targets.

Target metric may be a single value for example such that the control loop of hardware level controller may be implemented as a single input e.g. external input multiple output SIMO control loop. A SIMO control loop may offer benefits such as a unified control and the ability to break the control loop down into smaller components. Target metric may be a single value even though target metric may indicate that the control loop should attempt to track e.g. trade off among various metrics. For example target metric may indicate the goal of maximum application throughput capped power consumption. In order to create a single value target metric may be generated based on an equation or function e.g. a linear combination that receives various metrics as inputs. Then scheduling service may communicate with hardware level controller to indicate how target metric should be interpreted such that hardware level controller may attempt to track target metric as closely as possible. Because scheduling service may send a single value target metric e.g. and receive a single value observed metric e.g. the network traffic between the computing node and the scheduling service may be reduced when compared to various other techniques that may use a software agent that sends individual statistics and receives individual control signals at frequent intervals.

Control module may also receive a feedback input e.g. internal input that is the observed metric output of metric observation module . Control module may use the feedback input to perform a comparison to target metric . Then control module may adjust various hardware control signals e.g. based on the comparison. More details regarding this comparison and how it may result in control signal adjustments may be described below.

Control module may output multiple control signals e.g. multiple outputs of an SIMO control loop . For example control module may output a control signal for processor components e.g. . As another example control module may output a control signal for memory components e.g. . Various other control signal outputs for various other hardware components e.g. in may be adjusted by module . In general various hardware components e.g. CPUs memory network interfaces storage etc. may have discrete adjustable power states. In these various states the components may remain fully operational but their performance e.g. execution of tasks frequency voltage scaling power levels etc. may vary depending on the state. Each hardware component may have at least one control signal that may be used to change the state of the hardware component. Control module may tap into these control signals e.g. as signals etc. . Control module may include circuitry a routine an algorithm or the like that may generate control signals e.g. based on the external target metric input and the feedback input observed metric .

Metric observation module may monitor receive various performance metrics from various sources within the computing node for example hardware components main OS performance counters specific application calls and the like. Metric observation module may generate an observed metric e.g. a single value metric based on these various performance metrics. Metric observation module may create a single value observed metric in a manner similar to that described above with regard to target metric . For example observed metric may be generated based on an equation or function e.g. a linear combination that receives various metrics as inputs. In some situations such an equation or function may be the same or similar function as an equation function used by scheduling server to generate target metric . Metric observations module may receive an indication from scheduling service regarding how observed metric should be constructed based on various performance metrics. Alternatively each computing node may be individually configured to specify how observed metric should be constructed based on various performance metrics. Observed metric may then be compared to target metric to attempt to track target metric as closely as possible.

In some situations hardware level controller may be able to track the progress of multiple applications e.g. similar to application of . For example hardware level controller may include multiple control loops e.g. multiple control modules multiple metric observations modules etc. for example one control loop per application. As another example a single control loop of hardware level controller may accommodate multiple applications. In general a single control loop may most effectively track a single application but in some scenarios performance metrics from multiple applications may be fed into a single control loop. For example the multiple applications may be prioritized such that at any one time performance metrics from a single application are being fed into the control loop. As another example performance metrics from multiple applications may be combined e.g. using a function or equation to provide aggregated performance metrics to the control loop. Various descriptions herein may describe a single application and a single control loop but it should be understood that the techniques and solutions described herein may be expanded to multiple applications and or multiple control loops.

Hardware level controller may implement a control loop using various types of controllers and or feedback mechanisms techniques. As one example hardware level controller may use a proportional integral derivative PID controlling technique. It should be understood however that this is just one example and various other techniques of closed loop control may be used. A PID technique measures an error between an observed metric and a target metric. Then the error is used to tune or adjust the system. In the specific example of hardware level controller may compare e.g. via subtractor observed metric to target metric to generate an error e i . Such an error may be computed at various intervals for example at a regular discrete sampling interval. Thus the i in the error symbol e i may represent the sampling interval during which the particular error was calculated. During each sampling interval the error e i may be used by multiple control algorithms or control circuits e.g. one control algorithm circuit for each hardware component that is to be controlled by hardware level controller . For example blocks and summation unit may be included in a control algorithm circuit for a processor component. Likewise blocks and summation unit may be included in a control algorithm circuit or a memory component.

The following will describe the control algorithm circuit consisting of blocks and summation unit as one example. Other control algorithms circuits for other hardware components may function in a similar manner. According to the PID technique three values are calculated based on the error e i a proportional value an integral value and a derivative value . The proportional value indicates a present error the integral value indicates an accumulation of past errors and the derivative value indicates a prediction of future errors based on a current rate of change. The proportional value may be based on a proportional parameter for the particular hardware component e.g. Kpp for a processor component . The integral value may be based on an integral parameter for the particular hardware component e.g. Kpi for a processor component . The derivative value may be based on a derivative parameter for the particular hardware component e.g. Kpd for a processor component .

These parameters may affect how the control signals e.g. change in response to error and may in turn affect how observed metric changes in response to error. More particularly these parameters may affect how observed metric may approach target metric e.g. whether and how much observed metric overshoots the target metric oscillates around the target metric converges to the target metric etc. . These parameters may also affect how long it takes observed metric to come within an acceptable range of target metric . These parameters may be configurable and may be set in various ways for example by the scheduling service by the hardware level controller by an administrator or by a combination of these and or other sources. As a specific example an administrator may configure the computing node to behave a certain way in response to the error. As another specific example the scheduling service may indicate how the computing node should behave.

For each hardware component e.g. a processor component for blocks and summation unit the proportional integral and derivative values for the hardware component may be aggregated e.g. via summation unit to create a single control signal e.g. . In other words for example control signal may be a weighed sum of the PID values and . The control signal e.g. for a memory component and various other control signals for various other hardware components may be computed in a similar manner. These control signals may then control their respective hardware components.

In some examples of the present disclosure and referring to computing node e.g. via hardware level controller may synchronize metric information that is sent to scheduling service . Various other computing nodes that send metric information to the scheduling service may perform similar synchronization. When a scheduling service e.g. collects individual metrics e.g. from various computing nodes e.g. in a distributed system it may be important to ensure that the information is time synchronized e.g. within a certain error tolerance . Time synchronized information may allow the scheduling service to perform various routines in an organized manner routines such as extract global aggregated statistics ordering commands and or target metrics to various computing nodes and the like. Hardware level controller may automatically synchronize metric information that is sent to scheduling service e.g. by inserting network timestamps to tag data packets .

Hardware level controller may be similar to other hardware level controllers discussed herein e.g. . Hardware level controller may automatically synchronize metric information e.g. output by metric observation module before sending the metric information to scheduling service . Hardware level controller may use a network synchronization protocol such as precision time protocol PTP or network time protocol NTP . The following descriptions may use the PTP protocol but it should be understood that various other protocols may be used. The PTP protocol may rely on a master slave arrangement where a PTP master server e.g. may maintain a clock time. Then various slave servers or components may adjust their own clock times based on the clock time of the PTP master server. The PTP master server may send messages via a network in order to instruct the PTP slaves to update their clock times. In the example of hardware level controller may include a PTP slave module that may update maintain a clock time based on messages e.g. time update message received from PTP master server . PTP master server may be connected to the management network and messages from the PTP master server may be received by the management controller MC network interface similar to network interface of for example and passed to PTP slave module .

PTP slave module may receive time update messages form PTP master server at various times e.g. at regular intervals . Thus at various times e.g. at the moment a time update message arrives PTP slave module may have the correct current time as specified by PTP master server . Then at times that fall between time update messaging intervals the hardware level controller e.g. via local time advance module may locally advance the time received by the PTP slave module in order to keep the local time as close as possible to the clock time of the PTP master server . In some examples local time advance module may be included within PTP slave module . Thus between module and module hardware level controller may always have a synchronized time. It can be seen that utilizing the scheme just described each computing node e.g. in the network setup may have the same synchronized time e.g. within a small defined error .

Metric observation module e.g. similar to module of may receive current synchronized times from module and or module . Metric observation module may timestamp packets of information with the current synchronized times before sending such packets of information to scheduling service e.g. via MC network interface and management network . As one example metric observation module may collect various performance metrics from various sources of the computing node and may compute a single value observed metric as described herein. This collection and computation may take some time during which PTP slave module may receive at least one time update message. If time exists between when a time update massage is received by module and when module is ready to send the single value observed metric to scheduling service local time advance module may advance the time to maintain the current local time. Then when metric observation module is ready to send an observed metric it may insert or timestamp the packet s of the observed metric with the current local time. Then metric observation module may send the time stamped packets to the scheduling service. The scheduling service may use the timestamps to maintain a consistent and synchronized view of the distributed computing node environment and or the distributed application.

Metric observation module may maintain or receive a programmable aggregation frequency which may specify how often module should create a single value observed metric form various collected performance metrics. This aggregation frequency may be specified by the scheduling service e.g. such that the scheduling service can specify how often it receives observed metrics. This metric may be specified by a system administrator e.g. via scheduling service or directly via the various computing nodes.

As described above hardware level controller may be implemented at the hardware level and likewise the various modules e.g. of hardware level controller may be implemented at the hardware level. Additionally as described above hardware level controller may communicate directly with MC network interface . Thus hardware level controller may perform time synchronization at the hardware level without communicating with the main OS of the computing node without causing any drag or intervention to the application layer and without causing any performance drain on the main processor s of the computing node.

Method may start at step and continue to step where a computing node e.g. computing node of may receive a target metric from a scheduling service e.g. . At step the computing node may compare the target metric to a recent observed metric to determine an error. At step the computing node may determine adjustments to control signals of various hardware components based on the error as described in more detail above. At step the computing node may adjust the hardware components using the adjusted control signals. At step the computing node may receive performance indicators from various sources of the computing node as described in more detail above. shows two examples of where these performance metrics may come from. For example at step the computing node may receive direct metrics from at least one application e.g. from calls to hardware level controller included in the application code . As another example at step the computing node may receive indirect metrics e.g. from the main OS and or hardware components . At step the computing node may combine the various performance metrics into a unified observed metric. At step the computing node may insert a synchronized time e.g. as a timestamp into the observed metric as described above. At step the computing node may send the unified observed metric to the scheduling service. Method may eventually continue to step where method may stop.

Method and portions of method may repeat to form one or more control loops. For example at step once a unified observed metric is determined the observed metric may be fed back into earlier steps of method as shown by for example to form an inner control loop such that at step the target metric may be compared to the observed metric. Additionally once the observed metric is sent to the scheduling service at step the scheduling service may use that observed metric to generate an update target metric which may be fed back into step of method for example to form an outer control loop. In general at various times at least one of the steps of method may repeat more frequently than shown in .

Processor may be one or more central processing units CPUs CPU cores microprocessors and or other hardware devices suitable for retrieval and execution of instructions stored in a machine readable storage medium that is separate from machine readable storage medium . Processor may be one or more central processing units CPUs CPU cores microprocessors and or other hardware devices suitable for retrieval and execution of instructions stored in machine readable storage medium . Processor may be included in a management controller MC of computing node . Processor may fetch decode and execute instructions e.g. instructions to among other things perform autonomous metric tracking and adjustment. With respect to the executable instruction representations e.g. boxes shown in it should be understood that part or all of the executable instructions included within one box may in some examples be included in a different box shown in the figures or in a different box not shown.

Machine readable storage medium may be any electronic magnetic optical or other physical storage device that stores executable instructions. Thus machine readable storage medium may be for example Random Access Memory RAM an Electrically Erasable Programmable Read Only Memory EEPROM a storage drive an optical disc and the like. Machine readable storage medium may be disposed within a computing device e.g. as shown in . Machine readable storage medium may be included in a management controller MC and or a hardware level controller of computing node . In this situation the executable instructions may be installed on the computing device. Alternatively machine readable storage medium may be a portable e.g. external storage medium for example that allows a computing device e.g. to remotely execute the instructions or download the instructions from the storage medium. In this situation the executable instructions may be part of an installation package. As described below machine readable storage medium may be encoded with executable instructions to perform autonomous metric tracking and adjustment.

Target metric receiving instructions may receive e.g. via hardware level controller of computing node a target metric e.g. from a scheduling service e.g. external to the computing node. Performance metrics receiving instructions may receive e.g. via hardware level controller individual performance metrics e.g. of from the computing node. Adjustment determination instructions may determine e.g. via hardware level controller adjustments to be made to individual hardware components e.g. of the computing node based on the target metric and the individual performance metrics. Hardware component adjustment instructions may dynamically adjust e.g. via hardware level controller the individual hardware components e.g. using control signals e.g. that do not pass through a main operating system e.g. of the computing node.

Method may start at step and continue to step where a hardware level controller of computing node may receive e.g. via instructions a target metric from a scheduling service external to the computing node. At step the hardware level controller may receive e.g. via instructions individual performance metrics from the computing node. At step the hardware level controller may determine e.g. via instructions adjustments to be made to individual hardware components of the computing node based on the target metric and the individual performance metrics. At step the hardware level controller may dynamically adjust e.g. via instructions the individual hardware components using control signals that do not pass through a main operating system of the computing node. Method may eventually continue to step where method may stop.

It will be appreciated that the previous description of the disclosed examples is provided to enable any person skilled in the art to make or use the present disclosure. Various modifications to these examples will be readily apparent to those skilled in the art and the general principles defined herein may be applied to other examples without departing from the spirit or scope of the disclosure. Thus the present disclosure is not intended to be limited to the examples shown herein but is to be accorded the widest scope consistent with the principles and novel features disclosed herein.

