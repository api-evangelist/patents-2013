---

title: Integration testing method and system for web services
abstract: A method of testing a web service includes obtaining web service metadata from the web service. Test cases are generated automatically using the web service metadata, heuristic algorithm modules selected based on the contents of the web service metadata, and stochastic processes. Energy selection logic is used to update test parameters included in the test cases between rounds of testing until a set of test cases is generated that produces test results meeting one or more predetermined criteria. Testing is performed periodically using that set of test cases until the test results indicate that the web service has changed. New test cases are automatically generated and used to test the web service until test results meet the one or more predetermined criteria.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09367435&OS=09367435&RS=09367435
owner: VERTAFORE, INC.
number: 09367435
owner_city: Bothell
owner_country: US
publication_date: 20131212
---
The present disclosure relates generally to software testing and more particularly to automated testing of web services using a black box testing methodology.

Integration testing of web services can be problematic for a test developer. For example as the internal functionality of the web services changes a test developer must maintain a deep understanding of how a parent software application works to ensure that the web services are being structured appropriately and to ensure that all code paths are covered during integration testing. Accordingly integration testing of web services can be a highly complex inefficient and costly undertaking.

Black box testing methodologies have been developed for software applications wherein test cases are developed by examining the functionality of the software applications without examining the internal structures or workings of the software applications. To reduce the costs associated with the development of web services it can be desirable to develop black box testing methodologies for integration testing of web services.

A method performed by at least one computer system that tests a web service may be summarized as including obtaining web service metadata from the web service automatically generating a first set of test cases at least in part based on the web service metadata obtained from the web service automatically testing the web service using the first set of test cases and automatically generating first learning data regarding the first set of test cases.

The first set of test cases may be automatically generated at least in part based on a name of a function included in the web service metadata obtained from the web service. The first set of test cases may be automatically generated at least in part based on a name of a variable included in the web service metadata obtained from the web service. The first set of test cases may be automatically generated at least in part based on a data type of a variable included in the web service metadata obtained from the web service. The first set of test cases may be automatically generated at least in part based on at least one constraint on a variable included in the web service metadata obtained from the web service. The at least one constraint on the variable may include at least one of a minimum value a maximum value and a string length. The method may further include selecting one of a plurality of predetermined sets of test parameters based on the web service metadata and selecting a plurality of test parameters from the selected one of the plurality of predetermined sets of test parameters wherein the first set of test cases is automatically generated using the selected plurality of test parameters. The method may further include generating a plurality of random numbers wherein the plurality of test parameters are selected from the selected one of the plurality of predetermined sets of test parameters using the plurality of random numbers. The method may further include selecting one of a plurality of heuristic algorithm modules based on the web service metadata and obtaining a plurality of test parameters using the selected one of the plurality of heuristic algorithm modules wherein the first set of test cases is automatically generated using the obtained plurality of test parameters. The method may further include generating a plurality of random numbers wherein the selected one of the plurality of heuristic algorithm modules obtains the plurality of test parameters using the plurality of random numbers. The method may further include automatically generating a second set of test cases at least in part based on the web service metadata obtained from the web service automatically testing the web service using the second set of test cases automatically generating second learning data regarding the second set of test cases automatically determining at least in part based on the first and the second learning data whether a third set of test cases is to be generated at least in part based on the first or the second set of test cases responsive to determining that the third set of test cases is to be generated at least in part based on the first set of test cases automatically generating the third set of test cases at least in part based on the first set of test cases and responsive to determining that the third set of test cases is to be generated at least in part based on the second set of test cases automatically generating the third set of test cases at least in part based on the second set of test cases. The first learning data may be generated by associating each test case of the first set of test cases with at least one respective test metric and the second learning data may be generated by associating each test case of the second set of test cases with at least one respective test metric and the third set of test cases may be generated at least in part based on the first or the second learning data. The first learning data may be generated by associating each test case of the first set of test cases with at least one respective first code coverage metric and at least one respective first execution time metric the second learning data may be generated by associating each test case of the second set of test cases with at least one respective second code coverage metric and at least one respective second execution time metric and the third set of test cases may be generated at least in part based on the first or the second learning data. The method may further include evaluating energy selection logic using the first and the second learning data wherein a result of the evaluating of the energy selection logic is used to determine whether the third set of test cases is to be generated using the first or the second set of test cases. The evaluating of the energy selection logic may include calculating the sum of a plurality of test metrics associated with the first set of test cases calculating the sum of a plurality of test metrics associated with the second set of test cases and calculating a ratio that includes the sum of the plurality of test metrics associated with the first set of test cases and the sum of the plurality of test metrics associated with the second set of test cases. The evaluating of the energy selection logic may further include calculating the reciprocal of the sum of a plurality of test metrics associated with the first set of test cases and calculating the reciprocal of the sum of a plurality of test metrics associated with the second set of test cases wherein the ratio also includes the reciprocal of the sum of the plurality of test metrics associated with the first set of test cases and the reciprocal of the sum of the plurality of test metrics associated with the second set of test cases. The evaluating of the energy selection logic may further include multiplying the ratio by a term that provides a bias for determining that the third set of test cases is to be generated based on one of the first set and the second set of test cases. The magnitude of the term that provides the bias for determining that the third set of test cases is to be generated based on one of the first set and the second set of test cases may less than one. The method may further include automatically testing the web service a first time using the third set of test cases automatically generating third learning data regarding the testing performed the first time using the third set of test cases automatically testing the web service a second time using the third set of test cases automatically generating fourth learning data regarding the testing performed the second time using the third set of test cases automatically determining at least in part based on the third and the fourth learning data whether a fourth set of test cases is to be generated and responsive to determining that the fourth set of test cases is to be generated automatically generating the fourth set of test cases. The method may further include calculating the sum of a plurality of test metrics included in the third learning data calculating the sum of a plurality of test metrics included in the fourth learning data comparing the sum of the plurality of test metrics included in the third learning data to the sum of the plurality of test metrics included in the fourth learning data wherein a result of the comparing is used to determine whether the fourth set of test cases is to be generated. The method may further include calculating the sum of a plurality of test metrics included in the fourth learning data and calculating the variance of the sum of the plurality of test metrics included in the fourth learning data wherein the variance is used to determine whether the fourth set of test cases is to be generated. The method may further include configuring an endpoint used to communicate with the web service wherein the web service metadata is obtained from the web service via endpoint. The method may further include receiving an indication of a Uniform Resource Locator URL associated with the web service wherein the endpoint is configured using the URL. The method may further include receiving an indication of a data communications protocol for communicating with the web service wherein the endpoint is configured using the indication of the data communications protocol.

A system that tests a web service may be summarized as including at least one processor at least one processor readable storage medium communicably coupled to the at least one processor the at least one processor readable storage medium storing instructions that when executed by the at least one processor cause the system to obtain web service metadata from the web service automatically generate a first set of test cases at least in part based on the web service metadata obtained from the web service automatically test the web service using the first set of test cases and automatically generate first learning data regarding the first set of test cases.

The first set of test cases may be automatically generated at least in part based on a name of a function included in the web service metadata obtained from the web service. The first set of test cases may be automatically generated at least in part based on a name of a variable included in the web service metadata obtained from the web service. The first set of test cases may be automatically generated at least in part based on a data type of a variable included in the web service metadata obtained from the web service. The first set of test cases may be automatically generated at least in part based on at least one constraint on a variable included in the web service metadata obtained from the web service. The at least one constraint on the variable may include at least one of a minimum value a maximum value and a string length. The instructions when executed by the at least one processor may cause the system to select one of a plurality of predetermined sets of test parameters based on the web service metadata and select a plurality of test parameters from the selected one of the plurality of predetermined sets of test parameters wherein the first set of test cases is automatically generated using the selected plurality of test parameters. The instructions when executed by the at least one processor may cause the system to generate a plurality of random numbers wherein the plurality of test parameters are selected from the selected one of the plurality of predetermined sets of test parameters using the plurality of random numbers. The instructions when executed by the at least one processor may cause the system to select one of a plurality of heuristic algorithm modules based on the web service metadata and obtain a plurality of test parameters using the selected one of the plurality of heuristic algorithm modules wherein the first set of test cases is automatically generated using the obtained plurality of test parameters. The instructions when executed by the at least one processor may cause the system to generate a plurality of random numbers wherein the selected one of the plurality of heuristic algorithm modules obtains the plurality of test parameters using the plurality of random numbers. The instructions when executed by the at least one processor may cause the system to automatically generate a second set of test cases at least in part based on the web service metadata obtained from the web service automatically test the web service using the second set of test cases automatically generate second learning data regarding the second set of test cases automatically determine at least in part based on the first and the second learning data whether a third set of test cases is to be generated at least in part based on the first or the second set of test cases responsive to determining that the third set of test cases is to be generated at least in part based on the first set of test cases automatically generate the third set of test cases at least in part based on the first set of test cases and responsive to determining that the third set of test cases is to be generated at least in part based on the second set of test cases automatically generate the third set of test cases at least in part based on the second set of test cases. The first learning data may be generated by associating each test case of the first set of test cases with at least one respective test metric and the second learning data may be generated by associating each test case of the second set of test cases with at least one respective test metric and the third set of test cases may be generated at least in part based on the first or the second learning data. The first learning data may be generated by associating each test case of the first set of test cases with at least one respective first code coverage metric and at least one respective first execution time metric the second learning data may be generated by associating each test case of the second set of test cases with at least one respective second code coverage metric and at least one respective second execution time metric and the third set of test cases may be generated at least in part based on the first or the second learning data. The instructions when executed by the at least one processor may cause the system to evaluate energy selection logic using the first and the second learning data wherein a result of evaluating the energy selection logic is used to the determine whether the third set of test cases is to be generated using the first or the second set of test cases. The instructions when executed by the at least one processor may cause the system to calculate the sum of a plurality of test metrics associated with the first set of test cases calculate the sum of a plurality of test metrics associated with the second set of test cases and calculate a ratio that includes the sum of the plurality of test metrics associated with the first set of test cases and the sum of the plurality of test metrics associated with the second set of test cases wherein the energy selection logic is evaluated using the calculated ratio. The instructions when executed by the at least one processor may cause the system to calculate the reciprocal of the sum of a plurality of test metrics associated with the first set of test cases calculate the reciprocal of the sum of a plurality of test metrics associated with the second set of test cases wherein the ratio also includes the reciprocal of the sum of the plurality of test metrics associated with the first set of test cases and the reciprocal of the sum of the plurality of test metrics associated with the second set of test cases. The instructions when executed by the at least one processor may cause the system to multiply the ratio by a term that provides a bias for determining that the third set of test cases is to be generated based on one of the first set and the second set of test cases. The magnitude of the term that provides the bias for determining that the third set of test cases is to be generated based on one of the first set and the second set of test cases may be less than one. The instructions when executed by the at least one processor may cause the system to automatically test the web service a first time using the third set of test cases automatically generate third learning data regarding the testing performed the first time using the third set of test cases automatically test the web service a second time using the third set of test cases automatically generate fourth learning data regarding the testing performed the second time using the third set of test cases automatically determine at least in part based on the third and the fourth learning data whether a fourth set of test cases is to be generated and responsive to determining that the fourth set of test cases is to be generated automatically generate the fourth set of test cases. The instructions when executed by the at least one processor may cause the system to calculate the sum of a plurality of test metrics included in the third learning data calculate the sum of a plurality of test metrics included in the fourth learning data and compare the sum of the plurality of test metrics included in the third learning data to the sum of the plurality of test metrics included in the fourth learning data wherein a result of comparing the sum of the plurality of test metrics included in the third learning data to the sum of the plurality of test metrics included in the fourth learning data is used to determine whether the fourth set of test cases is to be generated. The instructions when executed by the at least one processor may cause the system to calculate the sum of a plurality of test metrics included in the fourth learning data and calculate the variance of the sum of the plurality of test metrics included in the fourth learning data wherein the variance is used to determine whether the fourth set of test cases is to be generated. The instructions when executed by the at least one processor may cause the system to configure an endpoint used to communicate with the web service wherein the web service metadata is obtained from the web service via endpoint. The instructions when executed by the at least one processor may cause the system to receive an indication of a Uniform Resource Locator URL associated with the web service wherein the endpoint is configured using the URL. The instructions when executed by the at least one processor may cause the system to receive an indication of a data communications protocol for communicating with the web service wherein the endpoint is configured using the indication of the data communications protocol.

In the following description certain specific details are set forth in order to provide a thorough understanding of various disclosed embodiments. However one skilled in the relevant art will recognize that embodiments may be practiced without one or more of these specific details or with other methods components materials etc. In other instances well known structures associated with computer systems have not been shown or described in detail to avoid unnecessarily obscuring descriptions of the embodiments.

Unless the context requires otherwise throughout the specification and claims which follow the word comprise and variations thereof such as comprises and comprising are to be construed in an open inclusive sense that is as including but not limited to. 

Reference throughout this specification to one embodiment or an embodiment means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment. Thus the appearances of the phrases in one embodiment or in an embodiment in various places throughout this specification are not necessarily all referring to the same embodiment. Furthermore the particular features structures or characteristics may be combined in any suitable manner in one or more embodiments.

As used in this specification and the appended claims the singular forms a an and the include plural referents unless the content clearly dictates otherwise. It should also be noted that the term or is generally employed in its sense including and or unless the content clearly dictates otherwise.

The Web Services Integration Testing System communicates with a Web Service Under Test via a communications network for example a system bus a wireless local area network WLAN a local area network LAN or a wide area network WAN . The Web Services Integration Testing System requests and receives web service metadata from the Web Service Under Test for example using a conventional web service metadata exchange protocol. The Stochastic Variation Module generates or selects test parameters using the web service metadata received from the Web Service Under Test configurable heuristic algorithm modules and or stochastic processes.

More particularly the Stochastic Variation Module includes computer executable instructions for generating or selecting test parameters using the web service metadata configurable heuristic algorithm modules predetermined test parameters and stochastic processes. The Stochastic Variation Module may parse the web service metadata using keywords which may be based on a particular type of the web service metadata e.g. OData or GData to identify web service information that is used to generate test parameters. The Stochastic Variation Module may parse the web service metadata to identify function s function names s input variable name s input variable data type s and or constraint s on input variable s . The Stochastic Variation Module uses results of the parsing to select heuristic algorithm modules that are then used to select or generate test parameters.

For example the Stochastic Variation Module may parse the web service metadata and determine that the Web Service Under Test includes a function that takes as input a first variable of type String named Name having a maximum string length of 255 characters a second variable of type String named Address having a maximum string length of 255 characters and a third variable of type Int32 named ID having a value that is between 1 and 1000. The Stochastic Variation Module then uses one or more heuristic algorithm modules that are logically associated with string variables to generate a set of test parameters for the variables named Name and Address and one or more heuristic algorithm modules that are logically associated with integer variables to generate a set of test parameters for the variable named ID .

The Stochastic Variation Module may associate a first heuristic algorithm module with string variables having names that include the word name . The first heuristic algorithm module may sequentially select or employ a first stochastic process to randomly select a number of test parameters from a first predetermined set of test parameters associated with string variables having names that include the word name . If the first predetermined set of test parameters includes 100 predetermined test parameters the first heuristic algorithm module may cause the first stochastic process to generate random numbers that are uniformly distributed between 0 and 99. For example the first heuristic algorithm module may use a stochastic process included in the Microsoft C Random class to generate three random numbers 23 47 and 78 and then use those random number to select test parameters from the first predetermined set of test parameters having indices of 23 47 and 78 for inclusion in a set of test parameters for the variable named Name .

The Stochastic Variation Module also may associate a second heuristic algorithm module with string variables having names that include the word address . The second heuristic algorithm module may employ a second stochastic process to randomly select a number of test parameters from a second predetermined set of test parameters associated with string variables having names that include the word address . If the second predetermined set of test parameters includes 200 test parameters the second heuristic algorithm module may cause the second stochastic process to generate random numbers uniformly distributed between 0 and 199. For example the second stochastic process may generate three random variables 29 141 and 154 and the second heuristic algorithm module may select the 29 141 and 154test parameters from the second predetermined set of test parameters for inclusion in a set of test parameters for the variable named Address .

Additionally the Stochastic Variation Module may associate a third heuristic algorithm module with integer variables that have defined minimum and maximum values. For example the third heuristic algorithm module may cause the Stochastic Variation Module to generate a set of test parameters that include the following values i one minus the minimum value i the minimum value iii one plus the minimum value iv the average of the minimum and the maximum values v one minus the maximum value vi the maximum value and vii one plus the maximum value.

The third heuristic algorithm module also may employ a third stochastic process to generate test parameters in a number of ranges. For example a first range may be defined as the minimum value minus 1000 through the minimum value minus two a second range may be defined as the average of the minimum and the maximum values plus and minus ten percent of the difference between the maximum and the minimum values a third range may be defined as the maximum value plus two through the maximum value plus 1000. The third heuristic algorithm module may employ the third stochastic process to generate test parameters in each of those ranges. Additionally the third heuristic algorithm module may include rules for altering those ranges. For example one such rule may cause the third heuristic algorithm module to expand the first range to include the minimum value minus 10 000 through the minimum value minus two after the third heuristic algorithm module has generated 100 test parameters for a particular variable.

The Stochastic Variation Module implements a Markov chain for each input variable of each function of the Web Service Under Test . A Markov chain can be represented as shown in Equation 1. Equation 1

The possible values of Xform a finite variation of test parameters that form the state space of the Markov chain. The state space of the Markov chain for each variable may be characterized using a model implemented by a heuristic algorithm module which uses predetermined rules to generate best guesses for values of test parameters. In between rounds of testing such a heuristic algorithm module may generate a new set of test parameters for a variable corresponding to X. Then based on feedback data obtained from a system hosting the Web Service Under Test for example the state of Xis accepted or rejected.

The probability of making a transition from a current state s to a new candidate state s is defined by an acceptance probability function P s s T that depends on the energies e E s and e E s of the two states and on a global time varying parameter T called the temperature. P s s T may be defined such that a state with a larger energy is more desirable than a state with a smaller energy or such that a state with a smaller energy is more desirable than a state with a larger energy. Equation 2 shows an example of energy selection logic for P s s T wherein a state with a larger energy is more desirable than a state with a smaller energy.

Because the energy selection logic in Equation 2 is defined such that a state having a larger energy is more desirable than a state having a smaller energy the quantities ExecutionTime s and ExecutionTime s in Equation 2 are reverse or reciprocal execution time metrics. For example if an execution time metric for the current state s is 2 seconds the reciprocal of the execution time metric i.e. 0.5 is used for ExecutionTime s and if an execution time metric for the new candidate state s is 4 seconds the reciprocal of the execution time metric i.e. 0.25 is used for ExecutionTime s in Equation 2. Accordingly the reverse execution time metric for the current state s i.e. 0.5 is larger than the reverse execution time metric for the new candidate state s i.e. 0.25 because the execution time metric for the current state s is more desirable than the execution time metric for the new candidate state s and the energy selection logic is defined such that a state with a larger energy is more desirable than a state with a smaller energy.

The parameter T in Equation 2 prevents the test parameters generated by the Stochastic Variation Module from becoming stuck at a local minimum that is worse than a global minimum. This enables the Stochastic Variation Module to escape a local minimum where all nearby states are inferior but a better state exists through more iteration. When T 0 in Equation 2 an algorithm used to evaluate P s s T may approximate the greedy algorithm by making transitions to new potential states only when the following ratio is greater than one.

In that connection the quantity 1 T in Equation 2 is defined to be greater than one to provide a bias to a new candidate state s that has lower energy i.e. less desirable than the current state s. For example the value of T may be selected as 0.01 to provide a slight bias to a new candidate state s that has slightly lower energy i.e. slightly less desirable than the current state s.

Equation 3 shows an example of energy selection logic for P s s T wherein a state with a smaller energy is more desirable than a state with a larger energy.

Because the energy selection logic in Equation 3 is defined such that a state having a smaller energy is more desirable than a state having a larger energy the quantities CodeCoverage s and CodeCoverage s in Equation 3 are reverse or reciprocal code coverage metrics. For example if a code coverage metric for the current state s is 75 the reciprocal of the code coverage metric i.e. 1.33 is used for CodeCoverage s and if a code coverage metric for the new candidate state s is 50 the reciprocal of the code coverage metric i.e. 2.0 is used for CodeCoverage s in Equation 3. Accordingly the reverse code coverage metric for the current state s i.e. 1.33 is smaller than the reverse code coverage metric for the new candidate state s i.e. 2.0 because the code coverage metric for the current state s is more desirable than the code coverage metric for the new candidate state s and the energy selection logic is defined such that a state with a smaller energy is more desirable than a state with a larger energy.

Additionally the quantity 1 T in Equation 3 is defined to be less than one to provide a bias to a new candidate state s that has a higher energy i.e. less desirable than the current state s. For example the value of T may be selected as 0.01 to provide a slight bias to a new candidate state s that has a slightly higher energy i.e. slightly less desirable than the current state s.

Code coverage and execution time are merely two examples of types of metrics that can be optimized by the Stochastic Variation Module . The energy selection logic for P s s T may include other types of metrics. For example P s s T can be defined to include a combination of code coverage metrics such as a combination of statement coverage metrics decision coverage metrics condition coverage metrics path coverage metrics and covered blocks metrics. Additionally P s s T can be defined to include other types of metrics for example web server system parameter metrics e.g. memory utilization metrics load information metrics e.g. CPU utilization metrics network metrics e.g. network bandwidth metrics and performance data metrics e.g. response time metrics .

The Stochastic Variation Module provides the sets of test parameters to the Test Case Generator Module . The Test Case Generator Module generates test cases using the test parameters provided by the Stochastic Variation Module the web service metadata provided by the Web Service Under Test and one or more heuristic algorithm modules. For example if the Test Case Generator Module parses the web service metadata and determines that that the Web Service Under Test implements persistent storage functions e.g. database functions the Test Case Generator Module may generate a corresponding set of test cases. For example the Test Case Generator Module may generate a first test case that attempts to create an entry using a first test parameter a second test case that attempts to read an entry using a second test parameter a third test case that attempts to update an entry using a third test parameter a fourth test case that attempts to delete an entry using a fourth test parameter and a fifth test case that attempts to perform a search using a sixth test parameter.

The Test Case Generator Module may generate test cases that include Arrange Act and Assert AAA sections. The Arrange section initializes objects and sets the value of the data that is passed to a function of the Web Service Under Test . The Act section invokes the Web Service Under Test with one or more test parameters. The Assert section verifies that the action of the function of the Web Service Under Test test behaves as expected. The Test Case Generator Module provides the test cases to the Test Runner Module .

The Test Runner Module performs testing of the Web Service Under Test using the test cases. That is for each of the test cases the Test Runner Module invokes a function of the Web Service Under Test described in the web service metadata providing one or more of the test parameters as input records any data returned by the function and determines whether the response returned by the function matches an expected response defined for the test case. For example if a function of the Web Service Under Test is expected to return a particular value the Test Runner Module verifies that the correct value is returned.

The Test Runner Module also generates test results data that for each test case may identify the test case and the test parameters a date and time when a test corresponding to the test case was initiated a date and time when a response from the Web Service Under Test was received if any any values included in the response such as results or error codes and an indication regarding whether the Web Service Under Test passed the test. The Test Runner Module provides the test result data to the Self Learning Module .

The Self Learning Module generates learning data by associating data with each of the test cases. In addition to the data included in the test results data e.g. data indicating whether the Web Service Under Test passed or failed a test corresponding to a test case the Self Learning Module associates feedback data with each of the test cases. The feedback data may be provided by the Web Service Under Test and the Additional Data Providers including the Production Log and or the Human Judges . Such feedback data may include for each test case one or more code coverage metrics an execution time metric and or system performance metrics. The code coverage metrics may include data indicating statement coverage decision coverage condition coverage path coverage and or covered blocks. The execution time metric may indicate the amount of time taken to execute a test using a function of the Web Service Under Test . The system performance metrics may include data indicating processor utilization memory utilization and or network utilization for example.

The Human Judges and or automated software tools running on one or more servers that host the Web Service Under Test may inspect the Production Log and or other logs and provide other useful information derived from memory dump data stack trace data and or software environment data to the Self Learning Module . The software development environment data may include data from the Common Language Runtime CLR virtual machine component of Microsoft s .NET framework the Java Runtime Environment JRE or a particular language environment e.g. C . For example if the Web Service Under Test terminates abnormally during testing using a test case memory dump data stack trace data and or software environment data may be analyzed to determine why the test case caused the Web Service Under Test to terminate abnormally. If a test case causes the Web Service Under Test to terminate abnormally the test case may be associated with data that makes it more likely that the test case would be selected again in futures rounds of testing.

The Self Learning Module also may associate a test case with data indicating that the test case is deemed to be exceptional. For example if a code coverage metric associated with a first test case is less than or equal to a first value the Self Learning Module may generate learning data in which the first test case is associated with data that makes it less likely the first test case would be selected again for use in future testing. Similarly if a code coverage metric associated with a second test case is greater than or equal to a second value the Self Learning Module may generate learning data in which the second test case is associated with data that makes it more likely the second test case would be selected again for use in future testing. The Stochastic Variation Module may process the learning data and identify test parameters that are deemed to provide exceptionally good code coverage and randomly select a number of those test parameters for future rounds of testing.

The Optimal Control Module uses the learning data to determine whether the most recently used set of test cases meet one or more acceptability criteria. If the Optimal Control Module determines that the most recently used set of test cases does not meet the acceptability criteria the Optimal Control Module may instruct the Stochastic Variation Module to generate a new set of test parameters and instruct the Test Case Generator Module to generate a new set of test cases using the new set of test parameters.

If the Optimal Control Module determines that the most recently used set of test cases meets the acceptability criteria the Optimal Control Module may cause the Test Runner Module to perform testing of the Web Service Under Test using that set of test cases on a regular basis e.g. daily . The Optimal Control Module monitors the test results data and or the learning data to determine whether that set of test cases is producing different test results e.g. lower code coverage or higher execution time which is indicative of changes to the Web Service Under Test . The Web Services Integration Testing System may perform the above process continually such that new tests cases are generated automatically when changes to the Web Service Under Test are detected.

For example if testing is performed on a daily basis for several weeks using a set of test cases and each time a code coverage metric of 96 is obtained and then testing is performed and a code coverage metric of 81 is obtained the Optimal Control Module may determine that the Web Service Under Test has changed. In response the Optimal Control Module may instruct the Stochastic Variation Module to generate a new set of test parameters and instruct the Test Case Generator Module to generate a new set of test cases using the new set of test parameters. By way of another example if testing is performed on a daily basis for several weeks using a set of test cases and each time an execution time metric of approximately one minute is obtained and then testing is performed and an execution time metric of ten minutes is obtained the Optimal Control Module may determine that the Web Service Under Test has changed. In response the Optimal Control Module may instruct the Stochastic Variation Module to generate a new set of test parameters and instruct the Test Case Generator Module to generate a new set of test cases using the new set of test parameters. In that connection the Optimal Control Module may calculate statistics including a mean and a variance e.g. standard deviation for one or more test result metrics e.g. execution time metrics if a particular test result metric varies from the mean by more than a predetermined number of standard deviations e.g. the Optimal Control Module may determine that the web service has changed and cause a new set of test parameters and a new set of test cases to be generated.

In one embodiment the Web Services Integration Testing System is designed in the Microsoft .NET Framework C language by utilizing run time Microsoft T4 text templates preprocessed templates and an XML configuration file system. In Microsoft Visual Studio VS a T4 text template is a mixture of text blocks and control logic that can generate a text file. The control logic is written as fragments of program code in Visual C or Visual Basic. The generated files are C .cs files with a set of test methods and test fixtures executable by Microsoft Test Manager or VS test runners. Each template is a mixture of the text as it will appear in a generated string and fragments of program code. The program fragments supply values for the variable parts of the string and also control conditional and repeated parts. Utilizing a set of T4 templates makes it is easier to see the final form of the output than using a long series of write statements for example. Additionally making changes to the output of generated test cases is easier and more reliable.

The computer system may include one or more processing units collectively processing unit a system memory and a system bus that couples various system components including the system memory to the processing units . The processing units may be any logic processing unit such as one or more central processing units CPUs digital signal processors DSPs application specific integrated circuits ASICs field programmable gate arrays FPGAs etc. The system bus can employ any known bus structures or architectures including a memory bus with memory controller a peripheral bus and a local bus. The system memory includes read only memory ROM and random access memory RAM . A basic input output system BIOS which can form part of the ROM contains basic routines that help transfer information between elements within the computer system such as during start up.

The computer system also may include a plurality of interfaces such as a network interface and an interface supporting a modem or any other wireless wired interfaces.

The computer system may include a hard disk drive for reading from and writing to a hard disk an optical disk drive for reading from and writing to removable optical disks and or a magnetic disk drive for reading from and writing to magnetic disks . The optical disk can be a CD ROM while the magnetic disk can be a magnetic floppy disk or diskette. The hard disk drive optical disk drive and magnetic disk drive may communicate with the processing unit via the system bus . The hard disk drive optical disk drive and magnetic disk drive may include interfaces or controllers not shown coupled between such drives and the system bus as is known by those skilled in the relevant art. The drives and and their associated computer readable storage media may provide non volatile and non transitory storage of computer readable instructions data structures program modules and other data for the computer system .

Although the depicted computer system is illustrated employing the hard disk drive optical disk drive and magnetic disk drive those skilled in the relevant art will appreciate that other types of computer readable storage media that can store data accessible by a computer may be employed such as magnetic cassettes flash memory Bernoulli cartridges RAMs ROMs smart cards etc. For example computer readable storage media may include but is not limited to random access memory RAM read only memory ROM electrically erasable programmable read only memory EEPROM flash memory compact disc ROM CD ROM digital versatile disks DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices solid state memory or any other medium which can be used to store the desired information and which may be accessed by processing unit

Program modules can be stored in the system memory such as an operating system one or more application programs other programs or modules and program data . Application programs may include instructions that cause the processor s to generate random numbers generate test parameters generate test cases run tests using the generated test cases obtain feedback data generate learning data and modify the test parameters based on the learning data. Other program modules may include instructions for handling security such as password or other access protection and communications encryption. The system memory also may include communications programs for example a Web client for permitting the computer system to access and exchange data with Web servers of the Internet corporate intranets extranets or other networks and devices as described herein as well as other server applications on server computing systems such as the Web Service Under Test . The program data may store predetermined sets of test parameters test cases test results data learning data and historical data. The Web client in the depicted embodiment is markup language based such as Hypertext Markup Language HTML Extensible Markup Language XML or Wireless Markup Language WML and operates with markup languages that use syntactically delimited characters added to the data of a document to represent the structure of the document. A number of Web clients or browsers are commercially available such as those from Mozilla Google and Microsoft.

While shown in as being stored in the system memory the operating system application programs other programs modules program data and client can be stored on the hard disk of the hard disk drive the optical disk of the optical disk drive and or the magnetic disk of the magnetic disk drive .

An operator can enter commands and information into the computer system through input devices such as a keyboard and or a pointing device such as a mouse and or via a graphical user interface. Other input devices can include a microphone joystick game pad tablet scanner etc. These and other input devices are connected to one or more of the processing units through the interface such as a serial port interface that couples to the system bus although other interfaces such as a parallel port a game port or a wireless interface or a universal serial bus USB can be used. A monitor or other display device is coupled to the system bus via a video interface such as a video adapter. The computer system can include other devices such as speakers printers etc.

The computer system can operate in a networked environment using logical connections to one or more remote computers and or devices described with reference to . For example the computer system can operate in a networked environment using logical connections to one or more mobile devices landline telephones and other service providers or information servers. Communications may be via a wired and or wireless network architecture for instance wired and wireless enterprise wide computer networks intranets extranets telecommunications networks cellular networks paging networks and other mobile networks.

It should be understood that the various techniques described herein may be implemented in connection with hardware software and or firmware or where appropriate with a combination of such. Thus the methods and apparatuses of the disclosure or certain aspects or portions thereof may take the form of program code i.e. instructions embodied in tangible media such as floppy diskettes CD ROMs DVDs hard drives flash drives or any other machine readable or processor readable storage medium wherein when the program code is loaded into and executed by a machine such as a processor of a computer or mobile device the machine becomes an apparatus for practicing various embodiments. In the case of program code execution on programmable computers or mobile devices such generally includes a processor a storage medium readable by the processor including volatile and non volatile memory and or storage elements at least one input device and at least one output device. One or more programs may implement or utilize the processes described in connection with the disclosure e.g. through the use of an API reusable controls or the like. Such programs are preferably implemented in a high level procedural or object oriented programming language to communicate with a computer system that hosts the Web Service Under Test shown in . However the program s can be implemented in assembly or machine language if desired. In any case the language may be a compiled or interpreted language and combined with hardware implementations.

In one embodiment the Web Services Integration Testing System configures the endpoint by processing a Windows Communication Foundation WCF client configuration file that includes address and binding information for the endpoint. The WCF client file may be created by specifying address information that uniquely identifies the endpoint for example a Universal Resource Locator URL corresponding to the Web Service Under Test and one or more variables included in the binding information specifying how to communicate with the endpoint for example a variable that specifies a transport protocol such as TCP or HTTP.

At the Web Services Integration Testing System obtains web service metadata via the endpoint configured at . In one embodiment the Web Services Integration Testing System obtains the web service metadata using the WS MetadataExchange MEX protocol.

In one embodiment the Web Services Integration Testing System obtains a Service Metadata Document at . The Service Metadata Document describes the data e.g. structure and organization of all the resources exposed as HTTP endpoints by the web service. The Service Metadata Document describes data in Entity Data Model EDM terms using the Extensible Markup Language XML language for describing models called the Conceptual Schema Definition Language CSDL . When exposed by an OData service as a Service Metadata Document the CSDL document is packed using the format described in EDMX. An .edmx file is an XML file that defines a model that can be used with Microsoft s Entity Framework EF . The model is made up of a conceptual model a storage model and the mapping between those models. An .edmx file also contains information that is used by the EF Designer to render a model graphically.

At the Web Services Integration Testing System generates an initial set of test parameters for testing the Web Service Under Test . The Web Services Integration Testing System may generate the initial set of test parameters by analyzing parameters structures and functions of the Web Service Under Test that are described in the web service metadata obtained at and by using one or more heuristic algorithm modules that are selected based on the results of the analyzing.

If the web service metadata is described in EDM terms the web services integration testing system may parse the metadata and extract data relating to EDM data types. EDM data types may include Null Binary Boolean Byte DateTime Decimal Double Single Guid Intl 6 Int32 Int64 SByte String Time and DateTimeOffset. If the web services metadata indicates that a function takes as input a variable of type String the Web Services Integration Testing System may select a heuristic algorithm module associated with string variables to generate the test parameters for example as described above in connection with . For example the application programs may include instructions that cause the processing unit to implement a plurality of heuristic algorithm modules associated with string variables and to select a particular heuristic algorithm module based on the name of a string variable.

More particularly if the name of a string variable includes last or Iname the Web Services Integration Testing System may select a heuristic algorithm module that generates test parameters for family names which may include special characters such as apostrophes. If the name of a string variable includes address or addr the Web Services Integration Testing System may select a heuristic algorithm module that generates postal addresses which may include a combination of letters and numbers having one or more predetermined formats for example.

The Web Services Integration Testing System then generates the initial set of test parameters using the selected heuristic algorithm module s . For example if the web service metadata obtained at indicates that the Web Service Under Test takes a variable Name as input that is of type String and that can be from null to 255 characters in length the selected heuristic algorithm module may cause the Web Services Integration Testing System to generate or select a set of test parameters that includes one or more predetermined control characters e.g. 0 . The Web Services Integration Testing System may store an array of predetermined test parameters for string variables that have been generated based on such rules or heuristic algorithm modules and randomly select a number of test parameters for the initial set of test parameters. For example the Web Services Integration Testing System may generate the initial set of test parameters by randomly selecting a number of test parameters from a predetermined set of test parameters for string variables that includes null test test 0 test0 test1 test2 etc.

The Web Services Integration Testing System then generates the initial set of test cases using the test parameters included in the initial set of test parameters. For example each test case includes a string of text that can be processed by Test Runner Module to invoke a function of the Web Service Under Test using one or more test parameters as input. Each test case also may include a result that is expected to be returned by the Web Service Under Test so that a determination can be made as to whether the Web Service Under Test passed a test included in the test case.

At the Web Services Integration Testing System runs tests using the test cases generated at . For example the Test Runner Module runs the tests by sending a plurality of messages via the testing endpoint configured at wherein each message includes one or more of the test parameters generated at . That is the Test Runner Module provides a first message corresponding to a first test case records any test results data received in response to the first message and then repeats this process for the remaining test cases. The test message may be formatted according to the Simple Object Access Protocol SOAP or the Representational State Transfer REST architecture for example.

At the Web Services Integration Testing System obtains feedback data. The feedback data may include for each test case test results metadata indicating an execution time metric and one or more code coverage metrics such as statement coverage data decision coverage data condition coverage data path coverage data and or covered blocks data for example. The feedback data also may include data regarding other parameters that are desired to be optimized such as server system parameters load information performance data. For example the feedback data may include data regarding CPU utilization memory usage memory swapping memory paging network latency network throughput network bandwidth and network utilization.

The Web Services Integration Testing System may obtain the feedback data e.g. via network interface from a computer system that hosts the Web Service Under Test . The Web Services Integration Testing System also may obtain additional feedback data from a user e.g. via keyboard .

At the Web Services Integration Testing System generates learning data based on the feedback data obtained at . For example the Self Learning Module associates each test case used to run the tests at with data indicating whether the web service provided a response and if so whether the response was the correct or expected response one or more code coverage metrics an execution time metric whether the test case is deemed to be an outlier case a CPU utilization metric a response time metric a memory utilization metric and or a variance of one or more of the foregoing metrics.

At the Optimal Control Module determines whether the tests run at are in an optimal state using the feedback data obtained at . The Optimal Control Module may compare values included in the feedback data to predetermined or operator specified threshold values to determine whether the test is in an optimal state. For example the processing unit may cause code coverage data indicating that a set of test cases resulted in statement coverage of 71 to be compared to a threshold value for minimum statement coverage of 95 cause execution time data indicating that the set of test cases took 2 minutes to be executed to be compared to a threshold value for maximum execution time of 60 minutes and then determine that the set of test cases is not in an optimal state based on those comparisons. That is if the processing unit determines that the set of test cases run at resulted in statement coverage that is less than the threshold value for minimum statement coverage or determines that the execution time of the set of test cases is greater than the threshold value for maximum execution time the processing unit determines that the set of test cases run at are not in an optimal state.

If the Optimal Control Module determines at that the set of test cases run at are in an optimal state the Optimal Control Module causes to be repeated using the same set of test cases. For example the Optimal Control Module may cause the Test Runner Module to run the same set of test cases on a daily nightly basis. If the Optimal Control Module determines at that the set of test cases is not in an optimal state the method proceeds to .

At the Optimal Control Module determines whether current test results are better than previous test results if any. If tests have been run only once at the Optimal Control Module determines that the current test results are better than previous test results. If tests have been run more than once at the Optimal Control Module uses energy selection logic to determine whether the results for a current set of test cases are better than the results for the previous set of test cases.

For example the processing unit may evaluate the energy selection logic shown in Equation 3 using a software module stored in the applications programs and a value of the variable T and learning data stored in the program data . The learning data is used to calculate values for CodeCoverage s ExecutionTime s CodeCoverage s and ExecutionTime s . For example the processing unit may calculate the value for ExecutionTime s by summing the execution time metrics associated with each test case from the previous set of test cases and calculate the value for ExecutionTime s by summing the execution time metrics associated with each test case from the current set of test cases. Additionally the processing unit may calculate the value for CodeCoverage s by taking the reciprocal of the sum of the code coverage metrics associated with each test case from the previous set of test cases and calculate the value for CodeCoverage s by taking the reciprocal of the sum of the code coverage metrics associated with each test case from the current set of test cases.

If the calculated value of P s s T is less than 1 the processing unit determines that the test results for the current set of test cases are better than the test results for the previous set of test cases and confirms the current state. For example the processing unit causes the current set of test cases to be copied to an area of memory reserved for test cases that will be used to generate the next set of test cases or otherwise causes the current set of test cases to be associated with the current state. If the calculated value of P is not less than 1 the processing unit determines that the test results for the current set of test cases are not better than the test results for the previous set of test cases and the method proceeds to .

At the Web Services Integration Testing System reverts to the previous state. For example the processing unit causes the previous set of test cases to be copied to an area of memory reserved for test cases that will be used to generate the next set of test cases or otherwise causes the previous set of test cases to be associated with the current state. The method then proceeds to .

At the Web Services Integration Testing System analyzes the learning data and determines which of the test cases associated with the current state are providing acceptable test results and which test parameters need to be altered. The Web Services Integration Testing System may use the learning data associated with each test case included in the set of test cases associated with the current state to rank the test cases according to execution time and or code coverage. Test cases that are not highly ranked may be updated with different test parameters.

Additionally or alternatively the Web Services Integration Testing System may use heuristic algorithm modules to evaluate test parameters included in the test cases associated with the current state. For example if the test cases associated with the current state are not producing acceptable results and the test parameters used in those test cases were obtained using a heuristic algorithm module the heuristic algorithm module may be used to identify one or more of the test parameters that are to be changed and to identify one or more of the test parameters that are not to be changed.

At the Web Services Integration Testing System generates a new set of test parameters using the test parameters included in the set of test cases associated with the current state the results of the analysis performed at the web service metadata and possibly other inputs e.g. from an operator . The Web Services Integration Testing System may use the same heuristic algorithm module used at or a different one to generate or select additional test parameters for inclusion in the new set of test cases along with the test parameters selected at for inclusion in the new set of test cases. For example the new set of test cases may include one or more of the test parameters used in test cases associated with the current state that have produced good code coverage metrics in additional to one or more test parameters that have been randomly selected from a predetermined set of test parameters that are associated with a particular type of variable e.g. string variables . The Test Case Generator Module then generates a new set of test case that include the new set of test parameters.

At the Web Services Integration Testing System determines whether the testing is to be ended. The Web Services Integration Testing System may determine whether the testing is to be ended by checking the value of one or more variables that define a software environment of Web Services Integration Testing System . For example the processing unit may cause the monitor to display a selectable icon that when selected by an operator causes a first variable to have a predetermined value e.g. 0 . The processing unit may determine that the testing is to be ended when the first variable has the predetermined value. Additionally or alternatively the processing unit may initialize a second variable to a value of zero when the method is started at and increment the second variable each time tests are run at . The processing unit may determine that the testing is to be ended when the second variable has a predetermined value e.g. 1 000 . In such a case the processing unit may cause a message to be generated and displayed via the monitor or sent via the network interface . For example the processing unit may cause an e mail message to be sent to a system administrator indicating that the Web Service Under Test has been tested using 1 000 sets of test cases and an optimal state has not been reached. In response the system administer may modify the test parameters and cause the process to be resumed.

If the Web Services Integration Testing System determines at that testing is to be ended the method ends. If Web Services Integration Testing System determines at that testing is not to be ended the method returns to and testing is performed using the new set of test cases generated at .

The Test Case Generator Module provides the set of test cases corresponding to State A to the Test Runner Module which performs testing of the function of the Web Service Under Test using those test cases. That is the Test Runner Module invokes the function multiple times each time providing one of the test parameters as input. The Test Runner Module may generate test result data by associating each test case with an indication regarding whether the Web Service Under Test returned the expected result for the test case for example. The Test Runner Module provides the test results data to the Self Learning Module .

The Self Learning Module uses the test results data and other data e.g. test results metadata to generate learning data. The Web Service Under Test or software executing on a system hosting the Web Service Under Test may provide the test results metadata to the Self Learning Module . The test results metadata includes a code coverage metric for each of the test cases. The Self Learning Module generates the learning data by associating a respective code coverage metric and a respective test result with each of the test cases. The Self Learning Module also sums the code coverage metrics associated with each of the test cases corresponding to State A and determines that a code coverage metric for State A is 71 .

For simplicity only one type of metric is discussed in this example i.e. a code coverage metric and only four states are shown in . The learning data may associate each test case with more than one code coverage metric e.g. a statement coverage metric a decision coverage metric a condition coverage metric a path coverage metric and a covered blocks metric an execution time metric and or metrics for other criteria to be optimized. For example the learning data may associate each test case with data indicating one or more of the following a response time metric a CPU utilization metric and a memory usage metric. Additionally the testing may have a different number of states.

The Optimal Control Module determines whether the set of test cases corresponding to State A is in an optimal state. The Optimal Control Module may use a predetermined acceptability criterion of at least 95 code coverage. The Optimal Control Module determines that the test cases corresponding to State A are not in an optimal state because the code coverage metric corresponding to State A i.e. 71 is not at least 95 . As a result the Optimal Control Module instructs the Stochastic Variation Module to generate a new set of test parameters using the test parameters included in the test cases corresponding to State A. For example the processing unit selects the test test parameter included the test cases corresponding to State A and adds numbers and or control characters according to a heuristic algorithm module. In this example a new set of test parameters includes test1 test2 and test 0 .

The Test Case Generator Module then generates a new set of test cases corresponding to State C using the new set of test parameters. The Test Case Generator Module provides the new set of test cases to the Test Runner Module which performs testing of the Web Service Under Test using the test cases. The Test Runner Module generates corresponding test results data and provides it to the Self Learning Module . Additionally the Web Service Under Test provides corresponding feedback data e.g. test results metadata including a code coverage metric for each of the test cases to the Self Learning Module . The Self Learning Module generates learning data by associating a respective code coverage metric and a respective code test result with each of the test cases. The Self Learning Module also sums the code coverage metrics associated with each test case included in the set of test cases corresponding to State C and determines that a code coverage metric for State C is 70 .

The Optimal Control Module determines whether the set of test cases corresponding to State C is in an optimal state. The Optimal Control Module determines that the test cases corresponding to State C are not in an optimal state because the code coverage metric corresponding to State C i.e. 70 is not at least 95 . The Optimal Control Module then determines whether the test results for the current state i.e. State C are better than the results test for the previous state i.e. State A . For example the Optimal Control Module evaluates Equation 4 with T equal to 0.01 CodeCoverage s equal to the reverse code coverage metric for State A and CodeCoverage s equal to the reverse code coverage metric for State C.

More particularly the Optimal Control Module first determines that 1 0.01 1 0.70 1 0.71 1.004. Because 1.004 is not less than 1 the Optimal Control Module determines that P s s T 0. That is the Optimal Control Module determines that the test results for State C are not better than the results test for the previous state i.e. State A . Accordingly the Optimal Control Module causes the current state to revert back to State A and instructs the Stochastic Variation Module to generate a new set of test parameters based on the test parameters corresponding to State A. The new set of test parameters include null test test 0 and test0 .

The Test Case Generator Module then generates four test cases corresponding to State B. The Test Case Generator Module provides the test cases to the Test Runner Module which performs testing of the Web Service Under Test using the test cases. The Test Runner Module generates and provides corresponding test results data to the Self Learning Module . Additionally the Web Service Under Test provides corresponding feedback data including a code coverage metric for each of the test cases to the Self Learning Module . The Self Learning Module generates learning data by associating each of the test cases with a respective code coverage metric. The Self Learning Module also sums the code coverage metrics associated with each test case included in the set of test cases corresponding to State B and determines that a code coverage metric for the set of test cases corresponding State B is 72 .

The Optimal Control Module determines whether the set of test cases corresponding to State B is in an optimal state. The Optimal Control Module determines that the test cases corresponding to State B are not in an optimal state because the code coverage metric corresponding to State B i.e. 72 is not at least 95 . The Optimal Control Module then determines whether the test results for the current state i.e. State B are better than the results test for the previous state i.e. State A using Equation 4.

More particularly the Optimal Control Module determines that 1 0.01 1 0.72 1 0.71 0.977. Because 0.977 is less than 1 the Optimal Control Module determines that P s s T 1. That is the Optimal Control Module determines that the test results for the current state i.e. State B are better than the results test for the previous state i.e. State A . Accordingly the Optimal Control Module confirms the current state as State B and instructs the Stochastic Variation Module to generate a new set of test parameters based on the test parameters corresponding to State B.

The Stochastic Variation Module then generates a new set of test parameters using the test parameters included in the set of test cases corresponding to State B. The Test Case Generator Module generates a new set of test cases using the new set of test parameters. In this example the new set of test cases corresponds to State C. The Optimal Control Module evaluates the new set of test cases using historical data e.g. included in the learning data and determines that the new set of test cases corresponds to a set of test cases that has been used already for testing. Accordingly the Optimal Control Module instructs the Stochastic Variation Module to generate another new set of test parameters based on the test parameters included in the test cases corresponding to State B.

The Stochastic Variation Module then generates a new set of test parameters null test test 0 and . The Test Case Generator Module uses the new set of test parameters to generate a new set of test cases corresponding to State D which becomes the new current state. The Test Case Generator Module provides the set of test cases corresponding to State D to the Test Runner Module which performs testing of the Web Service Under Test using the test cases. The Test Runner Module generates and provides corresponding test results data to the Self Learning Module . Additionally the Web Service Under Test provides corresponding feedback data including a code coverage metric for each of the test cases to the Self Learning Module .

The Self Learning Module generates learning data by associating each of the test cases with a respective code coverage metric. The Self Learning Module also sums the code coverage metrics associated with each of the test cases corresponding to State D and determines that a code coverage metric for State D is 96 . The Optimal Control Module then determines whether the test cases are in an optimal state. The Optimal Control Module determines that the test cases corresponding to State D are in an optimal state because the code coverage metric corresponding to State D i.e. 96 is at least 95 .

The Optimal Control Module instructs the Test Runner Module to perform testing using the test cases corresponding to State Don a regular basis. For example the Optimal Control Module instructs the Test Runner Module to perform testing using the set of test cases corresponding to State D every day at 3 00 am. Although not illustrated in if the test results for a subsequent test indicate that the code coverage metric is no longer 95 the Optimal Control Module determines that changes have been made to the Web Service Under Test instructs the Stochastic Variation Module to generate a new set of test parameters and instructs the Test Case Generator Module to generate a new set of test cases. The above process if repeated until a new set of test cases is generated that meets the acceptability criteria.

The various embodiments described above can be combined to provide further embodiments. All of the U.S. patents U.S. patent application publications U.S. patent applications foreign patents foreign patent applications and non patent publications referred to in this specification and or listed in the Application Data Sheet are incorporated herein by reference in their entirety. Aspects of the embodiments can be modified if necessary to employ concepts of the various patents applications and publications to provide yet further embodiments.

These and other changes can be made to the embodiments in light of the above detailed description. In general in the following claims the terms used should not be construed to limit the claims to the specific embodiments disclosed in the specification and the claims but should be construed to include all possible embodiments along with the full scope of equivalents to which such claims are entitled. Accordingly the claims are not limited by the disclosure.

