---

title: Instruction and logic for memory disambiguation in an out-of-order processor
abstract: A system includes a processor with a front end to receive an instruction stream reordered by a software scheduler and including a plurality of memory operations and alias information indicating how a given memory operation may be evaluated. Furthermore, the processor includes a hardware scheduler to reorder, in hardware, the instruction stream for out-of-order execution. In addition, the processor includes a calculation module to determine, for a given memory operation and based upon the alias information, a checking range of memory atoms subsequent to the given memory operation and a virtual order of the memory operation. The virtual order indicates an original ordering of the instructions. The processor also includes an alias unit to reorder the instruction stream, determine whether the hardware reordering caused an error, and determine whether the software reordering caused an error based upon the checking range and the virtual order.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09524170&OS=09524170&RS=09524170
owner: Intel Corporation
number: 09524170
owner_city: Santa Clara
owner_country: US
publication_date: 20131223
---
The present disclosure pertains to the field of processing logic microprocessors and associated instruction set architecture that when executed by the processor or other processing logic perform logical mathematical or other functional operations.

Multiprocessor systems are becoming more and more common. Applications of multiprocessor systems include dynamic domain partitioning all the way down to desktop computing. In order to take advantage of multiprocessor systems code to be executed may be separated into multiple threads for execution by various processing entities. Each thread may be executed in parallel with one another. Furthermore in order to increase the utility of a processing entity out of order execution may be employed. Out of order execution may execute instructions as input to such instructions is made available. Thus an instruction that appears later in a code sequence may be executed before an instruction appearing earlier in a code sequence.

The following description describes an instruction and processing logic for memory disambiguation in association with a processor virtual processor package computer system or other processing apparatus. Such a processing apparatus may include an out of order processor. Furthermore such a memory disambiguation may be performed without use of a memory order buffer. In the following description numerous specific details such as processing logic processor types micro architectural conditions events enablement mechanisms and the like are set forth in order to provide a more thorough understanding of embodiments of the present disclosure. It will be appreciated however by one skilled in the art that the embodiments may be practiced without such specific details. Additionally some well known structures circuits and the like have not been shown in detail to avoid unnecessarily obscuring embodiments of the present disclosure.

Although the following embodiments are described with reference to a processor other embodiments are applicable to other types of integrated circuits and logic devices. Similar techniques and teachings of embodiments of the present disclosure may be applied to other types of circuits or semiconductor devices that may benefit from higher pipeline throughput and improved performance. The teachings of embodiments of the present disclosure are applicable to any processor or machine that performs data manipulations. However the embodiments are not limited to processors or machines that perform 512 bit 256 bit 128 bit 64 bit 32 bit or 16 bit data operations and may be applied to any processor and machine in which manipulation or management of data may be performed. In addition the following description provides examples and the accompanying drawings show various examples for the purposes of illustration. However these examples should not be construed in a limiting sense as they are merely intended to provide examples of embodiments of the present disclosure rather than to provide an exhaustive list of all possible implementations of embodiments of the present disclosure.

Although the below examples describe instruction handling and distribution in the context of execution units and logic circuits other embodiments of the present disclosure may be accomplished by way of a data or instructions stored on a machine readable tangible medium which when performed by a machine cause the machine to perform functions consistent with at least one embodiment of the disclosure. In one embodiment functions associated with embodiments of the present disclosure are embodied in machine executable instructions. The instructions may be used to cause a general purpose or special purpose processor that may be programmed with the instructions to perform the steps of the present disclosure. Embodiments of the present disclosure may be provided as a computer program product or software which may include a machine or computer readable medium having stored thereon instructions which may be used to program a computer or other electronic devices to perform one or more operations according to embodiments of the present disclosure. Furthermore steps of embodiments of the present disclosure might be performed by specific hardware components that contain fixed function logic for performing the steps or by any combination of programmed computer components and fixed function hardware components.

Instructions used to program logic to perform embodiments of the present disclosure may be stored within a memory in the system such as DRAM cache flash memory or other storage. Furthermore the instructions may be distributed via a network or by way of other computer readable media. Thus a machine readable medium may include any mechanism for storing or transmitting information in a form readable by a machine e.g. a computer but is not limited to floppy diskettes optical disks Compact Disc Read Only Memory CD ROMs and magneto optical disks Read Only Memory ROMs Random Access Memory RAM Erasable Programmable Read Only Memory EPROM Electrically Erasable Programmable Read Only Memory EEPROM magnetic or optical cards flash memory or a tangible machine readable storage used in the transmission of information over the Internet via electrical optical acoustical or other forms of propagated signals e.g. carrier waves infrared signals digital signals etc. . Accordingly the computer readable medium may include any type of tangible machine readable medium suitable for storing or transmitting electronic instructions or information in a form readable by a machine e.g. a computer .

A design may go through various stages from creation to simulation to fabrication. Data representing a design may represent the design in a number of manners. First as may be useful in simulations the hardware may be represented using a hardware description language or another functional description language. Additionally a circuit level model with logic and or transistor gates may be produced at some stages of the design process. Furthermore designs at some stage may reach a level of data representing the physical placement of various devices in the hardware model. In cases wherein some semiconductor fabrication techniques are used the data representing the hardware model may be the data specifying the presence or absence of various features on different mask layers for masks used to produce the integrated circuit. In any representation of the design the data may be stored in any form of a machine readable medium. A memory or a magnetic or optical storage such as a disc may be the machine readable medium to store information transmitted via optical or electrical wave modulated or otherwise generated to transmit such information. When an electrical carrier wave indicating or carrying the code or design is transmitted to the extent that copying buffering or retransmission of the electrical signal is performed a new copy may be made. Thus a communication provider or a network provider may store on a tangible machine readable medium at least temporarily an article such as information encoded into a carrier wave embodying techniques of embodiments of the present disclosure.

In modern processors a number of different execution units may be used to process and execute a variety of code and instructions. Some instructions may be quicker to complete while others may take a number of clock cycles to complete. The faster the throughput of instructions the better the overall performance of the processor. Thus it would be advantageous to have as many instructions execute as fast as possible. However there may be certain instructions that have greater complexity and require more in terms of execution time and processor resources such as floating point instructions load store operations data moves etc.

As more computer systems are used in internet text and multimedia applications additional processor support has been introduced over time. In one embodiment an instruction set may be associated with one or more computer architectures including data types instructions register architecture addressing modes memory architecture interrupt and exception handling and external input and output I O .

In one embodiment the instruction set architecture ISA may be implemented by one or more micro architectures which may include processor logic and circuits used to implement one or more instruction sets. Accordingly processors with different micro architectures may share at least a portion of a common instruction set. For example Intel Pentium 4 processors Intel Core processors and processors from Advanced Micro Devices Inc. of Sunnyvale Calif. implement nearly identical versions of the x86 instruction set with some extensions that have been added with newer versions but have different internal designs. Similarly processors designed by other processor development companies such as ARM Holdings Ltd. MIPS or their licensees or adopters may share at least a portion a common instruction set but may include different processor designs. For example the same register architecture of the ISA may be implemented in different ways in different micro architectures using new or well known techniques including dedicated physical registers one or more dynamically allocated physical registers using a register renaming mechanism e.g. the use of a Register Alias Table RAT a Reorder Buffer ROB and a retirement register file. In one embodiment registers may include one or more registers register architectures register files or other register sets that may or may not be addressable by a software programmer.

An instruction may include one or more instruction formats. In one embodiment an instruction format may indicate various fields number of bits location of bits etc. to specify among other things the operation to be performed and the operands on which that operation will be performed. In a further embodiment some instruction formats may be further defined by instruction templates or sub formats . For example the instruction templates of a given instruction format may be defined to have different subsets of the instruction format s fields and or defined to have a given field interpreted differently. In one embodiment an instruction may be expressed using an instruction format and if defined in a given one of the instruction templates of that instruction format and specifies or indicates the operation and the operands upon which the operation will operate.

Scientific financial auto vectorized general purpose RMS recognition mining and synthesis and visual and multimedia applications e.g. 2D 3D graphics image processing video compression decompression voice recognition algorithms and audio manipulation may require the same operation to be performed on a large number of data items. In one embodiment Single Instruction Multiple Data SIMD refers to a type of instruction that causes a processor to perform an operation on multiple data elements. SIMD technology may be used in processors that may logically divide the bits in a register into a number of fixed sized or variable sized data elements each of which represents a separate value. For example in one embodiment the bits in a 64 bit register may be organized as a source operand containing four separate 16 bit data elements each of which represents a separate 16 bit value. This type of data may be referred to as packed data type or vector data type and operands of this data type may be referred to as packed data operands or vector operands. In one embodiment a packed data item or vector may be a sequence of packed data elements stored within a single register and a packed data operand or a vector operand may a source or destination operand of a SIMD instruction or packed data instruction or a vector instruction . In one embodiment a SIMD instruction specifies a single vector operation to be performed on two source vector operands to generate a destination vector operand also referred to as a result vector operand of the same or different size with the same or different number of data elements and in the same or different data element order.

SIMD technology such as that employed by the Intel Core processors having an instruction set including x86 MMX Streaming SIMD Extensions SSE SSE2 SSE3 SSE4.1 and SSE4.2 instructions ARM processors such as the ARM Cortex family of processors having an instruction set including the Vector Floating Point VFP and or NEON instructions and MIPS processors such as the Loongson family of processors developed by the Institute of Computing Technology ICT of the Chinese Academy of Sciences has enabled a significant improvement in application performance Core and MMX are registered trademarks or trademarks of Intel Corporation of Santa Clara Calif. .

In one embodiment destination and source registers data may be generic terms to represent the source and destination of the corresponding data or operation. In some embodiments they may be implemented by registers memory or other storage areas having other names or functions than those depicted. For example in one embodiment DEST1 may be a temporary storage register or other storage area whereas SRC1 and SRC2 may be a first and second source storage register or other storage area and so forth. In other embodiments two or more of the SRC and DEST storage areas may correspond to different data storage elements within the same storage area e.g. a SIMD register . In one embodiment one of the source registers may also act as a destination register by for example writing back the result of an operation performed on the first and second source data to one of the two source registers serving as a destination registers.

Embodiments are not limited to computer systems. Embodiments of the present disclosure may be used in other devices such as handheld devices and embedded applications. Some examples of handheld devices include cellular phones Internet Protocol devices digital cameras personal digital assistants PDAs and handheld PCs. Embedded applications may include a micro controller a digital signal processor DSP system on a chip network computers NetPC set top boxes network hubs wide area network WAN switches or any other system that may perform one or more instructions in accordance with at least one embodiment.

Computer system may include a processor that may include one or more execution units to perform an algorithm to perform at least one instruction in accordance with one embodiment of the present disclosure. One embodiment may be described in the context of a single processor desktop or server system but other embodiments may be included in a multiprocessor system. System may be an example of a hub system architecture. System may include a processor for processing data signals. Processor may include a complex instruction set computer CISC microprocessor a reduced instruction set computing RISC microprocessor a very long instruction word VLIW microprocessor a processor implementing a combination of instruction sets or any other processor device such as a digital signal processor for example. In one embodiment processor may be coupled to a processor bus that may transmit data signals between processor and other components in system . The elements of system may perform conventional functions that are well known to those familiar with the art.

In one embodiment processor may include a Level 1 L1 internal cache memory . Depending on the architecture the processor may have a single internal cache or multiple levels of internal cache. In another embodiment the cache memory may reside external to processor . Other embodiments may also include a combination of both internal and external caches depending on the particular implementation and needs. Register file may store different types of data in various registers including integer registers floating point registers status registers and instruction pointer register.

Execution unit including logic to perform integer and floating point operations also resides in processor . Processor may also include a microcode ucode ROM that stores microcode for certain macroinstructions. In one embodiment execution unit may include logic to handle a packed instruction set . By including the packed instruction set in the instruction set of a general purpose processor along with associated circuitry to execute the instructions the operations used by many multimedia applications may be performed using packed data in a general purpose processor . Thus many multimedia applications may be accelerated and executed more efficiently by using the full width of a processor s data bus for performing operations on packed data. This may eliminate the need to transfer smaller units of data across the processor s data bus to perform one or more operations one data element at a time.

Embodiments of an execution unit may also be used in micro controllers embedded processors graphics devices DSPs and other types of logic circuits. System may include a memory . Memory may be implemented as a dynamic random access memory DRAM device a static random access memory SRAM device flash memory device or other memory device. Memory may store instructions and or data represented by data signals that may be executed by processor .

A system logic chip may be coupled to processor bus and memory . System logic chip may include a memory controller hub MCH . Processor may communicate with MCH via a processor bus . MCH may provide a high bandwidth memory path to memory for instruction and data storage and for storage of graphics commands data and textures. MCH may direct data signals between processor memory and other components in system and to bridge the data signals between processor bus memory and system I O . In some embodiments the system logic chip may provide a graphics port for coupling to a graphics controller . MCH may be coupled to memory through a memory interface . Graphics card may be coupled to MCH through an Accelerated Graphics Port AGP interconnect .

System may use a proprietary hub interface bus to couple MCH to I O controller hub ICH . In one embodiment ICH may provide direct connections to some I O devices via a local I O bus. The local I O bus may include a high speed I O bus for connecting peripherals to memory chipset and processor . Examples may include the audio controller firmware hub flash BIOS wireless transceiver data storage legacy I O controller containing user input and keyboard interfaces a serial expansion port such as Universal Serial Bus USB and a network controller . Data storage device may comprise a hard disk drive a floppy disk drive a CD ROM device a flash memory device or other mass storage device.

For another embodiment of a system an instruction in accordance with one embodiment may be used with a system on a chip. One embodiment of a system on a chip comprises of a processor and a memory. The memory for one such system may include a flash memory. The flash memory may be located on the same die as the processor and other system components. Additionally other logic blocks such as a memory controller or graphics controller may also be located on a system on a chip.

Computer system comprises a processing core for performing at least one instruction in accordance with one embodiment. In one embodiment processing core represents a processing unit of any type of architecture including but not limited to a CISC a RISC or a VLIW type architecture. Processing core may also be suitable for manufacture in one or more process technologies and by being represented on a machine readable media in sufficient detail may be suitable to facilitate said manufacture.

Processing core comprises an execution unit a set of register files and a decoder . Processing core may also include additional circuitry not shown which may be unnecessary to the understanding of embodiments of the present disclosure. Execution unit may execute instructions received by processing core . In addition to performing typical processor instructions execution unit may perform instructions in packed instruction set for performing operations on packed data formats. Packed instruction set may include instructions for performing embodiments of the disclosure and other packed instructions. Execution unit may be coupled to register file by an internal bus. Register file may represent a storage area on processing core for storing information including data. As previously mentioned it is understood that the storage area may store the packed data might not be critical. Execution unit may be coupled to decoder . Decoder may decode instructions received by processing core into control signals and or microcode entry points. In response to these control signals and or microcode entry points execution unit performs the appropriate operations. In one embodiment the decoder may interpret the opcode of the instruction which will indicate what operation should be performed on the corresponding data indicated within the instruction.

Processing core may be coupled with bus for communicating with various other system devices which may include but are not limited to for example synchronous dynamic random access memory SDRAM control static random access memory SRAM control burst flash memory interface personal computer memory card international association PCMCIA compact flash CF card control liquid crystal display LCD control direct memory access DMA controller and alternative bus master interface . In one embodiment data processing system may also comprise an I O bridge for communicating with various I O devices via an I O bus . Such I O devices may include but are not limited to for example universal asynchronous receiver transmitter UART universal serial bus USB Bluetooth wireless UART and I O expansion interface .

One embodiment of data processing system provides for mobile network and or wireless communications and a processing core that may perform SIMD operations including a text string comparison operation. Processing core may be programmed with various audio video imaging and communications algorithms including discrete transformations such as a Walsh Hadamard transform a fast Fourier transform FFT a discrete cosine transform DCT and their respective inverse transforms compression decompression techniques such as color space transformation video encode motion estimation or video decode motion compensation and modulation demodulation MODEM functions such as pulse coded modulation PCM .

In one embodiment SIMD coprocessor comprises an execution unit and a set of register files . One embodiment of main processor comprises a decoder to recognize instructions of instruction set including instructions in accordance with one embodiment for execution by execution unit . In other embodiments SIMD coprocessor also comprises at least part of decoder to decode instructions of instruction set . Processing core may also include additional circuitry not shown which may be unnecessary to the understanding of embodiments of the present disclosure.

In operation main processor executes a stream of data processing instructions that control data processing operations of a general type including interactions with cache memory and input output system . Embedded within the stream of data processing instructions may be SIMD coprocessor instructions. Decoder of main processor recognizes these SIMD coprocessor instructions as being of a type that should be executed by an attached SIMD coprocessor . Accordingly main processor issues these SIMD coprocessor instructions or control signals representing SIMD coprocessor instructions on the coprocessor bus . From coprocessor bus these instructions may be received by any attached SIMD coprocessors. In this case SIMD coprocessor may accept and execute any received SIMD coprocessor instructions intended for it.

Data may be received via wireless interface for processing by the SIMD coprocessor instructions. For one example voice communication may be received in the form of a digital signal which may be processed by the SIMD coprocessor instructions to regenerate digital audio samples representative of the voice communications. For another example compressed audio and or video may be received in the form of a digital bit stream which may be processed by the SIMD coprocessor instructions to regenerate digital audio samples and or motion video frames. In one embodiment of processing core main processor and a SIMD coprocessor may be integrated into a single processing core comprising an execution unit a set of register files and a decoder to recognize instructions of instruction set including instructions in accordance with one embodiment.

Some instructions may be converted into a single micro op whereas others need several micro ops to complete the full operation. In one embodiment if more than four micro ops are needed to complete an instruction decoder may access microcode ROM to perform the instruction. In one embodiment an instruction may be decoded into a small number of micro ops for processing at instruction decoder . In another embodiment an instruction may be stored within microcode ROM should a number of micro ops be needed to accomplish the operation. Trace cache refers to an entry point programmable logic array PLA to determine a correct micro instruction pointer for reading the micro code sequences to complete one or more instructions in accordance with one embodiment from micro code ROM . After microcode ROM finishes sequencing micro ops for an instruction front end of the machine may resume fetching micro ops from trace cache .

Out of order execution engine may prepare instructions for execution. The out of order execution logic has a number of buffers to smooth out and re order the flow of instructions to optimize performance as they go down the pipeline and get scheduled for execution. The allocator logic allocates the machine buffers and resources that each uop needs in order to execute. The register renaming logic renames logic registers onto entries in a register file. The allocator also allocates an entry for each uop in one of the two uop queues one for memory operations and one for non memory operations in front of the instruction schedulers memory scheduler fast scheduler slow general floating point scheduler and simple floating point scheduler . Uop schedulers determine when a uop is ready to execute based on the readiness of their dependent input register operand sources and the availability of the execution resources the uops need to complete their operation. Fast scheduler of one embodiment may schedule on each half of the main clock cycle while the other schedulers may only schedule once per main processor clock cycle. The schedulers arbitrate for the dispatch ports to schedule uops for execution.

Register files may be arranged between schedulers and execution units in execution block . Each of register files perform integer and floating point operations respectively. Each register file may include a bypass network that may bypass or forward just completed results that have not yet been written into the register file to new dependent uops. Integer register file and floating point register file may communicate data with the other. In one embodiment integer register file may be split into two separate register files one register file for low order thirty two bits of data and a second register file for high order thirty two bits of data. Floating point register file may include 128 bit wide entries because floating point instructions typically have operands from 64 to 128 bits in width.

Execution block may contain execution units . Execution units may execute the instructions. Execution block may include register files that store the integer and floating point data operand values that the micro instructions need to execute. In one embodiment processor may comprise a number of execution units address generation unit AGU AGU fast ALU fast ALU slow ALU floating point ALU floating point move unit . In another embodiment floating point execution blocks may execute floating point MMX SIMD and SSE or other operations. In yet another embodiment floating point ALU may include a 64 bit by 64 bit floating point divider to execute divide square root and remainder micro ops. In various embodiments instructions involving a floating point value may be handled with the floating point hardware. In one embodiment ALU operations may be passed to high speed ALU execution units . High speed ALUs may execute fast operations with an effective latency of half a clock cycle. In one embodiment most complex integer operations go to slow ALU as slow ALU may include integer execution hardware for long latency type of operations such as a multiplier shifts flag logic and branch processing. Memory load store operations may be executed by AGUs . In one embodiment integer ALUs may perform integer operations on 64 bit data operands. In other embodiments ALUs may be implemented to support a variety of data bit sizes including sixteen thirty two 128 256 etc. Similarly floating point units may be implemented to support a range of operands having bits of various widths. In one embodiment floating point units may operate on 128 bit wide packed data operands in conjunction with SIMD and multimedia instructions.

In one embodiment uops schedulers dispatch dependent operations before the parent load has finished executing. As uops may be speculatively scheduled and executed in processor processor may also include logic to handle memory misses. If a data load misses in the data cache there may be dependent operations in flight in the pipeline that have left the scheduler with temporarily incorrect data. A replay mechanism tracks and re executes instructions that use incorrect data. Only the dependent operations might need to be replayed and the independent ones may be allowed to complete. The schedulers and replay mechanism of one embodiment of a processor may also be designed to catch instruction sequences for text string comparison operations.

The term registers may refer to the on board processor storage locations that may be used as part of instructions to identify operands. In other words registers may be those that may be usable from the outside of the processor from a programmer s perspective . However in some embodiments registers might not be limited to a particular type of circuit. Rather a register may store data provide data and perform the functions described herein. The registers described herein may be implemented by circuitry within a processor using any number of different techniques such as dedicated physical registers dynamically allocated physical registers using register renaming combinations of dedicated and dynamically allocated physical registers etc. In one embodiment integer registers store 32 bit integer data. A register file of one embodiment also contains eight multimedia SIMD registers for packed data. For the discussions below the registers may be understood to be data registers designed to hold packed data such as 64 bit wide MMX registers also referred to as mm registers in some instances in microprocessors enabled with MMX technology from Intel Corporation of Santa Clara Calif. These MMX registers available in both integer and floating point forms may operate with packed data elements that accompany SIMD and SSE instructions. Similarly 128 bit wide XMM registers relating to SSE2 SSE3 SSE4 or beyond referred to generically as SSEx technology may hold such packed data operands. In one embodiment in storing packed data and integer data the registers do not need to differentiate between the two data types. In one embodiment integer and floating point may be contained in the same register file or different register files. Furthermore in one embodiment floating point and integer data may be stored in different registers or the same registers.

In the examples of the following figures a number of data operands may be described. illustrates various packed data type representations in multimedia registers in accordance with embodiments of the present disclosure. illustrates data types for a packed byte a packed word and a packed doubleword dword for 128 bit wide operands. Packed byte format of this example may be 128 bits long and contains sixteen packed byte data elements. A byte may be defined for example as eight bits of data. Information for each byte data element may be stored in bit through bit for byte bit through bit for byte bit through bit for byte and finally bit through bit for byte . Thus all available bits may be used in the register. This storage arrangement increases the storage efficiency of the processor. As well with sixteen data elements accessed one operation may now be performed on sixteen data elements in parallel.

Generally a data element may include an individual piece of data that is stored in a single register or memory location with other data elements of the same length. In packed data sequences relating to SSEx technology the number of data elements stored in a XMM register may be 128 bits divided by the length in bits of an individual data element. Similarly in packed data sequences relating to MMX and SSE technology the number of data elements stored in an MMX register may be 64 bits divided by the length in bits of an individual data element. Although the data types illustrated in may be 128 bits long embodiments of the present disclosure may also operate with 64 bit wide or other sized operands. Packed word format of this example may be 128 bits long and contains eight packed word data elements. Each packed word contains sixteen bits of information. Packed doubleword format of may be 128 bits long and contains four packed doubleword data elements. Each packed doubleword data element contains thirty two bits of information. A packed quadword may be 128 bits long and contain two packed quad word data elements.

In a processor pipeline may include a fetch stage a length decode stage a decode stage an allocation stage a renaming stage a scheduling also known as a dispatch or issue stage a register read memory read stage an execute stage a write back memory write stage an exception handling stage and a commit stage .

In arrows denote a coupling between two or more units and the direction of the arrow indicates a direction of data flow between those units. shows processor core including a front end unit coupled to an execution engine unit and both may be coupled to a memory unit .

Core may be a reduced instruction set computing RISC core a complex instruction set computing CISC core a very long instruction word VLIW core or a hybrid or alternative core type. In one embodiment core may be a special purpose core such as for example a network or communication core compression engine graphics core or the like.

Front end unit may include a branch prediction unit coupled to an instruction cache unit . Instruction cache unit may be coupled to an instruction translation lookaside buffer TLB . TLB may be coupled to an instruction fetch unit which is coupled to a decode unit . Decode unit may decode instructions and generate as an output one or more micro operations micro code entry points microinstructions other instructions or other control signals which may be decoded from or which otherwise reflect or may be derived from the original instructions. The decoder may be implemented using various different mechanisms. Examples of suitable mechanisms include but are not limited to look up tables hardware implementations programmable logic arrays PLAs microcode read only memories ROMs etc. In one embodiment instruction cache unit may be further coupled to a level 2 L2 cache unit in memory unit . Decode unit may be coupled to a rename allocator unit in execution engine unit .

Execution engine unit may include rename allocator unit coupled to a retirement unit and a set of one or more scheduler units . Scheduler units represent any number of different schedulers including reservations stations central instruction window etc. Scheduler units may be coupled to physical register file units . Each of physical register file units represents one or more physical register files different ones of which store one or more different data types such as scalar integer scalar floating point packed integer packed floating point vector integer vector floating point etc. status e.g. an instruction pointer that is the address of the next instruction to be executed etc. Physical register file units may be overlapped by retirement unit to illustrate various ways in which register renaming and out of order execution may be implemented e.g. using one or more reorder buffers and one or more retirement register files using one or more future files one or more history buffers and one or more retirement register files using register maps and a pool of registers etc. . Generally the architectural registers may be visible from the outside of the processor or from a programmer s perspective. The registers might not be limited to any known particular type of circuit. Various different types of registers may be suitable as long as they store and provide data as described herein. Examples of suitable registers include but might not be limited to dedicated physical registers dynamically allocated physical registers using register renaming combinations of dedicated and dynamically allocated physical registers etc. Retirement unit and physical register file units may be coupled to execution clusters . Execution clusters may include a set of one or more execution units and a set of one or more memory access units . Execution units may perform various operations e.g. shifts addition subtraction multiplication and on various types of data e.g. scalar floating point packed integer packed floating point vector integer vector floating point . While some embodiments may include a number of execution units dedicated to specific functions or sets of functions other embodiments may include only one execution unit or multiple execution units that all perform all functions. Scheduler units physical register file units and execution clusters are shown as being possibly plural because certain embodiments create separate pipelines for certain types of data operations e.g. a scalar integer pipeline a scalar floating point packed integer packed floating point vector integer vector floating point pipeline and or a memory access pipeline that each have their own scheduler unit physical register file unit and or execution cluster and in the case of a separate memory access pipeline certain embodiments may be implemented in which only the execution cluster of this pipeline has memory access units . It should also be understood that where separate pipelines are used one or more of these pipelines may be out of order issue execution and the rest in order.

The set of memory access units may be coupled to memory unit which may include a data TLB unit coupled to a data cache unit coupled to a level 2 L2 cache unit . In one exemplary embodiment memory access units may include a load unit a store address unit and a store data unit each of which may be coupled to data TLB unit in memory unit . L2 cache unit may be coupled to one or more other levels of cache and eventually to a main memory.

By way of example the exemplary register renaming out of order issue execution core architecture may implement pipeline as follows 1 instruction fetch may perform fetch and length decoding stages and 2 decode unit may perform decode stage 3 rename allocator unit may perform allocation stage and renaming stage 4 scheduler units may perform schedule stage 5 physical register file units and memory unit may perform register read memory read stage execution cluster may perform execute stage 6 memory unit and physical register file units may perform write back memory write stage 7 various units may be involved in the performance of exception handling stage and 8 retirement unit and physical register file units may perform commit stage .

Core may support one or more instructions sets e.g. the x86 instruction set with some extensions that have been added with newer versions the MIPS instruction set of MIPS Technologies of Sunnyvale Calif. the ARM instruction set with optional additional extensions such as NEON of ARM Holdings of Sunnyvale Calif. .

It should be understood that the core may support multithreading executing two or more parallel sets of operations or threads in a variety of manners. Multithreading support may be performed by for example including time sliced multithreading simultaneous multithreading where a single physical core provides a logical core for each of the threads that physical core is simultaneously multithreading or a combination thereof. Such a combination may include for example time sliced fetching and decoding and simultaneous multithreading thereafter such as in the Intel Hyperthreading technology.

While register renaming may be described in the context of out of order execution it should be understood that register renaming may be used in an in order architecture. While the illustrated embodiment of the processor may also include a separate instruction and data cache units and a shared L2 cache unit other embodiments may have a single internal cache for both instructions and data such as for example a Level 1 L1 internal cache or multiple levels of internal cache. In some embodiments the system may include a combination of an internal cache and an external cache that may be external to the core and or the processor. In other embodiments all of the cache may be external to the core and or the processor.

Processor may include any suitable mechanism for interconnecting cores system agent and caches and graphics module . In one embodiment processor may include a ring based interconnect unit to interconnect cores system agent and caches and graphics module . In other embodiments processor may include any number of well known techniques for interconnecting such units. Ring based interconnect unit may utilize memory control units to facilitate interconnections.

Processor may include a memory hierarchy comprising one or more levels of caches within the cores one or more shared cache units such as caches or external memory not shown coupled to the set of integrated memory controller units . Caches may include any suitable cache. In one embodiment caches may include one or more mid level caches such as level 2 L2 level 3 L3 level 4 L4 or other levels of cache a last level cache LLC and or combinations thereof.

In various embodiments one or more of cores may perform multithreading. System agent may include components for coordinating and operating cores . System agent unit may include for example a power control unit PCU . The PCU may be or include logic and components needed for regulating the power state of cores . System agent may include a display engine for driving one or more externally connected displays or graphics module . System agent may include an interface for communications busses for graphics. In one embodiment interface may be implemented by PCI Express PCIe . In a further embodiment interface may be implemented by PCI Express Graphics PEG . System agent may include a direct media interface DMI . DMI may provide links between different bridges on a motherboard or other portion of a computer system. System agent may include a PCIe bridge for providing PCIe links to other elements of a computing system. PCIe bridge may be implemented using a memory controller and coherence logic .

Cores may be implemented in any suitable manner. Cores may be homogenous or heterogeneous in terms of architecture and or instruction set. In one embodiment some of cores may be in order while others may be out of order. In another embodiment two or more of cores may execute the same instruction set while others may execute only a subset of that instruction set or a different instruction set.

Processor may include a general purpose processor such as a Core i3 i5 i7 2 Duo and Quad Xeon Itanium XScale or StrongARM processor which may be available from Intel Corporation of Santa Clara Calif. Processor may be provided from another company such as ARM Holdings Ltd MIPS etc. Processor may be a special purpose processor such as for example a network or communication processor compression engine graphics processor co processor embedded processor or the like. Processor may be implemented on one or more chips. Processor may be a part of and or may be implemented on one or more substrates using any of a number of process technologies such as for example BiCMOS CMOS or NMOS.

In one embodiment a given one of caches may be shared by multiple ones of cores . In another embodiment a given one of caches may be dedicated to one of cores . The assignment of caches to cores may be handled by a cache controller or other suitable mechanism. A given one of caches may be shared by two or more cores by implementing time slices of a given cache .

Graphics module may implement an integrated graphics processing subsystem. In one embodiment graphics module may include a graphics processor. Furthermore graphics module may include a media engine . Media engine may provide media encoding and video decoding.

Front end may be implemented in any suitable manner such as fully or in part by front end as described above. In one embodiment front end may communicate with other portions of processor through cache hierarchy . In a further embodiment front end may fetch instructions from portions of processor and prepare the instructions to be used later in the processor pipeline as they are passed to out of order execution engine .

Out of order execution engine may be implemented in any suitable manner such as fully or in part by out of order execution engine as described above. Out of order execution engine may prepare instructions received from front end for execution. Out of order execution engine may include an allocate module . In one embodiment allocate module may allocate resources of processor or other resources such as registers or buffers to execute a given instruction. Allocate module may make allocations in schedulers such as a memory scheduler fast scheduler or floating point scheduler. Such schedulers may be represented in by resource schedulers . Allocate module may be implemented fully or in part by the allocation logic described in conjunction with . Resource schedulers may determine when an instruction is ready to execute based on the readiness of a given resource s sources and the availability of execution resources needed to execute an instruction. Resource schedulers may be implemented by for example schedulers as discussed above. Resource schedulers may schedule the execution of instructions upon one or more resources. In one embodiment such resources may be internal to core and may be illustrated for example as resources . In another embodiment such resources may be external to core and may be accessible by for example cache hierarchy . Resources may include for example memory caches register files or registers. Resources internal to core may be represented by resources in . As necessary values written to or read from resources may be coordinated with other portions of processor through for example cache hierarchy . As instructions are assigned resources they may be placed into a reorder buffer . Reorder buffer may track instructions as they are executed and may selectively reorder their execution based upon any suitable criteria of processor . In one embodiment reorder buffer may identify instructions or a series of instructions that may be executed independently. Such instructions or a series of instructions may be executed in parallel from other such instructions. Parallel execution in core may be performed by any suitable number of separate execution blocks or virtual processors. In one embodiment shared resources such as memory registers and caches may be accessible to multiple virtual processors within a given core . In other embodiments shared resources may be accessible to multiple processing entities within processor .

Cache hierarchy may be implemented in any suitable manner. For example cache hierarchy may include one or more lower or mid level caches such as caches . In one embodiment cache hierarchy may include an LLC communicatively coupled to caches . In another embodiment LLC may be implemented in a module accessible to all processing entities of processor . In a further embodiment module may be implemented in an uncore module of processors from Intel Inc. Module may include portions or subsystems of processor necessary for the execution of core but might not be implemented within core . Besides LLC Module may include for example hardware interfaces memory coherency coordinators interprocessor interconnects instruction pipelines or memory controllers. Access to RAM available to processor may be made through module and more specifically LLC . Furthermore other instances of core may similarly access module . Coordination of the instances of core may be facilitated in part through module .

Each processor may be some version of processor . However it should be noted that integrated graphics logic and integrated memory control units might not exist in processors . illustrates that GMCH may be coupled to a memory that may be for example a dynamic random access memory DRAM . The DRAM may for at least one embodiment be associated with a non volatile cache.

GMCH may be a chipset or a portion of a chipset. GMCH may communicate with processors and control interaction between processors and memory . GMCH may also act as an accelerated bus interface between the processors and other elements of system . In one embodiment GMCH communicates with processors via a multi drop bus such as a frontside bus FSB .

Furthermore GMCH may be coupled to a display such as a flat panel display . In one embodiment GMCH may include an integrated graphics accelerator. GMCH may be further coupled to an input output I O controller hub ICH which may be used to couple various peripheral devices to system . External graphics device may include be a discrete graphics device coupled to ICH along with another peripheral device .

In other embodiments additional or different processors may also be present in system . For example additional processors may include additional processors that may be the same as processor additional processors that may be heterogeneous or asymmetric to processor accelerators such as e.g. graphics accelerators or digital signal processing DSP units field programmable gate arrays or any other processor. There may be a variety of differences between the physical resources in terms of a spectrum of metrics of merit including architectural micro architectural thermal power consumption characteristics and the like. These differences may effectively manifest themselves as asymmetry and heterogeneity amongst processors . For at least one embodiment various processors may reside in the same die package.

While may illustrate two processors it is to be understood that the scope of the present disclosure is not so limited. In other embodiments one or more additional processors may be present in a given processor.

Processors and are shown including integrated memory controller units and respectively. Processor may also include as part of its bus controller units point to point P P interfaces and similarly second processor may include P P interfaces and . Processors may exchange information via a point to point P P interface using P P interface circuits . As shown in IMCs and may couple the processors to respective memories namely a memory and a memory which in one embodiment may be portions of main memory locally attached to the respective processors.

Processors may each exchange information with a chipset via individual P P interfaces using point to point interface circuits . In one embodiment chipset may also exchange information with a high performance graphics circuit via a high performance graphics interface .

A shared cache not shown may be included in either processor or outside of both processors yet connected with the processors via P P interconnect such that either or both processors local cache information may be stored in the shared cache if a processor is placed into a low power mode.

Chipset may be coupled to a first bus via an interface . In one embodiment first bus may be a Peripheral Component Interconnect PCI bus or a bus such as a PCI Express bus or another third generation I O interconnect bus although the scope of the present disclosure is not so limited.

As shown in various I O devices may be coupled to first bus along with a bus bridge which couples first bus to a second bus . In one embodiment second bus may be a low pin count LPC bus. Various devices may be coupled to second bus including for example a keyboard and or mouse communication devices and a storage unit such as a disk drive or other mass storage device which may include instructions code and data in one embodiment. Further an audio I O may be coupled to second bus . Note that other architectures may be possible. For example instead of the point to point architecture of a system may implement a multi drop bus or other such architecture.

In some embodiments instructions that benefit from highly parallel throughput processors may be performed by the GPU while instructions that benefit from the performance of processors that benefit from deeply pipelined architectures may be performed by the CPU. For example graphics scientific applications financial applications and other parallel workloads may benefit from the performance of the GPU and be executed accordingly whereas more sequential applications such as operating system kernel or application code may be better suited for the CPU.

In processor includes a CPU GPU image processor video processor USB controller UART controller SPI SDIO controller display device memory interface controller MIPI controller flash memory controller dual data rate DDR controller security engine and IS IC controller . Other logic and circuits may be included in the processor of including more CPUs or GPUs and other peripheral interface controllers.

One or more aspects of at least one embodiment may be implemented by representative data stored on a machine readable medium which represents various logic within the processor which when read by a machine causes the machine to fabricate logic to perform the techniques described herein. Such representations known as IP cores may be stored on a tangible machine readable medium tape and supplied to various customers or manufacturing facilities to load into the fabrication machines that actually make the logic or processor. For example IP cores such as the Cortex family of processors developed by ARM Holdings Ltd. and Loongson IP cores developed the Institute of Computing Technology ICT of the Chinese Academy of Sciences may be licensed or sold to various customers or licensees such as Texas Instruments Qualcomm Apple or Samsung and implemented in processors produced by these customers or licensees.

In some embodiments one or more instructions may correspond to a first type or architecture e.g. x86 and be translated or emulated on a processor of a different type or architecture e.g. ARM . An instruction according to one embodiment may therefore be performed on any processor or processor type including ARM x86 MIPS a GPU or other processor type or architecture.

For example instruction set architecture may include processing entities such as one or more cores and a graphics processing unit . Cores may be communicatively coupled to the rest of instruction set architecture through any suitable mechanism such as through a bus or cache. In one embodiment cores may be communicatively coupled through an L2 cache control which may include a bus interface unit and an L2 cache . Cores and graphics processing unit may be communicatively coupled to each other and to the remainder of instruction set architecture through interconnect . In one embodiment graphics processing unit may use a video code defining the manner in which particular video signals will be encoded and decoded for output.

Instruction set architecture may also include any number or kind of interfaces controllers or other mechanisms for interfacing or communicating with other portions of an electronic device or system. Such mechanisms may facilitate interaction with for example peripherals communications devices other processors or memory. In the example of instruction set architecture may include a liquid crystal display LCD video interface a subscriber interface module SIM interface a boot ROM interface a synchronous dynamic random access memory SDRAM controller a flash controller and a serial peripheral interface SPI master unit . LCD video interface may provide output of video signals from for example GPU and through for example a mobile industry processor interface MIPI or a high definition multimedia interface HDMI to a display. Such a display may include for example an LCD. SIM interface may provide access to or from a SIM card or device. SDRAM controller may provide access to or from memory such as an SDRAM chip or module. Flash controller may provide access to or from memory such as flash memory or other instances of RAM. SPI master unit may provide access to or from communications modules such as a Bluetooth module high speed 3G modem global positioning system module or wireless module implementing a communications standard such as 802.11.

Instruction architecture may include a memory system communicatively coupled to one or more execution entities . Furthermore instruction architecture may include a caching and bus interface unit such as unit communicatively coupled to execution entities and memory system . In one embodiment loading of instructions into execution entities may be performed by one or more stages of execution. Such stages may include for example instruction prefetch stage dual instruction decode stage register rename stage issue stage and writeback stage .

In one embodiment memory system may include an executed instruction pointer . Executed instruction pointer may store a value identifying the oldest undispatched instruction within a batch of instructions. The oldest instruction may correspond to the lowest Program Order PO value. A PO may include a unique number of an instruction. Such an instruction may be a single instruction within a thread represented by multiple strands. A PO may be used in ordering instructions to ensure correct execution semantics of code. A PO may be reconstructed by mechanisms such as evaluating increments to PO encoded in the instruction rather than an absolute value. Such a reconstructed PO may be known as an RPO. Although a PO may be referenced herein such a PO may be used interchangeably with an RPO. A strand may include a sequence of instructions that are data dependent upon each other. The strand may be arranged by a binary translator at compilation time. Hardware executing a strand may execute the instructions of a given strand in order according to PO of the various instructions. A thread may include multiple strands such that instructions of different strands may depend upon each other. A PO of a given strand may be the PO of the oldest instruction in the strand which has not yet been dispatched to execution from an issue stage. Accordingly given a thread of multiple strands each strand including instructions ordered by PO executed instruction pointer may store the oldest illustrated by the lowest number PO in the thread.

In another embodiment memory system may include a retirement pointer . Retirement pointer may store a value identifying the PO of the last retired instruction. Retirement pointer may be set by for example retirement unit . If no instructions have yet been retired retirement pointer may include a null value.

Execution entities may include any suitable number and kind of mechanisms by which a processor may execute instructions. In the example of execution entities may include ALU multiplication units MUL ALUs and floating point units FPU . In one embodiment such entities may make use of information contained within a given address . Execution entities in combination with stages may collectively form an execution unit.

Unit may be implemented in any suitable manner. In one embodiment unit may perform cache control. In such an embodiment unit may thus include a cache . Cache may be implemented in a further embodiment as an L2 unified cache with any suitable size such as zero 128 k 256 k 512 k 1 M or 2 M bytes of memory. In another further embodiment cache may be implemented in error correcting code memory. In another embodiment unit may perform bus interfacing to other portions of a processor or electronic device. In such an embodiment unit may thus include a bus interface unit for communicating over an interconnect intraprocessor bus interprocessor bus or other communication bus port or line. Bus interface unit may provide interfacing in order to perform for example generation of the memory and input output addresses for the transfer of data between execution entities and the portions of a system external to instruction architecture .

To further facilitate its functions bus interface unit may include an interrupt control and distribution unit for generating interrupts and other communications to other portions of a processor or electronic device. In one embodiment bus interface unit may include a snoop control unit that handles cache access and coherency for multiple processing cores. In a further embodiment to provide such functionality snoop control unit may include a cache to cache transfer unit that handles information exchanges between different caches. In another further embodiment snoop control unit may include one or more snoop filters that monitors the coherency of other caches not shown so that a cache controller such as unit does not have to perform such monitoring directly. Unit may include any suitable number of timers for synchronizing the actions of instruction architecture . Also unit may include an AC port .

Memory system may include any suitable number and kind of mechanisms for storing information for the processing needs of instruction architecture . In one embodiment memory system may include a load store unit for storing information such as buffers written to or read back from memory or registers. In another embodiment memory system may include a translation lookaside buffer TLB that provides look up of address values between physical and virtual addresses. In yet another embodiment bus interface unit may include a memory management unit MMU for facilitating access to virtual memory. In still yet another embodiment memory system may include a prefetcher for requesting instructions from memory before such instructions are actually needed to be executed in order to reduce latency.

The operation of instruction architecture to execute an instruction may be performed through different stages. For example using unit instruction prefetch stage may access an instruction through prefetcher . Instructions retrieved may be stored in instruction cache . Prefetch stage may enable an option for fast loop mode wherein a series of instructions forming a loop that is small enough to fit within a given cache are executed. In one embodiment such an execution may be performed without needing to access additional instructions from for example instruction cache . Determination of what instructions to prefetch may be made by for example branch prediction unit which may access indications of execution in global history indications of target addresses or contents of a return stack to determine which of branches of code will be executed next. Such branches may be possibly prefetched as a result. Branches may be produced through other stages of operation as described below. Instruction prefetch stage may provide instructions as well as any predictions about future instructions to dual instruction decode stage.

Dual instruction decode stage may translate a received instruction into microcode based instructions that may be executed. Dual instruction decode stage may simultaneously decode two instructions per clock cycle. Furthermore dual instruction decode stage may pass its results to register rename stage . In addition dual instruction decode stage may determine any resulting branches from its decoding and eventual execution of the microcode. Such results may be input into branches .

Register rename stage may translate references to virtual registers or other resources into references to physical registers or resources. Register rename stage may include indications of such mapping in a register pool . Register rename stage may alter the instructions as received and send the result to issue stage .

Issue stage may issue or dispatch commands to execution entities . Such issuance may be performed in an out of order fashion. In one embodiment multiple instructions may be held at issue stage before being executed. Issue stage may include an instruction queue for holding such multiple commands. Instructions may be issued by issue stage to a particular processing entity based upon any acceptable criteria such as availability or suitability of resources for execution of a given instruction. In one embodiment issue stage may reorder the instructions within instruction queue such that the first instructions received might not be the first instructions executed. Based upon the ordering of instruction queue additional branching information may be provided to branches . Issue stage may pass instructions to executing entities for execution.

Upon execution writeback stage may write data into registers queues or other structures of instruction set architecture to communicate the completion of a given command. Depending upon the order of instructions arranged in issue stage the operation of writeback stage may enable additional instructions to be executed. Performance of instruction set architecture may be monitored or debugged by trace unit .

Execution pipeline may include any suitable combination of steps or operations. In predictions of the branch that is to be executed next may be made. In one embodiment such predictions may be based upon previous executions of instructions and the results thereof. In instructions corresponding to the predicted branch of execution may be loaded into an instruction cache. In one or more such instructions in the instruction cache may be fetched for execution. In the instructions that have been fetched may be decoded into microcode or more specific machine language. In one embodiment multiple instructions may be simultaneously decoded. In references to registers or other resources within the decoded instructions may be reassigned. For example references to virtual registers may be replaced with references to corresponding physical registers. In the instructions may be dispatched to queues for execution. In the instructions may be executed. Such execution may be performed in any suitable manner. In the instructions may be issued to a suitable execution entity. The manner in which the instruction is executed may depend upon the specific entity executing the instruction. For example at an ALU may perform arithmetic functions. The ALU may utilize a single clock cycle for its operation as well as two shifters. In one embodiment two ALUs may be employed and thus two instructions may be executed at . At a determination of a resulting branch may be made. A program counter may be used to designate the destination to which the branch will be made. may be executed within a single clock cycle. At floating point arithmetic may be performed by one or more FPUs. The floating point operation may require multiple clock cycles to execute such as two to ten cycles. At multiplication and division operations may be performed. Such operations may be performed in four clock cycles. At loading and storing operations to registers or other portions of pipeline may be performed. The operations may include loading and storing addresses. Such operations may be performed in four clock cycles. At write back operations may be performed as required by the resulting operations of .

Electronic device may include processor communicatively coupled to any suitable number or kind of components peripherals modules or devices. Such coupling may be accomplished by any suitable kind of bus or interface such as IC bus system management bus SMBus low pin count LPC bus SPI high definition audio HDA bus Serial Advance Technology Attachment SATA bus USB bus versions 1 2 3 or Universal Asynchronous Receiver Transmitter UART bus.

Such components may include for example a display a touch screen a touch pad a near field communications NFC unit a sensor hub a thermal sensor an express chipset EC a trusted platform module TPM BIOS firmware flash memory a digital signal processor a drive such as a solid state disk SSD or a hard disk drive HDD a wireless local area network WLAN unit a Bluetooth unit a wireless wide area network WWAN unit a global positioning system GPS a camera such as a USB 3.0 camera or a low power double data rate LPDDR memory unit implemented in for example the LPDDR3 standard. These components may each be implemented in any suitable manner.

Furthermore in various embodiments other components may be communicatively coupled to processor through the components discussed above. For example an accelerometer ambient light sensor ALS compass and gyroscope may be communicatively coupled to sensor hub . A thermal sensor fan keyboard and touch pad may be communicatively coupled to EC . Speaker headphones and a microphone may be communicatively coupled to an audio unit which may in turn be communicatively coupled to DSP . Audio unit may include for example an audio codec and a class D amplifier. A SIM card may be communicatively coupled to WWAN unit . Components such as WLAN unit and Bluetooth unit as well as WWAN unit may be implemented in a next generation form factor NGFF .

Embodiments of the present disclosure involve an instruction and logic for memory disambiguation in an out of order processor. Such memory disambiguation may be performed without use of a memory order buffer. is a block diagram of a system for implementing an instruction and logic for memory disambiguation in accordance with embodiments of the present disclosure.

In some processors a memory order buffer may be used to reconstruct an original program order PO for memory operations that have been reordered by out of order scheduling hardware. However such a memory order buffer may be unable to reconstruct program order for memory operations that have been reordered by software schedulers. Such software schedulers may be present in for example just in time compiles or binary translators that may then provide resultant software reordered code to hardware. Software schedulers provide reordering that may identify and issue critical memory operation atoms much sooner than in other out of order mechanisms. In various embodiments system may reconstruct an original program order for both out of order operations made by software and by hardware. In a further embodiment the original program order may be reconstructed from an instruction stream that was first reordered by software in an out of order fashion provided to hardware and reordered again in an out of order fashion by the hardware. Any suitable out of order operations may have been used. For example software may have reordered the memory operations in an original instruction stream by using a binary translation software layer that reorders guest x86 memory operations in a virtual machine operation. In another example hardware may have reordered the memory operations through out of order memory operation scheduling.

System may fetch dispatch execute and retire instructions out of order. System may be implemented in any suitable manner to perform memory disambiguation while executing instructions.

In one embodiment such instructions may include those in original instruction stream . In such an embodiment system may include a software scheduler to reorder memory operations in original instruction stream to produce software reordered instruction stream . Software scheduler may reorder memory operations and attach alias packets to the memory operations. The alias packets may contain information about how rescheduled memory operations are to be evaluated. In another embodiment system may receive instructions that have already been reordered by software such as those in software reordered instruction stream . In such an embodiment system might not include software scheduler . Furthermore system may utilize any suitable number or kind of out of order processors or processing entities. System may illustrate elements of such a processor which may include any processor core logical processor processor or other processing entity such as those illustrated in .

In one embodiment processor may include a front end to fetch instructions to be executed and to prepare such instructions to be used later by other elements of processor . In another embodiment processor may include a hardware scheduler to reorder instructions in hardware and assign the instructions to be executed by various elements of processor . Hardware scheduler may include an out of order hardware scheduler. Such instructions may be executed after being assigned to execution elements of processor . In yet another embodiment processor may include a memory execution unit MEU to perform memory disambiguation. Such disambiguation may include determining whether its received instructions include any memory operation conflicts. Such memory operation conflicts may include whether memory operations performed out of order are prone to error conditions or faults. As the instructions may have been executed but not retired a determined error or fault may cause processor to roll back execution to a previously known point wherein the instructions responsible for the error or fault may be executed in order. Otherwise processor may retire the instructions if MEU determines that no fault or error has occurred. In still yet another embodiment processor may include a rotating alias register RAR bank to track ordering of instructions as they are reordered by hardware and software. MEU may access RAR bank to perform memory disambiguation.

Furthermore although specific elements of system may be described herein as performing a specific function any suitable portion of system may perform the functionality described herein.

Software scheduler may be implemented in any suitable manner to accept original instruction stream and reorder memory operations thereof to produce software reordered instruction stream . In one embodiment software scheduler may be implemented as a binary translator. In another embodiment software scheduler may be implemented as a just in time compiler. The ability of software scheduler to reorder operations and instructions may be implemented by a reorder module . Reorder module may be implemented by any suitable entity such as a library function application programming interface script executable application program instructions or code. Reorder module may be called or otherwise accessed by software scheduler . In various embodiments reorder module may be communicatively coupled to but not included within software scheduler . Software scheduler and reorder module may include instructions stored in machine readable memory and may be executable by a processor. The instructions when read by the processor may perform the described functionality of software scheduler and reorder module . Software scheduler and reorder module may operate on any suitable machine such as a laptop server mobile device computer blade or virtual machine. Such a machine may be the same or different than a machine including processor .

Software scheduler may produce software reordered instruction stream . Software reordered instruction stream may include any suitable number of memory operations . Such memory operations may include an instruction for accessing memory of or communicatively coupled to processor . Software scheduler may attach to each such memory operation alias packets for identifying potential violations of memory dependencies. Such alias packets may include for example one or more offsets . Offsets may define how memory operations as resident within software reordered instruction stream may be evaluated to determine whether an error or fault has occurred. Determining such errors may include for example whether given memory access with a higher PO executed out of order and before another memory with a lower PO violates data dependency. Any suitable set of rules or logic for evaluating a given memory access may be used.

In another embodiment given an MID of A a high offset may specify a range of addresses above MID plus the high offset. Within range memory operation atoms may be checked according to check criteria but only if a given atom has been executed before A . Thus range may include operations originally ordered in original instruction stream with a lower PO than the operation indicated by MID furthermore such operations are still are ordered higher than the operation indicated by MID . As such operations may have been reordered they may be checked for memory violations such as data dependency violation. The high offset may thus provide memory disambiguation such that appropriate checks may be applied to the operations in range .

In one embodiment the high offset and the low offset may be equal. In such embodiments the high offset and the low offset may illustrate the specific number of operations by which the operation A of MID has been reordered. In another embodiment the high offset and the low offset may be unequal. In such an embodiment a range of addresses between at MID plus the low offset and MID plus the high offset may be checked for memory violations regardless of execution order.

Returning to front end may be implemented in any suitable manner to receive software reordered instruction stream and fetch instructions for execution. For example front end may be implemented fully or in part by front end front end unit or front end . Front end may pass software reordered instruction stream to hardware scheduler . In one embodiment software reordered instruction stream may be passed to hardware scheduler through passing of memory operations and alias operations associated with each such memory operation. Such alias operations may include for example offsets .

Hardware scheduler may be implemented in any suitable manner. For example hardware scheduler may be implemented in part by out of order execution engine fast scheduler slow general floating point scheduler simple floating point scheduler execution engine unit scheduler units or resource schedulers . Hardware scheduler may reorder memory operations to better take advantage of available resources of processor and output the result as hardware and software reordered instruction stream . Hardware scheduler may include a resource scheduler to assign various resources of processor to execute portions of hardware and software reordered instruction stream . Thus hardware scheduler may effect resource allocations. After execution the operations of hardware and software reordered instruction stream may be checked for memory disambiguation by MEU .

Furthermore hardware scheduler may include a RAT module to rename references such that they are remapped to physical resources. A given logical reference in the instructions may be mapped to multiple different physical resources depending upon where the logical reference is located. RAT module may maintain a list of memory operations as they were originally fetched by front end thus reflecting software reordered instruction stream .

In addition RAT module may include an MID calculation module to calculate MID values identifying memory operations in software reordered instruction stream . The MID values may be used to reconstruct original instruction stream from hardware and software reordered instruction stream . MID values may be calculated such that memory operations may be identified in a circular bank such as RAR bank . MID calculation module may add an MID to each memory operation received and propagated by hardware scheduler .

Furthermore MID calculation module may calculate a virtual age or an order for each memory operation in software reordered instruction stream . The virtual age or order may be used to infer the original position of the memory operation in original instruction stream . The virtual age or order may be based upon alias information such as offsets . The virtual age or order may be stored in RAR bank . The virtual age or order may describe the position of the memory operation as it is ordered within hardware processor based upon the original position of the memory operation as it was ordered in software reordered instruction stream . MID calculation module may accept indications of memory operations and associated alias operations through software reordered instruction stream . Furthermore MID calculation module may add additional indications of memory operations and associated alias operations such as the virtual age or order as well as other aliasing information for the reordering of operations performed in hardware by hardware scheduler . The indications of memory operations and alias operations may be passed to for example resource scheduler or RAR bank .

MEU may accept indications of memory operations as they are executed. However such memory operations might not yet have been retired. MEU may perform memory disambiguation on the operations to determine whether any memory errors such as data dependency errors have occurred. The data dependency errors may be the result of errors in scheduling execution of memory operations in an out of order fashion. If any such errors have occurred MEU may cause a rollback to a previously known execution state predating the scheduling of operations causing such errors. The memory operations may then be reexecuted in linear in order fashion to avid the errors. If errors have occurred MEU may generate a fault. If errors have not occurred MEU may for example retire the memory operations or otherwise indicate that the memory operations are valid and are eligible for retirement. Such retirement may cause resource reclamation.

MEU may be implemented in any suitable manner. In one embodiment to perform memory disambiguation MEU may include an alias hardware unit . Alias hardware unit may accept indications of executed memory operations and associated aliases. Such alias information may be stored for example in RAR bank . MEU may reorder memory operations such that the memory operations are arranged in the manner received by hardware scheduler . Such ordering may conform to the ordering of software reordered instruction stream . The ordering may be made in any suitable manner. For example MEU may interpret alias operations inserted by hardware scheduler .

After MEU reorders memory operations to the order of software reordered instruction stream alias hardware unit may perform memory disambiguation by checking for memory access violations. Alias hardware unit may evaluate information about the memory operations contained within RAR bank to evaluate each received memory operation. Alias hardware unit may apply any suitable logic rules or criteria to perform memory disambiguation. In one embodiment alias hardware unit may include rules by which the contents or RAR bank will be evaluated for memory disambiguation.

RAR bank may be implemented in any suitable manner. In one embodiment RAR bank may be included in MEU . In another embodiment RAR bank may be implemented separately from MEU . In various embodiments RAR bank may be implemented in content addressable memory CAM . By using CAM given a data word such as a MID the CAM may be searched and any data within the CAM associated with the data word such as one or more entries may be returned.

RAR bank may include any suitable number and kind of entries. In one embodiment MID may identify the position of an entry in RAR bank . The number of memory atoms to be covered by RAR bank may be larger than the number of MID entries available in RAR bank . Accordingly in a further embodiment RAR bank may be implemented as a rotating buffer wherein the MID may be set to a hash of a current count of the number of memory atoms. For example RAR bank may include sixty four different MID values. Access to RAR bank by a given MID value may return multiple results. A MOD operation may be applied to the current count using the parameter of sixty four such that the current count is divided by sixty four and the MID output of the function is the remainder in whole numbers of the division operation. A current count of sixty five may thus result in an MID of one as would a current count of one.

In another embodiment RAR bank may include any suitable indication of the location of a memory operation. For example RAR bank may include an identification of a page of memory on which the memory operation resides. In another example RAR bank may include a starting address and an ending address for the byte granularity of the memory operation. The starting and ending addresses may point into a 4 k page boundary memory space.

In yet another embodiment RAR bank may include a virtual age or order for each memory operation. In a further embodiment RAR bank may include a virtual age including a range of MID addresses against which the memory operation will be checked. In another further embodiment RAR bank may include a virtual age for use within the checking operation itself.

In still yet another embodiment RAR bank may include any other information useful for performing memory disambiguation. For example RAR bank may include indications of whether a memory operation is valid or has been executed. In another example RAR bank may include indications of whether a memory operation is a store operation or a load operation.

Operation map may illustrate ranges and defined by low offset and high offset from MID . Range may illustrate a range of addresses of atoms which will be checked if such atoms are executed after the memory operation A identified by MID . Range may illustrate a range of addresses of atoms which will be checked regardless of whether such atoms are executed before the memory operation A identified by MID . Furthermore range may illustrate a range of addresses of atoms which will be checked if such atoms are executed before the memory operations A identified by MID .

The evaluation of a given memory operation against elements of RAR bank may be made using any suitable criteria or logic. For example rules may include check criteria . Check criteria may specify whether or not a fault or error should be generated based upon the contents of a memory operation and another atom from RAR bank . Such a fault or error may include for example a violation of data dependency. Check criteria may be applied to the given memory operation and atoms identified by rangers . For example check criteria may be applied to A and atoms in range which are executed after A to A and atoms in range regardless of execution order and to A and atoms in range which are executed before A .

Check criteria may include any suitable criteria or rules. The criteria or rules may take into account the ability of system to map logical resource use to different physical resources. In one embodiment check criteria may specify whether a load operation may be executed out of order with another load operation. For example a later load operation may be executed before or bypass an earlier load operation. In another embodiment check criteria may specify whether a load operation may be executed out of order with a store operation. For example a later load operation may be executed before or bypass an earlier store operation. In another example a later store operation may be executed before or bypass an earlier load operation. In yet another embodiment check criteria may specify whether that a store operation may be executed out of order with another store operation. For example a later store operation might not be executed before or bypass an earlier store operation.

At MID calculation module may initialize any necessary operations. MID calculation module may determine a number of memory atoms representing memory operations in software reordered instruction stream . An index for tracking MID such as MID index may be set to zero. Furthermore an index for tracking the number of memory atoms processed such as Memory Atom index may be set to zero. RAR bank may be cleared or otherwise initialized.

At MID calculation module may determine the MID and ages for each memory atom in RAR bank . Any suitable operations such as those described herein may be performed. The operations described below may be repeated for each memory atom captured in software reordered instruction stream .

At MID calculation module may increment the index of MID entries. For example MID index may be incremented. Furthermore MID index may be MOD d by the number of MIDs in RAR bank such as sixty four.

At the MID of the corresponding RAR bank entry may be set to the present MID index value. This may be expressed by for example RAR.MID Memory Atom index MID index. RAR bank may be represented by the object RAR and the MID field of the memory atom may be assigned the present MID value.

At MID calculation module may begin to determine the effect of offsets upon the RAR bank entry. In one embodiment it may be determined whether both a high and low offset were specified in the alias packet associated with the memory operation. If so method may proceed to to adopt offset values for RAR bank calculations. If not method may proceed to for additional determinations.

At MID calculation module may adopt the offset values of offsets if both offsets are specified in offsets of the alias messages. In one embodiment the low offset of the memory operation entry in RAR bank may be set to the low offset of offsets and the high offset of the memory operation entry in RAR bank may be set to the high offset of offsets . This may be expressed by for example RAR.Offset Low Memory Atom index Alias.Offset Low Memory Atom index and RAR.Offset High Memory Atom index Alias.Offset High Memory Atom index . Method may proceed to .

At in another embodiment MID calculation module may determine whether only the high offset is specified in offsets of the alias messages. If so method may proceed to to adopt offset values for RAR bank calculations. If not method may proceed to to adopted different offset values for RAR bank calculations.

At MID calculation module may assume the low offset is zero and adopt the high offset if only the high offset is specified in offsets of the alias messages. In one embodiment the low offset of the memory operation entry in RAR bank may be set zero and the high offset of the memory operation entry in RAR bank may be set to the high offset of offsets . This may be expressed by for example RAR.Offset Low Memory Atom index 0 and RAR.Offset High Memory Atom index Alias.Offset High Memory Atom index . Method may proceed to .

At MID calculation module may assume the high offset is the same as the low offset and adopt the high offset if only the low offset is specified in offsets of the alias messages. In one embodiment the low offset of the memory operation entry in RAR bank may be set to the low offset of offsets and the high offset of the memory operation entry in RAR bank may also be set to the low offset of offsets . This may be expressed by for example RAR.Offset Low Memory Atom index Alias.Offset Low Memory Atom index and RAR.Offset High Memory Atom index RAR.Offset Low Memory Atom index . Method may proceed to .

At offsets from offset may have been populated into RAR bank as appropriate. In one embodiment MID calculation module may determine ranges of memory atoms to be checked against the memory operation based on such offsets. In a further embodiment MID calculation module may determine that the range of memories to be checked may include the atoms after the MID value and the adopted low offset. Furthermore the value of the MID value plus the adopted low offset may be MOD d to normalize the check range to the sixty four MID entries of RAR bank . This may be expressed by for example RAR.CheckRange Memory Atom index RAR.Offset Low Memory Atom index RAR.MID Memory Atom index MOD 64 The CheckRange field may establish for the given memory operation the last memory atom of RAR bank that is to be checked against the memory operation.

At in another embodiment MID calculation module may determine a virtual age or order for the memory operation. In a further embodiment this virtual age may be adjusted based upon offsets adopted by MID calculation module . Specifically the virtual age may be the MID plus the offset setting after which memory atoms may be checked if executed before the memory operation. Such an offset setting may be the high offset. Furthermore the value of the virtual age may be MOD d to normalize the age to the sixth four MID entries of RAR bank . This may be expressed by for example RAR.Age Memory Atom index RAR.Offset High Memory Atom index RAR.MID Memory Atom index MOD 64 The Age field may establish for the given memory operation the adjusted virtual age or order which may be used to evaluate memory operations against each other.

At MID calculation module may increment the index for tracking the number of memory atoms that have been processed. At MID calculation module may determine whether it has reached the end of the memory operations. If so method may terminate. If not method may repeat at .

After execution of method calculation module may have populated RAR bank . Subsequently upon execution of the memory operations alias hardware unit may perform memory disambiguation on each memory operation by applying rules to elements of RAR bank .

Alias hardware unit may thus utilize the high offset as included in the virtual age or order to perform static reordering of memory operations in the hardware. While the MIDs may be assigned in program order of software reordered instruction stream adjustment using the virtual age or order may reorder MIDs to reflect the PO of original instruction stream . The use of virtual age as an offset from MID may be thus used in comparisons to determine which atom is older in PO of original instruction stream . Such comparisons may be used to evaluate reorderings performed by software scheduler . With this approach the alias hardware unit may perform memory disambiguation on both reorderings performed in software and in the hardware scheduler of the processor.

In yet another embodiment RAR bank may include a field for information specifying a range of atoms in RAR bank which will be compared against a given memory operation. Field of information may be set by for example MID calculation module at of . Field of information may provide enablement to entries of RAR bank such that they may be compared to a given memory operation . Field of information may include an upper limit defining the last atom which may be compared to the given memory operation.

Alias hardware unit may access the given memory operation under evaluation through the memory operation s MID in RAR bank . In one embodiment all operations in RAR bank which may include multiple entries corresponding to the MID may be returned. Field values of each such entry may be compared atom by atom with corresponding field values of atoms within the range enabled by field of information . In one embodiment such a range may include the MID value plus one MOD d by sixty four through the check range value stored in RAR bank memory operation also MOD d by sixty four .

For each atom within the range specified by field of information alias hardware unit may determine whether any memory violations have occurred with respect to memory operation . In one embodiment alias hardware unit may compare a given atom s data from RAR bank with each piece of data from a memory operation returned by the MID lookup in RAR bank . In a further embodiment if more than one memory operation is returned by the MID lookup in RAR bank the atom may be compared against each such memory operation . In another further embodiment each additional atom within the range enabled by field of information may be compared against each such memory operation .

Alias hardware unit may utilize any suitable logic or rules to compare memory operation against atoms enabled in RAR bank . Such logic or rules may include for example those included in rules . In the example of alias hardware unit may include memory fault logic for implementing logic to determine whether comparisons of atoms of RAR bank and memory operation indicate an overlap in memory operations that violate for example data dependency. Such an overlap causing a violation may cause an error or fault. Memory fault logic may be implemented in hardware such as logic gates.

Memory fault logic may check whether a plurality of conditions is present in the comparison of RAR bank atoms and memory operation . In one embodiment if any such condition fails then a memory fault might not be generated. Conversely if all such conditions are true then a memory fault may be generated.

In one embodiment memory fault logic may include a condition that a given atom s virtual age is older than the virtual age of memory operation .

In another embodiment memory fault logic may include a condition that a given atom s end address is greater or equal to the end address of memory operation . In yet another embodiment memory fault logic may include a condition that a given atom s start address is less or equal to the start address of memory operation .

In still yet another embodiment memory fault logic may include a condition that determines whether a given atom is on the same memory page as memory operation . Such a condition may eliminate faults that otherwise may be attributed to an incorrect memory operation . If a given atom and a given memory operation are not even on the same memory page then memory operation may represent a memory operation with an MID equivalent to another such memory operation that actually be related to the memory atom. As noted above looking up an MID in RAR bank implemented with CAM may retrieve multiple entries corresponding to the same MID.

In another embodiment memory fault logic may include a condition that determines whether either the atom or memory operation includes a store operation. Such a condition may eliminate the possibility of a fault if both memory operation and the atom are load operations as two load operations may be allowed even if out of order. In yet another embodiment fault logic may include a condition that determines whether both the atom and memory operation are valid. If not the determination of a fault may not yet be ready to be made.

In the example of alias hardware unit may apply three stages in succession. The conditions of may be selected such that all conditions therein must be met to determine that a memory error or fault has occurred. If application of stage fails to meets its criteria for a memory fault determination stages and may be omitted. Such stages may be unnecessary to be executed if a memory fault determination is impossible due to the determination in stage . If application of stage meets its criteria for a memory fault determination stage may be applied. If application of stage fails to meets its criteria for a memory fault determination stage may be omitted. If application of stage meets its criteria for a memory fault determination stage may be applied. If stage meets its criteria for a memory fault determination alias hardware unit may issue a memory fault. Execution of memory operations may be rolled back to a previously known safe point. The memory operations may be reexecuted linearly according to PO to avoid the memory fault. Such a linear execution may be made according to the virtual age or order fields of RAR register bank .

In one embodiment stage may include checks for conditions . Conditions may be performed together during application of stage . If any of conditions fail then according to memory fault logic a memory fault is not possible. Accordingly stage may terminate execution of the pipeline. If all of conditions are met then according to memory fault logic a memory fault is still possible. Execution may then be passed to stage .

In another embodiment stage may include checks for conditions . Conditions may be performed together during application of stage . If any of conditions fail then according to memory fault logic a memory fault is not possible. Accordingly stage may terminate execution of the pipeline. If all of conditions are met then according to memory fault logic a memory fault may still be possible. Execution may then be passed to stage .

In yet another embodiment stage may include checks for conditions . Condition may be performed together during application of stage . If condition fails then according to memory fault logic a memory fault is not possible. Alias hardware unit may issue an indication that no memory error or fault has been issued. If condition is met then according to memory fault logic a memory fault should be issued by alias hardware unit .

Any suitable order of execution of stages and their respective conditions may be performed by alias hardware unit . As described above in one embodiment stages may be executed in sequential order. In another embodiment conditions may be checked within a given stage for multiple entries in RAR bank . Furthermore checking of conditions within a given stage for multiple entries in RAR bank may be made before moving to the next stage.

In various embodiments at A alias hardware unit may for a given memory operation apply all conditions of stage to each memory atom entry within RAR bank within the enabled check range for the given memory operation. The results may include for each memory atom entry within the enabled check range an indication of whether the memory atom entry meets the conditions of stage for a memory error or fault. For each memory atom failing the conditions for a memory error or fault checking may be stopped with respect to the given memory operation.

In various other embodiments at B alias hardware unit may perform analysis defined in stages subsequent to stage for the given memory operation. Such analysis might be performed only for each memory atom meeting the conditions of the previous stage for a memory error or fault. Analysis performed for subsequent stages may be performed in a manner similar to that described above for stage . For example alias hardware unit may for the given memory operation apply all conditions of stage to each memory atom entry within RAR bank that passed stage . The results may include for each memory atom entry that passed stage an indication of whether the memory atom entry meets the conditions of stage for a memory error or fault. For each memory atom failing the conditions for a memory error or fault checking may be stopped with respect to the given memory operation. Alias hardware unit may for the given memory operation apply all conditions of stage to each memory atom entry within RAR bank that passed stage . The results may include for each memory atom entry that passed stage an indication of whether the memory atom entry meets the conditions of stage for a memory error or fault. Alias hardware unit may provide an indication of whether all the conditions of the pipeline were met yielding a memory error or fault.

Table illustrates example operation of pipeline execution of alias hardware unit . The pipeline execution may be made for an example memory operation A yielded by a given MID using CAM search in RAR bank . If multiple memory operations are yielded from the MID the pipeline execution may be repeated for each such memory operation. Four example memory operation atoms of RAR bank are illustrated as Op Op Op and Op. Such memory operation atoms may be included with the range enabled by the memory operation s entry within RAR bank .

First alias hardware unit may compare memory operation A with Op Op Op and Op using criteria of stage . The results may illustrate that Op passes all such criteria Op passes two criteria but fails one criterion Op passes all such criteria and Op passes all such criteria. Accordingly Op might be unable to generate a memory fault or error. Op Op and Op may still be capable of generating a memory fault or error.

Next alias hardware unit may compare memory operation A with Op Op and Op using criteria of stage . Comparison of Op may be unnecessary. The results may illustrate that Op passes all such criteria Op passes a criteria but fails one criterion and Op passes all such criteria. Accordingly Op may be unable to generate a memory fault or error. Op and Op may still be capable of generating a memory fault or error.

Finally alias hardware unit may compare memory operation A with Op and Op using criteria of stage . Comparison of Op and Op may be unnecessary. The results may illustrate that Op fails the criteria and that Op passes the criteria. Accordingly Op might not generate a memory fault or error while Op may generate a memory fault or error.

In one embodiment at an instruction stream to be executed may be received. At the instruction stream may be reordered by a software scheduler. Such a scheduler may be included in for example a just in time compiler or a binary translator. Software scheduling may be performed by executing code for a software scheduler as stored on machine readable media. The code may be loaded on a processor and executed resulting in the software scheduling of the instruction stream. The instructions that are reordered may include memory operations. At the software scheduler may add alias packets of information to the stream of instructions such as offsets defining ranges of operations and how such ranges are to be evaluated for memory errors or faults. Each packet may be associated with a memory operation.

At the stream of instructions as reordered by the software scheduler may be received at a processor. Instructions may be fetched for execution by a front end of the processor.

At for each memory operation fetched from the stream of instructions various determinations may be made. In one embodiment a MID may be calculated for each memory operation. In another embodiment a virtual age or order for the memory operation may be calculated. In yet another embodiment a range of memory atoms to be checked for memory faults or errors may be determined. Such determinations may be made by a MID calculation module in hardware. Furthermore the determinations may be made based upon offsets included in alias packets associated with a given memory operation. The determinations may be stored in any suitable location such as a RAR bank implemented in CAM. The determinations may be implemented by for example fully or in part by method .

At a hardware scheduler may further reorder the instructions. The reordering may be made to take advantage of out of order processing capabilities of the processor. A hardware PO may be assigned to the instructions.

At a resource scheduler in the processor may assign the instructions to various executing elements of the processor for execution. In one embodiment at such execution may be performed out of order with respect to the hardware PO and with respect to the original order of the instructions as received at the software scheduler.

At after execution memory operations may be reordered according to hardware PO. Reordering may be performed by an alias hardware unit. The reordering may restore the order of the memory operations as they appeared in the stream of instructions received from the software scheduler. In one embodiment the memory operations may have executed but not yet been retired.

At the alias hardware unit may perform a CAM search for memory operations in the list of memory operations as it is stored in the RAR bank. The memory operation search may be made by searching for an associated MID. In one embodiment more than one memory operation may be returned.

In one embodiment at the alias hardware unit may determine a range of memory operation atoms of RAR bank that will each be checked against the memory operation. Such a determination may be made by accessing a range field in the RAR bank. The range field may have been set according to offsets of the memory operation and may reflect the original order of instructions.

At the alias hardware unit may compare the memory instruction against the atoms within the range may be performed. In one embodiment the alias hardware unit may evaluate the memory instruction against the atoms while considering the instructions as ordered and scheduled by the hardware. Thus the alias hardware unit may evaluate whether the reordering performed by hardware has caused any memory errors or faults. In another embodiment the alias hardware unit may evaluate the instruction against the atoms while considering the instructions as ordered and scheduled by software. Thus the alias hardware unit may evaluate whether the reordering performed by software has caused any memory errors or faults. In various embodiments the alias hardware unit may evaluate the hardware reordering and the software reordering using a set of memory fault logic and rules.

For example at the alias hardware unit may compare the memory operation against the range of atoms using a pipeline stage of execution. The pipeline stage may include criteria defined by a portion of the set of memory fault logic and rules. The comparison may be made nearly simultaneously between the memory operation and each of the range of atoms.

In one embodiment at any atoms for which there is no possibility of a memory error or fault may be removed from consideration. Such a lack of memory error or fault may include for example comparisons between the memory operation and an atom which illustrate that one or more criteria for a memory error or fault have failed.

At the alias hardware unit may determine whether there are any other atoms still pending within the sequence of pipeline stages and whether any more additional pipeline stages are to be executed. If there are pending atoms and there are additional pipeline stages method may proceed to . If there are no more pending atoms or if there are no more additional pipeline stages method may proceed to .

At the alias hardware unit may consider the next pipeline stage of execution. Method may proceed to .

At the alias hardware unit may evaluate whether any faults have been determined so far. If not method may proceed to . If so method may proceed to .

At a memory fault associated with the memory operation may be generated and sent to other portions of the processor. Execution of the instructions may be halted. At execution may be rolled back to a previously known state before allocation and execution of the memory operation that caused the fault. At the memory operation and surrounding instructions may be executed in order. In one embodiment such an order may include the order of instructions as originally received by the software scheduler. Such an order may be reconstructed by for example reference to the virtual age or order of memory operations as stored in the RAR bank. Method may proceed to .

At the alias hardware unit may determine whether there are additional unprocessed memory operations associated with the MID used to access the RAR bank. If so method may proceed to . If not method may proceed to .

At the alias hardware unit may advance to the next unprocessed memory operation. Method may proceed to .

At the alias hardware unit may determine whether there are additional unprocessed MIDs in the RAR bank representing memory operations that are to be evaluated. If so method may proceed to . If not method may proceed to .

At the processor may reclaim resources used in the memory operations if not already performed after determining that a given memory operation did not include memory faults or errors. At the processor may determine whether or not additional instructions to be fetched have arrived and require evaluation. If not method may terminate. If so method may proceed to .

Methods may be initiated by any suitable criteria. Furthermore although methods describe an operation of particular elements methods may be performed by any suitable combination or type of elements. For example method may be performed by MID calculation module or any other element operable to implement method . In another example method may be implemented by the elements illustrated in or any other system operable to implement method . As such the preferred initialization point for methods and the order of the elements comprising methods may depend on the implementation chosen. In some embodiments some elements may be optionally omitted reorganized repeated or combined.

Embodiments of the mechanisms disclosed herein may be implemented in hardware software firmware or a combination of such implementation approaches. Embodiments of the disclosure may be implemented as computer programs or program code executing on programmable systems comprising at least one processor a storage system including volatile and non volatile memory and or storage elements at least one input device and at least one output device.

Program code may be applied to input instructions to perform the functions described herein and generate output information. The output information may be applied to one or more output devices in known fashion. For purposes of this application a processing system may include any system that has a processor such as for example a digital signal processor DSP a microcontroller an application specific integrated circuit ASIC or a microprocessor.

The program code may be implemented in a high level procedural or object oriented programming language to communicate with a processing system. The program code may also be implemented in assembly or machine language if desired. In fact the mechanisms described herein are not limited in scope to any particular programming language. In any case the language may be a compiled or interpreted language.

One or more aspects of at least one embodiment may be implemented by representative instructions stored on a machine readable medium which represents various logic within the processor which when read by a machine causes the machine to fabricate logic to perform the techniques described herein. Such representations known as IP cores may be stored on a tangible machine readable medium and supplied to various customers or manufacturing facilities to load into the fabrication machines that actually make the logic or processor.

Such machine readable storage media may include without limitation non transitory tangible arrangements of articles manufactured or formed by a machine or device including storage media such as hard disks any other type of disk including floppy disks optical disks compact disk read only memories CD ROMs compact disk rewritables CD RWs and magneto optical disks semiconductor devices such as read only memories ROMs random access memories RAMs such as dynamic random access memories DRAMs static random access memories SRAMs erasable programmable read only memories EPROMs flash memories electrically erasable programmable read only memories EEPROMs magnetic or optical cards or any other type of media suitable for storing electronic instructions.

Accordingly embodiments of the disclosure may also include non transitory tangible machine readable media containing instructions or containing design data such as Hardware Description Language HDL which defines structures circuits apparatuses processors and or system features described herein. Such embodiments may also be referred to as program products.

In some cases an instruction converter may be used to convert an instruction from a source instruction set to a target instruction set. For example the instruction converter may translate e.g. using static binary translation dynamic binary translation including dynamic compilation morph emulate or otherwise convert an instruction to one or more other instructions to be processed by the core. The instruction converter may be implemented in software hardware firmware or a combination thereof. The instruction converter may be on processor off processor or part on and part off processor.

Thus techniques for performing one or more instructions according to at least one embodiment are disclosed. While certain exemplary embodiments have been described and shown in the accompanying drawings it is to be understood that such embodiments are merely illustrative of and not restrictive on other embodiments and that such embodiments not be limited to the specific constructions and arrangements shown and described since various other modifications may occur to those ordinarily skilled in the art upon studying this disclosure. In an area of technology such as this where growth is fast and further advancements are not easily foreseen the disclosed embodiments may be readily modifiable in arrangement and detail as facilitated by enabling technological advancements without departing from the principles of the present disclosure or the scope of the accompanying claims.

