---

title: Highly scalable tree-based trylock
abstract: A tree-based trylock technique for reducing contention on a root trylock includes attempting to acquire a trylock at each node of a tree-based hierarchical node structure while following a traversal path that begins at a leaf node, passes through one or more of internal nodes, and ends at a root node having the root trylock. The trylock acquisition operation succeeds if each trylock on the traversal path is acquired, and fails if any trylock on the traversal path cannot be acquired. A trylock housekeeping operation releases all non-root trylocks visited by the trylock acquisition operation, such that if the trylock acquisition operation succeeds, only the root trylock will be remain acquired at the end of the operation, and if the trylock acquisition operation fails, none of the trylocks will be remain acquired at the end of the operation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09400818&OS=09400818&RS=09400818
owner: International Business Machines Corporation
number: 09400818
owner_city: Armonk
owner_country: US
publication_date: 20130916
---
This application is a continuation under 35 U.S.C. 120 of application Ser. No. 13 924 667 filed Jun. 24 2013 entitled Highly Scalable Tree Based Trylock. 

The present disclosure relates to computer systems and methods in which data resources are shared among data consumers while preserving data integrity and consistency relative to each consumer. More particularly the disclosure concerns a hierarchical locking technique that may be implemented as part of a mutual exclusion mechanism known as read copy update or in other computing environments.

By way of background read copy update also known as RCU is a mutual exclusion technique that permits shared data to be accessed for reading without the use of locks writes to shared memory memory barriers atomic instructions or other computationally expensive synchronization mechanisms while still permitting the data to be updated modify delete insert etc. concurrently. The technique is well suited to both uniprocessor and multiprocessor computing environments wherein the number of read operations readers accessing a shared data set is large in comparison to the number of update operations updaters and wherein the overhead cost of employing other mutual exclusion techniques such as locks for each read operation would be high. By way of example a network routing table that is updated at most once every few minutes but searched many thousands of times per second is a case where read side lock acquisition would be quite burdensome.

The read copy update technique implements data updates in two phases. In the first initial update phase the actual data update is carried out in a manner that temporarily preserves two views of the data being updated. One view is the old pre update data state that is maintained for the benefit of read operations that may have been referencing the data concurrently with the update. The other view is the new post update data state that is seen by operations that access the data following the update. In the second deferred update phase the old data state is removed following a grace period that is long enough to ensure that the first group of read operations will no longer maintain references to the pre update data. The second phase update operation typically comprises freeing a stale data element to reclaim its memory. In certain RCU implementations the second phase update operation may comprise something else such as changing an operational state according to the first phase update.

It is assumed that the data element list of is traversed without locking by multiple readers and occasionally updated by updaters that delete insert or modify data elements in the list. In the data element B is being referenced by a reader r as shown by the vertical arrow below the data element. In an updater u wishes to update the linked list by modifying data element B. Instead of simply updating this data element without regard to the fact that r is referencing it which might crash r u preserves B while generating an updated version thereof shown in as data element B and inserting it into the linked list. This is done by u acquiring an appropriate lock to exclude other updaters allocating new memory for B copying the contents of B to B modifying B as needed updating the pointer from A to B so that it points to B and releasing the lock. In current versions of the Linux kernel pointer updates performed by updaters can be implemented using the rcu assign pointer primitive. As an alternative to locking during the update operation other techniques such as non blocking synchronization or a designated update thread could be used to serialize data updates. All subsequent post update readers that traverse the linked list such as the reader r will see the effect of the update operation by encountering B as they dereference B s pointer. On the other hand the old reader r will be unaffected because the original version of B and its pointer to C are retained. Although r will now be reading stale data there are many cases where this can be tolerated such as when data elements track the state of components external to the computer system e.g. network connectivity and must tolerate old data because of communication delays. In current versions of the Linux kernel pointer dereferences performed by readers can be implemented using the rcu dereference primitive.

At some subsequent time following the update r will have continued its traversal of the linked list and moved its reference off of B. In addition there will be a time at which no other reader process is entitled to access B. It is at this point representing an expiration of the grace period referred to above that u can free B as shown in .

In the context of the read copy update mechanism a grace period represents the point at which all running tasks e.g. processes threads or other work having access to a data element guarded by read copy update have passed through a quiescent state in which they can no longer maintain references to the data element assert locks thereon or make any assumptions about data element state. By convention for operating system kernel code paths a context switch an idle loop and user mode execution all represent quiescent states for any given CPU running non preemptible code as can other operations that will not be listed here . The reason for this is that a non preemptible kernel will always complete a particular operation e.g. servicing a system call while running in process context prior to a context switch.

In four tasks and running on four separate CPUs are shown to pass periodically through quiescent states represented by the double vertical bars . The grace period shown by the dotted vertical lines encompasses the time frame in which all four tasks that began before the start of the grace period have passed through one quiescent state. If the four tasks and were reader tasks traversing the linked lists of or none of these tasks having reference to the old data element B prior to the grace period could maintain a reference thereto following the grace period. All post grace period searches conducted by these tasks would bypass B by following the updated pointers created by the updater.

Grace periods may be synchronous or asynchronous. According to the synchronous technique an updater performs the first phase update operation blocks waits until a grace period has completed and then implements the second phase update operation such as by removing stale data. According to the asynchronous technique an updater performs the first phase update operation specifies the second phase update operation as a callback then resumes other processing with the knowledge that the callback will eventually be processed at the end of a grace period. Advantageously callbacks requested by one or more updaters can be batched e.g. on callback lists and processed as a group at the end of an asynchronous grace period. This allows asynchronous grace period overhead to be amortized over plural deferred update operations.

More recently RCU grace period processing has been adapted to account for processor low power states such as on Intel processors the C1E halt state or the C2 or deeper halt states . Operating systems can take advantage of low power state capabilities by using mechanisms that withhold regular timer interrupts from processors in a low power state unless the processors need to wake up to perform work. The dynamic tick framework also called dyntick or nohz in existing versions of the Linux kernel is one such mechanism. In RCU implementations designed for low power applications in the Linux kernel a compiler configuration option called RCU FAST NO HZ is available. This option allows processors to be placed in low power states even if there are pending RCU callbacks provided none require immediate invocation and the processor is not needed for grace period advancement processing.

One characteristic of the RCU FAST NO HZ option is that quiescent states are periodically forced in order to expedite callback processing so that processors can enter low power states more quickly. Quiescent state forcing is regulated by a global lock that serializes access to the quiescent state forcing mechanism. The global quiescent state forcing lock is acquired only with a trylock primitive called raw spin trylock irqsave which either immediately acquires the lock or returns failure. As such the contention on the global quiescent state forcing lock should be zero. Unfortunately on large systems thousands of CPUs enabling RCU FAST NO HZ has historically been susceptible to extreme memory contention due to a high rate of attempts to acquire the global quiescent state forcing lock resulting in throughput dropping to nearly zero. This high level of memory contention can also result from RCU s implementation of the asynchronous callback processing primitive call rcu which can invoke quiescent state forcing when large numbers of RCU callbacks are enqueued on the CPU in question.

Although an immediate solution is to disable RCU FAST NO HZ on large systems this approach results in sub optimal energy efficiency. Accordingly there is a need for a technique for acquiring the global quiescent state forcing lock with reduced memory contention. Other RCU operations as well as many non RCU operations could likewise benefit from a locking technique with reduced lock contention in systems with many processors.

A method system and computer program product are provided to implement tree based trylock operations that reduce contention on a root trylock in a computer system having two or more processors operatively coupled to one or more memory devices. In an example embodiment a lock hierarchy is provided in which plural trylocks are distributed among nodes of a tree based node structure having a plurality of leaf nodes one or more internal nodes and a root node. The processors are assigned to the leaf nodes in a distributed and balanced manner in order to minimize memory contention on the trylocks. A trylock acquisition operation is implemented on a selected one of the processors for acquiring a root trylock associated with the root node. The trylock acquisition operation attempts to acquire one of the trylocks at each node of the node structure that lies on a traversal path beginning at one of the leaf nodes passing through one or more of the internal nodes and ending at the root node. The trylock acquisition operation succeeds if each trylock on the traversal path is acquired and fails if any trylock on the traversal path cannot be acquired. A trylock housekeeping operation releases all non root trylocks visited by the trylock acquisition operation such that if the trylock acquisition operation succeeds only the root trylock will remain acquired at the end of the operation and if the trylock acquisition operation fails none of the trylocks will be remain acquired at the end of the operation.

In an example embodiment the root trylock guards a guarded operation for which duplicate initiations are redundant or guards a global lock that in turn guards the guarded operation.

In an example embodiment the trylock acquisition operation further includes checking at each node on the traversal path a condition indicating that an operation protected by the root trylock has already been initiated by another one of the processors and failing the trylock acquisition operation if the condition exists.

In an example embodiment the trylock housekeeping operation is performed at each of the nodes on the traversal path by releasing a trylock acquired at an immediately preceding node.

In an example embodiment a global lock acquisition is performed to acquire a global lock after the trylock acquisition operation successfully acquires the root trylock the root trylock being released if the global lock acquisition is successful.

In an example embodiment the global lock acquisition operation includes checking a condition indicating that an operation guarded by the global lock has already been initiated by another one of the processors and failing the global lock acquisition operation if the condition exists.

The present disclosure provides a hierarchy of conditional trylocks with a trylock at each node of the hierarchy including a root trylock at the root node of the hierarchy that guards an operation for which duplicate initiations are redundant or which indirectly guards the operation by guarding a global lock that directly guards the operation. One example of such an operation is quiescent state forcing as described in the Background section above. In order to reduce contention on the root trylock each CPU or thread is assigned to one of the leaf nodes and conditionally acquires a trylock at each level of the hierarchy in an at most once manner beginning at its assigned leaf node and moving upwardly through the hierarchy in leaf to root fashion.

To conditionally acquire a given trylock a CPU thread asserts the trylock and immediately checks the result. If the acquisition fails the overall acquisition fails. Otherwise if the acquisition succeeds the CPU thread moves up to the next level of the hierarchy and repeats. Regardless whether the root trylock acquisition effort succeeds or fails at some point along the node traversal path all non root trylocks must be released. Releasing these lower level trylocks may be efficiently handled by the CPU thread at each node as it moves upwardly through the lock hierarchy. In particular after or before the CPU thread attempts to acquire a trylock at a given level of the hierarchy it may release the prior trylock acquired in the immediately preceding level. Other trylock release methods may also be used.

If the CPU thread acquires the trylock at the root of the hierarchy it has acquired the root trylock. Additionally a global lock that protects a particular operation may be provided as a separate lock that is acquired by the CPU thread after it acquires the root trylock. In the example embodiment in which the operation in question is RCU quiescent state forcing such an additional global lock is used because it already exists in RCU implementations for the Linux kernel. The purpose of the global lock is to synchronize setting of a flag and a wakeup to the quiescent state forcing operation protected by the lock. In this embodiment additional efficiency may be obtained by checking the flag at each level of the hierarchy. If the flag is already set then a quiescent state forcing operation has already been requested and there is no need to push further up the hierarchy. Other embodiments may likewise use this technique checking the state of a flag or other condition indicator as the lock hierarchy is traversed in order to avoid wasted effort.

Turning now to the figures wherein like reference numerals represent like elements in all of the several views illustrates an example multiprocessor computer system in which the grace period processing technique described herein may be implemented. In a computer system includes multiple processors . . . a system bus and a program memory . There are also cache memories . . . and cache controllers . . . respectively associated with the processors . . . . A conventional memory controller is again associated with the memory . As shown the memory controller may reside separately from processors . . . e.g. as part of a chipset .

The computer system may represent any of several different types of computing apparatus. Such computing apparatus may include but are not limited to general purpose computers special purpose computers portable computing devices communication and or media player devices set top devices embedded systems and other types of information handling machines. The term processor as used with reference to the processors . . . encompasses any program execution unit capable of executing program instructions including but not limited to a packaged integrated circuit device such as a microprocessor a processing core within a packaged integrated circuit device such as a microprocessor core or a hardware thread comprising one or more functional units within a processing core such as an SMT thread . Each such execution unit may be referred to as a CPU central processing unit . The processors . . . may be situated within a single computing device or node e.g. as part of a single node SMP system or they may be distributed over plural nodes e.g. as part of a NUMA system a cluster or a cloud . The memory may comprise any type of tangible storage medium capable of storing data in computer readable form for use in program execution including but not limited to any of various types of random access memory RAM various flavors of programmable read only memory PROM such as flash memory and other types of primary storage i.e. program memory . The cache memories . . . may be implemented in several levels e.g. as level 1 level 2 and level 3 caches and the cache controllers . . . may collectively represent the cache controller logic that supports each cache level. As illustrated the memory controller may reside separately from processors . . . for example as part of a discrete chipset. Alternatively the memory controller could be provided by plural memory controller instances that are respectively integrated with the processors . . . .

Each CPU embodied by a given processor is operable to execute program instruction logic under the control of a software program stored in the memory or elsewhere . As part of this program execution logic update operations updaters may execute within a process thread or other execution context hereinafter task on any of the processors . Each updater runs periodically to perform updates on a set of shared data that may be stored in the shared memory or elsewhere . In reference numerals . . . illustrate individual data updaters that respectively execute on the several processors . . . . As described in the Background section above the updates performed by an RCU updater can include modifying elements of a linked list inserting new elements into the list deleting elements from the list and other types of operations. To facilitate such updates the processors may be programmed from instructions stored in the memory or elsewhere to implement a read copy update RCU subsystem as part of their processor functions. In reference numbers . . . represent individual RCU instances that may periodically execute on the several processors . . . . Any given processor may also execute a read operation reader . Each reader runs from program instructions stored in the memory or elsewhere in order to periodically perform read operations on the set of shared data stored in the shared memory or elsewhere . In reference numerals . . . illustrate individual reader instances that may respectively execute on the several processors . . . . Such read operations will typically be performed far more often than updates this being one of the premises underlying the use of read copy update. Moreover it is possible for several of the readers to maintain simultaneous references to one of the shared data elements while an updater updates the same data element.

During operation of the computer system an updater will occasionally perform an update to one of the shared data elements . In accordance the philosophy of RCU a first phase update may be performed in a manner that temporarily preserves a pre update view of the shared data element for the benefit of readers that may be concurrently referencing the shared data element during the update operation. Following the first phase update the updater may register a callback with the RCU subsystem for the deferred destruction of the pre update view following a grace period second phase update . As described in the Background section above this is known as asynchronous grace period processing. Alternatively the updater may request a synchronous expedited grace period.

The grace period processing performed by the RCU subsystem entails starting new grace periods and detecting the end of old grace periods so that the RCU subsystem knows when it is safe to free stale data or take other actions . Grace period processing may further entail the management of callback lists that accumulate callbacks until they are ripe for batch processing at the end of a given grace period.

Turning now to example components of the RCU subsystem are shown. Among these components is a set of RCU subsystem data structures that includes an RCU state structure for tracking per processor quiescent states. For purposes of the present embodiment it may be assumed that the RCU subsystem is configured as a hierarchical RCU implementation as is conventionally provided in large multiprocessor computer systems running the Linux kernel. In that case the RCU subsystem data structures will further include a tree of RCU nodes embedded as a linear array within the RCU state structure . illustrates an example three level RCU node tree that includes six bottom level leaf RCU nodes three intermediate level internal RCU nodes a top level root RCU node .

In addition to the embedded RCU node tree the RCU state structure includes a quiescent state indicator for indicating when all of the processors have passed through a quiescent state and a grace period may be ended. The RCU state structure also includes a quiescent state force flag to indicate that a quiescent state forcing operation has been requested by one of the processors .

One of the purposes of the RCU node tree is to reduce contention for the quiescent state indicator which could be problematic if the indicator was protected by a single global lock. See P. McKenney Hierarchical RCU Nov. 4 2008. Instead of all the processors using a global lock to access the quiescent state indicator subsets of the processors are assigned to individual leaf RCU nodes . Similarly subsets of the leaf RCU nodes are assigned to individual internal RCU nodes . The internal RCU nodes are likewise assigned to the root RCU node . As shown in each leaf RCU node maintains an array A of quiescent state bits with each bit corresponding to one of the leaf node s assigned processors. By way of example shows four quiescent state bits associated with four of the processors . A bit array lock B serializes access to the quiescent state bit array A. Similarly each internal RCU node maintains an array A of quiescent state bits with each bit corresponding to one of the internal node s assigned RCU leaf nodes . A bit array lock B serializes access to the quiescent state bit array A. The root RCU node maintains an array A of quiescent state bits to which the internal RCU nodes are respectively assigned. A bit array lock B serializes access to the quiescent state bit array A.

When a processor passes through a quiescent state it asserts the RCU node lock B in its assigned leaf RCU node and sets its assigned quiescent state bit in the bit array A. To propagate the quiescent state information upwardly through the RCU node tree the last processor to set its bit in a leaf RCU node acquires the RCU node lock B in the internal RCU node to which the leaf node is assigned and sets the internal node s quiescent state bit in the bit array A. In similar fashion the last processor to set a bit in an internal RCU node acquires the RCU node lock B in the root RCU node and sets the root node s quiescent state bit in the bit array A. Finally the last processor to set a bit in the root node bit array A accesses and sets the quiescent state indicator thereby signaling that the current RCU grace period may end.

It should be noted that a production read copy update implementation will typically include many additional data structures that are not shown in . A discussion of such data structures is omitted for ease of description and in order to focus attention on the tree based trylock technique disclosed herein. As will be described in more detail below the RCU node tree may be conveniently used to provide a hierarchical tree structure for the new trylock technique. Other hierarchical trees may be used in other implementations.

Returning now to the components of the RCU subsystem also include several RCU subsystem support functions namely an RCU reader API Application Programming Interface an RCU updater API and a set of grace period detection and callback functions .

As shown in the RCU reader API comprises a reader registration component A and a reader unregistration component B. These components are respectively invoked by readers as they enter and leave their RCU read side critical sections. This allows the RCU subsystem to track reader operations and determine when readers are engaged in RCU protected read side critical section processing. In an example embodiment the reader registration component A and the reader unregistration component B may be respectively implemented using the rcu read lock and rcu read unlock primitives found in existing read copy update implementations.

As also shown in the RCU updater API comprises a register callback component A. The register callback component A is used by updaters to register a callback following a first phase update to a shared data element . An invocation of the register callback component A initiates processing that places the callback on one of the RCU callback lists associated with the processor that runs the updater . This may start an asynchronous grace period if one is not already underway so that the callback can be processed after the grace period has ended as part of second phase update processing to remove stale data or perform other actions . In an example embodiment the register callback component A may be implemented using the existing call rcu primitive found in conventional read copy update implementations.

With continuing reference to the grace period detection and callback processing component of the RCU subsystem include a force quiescent state component that implements the tree based trylock technique disclosed herein. The grace period detection and callback processing component also includes a number of other conventional RCU components that are responsible for various operations such as starting new grace periods detecting the end of old grace periods and processing callbacks as grace periods end. A discussion of such components is omitted for ease of description and in order to focus attention on the tree based trylock technique disclosed herein.

As described in the Introduction section above the tree based trylock technique disclosed herein contemplates a hierarchical tree of nodes with a trylock at each node and including a root trylock at the root node of the hierarchy. As is known in the art a trylock is a lock having at most once semantics such that lock acquisition is tried only once and either succeeds or fails. The processors are assigned to different leaf nodes in a distributed and balanced manner in order to minimize memory contention. A CPU or a thread running within a CPU that desires to acquire the root trylock begins at one of the leaf nodes of the lock hierarchy and works its way to the root trylock node following a traversal path conditionally acquiring trylocks at lower level nodes as it does so and releasing all trylocks and abandoning the acquisition effort if it fails to acquire a trylock at any given level. As stated above in a hierarchical RCU implementation as described above in connection the RCU nodes and of the RCU node hierarchy provide convenient data structures for holding the tree based trylocks contemplated by the present disclosure. Such tree based trylocks are shown in the RCU node diagrams of and are identified as force quiescent state FQS locks insofar as the present embodiment uses the trylocks to serialize access to the quiescent state force flag shown in . In the leaf RCU node is shown as having a force quiescent state trylock . In the internal RCU node is shown as having a force quiescent state trylock . In the root RCU node is shown as having a root force quiescent state trylock . In addition the root RCU node includes a global force quiescent state lock which may or may not be a trylock that protects access to the quiescent state force indicator .

With reference now to example C language pseudocode that may be used to implement the force quiescent state component is shown. The function name given to the quiescent state component is force quiescent state The argument to this function is a pointer rsp to the RCU state structure shown in . The purpose of the function is to set the quiescent state force flag which is given the name GP FLAGS FQS in the code and is implemented as a bit in flags field of the RCU state structure called rsp gp flags. Line of the code disables interrupts in order to pin the current task onto a particular CPU allowing line to obtain a stable reference to this CPU s leaf RCU node named rnp . As an alternative it is possible to take a snapshot of the CPU number without disabling interrupts. This alternative approach risks additional cache misses but provides better scheduling latency. Moreover a user level implementation of the disclosed tree based trylock technique would normally be incapable of disabling interrupts.

The loop spanning lines traverses the RCU node tree see from leaf to root. Line checks the quiescent state flag to see if another task already performed the required work and if not line attempts to acquire the current RCU node s force quiescent state trylock see elements and of respectively referred to in the code as fqslock. To expedite the memory access each force quiescent state trylock or may be aligned to its own cache line. The Boolean result of lines and is stored in the ret variable. If line determines that a trylock acquired during the previous pass through the loop is still being held line releases it using the tree unlock component of . Line tests the ret variable. If the line determined that the quiescent state force flag is already set or if the attempt on line to acquire the force quiescent state trylock or failed line increments a statistical counter which can lose counts line re enables interrupts and line returns to the caller. Otherwise line prepares for the next pass through the loop.

Upon exit from the loop the root RCU node will have been reached and the root force quiescent state trylock will have been acquired. Line unconditionally acquires the RCU node s global force quiescent state lock whose name in the code is simply lock. Line releases the root force quiescent state lock using the tree unlock component of . Line optionally makes one final check of the quiescent state force flag to see if it is already set by another task and if so lines increment the statistical counter release the root RCU node s global force quiescent state lock and return to the caller. Otherwise line sets the quiescent state force flag line releases the root RCU node s global force quiescent state lock and line wakes up a thread that attempts to force quiescent states in order to end the current grace period.

It is possible to abstract the forgoing tree based trylock operations for use in other environments it being understood that the RCU force quiescent state scenario described above is merely on example embodiment of the disclosed subject matter. More generally any case where a large number of CPUs might need to awaken or otherwise signal a specific CPU thread to perform an operation with at most once semantics can use the disclosed mechanism. This sort of arrangement offers cache locality advantages and also a reduction in synchronization overhead in cases where a specific type of processing can be guaranteed to never consume more than one CPU s worth of processing.

The first few lines of illustrates an abstract tree lock node that is analogous to the RCU nodes and of the RCU node tree of . also illustrates an abstract tree try lock function that is analogous to the loop spanning lines of the force quiescent state function in . As previously described this loop attempts to acquire the root RCU node s force quiescent state trylock shown in . likewise illustrates an abstract tree unlock function that is analogous to the raw spin unlock function on line in that unlocks the root RCU node s force quiescent state trylock .

Lines of show the structure of the aforementioned tree lock node. A set of tree lock nodes is arranged hierarchically in a tree of such nodes a tree lock tree one example of which is shown by reference number in . The tree lock tree includes a set of leaf tree lock nodes a set of internal tree lock nodes and a top level root tree lock node . As shown in line of each of the tree lock nodes and contains a trylock that is named try lock. As shown in the trylock in the leaf tree lock nodes is identified by reference number the trylock in the internal tree lock nodes is identified by reference number and the trylock in the root tree lock node is identified by reference number the root trylock . As shown in line each of the tree lock nodes and also contains a reference to the node s immediate parent with the root node s parent field being NULL. As shown in line each of the lower level tree lock nodes and also contains a reference to the root tree lock node as does the root tree lock node itself. As shown in line each of the tree lock nodes and may optionally contain a failcount field. This field is used as a failure counter that is analogous to the statistics counter used in lines and of except that it is maintained on a per node basis rather than globally as in . The procedures to initialize the full tree lock tree and to point threads at their respective leaf nodes are straightforward tasks that will not be belabored here.

The tree try lock function shown on lines of traverses the tree lock tree in an effort to acquire the root trylock . The check function pointer passed as an argument in line is analogous to the check in line of the force quiescent state function of to determine if a flag or other variable guarded by the root lock being acquired is already set. The check function takes arg as its sole parameter. The loop spanning lines traverses the tree lock tree from leaf to root starting with a specified leaf tree lock node passed as an argument to the tree try lock function. Line sets local variable ret to true if either the check function determines that the required work is already done or the spin trylock function fails to acquire the current tree lock node s trylock or either of which will terminate the traversal up the tree lock tree . Lines and release the trylock acquired if any during the previous pass through the loop. Line increments this node s failure counter and line returns to the caller to indicate failure. Otherwise line prepares for the next pass through the loop. If these operations succeed in locking the root trylock line reports success to the caller. Lines show the tree unlock function which must be called after the tree try lock function reports success. This function simply releases the root trylock .

An example usage of the tree based try lock technique of is shown in . Line of declares the variable protected by the root trylock a flag variable called myflag that is analogous to the gp flags variable checked in lines and of . Line defines the value to which myflag is be set in order to initiate some operation for which duplicate initiations are redundant. This value is defined as a flag called MY BIT that has magnitude of 0x1. The MY BIT value is analogous to the RCU GP FLAG FQS value checked in lines and of . Line of declares a global lock called my lock that protects the myflag variable. This global lock could be located anywhere in memory including in the root tree lock node if desired. Optionally the global lock could be eliminated and the root trylock could be used exclusively to guard the myflag variable.

The code of shows how the disclosed tree based trylock technique may be used to efficiently and scalably provide the do at most once semantics that are required for posting new work to a thread that implements an operation for which duplicate initiations are redundant.

Turning now to flow diagrams are shown to further elucidate operations of the tree try lock function and the set my bit function Operations that are considered optional such as the check function and statistics logging are shown in dashed line representation. It is assumed that a tree lock tree such as that shown in has been established and initialized. Block of illustrating operations of the tree try lock function starts the loop of lines in . So long as the root tree lock node has not been processed processing proceeds to block which releases the trylock of the previous tree lock node so long as the current node is not the initial leaf tree lock node see line of . Preferably the lower level trylock is not released until after the next block is implemented. However the lower level trylock release can be performed beforehand. Blocks and implement line of . Block attempts to acquire the trylock belonging to the current tree lock node. Optional block checks to see if the MY BIT flag has already been set. If the trylock acquisition is unsuccessful or if the MY BIT flag is set optional block increments the current tree lock node s failcount counter see line of and block returns failure see line of . If the trylock acquisition in block is successful and if the MY BIT flag is not already set block prepares for the next pass through the loop see line of . Assuming the loop successfully processes the root tree lock node in the foregoing manner block returns success to the caller see line in .

Turning now to which illustrates the set my bit function of block attempts to invoke the tree try lock function and returns failure in block if unsuccessful see lines of . If the root trylock is acquired block acquires the global mylock guarding the variable of interest namely the myflag variable see line of . Note that as an alternative to using the global mylock the root trylock could serve as the global lock that protest the myflag variable. Block releases the root trylock see line of . Optional block checks to see if the MY BIT flag is already set and if it is block releases the global mylock and block returns to the caller see lines in . Assuming the MY BIT flag is not set block sets the MY BIT flag in the myflag variable see line of block releases the global mylock and block returns to the caller see line of .

Accordingly a highly scalable tree based trylock technique has been disclosed. It will be appreciated that the foregoing concepts may be variously embodied in any of a data processing system a machine implemented method and a computer program product in which programming logic is provided by one or more machine readable non transitory data storage media for use in controlling a data processing system to perform the required functions. Example embodiments of a data processing system and machine implemented method were previously described in connection with . With respect to a computer program product digitally encoded program instructions may be stored on one or more computer readable non transitory data storage media for use in controlling a computer or other digital machine or device to perform the required functions. The program instructions may be embodied as machine language code that is ready for loading and execution by the machine apparatus or the program instructions may comprise a higher level language that can be assembled compiled or interpreted into machine language. Example languages include but are not limited to C C assembly to name but a few. When implemented on a machine comprising a processor the program instructions combine with the processor to provide a particular machine that operates analogously to specific logic circuits which themselves could be used to implement the disclosed subject matter.

Example computer readable non transitory data storage media for storing such program instructions are shown by reference numerals memory and cache of the computer system of . The system may further include one or more secondary or tertiary storage devices not shown that could store the program instructions between system reboots. A further example of computer readable non transitory data storage media that may be used to store the program instructions is shown by reference numeral in . The data storage media are illustrated as being portable optical storage disks of the type that are conventionally used for commercial software sales such as compact disk read only memory CD ROM disks compact disk read write CD R W disks and digital versatile disks DVDs . Such data storage media can store the program instructions either alone or in conjunction with an operating system or other software product that incorporates the required functionality. The computer readable non transitory data storage media could also be provided by other portable data storage media such as floppy disks flash memory sticks etc. or data storage media combined with drive systems e.g. disk drives . As is the case with the memory and the cache of the computer readable non transitory data storage media may be incorporated in data processing platforms that have integrated random access memory RAM read only memory ROM or other semiconductor or solid state memory all of which represent further examples of computer readable non transitory data storage media. More broadly the computer readable non transitory data storage media could comprise any electronic magnetic optical infrared semiconductor system or apparatus or device or any other tangible entity representing a machine manufacture or composition of matter that can contain store communicate or transport the program instructions for use by or in connection with an instruction execution system apparatus or device such as a computer. For all of the above forms of computer readable non transitory data storage media when the program instructions are loaded into and executed by an instruction execution system apparatus or device the resultant programmed system apparatus or device becomes a particular machine for practicing embodiments of the method s and system s described herein.

Although various example embodiments have been shown and described it should be apparent that many variations and alternative embodiments could be implemented in accordance with the disclosure. It is understood therefore that the invention is not to be in any way limited except in accordance with the spirit of the appended claims and their equivalents.

