---

title: Multi-space connected virtual data objects
abstract: A see-through head mounted display apparatus includes a display and a processor. The processor determines geo-located positions of points of interest within a field of view and generates markers indicating information regarding an associated real world object is available to the user. Markers are rendered in the display relative to the geo-located position and the field of view of the user. When a user selects a marker though a user gesture, the device displays a near-field virtual object having a visual tether to the marker simultaneously with the marker. The user may interact with the marker to view, add or delete information associated with the point of interest.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09235051&OS=09235051&RS=09235051
owner: Microsoft Technology Licensing, LLC
number: 09235051
owner_city: Redmond
owner_country: US
publication_date: 20130618
---
Mixed reality is a technology that allows virtual imagery to be mixed with a real world physical environment. A see through head mounted display HMD device may be worn by a user to view the mixed imagery of real objects and virtual objects displayed in the user s field of view. A user may further interact with virtual objects for example by performing hand head or voice gestures to move the objects alter their appearance or simply view them.

HMDs may be capable of providing a large amount of information on object in a real world environment to a wearer. In a real world environment rendering all the available information associated with real world objects in an HMD results in a crowded display making it difficult to understand one s physical environment.

Technology for the display of information on points of interest in a see through head mounted display device is provided. For any scene which a wearer of an HMD has a field of view a number of markers of physical points of interest in the field of view are displayed to the user. Each marker represents an indication that additional information regarding the point of interest is available. The wearer may select markers in the scene and be provided with additional information in the form of a near field virtual object. The near field virtual object is displayed with a visual indication or tether to the marker. When a user physically moves the display shows the near field object and markers relative to the user movement and positions of the markers.

In one aspect a see through head mounted display device includes a display and a processor. The processor determines geo located positions of points of interest within a field of view of a user and generates markers indicating information regarding an associated real world object is available to the user. Markers are rendered in the display relative to the geo located position and the field of view of the user. When a user selects a marker though a user gesture the device displays a near field virtual object having a visual tether to the marker simultaneously with the marker. The user may interact with the marker to view add or delete the information.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

Technology is presented which allows for the display of information on points of interest in a see through head mounted display device. The technology allows a wearer of a device to select and view a multitude of different types of information for an environment in an uncluttered manner. For any scene which a wearer of an HMD has a field of view a number of markers for points of interest in the field of view are displayed to the user. Each marker represents an indication that additional information regarding the point of interest is available. The wearer of may select markers in the scene and be provided with additional information in the form of a near field virtual object. The near field virtual object is displayed with a visual indication or tether to the original location of the marker. The technology detects user movement and calibrates the display to show the near field object and markers relative to the user movement and positions of the markers. The wearer can close a near field object and select a different object. The markers may be references in relation to a geographic coordinate system in real world space and the information provided by third party service providers.

The technology makes use of a see through head mounted display device. A see through head mounted display device may include a display element. The display element is to a degree transparent so that a user can look through the display element at real world objects within the user s field of view FOV . The display element also provides the ability to project virtual images into the FOV of the user such that the virtual images may also appear alongside the real world objects. The system automatically tracks where the user is looking so that the system can determine where to insert the virtual image in the FOV of the user. Once the system knows where to project the virtual image the image is projected using the display element.

In one alternative one or more of the processing units may cooperate to build a model of the environment including the positions of all users real world objects and virtual three dimensional objects in the room or other environment. The positions may be reference by a local Cartesian coordinate system using for example a location or the wearer as a point of reference or a global coordinate system. The positions of each head mounted display device worn by the users in the environment may be calibrated to the model of the environment and to each other. This allows the system to determine each user s line of sight and FOV of the environment. Thus a virtual object may be displayed to each user but the display of the virtual image from each user s perspective may be relative adjusting the virtual image for parallax and any occlusions from or by other objects in the environment. The model of the environment referred to herein as a scene map as well as all tracking of the user s FOV and objects in the environment may be generated by the mobile processing unit.

In one embodiment as a user moves around within a mixed reality environment virtual objects may be repositioned relative to the positions of each user or may remain static relative to a fixed geographic coordinate location.

Any number of different types of social media services may provide information on the points of interest within the environment . Examples of social media services which provide such types of information include Yelp Foursquare and SoCL. Without a see through head mounted display user would be gather a view information provided from such social media services on a computing device. Such devices have the ability determine points of interest for the user from the user s location in the environment which are within a given range from the user . Each service provides additional information on the point of interest within the environment and the user accesses such information by selecting one or more links provided by the social media service.

Markers need not be from third party sources but may represent user generated information. Marker may be a user generated point of interest previously marked by the user or marked by a user s and transmitted to the user or user s stored information. Marker may illustrate a favorite location of the user previously defined by the user using the present technology.

Each of the markers is rendered so that they appear physically near to the point of interest with which they are associated. The appearance is generated by the see through head mounted display device to provide the user with a perspective that the marker is physically close to the point of interest.

As illustrated in markers may have an appearance associated with their distance from the wearer. In some points of interest which are physically closer to the user see such as marker appear larger and relative to the location with which they are associated then markers which are physically more distant such as marker . As such the relationship of the marker to the point of interest in the user may be understood by the user based on the relative distances to both the user and the point of interest.

In alternative embodiments markers may be provided with an alternative appearance in terms of size highlighting or other means as a result of other factors. One such factor may be the provision of highlighting by the operator or owner of a point of interest for which a marker is associated.

While viewing near field object it is possible that the wearer may change positions relative to the environment. illustrates that as the user s perspective changes relative to the marker the user s view of the near field virtual change as well. The manner in which the user s perspective of the near field object will change may depend on whether the near field object is world locked or user locked. In alternative embodiments the near field object may be linked to the global coordinate system or may be linked to a local coordinate system referenced to the wearer.

When a wearer is finished interacting with a near field object the user may close the object by means of a gesture. In one embodiment the object may appear to shrink back to the associated marker in a reverse of the animation represented and discussed above with respect to .

As illustrated in any of the markers within the field of view of the wearer of the see through head mounted display device may generate a near field virtual object by interaction from the user. In the user has selected marker in a new near field object C is generated. A tether visual indication links marker to object C.

Each near field object may be populated with information from one or more sources. The sources which may be provided include the user s own information retrieved from a user database information from the user s social media friends sponsored information from owners of points of interest sponsorship of a marker or information from other third party social media sources including users who are not the wearer s social media friends.

While virtual object shown in appears as a terminal any type of virtual object may emanate from any one or more of the markers. In addition different virtual objects may emanate from each marker. This allows customization of the near field object based on the type of information presented and allows points of interest to define their own types of virtual objects to be displayed to wearers. Types of input which may be retrieved from the see through head mounted display device include any type of information which may be determined by the device relative to user movements user utterances or user motion. The device detects such motion and data to add information through the represented interface of the virtual object . While the user perceives an interaction with the virtual object actual data for the input is determined by the see through head mounted display device.

As seen in a head mounted display device may include an integrated processing unit . In other embodiments the processing unit may be separate from the head mounted display device and may communicate with the head mounted display device via wired or wireless communication.

Head mounted display device which in one embodiment is in the shape of glasses is worn on the head of a user so that the user can see through a display and thereby have an actual direct view of the space in front of the user. The use of the term actual direct view refers to the ability to see the real world objects directly with the human eye rather than seeing created image representations of the objects. For example looking through glass at a room allows a user to have an actual direct view of the room while viewing a video of a room on a television is not an actual direct view of the room. More details of the head mounted display device are provided below.

The processing unit may include much of the computing power used to operate head mounted display device . In embodiments the processing unit communicates wirelessly e.g. WiFi Bluetooth infra red or other wireless communication means to one or more central service .

A portion of the frame of head mounted display device will surround a display that includes one or more lenses. In order to show the components of head mounted display device a portion of the frame surrounding the display is not depicted. The display includes a light guide optical element opacity filter see through lens and see through lens . In one embodiment opacity filter is behind and aligned with see through lens light guide optical element is behind and aligned with opacity filter and see through lens is behind and aligned with light guide optical element . See through lenses and are standard lenses used in eye glasses and can be made to any prescription including no prescription . Light guide optical element channels artificial light to the eye. More details of opacity filter and light guide optical element are provided in U.S. Published Patent Application No. 2012 0127284 entitled Head Mounted Display Device Which Provides Surround Video which application published on May 24 2012 and which publication is incorporated herein by reference in its entirety.

Control circuits provide various electronics that support the other components of head mounted display device . More details of control circuits are provided below with respect to . Inside or mounted to temple are ear phones inertial measurement unit and temperature sensor . In one embodiment shown in the inertial measurement unit or IMU includes inertial sensors such as a three axis magnetometer A three axis gyro B and three axis accelerometer C. The inertial measurement unit senses position orientation and sudden accelerations pitch roll and yaw of head mounted display device . The IMU may include other inertial sensors in addition to or instead of magnetometer A gyro B and accelerometer C.

Microdisplay projects an image through lens . There are different image generation technologies that can be used to implement microdisplay . For example microdisplay can be implemented in using a transmissive projection technology where the light source is modulated by optically active material backlit with white light. These technologies are usually implemented using LCD type displays with powerful backlights and high optical energy densities. Microdisplay can also be implemented using a reflective technology for which external light is reflected and modulated by an optically active material. The illumination is forward lit by either a white source or RGB source depending on the technology. Digital light processing DLP liquid crystal on silicon LCOS and Mirasol display technology from Qualcomm Inc. are all examples of reflective technologies which are efficient as most energy is reflected away from the modulated structure and may be used in the present system. Additionally microdisplay can be implemented using an emissive technology where light is generated by the display. For example a PicoP display engine from Microvision Inc. emits a laser signal with a micro mirror steering either onto a tiny screen that acts as a transmissive element or beamed directly into the eye e.g. laser .

Light guide optical element transmits light from microdisplay to the eye of the user wearing head mounted display device . Light guide optical element also allows light from in front of the head mounted display device to be transmitted through light guide optical element to eye as depicted by arrow thereby allowing the user to have an actual direct view of the space in front of head mounted display device in addition to receiving a virtual image from microdisplay . Thus the walls of light guide optical element are see through. Light guide optical element includes a first reflecting surface e.g. a mirror or other surface . Light from microdisplay passes through lens and becomes incident on reflecting surface . The reflecting surface reflects the incident light from the microdisplay such that light is trapped inside a planar substrate comprising light guide optical element by internal reflection. After several reflections off the surfaces of the substrate the trapped light waves reach an array of selectively reflecting surfaces . Note that one of the five surfaces is labeled to prevent over crowding of the drawing. Reflecting surfaces couple the light waves incident upon those reflecting surfaces out of the substrate into the eye of the user. More details of a light guide optical element can be found in United States Patent Publication No. 2008 0285140 entitled Substrate Guided Optical Devices published on Nov. 20 2008 incorporated herein by reference in its entirety.

Head mounted display device also includes a system for tracking the position of the user s eyes. As will be explained below the system will track the user s position and orientation so that the system can determine the FOV of the user. However a human will not perceive everything in front of them. Instead a user s eyes will be directed at a subset of the environment. Therefore in one embodiment the system will include technology for tracking the position of the user s eyes in order to refine the measurement of the FOV of the user. For example head mounted display device includes eye tracking assembly which has an eye tracking illumination device A and eye tracking camera B . In one embodiment eye tracking illumination device A includes one or more infrared IR emitters which emit IR light toward the eye. Eye tracking camera B includes one or more cameras that sense the reflected IR light. The position of the pupil can be identified by known imaging techniques which detect the reflection of the cornea. For example see U.S. Pat. No. 7 401 920 entitled Head Mounted Eye Tracking and Display System issued Jul. 22 2008 incorporated herein by reference. Such a technique can locate a position of the center of the eye relative to the tracking camera. Generally eye tracking involves obtaining an image of the eye and using computer vision techniques to determine the location of the pupil within the eye socket. In one embodiment it is sufficient to track the location of one eye since the eyes usually move in unison. However it is possible to track each eye separately.

In one embodiment the system will use four IR LEDs and four IR photo detectors in rectangular arrangement so that there is one IR LED and IR photo detector at each corner of the lens of head mounted display device . Light from the LEDs reflect off the eyes. The amount of infrared light detected at each of the four IR photo detectors determines the pupil direction. That is the amount of white versus black in the eye will determine the amount of light reflected off the eye for that particular photo detector. Thus the photo detector will have a measure of the amount of white or black in the eye. From the four samples the system can determine the direction of the eye.

Another alternative is to use four infrared LEDs as discussed above but one infrared CCD on the side of the lens of head mounted display device . The CCD will use a small mirror and or lens fish eye such that the CCD can image up to 75 of the visible eye from the glasses frame. The CCD will then sense an image and use computer vision to find the image much like as discussed above. Thus although shows one assembly with one IR transmitter the structure of can be adjusted to have four IR transmitters and or four IR sensors. More or less than four IR transmitters and or four IR sensors can also be used.

Another embodiment for tracking the direction of the eyes is based on charge tracking. This concept is based on the observation that a retina carries a measurable positive charge and the cornea has a negative charge. Sensors are mounted by the user s ears near earphones to detect the electrical potential while the eyes move around and effectively read out what the eyes are doing in real time. Other embodiments for tracking eyes can also be used.

Some of the components of e.g. room facing camera eye tracking camera B microdisplay opacity filter eye tracking illumination A earphones and temperature sensor are shown in shadow to indicate that there are two of each of those devices one for the left side and one for the right side of head mounted display device . shows the control circuit in communication with the power management circuit . Control circuit includes processor memory controller in communication with memory e.g. D RAM camera interface camera buffer display driver display formatter timing generator display out interface and display in interface .

In one embodiment all of the components of control circuit are in communication with each other via dedicated lines or one or more buses. In another embodiment each of the components of control circuit is in communication with processor . Camera interface provides an interface to the two room facing cameras and stores images received from the room facing cameras in camera buffer . Display driver will drive microdisplay . Display formatter provides information about the virtual image being displayed on microdisplay to opacity control circuit which controls opacity filter . Timing generator is used to provide timing data for the system. Display out interface is a buffer for providing images from room facing cameras to the processing unit . Display in interface is a buffer for receiving images such as a virtual image to be displayed on microdisplay . Display out interface and display in interface communicate with band interface which is an interface to processing unit .

Power management circuit includes voltage regulator eye tracking illumination driver audio DAC and amplifier microphone preamplifier and audio ADC temperature sensor interface and clock generator . Voltage regulator receives power from processing unit via band interface and provides that power to the other components of head mounted display device . Eye tracking illumination driver provides the IR light source for eye tracking illumination A as described above. Audio DAC and amplifier output audio information to the earphones . Microphone preamplifier and audio ADC provides an interface for microphone . Temperature sensor interface is an interface for temperature sensor . Power management circuit also provides power and receives data back from three axis magnetometer A three axis gyro B and three axis accelerometer C.

Power management circuit includes clock generator analog to digital converter battery charger voltage regulator head mounted display power source and temperature sensor interface in communication with temperature sensor possibly located on the wrist band of processing unit . Analog to digital converter is used to monitor the battery voltage the temperature sensor and control the battery charging function. Voltage regulator is in communication with battery for supplying power to the system. Battery charger is used to charge battery via voltage regulator upon receiving power from charging jack . HMD power source provides power to the head mounted display device .

As noted above see through head mounted display device may be utilized with a central service . In one aspect the central service is illustrated in . Central service may be operated on one or more processing devices or servers. The logical components of the central service are illustrated in .

Central service may include user information a data service a third party service aggregator a mapping engine data storage services and communications interface which communicates with the see through mounted display device . User information can include login information requiring any wearer of a device to authenticate their identity before accessing secure information available to users of the see through head mounted display device . Each user may have available via the service user s address book and social contact data as well as the user s location history . In alternative embodiments all or portions of the user information may be stored locally on the device . Data service may include point of interest information which can include a rating review service of interest location information and point of interest virtual objects . Third party service aggregator retrieves information from third party social media services each of which may provide information that can be used to populate near field virtual objects. Third party service aggregator interacts with the application programming interface API of each of the third party interaction services .

Mapping engine associates geographic coordinate location information with data from the data service and from third party service aggregator . Mapping engine also receives location information from user devices and updates the user location history . The mapping engine may also provide 3D scene data to the see through head mounted display device . This information can be updated as the user location tracking information is provided from the device to the central service . The mapping engine can further associate the type of marker to be generated with the third party information services provider.

Third party social interaction services may include with their information the geographic coordinates for points of interest for which they provide information. Many social media services include along with their reviews such geographic coordinate location information. The mapping engine may verify this information or add geographic coordinate information to user generated data. This location can be derived from the user s location history as well as input at the time the user is creates information on a particular point of interest.

Point of interest information includes point of interest information provided by users of see through head mounted displays as well as information provided by other users of the central service who contribute to the data service . POI objects can include default object types rendered depending on the source and or type of information available for the near field object as well as specialized objects provided by operators of the point of interest for use in conjunction with the see through head mounted display device. Each point of interest may have an object associated with it based on the type of point of interest the service from which the information originated or may use a default near field virtual object.

Central service may be operated by one or more system administrators provide a central service two wearers of the see through head mounted display device two.

As noted herein the appearance of any of the markers may in some embodiments be dependent upon the distance of the wearer to the point of interest with which the marker is associated. is a top down perspective of the user relative to the view shown in and illustrates a user viewing the respective markers and associated with different points of interest. As illustrated in the user is at a different physical distances to each of the respective points of interest represented by . Each points of interest has an associated global coordinates also illustrated in . Distances shown in can be used to calculate the size of the marker as it appears in the user s view such as that shown in . Hence the relative sizes of the markers are proportional to their distance from the wearer. Alternatively the system administrator of service can allow markers to be displayed based on incentives provided from the operator of the point of interest or other design factors such as the type of point of interest for which the marker is displayed.

In an example where a user is looking at a virtual object in his FOV if the user moves his head left to move the FOV left the display of the virtual object may be shifted to the right by an amount of the user s FOV shift so that the net effect is that the virtual object remains stationary within the FOV.

In step the see through head mounted display device may gather data from the environment. This may be image data sensed by the depth camera and RGB camera of scene facing camera . This may be image data sensed the eye tracking assemblies and this may be acceleration position data sensed by the IMU and IMU .

As explained below the system develops a 3 D scene map of the mixed reality environment. For some environments this may be accomplished with the depth camera of the scene facing camera . For large scale environments in step the processing unit may receive a predefined 3 D depth map of real world objects in the mixed reality environment from central service . As described above as a user moves around the processing unit may report the user s location to a central service . If available the central service then sends a predefined 3 D depth map of the surrounding area to the processing unit .

While shown as taking place after step the receipt of the predefined 3 D map in step may take place before during or after step or the other steps described below. Moreover receipt of the predefined 3 D map need not take place every frame. The predefined 3 D map may be downloaded or updated periodically for example when processing unit is idle. Further the processing unit may download a number of different predefined 3 D maps for different locations and store those in memory. In this instance it may happen that a predefined 3 D map may be retrieved from memory of the processing unit when a user is at a new location and there is no need to have a wireless connection to the central service at that time.

In addition the information received at step can include a marker information and point of interest information can be used by the see through head mounted display as discussed herein.

In step the system determines the position the orientation and the FOV of the head mounted display device for the user . Further details of step are provided in U.S. patent application Ser. No. 13 525 700 entitled Virtual Object Generation Within a Virtual Environment which application is incorporated by reference herein in its entirety

In step the 3 D map transformed to the user s point of view using the user s current position orientation and FOV. With the knowledge of the scene and the user s position a scene map may be developed in step identifying the geometry of the environment as well as the geometry and positions of objects within the environment. In embodiments the scene map generated in a given frame may include the local or global coordinate positions of real world objects and virtual objects in the scene. In embodiments the scene map of the user s mixed reality environment may be developed from a depth map generated by data received from the scene facing camera from the predefined 3 D map received from the central service or a combination of both.

In step the system may use the scene map of the user position and FOV to determine the position and appearance of virtual objects at the current instance. For example where a mixed reality environment includes one or more markers the system can update their projection as they would appear from the user s current position and FOV.

In step the processing unit renders those virtual objects which appear within the FOV of the head mounted display device . In step the GPU of processing unit may next render an virtual object to be displayed to the user. Portions of the rendering operations may have already been performed in a rendering setup step.

At step a determination is made as to whether not a user has moved within the environment. If user has not moved the system continues to check for user movement. If user has moved then at step the position orientation and FOV of the head mounted display is again determined in a manner similar to step . At step the position of the user as well as all objects rendered in the user s field of view in the see through head mounted display are updated and re rendered.

Returning to step in addition to checking for user movement relative to the environment and the markers the system also checks at step is to whether not the object interaction by the user has been made. Object interaction can include selecting a marker as discussed above with respect to . If no selection has been made the system continues to check for object interaction at . If an object interaction has been made then a near field virtual object will be displayed at . Display the near field virtual object at may be any of the previously discussed virtual object types illustrated above with respect to .

Once the near field virtual object has been displayed at the system again continually checks for user movement relative to both the world coordinate position and the near field virtual object at .

A user movement with the near field object displayed should change the user s view of both the markers handled at steps and the near field object. If the user moves relative to the near field virtual object at then a calculation of the change in the viewer s perspective of the near field object is made at step . This calculation is relative to the reference system of the near field virtual object. One embodiment of this method is discussed below with respect to depending on whether or not the near field virtual object is will user locked or world locked. In addition to checking to user movement at a determination will be made at step as to whether or not near field virtual object interaction occurs. If near field object interaction if near field object interaction does occur at step then the users in view of the object as well as any data contributed by the user in interacting with the object will be updated at .

Near field object interaction can include closing the object using virtual controls to review information on the object or adding deleting information from the object and hence the information source . As such interaction occurs the changes to the information are updated in the display of the near field object.

At the method checks to determine whether or not the user is attempting to close the near field virtual object. If so the object will be closed using for example an animation which renders the object shrinking relative to the marker in an inverse manner to that in which the object appeared.

Any one or more of the methods herein may be implemented by computer readable code stored on a computer storage medium which may be suitably read by any one or more of the processing unit and or head mounted display device .

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

