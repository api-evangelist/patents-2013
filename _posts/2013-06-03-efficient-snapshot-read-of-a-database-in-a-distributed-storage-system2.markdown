---

title: Efficient snapshot read of a database in a distributed storage system
abstract: A computer system issues a batch read operation to a tablet in a first replication group in a distributed database and obtains a most recent version of data items in the tablet that have a timestamp no great than a snapshot timestamp T. For each data item in the one tablet, the computer system determines whether the data item has a move-in timestamp less than or equal to the snapshot timestamp T, which is less than a move-out timestamp, and whether the data item has a creation timestamp less than the snapshot timestamp T, which is less than or equal to a deletion timestamp. If the determination is true, the computer system determines whether the move-out timestamp has an actual associated value and, if so, the computer system determines a second tablet in a second replication group in the database that includes the data item and issues the snapshot read operation to the second table in the second replication group to obtain a most-recent version of the data item that has a timestamp no greater than the snapshot timestamp T; otherwise, the computer system issues the snapshot read to the one tablet to obtain a most recent version of the data item that has a timestamp no greater than the snapshot timestamp T.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09659038&OS=09659038&RS=09659038
owner: GOOGLE INC.
number: 09659038
owner_city: Mountain View
owner_country: US
publication_date: 20130603
---
This application claims priority to U.S. Provisional Application No. 61 655 458 filed Jun. 4 2012 entitled Efficient Snapshot Read of a Database In A Distributed Storage System which is incorporated by reference herein in its entirety.

This application is related to U.S. application Ser. No. 13 898 411 filed May 20 2013 entitled Organizing Data in a Distributed Storage System which application is incorporated by reference herein in its entirety.

The disclosed implementations relate generally to the management of a distributed storage system and in particular to system and method for organizing a large volume of data in a distributed storage system.

The Internet has become a popular venue for people across the globe for storing and exchanging information. As the amount of data managed by the Internet rapidly increases both individually and collectively and the service associated with the data gets more complicated it is becoming a constant challenge for an Internet based service provider to manage such a large volume of data and render the associated service efficiently in response to different data access requests by users from anywhere in the world.

In accordance with some embodiments a method for performing a snapshot read operation in a distributed database is performed at a computer system having a plurality of processors and memory storing programs for execution by the processors. The computer system receives a data access request from another computer system. In response to the data access request the computer system issues a batch read operation to one tablet in a first replication group in the database to obtain a most recent version of data items in the one tablet that have a timestamp no great than a snapshot timestamp T. For each of a plurality of data items in the one tablet the computer system obtains a state of the data item identifies a tablet to issue the snapshot read operation to the identified tablet to obtain a most recent version of the data item by comparing the state of the data item with the snapshot timestamp T and provides the most recent version of the data item to the requesting computer system.

In accordance with some embodiments a computer system comprises a plurality of processors and memory storing programs for execution by the processors the programs including instructions for receiving a data access request from another computer system in response to the data access request issuing a batch read operation to one tablet in a first replication group in the database to obtain a most recent version of data items in the one tablet that have a timestamp no great than a snapshot timestamp T for each of a plurality of data items in the one tablet obtaining a state of the data item identifying a tablet to issue the snapshot read operation to the identified tablet to obtain a most recent version of the data item by comparing the state of the data item with the snapshot timestamp T and providing the most recent version of the data item to the requesting computer system.

In accordance with some embodiments a non transitory computer readable medium stores one or more programs for execution by one or more processors of a computer system the one or more programs including instructions receiving a data access request from another computer system in response to the data access request issuing a batch read operation to one tablet in a first replication group in the database to obtain a most recent version of data items in the one tablet that have a timestamp no great than a snapshot timestamp T for each of a plurality of data items in the one tablet obtaining a state of the data item identifying a tablet to issue the snapshot read operation to the identified tablet to obtain a most recent version of the data item by comparing the state of the data item with the snapshot timestamp T and providing the most recent version of the data item to the requesting computer system.

In some embodiments the zone master monitors the performance of the tablet servers by periodically e.g. after every 10 seconds communicating with the tablet servers . A tablet server reports to the zone master its current status information including its CPU and memory usage etc. as well as other information used for determining the association between a set of tablets and the tablet server. Based on such information the zone master determines whether or not to assign a tablet in the data store to a corresponding tablet server. For example the zone master may identify some tablets associated with one tablet server which is deemed to be overloaded and assign the identified tablets to another tablet server . In addition the zone master publishes the updated tablet to tablet server map through the location proxy servers . When another entity e.g. a front end server or a tablet server wants to learn which tablet server is responsible for managing a particular tablet the entity can query one of the location proxy servers by providing a tablet ID and receiving a corresponding tablet server ID associated with the tablet ID. After identifying a tablet server for a particular tablet through the location lookup service the entity can communicate with the identified tablet server for any read write access requests directed at the tablet.

When the client wants to access data associated with a customer it submits a data access request to a front end server . In some embodiments different front end servers shown in are responsible for providing data related services to different clients. For example some of the front end servers are configured for handling data access requests from clients for access email service data and some other front end servers are configured for handling data access requests from clients for access advertising service data. In some embodiments data associated with an online service application is further partitioned into multiple portions and each front end server is configured for handling a subset of the data access requests for one or more portions of the data. In some embodiments the front end servers of the distributed storage system are located at different geographical locations to provide services to nearby clients that submit data access requests associated with different online services. As shown in a client submits a data access request by invoking an application programming interface API accepted by the front end server . The data access request includes identification information of the one or more customers. In some embodiments the data associated with a customer corresponds to a row in a data table and the row of customer data is further divided into multiple columns. The data access request may include a row identifier and possibly one or more column identifiers if the client is interested in accessing data in the one or more columns associated with each customer. Upon receipt of the data access request the front end server needs to determine where the client requested data is located in the distributed storage system such as information about the zone and tablet s that have the client requested data.

As shown in different components in the distributed storage system are organized into two domains based on their respective roles i the logical domain including the front end servers and ii the physical domain including the zones . The front end servers handle data access requests from the external clients and use data structures such as the directory map and the group map to free the clients from understanding how data is physically stored in the distributed storage system . In some embodiments each customer of the on line advertising service is associated with one directory in the directory map . Based on the logical directory identifier provided by a client the front end server identifies a particular entry in the directory map which corresponds to the particular customer.

To improve the scalability of the distributed storage system data associated with a customer is divided into multiple segments which are referred to as splits each split having a unique split ID in the directory map . As shown in each directory e.g. directory includes one or more splits and . The number of splits associated with a directory is determined by the size of data associated with a corresponding customer. The more data the customer has the more splits the data may be divided into. When the size of data within a split reaches a predefined threshold no more data is added to the split and a new split is generated for hosting new data associated with the account. In some embodiments there is no limit on the size of data for an account. In other embodiments the size of data for an account is set to a predetermined limit. The predetermined limit may be determined by the distributed storage system e.g. a global limit of the size of data that is applied to all accounts the application for which the data for the account is associated e.g. a web mail application may impose a limit of the size of data for its accounts that are different than an advertisement application and or may be increased if an end user purchases more storage space for an account. Note that the client i.e. the on line advertising service provider does not need to know which split s has the client requested data. Instead the client specifies the requested data in a request using a format defined by the client associated online service provider and the front end server translates the client request into a set of split IDs that identify the splits including the client requested data.

To improve the reliability and efficiency of the distributed storage system data associated with a customer is replicated into multiple copies and stored in different tablets of one or more zones. In other words the basic data unit for a particular customer in a front end server is a split and the basic data unit in a zone is a tablet. As shown in a group in the group map is defined to associate a split in the directory map with a plurality of tablets in a particular zone. In this example the split is a data structure associating the split ID with a group ID which corresponds to an entry in the group map . The split also includes a split size indicating the actual amount of data currently within this split and a data range indicator . As will be described below the data range indicator is used for indicating whether the split has space for more data or not. When the split runs out of space a new split e.g. split will be created for hosting new data associated with the account. In this example the split is associated with the group not the group . Note that different splits associated with an account may belong to the same group of splits or different groups of splits. Each group includes a plurality e.g. hundreds or even thousands of splits associated with different accounts and has a predefined group limit The exact association between a split and a group is dynamically determined based in part on the remaining capacity of a particular group. In some embodiments the front end server tries to add different splits associated with the same account to the same group because these splits are likely to be accessed by a client at the same time and it is probably more convenient for them to be within the same group and therefore the same set of tablets which are replicas of the group. If the group e.g. group runs out of space the front end server may identify another group e.g. group for the split . In some embodiments the data replication policy is defined for each account the group is chosen for the split because it has the same number of tablets as the group . In other words splits associated with different accounts that have different data replication policies should be added to different groups with different numbers of tablets.

In accordance with a data replication policy provided by the client a predefined number of instances of the group are generated in the distributed storage system each instance of the group being referred to as a tablet. As shown in the group has a group ID which is the same as the group ID in the split a group size and a list of tablet IDs e.g. tablet IDs of the group. Splits associated with different directories e.g. directory and directory both belong to the group suggesting that the two accounts corresponding to the two directories have the same data replication policy. In response to a client request the front end server first identifies one or more split IDs in the directory map which are associated with a customer identified by the client request and then identifies a group and an associated list of tablet IDs for each split ID. depict an embodiment in which different splits as identified by the split IDs and associated with one customer are assigned to different groups the group and the group . This situation happens when the size of a group reaches a predefined group limit such that it is less efficient to keep all the data associated with one customer e.g. multiple splits in one group and therefore one tablet.

After the front end server identifies the tablet IDs in a group of splits that includes the client requested data the process of accessing the client requested data is shifted from the logical domain to the physical domain i.e. a zone that includes the tablets associated with the identified tablet IDs. In some embodiments a tablet ID or includes a respective zone ID embedded therein. Therefore after identifying the tablet IDs the front end server also knows which zone has the client requested data. As noted above each zone includes one or more location proxy servers that provide the location look up service for identifying a particular tablet server for each tablet. Based on the zone ID included in a tablet ID the front end server submits a query to a respective location proxy server at a particular zone identified by the zone ID the query including one or more tablet IDs . The location proxy server then returns one or more tablet server IDs each tablet server ID identifying a respective tablet server e.g. the tablet server or the tablet server that has been chosen by the zone master for managing the data access requests to the particular tablet. Upon receipt of the one or more tablet server IDs the front end sever submits a request to a corresponding tablet server the request including identification of one or more splits e.g. splits and within the tablet e.g. the tablet . In response to the request each tablet server identifies a corresponding tablet in the data store and performs the operations to the identified tablet accordingly.

Referring back to a tablet includes splits corresponding to group . Similarly a tablet includes splits associated with group . By including as many splits as possible within a group the total number of groups and hence tablets can be reduced.

As shown in a tablet server further includes tablet metadata associated with tablets managed by the tablet server. In this example the tablet metadata includes a directory to group map and group state information . The directory to group map locates a particular group within a tablet for a particular directory in the directory map. The group state information includes the state information for a particular group replica such as the log data view information the list of group replicas etc. Given a directory associated with a tablet the tablet server can scan the directory to group map for an entry that has the same directory name. Once an entry is identified the tablet server can access the corresponding group state information using a group ID within this entry. In some embodiments the tablet server supports the removal of a range of directories from a tablet by eliminating data associated with each directory within the range from the tablet when removing a directory replica from the tablet.

In some embodiments one group in the distributed storage system may be split into two or more groups for several reasons. For example a tablet containing the group is overloaded or too large. In this case splitting the group may help reduce the load of the tablet if e.g. this group contributes to a significant fraction of the load or size of the tablet. Sometimes access patterns for some directories within a group are very different from access patterns for the rest of the group. For example a group may be split into two if most directories in the group are accessed from US but a particular set of directories in the group are mostly accessed from Europe.

As shown in the directory set is associated with the group which is a member of the tablet . The directory set includes a directory set and a directory set . It is assumed that the directory set will be moved to another group. To do so a new group group is created on a tablet . After the creation the group can be changed via the normal replica addition or removal. In this example the group split is implemented as a single site transaction on the group and the transaction update is applied at every replica of the group . As shown in at the completion of the transaction the directory set is associated with the group and the directory set is associated with the group . The metadata associated with the group is populated with the portion of the metadata associated with the group corresponding to the directory set and the metadata associated with the group is updated to indicate that the directory set has been moved to the group .

By allowing each account to have its own data replication policy the distribute storage system offers both flexibility and scalability to different types of online service applications that use the distributed storage system for storing their data. For example an account that needs frequent access to its data from one or more geographical locations may specify such need in its data replication policy so that the distributed storage system may create more replicas for the data associated with the account at a zone close to the data accessing locations and reduce the latency required for a client to access such data.

As shown in the distributed storage system creates in a predefined number e.g. three of replicas for each group of splits e.g. the group that includes a member of the first set of splits and allocates the three replicas in a zone e.g. the zone in accordance with the data replication policy associated with the first account. In this example the zone includes three tablets each being one replica of the group and each tablet includes a copy of the first set of splits associated with the first account . As described above each tablet in a zone is assigned to a tablet server for managing data access requests directed to the tablet. In this case the three tablets are managed by two tablet servers and . In other words tablets associated with the same group of splits may be managed by the same tablet server or different tablet servers depending on the load balance of the respective tablet servers in a zone. Similarly the distributed storage system creates in a predefined number e.g. two of replicas for each group of splits e.g. the group that includes a member of the second set of splits and allocates the replicas in a zone e.g. the zone in accordance with the data replication policy associated with the second account. In this example the zone includes two tablets each being a replica of the group and each tablet includes a copy of the second set of splits associated with the second account and is managed by a respective tablet server or . It should be noted that the data replication policy of a group of splits is driven by the data replication policies of the different splits in the group which are driven by the data replication policies of the accounts associated with the different splits. The distributed storage system is responsible for putting those splits having the same or similar data replication policies into the same group to improve the system s efficiency. In some embodiments the enforcement of the account level or directory level data replication policy is determined by the distributed storage system based on the availability of resources at different zones. In other words it is possible that the distributed storage system may not always store splits associated with a particular account strictly in accordance with the account s data replication policy. For example the splits may be initially stored in a zone different from a zone defined by the account s data replication policy and then moved to the zone. In this case the distributed storage system allows an account to specify its desired placement of the splits associated with the account in the directory map and will try to satisfy such requirement whenever it is possible. In some embodiments an account may change its data replication policy from time to time. For example an email account user may temporarily move from North America to Europe. When the email application detects such movement it may notify the distributed storage system to move tablets associated with the email account from a data center in North America to a data center in Europe to provide a better service to the end user.

In some embodiments each account has only one split whose data range parameter has a value of . As shown in the distributed storage system determines whether the split associated with the account reaches its limit according to a predefined schedule e.g. periodically . If not no the distributed storage system stops checking this account and proceeds to check another account. Otherwise yes the distributed storage system will take further actions by creating a new split for the account.

As shown in when the existing split runs out of space the distributed storage system updates the data range parameter in the split from to ABC. Note that the expression ABC is an expression that corresponds to the actual upper limit of the data within the split . From this parameter the distributed storage system can tell what data is within each split. In response to a client request for a particular piece of data the distributed storage system can use the data range parameter to determine which split or splits have the client requested data. By doing so the distributed storage system also marks the first split as not accepting any new data. The distributed storage system then creates a second split e.g. the split in for the account. As shown in the second split includes a data range parameter that has a value of indicating that more data can be added to the second split and a split size parameter that grows as more and more data is added to the second split . Moreover the distributed storage system selects a second group of splits e.g. the group in for the second split and adds the second split to a second group of splits. It should be noted that the second group may be the same group that includes the split or a different one. Finally the distributed storage system replicates the second group of splits in a particular zone e.g. the zone in in accordance with a data replication policy associated with the account. In this example the second group has two tablets in the zone which are managed by two respective tablet servers and . In some embodiments the creation of a new split for an account may be triggered by the movement of data within the distributed storage system e.g. in response to a change of the data replication policy. In either case the creation of new splits for the account ensures that the client can add more data to this account without disrupting the service associated with the account.

In response the front end server in identifies one or more split identifiers associated with the logical identifier in accordance with the data access request. As shown in the front end server identified two splits within the directory map they are the split and the split . Each split s metadata includes a split ID that identifies a split associated with the account. For each of the identified one or more split identifiers the front end server identifies a group identifier e.g. the group IDs and in each group identifier corresponds to a group of splits. In the example shown in the group ID corresponds to the group of splits that includes a split corresponding to the split in the directory map and the group ID corresponds to the group of splits that includes a split corresponding to the split in the directory map . The distributed storage system selects one of the multiple tablets associated with each identified group based on information about the tablet servers managing the multiple tablets. For example the zone includes two tablets associated with each of the two identified groups and . In particular the tablet is identified for the group and the tablet includes a split that corresponds to the split in the directory map . The tablet is identified for the group and the tablet includes a split that corresponds to the split in the directory map . For each selected tablet there is a corresponding tablet server in the zone that is responsible for managing data access requests to the tablet. In this example the tablet server is responsible for managing the tablet and the tablet server is responsible for managing the tablet . After identifying each tablet server the front end server communicates with the tablet server that manages the selected tablet for the split corresponding to the split identifier for the split associated with the client request and receives the split from the tablet server. After receiving the splits from different tablet servers the front end server forwards the splits from the respective tablet servers to the requesting client in satisfying the client s data access request. It should be noted that the client access request may be a read only request or a read and write request. Since each group of splits includes multiple tablets any data update to one split within the group should be replicated within each tablet associated with the group.

In some embodiments the programs or modules identified above correspond to sets of instructions for performing a function described above. The sets of instructions can be executed by one or more processors e.g. the CPUs . The above identified modules or programs i.e. sets of instructions need not be implemented as separate software programs procedures or modules and thus various subsets of these programs or modules may be combined or otherwise re arranged in various embodiments. In some embodiments memory stores a subset of the modules and data structures identified above. Furthermore memory may store additional modules and data structures not described above.

Although shows a tablet server is intended more as functional description of the various features which may be present in a set of tablet servers than as a structural schematic of the embodiments described herein. In practice and as recognized by those of ordinary skill in the art items shown separately could be combined and some items could be separated. For example some items shown separately in could be implemented on single servers and single items could be implemented by one or more servers. The actual number of servers used to implement a tablet and how features are allocated among them will vary from one embodiment to another and may depend in part on the amount of data traffic that the system must handle during peak usage periods as well as during average usage periods.

In some embodiments the distributed storage system provides a mechanism to perform a snapshot read operation of at least a portion of a database in the distributed storage system . A snapshot read operation is an operation that obtains a snapshot of the state of the database or portion thereof at a snapshot time T. In some embodiments a data item in the database includes a sequence of versions of the data item where each version of the data item is associated with a timestamp. Thus when a snapshot read operation is performed on the data item to obtain a version of the data item at the snapshot time T the most recent version of the data item that has a timestamp no greater than the snapshot timestamp T is read or otherwise obtained . Note that the term data item is used herein to refer to a row of a database a portion of a row in the database a directory e.g. as described above with reference to and or a split e.g. as described above with reference to .

In some embodiments the snapshot read operation is performed on the database as a whole e.g. a whole database scan operation for a snapshot time T. In these embodiments the snapshot read operation obtains a most recent version of each item in the database that has a timestamp no greater than the snapshot timestamp T. One complication to the snapshot read operation is the fact that a data item may move between tablets e.g. for load balancing for availability etc. as described above. When a data item is moved from a first tablet to a second tablet all versions of the data item are copied from the first tablet to the second tablet. Once the move operation is complete future versions of the data item are stored only to the second tablet. The data item including all versions of the data item are deleted asynchronously on the first tablet. Thus when a snapshot read operation is performed on each tablet items that have moved to another tablet since the snapshot timestamp T are detected and the snapshot read operation for these items are directed to the tablets including the items.

In some embodiments a state of a data item on a tablet is stored in a data structure that includes one or more of a creation timestamp corresponding to a time when the data item was created on the tablet a deletion timestamp corresponding to a time when the data item was deleted from the tablet a move in timestamp corresponding to a time when the data item was moved into the tablet and a move out timestamp corresponding to a time when the data item was moved out of the tablet. A value of infinity may be used to indicate that particular parameter does not have a timestamp associated with it. For example a value of infinity for the deletion timestamp means that the data item has not been deleted from the tablet. Similarly a value of infinity for the move out timestamp means that the data item has not been moved out of the tablet.

When the move out timestamp for the data item is less than infinity yes the snapshot read module determines a second tablet in a second replication group that includes the data item and issues a snapshot read operation to the second tablet in the second replication group to obtain a most recent version of the data item that has a timestamp no greater than T. In other words when the data item has been moved out of the one tablet the snapshot read module cannot obtain the data item from the one tablet. Accordingly the snapshot read module has to issue the snapshot read operation to the tablet that now includes the data item. Note that after the data item was moved out of the one tablet the data item may subsequently be split between two or more tablets. For example if the data item is a directory the data in the splits may be partitioned and stored across multiple tablets. Thus in some embodiments operations and are modified as follows. The snapshot read module determines two or more tablets each of which is in a separate and distinct replication group that include the data item and issues snapshot read operations to the two or more tablets to obtain a most recent version of the data item that has a timestamp no greater than T.

When the move out timestamp for the data item is infinity or greater than or equal to some value that indicates that the data item has not been moved out of the tablet no the snapshot read module performs a snapshot read at the one tablet to obtain a most recent version of the data item that has a timestamp no greater than T. In other words since the data item has not been moved out of the one tablet the snapshot read module obtains the data item from the one tablet.

After obtaining the version of the data item that has a timestamp no greater than T the snapshot read module provides the version of the data item to a requesting computer system e.g. the client the front end server etc. .

When the snapshot timestamp T does not satisfy the conditions in operation no the snapshot read module does nothing. In this case if the data item exists in the distributed storage system at the snapshot timestamp T another tablet would have the data item and that tablet would read the data item.

The snapshot read operation may be performed on a single item e.g. row of the database on multiple items on all or a portion of the items in a tablet or multiple tablets and or on all or a portion of the items in the database.

In some embodiments the snapshot read operation reads a portion of the database. In some embodiments the snapshot read operation includes parameters that specify the subset of the database to be read. For example the snapshot read operation may include a regular expression used to filter data items or a range of identifiers for data items that are to be read. In some embodiments a data item is assigned to a tablet based on a type of the data item. For example tablets T T T may only store data items that have a first type and tablets T T T may only store data items that have a second type. Thus a snapshot read operation may be performed based on the type of the data item. For example operation in is modified as follows to perform a snapshot read operation for data items having a first type. For each tablet of a database the distributed storage system determines whether the tablet stores data items of the first type. When the tablet stores the data items of the first type the distributed storage system issues a batch read operation to the tablet to obtain a most recent version of data items in the tablet that have a timestamp no greater than a snapshot timestamp T. Otherwise the distributed storage system ignores e.g. does not issue the batch read operation to the tablet. Note that the type of data items stored in a tablet may be stored in the tablet s metadata.

As discussed above data items may be moved from a first tablet to a second tablet. When a data item is moved from the first tablet to the second tablet the versions of the data item in the first tablet may not be deleted immediately e.g. the versions of the data item may be deleted in a background garbage collection or compaction process . To avoid the expensive operations of issuing snapshot read operations to the second tablet e.g. operations and in in some embodiments the snapshot read module performs snapshot read operation at the first tablet if the move out timestamp for the data item is greater than or equal to a compaction timestamp corresponding to the most recent data item that was deleted from the tablet. Otherwise the snapshot read module issues a snapshot read operation to the second tablet to obtain a most recent version of the data item that has a timestamp no greater than T. In other words if the move out timestamp of the data item is greater than or equal to the timestamp for the most recent data item that was deleted that means the data item has not been deleted from the first tablet yet and therefore the snapshot read operation of the data item may be performed on the first tablet. In doing so the expensive operations of issuing snapshot read operations to the second tablet can be avoided even though the data item was moved to the second tablet. If the move out timestamp of the data item is less than the timestamp for the most recent data item that was deleted that means the data item may have been deleted depending on the compaction schedule and therefore the snapshot read operation should be performed on the second tablet.

The methods illustrated in may be governed by instructions that are stored in a computer readable storage medium and that are executed by at least one processor of at least one server. Each of the operations shown in may correspond to instructions stored in a non transitory computer memory or computer readable storage medium. In various embodiments the non transitory computer readable storage medium includes a magnetic or optical disk storage device solid state storage devices such as Flash memory or other non volatile memory device or devices. The computer readable instructions stored on the non transitory computer readable storage medium may be in source code assembly language code object code or other instruction format that is interpreted and or executable by one or more processors.

Plural instances may be provided for components operations or structures described herein as a single instance. Finally boundaries between various components operations and data stores are somewhat arbitrary and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the embodiment s . In general structures and functionality presented as separate components in the example configurations may be implemented as a combined structure or component. Similarly structures and functionality presented as a single component may be implemented as separate components. These and other variations modifications additions and improvements fall within the scope of the embodiment s .

It will also be understood that although the terms first second etc. may be used herein to describe various elements these elements should not be limited by these terms. These terms are only used to distinguish one element from another. For example a first contact could be termed a second contact and similarly a second contact could be termed a first contact which changing the meaning of the description so long as all occurrences of the first contact are renamed consistently and all occurrences of the second contact are renamed consistently. The first contact and the second contact are both contacts but they are not the same contact.

The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the claims. As used in the description of the embodiments and the appended claims the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will also be understood that the term and or as used herein refers to and encompasses any and all possible combinations of one or more of the associated listed items. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

As used herein the term if may be construed to mean when or upon or in response to determining or in accordance with a determination or in response to detecting that a stated condition precedent is true depending on the context. Similarly the phrase if it is determined that a stated condition precedent is true or if a stated condition precedent is true or when a stated condition precedent is true may be construed to mean upon determining or in response to determining or in accordance with a determination or upon detecting or in response to detecting that the stated condition precedent is true depending on the context.

The foregoing description included example systems methods techniques instruction sequences and computing machine program products that embody illustrative embodiments. For purposes of explanation numerous specific details were set forth in order to provide an understanding of various embodiments of the inventive subject matter. It will be evident however to those skilled in the art that embodiments of the inventive subject matter may be practiced without these specific details. In general well known instruction instances protocols structures and techniques have not been shown in detail.

The foregoing description for purpose of explanation has been described with reference to specific embodiments. However the illustrative discussions above are not intended to be exhaustive or to limit the embodiments to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles and their practical applications to thereby enable others skilled in the art to best utilize the embodiments and various embodiments with various modifications as are suited to the particular use contemplated.

