---

title: Managing shared state information produced by applications
abstract: A shared renderer maintains shared state information to which two or more augmented reality application contribute. The shared renderer then provides a single output presentation based on the shared state information. Among other aspects, the shared renderer includes a permission mechanism by which applications can share information regarding object properties. The shared renderer may also include: a physics engine for simulating movement of at least one object that is represented by the shared state information; an annotation engine for managing a presentation of annotations produced by plural applications; and/or an occlusion engine for managing the behavior of the output presentation when two or more objects, produced by two or more applications, overlap within the output presentation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09424239&OS=09424239&RS=09424239
owner: Microsoft Technology Licensing, LLC
number: 09424239
owner_city: Redmond
owner_country: US
publication_date: 20130906
---
An augmented reality application typically provides an output presentation which combines information captured from the environment with some type of supplemental information. For example one type of augmented reality application presents an image of the environment together with labels that annotate objects within the image. Another type of augmented reality application provides a figure which duplicates the actual movement of a human user together with a virtual object with which the user may interact.

An application that captures information from the surrounding environment raises privacy concerns. For example the above described augmented reality applications capture images or videos of the environment that information in turn can potentially include sensitive items such as human faces personal writing account numbers etc. The owner of this private information will often prefer or insist that the information is not released to unauthorized parties. Such an undesirable release can occur in various circumstances. In a first case an unauthorized application may extract the private information from information that it obtains through a computing device s sensing mechanisms e.g. a video camera microphone etc. . In a second case an unauthorized application may obtain the private information from another augmented reality application.

In practice a developer may create an augmented reality application as a one off self contained unit of code. Similarly a computing device may execute the application as a standalone unit of functionality. If the user wishes to run another augmented reality application on the same computing device he or she may close down the first application and start up the second application. Pursuant to this approach each developer may address the above described privacy concerns in a separate typically ad hoc manner within the application code itself

A shared renderer is described herein which manages shared state information. The shared state information is produced by two or more augmented reality applications each of which contributes objects and other information to the shared state information. For example two or more augmented reality applications may sense the same environment and create two different sets of labels associated with entities within the environment. The shared renderer can represent those labels as parts of the shared state information. An output renderer presents a single output presentation based on this shared state information.

According to one illustrative aspect each object may have zero one or more properties. Further the shared renderer can associate permission information with each object property. The shared renderer includes a mechanism by which any application can share information regarding an object property with another application based on the permission information.

According to another illustrative feature the shared renderer may include a physics engine. The physics engine is configured to simulate movement of at least one object that is represented in the shared state information based on instructions from at least one application.

According to another illustrative feature the shared renderer may include an annotation engine. The annotation engine is configured to manage a presentation of annotations produced by plural applications.

According to another illustrative feature the shared renderer may include an occlusion engine. The occlusion engine is configured to manage the behavior of an output presentation when two or more objects produced by two or more applications overlap within the output presentation.

The above functionality can be manifested in various types of systems components methods computer readable storage media data structures graphical user interface presentations articles of manufacture and so on.

This Summary is provided to introduce a selection of concepts in a simplified form these concepts are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

The same numbers are used throughout the disclosure and figures to reference like components and features. Series 100 numbers refer to features originally found in series 200 numbers refer to features originally found in series 300 numbers refer to features originally found in and so on.

This disclosure is organized as follows. Section A provides an overview of a reality sensing framework in which plural applications sense information from an environment and contribute to a shared output presentation. The applications may correspond to augmented reality applications and or some other type of environmental sensing applications. Section B sets forth illustrative methods which explain the operation of the functionality of Section A. Section C describes illustrative computing functionality that can be used to implement any aspect of the features described in Sections A and B.

As a preliminary matter some of the figures describe concepts in the context of one or more structural components variously referred to as functionality modules features elements etc. The various components shown in the figures can be implemented in any manner by any physical and tangible mechanisms for instance by software running on computer equipment hardware e.g. chip implemented logic functionality etc. and or any combination thereof. In one case the illustrated separation of various components in the figures into distinct units may reflect the use of corresponding distinct physical and tangible components in an actual implementation. Alternatively or in addition any single component illustrated in the figures may be implemented by plural actual physical components. Alternatively or in addition the depiction of any two or more separate components in the figures may reflect different functions performed by a single actual physical component. to be described in turn provides additional details regarding one illustrative physical implementation of the functions shown in the figures.

Other figures describe the concepts in flowchart form. In this form certain operations are described as constituting distinct blocks performed in a certain order. Such implementations are illustrative and non limiting. Certain blocks described herein can be grouped together and performed in a single operation certain blocks can be broken apart into plural component blocks and certain blocks can be performed in an order that differs from that which is illustrated herein including a parallel manner of performing the blocks . The blocks shown in the flowcharts can be implemented in any manner by any physical and tangible mechanisms for instance by software running on computer equipment hardware e.g. chip implemented logic functionality etc. and or any combination thereof.

As to terminology the phrase configured to encompasses any way that any kind of physical and tangible functionality can be constructed to perform an identified operation. The functionality can be configured to perform an operation using for instance software running on computer equipment hardware e.g. chip implemented logic functionality etc. and or any combination thereof.

The term logic encompasses any physical and tangible functionality for performing a task. For instance each operation illustrated in the flowcharts corresponds to a logic component for performing that operation. An operation can be performed using for instance software running on computer equipment hardware e.g. chip implemented logic functionality etc. and or any combination thereof. When implemented by computing equipment a logic component represents an electrical component that is a physical part of the computing system however implemented.

The phrase means for in the claims if used is intended to invoke the provisions of 35 U.S.C. 112 sixth paragraph. No other language other than this specific phrase is intended to invoke the provisions of that portion of the statute.

The following explanation may identify one or more features as optional. This type of statement is not to be interpreted as an exhaustive indication of features that may be considered optional that is other features can be considered as optional although not expressly identified in the text. Finally the terms exemplary or illustrative refer to one implementation among potentially many implementations.

This section describes a reality sensing framework that hosts one or more augmented reality applications. As noted above an augmented reality application operates by using one or more sensing mechanisms to capture any aspects of an environment. The application then generates some kind of supplemental information such as a label virtual object e.g. an avatar etc. The application then provides an output presentation which combines information regarding the environment with the supplemental information.

In other cases the reality sensing framework may host one or more other types of environment sensing applications. These other types of environment sensing applications capture aspects of environment but do not combine a representation of the environment and supplemental information in the same manner described above. Nevertheless to facilitate and simplify the description the reality sensing framework will be principally described herein with reference to augmented reality applications.

From a high level perspective the reality sensing framework includes a recognition system for extracting different objects from the environment based on perceptual information provided by one or more sensing mechanisms. Different augmented reality applications subscribe to and receive different types of objects generated by the recognition system. A single shared renderer receives and manages information generated by the augmented reality applications. The reality sensing framework provides the output presentation based on shared state information maintained by the shared renderer.

But before delving into the illustrative specifics of the reality sensing framework consider the specific scenario depicted in in which the reality sensing framework is used to provide an augmented reality experience. That scenario will serve as a running example throughout this disclosure.

The real world shown in includes a first person who is standing in front of a whiteboard . Among other characteristics the first person possesses a face and a badge . Further the first person extends his arm out as if to point in a particular direction. The whiteboard includes various instances of writing such as writing and writing .

A second person captures the above described scene using a computing device such as a smartphone. More specifically the second person orients the computing device such that its video camera not shown captures a video representation of at least the first person and the whiteboard . The computing device can also include one or more other environment sensing mechanism such as but not limited to one or more microphones one or more motion sensing devices such as an accelerometer gyroscope etc. and so on. These other sensing mechanisms may capture other aspects of the real world .

The computing device may also include one or more output devices such as a display mechanism . The display mechanism provides an output presentation produced by the augmented reality applications. In the scenario shown in the display mechanism acts as a window to the real world from the vantage point of the second person . In other words the content presented on the display mechanism mirrors the actual world in front of the computing device as if the user was looking through a window onto the world.

However other setups may be used to deliver an augmented reality experience. In another case for instance the second person may interact with the augmented reality applications within a space created by one or more sensing mechanisms. For example the second person may operate within a field of view created by plural video cameras each of which captures the second person from a different vantage point. The second person may consume the output presentation produced by the augmented reality applications on any output device such as a display monitor that is placed in front of the second person .

In still another case the second person may interact with the augmented reality applications via any type of wearable computing device. For example such a computing device may be affixed to eyewear apparel a watch jewelry or other wearable item. Any type of sensing mechanisms and output devices may be affixed or otherwise associated with the wearable item. For example a video camera affixed to eyewear can capture a video representation of the scene in front of the second person while a display mechanism affixed to the eyewear may deliver the output presentation provided by the augmented reality applications. The above described form factors are cited by way of example not limitation still other arrangements are possible.

Assume in the present example that the reality sensing framework includes five augmented reality applications working together to generate shared state information. The display mechanism may present a single output presentation based on the shared state information as shown in the bottom portion of . From a high level perspective each of the five augmented reality applications provides a different instance of supplemental information which contributes to the shared state information. The shared renderer combines the separate instances of the supplemental information together with a visual and or audio representation of the real world .

Assume that a first augmented reality application receives skeleton information from the recognition system. The skeleton information expresses a pose adopted by the first person . The first augmented reality application then generates labels based on the skeleton information and provides those labels to the output presentation . In the example of the first augmented reality application provides a label that indicates that the first person constitutes A person who is standing. 

A second augmented reality application also adds labels to the output presentation but operates in a different manner than the first augmented reality application. That is the second augmented reality application receives face recognition data from the recognition system. The second augmented reality application then performs a lookup operation to match the face recognition data with one or more previously registered instances of face recognition data each of which is tagged with a name. The second augmented reality application then provides labels associated with one or more matching names on the output presentation . shows one such label generated by the second augmented reality application.

The first and second augmented reality applications described above are henceforth referred to as the first and second annotation applications respectively. In general the reality sensing framework can work in conjunction with any number of annotation applications.

A third augmented reality application receives position data from the recognition system which indicates the positions of the first person s hands. The third augmented reality application then generates a virtual object such as the virtual ball shown in . The third augmented reality application then displays the virtual ball in proximity to the position of the first person s outstretched hand. For example the third augmented reality application can perform this function for the purpose of highlighting the position of a lecturer s outstretched hand. For ease of reference this type of application is henceforth referred to as a virtual highlighter application. More generally the above described type of augmented reality application can provide any type of virtual object or objects to the output presentation . In some cases a physics engine to be described below provided by the shared renderer animates a virtual object with realistic and or fanciful dynamics.

A fourth augmented reality application receives text information from the recognition system. The recognition system in turn may regenerate the text information by processing an RGB representation of the whiteboard using any type of text recognition technology such as optical character recognition technology. In the specific example of the recognition system receives an image of the writing and converts it into the text Release Date is Jun. 16 2014. The fourth augmented application then forms a search string based on the tokens in the text. It then uses the search string to search within a corpus of Email messages and identifies any Email messages which match the search string. Finally the fourth augmented reality application presents a prompt such as prompt which identifies the matching Email messages. The prompt may also include a hyperlink or other control feature which allows the viewer to access the matching Email messages. The above described augmented reality application is referred to below as an Email retrieval application.

A fifth augmented reality application receives the output of the second annotation application which in the example of identifies the name of the first person John Cheng . The fifth augmented reality application can then present a prompt which invites the viewer to access a social network page associated with the identified person i.e. John Cheng . The fifth augmented reality application can display the prompt in close proximity to the label . This augmented reality application is referred to below as a social network application.

As can be appreciated the assortment of augmented reality applications described above is cited by way of illustration not limitation. The reality sensing framework can accommodate any number of augmented reality applications including just one application each of which may perform any function and each of which may be active or inactive at any time.

As a clarifying comment note that the output presentation also includes a redacted depiction of the first person and the whiteboard . The reality sensing framework can present such a depiction in different ways. In one case the reality sensing framework can use a sixth augmented reality application to receive objects from the recognition system which represent the first person and the whiteboard . For example the recognition system can generate these objects by using a depth image provided by a depth camera to create a three dimensional model of the first person and a three dimensional model of the whiteboard . The recognition system can then paste RGB information provided by a video camera onto the three dimensional models as texture information to thereby produce realistic looking objects associated with the first person and the whiteboard . As shown the recognition system can also optionally redact certain parts from these objects such as the face of the first person the writing on the whiteboard the writing on the badge and so on. The sixth augmented reality application can then send the redacted objects to the output presentation .

In another implementation the computing device which implements the reality sensing application can receive raw perceptual information from one or more sensing mechanisms such as a video camera a microphone etc. The computing device can then pass this raw perceptual information to the display mechanism of the computing device without also providing it to any of the augmented reality applications. Alternatively or in addition the computing device can display a processed representation of the real world provided by the recognition system again without providing it to any of the augmented reality applications.

At this juncture consider a few high level aspects of the problem posed by the scenario shown in . The real world includes several items which may be considered private or sensitive in nature. For example the whiteboard contains writing that may contain secret information. For example consider the case in which the first person is an employee of a company and that person has written the launch date of a commercial product as conveyed by writing on a company whiteboard . The company may wish to prevent that information from being released outside the company.

Similarly the real world encompasses the first person himself That person may consider his face speech badge etc. as private information. As such the first person may wish to prevent these personal items from being communicated to non authorized entities. In the scenario of the second person is capturing a representation of the first person this arrangement means that the first person will be the one who is primarily concerned about the release of personal information. But more generally anyone who is encompassed by or indirectly impacted by a sensed environment may be concerned about breaches of privacy.

More precisely there are various points of concern in the above described scenario. First an owner of private information may wish to restrict the type of information that each augmented reality application receives. Second the owner of private information may wish to control the manner in which an augmented reality application which has rightfully been given access to private information is permitted to share the private information with other entities. For example the first person may wish to restrict the second annotation application from sharing the detected name John Cheng with another application. Third the owner of the private information may wish to restrict the manner in which any augmented reality application outputs sensitive information such as the manner in which any augmented reality displays the face of the first person etc. on the display mechanism of the computing device .

A second technical challenge ensues from the fact that plural augmented reality applications are providing supplemental information to the same output presentation . There is a risk that the output of one augmented application may interfere with the output of other augmented reality applications.

Advancing to this figure shows a reality sensing framework which provides a solution to at least the above described problems. The reality sensing framework includes a collection of one or more augmented reality applications and or any other environment sensing applications. The reality sensing framework may receive the applications from any source s in any manner. In one case a user can explicitly download or otherwise obtain an augmented reality application from any source such as an online marketplace of such applications. In another case a user may more passively select an application such as by visiting an online website which invokes the application or by triggering any other event which automatically invokes the application. An application corresponds to a body of computer implemented instructions expressed in any computing language or combination of computing languages.

The applications correspond to potentially untrusted functionality . The remainder of the reality sensing framework corresponds to trusted functionality . The untrusted functionality is untrusted in the sense that it does not offer the same type of robust security guarantees as the trusted functionality . In one case the trusted functionality may be implemented at least in part by functionality provided by a computing device s operating system. In another case the trusted functionality can be provided at least in part by a software layer which resides between the operating system and the applications . Still other implementations of the trusted functionality are possible.

The trusted functionality includes two main flows of information. In the first main flow the trusted functionality extracts selected objects from a sensed environment and sends them to the applications . In the second main flow the applications pass supplemental information to an output presentation. Subsection A. provides a detailed explanation of the first main flow while Subsection A. provides additional details regarding the second main flow. The remainder of this subsection provides an overview of the overall reality sensing framework . Note however that the functionality associated with the first main flow as described in Subsection A. can be used in conjunction with other types of functionality besides the functionality associated with the second main flow as described in Subsection A. and vice versa.

As to the first information flow one or more environment sensing mechanisms receive perceptual information from the environment . The environment includes one or more features which characterize the environment in its present real time state such as entities and events within the real world of . In addition or alternatively the environment includes one or more archived features providing historical perceptual information extracted from the environment at some previous time or times.

The environment sensing mechanisms can include any equipment for sensing any aspect of the environment . The environment sensing mechanisms can include but are not limited to video cameras still image cameras movement sensing devices microphones vibration sensors strain gauges haptic input sensors depth sensing mechanisms and so on. A depth sensing mechanism can use any technique to capture depth information such as a structured light technique a time of flight technique a stereoscopic technique and so forth. One commercial system for capturing and processing depth images is the Kinect system provided by Microsoft Corporation of Redmond Wash.

A recognition system extracts various higher level items of information from the raw perceptual information. These items of information are referred to as objects herein and can take any form. For example consider the case in which one sensing mechanism provides raw video information. The recognition system can determine whether the video information depicts a human subject. If so the recognition system can generate skeleton information from the video information which describes the real time pose of the human subject. The recognition system can then pass the skeleton information to one or more applications which request this information in lieu of the raw video information. More specifically the recognition system includes one or more recognizers e.g. Rec Rec . . . Rec n . Each recognizer generates a different abstract representation of the perceptual information obtained from the environment .

An error correction system reduces the incidence of false positives in the output of the recognition system. More specifically the error correction system may include one more error correction mechanisms EC EC . . . EC n each of which works in conjunction with a particular recognizer or recognizers . For example the error correction mechanism EC works in conjunction with the recognizer Rec by reducing the incidence of false positives generated by the recognizer Rec . Each error correction mechanism can interact with its counterpart recognizer in any manner such as by modifying the input information received by the recognizer modifying the output information generated by the recognizer and or altering the internal processing performed by the recognizer.

For example consider the writing shown in . It is apparent to a human observer that this writing represents some type of symbolic information such as a mathematical or chemical formula. However this writing also somewhat resembles the features of a human face. A recognizer that generates face recognition data can be said to generate a false positive when it incorrectly interprets the writing as a face. This is problematic from a security standpoint because the writing may contain private information that is not intended to be sent to whatever application is authorized to receive face data. The error correction mechanism associated with this recognizer will reduce the likelihood that this false recognition will occur. Examples of how the error correction system can perform its service will be set forth in greater detail at a later juncture of this description.

A perceptual management module henceforth referred to as just a management module manages the interaction between the applications and the recognition system . For example the management module allows applications to subscribe to certain objects provided by the recognition system . The management module then forwards those objects to the subscribing applications when new instances of those objects are provided by the recognition system .

Now referring to the second main flow identified above a shared renderer maintains shared state information. The shared state information combines objects that are generated by the various applications . The shared renderer then provides an output presentation to one or more output devices such as a display mechanism speaker etc.

The shared renderer also implements various services. For example the shared renderer provides a mechanism by which one application can share aspects of its output objects with one or more other applications. Further the shared renderer may implement any of a physics engine an annotation engine and or an occlusion engine all of which are described in Subsection A..

Overall the reality sensing framework provides a solution to the above noted challenges described with reference to . First the reality sensing framework can allow each application to access only that information that it requires to perform its functions and nothing more. For example the first annotation application can perform its functions based on skeleton information. It does not need raw video or depth information. As such the reality sensing framework can prevent the first annotation application from receiving raw perceptual information this constraint in turn prevents the first annotation application from receiving sensitive information which is expressed by the raw video information such as the writing on the whiteboard and the first person s face etc. Second the shared renderer provides an efficient global mechanism for managing the manner in which each application can share object information with other applications if permitted at all.

Overall the reality sensing framework provides a technique for handling perceptual information that can be considered least privileged because an application only gets the information that it needs and fine grained because an application can pick out and receive particular kinds of information items rather than receiving more encompassing information on an all or nothing basis .

The shared renderer also provides a global mechanism for managing the presentation of output information from two or more augmented reality applications. The shared renderer can thereby reduce interference between applications which write to the same output space.

Moreover the reality sensing framework provides the above solutions in a trusted and application agnostic platform rather than relying on the individual augmented reality applications to implement separate respective ad hoc solutions. This characteristic may promote acceptance of augmented reality technology among users software developers device manufacturers and members of the public who may feel impacted by the use of augmented reality applications by others .

In another implementation the local computing device can implement some aspects of the reality sensing framework while a remote computing framework may implement other aspects of the reality sensing framework . In one implementation the remote computing framework may be implemented as one or more remote servers. The local computing device may interact with the remote computing framework via any communication conduit such as a local area network a wide area network e.g. the Internet or a point to point link and so forth or combination thereof.

In one illustrative allocation of functions the local computing device can implement one or more local recognizers while the remote computing framework can implement one or more remote recognizers. Hence the recognition system in this system is distributed over at least two different locations. More specifically the remote computing framework can implement the most computationally intensive recognizers in the recognition system such as those recognizers that perform complex image processing tasks such as in one case a face recognition task .

In one implementation the delegation of recognition tasks between the local computing device and the remote computing framework is static or at least slowly varying. In another case a routing module can dynamically delegate tasks between the local computing device and the remote computing framework based on at least one computational workload factor and or other consideration s . For example the routing module can automatically delegate a recognition task to the remote computing framework when the amount of uncompleted work in a workload queue exceeds a prescribed threshold. In addition or alternatively the routing module can automatically migrate a recognition task from the local computing device to the remote computing framework if the task is taking more than a prescribed amount of time to complete on the local computing device and or based on other considerations. The routing module may be implemented by the management module and or other component of the reality sensing framework .

A remote recognizer may provide service to any number of local computing devices at the same time. For example the remote recognizer may provide service to both the local computing device and a local computing device . In this way the remote recognizer simulates the operation of a virtual machine by providing service to two or more independent tasks on the same physical platform but without necessarily providing actual virtual machine functionality.

The management module includes plural components which perform different respective functions. A recognizer registration module registers the names of the respective recognizers that are provided by the recognition system . The applications make reference to the recognizers based on those registered names.

A recognizer configuration module manages the configuration of each recognizer and the configuration of the collection of recognizers as a whole. For example in one case the recognizer configuration module can send a creation command to the recognition system which instructs the recognition system to create an instance of a particular kind of recognizer. The recognizer configuration module sends a destroy command to instruct the recognition system to discard a previous instance of a particular recognizer and to release any sensing mechanism resources associated with that recognizer. Upon creation each recognizer may be inserted within a data flow graph having zero one or more child nodes and zero one or more parent and ancestor nodes.

Each recognizer expresses the object or objects that it generates as an event. For example an event that is generated by a recognizer that performs face recognition may contain a data structure that includes data that describes a face but without including the full RGB representation of the face .

A permission management module receives a request by an application to access certain objects provided by respective recognizers. For example in one implementation the application can make this request at the time that a user seeks to install the application in the reality sensing framework . In another case the application can make this request each time the application seeks to access a prescribed object. The application can make such a request at yet other junctures.

In response to such a request a privacy visualization module presents a visual representation of the objects that an application seeks to access from the recognition system . The user can then provide feedback information which either grants or denies each identified access right. If the user accepts an access right the permission management module stores permission information in a data store which indicates that the identified application has permission to access a particular object or collection of objects.

An event handling module sends particular events to those applications that are entitled to receive the events as determined based on the permission information stored in the data store . For example in one implementation the event handling module can perform a query loop in which it calls each recognizer in sequence to determine if it has any new events being sought by the applications. If a new event is discovered the event handling module sends it to the applications which are entitled to receive it. The event itself as mentioned above expresses one or more objects. Instead of actively polling the recognizers in the manner described above the recognizers can independently push their events to the event handling module which then selectively forwards the events to the applications which are entitled to receive them. Still other strategies can be used to harvest events from the recognition system .

To perform the above functions the applications can register callbacks in the event handling module . A callback informs the event handling module what type of event that a particular application is entitled to receive and where to send that event.

The event handling module can also invoke particular functions when calling a recognizer e.g. by invoking a particular application programming interface API provided by the recognizer. For example by calling a comparison function the event handling module can instruct the recognizer to compare two instance of input information such as two successive instances of perceptual information generated by a sensing mechanism. If the instances are the same the recognizer can be configured to refrain from performing its recognition task on the later instance of perceptual information since nothing has changed since the last instance of perceptual information was received . In another case the event handling module can instruct a recognizer to examine a particular portion of an instance of input information such as a particular region within an image.

A learning module modifies the behavior of any other component in the management module and or recognition system based on feedback information regarding the performance of these components. For example the learning module can detect that a particular recognizer is producing a high rate of false positives or is infrequently used. Based on this information the learning module can decide to replace this recognizer with another recognizer that performs the same function. In another example the learning module can examine the volume or distribution of events received from the recognition system . Based on this information the learning module may decide to change the manner in which the event handling module collects events from the recognition system .

The recognition system includes a collection of recognizers organized into a data flow graph. A video recognizer may be viewed as a driver of the video cameras . It receives raw video information from the video cameras and issues an output video event which expresses that video information. A depth data recognizer may be considered a driver of the depth capture mechanisms . It receives a depth image from the depth data capture mechanisms and issues an output depth event which expresses that depth image. A depth image identifies the distances between points in a scene and a reference point. 

A face detector recognizer receives a video event from the video recognizer . The face detector recognizer analyzes the video information conveyed by that event to determine whether it contains a representation of the face of at least one human subject. If so the face detector recognizer issues an output event which expresses face data but without expressing the full RGB representation of the subject s face.

A text recognizer receives a video event recognizes text in the video information contained in the video event e.g. using optical character recognition technology or the like and generates an output event which expresses the text data.

A skeleton recognizer receives a depth event from the depth data recognizer and a video event from the video recognizer . Based on this input information the skeleton recognizer generates a skeletonized representation of a human subject if such a subject is present within the scene. In performing this task the skeleton recognizer can compare information extracted from the scene with a database of known possible body poses. The skeleton recognizer expresses its findings as a skeleton event. A hand recognizer receives the skeleton event as input information. Based on information expressed in that event the hand recognizer determines the positions and orientations of the user s hands. The hand recognizer generates a hand event which expresses its findings.

In keeping with the principle of least privileged and fine grained access different applications consume different objects provided by different respective recognizers. The first annotation application receives the skeleton event provided by the skeleton recognizer . The second annotation application receives a face event generated by the face detector recognizer . The virtual highlighter application receives a hand event from the hand recognizer . The Email retrieval application receives a text event generated by the text recognizer . And the whiteboard beautification application receives a raw video event generated by the video recognizer . The social network application does not receive any data directly from the recognition system rather the social network application receives name information that is shared by the second annotation application . Although not shown any application may alternatively receive events from two or more recognizers.

Hence it can be seen that only the whiteboard beautification application receives raw information from a sensing mechanism. This provision enables the reality sensing framework to limit the exposure of private information to applications which actually need this information. For example the first annotation application cannot see the video information associated with the writing on the whiteboard or the first person s face it just receives data which indicates the positions and orientations of the first person s hands.

Although not shown in a hypothetical seventh augmented reality application was described above which constructs realistic looking three dimensional versions of objects in the real world . That application can receive information from a model construction recognizer not shown . That recognizer in turn may receive video events generated by the video recognizer and depth events generated by the depth data recognizer . Based on the depth information the model construction recognizer can construct three dimensional models of objects in the real world . The model constructor recognizer can then paste the video information as a texture onto the models. The model constructor recognizer can optionally also receive input from the face detector recognizer and the text recognizer . Based on these inputs the model constructor recognizer can replace face information and text information with blanked out or otherwise obscured content thereby effectively redacting the sensitive information in the video information.

The recognizer configuration module of dynamically configures the recognition system based on the current demands for objects imposed by the active applications. Hence when an application becomes active or an application becomes inactive the recognizer configuration module can dynamically modify the data flow graph e.g. by adding nodes to it or pruning nodes from it. To do so the recognizer configuration module consults stored information which identifies the input sources and output sinks associated with each recognizer.

In contrast shows a user interface presentation that invites the user to inspect the access rights associated with any application that has already been installed e.g. by clicking the View Permissions command associated with this application or by clicking the View all Permissions command to view the permissions granted by all installed applications or all currently running applications . These actions will again cause the visualization module to display the user interface presentation shown in .

The visualization module can invoke the user interface presentation shown in in yet other circumstances based on other triggering events. For instance the visualization module can also display the user interface presentation shown during the running of the application e.g. in a utility type panel in the margin of the application s display presentation or the like.

Advancing to a first part of the user interface presentation may provide a visual representation of the objects to which each application requests access or already has access . For example assume that the applications in question request access to skeleton events provided by the skeleton recognizer and face events provided by the face detector recognizer . The user interface presentation may devote a first display region for providing a visualization of the skeleton information expressed by the skeleton events and a second display region for showing a visualization of the face information expressed by the face events. Alternatively the first part can provide a single visualization that shows access rights of two or more applications rather than showing separate regions for respective applications.

The user interface presentation can optionally include a second part which displays a visualization of the raw data extracted by the sensing mechanisms. For example the user interface presentation can include a third region that shows the raw video information provided by the video cameras etc.

The first part of the user interface presentation may also include a command associated with each object request. This command invites the user to selectively permit the identified application to access the identified object. For example the region includes a Permit command which enables the first annotation application to receive skeleton events and a Permit command which enables the second annotation application to receive face events. Alternatively or in addition a global permit command may allow a user to authorize all applications to receive their designated requested objects.

The visualization module can present the visualizations shown in in any manner. In one case the visualization module relies on archived stock images and or video snippets to show the access privileges associated with a particular application. These stock examples may have no relation to the entities within the real world that are being observed at the current time. In another case the visualization module can display the actual output information generated by the recognizers with respect to whatever scene is being captured by the reality sensing framework at the present time. But in this later implementation the management module can prevent the events from actually being delivered to the application s if the user has not yet permitted this delivery.

In one implementation the visualization module can handle all aspects of the visualization tasks described above. In another implementation the visualization module can make calls to visualization functions provided by respective recognizers. The recognizers can respond to the calls by returning display information. The visualization module then presents the forwarded display information in the user interface presentation . Alternatively the recognizers can directly provide information to the user interface presentation without the mediation of the visualization module . Still other implementations are possible.

The permission management module stores permission information based on the selections made by the user via the user interface presentation . The event handling module then references that permission information in deciding what events that it is permitted to send to the applications running at a particular time.

More generally shows that the recognizer receives the output result generated by the error correction mechanism as one of its inputs. But in other cases the error correction mechanism can act on the output events generated by the recognizer or the error correction mechanism can play some role in the internal processing performed by the recognizer or any combination thereof. In general it may be said that the error correction mechanism and the recognizer work in conjunction in any manner to reduce the incidence of false positives generated by the recognizer .

Consider two particular and non limiting instantiations of the functionality shown in . In a first implementation the error correction mechanism receives video or image input information from one or more source recognizers such as the video recognizer of . The error correction mechanism then blurs the input information using any type of blurring filter such as a box filter to generated blurred output information. The recognizer receives the blurred output information as an input and operates on this blurred information to generate its output events. By burring the input information to the recognizer the recognizer is less likely to incorrectly identify an entity within the input information as a specified object such as a face e.g. because the blurring may reduce the similarity between the entity being analyzed and the object of interest being sought such as a face .

In a second implementation the error correction mechanism compares a current frame of image information with one or more previous frames of image information. Or the error correction mechanism can compare a region in the current frame of image information with a counterpart region in one or more previous frames of image information. For example the error correction mechanism can form the difference between counterpart pixels in the current frame and the immediately previous frame sum these differences and then compare the resultant sum with a threshold to determine whether the two frames are substantially the same or different. In another implementation the error correction mechanism can form a measure which reflects the difference between the current frame and two or more previous frames such as by forming a pairwise comparison between the current frame and each previous frame to generate an individual difference measure and then generating an average of the individual difference measures. The recognizer then processes the current frame to determine whether it contains a specified object such as face data etc. However the recognizer can reject a conclusion that the current frame contains the specified object if 1 no object was detected in the previous frame s and 2 the current frame and the previous frame s are substantially the same based on the output result of the error correction mechanism . More generally the above described operations may be distributed between the error correction mechanism and the recognizer in any manner.

The error correction mechanism shown in may alternatively correspond to a component within the face detection recognizer itself. In that setup the error correction mechanism rejects an indication that video information contains a face if that video information originates before or behind a designated plane in a depth image. Broadly stated the error correction mechanism and or the recognizer can operate by using the output information generated by a first source recognizer e.g. the plane detection recognizer to designate a region within the output information generated by a second source recognizer e.g. the video recognizer or otherwise qualify the analysis of the output information provided by the second source recognizer.

In another application of the functionality of the error correction mechanism can receive information from a face detector which indicates the presence or absence of a face in the scene. The error correction mechanism can then blank out all portions of video information which do not correspond to faces. An eye recognizer receives the redacted output information generated by the error correction mechanism and then analyzes this information to determine the positions and orientations of the eyes in the scene. By virtue of the fact that the input information that is fed to this recognizer contains only face related video information the recognizer can reduce the likelihood that it will falsely interpret an entity in the scene as an eye when in fact it is not an eye.

In general any error correction mechanism can be implemented as a component that is independent of and agnostic with respect to the operation of the recognizer to which it is coupled. And as explained above the error correction mechanism can be coupled to its recognizer in various ways such as by conditioning the input that is fed to the recognizer modifying the output that is provided by the recognizer and or affecting the internal processing that is performed within the recognizer. By virtue of its independence from its recognizer an error correction mechanism can be coupled to any recognizer of a particular class without regard to the particular manner in which the recognizer is designed. In another implementation a custom recognizer specific error correction mechanism can be designed for use with respect to one or more particular recognizers.

Before delving into the illustrative specifics of the shared renderer consider a specific illustrative scenario depicted in in which one application may share information with another application via the shared renderer . The first application corresponds to the virtual highlighter application which determines the position of a person s hands and then displays a virtual object such as a virtual ball on an output presentation in proximity to one of the person s hands. The second application corresponds to the first annotation application which displays labels based on skeleton events provided by the skeleton recognizer .

In addition to annotating parts of the sensed scene the first annotation application can receive information from the virtual highlighter application which describes various properties of the virtual ball such as the position color shape etc. of the virtual ball . The first annotation application can then present a label on the output presentation which describes the virtual ball as in the illustrative description A Yellow Ball. To allow this information sharing to occur the virtual highlighter application has in a preliminary operation granted the first annotation application the authority to read information regarding the virtual ball .

For example in the scenario of the user has instructed the measurement application to measure the horizontal span of the beautified writing . In response the measurement application can provide a visual ruler indicator to indicate the horizontal span of beautified writing that is being measured together with an output label which indicates the resultant measurement e.g. 3 feet in this case . To perform the above functions in a preliminary operation the beautification application grants the measurement application the right to read geometry data associated with objects generated and displayed by the beautification application . In another case the measurement application can determine the distance between two objects produced by two respective applications when those two applications separately grant the measurement application rights to access their geometry data.

In addition a first application can give a second application the authority to write information regarding an identified object property to the first application. For example the label shown in generated by the first annotation application has a position. The first annotation application can authorize the virtual highlighter application the authority to write position information to the first annotation application that pertains to the position of the virtual ball which the first annotation application then uses to define the position of the label .

The application interaction module may also receive permission information from the applications . An instance of the permission information establishes a set of zero one or more permissions regarding a particular object property. An application referred to as the permission setting or controlling application may issue such an instance of the permission information if it owns the object to which the property pertains or otherwise has authority with respect to that object property for the access mode read or write under consideration. The permission information identifies the other applications which are entitled to receive or provide information regarding the particular object property. An application which supplies information regarding a particular object property is referred to as a sharer application.

For example with respect to one object property that is controlled by the virtual highlighter application corresponds to the position of the virtual ball . The virtual highlighter application can set permission information which indicates that the first annotation application is permitted to read the position of the virtual ball . In another case the first annotation application can set permission information which authorizes the virtual highlighter application to write position data corresponding to the position of the virtual ball to the first annotation application which the first annotation application subsequently uses to define the position of the label . In both cases the virtual highlighter application is the sharer application.

The application interaction module stores all information regarding objects object properties and permission information in a data store . That information collectively constitutes shared state information. That is the shared state information reflects the shared output space to which all of the applications contribute. The shared output space may represent these objects in any number of dimensions such as two dimensions three dimensions and so on. For example the reality sensing framework can create objects having three dimensions by virtue of its receipt and processing of depth information provided by one or more depth capture mechanisms.

The application interaction module may manage inter application sharing in the following illustrative manner. First the application interaction module detects if an update has occurred that affects any object property in the shared state information. For example an application may provide an update by creating a new object property a new value for an object property a new instance of permission information and so on. Or an internal engine of the shared renderer may provide an update e.g. as when a physics engine moves an object. The application interaction module can then determine whether the change is permitted. If so the application interaction module can commit the change to the shared state information. The application interaction module can then consult the permission information associated with the object property under consideration to determine what applications are entitled to receive information regarding the update. The application interaction module will then send information regarding the update to those identified applications.

An output renderer provides an output presentation to one or more output devices based on the shared state information in the data store . For example the output renderer may generate the type of visual output presentations shown in respectively.

One or more engines may also manipulate objects in the shared state information based on instructions from the applications and or other considerations. For example a physics engine can dynamically manipulate any object in the shared state information to simulate real life motion dynamics and or other real life phenomena. Alternatively or in addition the physics engine can manipulate any object pursuant to fanciful dynamics which do not necessarily correlate with real world interactions. Any physics engine can be used to perform this task such as but not limited to the Unity game engine produced by Unity Technologies of San Francisco Calif.

More specifically the physics engine can receive high level instructions from an application which define the type of movement that is being imparted to a particular object that is controlled by the application. The physics engine can then simulate the movement using a database of simulation primitives. Note however that the physics engine operates to simulate the movement of objects within an output space produced by plural applications not necessarily a single application. Hence the physics engine is tasked with the responsibility of managing the manner in which the objects of one application may interfere with the objects of another such as by managing the collision of an object produced by one application with an object produced by another application.

An annotation engine manages the presentation of annotations produced by plural applications such as the first annotation application and the second annotation application described above. The annotation engine can use various principles and algorithms to perform this task. For example the annotation engine can display each label within a prescribed distance of the object that it modifies within the output presentation. Further the annotation engine can reduce clutter by limiting each application to displaying only a prescribed number of annotations. The annotation engine can apply yet other algorithms such as various clutter reduction strategies that take into account the nature of the scene depicted in the output presentation. One such clutter reduction strategy can attempt to maximize the spatial separation between labels subject to other constraints.

An occlusion engine provides various services to address the situation in which one object overlaps another in the output presentation such as two labels two virtual objects a label and a virtual object etc. A first service can operate to prevent or minimize the occurrence of occlusions e.g. by moving one or more objects so that they no longer overlap. A second service can provide a visual indicator which alerts the user that two or more objects overlap. A third service may allow a user to manually change the positions and or orientations of one or more objects e.g. to reveal portions of an object that were previously occluded by another object. Still other occlusion related services are possible.

As a point of clarification in some cases the above described engines are able to modify the positions or other properties of objects that compose the shared state information. But the shared state information also includes a subset of objects which directly correlate to entities and events which occur in the real world . For example one such object may correspond to face redacted version of a real human being or writing redacted version of a real whiteboard. In one implementation the various engines may not be permitted to modify the positions or other properties of objects which directly map to real world entities. In other implementations this restriction may be relaxed or removed to various extents insofar as the engines are capable of modifying an object which represents a real life entity which may not be the case in all implementations .

In one implementation the applications specify the permission information on a per object property basis e.g. based on programmatic definitions in the code which implement the applications . The applications then send information to the application interaction module which identifies the permissions information. In another case an access control module implements one or more techniques by which an end user may manually specify permission information on a per object property basis and or any other basis. In one case the user may achieve this result by manipulating gadgets that appear within a user interface presentation. The ensuing description will provide examples of this mode of operation.

A learning module can receive feedback information regarding any aspect of the performance of the shared renderer . Based on this information the learning module can modify the behavior of any component of the shared renderer . For example any entity can provide feedback information regarding whether any component of the shared renderer is performing a satisfactory service. Such feedback providing entities can include any of an end user an application a shared renderer engine etc. Based on this input the learning module can modify the operation of one or more components of the shared renderer with the objective of improving the performance of these components. For example the user may indicate that the annotation engine is producing an output presentation that provides too many labels produced by a certain application. In response the learning module can modify the annotation engine to decrease the number of annotations that the identified application is allotted within the output presentation.

In operation a user can select any gadget associated with any display item and drag it over to another display item e.g. using a mouse device a touch gesture or a free space gesture etc. This action has the effect of conferring an access right to the recipient application. For example in the case of the user has dragged the geometry data gadget of the whiteboard beautification app item over to the measurement app item . In particular the user has selected a read icon associated with the geometry data gadget . This action has the effect of creating permission information which enables the measurement application to read geometry data for objects created by the whiteboard beautification application . Had the user performed the same movement with respect to the write icon the permission information would give the measurement application the right to write geometry data to the whiteboard beautification application .

The above described user interface functionality is cited by way of example not limitation. Other user interface techniques can be used to achieve the same results described above or to achieve other permission granting effects. For example the user interface presentation shown in can also include a bar of gadgets . A user can drag any of these gadgets to a particular application s display item. Doing so gives the corresponding application certain rights with respect to all other applications rather than a particular application as in the example above . For example the user can drag the geometry data gadget onto the measurement app item which gives the measurement application the right to read the geometry data of any object created by any other application not just the whiteboard beautification application .

In another case a user interface presentation can also present items associated with particular objects created by respective applications together with a set of gadgets associated with each object. Each gadget corresponds to a particular property associated with the object under consideration. A user can then perform the above described drag and drop operation to transfer access rights on a per object basis. For example a user can drag a gadget associated with a particular object to an application item indicating that the recipient application now has access rights with respect to a particular object and a particular property. Or a user can drag a gadget associated with a first object associated with a first application to second object associated with a second application . This action conveys object to object access rights. For example with reference to the scenario of a user can drag a position gadget associated with the virtual ball to a label object associated with the label to indicate that the second application can only use the position of the ball to define the position of the label .

Any application may create and register one or more gadgets for storage in the access control module . For example the whiteboard beautification application can register the geometry data gadget in the access control module .

In another case the visual semantics of the drag and drop operation shown in can be reversed in the following manner. In the case of for example the measurement app item can have a gadget associated with geometry data and designating a read mode of operation for this property. The user can drag this gadget to the whiteboard beautification app item . This gives the measurement application that same rights as before that is the ability to read geometry data for any object produced by the whiteboard beautification application. Or the user can drag the geometry data gadget to a specific object produced by the whiteboard beautification application such as a whiteboard object. This gives the measurement application the right to selectively read geometry data pertaining to the whiteboard object but not other objects produced by the whiteboard beautification application. In a variation the measurement application can register the geometry data gadget whereupon it appears in the bar of gadgets with a tag which indicates that it pertains to the measurement application. The user can perform drag and drop operations using this geometry gadget in the same manner described above to produce the same effect described above.

In summary in the above alternative semantics the application seeking a permission is the source of the drag and drop operation rather than the target of the drag and drop operation. That is the alternative semantics applies a requestor metaphor in which an application specifies the privileges it requests as opposed to a grantor metaphor in which a privilege conferring application grants a privilege to another application.

Starting with this figure shows a procedure that represents one manner of operation of the management module of . In block the management module detects an event that triggers an examination of an application s access rights. This event may correspond to a request to install or otherwise run the application or a request to inspect the access rights of an application that has already been installed. In block the management module displays a visual representation of the access rights that will be or have been conferred to the application. shows one such visual representation of the access rights. In block the management module receives feedback information from a user which indicates whether the application is permitted to access the identified objects. This feedback can be provided on a per object basis or globally for all objects associated with an application. In block the management module stores permission information that reflects the user s choices in block . The operations in blocks have been described with respect to a single application under investigation but the same operations can be performed to define permission information for two or more applications that operate at the same time. In that case the management module can provide separate visualizations for the respective applications or a single integrated visualization.

In block the management module receives an event pertaining to a particular object from the recognition system . In block the management module consults the permission information created in blocks to determine the application or applications that are permitted to receive the event. In block the management module sends the event to the application or applications that are entitled to receive the event as determined in block .

More specifically as per option A in block the shared renderer can receive the permission information from an application which is authorized to set the permission information. For example the application can programmatically specify the permission information in its code and then communicate that permission information to the shared renderer .

As per option B in block the shared renderer can display a user interface presentation having a gadget associated with the object property. In block the shared renderer can receive an indication that the user has interacted with the gadget such as by dragging it to a region associated with a particular application or object. In block the shared renderer can create permission information based on the interaction identified in block . Alternatively the shared renderer can receive the permission information as a result of the user s interaction with the user interface presentation shown in or some other user control panel.

The computing functionality can include one or more processing devices such as one or more central processing units CPUs and or one or more graphical processing units GPUs and so on.

The computing functionality can also include any storage resources for storing any kind of information such as code settings data etc. Without limitation for instance the storage resources may include any of RAM of any type s ROM of any type s flash devices hard disks optical disks and so on. More generally any storage resource can use any technology for storing information. Further any storage resource may provide volatile or non volatile retention of information. Further any storage resource may represent a fixed or removal component of the computing functionality . The computing functionality may perform any of the functions described above when the processing devices carry out instructions stored in any storage resource or combination of storage resources.

As to terminology any of the storage resources or any combination of the storage resources may be regarded as a computer readable medium. In many cases a computer readable medium represents some form of physical and tangible entity. The term computer readable medium also encompasses propagated signals e.g. transmitted or received via physical conduit and or air or other wireless medium etc. However the specific terms computer readable storage medium and computer readable medium device expressly exclude propagated signals per se while including all other forms of computer readable media.

The computing functionality also includes one or more drive mechanisms for interacting with any storage resource such as a hard disk drive mechanism an optical disk drive mechanism and so on.

The computing functionality also includes an input output module for receiving various inputs via input devices and for providing various outputs via output devices . Illustrative input devices include a keyboard device a mouse input device a touchscreen input device a digitizing pad one or more cameras a voice recognition mechanism any movement detection mechanisms e.g. an accelerometer gyroscope etc. and so on. One particular output mechanism may include a presentation device and an associated graphical user interface GUI . Other output devices include a printer a model generating mechanism a tactile output mechanism an archival mechanism for storing output information and so on. The computing functionality can also include one or more network interfaces for exchanging data with other devices via one or more communication conduits . One or more communication buses communicatively couple the above described components together.

The communication conduit s can be implemented in any manner e.g. by a local area network a wide area network e.g. the Internet point to point connections etc. or any combination thereof. The communication conduit s can include any combination of hardwired links wireless links routers gateway functionality name servers etc. governed by any protocol or combination of protocols.

Alternatively or in addition any of the functions described in the preceding sections can be performed at least in part by one or more hardware logic components. For example without limitation the computing functionality can be implemented using one or more of Field programmable Gate Arrays FPGAs Application specific Integrated Circuits ASICs Application specific Standard Products ASSPs System on a chip systems SOCs Complex Programmable Logic Devices CPLDs etc.

In closing the description may have described various concepts in the context of illustrative challenges or problems. This manner of explanation does not constitute a representation that others have appreciated and or articulated the challenges or problems in the manner specified herein. Further the claimed subject matter is not limited to implementations that solve any or all of the noted challenges problems.

More generally although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

