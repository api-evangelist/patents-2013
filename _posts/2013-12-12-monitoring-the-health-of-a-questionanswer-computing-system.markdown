---

title: Monitoring the health of a question/answer computing system
abstract: An approach is provided for monitoring the health of a Question/Answer (QA) Computing System. In the approach, performed by an information handling system, a number of static questions are periodically submitted to the QA system, wherein each of the static questions corresponds to a previously established verified answer. Responses are received from the QA system, with the responses including answers corresponding to the submitted static questions. Monitoring the performance of the QA system based on the received responses. When the monitoring detects a problem with the QA system, a user is notified of the detected problem.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09286153&OS=09286153&RS=09286153
owner: International Business Machines Corporation
number: 09286153
owner_city: Armonk
owner_country: US
publication_date: 20131212
---
The User Interface for a Question Answering Computing System can produce responses to questions when entered. However the response to the questions can take some amount of seconds if not minutes. Over time the amount of time a question takes to answer and the accuracy of the answers can degrade. One reason for degradation could be hardware issues that adversely affect processing but are not severe enough to emerge as node failures. Another degradation mechanism could be incremental ingestion of corpus causing the machine learning state to gradually fall out of alignment eventually requiring a machine learning retraining process.

An approach is provided for monitoring the health of a Question Answer QA Computing System. In the approach performed by an information handling system a number of static questions are periodically submitted to the QA system wherein each of the static questions corresponds to a previously established verified answer. Responses are received from the QA system with the responses including answers corresponding to the submitted static questions. Monitoring the performance of the QA system based on the received responses. When the monitoring detects a problem with the QA system a user is notified of the detected problem.

The foregoing is a summary and thus contains by necessity simplifications generalizations and omissions of detail consequently those skilled in the art will appreciate that the summary is illustrative only and is not intended to be in any way limiting. Other aspects inventive features and advantages of the present invention as defined solely by the claims will become apparent in the non limiting detailed description set forth below.

As will be appreciated by one skilled in the art aspects of the present invention may be embodied as a system method or computer program product. Accordingly aspects of the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon.

Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable RF etc. or any suitable combination of the foregoing.

Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer server or cluster of servers. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects of the present invention are described below with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks.

The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

Knowledge manager may be configured to receive inputs from various sources. For example knowledge manager may receive input from the network a corpus of electronic documents or other data a content creator content users and other possible sources of input. In one embodiment some or all of the inputs to knowledge manager may be routed through the network . The various computing devices on the network may include access points for content creators and content users. Some of the computing devices may include devices for a database storing the corpus of data. The network may include local network connections and remote connections in various embodiments such that knowledge manager may operate in environments of any size including local and global e.g. the Internet. Additionally knowledge manager serves as a front end system that can make available a variety of knowledge extracted from or represented in documents network accessible sources and or structured data sources. In this manner some processes populate the knowledge manager with the knowledge manager also including input interfaces to receive knowledge requests and respond accordingly.

In one embodiment the content creator creates content in a document for use as part of a corpus of data with knowledge manager . The document may include any file text article or source of data for use in knowledge manager . Content users may access knowledge manager via a network connection or an Internet connection to the network and may input questions to knowledge manager that may be answered by the content in the corpus of data. As further described below when a process evaluates a given section of a document for semantic content the process can use a variety of conventions to query it from the knowledge manager. One convention is to send a well formed question. Semantic content is content based on the relation between signifiers such as words phrases signs and symbols and what they stand for their denotation or connotation. In other words semantic content is content that interprets an expression such as by using Natural Language NL Processing. In one embodiment the process sends well formed questions e.g. natural language questions etc. to the knowledge manager. Knowledge manager may interpret the question and provide a response to the content user containing one or more answers to the question. In some embodiments knowledge manager may provide a response to users in a ranked list of answers.

In some illustrative embodiments knowledge manager may be the IBM Watson QA system available from International Business Machines Corporation of Armonk N.Y. which is augmented with the mechanisms of the illustrative embodiments described hereafter. The IBM Watson knowledge manager system may receive an input question which it then parses to extract the major features of the question that in turn are then used to formulate queries that are applied to the corpus of data. Based on the application of the queries to the corpus of data a set of hypotheses or candidate answers to the input question are generated by looking across the corpus of data for portions of the corpus of data that have some potential for containing a valuable response to the input question.

The IBM Watson QA system then performs deep analysis on the language of the input question and the language used in each of the portions of the corpus of data found during the application of the queries using a variety of reasoning algorithms. There may be hundreds or even thousands of reasoning algorithms applied each of which performs different analysis e.g. comparisons and generates a score. For example some reasoning algorithms may look at the matching of terms and synonyms within the language of the input question and the found portions of the corpus of data. Other reasoning algorithms may look at temporal or spatial features in the language while others may evaluate the source of the portion of the corpus of data and evaluate its veracity.

The scores obtained from the various reasoning algorithms indicate the extent to which the potential response is inferred by the input question based on the specific area of focus of that reasoning algorithm. Each resulting score is then weighted against a statistical model. The statistical model captures how well the reasoning algorithm performed at establishing the inference between two similar passages for a particular domain during the training period of the IBM Watson QA system. The statistical model may then be used to summarize a level of confidence that the IBM Watson QA system has regarding the evidence that the potential response i.e. candidate answer is inferred by the question. This process may be repeated for each of the candidate answers until the IBM Watson QA system identifies candidate answers that surface as being significantly stronger than others and thus generates a final answer or ranked set of answers for the input question. More information about the IBM Watson QA system may be obtained for example from the IBM Corporation website IBM Redbooks and the like. For example information about the IBM Watson QA system can be found in Yuan et al. Watson and Healthcare IBM developerWorks 2011 and The Era of Cognitive Systems An Inside Look at IBM Watson and How it Works by Rob High IBM Redbooks 2012.

Types of information handling systems that can utilize QA system range from small handheld devices such as handheld computer mobile telephone to large mainframe systems such as mainframe computer . Examples of handheld computer include personal digital assistants PDAs personal entertainment devices such as MP3 players portable televisions and compact disc players. Other examples of information handling systems include pen or tablet computer laptop or notebook computer personal computer system and server . As shown the various information handling systems can be networked together using computer network . Types of computer network that can be used to interconnect the various information handling systems include Local Area Networks LANs Wireless Local Area Networks WLANs the Internet the Public Switched Telephone Network PSTN other wireless networks and any other network topology that can be used to interconnect the information handling systems. Many of the information handling systems include nonvolatile data stores such as hard drives and or nonvolatile memory. Some of the information handling systems shown in depicts separate nonvolatile data stores server utilizes nonvolatile data store and mainframe computer utilizes nonvolatile data store . The nonvolatile data store can be a component that is external to the various information handling systems or can be internal to one of the information handling systems. An illustrative example of an information handling system showing an exemplary processor and various components commonly accessed by the processor is shown in .

Northbridge and Southbridge connect to each other using bus . In one embodiment the bus is a Direct Media Interface DMI bus that transfers data at high speeds in each direction between Northbridge and Southbridge . In another embodiment a Peripheral Component Interconnect PCI bus connects the Northbridge and the Southbridge. Southbridge also known as the I O Controller Hub ICH is a chip that generally implements capabilities that operate at slower speeds than the capabilities provided by the Northbridge. Southbridge typically provides various busses used to connect various components. These busses include for example PCI and PCI Express busses an ISA bus a System Management Bus SMBus or SMB and or a Low Pin Count LPC bus. The LPC bus often connects low bandwidth devices such as boot ROM and legacy I O devices using a super I O chip . The legacy I O devices can include for example serial and parallel ports keyboard mouse and or a floppy disk controller. The LPC bus also connects Southbridge to Trusted Platform Module TPM . Other components often included in Southbridge include a Direct Memory Access DMA controller a Programmable Interrupt Controller PIC and a storage device controller which connects Southbridge to nonvolatile storage device such as a hard disk drive using bus .

ExpressCard is a slot that connects hot pluggable devices to the information handling system. ExpressCard supports both PCI Express and USB connectivity as it connects to Southbridge using both the Universal Serial Bus USB the PCI Express bus. Southbridge includes USB Controller that provides USB connectivity to devices that connect to the USB. These devices include webcam camera infrared IR receiver keyboard and trackpad and Bluetooth device which provides for wireless personal area networks PANs . USB Controller also provides USB connectivity to other miscellaneous USB connected devices such as a mouse removable nonvolatile storage device modems network cards ISDN connectors fax printers USB hubs and many other types of USB connected devices. While removable nonvolatile storage device is shown as a USB connected device removable nonvolatile storage device could be connected using a different interface such as a Firewire interface etcetera.

Wireless Local Area Network LAN device connects to Southbridge via the PCI or PCI Express bus . LAN device typically implements one of the IEEE 802.11 standards of over the air modulation techniques that all use the same protocol to wireless communicate between information handling system and another computer system or device. Optical storage device connects to Southbridge using Serial ATA SATA bus . Serial ATA adapters and devices communicate over a high speed serial link. The Serial ATA bus also connects Southbridge to other forms of storage devices such as hard disk drives. Audio circuitry such as a sound card connects to Southbridge via bus . Audio circuitry also provides functionality such as audio line in and optical digital audio in port optical digital output and headphone jack internal speakers and internal microphone . Ethernet controller connects to Southbridge using a bus such as the PCI or PCI Express bus. Ethernet controller connects information handling system to a computer network such as a Local Area Network LAN the Internet and other public and private computer networks.

While shows one information handling system an information handling system may take many forms some of which are shown in . For example an information handling system may take the form of a desktop server portable laptop notebook or other form factor computer or data processing system. In addition an information handling system may take other form factors such as a personal digital assistant PDA a gaming device ATM machine a portable telephone device a communication device or other devices that include a processor and memory.

Dynamic question set performance is performed on a periodic basis e.g. every week when a set of questions is compiled based on the input of users in the period just ending. A human administrator curates the set of questions to assign ground truth values. The dynamic questions and their answers are also stored in GroundTruth data store . In addition end user input such as answer rating feedback might also be harvested to identify candidate questions. The set of questions used in dynamic question set performance changes over time from period to period. Several periods sets can be used simultaneously by running them through the system and recording performance and answer quality.

Annotator health feedback utilizes health and performance status provided by QA system components. Some individual annotators in the pipeline will record the health and performance status of one or more components included in the pipeline utilized by QA system to answer questions. One such example is a primary search component that logs statistics on the query attempted the query results and time elapsed. The annotator might also have its own determination of operating health and can record that as well. In one embodiment the health and performance data regarding a pipeline component are stored in data store into which the annotator logs statistics such as the query attempted the query results the time elapsed etc. In one embodiment annotator health feedback is built around an annotator logging its health status in a data store such as data store and then that status being read later on. In another embodiment annotator health feedback might be a service e.g. using a Representational State Transfer REST etc. that is called by annotator health feedback when a problem is detected or using some other trigger. In this embodiment the health monitor is notified of a problem rather than the monitor having to periodically poll the annotator to identify problems.

In an embodiment that utilizes multiple QA systems the health of one of the QA systems could affect how questions are routed through IP splitter and load balancer use to direct incoming questions to one of the QA systems. In this embodiment load balancer would stop or limit incoming questions from being routed to a QA system that is experiencing problems etc. with those problems having been detected by the components shown in to self monitoring the health of the QA Systems.

A decision is made by the process as to whether to perform the static baseline question set performance analysis on a QA system decision . If the static baseline question set performance analysis is being performed then decision branches to the yes branch whereupon at predefined process the process performs the static baseline question set performance analysis see and corresponding text for processing details . The results from the static baseline question set performance analysis are stored in data store . On the other hand if the static baseline question set performance analysis is not being performed then decision branches to the no branch bypassing predefined process .

A decision is made by the process as to whether to perform the dynamic baseline question set performance analysis on a QA system decision . If the dynamic baseline question set performance analysis is being performed then decision branches to the yes branch whereupon at predefined process the process performs the dynamic baseline question set performance analysis see and corresponding text for processing details . The results from the dynamic baseline question set performance analysis are stored in data store . On the other hand if the dynamic baseline question set performance analysis is not being performed then decision branches to the no branch bypassing predefined process .

A decision is made by the process as to whether to perform the annotator health feedback analysis on a QA system decision . If the annotator health feedback analysis is being performed then decision branches to the yes branch whereupon at predefined process the process performs the annotator health feedback analysis see and corresponding text for processing details . The results from the annotator health feedback analysis are stored in data store . On the other hand if the annotator health feedback analysis is not being performed then decision branches to the no branch bypassing predefined process .

After one or more analysis routines have been performed at predefined process the process provides a user of the QA system such as a system administrator with health feedback regarding the QA system see and corresponding text for processing details . The output of predefined process is QA system health report that is used to notify one or more users such as a QA system administrator of any problems detected in the QA system. At step the process waits for the next time to process the QA system health monitoring profile e.g. weekly etc. or until a requestor such as a QA system administrator requests performance of one or more QA system health analysis routines. When the next request is received or time to process the health monitoring profile arrives then process will loop back to step and re perform the processes shown in as described above.

At step the process sets a timer coinciding with the submission of the selected question to the QA system at step . When the response is received from the QA system at step the timer is stopped at step . At step the process analyzes and records the performance of the QA system based on the received response metrics such as the amount of time taken by the QA system to answer the question. This performance record is stored in data store which is used to record the historical static performance data for the QA system that is being tested. At step the process analyzes and records the answer quality of the answer received by the QA system in response to the submitted question. In one embodiment the answer quality is based on whether the QA system responded with an answer that matches the verified answer that corresponds to the question retrieved from data store . In one embodiment the answer quality is further based on a confidence level that the QA system established for the answer that was returned at step .

A decision is made by the process as to whether there are more static questions to process decision . If there are more static questions to process then decision branches to the yes branch which loops back to select and process the next static question as described above. This looping continues until all of the static questions have been processed at which point decision branches to the no branch for further processing.

At step the process analyzes the general answer quality using the historical answer quality data stored in data store over a period of time and generates one or more general answer quality results such the degradation of general answer quality over time. The results of the general answer quality analysis are stored in static analysis results data store .

At step the process analyzes the general answer performance using the historical answer quality data stored in data store over the period of time and generates one or more general answer performance results such the degradation of general answer performance over time. The results of the general answer performance analysis are stored in static analysis results data store .

At step the process analyzes the answer quality of answers for each question type e.g. date questions factoid questions etc. using the historical answer quality data stored in data store over a period of time and generates one or more answer quality results for each question type such the degradation of answer quality pertaining to each particular question type over time. The results of the analysis of answer quality for each question type are stored in static analysis results data store .

At step the process analyzes the answer performance of answers for each question type using the historical answer quality data stored in data store over the period of time and generates one or more answer performance results for each question type such the degradation of answer performance pertaining to each particular question type over time. The results of the analysis of answer performance for each question type are stored in static analysis results data store . After the static answer analysis results have been generated and stored in data store the process returns to the calling routine see at .

At step the process receives a dynamic question from GroundTruth data store . Dynamic questions are identified as dynamic question set within GroundTruth data store . In one embodiment the dynamic questions and their verified answers that are included in dynamic base set are questions that have been collected over time during actual usage of the QA system with answers that have been verified as correct by at least one SME Subject Matter Expert . In addition metadata regarding the dynamic questions including performance metrics recorded when the questions were initially processed by the QA system are also included in dynamic set . Metadata regarding questions also includes a question type associated with each of the dynamic questions. A question type might classify a given question as being a category relationship question a fill in the blanks question an abbreviation question a puzzle question an etymology question a verb question a translation question a number question a common bond question a definition question a multiple choice question a date question a factoid question or some other question type.

At step the process receives an incoming new question from a user of the QA system and checks to see of a substantially similar question is already in dynamic set of questions . A decision is made by the process as to whether to simply test the dynamic question retrieved from dynamic set of questions decision . If the question is not simply being tested then decision branches to the no branch whereupon a decision is made by the process as to whether based on the dynamic collection profile the system should collect the question received from a user of the QA system at step decision . If based on the dynamic collection profile the system should collect this question from the user then decision branches to the yes branch whereupon at step the question received from the user is added to dynamic question set after having its answer verified by an SME. Processing then proceeds to step to commence analyzing the initial performance of the QA system in answering the question that was just added to the dynamic question set. Returning to if the process is not collecting a new question and is simply testing the question received at step then decision branches to the yes branch to perform a subsequent performance analysis on a question that was previously added to dynamic question set .

At step the process sets a timer coinciding with the submission of the selected question to the QA system at step . When the response is received from the QA system at step the timer is stopped at step . At step the process analyzes and records the performance of the QA system based on the received response metrics such as the amount of time taken by the QA system to answer the question. This performance record is stored in data store which is used to record the historical dynamic performance data for the QA system that is being tested. At step the process analyzes and records the answer quality of the answer received by the QA system in response to the submitted question. In the case of a new dynamic question being tested the data added to historical data store represents the initial performance metrics pertaining to the question. In one embodiment the answer quality is based on whether the QA system responded with an answer that matches the verified answer that corresponds to the question retrieved from data store . In one embodiment the answer quality is further based on a confidence level that the QA system established for the answer that was returned at step .

A decision is made by the process as to whether it is time to analyze the historical dynamic performance data or if additional dynamic performance data should be collected decision . If more dynamic performance data is being collected then decision branches to the no branch which loops back to select and process the next dynamic question as described above. This looping continues until all of the stored dynamic questions from data store have been processed at which point decision branches to the yes branch to analyze the dynamic performance data.

At step the process analyzes the general answer quality using the historical answer quality data stored in data store over a period of time and generates one or more general answer quality results such the degradation of general answer quality over time. The results of the general answer quality analysis are stored in dynamic analysis results data store .

At step the process analyzes the general answer performance using the historical answer quality data stored in data store over the period of time and generates one or more general answer performance results such the degradation of general answer performance over time. The results of the general answer performance analysis are stored in dynamic analysis results data store .

At step the process analyzes the answer quality of answers for each question type e.g. date questions factoid questions etc. using the historical answer quality data stored in data store over a period of time and generates one or more answer quality results for each question type such the degradation of answer quality pertaining to each particular question type over time. The results of the analysis of answer quality for each question type are stored in dynamic analysis results data store .

At step the process analyzes the answer performance of answers for each question type using the historical answer quality data stored in data store over the period of time and generates one or more answer performance results for each question type such the degradation of answer performance pertaining to each particular question type over time. The results of the analysis of answer performance for each question type are stored in dynamic analysis results data store . After the dynamic answer analysis results have been generated and stored in data store the process returns to the calling routine see at .

Returning to annotator health feedback processing commences at whereupon at step the process selects the first QA system pipeline component that records health and performance data of a QA system component. In one embodiment QA system profile retrieved from data store is used to identify the QA system pipeline components that record health and performance data.

At step the process locates the data store such as data store that is used to store the health and performance data for the selected QA system pipeline component. At step the process records the QA system pipeline component in data store with data store being used to store annotator analysis results. The recording of the component is used to identify which component data is currently being reported. At step the process retrieves the current status of the selected QA system pipeline component from data store and records stores the current status in data store . At step the process retrieves the machine data corresponding to the selected QA system pipeline component from data store and records stores the machine data in data store . At step the process retrieves the port and other health and or performance data corresponding to the selected QA system pipeline component from data store and records stores the port and other health and or performance data in data store .

A decision is made by the process as to whether there are more QA system pipeline components that record health and performance data that need to be processed based on the QA system pipeline profile decision . If there are more QA system pipeline components to process then decision branches to the yes branch which loops back to select and process the annotator health feedback data corresponding to the next component as described above. This looping continues until all of the QA system pipeline components that record health and performance data have been processed at which point decision branches to the no branch and processing returns to the calling routine see at .

At step the process analyzes the overall performance of the QA system and compares the overall performance to one or more thresholds for overall system health status. Process receives performance analysis results gathered by the static performance routine from data store the dynamic performance routine from data store and the annotator performance routine from data store . Based on a comparison of the overall performance with the appropriate thresholds a detection of a possible overall performance problem is noted such as by using a color coding e.g. green yellow red etc. to notify a user of the QA system of the overall performance health of the QA system. The detection of a possible overall QA system problem and the overall QA system health analysis data is written to QA system health report .

At step the process retrieves metrics based on static analysis results such as the last static baseline questions run date number of static questions asked the number of questions answered with first rank the aggregate confidence score and the timing and other static performance analysis data. In addition the static analysis results include analysis data based on each question type e.g. factoid questions date questions etc. . The static performance analysis results are retrieved from data store . The retrieved metrics and data are written to QA system health report . At step the process analyzes the static performance analysis results both the answer quality results as well as the performance e.g. timing etc. results and compares the results to corresponding thresholds. The static performance analysis results are retrieved from data store while the thresholds were previously retrieved from data store . In one embodiment the thresholds for the answer quality results and the performance results are used to indicate an alert level e.g. green yellow red etc. . In addition thresholds are applied to each question type and the user is alerted e.g. green yellow red etc. if a problem is detected in the QA system s processing of one or more question types. The detection of possible problems with the QA system answering static questions including various question types is reported to a user of the QA system by writing the detections to QA system health report .

At step the process retrieves metrics based on dynamic analysis results such as the last dynamic baseline questions run date number of dynamic questions asked the number of questions answered with first rank the aggregate confidence score and the timing and other dynamic performance analysis data. In addition the dynamic analysis results include analysis data based on each question type e.g. factoid questions date questions etc. . The dynamic performance analysis results are retrieved from data store . The retrieved metrics and data are written to QA system health report . At step the process analyzes the dynamic performance analysis results both the answer quality results as well as the performance e.g. timing etc. results and compares the results to corresponding thresholds. The dynamic performance analysis results are retrieved from data store while the thresholds were previously retrieved from data store . In one embodiment the thresholds for the answer quality results and the performance results are used to indicate an alert level e.g. green yellow red etc. . In addition thresholds are applied to each question type and the user is alerted e.g. green yellow red etc. if a problem is detected in the QA system s processing of one or more question types. The detection of possible problems with the QA system answering dynamic questions including various question types is reported to a user of the QA system by writing the detections to QA system health report .

At step the process reports data regarding performance and health for each QA system pipeline component that gathers health and performance data. In one embodiment the reporting includes data such as the component name the current status of the component machine information corresponding to the component port information of the component and other health and or performance data gathered by components. The QA system component performance and health data is retrieved from data store . At step the process analyzes the annotator analysis results retrieved from data store and compares the analysis results to thresholds previously retrieved from data store . The thresholds are used to identify any performance and or quality problems with any of the components and notify user of such problems such as by using the color coded alert system e.g. green yellow red etc. . The detection of possible problems with any of the QA system components is reported to a user of the QA system by writing the detections to QA system health report . Processing then returns to the calling routine see at .

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that in some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

While particular embodiments of the present invention have been shown and described it will be obvious to those skilled in the art that based upon the teachings herein that changes and modifications may be made without departing from this invention and its broader aspects. Therefore the appended claims are to encompass within their scope all such changes and modifications as are within the true spirit and scope of this invention. Furthermore it is to be understood that the invention is solely defined by the appended claims. It will be understood by those with skill in the art that if a specific number of an introduced claim element is intended such intent will be explicitly recited in the claim and in the absence of such recitation no such limitation is present. For non limiting example as an aid to understanding the following appended claims contain usage of the introductory phrases at least one and one or more to introduce claim elements. However the use of such phrases should not be construed to imply that the introduction of a claim element by the indefinite articles a or an limits any particular claim containing such introduced claim element to inventions containing only one such element even when the same claim includes the introductory phrases one or more or at least one and indefinite articles such as a or an the same holds true for the use in the claims of definite articles.

