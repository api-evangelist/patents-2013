---

title: Integration of an independent three-dimensional rendering engine
abstract: Systems and approaches provide for a user interface (UI) that is based on the position of a user's head with respect to a computing device. In particular, a three-dimensional (3D) rendering engine that is independent of a particular operating system can be integrated with the UI framework of the operating system such that a window or view into a fully 3D world can be drawn using the independent renderer. This window or view can then be laid out and manipulated in a manner similar to other elements of the UI framework. Further, the 3D window or view can be configured to monitor head tracking data as input events to the UI framework. The contents of the window or view can be redrawn or rendered based on the head tracking data to simulate three-dimensionality of the content.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09317175&OS=09317175&RS=09317175
owner: Amazon Technologies, Inc.
number: 09317175
owner_city: Reno
owner_country: US
publication_date: 20130924
---
As computing devices such as laptops tablets and smartphones become increasingly sophisticated new and interesting approaches have arisen for enabling a computing device to convey information to a user and vice versa. For instance a graphical user interface GUI incorporating windows icons and menus can be an advancement over a command line interface by simplifying how a user operates an underlying software application. A user can cause the underlying application to perform a desired function without having to memorize a command or its syntax and manually typing in the command. As another example a touch based interface can be an improvement over an interface that relies on a directional keypad e.g. up down left right by giving users a more immediate and precise way of selecting a UI element. Further a touch based interface may provide a more intuitive manner of operating a computing device by enabling users to directly manipulate UI elements. As devices become more powerful and new techniques are developed for users to interact with computing devices UIs can be further improved upon to provide a more compelling and user friendly computing experience.

In various embodiments user interfaces that are based at least in part on a user s position with respect to a computing device and or motion orientation of the computing device are provided. One or more three dimensional 3D graphical elements may be presented on a two dimensional 2D display screen or other display element. One or more processes can be used to determine a relative position direction and or viewing angle of the user. For example head or face tracking or tracking of a facial feature such as a user s eyebrows eyes nose mouth etc. and or related information e.g. motion and or orientation of the computing device can be utilized to determine the relative position of the user and information about the relative position can be used to render one or more of the 3D graphical elements to correspond to the user s relative position. Such a rendering can give the impression that the 3D graphical elements are fixed in space and that the 3D graphical elements are arranged in 3D space. Conventional systems may only provide support for low level 3D drawing and rendering. For example the 3D capabilities for the Android and iOS platforms may be limited to Open Graphics Library OpenGL or the version of OpenGL optimized for mobile devices OpenGL for Embedded Systems OpenGL ES while Windows Phone may be limited to Direct3D . Users however may desire highly detailed 3D graphical elements that the conventional system s built in 3D graphics library may be not be capable of drawing and or may be inefficient in rendering. Further a head tracking based user interface may demand low latency and the conventional system s inherent 3D graphics subsystem may not provide the performance necessary for rendering 3D graphics in response to user and or device motion.

In some embodiments an independent 3D rendering engine that may have a broader set of features and or better performance than the built in or inherent 3D graphics capabilities of a conventional system can be utilized for drawing and rendering 3D graphics. For example an independent 3D rendering engine may offer features such as support for 3D meshes models and scenes not available for OpenGL or Direct3D as well as perform rendering more efficiently than these built in libraries. A conventional approach to provide an independent rendering engine to an operating system is to port the independent renderer to the operating system. While 3D content may then be drawn and rendered using the independent rendering library such an approach may be not be closely integrated with the user interface framework of the operating system. For instance this technique may be suitable for 3D games that are displayed in full screen mode and outside of the UI framework of the conventional system but users may prefer a more cohesive look and feel for other user applications and or desire the full functionality of a user interface framework accompanied with highly detailed complex 3D content.

Systems and methods in accordance with various embodiments of the present disclosure may overcome one or more of the aforementioned and other deficiencies experienced in providing content for display on a computing device. In particular a 3D rendering engine or library that is independent of an operating system or platform can be integrated with a UI framework of the operating system to draw or render 3D content. In this way 3D content can be generated using the independent renderer yet the UI elements containing 3D content rendered by the independent 3D rendering engine can behave and be manipulated like other user interface elements of the UI framework. In some embodiments the UI elements containing the 3D content rendered by the independent renderer can be adapted to recognize user or device motion as input events for a computing device. For instance a representation of a face or head of a user can be captured using a camera of the device and or motion and orientation of the device can be estimated using a sensor or input element such as an accelerometer gyroscope and or magnetometer. The computing device can use a position of the user in an image captured by the camera and or the device motion and orientation data to determine a user s position with respect to the device or related information such as the user s viewing angle . As the user moves his head and or moves the device the user s position with respect to the device may change. These changes in the user s relative position may be monitored by the computing device and a change in relative position meeting determined thresholds can cause one or more 3D graphical elements to be redrawn or rendered using the independent renderer to correspond to the new relative position so as to enhance simulated three dimensionality.

Various other functions and advantages are described and suggested below in accordance with the various embodiments.

In this example a vector is shown from a point at the center of the display screen to a point on the head of the user such as between the user s eyes. Vector is a three dimensional vector that can be initially determined for example using a facial detection process performed on the image s captured by the camera . In other embodiments the vector can be determined using sensors or other input devices of a computing system such as accelerometers gyroscopes magnetometers proximity sensors among others. In at least some embodiments vector can be used by the device to smoothly animate content displayed on the screen to compensate for a change in perspective of the user with respect to the screen. In other embodiments multiple vectors can be determined between the computing device and various features of the user such as the user s eyebrows eyes irises pupils or mouth. In still other embodiments other representations or models can be used to define the relationship between the user and the computing device such as primitive geometric shapes e.g. rectangle ellipse circle contours e.g. edges comprising the boundaries of the user s head or face silhouettes e.g. regions within the contours of the user s head or face etc.

As illustrated in when the user positions the computing device in front of himself and gazes directly towards the display screen such that the x y plane of the device is perpendicular or substantially perpendicular to the user s line of sight where the lateral axis corresponds to the x axis the longitudinal axis corresponds to the y axis and the depth of the device corresponds to the z axis the image data captured by the camera of the device can include the image . Although one image is shown to be captured in this instance it will be appreciated that multiple images captured by a same camera at successive times multiple images captured by multiple cameras at the same time or substantially the same time or some combination thereof can be analyzed in various embodiments. Further other embodiments may additionally or alternatively use other approaches such as proximity sensors to determine the position or the viewing angle of the user with respect to a computing device. In this example a user detection process is adapted for locating the head or face of a user in the image s captured by the camera . Here the user detection process locates the head or face of the user within the image and provides as output the dimensions of a bounded box indicating where the head or the face of the user is located in the image. In this example the device is capable of rendering one or more three dimensional graphical elements for display on the two dimensional display screen according to the position or viewing angle of the user with respect to the display screen. The device can rely on the position of the head or face of the user with respect to the device and or the apparent motion of the user s head or face with respect to the device as defined by vector to render an appropriate perspective of a 3D object. illustrates an example of a 3D box with a top surface a left front facing surface and right front facing surface that has been drawn based on the current position or viewing angle of the user with respect to the device as seen in . In this example the front facing surfaces and appear to be equal in scale when the user s line of sight is perpendicular or substantially perpendicular to the display screen .

in an example situation illustrates that the user has tilted the computing device to the right with respect to the user i.e. the user has rotated the device to his right along the longitudinal or y axis to obtain a second view or perspective of the 3D box as depicted in . As seen in the tilt or rotation of the device causes the camera to capture a different view or perspective of the user within image here a three quarters profile of the head or face of the user facing leftward. The position of the user s face may also be shifted towards a right region of the image because of the rightward tilt or rotation of the device. In this example the apparent motion of the face or head of the user which may be primarily due to the motion of the device can be tracked from the initial position detected in to the new position depicted in . For instance the 3D vector between the point between the user s eyes and the center of the front face of the device may be recalculated based on the new position and orientation of the user with respect to the computing device. Based on the apparent movement of the user s head or face the 3D box can be redrawn rendered for display to correspond to the new perspective of the user . For example as the user tilts the device rightward the device will render and or animate the 3D box such that the left front facing surface of the cube appears more prominently on the display screen and the right front facing surface appears at a more oblique angle.

In an example situation illustrates that the user has tilted the computing device to the left with respect to the user i.e. the user has rotated the device to his left along the longitudinal or y axis to obtain a third view or perspective of the box. As seen in the tilt or rotation of the device causes the camera to capture a three quarters profile of the head or face of the user facing rightward and the position of the user s face has also shifted towards a left region of the image. In this example the apparent movement of the face or head of the user which again may be primarily due to the movement of the device can be tracked from a previous position to the new position depicted in . The 3D box can be rendered for display based on the apparent motion of the user s head or face as seen in . For example if the user is initially positioned with respect to the computing device as seen in and the user tilts the device towards his left until the user is positioned with respect to the device as depicted in the cube will transition or be animated from the perspective seen in to the perspective seen in . As the user continues rotating the device towards his left the 3D box may be animated or rendered from the perspective seen in to the perspective seen in wherein the right front facing surface of the cube appears more prominently on the display screen and the left front facing surface appears at a more oblique angle.

It will be appreciated that when the user tilts the device forward i.e. rotating the device along the x axis such that the top portion of front of the device is further away from the user and the bottom portion of the front of the device is closer to the user details of a bottom surface not shown of 3D box may be displayed and the top surface may be hidden. Likewise tiling the device backwards i.e. rotating the device along the x axis such that the top portion of the front of the device is closer to the user and the bottom portion of the front of the device is further from the user may cause the top surface to be displayed more prominently and hide the bottom surface not shown of the box. It will be understood that the head or face of the user can generally be tracked according to six degrees of freedom e.g. motion along the x y and z axes or forward backward up down and left right and rotation along the x y and z axes or pitch yaw and roll and the device can be configured to appropriately respond to such various motions. For example when the user tilts the device backwards and to his right i.e. such that the top left corner of the device is the closest point of the device to the user the top surface and left front facing surface of the box may be displayed more prominently and the right front facing surface may be appear at a more oblique angle and the bottom surface may remain hidden. When the user tilts the device forward and towards his left i.e. such that the bottom right corner of the device is the closest point of the device to the user the bottom surface and right front facing surface can be displayed with more details and the top surface and left front facing surface may appear more obscured or hidden. Approaches for tracking a user s head or face to determine a position or viewing angle of the user with respect to the device are discussed in co pending U.S. patent application Ser. No. 13 965 126 entitled Robust User Detection and Tracking filed Aug. 12 2013 which is incorporated herein by reference.

Although a single 3D box is shown in the example of it will be appreciated that much more complex 3D graphical elements and backgrounds are used in various embodiments. Animating or rendering multiple complex 3D graphical elements may exceed the 3D graphics capabilities of a conventional system that relies on a low level 3D graphics library such as OpenGL or Direct3D particularly for a user interface that is sensitive to latency such an interface based on head tracking. However in accordance with various embodiments integrating an independent 3D rendering engine with the system s UI framework can enable highly detailed 3D graphics to be drawn and rendered in response to head tracking data captured by a computing device. As used herein a UI framework refers to a component of an operating system or platform enabling users to interact with a computing device running the operating system or platform and includes approaches for the user to provide input to the computing device and for the computing device to provide output to the user. User input can include key or keyboard input touch gestures or other pointer based approaches e.g. mouse pointing stick track pad voice or other audio commands visual gestures such as hand gestures or head gestures and movements among other input techniques. Output provided by the computing device can include displaying text and or graphics audio flashing of light emitting diodes LEDs or other light sources among other approaches for presenting data to a user. Examples of UI frameworks include the View System of the Android operating system the UIKit of the iOS platform the Presentation Core for the Windows Phone platform among others. As mentioned a 3D rendering engine may be independent of an operating system when it is not incorporated or built into the operating system. For example Android and iOS incorporate OpenGL or OpenGL ES and Windows Phone incorporates Direct3D . On the other hand a separate 3D rendering engine such as Open Graphics Rendering Engine OGRE 3D Irrlicht Engine OpenSceneGraph Euclid among others can be characterized as independent of the operating system. A characteristic of an independent 3D renderer may be lack of integration with the UI framework of the operating system. For example typical UI elements or views of the Android platform can be rotated translated scaled or faded e.g. transparency effect . For example Android provides a Native Development Kit NDK to enable developers to port native code to a computing device operating the Android platform including porting an independent 3D rendering engine. However such an approach may not be integrated with the UI framework as 3D content may be rendered full screen and the UI framework is disabled or otherwise unused. That is a native window cannot be manipulated using the UI framework e.g. rotated translated scaled or faded and cannot leverage the layout measurement optimizations and other functionality provided by the UI framework.

The next layer in the software stack is the system libraries layer which can provide support for functionality such as windowing e.g. Surface Manager 2D and 3D graphics rendering Secure Sockets Layer SSL communication. SQL database management audio and video playback font rendering webpage rendering System C libraries among others. In an embodiment system source libraries layer can comprise open source libraries such as the Skia Graphics Library SGL e.g. 2D graphics rendering Open Graphics Library OpenGL e.g. 3D graphics rendering Open SSL e.g. SSL communication SQLite e.g. SQL database management Free Type e.g. font rendering WebKit e.g. webpage rendering libc e.g. System C libraries among others. Thus dependencies are established between higher levels of the software stack of operating system and these system libraries. In particular SGL and OpenSGL can be characterized as being built in or inherent to the operating system .

Located on the same level as the system libraries layer is the runtime layer which can include core libraries and the virtual machine engine. In an embodiment the virtual machine engine may be based on Dalvik . The virtual machine engine provides a multi tasking execution environment that allows for multiple processes to execute concurrently. Each application running on the device is executed as an instance of a Dalvik virtual machine. To execute within a Dalvik virtual machine application code is translated from Java class files .class .jar to Dalvik bytecode .dex . The core libraries provide for interoperability between Java and the Dalvik virtual machine and expose the core APIs for Java including data structures utilities file access network access graphics among others.

The application framework comprises a set of services through which user applications interact. These services manage the basic functions of a computing device such as resource management voice call management data sharing among others. The Activity Manager controls the activity life cycle of user applications. The Package Manager enables user applications to determine information about other user applications currently installed on a device. The Window Manager is responsible for organizing contents of a display screen. The Resource Manager provides access to various types of resources utilized by user application such as strings and user interface layouts. Content Providers allow user applications to publish and share data with other user applications. The View System is an extensible set of UI elements or views used to create user interfaces for user applications. The Notification Manager allows for user applications to display alerts and notifications to end users. The Telephony Manager manages voice calls. The Location Manager provides for location management such as by GPS or cellular network. Other hardware managers in the application framework include the Bluetooth Manager WiFi Manager USB Manager Sensor Manager among others.

Located at the top of the software stack are user applications such as a home screen application email application web browser telephony application among others.

A view system also sometimes referred to as a user interface framework such as view system of can provide a manner for presenting data to a user e.g. graphics text and audio and the means for the user to interact with that data e.g. key presses such as via a keyboard or buttons virtual pointer gestures such as via a mouse or trackpad physical pointer gestures such as via touch or a stylus device motion gestures facial movement or gestures hand gestures among others . In some embodiments a UI framework or view system can be abstracted out as individual user interface elements or views in the example operating system of . A UI element or view occupies a rectangular area of a display screen and can be responsible for drawing and event handling. An example of a UI element or view can include a text field image button as well as UI elements containing such UI elements or views. UI elements or views may be arranged for display on the display screen by placing them within layout container views or view groups in the example operating system of .

Users may interact with a UI element or view such as by clicking on touching or swiping the UI element or view issuing an audio command shaking a device among other possibilities. These user interactions are sometimes referred to as input events and UI elements or views may be configured to monitor for specific input events via event listeners. An event listener is an implementation of a UI element or view that contains a callback function. These functions will be invoked by the UI framework or view system for the UI element or view to which the listener has been implemented when triggered by a particular user interaction with the UI element or view. For instance in the example of the UI element or view containing icons may register a head tracking based event listener for tracking the user s position or viewing angle with respect to the computing device. As mentioned in various embodiments head or face tracking and or device motion detection can be used to estimate the position of the user s head or face with respect to a computing device. Such input data can be captured using cameras sensors such as accelerometers gyroscopes and magnetometers and or other input elements of a computing device as discussed elsewhere herein or incorporated by reference herein. A new relative position of the user or related information such as the viewing angle of the user with respect to the computing device can be determined by analyzing such input data and the new relative position or viewing angle may be recognized as an input event that will trigger the head tracked based event listener s callback function. The callback function may cause the icons to be redrawn or rendered according to the new relative position or viewing angle.

When a UI element or view is drawn for the first time or needs to be redrawn or rendered in response to an input event such as a new position or viewing angle of the user with respect to a computing device the UI framework or view system first traverses the UI element or view hierarchy to measure and layout the UI elements or views according to the appropriate size and position of each UI element or view before drawing each UI element or view. illustrates an example of an implementation for this process that can be used in accordance with an embodiment. In the UI framework or view system drawing can be initiated by a call to invalidate a UI element or view or mark the UI element or view as dirty such as in response to the input event. For instance user interaction with a UI element or view may cause a change to the UI element or view. Pressing a virtual button may change the color of the button cause the button to appear to be depressed cause a menu or new window to be displayed etc. As a result the button may have to be redrawn or rendered. To the extent the button also causes a menu or new window to be displayed the portion of the display screen where the menu or new window will be displayed also must be redrawn or rendered. In conventional approaches the entire UI element or view may have to be redrawn or rendered even when only a limited portion of the UI element or view to change in response to user interaction. To optimize drawing of the UI elements those portions of the UI element or view that need to be redrawn or rendered are invalidated marked as dirty or otherwise marked as requiring redrawing. In this example UI element or view has been invalidated or marked as requiring redrawing which propagates the invalidation or marking message up the UI element or view hierarchy as indicated by the dashed line to determine the dirty regions i.e. those regions of the screen that need to be redrawn. First the invalidation or marking message is propagated from UI element or view to its parent layout container or view group then to the parent of layout container or view group layout container or view group and finally to the root of the UI element or view hierarchy. UI root element or view root . In many instances the UI root element or view root encapsulates the penultimate layout container view group or UI element view but the UI root element or view root includes additional functions that are not implemented by a layout container or view group or a UI element or view. The UI root element or view root then schedules traversals of all of the UI elements or views within the hierarchy to measure and layout the UI elements or views appropriately and draw those UI elements or views intersecting with the dirty regions as indicated by the solid line.

A potential disadvantage with this approach is that the drawing traversal may result in the redrawing of a UI element or view that has not been changed. For example a user application may invoke invalidate on a button that overlays a more complex UI element or view such as a map UI element or view. Thus the map UI element or view may be redrawn despite no changes occurring to the map UI element or view. For a computing device that supports hardware acceleration e.g. via a GPU drawing can be optimized by deferring drawing of each UI element or view and storing the draw commands as display lists. A display list can be thought of as a cache and a display list item is a cached instance of the output of a drawing command that can be retrieved without having to re execute all of the code of the drawing command. Further a display list item can be manipulated by operations such as translation rotation scale skew transparency change among others without first re executing all of the code of the drawing command. By retrieving a drawing command output from a display list less code may be executed in the aggregate. In addition the UI framework or view system can record or update display lists that have been marked dirty and a UI element or view that has not been invalidated can be retrieved by re issuing the corresponding display list that was previously recorded. Thus in at least some embodiments updating of a user interface can include propagation of invalidation messages through the UI element or view hierarchy recording and updating display lists and drawing the display lists.

In some operating systems or platforms there may be multiple approaches for drawing or rendering graphics for user interface elements. For instance the example operating system of may enable user applications to employ standard or default components e.g. text view button view image view that handle drawing of UI elements or views without additional processing. In many situations however a user may desire customized drawing of UI elements. Various approaches can be implemented for such customized rendering within the example operating system of including 2D or 3D software based rendering or GPU based rendering within the example operating system or native rendering based on existing applications written in native code or which use native libraries. That is the example operating system provides support for third party applications libraries or systems written in native code such as C or C . However the conventional approach implemented by the operating system for native rendering does not include support for the user interface framework or view system of the example operating system. For example the UI framework or view system is effectively disabled or otherwise unused during execution of a native application and the native application uses a full screen for rendering content without the layout measurement optimizations and other functionality provided by the UI framework or view system.

A surface corresponds to an off screen buffer into which a user application renders content. For example an application might draw 2D objects into the surface using SGL or 2D rendering of GPU accelerated UI elements using the HWUI library . Surface manager or Surface Flinger handles surface composition or effects such as fading or page flipping for one or more surfaces such as surface of multiple user applications such as user application . Although this example shows that each user application corresponds to only one surface it will be appreciated that in other embodiments user applications may correspond to multiple surfaces. Surface Flinger determines what will be presented on a display screen when multiple surfaces are being rendered and performs overlays inter surface animations or effects among other possibilities. Surface Flinger uses HW composer a hardware abstraction layer HAL to perform composition using hardware resources like a 3D GPU or a 2D graphics engine. The 2D Blitter is another hardware abstraction layer enabling the use of special hardware to speed up graphics operations like blitting or bit level block transfer for accelerating software rendering on systems without 3D GPUs. In performing surface compositioning Surface Flinger may also use the same libraries and other resources that a surface may use such as SGL OpenGL libagl PixelFlinger GPU among others.

The conventional graphics subsystem provides limited approaches for 3D rendering of elements of the view system of operating system of including surface views texture views RenderScript and OpenGL wrapper functions. Surface views are embedded within the view hierarchy but use a separate drawing surface than the rest of the view hierarchy and run on a thread separate from the main UI thread. When a surface view is instantiated a secondary window is created at the location of the parent view and the surface view punches a hole in the parent view by causing the parent view to be displayed transparently. This approach enables video or OpenGL content to be presented in the secondary window of the surface view but it is difficult to manipulate the window of the surface view itself such as performing animations or transformations like other views. For example other UI elements or views can be rotated scaled or translated or faded while it may be difficult and or inefficient to perform such transformations for surface views. In some embodiments a typical UI element or view can be bound by a 2D rectangle or x y plane that can be rotated 360 around the z axis of the rectangle. The typical UI element or view can also be scaled or increased or decreased in size while the relative proportions of the UI element or view remain the same. The typical UI element or view can also be translated or moved to a new x y position. The typical UI element or view can also be faded or have a transparency effect applied such that the UI element or view can appear translucent or opaque. Similar to surface views a texture view enables a content stream e.g. video or an OpenGL scene to be displayed in the window of the texture view and the texture view runs on a separate thread from the main UI thread. A texture view is also similar to a surface view in that the contents of the texture view are rendered in a different context from the UI element or view hierarchy. In particular a different OpenGL thread or video thread is streamed inside a texture view. Unlike a surface view however a texture view does not create a separate window and behaves like a regular UI element or view in that the texture view can be animated transformed scrolled among other possibilities. Further texture views require a GPU accelerated device.

RenderScript is a language based on C99 API and runtime library that can be used for rendering user interface components. RenderScript can be implemented at the application framework level of and includes additional components written using the RenderScript language. These scripts are initialized from the virtual machine of the runtime library . RenderScript has a reference to a drawing surface that it renders to in a thread that is separate from the main UI thread. The example architecture of also provides for OpenGL wrapper functions that call the underlying OpenGL APIs from within views. The wrapper functions are provided for legacy support and are not intended for current use.

There may be certain disadvantages with each of the conventional approaches for enabling 3D rendering of the UI elements of the UI framework or view system. First all of the approaches are limited to the built in OpenGL rendering library. Users may prefer to use a rendering engine that is independent of the operating system of such as OGRE 3D Irrlicht OpenSceneGraph Euclid among others. Such engines may offer a richer feature set than OpenGL . For example OGRE 3D e.g. version 1.8 or later provides fixed function operations such as multi texture and multi pass blending texture coordinate generation and modification independent color and alpha operations mesh features such as flexible mesh data formats Biquadric Bezier patches for curved surfaces progressive meshes and static geometry batching skeletal animation including multiple animation blending with variable weights variable multiple bone weight skinning manual bone control and configurable interpolation modes flexible animation support including morph animation or pose animation animate of scene nodes including spline interpolation when necessary scene features including a hierarchical scene graph multiple shadow rendering techniques in both modulative and additive approaches stencil and texture based and scene querying special effects such as full screen post processing particle systems skyboxes skyplanes skydomes biliboarding for sprite graphics ribbon trails among others. Drawing or rendering performance of UI elements may also be better in certain use cases using an independent renderer rather than the built in or inherent OpenGL rendering library.

Further surface views texture views and RenderScript each render on a separate thread from the main UI thread and rendered content is drawn onto surfaces separate from the view hierarchy. As a result the OpenGL context is separate from the view hierarchy surface in these conventional approaches and it may be difficult and or inefficient to manipulate these views like other UI elements or views in the UI framework or view system of . For example operations such as rotate translate scale or setting transparency are not readily available for surface views because surface views are outside of the view hierarchy and composition order is not preserved. Although texture views allow for the window containing OpenGL content to be manipulated by the view system e.g. composition order is preserved the actual surface of a texture is streamed from a separate thread and thus the contents of a texture view are indirectly drawn to the UI framework or view system such that performance may be hindered.

Approaches in accordance with various embodiments integrate a 3D rendering engine that is independent of the operating system of e.g. a 3D rendering library other than OpenGL with the user interface framework or view system . That is a UI element or view can be configured to utilize an independent 3D renderer for drawing the contents of the UI element or view while still being capable of being laid out measured and or otherwise manipulated like other elements of the UI framework or view system. illustrates an example graphics architecture that can be used in accordance with an embodiment. Built in graphics libraries such as SGL and OpenGL remain available to user applications . However an independent 3D rendering engine is also exposed to the UI framework or view system for those user applications that wish to render 3D content using an independent renderer. The independent 3D renderer can include OGRE 3D Irrlicht OpenSceneGraph Euclid among others.

In the example of the independent renderer can express its content as OpenGL format in order for Surface Flinger to composite the content drawn by independent renderer with other views that may draw using SGL or OpenGL . In another embodiment the independent renderer can be utilized for compositing each surface to be displayed for a computing device. For example a first user application may generate a first UI element or view that contains one or more first 3D graphical elements rendered using the independent renderer. A second user application may generate a second UI element or view that contains one or more second graphical elements rendered using the built in or inherent 2D graphics library that is software based e.g. SGL the built in or inherent 2D graphics library that is GPU accelerated e.g. OpenGL or the built in or inherent 3D graphics library e.g. OpenGL . To display both UI elements or views simultaneously such as both the UI elements or views being fully displayed in separate portions of the display screen or the first UI element or view stacked on top of the second UI element or view or vice versa the UI elements or views may be composited using the independent rendering engine. In this manner at least a portion of both UI elements or views can be displayed concurrently and at least a portion of one or more of the first 3D graphical elements and at least a portion of one or more of the second graphical elements 2D or 3D may be displayed on the display screen at the same time. It will be appreciated that the independent renderer can be used to composite multiple UI elements or views including multiple 3D graphical elements rendered by the independent renderer and or multiple UI elements or views including multiple graphical elements rendered by the built in or inherent 2D or 3D renderers of the example operating system of e.g. SGL or OpenGL .

Returning to the example of it can be also seen that independent rendering engine supports hardware acceleration e.g. via the GPU . In some embodiments an independent renderer may be written in a native language such as C or one of its variants including K R C C89 C90 C11 or Embedded C C or Objective C among others and can be ported to the operating system of using compilers linkers etc. e.g. provided by a native development kit NDK . Wrapper classes or functions can be used to encapsulate the application programming interface API of the native and independent rendering library which can then be invoked by the UI element or view hierarchy for 3D rendering by the independent renderer. An interface such as Java Native Interface JNI can be used to enable UI elements which may be written in Java to be able to invoke the functionality of the independent renderer which may be written in a language other than Java such as C or a variation thereof. In another embodiment the independent renderer can be written in a software programming language directly supported by an operating system such as Java for the operating system of .

In some embodiments a model view can be created for 3D rendering by the independent rendering engine . The model view can be rotated scaled translated or otherwise transformed like other UI elements or views of the UI framework or view system. Similar to other UI elements or views the model view can also register event listeners such as a head tracking based event listener to enable user interaction with the model view. The model view may also implement the same invalidate and draw traversals used by other UI elements or views such as when a head tracking event indicates that the user s viewing angle has changed and the contents of the model view need to be redrawn or rendered to correspond to the new viewing angle. However unlike conventional approaches in at least some embodiments rendering of 3D content by the 3D renderer can occur during the traversal of the display list and the 3D rendering context can be shared with the surface to which the UI element or view hierarchy will be drawn. The wrapper classes or functions corresponding to the independent renderer can be instantiated in the UI framework or view system and model views may retrieve pointers to the underlying API of the independent renderer. During the display list traversal the pointers to the underlying API of the independent rendering engine may be invoked to draw 3D content of the model view. As a consequence 3D rendering can occur at view composition time and composition order may be preserved. In addition in some embodiments the 3D rendering context and UI element or view hierarchy surface can be shared. Processing for the independent renderer and the UI element or view hierarchy can be performed on the same thread such that 3D content can be written directly to the UI element or view hierarchy surface rather than streamed such as in the approach of the texture view. This can improve performance by reducing the amount of memory allocations and eschew inter process communications that may be performed by conventional approaches.

The position of the user can be monitored and the movement of the user and or movement of the device meeting predetermined thresholds can be recognized as input events for the UI framework or view system. For instance the range of head motion for an average adult male includes a forward to backward movement of the neck from 60.4 to 69.6 a right to left bending of the neck from 40.9 to 36.3 and a right to left rotation of the head from 79.8 to 75.3 . A movement of the user s head up to a certain threshold such as 0.1 for precise applications or 10.0 for less precise use cases can trigger an event listener s callback function. In other embodiments movement of the user s head can be measured as displacements or as a combination of a rotation and a translation.

In some embodiments the position of the user s head can be estimated based on where the representation of the user s head is detected in images captured by one or more cameras. Depending on the application the relative position of the user can be represented in various ways. In certain embodiments the relative position can be represented as the raw pixel coordinates of the representation of the user s head detected in the captured image s . In other embodiments the relative position of can be computed with respect to the computing device. For example in one embodiment a three dimensional vector can be defined between a first point corresponding to the computing device such as the center of a display screen of the device and a second point corresponding to the user s head such as a point between the user s eyes. Determining the relative position user can include computing the values of this vector as the user s head face and or eyes move with respect to the device or the device is moved with respect to the user .

In still other embodiments more robust position information can be estimated by analyzing multiple images from multiple cameras captured at the same time or substantially at the same time in a process referred to as reconstruction. When there are two images or a stereo pair of images the reconstruction process may include finding a plurality of corresponding points between two images determining the fundamental matrix from the corresponding points determining the camera matrices from the fundamental matrix triangulation of the 3D points that project to the corresponding 2D points in the two images and rectifying the projective reconstruction to metric. Variations on this approach are possible such as where the cameras are calibrated. Approaches for camera calibration include the direct linear transformation DLT method or the algorithm set forth in Tsai Roger. A versatile camera calibration technique for high accuracy 3D machine vision metrology using off the shelf TV cameras and lenses. 3 no. 4 1987 323 344 or the algorithm set forth in Zhang Zhengyou. A flexible new technique for camera calibration. 22 no. 11 2000 1330 1334 each of which is incorporated herein by reference. In the case where the cameras are calibrated the essential matrix can be computed instead of the fundamental matrix and determining the camera matrices may be unnecessary.

Finding corresponding points between two images involves feature matching which is discussed elsewhere herein or incorporated elsewhere herein. The fundamental matrix is a mapping from the two dimensional projective plane of the first image to the pencil of epipolar lines corresponding to the second image. Approaches for determining the fundamental matrix include the seven point correspondences algorithm the normalized eight point algorithm the algebraic minimization algorithm minimization of epipolar distance minimization of symmetric epipolar distance the maximum likelihood Gold Standard method random sample consensus RANSAC least median of squares among others. In some embodiments the essential matrix may be calculated if the camera calibration matrices are known.

Triangulation computes the 3D point that project to each point correspondence between the two images. Approaches for triangulation include linear methods the optimal triangulation method among others. Rectifying the projective reconstruction to metric can be implemented directly such as by computing the homography for five or more ground control points with known Euclidean positions. Another approach for rectifying the projective reconstruction is referred to as the stratified method which may involve an affine reconstruction and a metric reconstruction.

One of ordinary skill in the art will appreciate that other embodiments may reconstruct 3D points from multiple 2D images such as approaches based on calculating the trifocal tensor for three images or techniques based on the factorization algorithm or bundle adjustment for n images. These various approaches for reconstruction are discussed in Hartley Richard et al. . Vol. 2. Cambridge 2000 which is hereby incorporated herein by reference.

Alternatively or in addition other sensors such as accelerometers gyroscopes magnetometers or some combination thereof can also be used to estimate a user s position relative to a computing device. For example when only motion orientation sensors are used it may be assumed that the absolute position of the user s head remains the same or substantially the same when the device is moved or its orientation changed. The motion and or orientation of the device can be determined from the motion orientation sensors and the relative position of the user can be estimated from the data captured by the motion orientation sensors based on the assumption that the user s absolute position remains the same. In other embodiments image analysis techniques can be combined with approaches using motion orientation sensors. The relative position of the user with respect to the device or additional analysis of the user s position such as determining a viewing angle of the user with respect to the device can be used as an input event by the UI framework or view system. The UI element or view can register a head tracking based event listener to monitor such input events.

As mentioned upon initial display the 3D graphical elements can include a first manner of presentation such as being displayed according to a first perspective projection rotation scale translation among other transformations corresponding to a user viewing angle that is perpendicular or substantially perpendicular relative to the screen. In some embodiments the 3D graphical elements can also include a first set of textural shading a first set of shadows and or a first set of reflections. A manner of presentation for a 3D object can therefore include a particular set of textural shading a set of shadows a set of reflections a rotation a scale a translation a perspective projection etc. for the 3D object at a specified relative position or viewing angle of the user. When the user s relative position or viewing angle is determined to have changed the 3D graphical elements can be redrawn or rendered according to the new relative position and or viewing angle of the user with respect to the computing device. Thus a second perspective projection rotation scale translation set of textural shades set of shadows set of reflections etc. for the 3D graphical elements can be generated based on the new relative position and or viewing angle of the user . The rendering can be performed using the independent 3D rendering engine and the UI element or view including the redrawn or rendered 3D graphical elements can be displayed to give the user the impression that he is interacting with the 3D graphical elements in a 3D environment.

The computing device includes at least one capacitive component or other proximity sensor which can be part of or separate from the display assembly. In at least some embodiments the proximity sensor can take the form of a capacitive touch sensor capable of detecting the proximity of a finger or other such object as discussed herein. The computing device also includes various power components known in the art for providing power to a computing device which can include capacitive charging elements for use with a power pad or similar device. The computing device can include one or more communication elements or networking sub systems such as a Wi Fi Bluetooth RF wired or wireless communication system. The device in many embodiments can communicate with a network such as the Internet and may be able to communicate with other such devices. In some embodiments the device can include at least one additional input element able to receive conventional input from a user. This conventional input can include for example a push button touch pad touchscreen wheel joystick keyboard mouse keypad or any other such component or element whereby a user can input a command to the device. In some embodiments however such a device might not include any buttons at all and might be controlled only through a combination of visual and audio commands such that a user can control the device without having to be in contact with the device.

The device also can include one or more orientation and or motion sensors . Such sensor s can include an accelerometer or gyroscope operable to detect an orientation and or change in orientation or an electronic or digital compass which can indicate a direction in which the device is determined to be facing. The mechanism s also or alternatively can include or comprise a global positioning system GPS or similar positioning element operable to determine relative coordinates for a position of the computing device as well as information about relatively large movements of the device. The device can include other elements as well such as may enable location determinations through triangulation or another such approach. These mechanisms can communicate with the processor whereby the device can perform any of a number of actions described or suggested herein.

In some embodiments the device can include the ability to activate and or deactivate detection and or command modes such as when receiving a command from a user or an application or retrying to determine an audio input or video input etc. For example a device might not attempt to detect or communicate with devices when there is not a user in the room. If a proximity sensor of the device such as an IR sensor detects a user entering the room for instance the device can activate a detection or control mode such that the device can be ready when needed by the user but conserve power and resources when a user is not nearby.

In some embodiments the computing device may include a light detecting element that is able to determine whether the device is exposed to ambient light or is in relative or complete darkness. Such an element can be beneficial in a number of ways. For example the light detecting element can be used to determine when a user is holding the device up to the user s face causing the light detecting element to be substantially shielded from the ambient light which can trigger an action such as the display element to temporarily shut off since the user cannot see the display element while holding the device to the user s ear . The light detecting element could be used in conjunction with information from other elements to adjust the functionality of the device. For example if the device is unable to detect a user s view location and a user is not holding the device but the device is exposed to ambient light the device might determine that it has likely been set down by the user and might tum off the display element and disable certain functionality. If the device is unable to detect a user s view location a user is not holding the device and the device is further not exposed to ambient light the device might determine that the device has been placed in a bag or other compartment that is likely inaccessible to the user and thus might turn off or disable additional features that might otherwise have been available. In some embodiments a user must either be looking at the device holding the device or have the device out in the light in order to activate certain functionality of the device. In other embodiments the device may include a display element that can operate in different modes such as reflective for bright situations and emissive for dark situations . Based on the detected light the device may change modes.

In some embodiments the device can disable features for reasons substantially unrelated to power savings. For example the device can use voice recognition to determine people near the device such as children and can disable or enable features such as Internet access or parental controls based thereon. Further the device can analyze recorded noise to attempt to determine an environment such as whether the device is in a car or on a plane and that determination can help to decide which features to enable disable or which actions are taken based upon other inputs. If speech or voice recognition is used words can be used as input either directly spoken to the device or indirectly as picked up through conversation. For example if the device determines that it is in a car facing the user and detects a word such as hungry or eat then the device might turn on the display element and display information for nearby restaurants etc. A user can have the option of turning off voice recording and conversation monitoring for privacy and other such purposes.

In some of the above examples the actions taken by the device relate to deactivating certain functionality for purposes of reducing power consumption. It should be understood however that actions can correspond to other functions that can adjust similar and other potential issues with use of the device. For example certain functions such as requesting Web page content searching for content on a hard drive and opening various applications can take a certain amount of time to complete. For devices with limited resources or that have heavy usage a number of such operations occurring at the same time can cause the device to slow down or even lock up which can lead to inefficiencies degrade the user experience and potentially use more power. In order to address at least some of these and other such issues approaches in accordance with various embodiments can also utilize information such as user gaze direction to activate resources that are likely to be used in order to spread out the need for processing capacity memory space and other such resources.

In some embodiments the device can have sufficient processing capability and the camera and associated image analysis algorithm s may be sensitive enough to distinguish between the motion of the device motion of a user s head motion of the user s eyes and other such motions based on the captured images alone. In other embodiments such as where it may be desirable for an image process to utilize a fairly simple camera and image analysis approach it can be desirable to include at least one orientation determining element that is able to determine a current orientation of the device. In one example the one or more orientation and or motion sensors may comprise a single or multi axis accelerometer that is able to detect factors such as three dimensional position of the device and the magnitude and direction of movement of the device as well as vibration shock etc. Methods for using elements such as accelerometers to determine orientation or movement of a device are also known in the art and will not be discussed herein in detail. Other elements for detecting orientation and or movement can be used as well within the scope of various embodiments for use as the orientation determining element. When the input from an accelerometer or similar element is used along with the input from the camera the relative movement can be more accurately interpreted allowing for a more precise input and or a less complex image analysis algorithm.

When using a camera of the computing device to detect motion of the device and or user for example the computing device can use the background in the images to determine movement. For example if a user holds the device at a fixed orientation e.g. distance angle etc. to the user and the user changes orientation to the surrounding environment analyzing an image of the user alone will not result in detecting a change in an orientation of the device. Rather in some embodiments the computing device can still detect movement of the device by recognizing the changes in the background imagery behind the user. So for example if an object e.g. a window picture tree bush building car etc. moves to the left or right in the image the device can determine that the device has changed orientation even though the orientation of the device with respect to the user has not changed. In other embodiments the device may detect that the user has moved with respect to the device and adjust accordingly. For example if the user tilts his head to the left or right with respect to the device the content rendered on the display element may likewise tilt to keep the content in orientation with the user.

The various embodiments can be further implemented in a wide variety of operating environments which in some cases can include one or more user computers or computing devices which can be used to operate any of a number of applications. User or client devices can include any of a number of general purpose personal computers such as desktop or laptop computers running a standard operating system as well as cellular wireless and handheld devices running mobile software and capable of supporting a number of networking and messaging protocols. Such a system can also include a number of workstations running any of a variety of commercially available operating systems and other known applications for purposes such as development and database management. These devices can also include other electronic devices such as dummy terminals thin clients gaming systems and other devices capable of communicating via a network.

The operating environments can include a variety of data stores and other memory and storage media as discussed above. These can reside in a variety of locations such as on a storage medium local to and or resident in one or more of the computers or remote from any or all of the computers across the network. In a particular set of embodiments the information may reside in a storage area network SAN familiar to those skilled in the art. Similarly any necessary files for performing the functions attributed to the computers servers or other network devices may be stored locally and or remotely as appropriate. Where a system includes computerized devices each such device can include hardware elements that may be electrically coupled via a bus the elements including for example at least one central processing unit CPU at least one input component e.g. a mouse keyboard controller touch sensitive display element or keypad and at least one output device e.g. a display device printer or speaker . Such a system may also include one or more storage components such as disk drives optical storage devices and solid state storage systems such as random access memory RAM or read only memory ROM as well as removable media memory cards flash cards etc.

Such devices can also include a computer readable storage media reader a communications component e.g. a modem a network card wireless or wired an infrared communication device and working memory as described above. The computer readable storage media reader can be connected with or configured to receive a computer readable storage medium representing remote local fixed and or removable storage devices as well as storage media for temporarily and or more permanently containing storing transmitting and retrieving computer readable information. The system and various devices also typically will include a number of software applications modules services or other elements located within at least one working memory device including an operating system and application programs such as a client application or Web browser. It should be appreciated that alternate embodiments may have numerous variations from that described above. For example customized hardware might also be used and or particular elements might be implemented in hardware software including portable software such as applets or both. Further connection to other computing devices such as network input output devices may be employed.

Storage media and computer readable media for containing code or portions of code can include any appropriate media known or used in the art including storage media and communication media such as but not limited to volatile and non volatile removable and non removable media implemented in any method or technology for storage and or transmission of information such as computer readable instructions data structures program modules or other data including RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disk DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage systems or any other medium which can be used to store the desired information and which can be accessed by a system device. Based on the disclosure and teachings provided herein a person of ordinary skill in the art will appreciate other ways and or methods to implement the various embodiments.

The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. It will however be evident that various modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims.

