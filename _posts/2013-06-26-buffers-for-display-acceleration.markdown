---

title: Buffers for display acceleration
abstract: Embodiments enable a graphics processor to more efficiently process graphics and compositing processing commands. In certain embodiments, a client application submits client graphics commands to a graphics driver. The client in certain embodiments can notify a window server that client graphics commands have been submitted. In response, the window server can generate compositing processing commands and provide these commands to the graphics driver. Advantageously, a graphics processor can execute the client graphics commands while the window server generates compositing processing commands. As a result, processing resource can be used more efficiently.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08842133&OS=08842133&RS=08842133
owner: Apple Inc.
number: 08842133
owner_city: Cupertino
owner_country: US
publication_date: 20130626
---
This application is a continuation of co pending U.S. application Ser. No. 12 397 334 filed on Mar. 3 2009 which claims priority to U.S. Provisional Application No. 61 033 753 filed on Mar. 4 2008 which provisional application is incorporated herein by reference in its entirety. This application is related to U.S. patent application Ser. No. 12 397 330 filed on Mar. 3 2009.

Many types of media devices currently exist for presenting media content to a user. For example televisions radios computers and a variety of portable or handheld devices are available for presenting various types of audio and or visual information to a user. Some examples of portable devices include music players e.g. MP3 players cell phones smart phones personal digital assistants PDAs portable televisions laptops and the like.

In presenting media to a user media devices generally display graphics and or video content. Some media devices display multiple types of content at once by layering one type of content over another. For example a user interface layer might include a status bar that is displayed at the same time as a video layer is displayed. The process of generating a combination of multiple layers for display can be referred to as compositing.

One drawback to compositing is that it can be resource intensive causing some devices to suffer in performance. For portable devices that tend to have lower processing and or memory power the performance loss can be significant.

In certain embodiments systems and methods are provided for managing a display that address some or all of the above mentioned problems. For example in certain embodiments systems and methods are provided for enabling a graphics processor to process certain commands while compositing processing commands are being generated. In certain embodiments these compositing processing commands are provided to the graphics processor without the graphics processor or its driver have to request them. As a result processing resources can be used more efficiently to improve performance in certain implementations.

The features of these systems and methods will now be described with reference to the drawings summarized above. Throughout the drawings reference numbers are re used to indicate correspondence between referenced elements. The drawings associated descriptions and specific implementation are provided to illustrate embodiments of the inventions disclosed herein and not to limit the scope of the inventions disclosed herein.

In addition methods and processes described herein are not limited to any particular sequence and the blocks or states relating thereto can be performed in other sequences that are appropriate. For example described blocks or states may be performed in an order other than that specifically disclosed or multiple blocks or states may be combined into a single block or state. Moreover the various modules of the systems described herein can be implemented as software applications modules or components on one or more computers such as servers. While the various modules are illustrated separately they may share some or all of the same underlying logic or code.

In certain embodiments computer system can include general purpose e.g. PC or Apple Mac and special purpose e.g. graphics work stations computer systems which may include one or more services databases and the like. In addition computer system can be a hand held or portable device such as a laptop personal digital system PDA cell phone smart phone or the like. More generally any processor based system may be used as computer system .

Computer system can include one or more user input devices . One user input device is shown for illustrative purposes. User input device can be any device such as a mouse keyboard or the like. User input devices receives input from one or more users and provides the input to controller . Controller can include interface logic or the like that enables user input device to communicate with central processor CPU .

CPU is included in computer system for receiving input from user input devices and for processing one or more software programs stored in memory . In addition CPU can output images and video to display through graphics system . While one CPU is illustrated computer system in certain embodiments can include by way of example one or more additional processors program logic or other substrate configurations representing data and instructions which can operate as described herein. In certain embodiments CPU can include controller circuitry processor circuitry single or multi core processors including stream processors general purpose single chip or multi chip microprocessors digital signal processors embedded microprocessors microcontrollers graphics processors and the like.

Graphics system in certain embodiments includes one or more graphics processors GPU memory controllers and the like. The GPU not shown of graphic system can have a single core or multiple cores and may have multiple stream processors in certain implementations.

Graphics system in certain embodiments includes buffers which may enhance processing efficiency of the graphics system . Buffers can increase the efficiency for example of compositing processing of graphics and or video images. In one implementation buffers facilitate increased processing efficiency by reducing CPU idle cycles which can occur if the GPU waits for compositing processing commands.

The functions and advantages of certain embodiments of graphics system are described in greater detail below with respect to .

In certain embodiments one or more clients or applications can run on computer system for example in a memory device not shown . Clients can include native clients and third party clients . Native clients can include applications that are provided by a vendor of computer system . Common examples of native clients include media players calendaring applications web browsers system utilities and the like. Third party clients can include any application developed by a third party for computer system . Some examples include games office software and the like.

Certain clients can send graphics routine calls to window server . Window server in certain embodiments is a module for managing the compositing of layers in graphical images and or video. Window server can be in one implementation the Core Animation system provided by Apple Inc. In one such embodiment the computer system can be implemented in a media device such as the Apple iPhone.

In certain embodiments window server responds to graphics routine calls from clients and by compositing graphics and or video layers corresponding to different clients an operating system user interface combinations of the same and the like. These layers may include video layers graphics layers effects layers such as transparency graphics layers and other layers. As one example of compositing window server can composite a user interface layer such as a popup message with a video game layer. Thus for example a smart phone or the like implementing system can alert a user to an incoming call by overlaying an application with a popup message. In other examples window server can composite a user interface status bar with a client window a volume control over a media application one client window over another client window and the like. Window server in certain embodiments also provides animation services to clients and may generate and or manage user interface windows.

In some currently available systems clients generate graphics commands by communicating directly with a window server. The window server in turn can use a graphics application programming interface API to generate graphics commands. As a result the window server in these currently available systems can arbitrate compositing between clients. In the system however clients can generate graphics commands both by using window server and also by directly using graphics API . Because the system enables the clients to generate graphics commands directly with the graphics API the window server may not arbitrate all graphics commands from the clients . Thus to determine which images from which clients are to be composited the clients can inform window server each time the clients generate an image or each time they begin generating graphics commands. Alternatively window server can monitor each client to determine when clients are generating graphics commands.

If window server detects or is notified that a client is generating an image window server can determine if compositing processing is to be used. Advantageously window server can generate compositing graphics commands while clients user graphics API to generate the clients graphics commands. Window server can store the compositing graphics commands for various layers in one or more compositing buffers which can be flushed when the graphics system is ready to receive the compositing commands. Further details on using window server to manage compositing processing are described below with respect to .

Graphics API can include routines for generating graphics commands for graphics system . Example graphics APIs include OpenGL OpenGL ES and DirectX and the like. By implementing certain routines from graphics API window server or certain clients can send routine calls to operating system .

Operating system may include one or more drivers not shown such as graphics drivers that translate the generated graphics commands into commands for GPU of graphics system . In addition the drivers can include display drivers for converting outputs from GPU into an image on display . Operating system can interact with central processor CPU to implement clients and certain graphics preprocessing functions. For example certain compositing preprocessing functions generated by window server can be implemented by CPU . In addition the drivers can be implemented using CPU .

Operating system in certain embodiments also provides graphics and video commands to graphic system . Graphics commands can include for example commands related to processing images using an RGB format. Video commands can include commands related to processing video streams and images using a YUV format. Other formats may be used in certain embodiments. As used herein in addition to its broad ordinary meaning the term graphics commands can refer to both graphics and video commands in certain embodiments.

The various graphics and or video commands for each frame of image data are received in graphic system by one or more command buffers . In certain embodiments command buffers include a pool of buffers stored in graphics memory or in system memory e.g. memory of . The command buffers can be configured as queues or as other data structures. Command buffers in certain embodiments store graphics and video commands until they are ready for processing by GPU .

Graphics processor GPU in various embodiments uses graphics memory to store frames of images created by executing graphics commands. In some embodiments the GPU does not have its own memory but instead shares memory with CPU . The frames can be provided to display controller which in turn provides the frame or image to display .

One or more clients generate sets of graphics commands where each set of graphics commands corresponds to an image. Clients may write these commands to command buffers asynchronously. A GPU driver described below can process sets of graphics commands in the order in which they were written to command buffers . Thus sets of commands in command buffers can be executed in a first in first out FIFO manner. In one embodiment one command buffer can be used for the each set of commands.

In addition to writing the graphics commands to a command buffer each client can inform window server of the new image corresponding to the graphics commands. Alternatively window server can detect the generation of the graphics commands. Window server can analyze the image to determine if compositing processing is to be used to combine the image with one or more other graphics and or video layers of other clients or user interface elements.

If window server determines from its analysis of an image e.g. a set of commands for an image that compositing processing is to be used window server can generate compositing graphics commands and store these commands into one or more compositing buffers . Advantageously window server can generate compositing commands while or substantially while GPU executes the commands for client stored in command buffer .

In certain embodiments GPU driver obtains the graphics commands corresponding to client from command buffer . GPU driver may include batch generator module which prepares a batch of graphics commands corresponding to the graphics commands retrieved from command buffers . Batch generator module can send a batch of graphics commands together in a single transaction to GPU . In one embodiment a batch of graphics commands can correspond to one frame of image data.

A hardware command queue may be coupled to GPU . Hardware command queue may be associated with an input output I port of GPU . Hardware command queue can receive the batch of graphics commands and provide the commands to GPU . GPU can then execute the commands. Advantageously GPU can execute the commands while window server generates compositing commands. More detailed timing of the execution of commands is illustrated with respect to below.

In certain embodiments GPU sends a notification message to a notification handler module of GPU driver . A notification message can be generated for example by an interrupt from GPU to a CPU hosting CPU driver . Notification handler module can in turn notify batch generate module that GPU is ready to receive additional commands. In response batch generate module can send additional commands to hardware command queue .

To the left of timing diagram are shown certain components from certain of the systems described above. These components include a client or application window server GPU driver GPU and display driver. To the right of each component are a series of blocks that illustrate actions that can be performed by particular components. For example to the right of the client are blocks and which include actions that can be performed by the client. Similarly blocks and to the right of the window server can be performed by the window server and so on.

Starting at an initial time at block the client assembles a set of graphics commands. The client can be any of the applications described above including native and third party applications. The set of graphics commands assembled by the client in one embodiment correspond to one frame or image. In one embodiment assembling graphics commands includes storing the commands in a command buffer such as the command buffer .

Thereafter these commands are provided from the command buffer to the GPU driver at block . At block the GPU driver processes the clients graphics commands received from the command buffer The GPU driver provides the commands to the CPU which at block processes the client commands.

The client at block after providing the commands to the GPU driver may inform the window server of the new image commands. At block in response the window server issues a swap command to the display driver. At block the display driver records the swap command. In certain embodiments the swap command can tell the display driver to combine multiple images layers but to wait to do so until the display driver receives a swap notification see blocks .

At block the window server determines if compositing processing is to be performed and assembles graphics commands related to that compositing processing. Compositing processing may be performed for example if another client exists that has written or is also writing an image on the screen. The second client might include for example user interface elements such as a popup window or the like. In certain embodiments the window server can generate graphics commands to composite multiple images or image layers with the new image submitted by the client. In assembling the graphics commands the window server may store the commands in a compositing buffer such as one of the compositing buffers described above with respect to .

In one embodiment if a 3D layer can be detached from an image frame the window server can generate a set of 2D or 3D graphics commands to assemble the rest of the layers e.g. a user interface except for the detached 3D image. If the 3D layer cannot be detached the GPU may be used to composite the 3D image with the rest of the layers instead. If the GPU is to perform some of the compositing the window server can still generate some 2D or 3D compositing commands that read from the client s image.

In one embodiment the window server appends a swap notification to the graphics command stored in the compositing buffer. The swap notification can be used to trigger the display to display the new image and any composited layers see blocks . If there are no layers to be composited with the client s new image the window server may still generate the swap notification.

Advantageously the window server in certain embodiments assembles and stores the graphics commands while the GPU is processing the client s graphics commands at block . As a result compositing commands are being generated for example by a CPU while the GPU is processing the client s graphics commands. The CPU and GPU are therefore working in parallel in certain embodiments thereby achieving greater processing efficiency than in currently available devices. This efficiency can be particularly advantageous for mobile devices such as smartphones which may have limited processing resources.

The window server provides the graphics commands to the GPU driver which processes the commands at block . The GPU driver then provides the window server s graphics commands to the GPU. At block the GPU executes the window servers graphics commands. As a result the GPU may therefore first process the client s commands and then process the window server s commands. Thereafter the GPU driver completes the window server commands at block which can include performing the swap notification. At block the display driver receives the swap notification and displays the image and any composited layers.

Returning to a previous point in time after block the client also assembles graphics commands for the next frame or image at block . The client assembles these commands by generating graphics commands and storing them in one of the command buffers as described above. Thus while the GPU is processing the client commands at block the client can be using the CPU in parallel to assemble the next frame s graphics commands at block . At block the GPU driver processes the client s next commands. Ellipsis illustrates that the process illustrated by the timing diagram can continue in a parallel fashion or pipelined fashion. Advantageously the parallel or pipelined nature of the processing shown illustrates the efficiency that can be achieved using certain embodiments of the systems described herein.

One advantage of certain embodiments of the process illustrated by the timing diagram is minimization of context switches. For example to minimize context switching in the CPU in one embodiment the contexts are cycle in order as follows the first CPU processes the client and then the window server followed by the GPU driver and ending with the display driver. The order of context switching can be varied in certain embodiments.

In addition the process of providing swap notifications can be abstracted away from the client. As a result the client can create its own graphics commands while the window server creates its own graphics commands. Additional clients can also generate graphics commands and due to the structure of the timing diagram these graphics commands can be executed in order as they are received. Thus synchronization can advantageously be performed between multiple clients without the window server arbitrating graphics requests between clients.

At block client graphics commands are received into a command buffer. The client graphics commands can correspond to an image frame of a client application which may include a native or third party application. In one embodiment the client graphics commands may be provided into the command buffer by a client application or graphics API.

At block it is determined whether multiple display layers are to be composited. This block can be performed by a window server in certain embodiments. If a single display layer is to be displayed there will be no compositing processing at block the client graphics commands are executed on a processor such as a graphics processor. Continuing at block a display controller can be caused to generate a non composited image based on the executed graphics commands. This block can be performed in one embodiment by a display controller or the like.

If multiple displays are to be composited at block compositing processing commands are generated. Advantageously these compositing processing commands can be generated by a window server while the client graphics commands are being executed on a processor such as a GPU. At block these compositing processing commands may be stored in a compositing buffer.

Once the client graphics commands have been executed by the processor the compositing commands can be executed by the processor at block . Thereafter a display controller can generate a composited image at block .

In some implementations the mobile device includes a touch sensitive display . The touch sensitive display can be implemented with liquid crystal display LCD technology light emitting polymer display LPD technology or some other display technology. The touch sensitive display can be sensitive to haptic and or tactile contact with a user.

In some implementations the touch sensitive display can include a multi touch sensitive display . A multi touch sensitive display can for example process multiple simultaneous touch points including processing data related to the pressure degree and or position of each touch point. Such processing facilitates gestures and interactions with multiple fingers chording and other interactions. Other touch sensitive display technologies can also be used e.g. a display in which contact is made using a stylus or other pointing device. Some examples of multi touch sensitive display technology are described in U.S. Pat. Nos. 6 323 846 6 570 557 6 677 932 and 6 888 536 each of which is incorporated by reference herein in its entirety.

In some implementations the mobile device can display one or more graphical user interfaces on the touch sensitive display for providing the user access to various system objects and for conveying information to the user. In some implementations the graphical user interface can include one or more display objects . In the example shown the display objects are graphic representations of system objects. Some examples of system objects include device functions applications windows files alerts events or other identifiable system objects.

In some implementations the mobile device can implement multiple device functionalities such as a telephony device as indicated by a Phone object an e mail device as indicated by the Mail object a map devices as indicated by the Maps object a Wi Fi base station device not shown and a network video transmission and display device as indicated by the Web Video object . In some implementations particular display objects e.g. the Phone object the Mail object the Maps object and the Web Video object can be displayed in a menu bar . In some implementations device functionalities can be accessed from a top level graphical user interface such as the graphical user interface illustrated in . Touching one of the objects or can for example invoke a corresponding functionality.

In some implementations the mobile device can implement a network distribution functionality. For example the functionality can enable the user to take the mobile device and provide access to its associated network while traveling. In particular the mobile device can extend Internet access e.g. Wi Fi to other wireless devices in the vicinity. For example mobile device can be configured as a base station for one or more devices. As such mobile device can grant or deny network access to other wireless devices.

In some implementations upon invocation of a device functionality the graphical user interface of the mobile device changes or is augmented or replaced with another user interface or user interface elements to facilitate user access to particular functions associated with the corresponding device functionality. For example in response to a user touching the Phone object the graphical user interface of the touch sensitive display may present display objects related to various phone functions likewise touching of the Mail object may cause the graphical user interface to present display objects related to various e mail functions touching the Maps object may cause the graphical user interface to present display objects related to various maps functions and touching the Web Video object may cause the graphical user interface to present display objects related to various web video functions.

In some implementations the top level graphical user interface environment or state of can be restored by pressing a button located near the bottom of the mobile device . In some implementations each corresponding device functionality may have corresponding home display objects displayed on the touch sensitive display and the graphical user interface environment of can be restored by pressing the home display object.

In some implementations the top level graphical user interface can include additional display objects such as a short messaging service SMS object a Calendar object a Photos object a Camera object a Calculator object a Stocks object a Address Book object a Media object a Web object a Video object a Settings object and a Notes object not shown . Touching the SMS display object can for example invoke an SMS messaging environment and supporting functionality likewise each selection of a display object and can invoke a corresponding object environment and functionality.

Additional and or different display objects can also be displayed in the graphical user interface of . For example if the device is functioning as a base station for other devices one or more connection objects may appear in the graphical user interface to indicate the connection. In some implementations the display objects can be configured by a user e.g. a user may specify which display objects are displayed and or may download additional applications or other software that provides other functionalities and corresponding display objects.

In some implementations the mobile device can include one or more input output I O devices and or sensor devices. For example a speaker and a microphone can be included to facilitate voice enabled functionalities such as phone and voice mail functions. In some implementations an up down button for volume control of the speaker and the microphone can be included. The mobile device can also include an on off button for a ring indicator of incoming phone ca Is. In some implementations a loud speaker can be included to facilitate hands free voice functionalities such as speaker phone functions. An audio jack can also be included for use of headphones and or a microphone.

In some implementations a proximity sensor can be included to facilitate the detection of the user positioning the mobile device proximate to the user s ear and in response to disengage the touch sensitive display to prevent accidental function invocations. In some implementations the touch sensitive display can be turned off to conserve additional power when the mobile device is proximate to the user s ear.

Other sensors can also be used. For example in some implementations an ambient light sensor can be utilized to facilitate adjusting the brightness of the touch sensitive display . In some implementations an accelerometer can be utilized to detect movement of the mobile device as indicated by the directional arrow . Accordingly display objects and or media can be presented according to a detected orientation e.g. portrait or landscape. In some implementations the mobile device may include circuitry and sensors for supporting a location determining capability such as that provided by the global positioning system GPS or other positioning systems e.g. systems using Wi Fi access points television signals cellular grids Uniform Resource Locators URLs . In some implementations a positioning system e.g. a GPS receiver can be integrated into the mobile device or provided as a separate device that can be coupled to the mobile device through an interface e.g. port device to provide access to location based services.

In some implementations a port device e.g. a Universal Serial Bus USB port or a docking port or some other wired port connection can be included. The port device can for example be utilized to establish a wired connection to other computing devices such as other communication devices network access devices a personal computer a printer a display screen or other processing devices capable of receiving and or transmitting data. In some implementations the port device allows the mobile device to synchronize with a host device using one or more protocols such as for example the TCP IP HTTP UDP and any other known protocol.

The mobile device can also include a camera lens and sensor . In some implementations the camera lens and sensor can be located on the back surface of the mobile device . The camera can capture still images and or video.

The mobile device can also include one or more wireless communication subsystems such as an 802.11b g communication device and or a Bluetooth communication device . Other communication protocols can also be supported including other 802.x communication protocols WiMax Wi Fi 3G code division multiple access CDMA global system for mobile communications GSM Enhanced Data GSM Environment EDGE etc.

In some implementations each of one or more system objects of device has a set of system object attributes associated with it and one of the attributes determines whether a display object for the system object will be rendered in the top level graphical user interface. This attribute can be set by the system automatically or by a user through certain programs or system functionalities as described below. shows an example of how the Notes object not shown in is added to and the Web Video object is removed from the top graphical user interface of device e.g. such as when the attributes of the Notes system object and the Web Video system object are modified .

Sensors devices and subsystems can be coupled to the peripherals interface to facilitate multiple functionalities. For example a motion sensor a light sensor and a proximity sensor can be coupled to the peripherals interface to facilitate the orientation lighting and proximity functions described with respect to . Other sensors can also be connected to the peripherals interface such as a positioning system e.g. GPS receiver a temperature sensor a biometric sensor or other sensing device to facilitate related functionalities.

A camera subsystem and an optical sensor e.g. a charged coupled device CCD or a complementary metal oxide semiconductor CMOS optical sensor can be utilized to facilitate camera functions such as recording photographs and video clips.

Communication functions can be facilitated through one or more wireless communication subsystems which can include radio frequency receivers and transmitters and or optical e.g. infrared receivers and transmitters. The specific design and implementation of the communication subsystem can depend on the communication network s over which the mobile device is intended to operate. For example a mobile device can include communication subsystems designed to operate over a GSM network a GPRS network an EDGE network a Wi Fi or WiMax network and a Bluetooth network. In particular the wireless communication subsystems may include hosting protocols such that the mobile device may be configured as a base station for other wireless devices.

An audio subsystem can be coupled to a speaker and a microphone to facilitate voice enabled functions such as voice recognition voice replication digital recording and telephony functions.

The I O subsystem can include a touch screen controller and or other input controller s . The touch screen controller can be coupled to a touch screen . The touch screen and touch screen controller can for example detect contact and movement or break thereof using any of a plurality of touch sensitivity technologies including but not limited to capacitive resistive infrared and surface acoustic wave technologies as well as other proximity sensor arrays or other elements for determining one or more points of contact with the touch screen .

The other input controller s can be coupled to other input control devices such as one or more buttons rocker switches thumb wheel infrared port USB port and or a pointer device such as a stylus. The one or more buttons not shown can include an up down button for volume control of the speaker and or the microphone .

In one implementation a pressing of the button for a first duration may disengage a lock of the touch screen and a pressing of the button for a second duration that is longer than the first duration may turn power to the mobile device on or off. The user may be able to customize a functionality of one or more of the buttons. The touch screen can for example also be used to implement virtual or soft buttons and or a keyboard.

In some implementations the mobile device can present recorded audio and or video files such as MP3 AAC and MPEG files In some implementations the mobile device can include the functionality of MP3 player such as an iPod . The mobile device may therefore include a 32 pin connector that is compatible with the iPod . Other input output and control devices can also be used.

The memory interface can be coupled to memory The memory can include high speed random access memory and or non volatile memory such as one or more magnetic disk storage devices one or more optical storage devices and or flash memory e.g. NAND NOR . The memory can store an operating system such as Darwin RTXC LINUX UNIX OS X WINDOWS or an embedded operating system such as VxWorks. The operating system may include instructions for handling basic system services and for performing hardware dependent tasks. In some implementations the operating system can be a kernel e.g. UNIX kernel .

The memory may also store communication instructions to facilitate communicating with one or more additional devices one or more computers and or one or more servers. The memory may include graphical user interface instructions to facilitate graphic user interface processing sensor processing instructions to facilitate sensor related processing and functions phone instructions to facilitate phone related processes and functions electronic messaging instructions to facilitate electronic messaging related processes and functions web browsing instructions to facilitate web browsing related processes and functions media processing instructions to facilitate media processing related processes and functions GPS Navigation instructions to facilitate GPS and navigation related processes and instructions camera instructions to facilitate camera related processes and functions and or other software instructions to facilitate other processes and functions e.g. access control management functions. The memory may also store other software instructions not shown such as web video instructions to facilitate web video related processes and functions and or web shopping instructions to facilitate web shopping related processes and functions. In some implementations the media processing instructions are divided into audio processing instructions and video processing instructions to facilitate audio processing related processes and functions and video processing related processes and functions respectively. An activation record and International Mobile Equipment Identity IMEI or similar hardware identifier can also be stored in memory .

Each of the above identified instructions and applications can correspond to a set of instructions for performing one or more functions described above. These instructions need not be implemented as separate software programs procedures or modules. The memory can include additional instructions or fewer instructions. Furthermore various functions of the mobile device may be implemented in hardware and or in software including in one or more signal processing and or application specific integrated circuits.

The disclosed and other embodiments and the functional operations described in this specification can be implemented in digital electronic circuitry or in computer software firmware or hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. The disclosed and other embodiments can be implemented as one or more computer program products i.e. one or more modules of computer program instructions encoded on a computer readable medium for execution by or to control the operation of data processing apparatus. The computer readable medium can be a machine readable storage device a machine readable storage substrate a memory device a composition of matter effecting a machine readable propagated signal or a combination of one or more them. The term data processing apparatus encompasses all apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can include in addition to hardware code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them. A propagated signal is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.

A computer program also known as a program software software application script or code can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code .

The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Computer readable media and computer readable storage media suitable for storing computer program instructions and data include all forms of non volatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user the disclosed embodiments can be implemented on a computer having a display device e.g. a CRT cathode ray tube LCD liquid crystal display monitor touch sensitive device or display for displaying information to the user and a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input.

While this specification contains many specifics these should not be construed as limitations on the scope of what is being claimed or of what may be claimed but rather as descriptions of features specific to particular embodiments. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a subcombination or variation of a subcombination.

Similarly while operations are depicted in the drawings in a particular order this should not be understand as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multitasking and parallel processing may be advantageous. Moreover the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

Each of the processes components and algorithms described above may be embodied in and fully automated by code modules executed by one or more computers or computer processors. The code modules may be stored on any type of computer readable medium or computer storage device. The processes and algorithms may also be implemented partially or wholly in application specific circuitry. The results of the disclosed processes and process steps may be stored persistently or otherwise in any type of computer storage. In one embodiment the code modules may advantageously be configured to execute on one or more processors. In addition the code modules may include but are not limited to any of the following software or hardware components such as software object oriented software components class components and task components processes methods functions attributes procedures subroutines segments of program code drivers firmware microcode circuitry data databases data structures tables arrays variables or the like.

The various features and processes described above may be used independently of one another or may be combined in various ways. All possible combinations and subcombinations are intended to fall within the scope of this disclosure. In addition certain method or process steps may be omitted in some implementations.

While certain embodiments of the inventions have been described these embodiments have been presented by way of example only and are not intended to limit the scope of the inventions. Indeed the novel methods and systems described herein may be embodied in a variety of other forms furthermore various omissions substitutions and changes in the form of the methods and systems described herein may be made without departing from the spirit of the inventions. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of the inventions disclosed herein.

