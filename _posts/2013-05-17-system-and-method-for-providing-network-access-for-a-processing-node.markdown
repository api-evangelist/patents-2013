---

title: System and method for providing network access for a processing node
abstract: A network interface controller includes a plurality of host interfaces configured to communicate with a plurality of processing nodes, a plurality of network interfaces configured to provide network communication for the processing nodes to a network, and a shared resource configured to provide link based services and stateless offload services for the processing nodes when communicating with the network.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09442876&OS=09442876&RS=09442876
owner: Dell Products, LP
number: 09442876
owner_city: Round Rock
owner_country: US
publication_date: 20130517
---
This application claims priority to U.S. Provisional Patent Application No. 61 649 064 entitled System and Method for Providing a Processing Node with Input Output Functionality Provided by an I O Complex Switch filed on May 18 2012 which is assigned to the current assignee hereof and is incorporated herein by reference in its entirety.

The present disclosure generally relates to information handling systems and more particularly relates to providing network access to a processing node.

As the value and use of information continues to increase individuals and businesses seek additional ways to process and store information. One option is an information handling system. An information handling system generally processes compiles stores or communicates information or data for business personal or other purposes. Technology and information handling needs and requirements can vary between different applications. Thus information handling systems can also vary regarding what information is handled how the information is handled how much information is processed stored or communicated and how quickly and efficiently the information can be processed stored or communicated. The variations in information handling systems allow information handling systems to be general or configured for a specific user or specific use such as financial transaction processing airline reservations enterprise data storage or global communications. In addition information handling systems can include a variety of hardware and software resources that can be configured to process store and communicate information and can include one or more computer systems graphics interface systems data storage systems and networking systems. Information handling systems can also implement various virtualized architectures.

The following description in combination with the Figures is provided to assist in understanding the teachings disclosed herein. The description is focused on specific implementations and embodiments of the teachings and is provided to assist in describing the teachings. This focus should not be interpreted as a limitation on the scope or applicability of the teachings.

In a particular embodiment processing system includes an input output input output complex switch and processing nodes and represents a highly scalable networked data processing system. For example processing system can include a rack mounted server system where input output complex switch represents a rack mounted switch and processing nodes represent one or more rack or chassis mounted servers blades processing nodes or a combination thereof. Input output complex switch includes a management controller an input output complex application specific integrated circuit ASIC a network interface ASIC a switch ASIC and a remote node component RNC controller . Input output complex ASIC includes a multi function Peripheral Component Interconnect Express PCIe module one or more additional multi function PCIe modules a vendor defined messaging VDM block a rack level remote direct memory access RRDMA block a serial attach small computer system interface SAS block and an RNC block . Multi function PCIe module includes a PCIe to PCIe P2P bridge endpoint a VDM endpoint an RRDMA endpoint an SAS endpoint and an RNC endpoint . Similarly multi function PCIe module includes a P2P bridge endpoint a VDM endpoint an RRDMA endpoint an SAS endpoint and an RNC endpoint .

Multi function PCIe module is connected to processing node via a PCIe link. For example multi function PCIe module can be connected to processing node via a x1 PCIe link a x2 PCIe link a x4 PCIe link a x8 PCIe link or a x16 PCIe link as needed or desired. Further multi function PCIe module can be connected to processing node via a backplane of a chassis that includes input output complex switch and processing nodes the multi function PCIe module can be connected to the processing node via an external PCIe cable or the multi function PCIe module can be connected to the processing node via a PCIe connector on either input output complex switch the processing node another board that connects the multi function PCIe module to the processing node or a combination thereof. Multi function PCIe module operates as a PCIe endpoint associated with processing node . As such multi function PCIe module is enumerated in the PCIe configuration space of processing node as being associated with a particular PCIe link number and a designated device number on the PCIe link. Further multi function PCIe module is enumerated in the PCIe configuration space as being associated with a particular function number of the device. For example multi function PCIe module can be identified as function 0. Multi function PCIe module includes a set of PCIe endpoint status and control registers that permit processing node to send data to to receive data from and to otherwise control the operation of the multi function PCIe module.

Multi function PCIe module is similar to multi function PCIe module and is connected to processing node via a PCIe link such as a x1 PCIe link a x2 PCIe link a x4 PCIe link a x8 PCIe link or a x16 PCIe link. Multi function PCIe module can be connected to processing node via a backplane an external PCIe cable or a PCIe connector and can be connected in the same way that multi function PCIe module is connected to processing node or can be connected differently. Multi function PCIe module operates as a PCIe endpoint associated with processing node and is enumerated in the PCIe configuration space of the processing node as being associated with a particular PCIe link number and a designated device number on the PCIe link. Further multi function PCIe module is enumerated in the PCIe configuration space as being associated with a particular function number of the device and includes a set of PCIe endpoint status and control registers that permit processing node to send data to to receive data from and to otherwise control the operation of the multi function PCIe module. Input output complex ASIC can include one or more additional multi function PCIe modules that are similar to multi function PCIe modules and and that are connected to one or more additional processing nodes such to processing nodes and . For example input output complex ASIC can include up to 16 multi function PCIe modules similar to multi function PCIe modules and that can be coupled to up to 16 processing nodes similar to processing nodes . In this example network interface ASIC can include 16 network interface ports. In another example input output complex ASIC can include more or less than 16 multi function PCIe modules and network interface ASIC can include more or less than 16 network interface ports. In another embodiment input output complex switch can include two or more input output complex ASICs similar to input output complex ASIC . For example input output complex switch can include four input output complex ASICs such that up to 64 processing nodes can be coupled to the input output switch complex. In this example network interface ASIC can include 64 network interface ports and each input output complex ASIC can be connected to 16 of the network interface ports.

Multi function PCIe modules and operate as multi function PCIe devices in accordance with the 3.0 . As such multi function PCIe module includes P2P endpoint VDM endpoint RRDMA endpoint SAS endpoint and RNC endpoint that each operate as PCIe endpoints associated with processing node and are enumerated in the PCIe configuration space of the processing node as being associated with the same PCIe link number and designated device number as multi function PCIe module but with different function numbers. For example P2P endpoint can be identified as function 1 VDM endpoint can be identified as function 2 RRDMA endpoint can be identified as function 3 SAS endpoint can be identified as function 4 and RNC endpoint can be identified as function 5. Similarly multi function PCIe module includes P2P endpoint VDM endpoint RRDMA endpoint SAS endpoint and RNC endpoint that each operate as PCIe endpoints associated with processing node and are enumerated in the PCIe configuration space of the processing node as being associated with the same PCIe link number and designated device number as multi function PCIe module but with different function numbers. For example P2P endpoint can be identified as function 1 VDM endpoint can be identified as function 2 RRDMA endpoint can be identified as function 3 SAS endpoint can be identified as function 4 and RNC endpoint can be identified as function 5. Each endpoint and includes a set of PCIe endpoint status and control registers that permit the respective processing nodes and to send data to to receive data from and to otherwise control the operation of the endpoints.

Northbridge operates as a PCIe root complex and includes multiple PCIe interfaces including a Non Volatile Memory Express NVMe interface and one or more PCIe interfaces that are provided to PCIe connectors and to PCIe slots . For example NVMe interface and PCIe interfaces can represent x1 PCIe links x2 PCIe links x4 PCIe links x8 PCIe links or x16 PCIe links as needed or desired. NVMe interface connects the northbridge to SSD and operates in conformance with the Non Volatile Memory Host Controller Interface NVMHCI Specification. PCIe connectors can be utilized to connect processing node to one or more input output complex switches such as input output switch complex . PCIe slot provides processing node with flexibility to include various types of expansion cards as needed or desired.

Northbridge includes error handling and containment logic . Error handling and containment logic executes error handling routines that describe the results of input output transactions issued on NVMe interface and PCIe interfaces . Error handling and containment logic includes status and control registers. The status registers include indications related to read transaction completion and indications related to write transaction completion. The error handling routines provide for input output errors to be handled within northbridge without stalling processor or crashing an operating system OS or virtual machine manager VMM operating on processing node .

Read completion status error routines return information about the status of read transactions. If an error results from a read transaction the routine indicates the type of error the cause of the error or both. For example a read transaction error can include a timeout error a target abort error a link down error another type of read transaction error or a combination thereof. The read completion status error routines also provide the address associated with the read transaction that produced the error. If a read transaction proceeds normally the read completion status routines return information indicating that the read transaction was successful and provide the address associated with the read transaction.

Write completion status error routines return information about the status of write transactions. If an error results from a write transaction the routine indicates the type of error the cause of the error or both. For example a write transaction error can include a timeout error a target abort error a link down error another type of write transaction error or a combination thereof. The write completion status error routines also provide the address associated with the write transaction that produced the error. If a write transaction proceeds normally the write completion status routines return information indicating that the write transaction was successful and provide the address associated with the write transaction.

The control registers operate to enable the functionality of the error handling routines including enabling read error handling and write error handling and enabling system interrupts to be generated in response to read errors and write errors. Device drivers associated with the transactions handled by northbridge utilize the error handling routines to capture the failed transactions to interrupt the device driver and to prevent the user program from consuming faulty data. In a particular embodiment the device drivers check for errors in the transactions by calling the appropriate error handling routine or reading the appropriate status register. In another embodiment the device drivers enable interrupts to handle errors generated by the transactions. For example if an error occurs in a read transaction a device driver can retry the read transaction on the same link or on a redundant link can inform the OS or application that a read error occurred before the OS or application consume the faulty data or a combination thereof. Similarly if an error occurs in a write transaction a device driver can retry the write transaction on the same link or on a redundant link can inform the OS or application that a write error occurred or a combination thereof.

Northbridge is connected to southbridge via a chipset interface . In the embodiment where processor represents an Intel processor and northbridge represents a compatible chipset northbridge southbridge represents a compatible southbridge such as an Intel input output controller hub ICH and chipset interface represents a Direct Media Interface DMI . In the embodiment where processor represents an AMD APU and northbridge represents a compatible chipset northbridge southbridge represents a compatible southbridge such as an AMD SB950 and chipset interface represents an A Link Express interface. uBMC is connected to southbridge via a southbridge interface . In a particular embodiment uBMC is connected to southbridge via a low pin count LPC bus an inter integrated circuit I2C bus or another southbridge interface as needed or desired. uBMC operates to provide an interface between a management controller such as management controller and various components of processing node to provide out of band server management for the processing node. For example uBMC can be connected to a power supply one or more thermal sensors one or more voltage sensors a hardware monitor main memory northbridge southbridge another component of processing node or a combination thereof. As such uBMC can represent an integrated Dell Remote Access Controller iDRAC an embedded BMC or another out of band management controller as needed or desired.

Processing node operates to provide an environment for running applications. In a particular embodiment processing node runs an operating system OS that establishes a dedicated environment for running the applications. For example processing node can run a Microsoft Windows Server OS a Linux OS a Novell OS or another OS as needed or desired. In another embodiment processing node runs a virtual machine manager VMM also called a hypervisor that permits the processing node to establish more than one environment for running different applications. For example processing node can run a Microsoft Hyper V hypervisor a VMware ESX ESXi virtual machine manager a Citrix XenServer virtual machine monitor or another virtual machine manager or hypervisor as needed or desired. When operating in either a dedicated environment or a virtual machine environment processing node can store the OS software or the VMM software in main memory or in SSD or the software can be stored remotely and the processing node can retrieve the software via one or more of PCIe links . Further in either the dedicated environment or the virtual machine environment the respective OS or VMM includes device drivers that permit the OS or VMM to interact with PCIe devices such as multi function PCIe module P2P endpoint VDM endpoint RRDMA endpoint SAS endpoint and RNC endpoint . In this way the resources associated with input output complex switch are available to the OS or VMM and to the applications or OS s that are operating thereon.

Note that the embodiments of processing node described herein are intended to be illustrative examples of processing nodes and are not intended to be limiting. As such the skilled artisan will recognize that the described embodiments are representative of a wide variety of available processing node architectures and that any other such processing node architectures are similarly envisioned herein. Moreover the skilled artisan will recognize that processing node architectures are rapidly changing and that future processing node architectures are likewise envisioned herein.

Returning to input output switch complex provides much of the functionality normally associated with a server processing node. For example through associated P2P endpoints and processing nodes and access the functionality of a network interface cards NICs in network interface that are connected to the P2P endpoints thereby mitigating the need for separate NICs within each processing node. Similarly through VDM endpoints and management controller accesses uBMCs similar to uBMC on processing nodes and in order to provide managed server functionality on the processing nodes without separate management interfaces on each processing node. Further by accessing SAS endpoints and processing nodes and have access to a large fast storage capacity that can replace and can be more flexible than individual disk drives or drive arrays associated with each processing node.

Moreover input output complex switch can include components that are needed by each processing node but that are not often used. In a particular embodiment RNC controller includes a serial peripheral interface SPI connected to a non volatile random access memory NVRAM a real time clock a video interface a keyboard mouse interface and a data logging port. The NVRAM provides a common repository for a wide variety of basic input output systems BIOSs or extensible firmware interfaces EFIs that are matched to the variety of processing node architectures represented the different processing nodes . By accessing RNC endpoints and at boot processing nodes and access the NVRAM to receive the associated BIOS or EFI receive real time clock information receive system clock information and provide boot logging information to the data logging port thereby mitigating the need for separate NVRAMs real time clocks and associated batteries and data logging ports on each processing node. Further a support technician can provide keyboard video and mouse functionality through a single interface in input output complex switch and access processing nodes and through RNC endpoints and without separate interfaces on the processing nodes.

Further input output complex switch provides enhanced functionality. In particular input output complex switch provides consolidated server management for processing nodes through management controller . Also the NVRAM provides a single location to manage BIOSs and EFIs for a wide variety of processing nodes and the common real time clock ensures that all processing nodes are maintaining a consistent time base. Moreover RRDMA endpoints and provide improved data sharing capabilities between processing nodes that are connected to a common input output complex ASIC . For example RRDMA endpoints and can implement a message passing interface MPI that permits associated processing nodes and to more directly share data without having to incur the overhead of layer 2 layer 3 switching involved in sharing data through switch ASIC . Note that the functionality described above is available via the PCIe link between processing nodes and and the associated multi function PCIe modules and thereby providing further consolidation of interfaces needed by the processing nodes to perform the described functions. Further the solution is scalable in that if the bandwidth of the PCIe links become constrained the number of lanes per link can be increased to accommodate the increased data loads without otherwise significantly changing the architecture of processing nodes and or of input output complex ASIC .

Further note that in consequence of input output switch complex providing the functionality normally associated with a processing node when connected to the input output complex switch processing nodes are maintained as stateless or nearly stateless processing nodes. Thus in a particular embodiment processing nodes can lose all context and state information when the processing nodes are powered off and any context and state information that is needed upon boot is supplied by input output switch complex . For example processing node does not need to maintain a non volatile image of a system BIOS or EFI because RNC controller supplies the processing node with the BIOS or EFI via RNC endpoint . Similarly any firmware that may be needed by processing node can be supplied by RNC controller .

Network interface ASIC can include a plurality of host interfaces a plurality of upstream network interfaces and a shared resource . Host interfaces can be configured to communicate with processing nodes such as processing node . In various embodiments host interfaces can be implemented as PCIe interfaces.

Upstream network interfaces can include a MAC Media Access Control layer and a physical layer . Upstream network interface can be configured to communicate with upstream network elements such as switch ASIC . In various embodiments upstream network interfaces can be implemented as Ethernet interfaces such as 100BASE TX 1000BASE T 10 GBASE R or the like.

Shared resource can include buffers and queues block non volatile storage link based services stateless offload services volatile storage and management block . Buffers and queues block can be configured to provide a unified pool of resources to implement multiple buffers and queues for handling the flow of traffic among processing nodes and upstream network elements. These can include transmit and receive buffers for each instance of a network interface. In various embodiments buffers and queues block can further implement priority queues for network traffic for network interface instances. In various embodiments the unified pool of resources can be dynamically allocated between network interface instances either during instantiation of the network interface instances or while operating such as based on network resource usage.

Link based services can be configured to provide a unified mechanism for providing link based services such as bandwidth policing prioritization and flow control for the network interface instances. For example link based services can implement priority flow control mechanisms such as using IEEE Std. 802.3x to provide flow control for a connection or using IEEE Std. 802.1 Qbb to provide priority based flow control such as for a class of service. In another example link based services can be configured to provide congestion management for example using congestion notification such as IEEE Std. 802.1Qau or other mechanisms to manage congestion among processing nodes and between processing nodes and upstream network elements. In another example link based services can provide traffic prioritization such as by implementing prioritization mechanism such as enhanced transmission selection such as IEEE Std. 802.1Qaz or other mechanisms.

Stateless offload services can be configured to provide a unified mechanism for providing hardware or software communication assist service that have no time contest and that are agnostic to the operation of higher protocol entities. As such stateless offload services can include TCP segmentation offload IP checksum offload Receive Side Scaling RSS i.e. spreading of interrupts to different processing nodes Large Send Offload LSO i.e. packaging TCP packets into larger buffers before transmitting and the like for the network interface instances. In a particular embodiment shared resources includes stateful offload services for providing hardware of software assist services that have a time context in relation to the state of the protocol stack and are integral to the operation of higher layer protocol software entities. For example the stateful offload services can includes TCP IP offload Internet SCSI iSCSI protocol offload Fibre Channle over Ethernet FCoE protocol offload Infiniband protocol offload and the like for the network interface instances.

Non volatile storage and volatile storage can be configured to provide common pools of resources across the network interface instances. For example non volatile storage can be configured to store a firmware that is common to a plurality of network interface instances rather than storing an individual firmware for each instance. Similarly volatile storage can be configured to store information related to network destinations such as a unified address resolution protocol ARP table neighbor discover protocol NDP table or a unified routing table that can be accessed by a plurality of network interface instances. In various embodiments non volatile storage and volatile storage may store information that is unique to a network interface instance that may not be accessed by other network instances. Examples may include specific configuration information encryption keys or the like.

Management block can provide unified management of shared resources for the network interface instances. Management block can be configured to provide set up and tear down services for a network interface instance such that when a processing node needs to establish a network interface the management block can direct the configuration of resources needed to establish the network interface instance or when the instance is no longer needed the management block can direct the freeing of the resources.

In a particular embodiment network interface ASIC supports the adaptation of an Open Fabrics alliance Enterprise Distribution OFED verbs Application Programming Interface API to a simple frame based physical layer and data link layer such as Raw or simply framed Ethernet and PCIe Transaction Layer Packet Data Link Packet TLP DLP interconnects are supported.

At creation of a network interface instance can be attempted. If a network interface instance is unable to be created then an error can be reported as indicated at .

Alternatively when a network interface instance can be created MAC layer services a physical layer services and port level services can be established as indicated at . At a check for an error when establishing the MAC layer physical layer and port level services can be performed. When an error is detected the error can be reported as indicated at .

Alternatively when establishment of the MAC layer physical layer and port level services is successful at a determination can be made as to the need for link based services such as bandwidth policing congestion control and the like. When link layer services are required the link layer services can be established at and an error check on the link layer services can be performed at . When there is an error with establishing link layer services the error can be reported at .

Alternatively from when link layer services are not needed or from when the link layer services are established without an error a determination can be made at as to the need for stateless offload services such as checksum and TCP segmentation offload. When the stateless offload services are required the stateless offload services can be established at and an error check on the stateless offload services can be performed at . When there is an error with establishing stateless offload services the error can be reported at .

Alternatively from when stateless offload services are not needed or from when the stateless offload services are established without an error a determination can be made at as to the need for management services. When the management services are required the management services can be established at and an error check on the management services can be performed at . When there is an error with establishing management services the error can be reported at .

Alternatively from when management services are not needed or from when the management services are established without an error the network interface can be registered at .

Further requests for MAC layer services including requests for link based services such as bandwidth policing congestion notification flow control quality of service prioritization and the like can be sent to the MAC layer services . Additionally a request for an MTU maximum transmission unit can be sent to MTU selection . MTU Selection can determine an MTU for the connection and provide MTU to the MAC layer services .

MAC layer services can break out the requests for various link based services and send the requests link based services . For example requests for flow control such as IEEE Std. 802.3x can be sent to the RX queue to enable flow control for the connection. Requests for priority flow control such as IEEE Std. 802.1Qbb can be sent to the RX priority queues to create priority receive queues for handling traffic of different classes and to enable flow control independently for the classes. Requests for bandwidth policing can be sent to the policers to allocate bandwidth to different classes of traffic. As each of the subrequests is handled information can be aggregated at and passed to the stateless offload services block.

Alternatively when TCP segmentation offload is needed TCP segments from a TCP session can be accumulated into a TCP max segment before sending as indicated at . At the onset of accumulation a TCP session keyed buffer can be allocated at for storing the TCP segments until the TCP max segment can be sent such as until sufficient number of segments have been accumulated for generating the TCP max segment.

In various embodiments the Network Interface ASIC can provide out of band communication between nodes. is a block diagram illustrating out of band communication between two processing nodes. Block diagram can include network interface instance network interface instance buffer manager and switch . Network interface instance can include transmit buffer and receive buffer and network interface instance can include transmit buffer and receive buffer . Additionally network interface instance can communicate with a first processing node via D in and network interface instance can communicate with a second processing node via D out .

Buffer manager can monitor traffic received on D in . Traffic directed to upstream network elements such as other computers on the Internet can be placed into the transmit buffer and passed to switch . Alternatively traffic intended for the second processing node can bypass switch and can be placed directly into receive buffer of network interface instance establishing an out of band path for the traffic.

In various embodiments the out of band path can be implemented by providing dedicated receive buffers within each network interface instance for the each of the other network interface instances. Alternatively the out of band path can be implemented with fewer dedicated receive buffers such as by allowing out of band data from multiple other processing nodes to be writing to one receive buffer within a network interface instance.

In various embodiments an out of band communication link can also be established by providing direct memory access over a PCIe path from the first node to the Network Interface ASIC to the second node. Specifically when the out of band path is created within the Network Interface ASIC data may be passed directly to the memory on the second node without needing to place it into the receive buffer .

In various embodiments high priority internode communication can be improved by avoiding congestion within a converged network. Using embodiments described herein node to node connections can be established at various network levels depending on the type of traffic availability of connection types and the like. is an exemplary flow diagram illustrating internode traffic routing.

At internode traffic communication between two nodes can be initiated. In various embodiments the internode traffic can be high priority high bandwidth traffic such as a transfer of large data or a virtual machine from one processing node to another. Due to the size and priority of the traffic it may be advantageous to minimize the impact of network congestion during the transfer of the data.

At it can be determined if the traffic is suitable for communication using RRDMA. In various embodiments RRDMA may provide a suitable interface when the software needing to transfer the data is RRDMA aware and when the processing nodes are connected to a common input output Complex ASIC. When RRDMA is suitable for the internode communication a link can be established between the RRDMA instances for the two processing nodes within the input output Complex ASIC as indicated at .

At it can be determined if the traffic is suitable for communication using an out of band link. In various embodiments an out of band link may provide a suitable path when the processing nodes share a common network interface ASIC. When the out of band link is suitable for the internode communication a link can be established between the network interface instances within the network interface ASIC as indicated at . In various embodiments the out of band link can be configured to pass communication from a first node directly into the receive buffer of the network interface instance for a second node thereby bypassing the transmit buffer the upstream network interface and any upstream switching architecture. Further depending on the priority of the traffic congestion control mechanisms can be employed to pause or slow communication from other processing nodes or upstream network elements that may otherwise enter the receive queue of the second processing node thereby maximizing the bandwidth available for the internode communication.

At when a direct NIC to NIC link is not appropriate communication can occur along with regular network traffic by being passed from the first processing node up to the switch and then back down to the second processing node. In general using this path may have a higher latency and lower bandwidth than either the RRDMA link or the NIC NIC link as the switch processing overhead and congestion caused by other network traffic passing through the switch may slow the data transfer.

In various embodiments the Network Interface ASIC can provide simplified congestion management for the processing nodes. For example congestion management can require each node in a communication path to share information such as buffer states to ensure that one node is not overrun with data. Specifically when a node s buffer is near capacity the node can notify other nodes in the path to pause or delay sending additional data until buffer space can be freed. The Network Interface ASIC can be aware of the buffer state for the buffers of the network interface instances without the need for additional information passing. Thus when a network interface instance is near overflow the network interface ASIC can pause or slow data flow from other network interface instances to the instance that is near overflow until the condition is passed.

In various embodiments congestion management can be implemented by deferring data flow from the processing node to the network interface ASIC until resources such as buffer space are allocated and reserved for receiving the data. The resources for receiving the data can be for example available space in a transmit queue at an outbound port or for out of band communication reserved memory space at a destination computing node. Once the destination resources are available the data can be pulled from the source node and passed to the destination resource without the need for buffering within the network interface ASIC while the resources are made available. Advantageously this can allow out of order transmission of data from the source node as data for a destination where the resources that are already available can be sent while data that is waiting for destination resources to be made available can be delayed. This can prevent transmission of data from the source node to the network interface ASIC from being delayed due to a buffer that is filled with data awaiting destination resources.

In various embodiments flow control can be provided for the out of band communication between two processing nodes by implementing shared directional queues between network interface instances within the network interface ASIC. is a diagram illustrating the use of shared cues for flow control in a network interface ASIC. Communication between network interface instance and network interface instance can proceed via queue and queue .

Queue can include a plurality of empty or processed entries and a plurality of to be processed entries . When network interface instance is ready to send data to network interface instance network interface instance can add entries to queue . When the number of empty or processed slots falls below a threshold network interface instance can wait to add entries to queue until more empty or processed slots are available. In various embodiments network interface instance can determine an amount of time to wait based on a queue quanta and a separation delta. The separation delta may be a minimum number of to be processed entries that are maintained within the queue. When network interface instance is ready to receive data from network interface instance network interface instance can process or remove entries from queue . When the number of to be processed entries falls below a separation delta network interface instance can wait to process entries from queue until more to be processed entries are available.

Similarly queue can include a plurality of empty or processed slots and a plurality of to be processed entries . When network interface instance is ready to send data to network interface instance network interface instance can add entries to queue . When the number of empty or processed slots falls below a threshold network interface instance can wait to add entries to queue until more empty or processed slots are available. In various embodiments network interface instance can determine an amount of time to wait based on a queue quanta and a separation delta. When network interface instance is ready to receive data from network interface instance network interface instance can process or remove entries from queue . When the number of to be processed entries falls below a separation threshold network interface instance can wait to process entries from queue until more to be processed entries are available.

Maintaining a threshold number of available slots within the queue ensures that network interface instance does not send data faster than network interface instance can process. Additionally maintaining a separation delta within the queue ensures that network interface instance does not over run the filled slots and attempt to process unused slots . Thus circular queue can provide flow control without requiring a pause instruction to be sent from network interface instance to network interface instance in order to prevent loss of data due to a buffer overflow.

Returning to VDM block operates to provide a single interface for management controller to access VDM endpoints and and one or more additional VDM endpoints associated with the one or more additional multi function PCIe modules. As such VDM endpoints and are connected to VDM block and the VDM block is connected to management controller . In a particular embodiment VDM endpoints and each have a dedicated connection to VDM block . In another embodiment VDM endpoints and share a common bus connection to VDM block . In either embodiment VDM block operates to receive management transactions from management controller that are targeted to one or more of processing nodes and to forward the management transactions to the associated VDM endpoint or targeted processing node. For example a technician may wish to determine an operating state of processing node and can send a vendor defined message over the PCIe link between the processing node and VDM endpoint and that is targeted to a uBMC on the processing node that is similar to uBMC . The uBMC can obtain the operating information from processing node and send a vendor defined message that includes the operating information to VDM endpoint . When VDM block receives the operating information from VDM endpoint the VDM block forwards the operating information to management controller for use by the technician. The technician may similarly send vendor defined messages to the uBMC to change an operating state of processing node .

In a particular embodiment the uBMC on one or more of processing nodes represents a full function BMC such as a Dell DRAC an Intel Active Management Technology controller or another BMC that operates to provide platform management features including environmental control functions such as system fan temperature power and voltage control and the like and higher level functions such as platform deployment asset management configuration management platform BIOS EFI and firmware update functions and the like. In another embodiment the uBMC on one or more of processing nodes represent a reduced function BMC that operates to provide the environmental control functions while the higher level functions are performed via RNC controller as described below. In yet another embodiment one or more of processing nodes do not include a uBMC but the environmental control functions are controlled via a northbridge such as northbridge that is configured to handle platform environmental control functions.

RRDMA block provides MPI messaging between processing nodes via RRDMA endpoints and and one or more additional RRDMA endpoints associated with the one or more additional multi function PCIe modules. As such RRDMA endpoints and are connected to RRDMA block via a dedicated connection to the RRDMA block or via a common bus connection to the RRDMA block. In operation when a processing node such as processing node needs to send data to another processing node an RRDMA device driver determines if the other processing node is connected to input output complex ASIC or is otherwise accessible through layer2 layer3 switching. If the other processing node is accessible through layer2 layer3 switching then the RRDMA driver encapsulates the data into transmission control protocol Internet protocol TCP IP packets that include the target processing node as the destination address. The RRDMA driver then directs the packets to P2P endpoint for routing through the associated NIC in network interface ASIC based upon the destination address.

If however the other processing node is connected to input output complex ASIC such as processing node then the RRDMA driver encapsulates the data as an MPI message that is targeted to processing node . The RRDMA driver then issues an MPI message to RRDMA endpoint to ring a doorbell associated with processing node . The MPI message is received from RRDMA endpoint by RRDMA block which determines that processing node is the target and issues the message to RRDMA endpoint . An RRDMA driver in processing node determines when the processing node is ready to receive the data and issues an MPI reply to RRDMA endpoint . The MPI reply is received from RRDMA endpoint by RRDMA block which issues the message to RRDMA endpoint . The RRDMA driver in processing node then sends the data via RRDMA block to processing node . In a particular embodiment the MPI messaging between processing nodes utilize InfiniBand communications. In another embodiment the RRDMA drivers in processing nodes utilize a small computer system interface SCSI RDMA protocol.

Note that utilizing RRDMA block for MPI data transfers provides a more direct path for data transfers between processing nodes than is utilized in layer2 layer 3 data transfers. In addition because processing nodes are closely connected to input output complex switch MPI data transfers can be more secure than layer2 layer3 data transfers. Moreover because the data is not encapsulated into TCP IP packets MPI data transfers through RRDMA block do not incur the added processing needed to encapsulate the data and the data transfers are less susceptible to fragmentation and segmentation than would be the case for layer 2 layer 3 data transfers.

SAS block operates to provide processing nodes with access to a large fast and flexible storage capacity via SAS endpoints and and one or more additional SAS endpoints associated with the one or more additional multi function PCIe modules. As such SAS endpoints and are connected to SAS block via a dedicated connection to the SAS block or via a common bus connection to the SAS block. In operation when a processing node such as processing node needs to store or retrieve data an SAS device driver in the processing node issues the appropriate SCSI transactions to SAS endpoint and the SAS endpoint forwards the SCSI transactions to SAS block . SAS block is connected via a SAS connection to a storage device and issues the SCSI transactions from SAS endpoint to the attached storage device. In a particular embodiment the storage device includes one or more disk drives arrays of disk drives other storage devices or a combination thereof. For example the storage device can include virtual drives and partitions that are each allocated to one or more processing node . In another embodiment SAS block operates to dynamically allocate the storage resources of the storage device based upon the actual or expected usage of processing nodes . In yet another embodiment SAS block operates as a redundant array of independent drives RAID controller.

RNC controller contains BIOS code lookup module flash images and debug port . RNC controller may correspond to RNC controller of and may be a component of an input output complex switch such as input output complex switch of . Lookup and flash images may correspond to the serial peripheral interface portion of RNC controller and debug port may correspond to the port portion of RNC controller .

Processing nodes and are connected to RNC controller by PCIe link . Only a portion of the complete path from the processing nodes and to RNC controller is shown in . A more complete path may correspond to the path from the processing nodes to RNC controller of . The more complete path may travel from the processing nodes to a multi function PCIe module an RNC endpoint an RNC block and finally to an RNC controller such as RNC controller in the manner described in .

BIOS code lookup module may be adapted to look up the location of the correct boot image of processing nodes and . The boot images may be indexed by type of hardware version of hardware type of operating system and version of operating system or by other characteristics of processing nodes and . In some embodiments correct boot images may be made available to BIOS code lookup module by IT management . The boot images may be contained on flash images . In other embodiments the boot images may be stored outside of RNC controller such as on an input output complex switch or on non volatile memory accessible through RNC controller such as from image library .

In the processing nodes and of processing system may boot over PCIe link from boot code stored in flash images on RNC controller . As part of boot a CPU of one of processing nodes and may initiate PCIe link . The CPU may enumerate the multifunction MF PCIe endpoints such as MF endpoints in and locate RNC controller . Once PCIe link is initiated the CPU may route its reset vector over PCIe link to RNC controller .

The reset vector is the first segment of code the CPU is instructed to run upon boot. The CPU may obtain the code over PCIe link by sending a request to fetch that code reset vector fetch over PCIe link . In some embodiments the CPU would embed an identifier in the PCIe packet sent over PCIe link to fetch the code. The identifier may describe the device ID of the CPU or node the hardware revision information about software such as an operating system running on the node and other information about the node. The MF PCIe would recognize the packet as a reset vector fetch and pass it on to the RNC block of the ASIC. That block may then send a packet to RNC controller . The RNC controller in turn would recognize the packet parse the identification information and perform a look up based on the device ID hardware revision and other information to obtain a location in the flash contained on RNC controller from which to read the boot instructions. The RNC controller would then map the read instructions to that location. If the primary RNC controller is not available over a primary PCIe link the PCIe complex in the CPU would route the reset vector over the secondary PCIe link to the secondary RNC controller thus providing a redundant link path for the reset vector fetch.

In some embodiments if the search through the lookup table did not produce a suitable boot image for the particular device and hardware version then RNC controller would search for a boot image in other locations. In further embodiments RNC controller might search for a suitable boot image in an internal location maintained by IT management. If that search also proved unsuccessful RNC controller might support a phone home capability. With that capability RNC controller could automatically download the up to date image from a download server by sending it a download request. RNC controller might lack current images if a new server was introduced into a server rack or a server underwent a hardware revision. In order to prevent a failure during an attempted boot RNC controller may insert no operation commands NOPs into the code provided as a result of the reset vector fetch as needed until the proper boot image was located on another RNC controller or phoning home obtained the correct image. Execution of a NOP generally has little or no effect other than consuming time. By inserting NOPs at the beginning of the code the server was to execute at the beginning of boot the server would be kept inactive until the proper code could be located. Then that code could be sent to the CPU for execution.

In further embodiments the functionality as described in may ensure that servers and other processing nodes boot off the correct images and may simplify updating firmware. The lookup feature based on device identification and hardware version may enable the IT department to monitor entries in a lookup table or other data structure to control the boot image used by each configuration of server. Management tools may allow the IT department to specify which image any server should boot from allowing IT to manage by server which version of flash each server should boot from. Further having a uniform storage for boot images may simplify updating them. Management tools may enable the IT department to update the boot images used by multiple servers on a rack by updating one flash image on RNC controller thus greatly simplifying updates in comparison to updating the firmware in each of the servers. Moreover the configuration makes it simpler to determine the need for updating boot images. For example the IT department may configure the system to monitor updates sites for firmware images and download the latest version to ensure that the latest version is always available. In particular a system might monitor Dell.com to ensure the latest flash revision for Dell servers is always available. Additionally further embodiments may provide a phone home capability to provide a uniform mechanism for updating firmware.

In other embodiments a CPU vendor may not support mapping the reset vector out via PCIe link to a RNC controller. In those embodiments a server may encompass a flash image that contained the minimal amount of code to get the CPU up and running to train the PCIe link and to start fetching code from an RNC controller. In this case the RNC controller may service the request for boot code using device emulation.

In these embodiments the minimal boot code may have the same capabilities as in the embodiments above of using a primary and secondary PCIe link based on availability along with image location service and phone home service. In a few embodiments some of processing nodes and may be able to boot from a Northbridge that has memory attached rather than from non volatile storage attached to a Southbridge. These embodiments may provide for non volatile memory express communications combined with PCIe link communications to enable solid state drive communications between a CPU and non volatile memory at boot time. In these embodiments the minimal boot image could be placed in a solid state drive connected to the Northbridge.

Debug port of RNC controller is a port to capture debug information logged during the boot process. These captures may receive debug information during boot from processing nodes and of processing system and write it to debug port storage . Debug port may consist of non volatile memory accessible through the PCIe bus and mapped in PCIe bus memory space. Debug port storage may provides a log of debug information during boot. The information may include for each node of processing system an identification of the node checkpoint information and error information. In the illustration of debug port storage contains data structures and with boot process information from devices and M respectively. The entries illustrated in data structure contain checkpoint information. The entries illustrated in data structure contain both checkpoint information and error information. IT alert module may monitor the debug information passing through the 1 debug port and debug port storage check for error messages and generate alerts if errors are found. In a particular embodiment IT alert module is connect to a data center administration console via a standard Ethernet mechanism and the IT alert module provides updates via an IT console dash board mobile text alerts email alert or error states indicators or LCD panel on I O complex switch .

In the embodiment of debug port storage organizes the information by device. The information for device id 1 and the information for device id M are each kept in a separate portion of storage. In further embodiments the identification of a device may be listed only once for the section of data pertaining to the device. In other embodiments the file may be in chronological order. Each entry may include identification information for the device reporting the information. In a few embodiments debug port may convert the boot debug information to a uniform format. It may for example use a uniform code to report errors. They may also use a uniform description of checkpoints passed. In other embodiments the nature of the boot debug information may differ from device to device.

IT alert module may monitor the information received by debug port . If the information includes an error message then IT alert module may issue an alert. In some further embodiments IT alert module may further take corrective measures. For example if one of processing nodes and of processing system fails IT alert module may order the booting of a spare server on the rack.

Some embodiments of may provide rack level port debug centralization in PCIe memory space. The entries to debug port storage may be written automatically in a uniform manner and may be tagged with information about the host node. Embodiments of may also provide for rack level automation of debug information to IT alerts. Because the information for a rack is written to a uniform place or places it is relatively easy for IT alert module to access the information and to issue alerts as needed. Management automation tools may constantly monitor these debug codes and send alerts to IT as configured. This method simplifies IT operation by centralizing debug information and allows greater intelligence in aggregate. Many embodiments of may also provide for rack level debug function redundancy thru a primary and secondary link. A node may attempt to write boot debug information over PCIe links to a primary RNC controller. If the primary RNC controller is unavailable however the node may be connected to a secondary RNC controller and may attempt to write the boot debug information to the secondary RNC controller.

These embodiments may provide an improvement over legacy methods. In legacy computer systems and rack systems each server on the rack may have written boot debug information to an input output port such as port in a proprietary format. The information may have been lost as soon as the node finished booting because the port was then used for other purposes. Further each server may have had a separate mechanism to alert for errors. Debug adapters BMCs and other modules are often used to latch this information during boot to alert the user where a server hung or had an error during initialization. In past architectures this was replicated on an individual server basis. Because there was no available method or mechanism for rack level logging of debug information this burden was incurred on every server.

In many embodiments the code for writing boot debug information is contained in BIOS. For these embodiments the systems of will enable the writing of port debug information in PCIe memory space. The BIOS code that directs the writing of debug information may be contained in flash images . Even legacy systems that initially boot from a minimal BIOS will transfer booting to the BIOS of flash images .

Image library may constitute an image library contained on bulk non volatile storage. The library may include boot images other Basic Input output System BIOS and Firmware images or Unified Extensible Firmware Interface UEFI modules. UEFI modules provide a software interface between operating systems and platform firmware such as BIOS. IT management may maintain the images determining when to add images delete images and replace images. Thus IT management may function as a centralized chassis resource manager for the images of image library . IT management may add or remove images by procedures similar to a file share procedure or through programmatic methods. IT management may also determine the assignment of images to processing nodes such as processing nodes and . IT management may then write the images assigned to a processing node to the flash images module of a RNC controller connected to the processing node via a PCIe link and may update the lookup tables such as lookup table .

In other embodiments a RNC controller may obtain some or all of the images used by processing nodes from image library rather than storing the images on the switch itself. Upon booting one of processing nodes and may fetch the assigned images from image library through a mechanism similar to the process for booting from a boot image of flash images .

Some embodiments may provide for an easy testing prior to putting a new image into service generally through a system. An upgrade process may operate as follows 

In further embodiments any devices with general load store capabilities that are components of a networked data processing system such as system of may reference image library . These devices may be local to a server node such as RAID controller devices or may be a shared device such as a storage controller.

Some embodiments of may simplify the process of updating BIOS and other firmware. For example it may enable a user to provide image version management by using 1 N means. The user may download and test a single image and place it in the image library for use by multiple computers in a networked data processing system. In addition some embodiments may provide easy to use methods for switching between multiple versions of images. To switch from one version of BIOS to another for a particular node for example the user may update an entry in lookup pertaining to that node or the user may replace a version in flash images with another version and reboot the node. In addition embodiments may reduce the downtime from updating to the time needed to reboot a server or hot reset a device. Since the images are stored off the server or device it does not need to be idle when it is loading the image. Further embodiments may ease implementation challenges with automated push. New software may be automatically downloaded stored in image library and distributed to RNC controllers thereby greatly reducing the effort required by management personnel. The result of embodiments of may be the implementation of a live consolidated selectable image library for the processing nodes on a single rack or on a large collection of racks.

In some embodiments a RNC controller may provide some but not all of the functions shown in or may contain fewer components. In some embodiments for instance booting may be done from BIOS in the individual nodes. In other embodiments boot images may be contained outside of a RNC controller such as on an external image library. In still other embodiments a RNC controller may provide additional functionality.

In the processing nodes of processing system may obtain real time clock time information from RTC over PCIe link . At startup the processing nodes of processing system may execute instructions contained in BIOS. In some embodiments as in the embodiments of the processing nodes of processing system may locate the BIOS code over PCIe links. The execution of those BIOS instructions may cause the processing nodes of processing system to send a command to RTC over PCIe links to obtain the time. In response the accessed RTC may send the real time over PCIe link to the processing nodes of processing system . The server may read this central RTC function and then load it into the local CPU Chipset registers for an operating system and applications to later use as the current time of day day month and year. In some embodiments the chipset components may then take over keeping the time function when power is applied to the processing nodes.

In many embodiments the processing nodes of processing system may request real time from RTC only at start up. Afterwards they may calculate the real time from the initial time and their own clock cycles. In other embodiments the processing nodes of processing system may access RTC at times other than start up. They may for example calculate the real time but make occasional checks to verify that their calculations do not diverge too far from the actual real time.

Some embodiments of the system of may provide a uniform real clock time for all of the processing nodes in a server rack may save on real estate of the processing nodes and may save on component costs. The processing nodes of processing system may have a uniform clock time because they may all obtain the clock time from the same real time clock rather than obtaining the time from different real time clocks. Additionally IT only has one or two in the case of backup locations to manage and update RTC information for an entire rack of servers.

Further the cost of components is lessened. Rather than each node of the processing nodes of processing system having its own real time clock and battery only two clocks and batteries are needed for the entire rack in the embodiment of . In one clock RTC supplies the real time to all of the processing nodes of processing system . By doing this a rack may eliminate the need to have a back up battery per server thus saving cost real estate and an IT component that may need servicing. It may also provide for automatic backup since each node of a rack may be connected a secondary RNC controller for backup as in the example of below.

Many embodiments of may also reliably provide real time clock information to the processing nodes of processing system even though there is not a real time clock on each server. Since RNC controllers are critical components of the systems the systems may rely on their operation to provide real time clock information.

Similarly to the operation of RTC system clock may provide a common system clock to processing nodes and of system by sending a periodic pulse to the nodes. In some embodiments system clock may be based upon a crystal vibrating at a frequency of 32 kHz and may send pulses at that frequency. Processing nodes and may use the frequency to time bus transactions such as the transactions over the PCIe links of system of . As a result of using a common system clock in some embodiments the bus transactions may be automatically synchronized. In further embodiments processing nodes and may apply a multiplier to the pulses sent by system clock to generate internal pulses for controlling computer cycles.

As with the real time clock the use of a common system clock may save cost real estate and additional servicing of an IT component and may provide backup from a secondary RNC controller. Because the number of clocks needed is greatly reduced highly precise clocks can be purchased by IT management. Further the synchronization may be especially important for real time applications. In particular it may prove important in audio video services and may also greatly simplify VM passing. In real time systems the different components may provide buffering to compensate for the tolerances in the timing of transactions. For example PCI Express has a 300 ppm clock tolerance Ethernet has a 100 ppm clock tolerance and SONET SDH has a 20 ppm clock tolerance. Systems designed to handle time aware or time sensitive data may compensate for these timing differences and clock tolerance discrepancies. The compensation usually results in additional buffering which adds to latency cost and power. In embodiments of system however the use of a single system clock for the processing nodes may provide for automatic synchronization. The nodes all derive their clock time from the same source and thus may keep clock times that are very close to each other. As a result it may be unnecessary for the nodes to compensate for timing differences.

Some embodiments of may provide for rack level shared video for the processing nodes of processing system . To connect one of processing nodes and to a video display the video display may be connected to RNC controller through VGA video connectors . In addition VGA hot swap module may establish a connection between VGA HW registers and real VGA controller . The establishment may involve a hot swap the connection may be made without rebooting the node.

VGA HW registers may consist of memory that emulates registers in real VGA controller . Real VGA controller may contain many registers for storing data related to the display on a video display. The registers may include pixel information and data to control the processing of the graphics information. To transmit graphics information to the video display a node may send graphics information such as bitmap information to VGA hardware registers . From there the information may pass to actual hardware registers on real VGA controller . In some embodiments real VGA controller may convert the string of bits it receives into electrical signals and send the electrical signals over VGA connector to the video display to control the display. Real VGA controller may include a Digital to Analog Converter DAC to convert the digital information held in the hardware registers into electrical signals. The video display may be used to display data generated by the operating system or by BIOS during boot. In particular the video display may be used as a crash cart connection. In network computing a crash cart may refer to a video screen keyboard and mouse on a portable cart. When a computer on a rack crashes the crash cart may be moved to the rack and the equipment hooked up to the rack in order to display debug and error information. In some embodiments of the crash cart has been rendered superfluous. To obtain that information an administrator may simply hot swap in the node and look at the video display for the rack.

Some embodiments of may also emulate video capacities to enable the proper functioning of racks. The architecture may present VGA hardware registers to a node to ensure that the operating system of the node believes it is connected to a VGA adapter even without an actual VGA function. Such functionality may be needed during for the proper operation of the rack. Windows in particular may check for the presence of certain VGA hardware during OS boot. It may detect the VGA hardware registers which imitate video adapter hardware registers and determine that the necessary VGA hardware is present during the boot. Embodiments of may also reduce the per server costs hardware the power costs and the space requirements for a rack of processing nodes by eliminating redundancy. Instead of a VGA controller per node there may be one per server rack in some embodiments. In addition the VGA function may be centralized. In particular if a primary input output complex switch is not available a node may be able to hook up to a video display or to a VGA HW register through a secondary RNC controller available as a backup through a secondary input output complex switch as in the example of below.

In other embodiments other graphics protocols may be used for video display including DMI HDMI and DisplayPort. Video displays may include CGA WVGA WS VGA HD 720 WXGA WSXGA HD 1080 K WUXGA XGA SXGA SXGA UXGA QXGA WQXGA and QSXGA displays or other displays known to those of skill in the art.

In other embodiments RNC controller may also provide keyboard and mouse functionality to processing nodes and . In these embodiments RNC controller may transmit emulated mouse and keyboard signals over PCIe link to the processing nodes and . In some further embodiments providing the keyboard and mouse functionality may require converting PCIe link signals to USB bus signals since the use of USB buses for keyboards and mice are standard.

Method includes searching for boot code for the processing node in a lookup table such as lookup table of the RNC controller at block . In some embodiments the processing node may embed an identifier in the PCIe packet sent over the PCIe link to fetch the boot code. The identifier may describe the device ID of the processing node the hardware revision information about software such as an operating system running on the processing node and other information about the processing node. The lookup table may index or otherwise associate boot code with identifiers of processing nodes.

Method includes testing whether the lookup is successful at block . If so at block the boot code is sent over the PCIe link to the processing node and it boots from the boot code. If not at block the RNC controller attempts another lookup of suitable boot code. In some embodiments the RNC controller may search for a suitable boot image in an internal location maintained by IT management. If that search also proved unsuccessful the RNC controller might support a phone home capability. Method includes testing whether the other lookup is successful at block . If so at block the boot code is sent over the PCIe link to the processing node and it boots from the boot code. If not the method ends.

The method includes monitoring the debug information at block . In some embodiments the debug information may be automatically monitored as by IT alert module . The debug information is checked for error messages at block . If no messages are found method may end. If messages are found at block an alert module may issue an alert.

Method includes the processing nodes applying a multiplier to the pulses sent by system clock to generate internal pulses to control computer cycles at block . Method includes the processing nodes applying a multiplier to the pulses sent by system clock to generate internal pulses to control computer cycles at block . Method ends at block with the processing nodes synchronizing Real Time transactions based on the internal pulses.

Method includes emulating a VGA controller for the processing nodes at block . Block includes the VGA hardware registers receiving VGA communications from processing nodes over the PCIe link at block . Some operating systems may for example check for the presence of a VGA adapter during boot. Block includes the VGA hardware registers transmitting responses over the PCIe link at block .

Method includes connecting a processing node to a video display at block . Block includes connecting the processing node to the real VGA controller in a hot swap through the actions of the VGA hot swap module at block . Block includes connecting the VGA controller to the video display at block . Block includes exchanging VGA messages between the processing node and the video display at block . In some embodiments for example the processing node may send pixel information about the images to be displayed and the video display may respond with status reports.

Method includes emulating a keyboard and mouse for the processing nodes at block . Block includes the keyboard and mouse emulators receiving communications from the processing nodes over the PCIe link at block . Block includes the keyboard and mouse emulators transmitting the emulated responses over the PCIe link at block .

Method includes connecting a processing node to a keyboard and mouse at block . Block includes connecting the processing node to the keyboard and mouse controllers at block . Block includes connecting the keyboard and mouse controllers to the keyboard and mouse respectively at block . Block includes exchanging messages between the processing node and the keyboard and mouse at block . In some embodiments for example the mouse may send information about its state which button is clicked and its position. The keyboard may send information about a depressed key or combination of keys and about the timing of the keystrokes. In response the processing node may send status information. In other embodiments other input devices may be used instead of or in addition to a mouse and a keyboard.

Multi function PCIe module is connected to processing node via a PCIe link and operates as a PCIe endpoint associated with processing node . As such multi function PCIe module is enumerated in the PCIe configuration space of processing node as being associated with a particular PCIe link number a designated device number on the PCIe link and a particular function number of the device. As such multi function PCIe module includes a set of PCIe endpoint status and control registers that permit processing node to send data to to receive data from and to otherwise control the operation of the multi function PCIe module. Similarly multi function PCIe module is connected to processing node via a PCIe link and operates as a PCIe endpoint associated with processing node and is enumerated in the PCIe configuration space of processing node as being associated with a particular PCIe link number a designated device number on the PCIe link and a particular function number of the device. As such multi function PCIe module includes a set of PCIe endpoint status and control registers that permit processing node to send and receive data and to otherwise control the operation of the multi function PCIe module.

Input output complex can include one or more additional multi function PCIe modules that are similar to multi function PCIe modules and and that are connected to one or more additional processing nodes such to processing nodes and . For example input output complex can include up to 16 multi function PCIe modules similar to multi function PCIe modules and that can be coupled to up to 16 processing nodes similar to processing nodes . In this example network complex can include 16 network interface ports. In another example input output complex can include more or less than 16 multi function PCIe modules and network complex can include more or less than 16 network interface ports. In another embodiment switch complex can include two or more input output complexes similar to input output complex . For example switch complex can include four input output complexes such that up to 64 processing nodes can be coupled to the input output switch complex. In this example network complex can include 64 network interface ports and each input output complex can be connected to 16 of the network interface ports.

Multi function PCIe modules and operate as multi function PCIe devices in accordance with the 3.0 . As such multi function PCIe module includes P2P endpoint VDM endpoint RRDMA endpoint SAS endpoint and RNC endpoint that each operate as PCIe endpoints associated with processing node and are enumerated in the PCIe configuration space of the processing node as being associated with the same PCIe link number and designated device number as multi function PCIe module but with different function numbers. For example P2P endpoint can be identified as function 1 VDM endpoint can be identified as function 2 RRDMA endpoint can be identified as function 3 SAS endpoint can be identified as function 4 and RNC endpoint can be identified as function 5. Similarly multi function PCIe module includes P2P endpoint VDM endpoint RRDMA endpoint SAS endpoint and RNC endpoint that each operate as PCIe endpoints associated with processing node and are enumerated in the PCIe configuration space of the processing node as being associated with the same PCIe link number and designated device number as multi function PCIe module but with different function numbers. For example P2P endpoint can be identified as function 1 VDM endpoint can be identified as function 2 RRDMA endpoint can be identified as function 3 SAS endpoint can be identified as function 4 and RNC endpoint can be identified as function 5. Each endpoint and includes a set of PCIe endpoint status and control registers that permit the respective processing nodes and to send data to to receive data from and to otherwise control the operation of the endpoints. In the embodiment illustrated by switch complex various levels of integration of the elements of switch complex are envisioned. For example network complex and switch complex can be integrated into a single ASIC management controller can be integrated with RNC controller I O complex can be integrated with the network complex and the switch complex or other combinations can be provided as needed or desired.

In the embodiments described herein an information handling system includes any instrumentality or aggregate of instrumentalities operable to compute classify process transmit receive retrieve originate switch store display manifest detect record reproduce handle or use any form of information intelligence or data for business scientific control entertainment or other purposes. For example an information handling system can be a personal computer a consumer electronic device a network server or storage device a switch router wireless router or other network communication device a network connected device cellular telephone tablet device etc. or any other suitable device and can vary in size shape performance price and functionality. The information handling system can include memory volatile e.g. random access memory etc. nonvolatile read only memory flash memory etc. or any combination thereof one or more processing resources such as a central processing unit CPU a graphics processing unit GPU hardware or software control logic or any combination thereof. Additional components of the information handling system can include one or more storage devices one or more communications ports for communicating with external devices as well as various input and output input output devices such as a keyboard a mouse a video graphic display or any combination thereof. The information handling system can also include one or more buses operable to transmit communications between the various hardware components. Portions of an information handling system may themselves be considered information handling systems.

When referred to as a device a module or the like the embodiments described herein can be configured as hardware. For example a portion of an information handling system device may be hardware such as for example an integrated circuit such as an Application Specific Integrated Circuit ASIC a Field Programmable Gate Array FPGA a structured ASIC or a device embedded on a larger chip a card such as a Peripheral Component Interface PCI card a PCI express card a Personal Computer Memory Card International Association PCMCIA card or other such expansion card or a system such as a motherboard a system on a chip SoC or a stand alone device . The device or module can include software including firmware embedded at a device such as a Pentium class or PowerPC brand processor or other such device or software capable of operating a relevant environment of the information handling system. The device or module can also include a combination of the foregoing examples of hardware or software. Note that an information handling system can include an integrated circuit or a board level product having portions thereof that can also be any combination of hardware and software.

Devices modules resources or programs that are in communication with one another need not be in continuous communication with each other unless expressly specified otherwise. In addition devices modules resources or programs that are in communication with one another can communicate directly or indirectly through one or more intermediaries.

Although only a few exemplary embodiments have been described in detail herein those skilled in the art will readily appreciate that many modifications are possible in the exemplary embodiments without materially departing from the novel teachings and advantages of the embodiments of the present disclosure. Accordingly all such modifications are intended to be included within the scope of the embodiments of the present disclosure as defined in the following claims. In the claims means plus function clauses are intended to cover the structures described herein as performing the recited function and not only structural equivalents but also equivalent structures.

