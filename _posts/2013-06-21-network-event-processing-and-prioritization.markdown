---

title: Network event processing and prioritization
abstract: The described implementations relate to processing of electronic data. One implementation is manifest as a system that can include an event analysis component and one or more processing devices configured to execute the event analysis component. The event analysis component can be configured to obtain multiple events that are generated by network devices in a networking environment. The event analysis component can also be configured to identify impactful events from the multiple events. The impactful events can have associated device-level or link-level impacts. The event analysis component can also be configured to determine one or more failure metrics for an individual impactful event. The one or more failure metrics can include at least a first redundancy-related failure metric associated with redundant failovers in the networking environment.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09350601&OS=09350601&RS=09350601
owner: Microsoft Technology Licensing, LLC
number: 09350601
owner_city: Redmond
owner_country: US
publication_date: 20130621
---
In many network contexts it is important to maintain reliable responsive communications with end users. In the case of an Internet Service Provider ISP connected clients may notice degraded service due to network device failures within the ISP network. In the case of a data center an application may execute on one or more servers and clients communicating with the application may notice degraded service due to network device failures within the data center. Traditionally network operators or engineers have used device generated events to debug and resolve network device failures.

However often the number of device generated events is too large for a human operator or engineer to practically analyze manually. Moreover the information provided in the device generated events does not necessarily lead directly to conclusions about how much impact the events have on end users. To address this issue some automated techniques have evolved to help network operators and engineers recognize and respond to network failures more quickly. Unfortunately existing techniques may not adequately prioritize failure events and this makes it more difficult for engineers and operators to quickly remedy network failures particularly high severity network failures which impact service performance availability or security.

The above listed example is intended to provide a quick reference to aid the reader and is not intended to define the scope of the concepts described herein.

This document relates to processing electronic data. One implementation is manifest as one or more computer readable storage devices storing instructions which when executed by one or more processing devices cause the one or more processing devices to perform acts. The acts can include obtaining multiple events that are generated by network devices in a networking environment. The acts can also include processing the multiple events to identify a subset of impactful events from the multiple events. The subset of impactful events can have associated device level impacts or link level impacts of traffic loss or latency inflation. The acts can also include determining a device level or link level failure metric for individual impactful events in the identified subset of impactful events. The device level or link level failure metric can be determined based on the traffic loss or latency inflation associated with the individual impactful events. The acts can also include determining a redundancy related failure metric for the individual impactful events. The redundancy related failure metric can be associated with individual network devices or links in the networking environment that are configured to perform redundant failovers. The acts can also include prioritizing the individual impactful events according to at least the device level or link level failure metric and the redundancy related failure metric.

Another implementation is manifest as a system that that can include an event analysis component and one or more processing devices configured to execute the event analysis component. The event analysis component can be configured to obtain multiple events that are generated by network devices in a networking environment. The event analysis component can also be configured to identify impactful events from the multiple events. The impactful events can have associated device level or link level impacts. The event analysis component can also be configured to determine one or more failure metrics for an individual impactful event. The one or more failure metrics can include at least a first redundancy related failure metric associated with redundant failovers in the networking environment.

Another implementation is manifest as a technique that can include obtaining multiple events that are generated by network devices in a networking environment and identifying individual impactful events from the multiple events based on associated impacts to the networking environment or applications or services hosted thereon. The technique can also include prioritizing the individual impactful events using one or more policies having one or more criteria.

This discussion relates to processing events on an electronic network. For example various network devices can generate events that indicate status of the network devices or status of links between the network devices. Of these events some will be indicative of significant network failures. However the presence of such events is not necessarily indicative of an impactful problem in the network. For example a network device or link may generate intermittent events that taken individually seem to suggest problems on the network device. However if the network device or link does not seem to be dropping any packets or exhibiting substantial latency these events do not indicate any kind of meaningful failure and can in some cases be ignored.

Even when a failure is impactful at the device or link level e.g. the failure causes some dropped packets or increased latency at an individual network device or link there may be circumstances where these impactful failures are mitigated from the perspective of the end user. For example one way to mitigate device level failures is to deploy network devices or links in redundancy groups. Generally a redundancy group contains multiple member devices links and individual member devices links can perform the functionality of other member devices links should they fail. The term successful failover indicates that a failure by a member of a redundancy group of devices or links has been successfully mitigated by one or more other members of the redundancy group. The term unsuccessful failover indicates that a failure by a member of a redundancy group of devices or links has not been successfully mitigated by the other members of the redundancy group.

Generally the disclosed implementations can help network engineers or operators distinguish between failure events that are impactful at the device or link level and failure events that are relatively inconsequential at the device or link level. Furthermore the disclosed implementations can rank or prioritize failure events with significant device level impact based on whether available mitigation mechanisms such as redundancy groups can be expected to reduce service disruptions due to the device or link level failures. For example consider two network devices that fail where a first one of the network devices is part of a redundancy group and a second one of the network devices does not have a redundant backup. The disclosed implementations may tend to rank failure events associated with the second network device more highly than failure events associated with the first device because failures of the first device are more likely to be masked by successful failovers. Analogous processing can be performed for failure events associated with particular links connecting various network devices.

To facilitate identifying and prioritizing impactful failures failure metrics are provided that can be derived from device generated events. Generally the disclosed failure metrics include device level or link level metrics relating to traffic loss and or latency inflation at individual network devices or links. The disclosed failure metrics also include redundancy related metrics that reflect whether individual redundancy failovers are successful and whether a given redundancy group can be expected to provide successful failovers on an ongoing basis. The redundancy related metrics can be determined for individual devices and links that are redundantly configured. The disclosed failure metrics also include functional failure metrics that reflect whether traffic flow within a network violates predefined rules and or potentially creates security issues.

Some of the present techniques can leverage event logs which may include events that are logged by various devices. The events in the logs can include failure events indicating failure of one or more network devices in the data center. Event logs can be processed to identify which events if any have associated device level impacts. Failure metrics can then be derived for the events with device level or link level impacts. The failure metrics can then be processed to prioritize or rank the events. For example one or more policies can be applied to the events to prioritize the events. The policies can have criteria that are applied to the failure metrics to perform the prioritizing.

From a logical standpoint the architecture can be organized into a hierarchy that includes a core layer an L3 aggregation layer and a L2 aggregation layer . This logical organization can be based on the functional separation of Layer 2 e.g. trunking VLANs etc. and Layer 3 e.g. routing responsibilities. The network devices shown in each layer of architecture are typical of some implementations in which the present concepts can be employed. However note that the present concepts can also be employed in various other network contexts including non hierarchical configurations or configurations with different hierarchies than discussed herein.

In addition shows aggregation switches and deployed in first redundancy group at the L2 aggregation layer aggregation switches and deployed in a second redundancy group at the L2 aggregation layer and aggregation switches and deployed in a third redundancy group at the L2 layer. As discussed above with respect to the access routers if aggregation switch fails traffic can still pass between application and network as long as aggregation switch is still functioning. Similar reasoning applies to application with respect to aggregation switches and and for application with respect to aggregation switches and .

More generally in a redundancy group the group contains multiple members and individual members can perform the switching routing functions when other member s of the redundancy group fail. Note also that illustrates core routers in a redundant configuration. In addition note that the examples above assume that a single functioning device can accommodate all traffic across the corresponding layer.

Generally speaking redundancy groups can be deployed in various configurations including active standby configurations and active active configurations. In active standby configurations one or more devices are active e.g. carrying traffic and one or more other devices are on standby e.g. not carrying traffic and can be activated to take over for a failing device. In active active configurations the devices in the group are active and when a device in the group fails traffic that would have otherwise been carried by the failing device can be carried by the remaining members of the group.

ToRs also known as host switches connect the servers hosted by the racks to a remainder of the architecture via an internal data center network represented by connecting lines in . Host ports in these ToR switches are often 10 100 1000 Ethernet with the uplinks being Gigabit Ethernet or 10GE ports. The ToRs can be connected upstream to aggregation switch . These aggregation switches can serve as an aggregation point for Layer 2 traffic and typically support high speed technologies such as 10 Gigabit Ethernet to carry large amounts of traffic e.g. data .

Traffic from an aggregation switch can be forwarded to an access router . The access router can use Virtual Routing and Forwarding VRF to create a virtual Layer 3 environment for each tenant. A tenant is an application such as a service hosted on servers which use network devices for connectivity route traffic from to users or other services to from its hosted servers. Thus illustrates three tenants applications and although varying numbers of tenants can execute on individual servers of racks . In some implementations the L3 aggregation layer can aggregate traffic from up to several thousand servers and route the traffic to core routers that can connect to the rest of the architecture and network .

Some implementations especially user facing applications may use load balancers to improve the performance of hosted applications. Redundant pairs of load balancers can connect to the aggregation switch and perform mapping between static IP addresses exposed to clients through DNS and dynamic IP addresses of the servers to process user requests to applications and . Load balancers can support different functionalities such as network address translation secure sockets layer or transport layer security acceleration cookie management and data caching.

Firewalls can be deployed in some implementations to protect applications from unwanted traffic e.g. DoS attacks by examining packet fields at IP Internet Protocol layer transport layer and sometimes even at the application layer against a set of defined rules. Generally software based firewalls can be attractive to quickly implement new features. However hardware based firewalls are often used in data centers to provide performance critical features.

Virtual private networks VPNs can augment the data center network infrastructure by providing switching optimization and security for web and client server applications. The virtual private networks can provide secure remote access. For example the virtual private networks can implement secure sockets layer transport layer security or other techniques.

Considering note that there are several points of failure which could result in the unavailability of application . For example application could have a software failure hardware failure misconfiguration protocol error or other malfunction that causes application to stop executing properly on the servers of racks . Additionally failure of ToR can result in unavailability of application as can concurrent failure of both of the redundantly configured core routers and both redundantly configured access routers and and or both redundantly configured aggregation switches and . Similar reasoning applies to application and as already mentioned above.

Network can include various wired and or wireless networks and combinations thereof. For example network can include the public Internet as well as various private networks or portions thereof that connect any of the devices data centers shown in . For the purposes of the following discussion it is generally sufficient that network provides connectivity between devices or data centers that share information.

Client device can interact with application and or by communicating over network with either data center or data center provided there is connectivity. Application interface can include logic for communicating with application and or e.g. formatting functionality display functionality etc. For example client device can be employed by an end user that wishes to use various features made available by application and or . In some cases application interface is a general purpose web browser a dedicated client side interface etc.

Server operations center can generally include one or more server devices configured to monitor the individual data centers for network problems. For example monitoring system can execute on the server devices to monitor data centers and . In some implementations network operators e.g. network engineers at server operations center may attempt to resolve issues on either data center and can track the issues using support tickets diaries or other techniques.

Event analysis component of analysis device can be configured to analyze various events in one or more data centers e.g. device generated events generated within data center or both. The analysis can include determining whether individual events are impactful at the device level. The analysis can also include determining the extent to which redundancy may mitigate problems caused by impactful device level failures.

Note that the various devices shown in system are illustrated with respect to logical roles that can be performed by the devices in operation of system . However the geographical locations of the various devices are not necessarily reflected by system . For example data centers and or may be collocated with server operations center and or analysis device . As another example the event analysis component and or monitoring system can be implemented on one or more devices inside an individual data center e.g. on one or more of server racks .

Furthermore note that illustrates server operations center as multiple server devices whereas analysis device and client device are illustrated as individual computing devices. This reflects one particular implementation and other implementations may provide characterization functionality and or client functionality as discussed herein via multiple devices. Likewise server operations center and or data center functionality as discussed herein may be performed by individual devices.

In addition functionality described herein with respect to a particular device or devices can be distributed across multiple devices or combined on a single device. For example monitoring system and event analysis component can be collocated at a server operations center on a single device or multiple devices. As another example the event analysis component and or monitoring can be employed on a device at one or both data centers of system .

Further note that in practice there may be additional instances of each computing device mentioned herein e.g. additional analysis devices server operations centers client devices and data centers. As discussed in more detail below each of the computing device s shown in can include one or more processing devices such as computer processors executing instructions stored on one or more computer readable storage media such as volatile or non volatile memories optical disks hard drives flash drives etc. Furthermore the various computing devices can be configured to accept various forms of input including gestures touch screens using voice recognition etc.

The monitoring system on server operations center can generally serve to obtain various data relating to the operation of data centers and . The obtained data can be provided to event analysis component for further processing as discussed in more detail below. For example the data obtained by the monitoring system can include device generated events such as Simple Network Management Protocol SNMP events.

In some implementations the monitoring system can accept events directly from individual devices or from device log streams e.g. from syslog . For example the event logs can be obtained from and include events generated by network devices such as core routers access routers aggregation switches and or ToRs as well as various other network devices firewalls load balancers etc. . The events can contain information about what type of network component experienced an event the event type the other end point of this component e.g. the one hop directly connected neighbor and a short machine generated description of the event.

In some implementations the monitoring system can combine multiple device generated events into a single logical event. For example a given network device may generate identical or very similar events at a high rate for a given period of time and the monitoring system may combine these events into a single logical event. Thus the event analysis component can operate on the combined logical events instead of processing the individual device generated events. Unless otherwise specified herein the term event can refer to either a device generated event or a logical event that represents multiple device generated events.

The monitoring system on server operations center can also obtain traffic data. For example traffic carried on network interfaces links can be logged using Simple Network Management Protocol SNMP polling that averages traffic seen every five minutes for example. Other sources of traffic data can be obtained from sampling based approaches such as NetFlow or sFlow the latter also captures the first few bytes say 100 bytes of each packet sampled from the traffic flows. Traffic monitoring systems can use the management information base MIB format to store the data that includes fields such as the interface type token ring Ethernet etc. the other end of the interface e.g. the one hop directly connected neighbor the interface status up down timestamp and or the number of bytes sent or received by the interface among others.

In some implementations the monitoring system can map the device generated events to a common schema e.g. one or more device generated events can be combined into a logical event that conforms to the common schema. Then the monitoring system can forward the events that have been mapped to the common schema to the event analysis component for further processing. Generally a given data center may have multiple different types of network devices from different vendors with different operating systems etc. Thus events generated by such network devices may be difficult to directly compare or analyze when in their native format e.g. as generated by a given device. By mapping the events to a common schema events that were originally generated in different native formats can be directly compared by the event analysis component.

One example schema is as follows Device Name Notified At Cleared At Ticket ID Device Type Data Center Property Property Abbreviation. Here Device Name is the name of the device that generated the event e.g. each device in a given data center may have a specific name used to distinguish that device from other devices in the network. Notified At may be the time when the event was initially generated by the device and Cleared At may be the time when the event was eventually cleared e.g. by replacing a hardware component with a spare rebooting replacing the device that generated the event etc. Device Type may indicate the functional type of the device e.g. access router aggregation switch etc. .

Data Center may identify the particular data center where the device that generated the event is located. Property may identify an application or service that could be affected by the event. In other words Property may identify an application or service that communicates traffic through the device that generated the event e.g. an event at aggregation switch could have application as the value for Property and an event at access router could have application as the value for Property. Property Abbreviation may be a shorthand notation for the potentially affected service or application.

In this case analysis device can include an application layer an operating system layer and a hardware layer . The event analysis component can be manifest as a program or application of the application layer among other configurations. In this example the event analysis component can include an impact module a failure metric module and a prioritization module . The event analysis component can process events provided directly over network by monitoring system . Alternatively the monitoring system can populate a database with events and the event analysis component can process the events in the database. As mentioned the events received from the monitoring system or populated in the database can be logical events that are mapped to a common schema and in some cases an individual logical event is a combined event representing multiple device generated events. In other implementations the event analysis component may operate directly on device generated events without any such preprocessing.

The impact module can receive events from the monitoring system or database and determine whether a given event has a device level impact. For events with device level or link level impacts the failure metric module can determine one or more failure metrics. The prioritization module can prioritize or rank different events based on the failure metrics. For example the prioritization module may apply one or more policies to the failure metrics to perform the ranking as discussed more below.

The hardware layer can include a processor storage memory e.g. one or more computer readable storage media a display device and or various other elements. For instance the other elements can include input output devices optical disc readers USB ports etc. Processor can execute computer readable instructions to provide functionality such as event analysis component functionality. Data and or computer readable instructions can be stored on storage memory and or received from another source such as optical storage device . The storage memory can include any one or more of volatile or non volatile memory devices hard drive storage devices flash storage devices e.g. memory sticks or memory cards and or optical storage devices e.g. CDs DVDs etc. among others.

Alternatively to the illustrated configuration of analysis device the analysis can employ a system on a chip SOC type design. In such a case functionality provided by the analysis device can be integrated on a single SOC or multiple coupled SOCs. For instance the computer can include shared resources and dedicated resources. An interface s can facilitate communication between the shared resources and the dedicated resources. As the name implies dedicated resources can be thought of as including individual portions that are dedicated to achieving specific functionalities. Shared resources can be storage processing units etc. that can be used by multiple functionalities.

Generally any of the functions described herein can be implemented using software firmware hardware e.g. fixed logic circuitry manual processing or a combination of these implementations. The term engine tool component or module as used herein generally represent software firmware hardware whole devices or networks or a combination thereof. In the case of a software implementation for instance these may represent program code that performs specified tasks when executed on a processor e.g. CPU or CPUs . The program code can be stored in one or more computer readable storage memory devices such as computer readable storage media. The features and techniques of the component are platform independent meaning that they may be implemented on a variety of commercial computing platforms having a variety of processing configurations.

As used herein the term computer readable media and computer readable medium can include signals and hardware. In contrast the terms computer readable storage media and computer readable storage medium exclude pure signals. Computer readable storage media can include computer readable storage devices. Examples of computer readable storage devices include volatile storage media such as RAM and non volatile storage media such as hard drives optical discs and flash memory among others.

In the example of the event analysis component can generate GUI screenshot by obtaining events from monitoring system and or database . As mentioned database can be populated by the monitoring system and can include events from one or more devices in one or more data centers. GUI screenshot shows prioritized events listed in the prioritized order as determined by the event analysis component.

Each event listed in GUI screenshot can include information such as an event ID device ID failure metrics and and event property . In some implementations event ID can be an identifier that uniquely identifies each logical event provided to the event analysis component by the monitoring system e.g. a monotonically increasing event ID. Device ID identifies the particular device that generated the event e.g. event ID was is shown as associated with device ID AR chosen for notational convenience to represent access router as shown in . Also note that events may be referred to herein as event event etc. when referencing the event associated with a particular event ID. Event property can identify an application or service that is potentially impacted by a given event e.g. an application or service hosted on one or more servers within the subtree of the device that generated the event. Note that the information shown in GUI screenshot is exemplary and can include other information such as identifying specific links that have failed other information from the event schema discussed above other failure metrics discussed herein etc.

The failure metrics shown in include a traffic lost metric a latency inflation metric a failover metric and an at risk redundancy metric . Traffic lost metric can be a device level metric that indicates whether a given device lost any traffic e.g. the value of 50 shown for event indicates that the traffic through AR dropped by 50 after event . In other words the traffic lost failure metric represents a ratio of traffic at the device after the event to the traffic at the device before the event. Alternatively as shown below with respect to the traffic lost failure metric can represent a volume of traffic lost as a result of the event e.g. traffic before event traffic during event event duration e.g. in bytes octets per second. Note that the traffic lost metric can also be computed at the link layer as well e.g. traffic lost over a particular link that connects network devices to one another.

Latency inflation metric can be another device level metric that indicates whether a given device had an increase in latency associated with a given event. For example the latency inflation metric can be represented as a percentage of the device latency before an event to the device latency after an event. Often latency inflations are a result of queuing delays on a given device due to buffers getting increasingly full after the event takes place. Other reasons for latency inflation can include faults in the traffic routing and switching plane cards inside the device e.g. supervisor engine in Access routers and Aggregation switches from Cisco where one or more of these cards may fail or not be completely operational. Note that the latency inflation metric can also be computed at the link layer as well e.g. latency inflation on a particular link that connects network devices to one another.

Failover metric and at risk redundancy metric can be considered redundancy related metrics in the sense that each of these metrics relates to the efficacy of redundancy at handling failure events. Failover metric and at risk redundancy metric can be computed on a device layer and or link layer for each impactful event. Failover metric indicates whether a redundancy failover successfully took place after the event. For event ID the failover value of No indicates that the failover was not successful e.g. access router did not successfully handle the failure by access router . For event the failover value of Yes indicates that the failover was successful e.g. access router was successfully able to handle the failure by access router . Generally failover can be considered successful if the total volume of traffic through a given redundancy group of devices or links does not change as a result of the failure. In other words traffic has been rerouted from the failed device to another device without losing traffic. Note that the failed device may cause retransmissions e.g. TCP retransmission to other devices in the redundancy group but as long as these retransmissions succeed the failover can generally be considered successful.

Note also that some implementations may use the failover metric on a link level. For example one link on a device may fail but another link on the same or another device maybe be redundantly configured to handle traffic for the failing link. If this link to link failover is successful the failover metric can have a value of Yes and can otherwise have a value of No. Unless link failovers are expressly mentioned however references herein to the failover metric refer to device failovers within a redundant group of devices.

At risk redundancy metric also relates to redundancy but indicates whether a redundancy group of devices or links that successfully handled a failover is at risk of not being able to handle subsequent failover attempts. For example the at risk redundancy metric may indicate whether the device that successfully handles a failover is operating as a single point of failure e.g. is the last device remaining operational device in a redundant group. For event ID the failure of access router leaves only access router to handle any traffic to from application . Thus access router is operating as a single point of failure in this instance and the at risk redundancy metric has a value of Yes. If there were a third member of the redundancy group and two of the devices were still operational then the value of at risk redundancy metric would be No because there would be two remaining redundant devices to handle the failure by access router .

Note also that links connecting a redundant device to another device can also be single points of failures. Thus in some cases the at risk redundancy metric reflects whether a given link is acting as a single point of failure or if another link is available in the event a failure occurs. Unless link failovers are expressly mentioned however references herein to the at risk redundancy metric refer to redundant groups of devices.

As mentioned above some implementations may prioritize events based on associated failure metrics. shows a method that can be applied in this context. For example method can be performed by event analysis component to prioritize various events.

At block the method can obtain a set of events logged at one or more data centers or other network locations. In some implementations the set of events can be obtained from a data center monitoring system or from a database that stores the set of events on behalf of the data center monitoring system. As discussed above in some implementations the events can be logical events corresponding to multiple device generated events that have been combined by the monitoring system and mapped to a common schema for further processing.

At block the method can process the events to identify a subset of impactful events. For example the method can filter the events by separating spurious and duplicate events to obtain a filtered subset of events that have some impact on a device link application or service. For example the subset of impactful events can include events associated with traffic loss latency inflation CPU or memory utilization number of connections improper routing of data to from of an application or service etc. As discussed more below the separating can be accomplished by applying a pipeline of event filters to the set of events to generate the filtered subset of the events. In some cases the pipeline can be created by selecting individual filters from a set of available event filters. The individual event filters may each apply different criteria to filter different events to create the filtered subset. Other implementations may use other techniques to identify the subset of impactful events e.g. by analyzing correlations between event characteristics and event impacts.

At block the method can determine one or more device level and or link level failure metrics for each event in the subset. For example the device level failure metrics can reflect traffic loss across individual network devices latency increases across individual network devices etc. Likewise the link level failure metrics can reflect traffic loss latency increases across individual links.

At block the method can determine one or more redundancy related failure metrics for each event in the subset. For example the redundancy level failure metrics can reflect whether a successful failover occurs in a redundancy group of devices or links whether the redundancy group is likely to continue to failover successfully etc.

At block the events can be prioritized according to various criteria. For example the criteria can be included in one or more policies that are applied in a cascaded fashion so that relatively high priority events are identified by earlier applied policies. The criteria in the policies can be based on the failure metrics e.g. a given policy may have a first criterion for a device level failure metric and a second criterion for a redundancy level failure metric. Criteria can also relate to link level metrics as well.

At block the prioritized events can be output for automated processing generating of various user interfaces etc. Several example visualizations that can be used to convey prioritized events are discussed below.

As mentioned above some implementations employ filtering of events to obtain impactful events e.g. events that are more than mere noise and that have some device level link level or application level impact. shows a filtering method for separating a subset of impactful events from less informative events from a set. This method can utilize events from various sources. Generally speaking method can be viewed as one way to implement block of method discussed above.

In some implementations the events can be manifest as Syslog SNMP events and can be filtered using a timing filter that filters events using timing related criteria. The timing filter can be used to fix various timing inconsistencies. In some implementations the timing filter can first group events with the same start and end time originating on the same interface into a single event. This process can remove duplicate events.

Next the timing filter can pick the earliest start and end times of multiple events that originated within a predefined time window on the same interface. For example any events that happened within a predefined time of 60 seconds on the same interface can be grouped into a single event e.g. characterized as a single event . This process can reduce or avoid any problems due to clock synchronization and log buffering. The timing filter can also be used to group two events using interface criteria e.g. by identifying events that originate on the same interface. For example events that have the same start time but different end times can be grouped into a single event that is assigned the earlier of the end times. The earliest end times can be utilized since events may not be marked as cleared long after their resolution.

The technique can employ a planned maintenance filter that applies planned maintenance criteria to filter events due to planned maintenance. Events caused by planned maintenance can have less value in understanding device behavior than unplanned events e.g. unexpected outages . Thus the planned maintenance filter can remove events that are caused by planned maintenance activities. In some implementations tickets from a network operations center NOC ticket database can be used to determine whether maintenance is planned for a given device to detect whether a given event is caused by planned maintenance. In other implementations network change notification systems or maintenance notification systems that track prior current and scheduled network changes and upgrades can be employed to detect events caused by planned maintenance.

The technique can employ a redundant failures filter . The redundant failures filter can apply redundant event criteria to filter events logged by devices that are scheduled for replacement or that have been detected as faulty by operators but are awaiting repairs. The redundant failures filter can identify these redundant events by arranging the devices in a list in descending order of their number of failures. In one implementation for the top k or the top percentage e.g. 5 of the devices in this list all events are merged that have the same NOC TicketID field. This constitutes a merged event reflecting individual events with the same ticket ID that are likely to have the same symptoms.

The technique can employ an impact filter that applies impact criteria to filter events. An event can be defined as having an impact when the event affects application reliability e.g. throughput loss number of failed connections or increased latency. In implementations without access to application level logs failure impact can be estimated by leveraging network traffic data and computing the ratio of the median traffic on a failed device link during a failure and its value in the recent past. For example the value of the recent past can be set as the preceding eight hour or other duration time correlation window. Other implementations can use other values. A failure can be considered to have an impact if this ratio is less than one or another threshold on the ratio can be used e.g. if traffic drops 10 within the time correlation window .

The impact filter can also employ trouble ticket inference to determine whether a trouble ticket is associated with a given event. If a trouble ticket associated with the event is present in NOC ticket database it is likely that the event has an impact. Likewise if the event does not have an associated trouble ticket it is likely that the event does not have an impact. Thus in some implementations impact filter can filter out events that do not have associated trouble tickets.

The above acts can collectively allow method to identify the impactful event subset . Note that other filters can alternatively or additionally be utilized. For example some implementations may employ filtering based on whether a given event impacts CPU and or memory utilization of a given network device number of connections e.g. for a load balancer filtering based on whether a recent configuration change may have caused the event etc. Furthermore one or more of the aforementioned filters may be omitted and or the filtering may be performed in a different order than that shown in etc.

Also note that various techniques can be used to correlate network related data such as trouble tickets traffic data device configurations performance counters etc. without or in addition to the specific filtering implementations shown above. For example method can include selecting individual events associated with particular values for performance counters e.g. CPU utilization counters. If a given event is associated with a network device CPU reaching 95 CPU utilization this can be considered a device level impact that may not exhibit traffic loss or latency symptoms. Note that such events may be considered impactful and thus included in the impactful event subset . Also note that other performance counters can be applied in a similar fashion e.g. memory utilization cache utilization etc.

Another example of events that may be selected for the impactful event subset is events associated with a particular device configuration change. For example a configuration change to the network could involve a modification to an access control list that causes one or more network devices to block traffic coming from a particular IP address or range of addresses. In this case an affected device might produce an event that specifically identifies the configuration change. Alternatively the configuration change can be detected by monitoring symptoms of the configuration change e.g. packets from a particular IP address or range of addresses being dropped. Note that impactful events can also be identified for other types of configuration changes e.g. a new web page that causes a data center to receive great deal of new traffic within a certain IP address range.

As mentioned one metric that can be used for characterizing and prioritizing the impactful events in the impactful event subset is a traffic loss metric. In some implementations traffic loss can be expressed as a percentage. Several equations can be used to obtain traffic loss over a given device or link. For example the estimated average of median traffic loss per event for a device can be defined as 

Here the equation implies that the estimated average of median traffic loss of a given device per event can be represented by the sum over all events of the difference in median network traffic before and after an event multiplied by the duration of the failure divided by the total events contributed by the device to normalize the loss on a per event basis. To obtain this value the median traffic before and after each event can be computed to determine the median traffic loss and then the median traffic loss can be averaged for a given event. Note that in this context the term event can refer to any individual failure episode and can refer to a single or multiple individual device and or link failures that collectively relate to the event.

Here the equation implies that the loss for the device in a day is the sum for each event associated with that device on that day of the difference in median traffic before and after the event multiplied by the duration of the failure. Note also that traffic flow can be aggregated e.g. summed across links or devices in a redundant group.

As mentioned one metric that can be used for characterizing and prioritizing events is a latency inflation metric. Latency inflation can be due to buffering of packets in an individual device or link due to sustained high load bursty transient load or an interface on a given device exhibiting intermittent connectivity etc. In some implementations latency inflation can be expressed as a percentage e.g. an amount of latency for a given device or link has changed since a given event. For example a device may be averaging 15 milliseconds of latency and the latency may double to 30 milliseconds e.g. the ratio of the new latency to the old latency so the latency inflation metric in this case would be 200 . To determine device latency implementations may periodically poll different network devices and maintain latency statistics for each device.

As mentioned one metric that can be used for characterizing and prioritizing events is a redundancy failover metric indicating whether a failover within a redundant group of devices or links is successful. In some implementations redundancy failover can be expressed as a Boolean value e.g. a 1 indicating that redundancy failover was successful and a 0 indicating the failover was not successful. For example the aggregate traffic across a given redundant group of devices can be monitored. If the aggregate traffic across the redundant group after the event is sufficiently lower than the aggregate traffic before the event then the failover can be considered to have failed. Otherwise the failover can be considered to have succeeded. Note that in this context the aggregate traffic loss can be evaluated for statistical significance to determine whether the loss is caused by a given event or simply due to fluctuations in demand for a given application service.

Further implementations may use quantitative values to represent failover success. For example the percentage of the aggregate traffic after the event relative to the aggregate traffic before the event can be used as the redundancy failover metric instead of a Boolean value. Consider a first event on a first device where the aggregate traffic flow across a device is reduced to 90 of the pre event traffic and a second event on a second device where the aggregate flow across the device is reduced to 10 of the pre event traffic. Other things being equal the second event may be prioritized higher than the first event because the traffic loss percentage is much greater.

As mentioned one metric that can be used for characterizing and prioritizing events is an at risk redundancy metric. This metric can indicate whether a future failover within a redundant group of devices or links is at risk of failing e.g. because a given device or link is operating as a single point of failure. In some implementations at risk redundancy can be expressed as a Boolean value that indicates whether a redundant device group is at risk of future unsuccessful failovers. In one example the value of the at risk redundancy metric is 0 if there are at least two remaining operational devices in a redundancy group or 1 if there is one or fewer remaining operational devices in the redundancy group. In this case the at risk redundancy metric simply reflects whether the redundancy group is acting as a single point of failure. Analogously the value of the at risk redundancy metric can be 0 when there are at least two remaining operational links in a redundancy group or 1 if there is only a single link remaining in the redundancy group.

Further implementations may consider the extent to which redundancy groups with multiple functioning devices links can still be at risk. For example some redundantly configured devices may exhibit some statistical correlation in their failures as determined experimentally or by observation of deployed devices. Consider a first pair of redundant access routers that tends to fail together due to problems common to both of the access routers. For example both access routers may be of a first device model that has some characteristic that causes the access routers to fail together e.g. a bug in the software firmware sensitivity to power outages power spikes overload conditions etc. In contrast consider a second pair of access routers e.g. of a different model that is much less likely to fail together.

Under these circumstances it may be useful to treat the first pair of redundant devices as risky because the failures of the devices are correlated. Conversely it may be useful to treat the second pair of devices as not risky because the devices do not tend to fail together. Some implementations may compute the correlation e.g. Pearson correlation Spearman s rank correlation coefficient and or Kendall tau rank correlation coefficient between tickets outage minutes or traffic loss for devices within an individual redundancy group. The at risk redundancy metric can then be derived from the computed correlations. For example the correlations can be compared to a threshold and devices having a correlation exceeding the threshold can be identified as risky because these devices may tend to fail together e.g. failures with a temporal relationship such as overlapping failures or tending to fail in succession .

Considering total outage time one way to correlate outage time between two devices is as follows. First the total downtime of a device A and the total downtime of a device B are determined from filtered events as discussed above. Next the filtered events are also used to determine individual events where both device A and B were down together or at least partially overlapping in terms of their time window of being unavailable. If there is a strong negative correlation between failures of devices A and B e.g. Pearson coefficient close to 1 then the overlapping failures will tend to be relatively minimal relative to the individual failure rates of the device. Said differently the devices are less likely to fail together than would be the case if there were no correlation. If there is minimal or no correlation e.g. Pearson coefficient of approximately 0 then the devices tend to fail independently of one another e.g. the failure of one device has little or no apparent impact on whether the other device fails. If there is a strong correlation e.g. Pearson coefficient close to 1 there is a strong correlation and the devices are more likely to fail together than would be the case if the failures were independent.

This last case relatively high correlation can be problematic for devices in a redundant configuration for reasons already mentioned. Similar processing can be employed with respect to network tickets by determining the Pearson coefficient for support tickets e.g. determining the total number of tickets for each individual device and the intersection of these tickets that relate to both devices. Assuming an equivalent number of tickets smaller intersections imply lower correlations and larger intersections imply higher correlations. Correlations can also be determined for losses of network traffic by individual devices e.g. if traffic loss by one device in a redundant pair tends to be highly correlated to traffic loss by another device in the redundant pair the pair can be flagged as a risky device pair.

Note that another way to show correlation of redundant device failures is the fraction or percentage of time over a time window e.g. month year when all devices are down e.g. this fraction is 10 when both devices in a redundant pair are down at the same time for 3 days in a 30 day month. Individual devices may be down for more than 10 of the month e.g. 20 each but their overlapping downtime is 10 in a month. One simple example is that device A is down the first four days of a week and a device B is down the last four days of a week. Here their individual downtime percentage is 4 7 100 but the correlated downtime percentage is 1 7 100.

Note that the concepts discussed herein are not limited to the aforementioned failure metrics. For example the concepts discussed herein can readily be implemented by omitting replacing or adding to the aforementioned failure metrics. In addition as already mentioned the specific representations of the metrics discussed herein e.g. Boolean values or percentages can also be adapted to different data types or other alternative representations.

One specific example of an additional failure metric that can be used relates to whether inter data center redundancy is effective. For example consider and note that applications and are hosted at both data center and . Thus even assuming application is completely unavailable from data center it may be that users can still access application from data center . In a sense data centers and are serving as a redundant device group on a macro level in this example.

An inter data center redundancy metric may reflect whether inter data center redundancy has been effective at mitigating any failures. Thus if application is completely unavailable from data center but is available from data center the inter data center redundancy metric in this example could have a Boolean value of 1 indicating that application successfully failed over to data center . If application is completely unavailable from both data centers the inter data center redundancy metric can have a Boolean value of 0 indicating that application did not successfully fail over. Policies can be defined on this metric in a manner analogous to that discussed above e.g. any events related to the failure of application e.g. by devices situated between application and the network root in architecture may be prioritized ahead of events related to the failure of application .

Another example of a metric suitable for use with the disclosed concepts can quantify aggregated traffic loss over data centers. For example assume data center and normally split traffic for application evenly 50 50 . Now assume a failure event occurs at data center and the traffic is redistributed so that data center is only handling 20 of the traffic and data center is handling 80 of the traffic. In this instance an aggregate traffic loss metric may have a value of 0 indicating the total traffic loss is 0 across both data centers. This is the case when the application traffic through the redundant group as a whole is not affected by the event even though the event may be impactful at the device level for the failing device.

In contrast assume the traffic is redistributed as 20 for data center and 60 for data center . In this example only 80 of the total traffic is being handled so the aggregate traffic loss metric may have a value of 20 indicating there is 20 aggregate traffic loss. Note that some implementations may use other representations to quantify the extent of aggregate traffic loss instead of percentages.

Another example of metric suitable for use with the disclosed concepts is a metric reflecting the level within the data center hierarchy of a failing device. In the following example policies the level of the failing device is used for ranking purposes. Generally it can be useful to prioritize events closer to the root because these devices are responsible for carrying traffic from more downstream devices.

The following examples show how the impactful event subset can be prioritized to obtain prioritized events as shown in . Generally the events in the impactful event subset are prioritized by applying one or more policies having one or more criteria. Each policy can also rank individual events that meet the criteria for that policy.

The events in the impactful event subset are first processed by policy . Policy relates to events that satisfy a single criterion the events are by failing devices that were not handled by a successful failover. In other words events that match policy are events where there are no other redundantly configured devices that managed to take over for the failing device. Policy ranks any such events by their proximity to the root of architecture e.g. core routers can be viewed as the root of the architecture and racks as leaves of the architecture.

Of the six events shown in impactful event subset two events do not have successful failovers events and . Event is at access router which is in L3 aggregation layer . Event is at aggregation switch which is at L2 aggregation layer . Thus event is ranked higher than event by policy .

The remaining four events in the impactful event subset are next processed by policy . Policy relates to events that satisfy two criteria the events have successful failovers but at risk redundancy. Policy ranks any events that meet these criteria by their proximity to the root of architecture in a manner similar to that discussed above with respect to policy .

Of the four events remaining in the impactful event subset events and two events meet the criteria specified by policy events and . Event is at access router which is in L3 aggregation layer . Event is at aggregation switch which is at L2 aggregation layer . Thus event is ranked higher than event by policy .

The remaining two events in the impactful event subset are next processed by policy . Policy relates to events that satisfy two criteria the events have successful failovers and no at risk redundancy. Policy ranks any events that meet these criteria by their traffic loss percentages such that events with higher traffic loss percentages are prioritized more highly than events with lower traffic loss percentages.

Both of the two events remaining in the impactful event subset events and meet the criteria specified by policy . Event is at aggregation switch and has a traffic loss of 58 . Event is at access router and has a traffic loss of 32 . Thus event is ranked higher than event . Note that policy in this example ranks an event at the L2 layer higher than an event at the L3 layer which was not the case for policies and .

Note that the examples discussed above are merely exemplary and policies criteria can be defined in many different ways. For example some implementations may prioritize events based on the aforementioned latency metric as well e.g. events relating to devices with relatively higher latency may be prioritized higher than events relating to devices with relatively lower latency. For devices with similar latency the events may be prioritized based on proximity to the root as discussed above. For example individual events could be sorted into latency bins e.g. 0 10 latency inflation 11 20 latency inflation etc. and the events within each bin could be prioritized based on proximity to the root. As another example a metric could be derived from both latency inflation and proximity to the root e.g. a mathematical function that uses both of these failure metrics as variables and outputs a value that is used to rank the events.

Note that some policies may also include associated automated tasks that are performed when certain event criteria are met. For example an example policy may have criteria that if an event shows an access router failure with 100 traffic loss across the access router an automatic reboot of the access router is performed. Other policies may specify disabling one or more ports associated with the event deactivating one or more faulty links rebooting reconfiguring one or more switches etc.

Policies may also be defined based on time to repair a given failure. For example consider two events that have similar consequences but the first event typically takes far longer to repair than the second event. In this case it may be useful to prioritize the second event so that at least the corresponding failure is resolved relatively quickly. As a specific example handling the second event may involve a software reboot of the device that generated the second event whereas handling the first event may involve hardware component replacement on the device that generated the first event e.g. a line card . Here it can be useful to prioritize the second event higher than the first event because the software reboot may take less time than the line card replacement.

Policies may also use criteria that are based on the application or service that is potentially impacted for a given event e.g. based on event property . For example application may be a particularly high priority application e.g. a real time service with paying subscribers subject to strict service level agreement constraints. On the other hand application may be a relatively low priority application e.g. a free version of the service where there are no contractual obligations involved. In this example events that potentially impact application e.g. events and may be ranked higher than events that impact application e.g. events and .

For example in some implementations events that meet certain redundancy criteria can be ranked based on which applications are potentially affected by the events. One policy might specify that events with unsuccessful failovers are ranked by application e.g. events associated with application ranked higher than events associated with application and events associated with application ranked higher than events associated with application . Another policy might rank events with successful failovers and at risk redundancy in a similar fashion.

Policies may also prioritize events based on performance criteria relating to hardware performance issues. As mentioned above some events may be associated with high CPU or memory utilization on a given network device. One example policy might rank events associated with greater than 95 CPU utilization according to levels of the network hierarchy e.g. so that an event associated with 96 CPU utilization by an access router may be ranked higher than an event associated with 99 CPU utilization by an aggregation switch. Note that further implementations may use such performance criteria for event prioritization in conjunction with other criteria mentioned herein.

Likewise policies may prioritize events based on associated configuration changes. For example events associated with a configuration change that blocks incoming traffic from a first set of IP addresses e.g. a major customer may be ranked above events associated with another configuration change that blocks incoming traffic from a different set of IP addresses e.g. a less important customer . Likewise events associated with packets having one destination address e.g. a web page specifically for a major customer may be ranked above other events associated with packets having a destination address for another web page that is utilized by another customer.

Note also that while the examples above discussed with respect to discuss device level metrics analogous processing can be performed on link level metrics. For example policies may have criteria relating to link level traffic loss or inflation. Likewise policies may have criteria relating to redundant failovers or at risk redundancy for given links.

As mentioned above block of method can involve outputting events e.g. by one or more visualizations. A first example visualization is shown in GUI screenshot of listing prioritized events in order of priority from top to bottom.

Circles and squares with a horizontal vertical pattern represent events from data center and circles and squares with a crosshatched diagonal pattern represent events from data center . The X axis of event graph area is a time axis starting with earlier times to the left. Thus the two most recent events shown in are closed events at data center and the earliest event is an open event from data center . The Y axis of event graph area represents both which data center a particular event is from and which device generated the event. Note that the event graph area can also visually represent individual links that failed e.g. instead of or in addition to representing the devices.

Events from data center are shown in a lower horizontal band and events from data center are shown in an upper horizontal band note additional bands may be added for additional data centers . Within each band the relative vertical position corresponds to the particular device that generated the corresponding event. Thus note the events in horizontal band are at the same vertical position implying that the same device in data center generated each event. In contrast the events in horizontal band are in different vertical positions implying that different devices in data center generated the events. Note that one specific alternative implementation can order the Y axis by relative priority of the events e.g. ordered by method as discussed above.

Device information area shows information about a device that generated a given event. For example if a user clicks on or otherwise selects an individual event from event graph area the device information area can be populated with information about the device that generated the event. Here the information includes the device name platform e.g. brand model data center where the device is located property affected application or service and device type. Some implementations use metadata about what devices are installed in various data centers to populate the device information area. For example the metadata can be maintained by a network inventory management system. Analogously visualization can show information about links associated with individual events e.g. a link identifier connected devices an impacted application or service etc.

Connectivity area shows connectivity information for the device that generated the selected event. For example the connectivity information can illustrate some or all of the directly connected neighbors of the device that generated the selected event. This can allow the user to see other devices that might also be impacted by the event. In further implementations users can click an individual connected device in the connectivity area to pull up device information about that connected device in the device information area .

Event ticket history area shows events and or maintenance tickets for the device that generated the selected event. The X axis of the event ticket history area represents time and the Y axis of the event ticket history area represents the number of events tickets for that device at a given time. In some implementations users can also select individual events tickets from the event ticket history area to view details about the selected event or ticket.

The device information area can include specific information such as the name of the device that failed and the name of the data center where that device is located. Further implementations can also include other information such as the name of specific links that caused the device to fail. Other device related information can also be displayed in the device information area e.g. software and or hardware revisions configuration settings etc.

The event information area can include information such as the traffic through a given device before an event occurred expressed in megabytes per second MBps or megabits per second Mbps . Likewise the event information area can include information such as the traffic through the device during the duration of the event. In this example traffic dropped from approximately 1900 MBps to 0 MBps during the event meaning that the device essentially stopped communicating traffic.

The event information area can also indicate the amount of estimated traffic that has been lost due to the event expressed in Gigabytes GB . The event information area can also indicate whether the corresponding event is open or closed in this case the event is closed. The duration represents the amount of time the event was open in this case 9.25 hours. The event information area can also indicate whether the event triggered a successful failover in this case the failover was successful.

The property impact area indicates various other device types that were impacted by the event. For each device type corresponding properties and numbers of devices are shown. The property name identifies applications and or services in the subtree of the device that generated the event. In this example application is in the subtree of the event and a firewall two aggregation switches a load balancer and a ToR in the subtree of access router are affected by the event.

The above introduced failure metrics generally fall into two categories e.g. device or link level metrics describing latency inflation traffic loss at an individual device and redundancy level metrics describing the effectiveness of redundancy at mitigating failures. Some further implementations provide functional metrics that characterize individual events based on whether the events have impacted proper functioning of a given device.

To facilitate the discussion of functional failure metrics shows architecture in an alternative configuration that includes intrusion detection and prevention systems IDPS and virtual private networks VPNs and firewalls FWs and and load balancers LBs and . The functionality of load balancers firewalls and VPNs are discussed above with respect to .

Consider a situation where incoming traffic is configured to be routed through firewall before being received by either aggregation switch and . In other words the firewall is responsible for filtering out potential attack traffic so that the potential attack traffic does not reach either aggregation switch. Now consider an event that causes some traffic to go directly to aggregation switch and or without passing through the firewall. Here there is a functional problem that may not be captured by the previously introduced metrics e.g. that the traffic is being forwarded incorrectly within the data center.

One example of a functional failure metric that can capture this scenario is a security threat metric indicating whether an event is associated with a security threat. For example a Boolean value can be used with a 1 indicating that a given event is associated with a security threat and a 0 indicating that there is no security threat. In the above example where traffic is bypassing the firewall such a security threat metric could have a value of 1. Further implementations may characterize security threats using more refined values e.g. a scale of 1 10 representing the severity of the threat.

Another functional failure metric indicates whether traffic is being incorrectly forwarded to a device e.g. unnecessarily. For example suppose firewall is only responsible for traffic associated with application and firewall is only responsible for traffic associated with application . Further suppose both firewalls are configured to perform similar or identical processing e.g. packets filtered by one firewall would be filtered by the other firewall as well. An event that causes traffic destined for application to be forwarded to both firewall and firewall may not be an immediate security concern if firewall is able to handle the traffic and will filter the traffic appropriately. Nevertheless this traffic flow is functionally incorrect in the sense that the traffic within data center is flowing through an unintended device firewall . Thus some implementations may provide a traffic rule violation metric e.g. a Boolean value that indicates whether traffic flows through a path that violates predefined configuration rules for the data center. In this example the traffic rule violation metric could have a value of 1 for an event that causes firewall to receive extra traffic but the security threat metric would have a value of 0 since there is no apparent security concern caused by the event. In contrast an event that causes traffic to bypass both firewalls entirely could have a value of 1 for both the traffic rule violation metric and the security threat metric.

Some implementations may prioritize individual events based on combinations of one or more functional failure metrics perhaps in conjunction with device level link level or redundancy related failure metrics. For example one policy could specify that events with security threats are prioritized over events without security threats irrespective of traffic loss latency redundancy or other failure metrics of the events. Of the events with security threats the events can be ranked based on proximity to the root of the network. Another policy could prioritize individual events that have traffic rule violations but not security threats in a similar manner. Yet another policy could prioritize events for a particular class of applications e.g. applications regarded as having business critical or confidential data.

First policy ranks events with threats by proximity to the root of the network. Since events and meet the security threat criterion these two events are ranked relative to each other by this policy. Here event is at an access router and event is at an aggregation switch. Thus event is closer to the root of data center and is ranked higher than event . Note that this can be the case even though event lost more traffic than event .

Next policy ranks events with traffic rule violations by proximity to the root. Of the remaining events and events and meet the traffic rule violation criterion so these two events are ranked relative to each other by this policy. Here event is ranked higher than event for reasons similar to those introduced above e.g. event was generated by a device access router that is closer to the root than the device that generated event aggregation switch .

Next policy ranks events without threats or traffic rule violations by traffic loss. Remaining event lost more total traffic 20 GB than remaining event 10 GB so event is ranked higher than event . After policy is applied each of the events in impactful event subset has been ranked as shown in prioritized events .

In some implementations the disclosed event analysis component can be provided as a tool that is intended to be deployed within a data center or to manage multiple data centers. For example an entity that operates data centers and could deploy such a tool in a single location that manages both data centers. In this case both data centers can provide their events to the tool and the tool can output a list of prioritized events that includes events from both data centers. In other implementations copies of the tool can be separately deployed at each data center. In this case each data center can have a separate list of ranked events.

Further implementations may also provide such a tool as a network service e.g. a third party may operate analysis device . The entity operating data centers and can provide events to the third party analysis device and use application interface to view various visualizations or other outputs mentioned herein.

The above description provides some ways in which the disclosed concepts are employed e.g. in data centers. However the concepts discussed herein are applicable in environments other than data centers and are not necessarily tied to the hierarchical network topology shown herein. Rather the disclosed implementations can be used in both data center and enterprise environments Internet Service Providers or other contexts where it is useful to understand the impact of various failure events. Note that the algorithmic details mentioned above can be modified to accommodate network topologies that are not hierarchical in nature or that are not organized in different levels.

The order in which the disclosed methods are described is not intended to be construed as a limitation and any number of the described blocks can be combined in any order to implement the method or an alternate method. Furthermore the methods can be implemented in any suitable hardware software firmware or combination thereof such that a computing device can implement the method. In one case the methods are stored on one or more computer readable storage media as a set of instructions such that execution by a processor of a computing device causes the computing device to perform the method.

Although techniques methods devices systems etc. pertaining to characterizing service levels are described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as exemplary forms of implementing the claimed methods devices systems etc.

