---

title: Selectively transferring high-priority non-audio data over a quality of service channel
abstract: In an embodiment, a transmitting UE is engaged with a target UE in a communication session supported at least in part via a QoS channel on which audio traffic is primarily carried and a non-QoS channel on which non-audio traffic is carried. The transmitting UE obtains audio data and non-audio data for transmission to the target UE during the communication session, and identifies a subset of higher-priority non-audio data within the obtained non-audio data. The transmitting UE transmits a stream of packets including both the audio data and the subset of higher-priority audio data over the QoS channel instead of the non-QoS channel based on the identification. The target UE receives the stream of packets on the QoS channel, and the target UE identifies and extracts the audio data and the higher-priority non-audio data. After extraction, the target UE plays the audio data and processes the higher-priority non-audio data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09444746&OS=09444746&RS=09444746
owner: QUALCOMM Incorporated
number: 09444746
owner_city: San Diego
owner_country: US
publication_date: 20130625
---
Embodiments of the invention relate to selectively transferring high priority non audio data over a Quality of Service QoS channel.

Wireless communication systems have developed through various generations including a first generation analog wireless phone service 1G a second generation 2G digital wireless phone service including interim 2.5G and 2.75G networks and third generation 3G and fourth generation 4G high speed data Internet capable wireless services. There are presently many different types of wireless communication systems in use including Cellular and Personal Communications Service PCS systems. Examples of known cellular systems include the cellular Analog Advanced Mobile Phone System AMPS and digital cellular systems based on Code Division Multiple Access CDMA Frequency Division Multiple Access FDMA Time Division Multiple Access TDMA the Global System for Mobile access GSM variation of TDMA and newer hybrid digital communication systems using both TDMA and CDMA technologies.

More recently Long Term Evolution LTE has been developed as a wireless communications protocol for wireless communication of high speed data for mobile phones and other data terminals. LTE is based on GSM and includes contributions from various GSM related protocols such as Enhanced Data rates for GSM Evolution EDGE and Universal Mobile Telecommunications System UMTS protocols such as High Speed Packet Access HSPA .

User application services involving concurrent transmission of voice and some other form of media real time or non real time are typically implemented by sending voice media as a separate independent stream. This is done for several reasons including providing positive user experience allocating preferential treatment to voice packets e.g. Quality of Service QoS etc. . When voice packets are transmitted concurrently with real time media such as video e.g. in a video conference for example the video stream is typically allocated a best effort BE without QoS. Thus depending upon network conditions the video packets may suffer packet loss jitter and or delay. This results in an inconsistent user experience.

In an embodiment a transmitting user equipment UE is engaged with a target UE in a communication session supported at least in part via a Quality of Service QoS channel on which audio traffic is primarily carried and a non QoS channel on which non audio traffic is carried. The transmitting UE obtains audio data and non audio data for transmission to the target UE during the communication session and identifies a subset of higher priority non audio data within the obtained non audio data. The transmitting UE transmits a stream of packets including both the audio data and the subset of higher priority audio data over the QoS channel instead of the non QoS channel based on the identification. The target UE receives the stream of packets on the QoS channel and the target UE identifies and extracts the audio data and the higher priority non audio data. After extraction the target UE plays the audio data and processes the higher priority non audio data.

Aspects of the invention are disclosed in the following description and related drawings directed to specific embodiments of the invention. Alternate embodiments may be devised without departing from the scope of the invention. Additionally well known elements of the invention will not be described in detail or will be omitted so as not to obscure the relevant details of the invention.

The words exemplary and or example are used herein to mean serving as an example instance or illustration. Any embodiment described herein as exemplary and or example is not necessarily to be construed as preferred or advantageous over other embodiments. Likewise the term embodiments of the invention does not require that all embodiments of the invention include the discussed feature advantage or mode of operation.

Further many embodiments are described in terms of sequences of actions to be performed by for example elements of a computing device. It will be recognized that various actions described herein can be performed by specific circuits e.g. application specific integrated circuits ASICs by program instructions being executed by one or more processors or by a combination of both. Additionally these sequence of actions described herein can be considered to be embodied entirely within any form of computer readable storage medium having stored therein a corresponding set of computer instructions that upon execution would cause an associated processor to perform the functionality described herein. Thus the various aspects of the invention may be embodied in a number of different forms all of which have been contemplated to be within the scope of the claimed subject matter. In addition for each of the embodiments described herein the corresponding form of any such embodiments may be described herein as for example logic configured to perform the described action.

A client device referred to herein as a user equipment UE may be mobile or stationary and may communicate with a radio access network RAN . As used herein the term UE may be referred to interchangeably as an access terminal or AT a wireless device a subscriber device a subscriber terminal a subscriber station a user terminal or UT a mobile terminal a mobile station and variations thereof. Generally UEs can communicate with a core network via the RAN and through the core network the UEs can be connected with external networks such as the Internet. Of course other mechanisms of connecting to the core network and or the Internet are also possible for the UEs such as over wired access networks WiFi networks e.g. based on IEEE 802.11 etc. and so on. UEs can be embodied by any of a number of types of devices including but not limited to PC cards compact flash devices external or internal modems wireless or wireline phones and so on. A communication link through which UEs can send signals to the RAN is called an uplink channel e.g. a reverse traffic channel a reverse control channel an access channel etc. . A communication link through which the RAN can send signals to UEs is called a downlink or forward link channel e.g. a paging channel a control channel a broadcast channel a forward traffic channel etc. . As used herein the term traffic channel TCH can refer to either an uplink reverse or downlink forward traffic channel.

Referring to UEs 1 . . . N are configured to communicate with an access network e.g. the RAN an access point etc. over a physical communications interface or layer shown in as air interfaces and or a direct wired connection. The air interfaces and can comply with a given cellular communications protocol e.g. CDMA EVDO eHRPD GSM EDGE W CDMA LTE etc. while the air interface can comply with a wireless IP protocol e.g. IEEE 802.11 . The RAN includes a plurality of access points that serve UEs over air interfaces such as the air interfaces and . The access points in the RAN can be referred to as access nodes or ANs access points or APs base stations or BSs Node Bs eNode Bs and so on. These access points can be terrestrial access points or ground stations or satellite access points. The RAN is configured to connect to a core network that can perform a variety of functions including bridging circuit switched CS calls between UEs served by the RAN and other UEs served by the RAN or a different RAN altogether and can also mediate an exchange of packet switched PS data with external networks such as Internet . The Internet includes a number of routing agents and processing agents not shown in for the sake of convenience . In UE N is shown as connecting to the Internet directly i.e. separate from the core network such as over an Ethernet connection of WiFi or 802.11 based network . The Internet can thereby function to bridge packet switched data communications between UE N and UEs 1 . . . N via the core network . Also shown in is the access point that is separate from the RAN . The access point may be connected to the Internet independent of the core network e.g. via an optical communication system such as FiOS a cable modem etc. . The air interface may serve UE 4 or UE 5 over a local wireless connection such as IEEE 802.11 in an example. UE N is shown as a desktop computer with a wired connection to the Internet such as a direct connection to a modem or router which can correspond to the access point itself in an example e.g. for a WiFi router with both wired and wireless connectivity .

Referring to an application server is shown as connected to the Internet the core network or both. The application server can be implemented as a plurality of structurally separate servers or alternately may correspond to a single server. As will be described below in more detail the application server is configured to support one or more communication services e.g. Voice over Internet Protocol VoIP sessions Push to Talk PTT sessions group communication sessions social networking services etc. for UEs that can connect to the application server via the core network and or the Internet .

Examples of protocol specific implementations for the RAN and the core network are provided below with respect to to help explain the wireless communications system in more detail. In particular the components of the RAN and the core network corresponds to components associated with supporting packet switched PS communications whereby legacy circuit switched CS components may also be present in these networks but any legacy CS specific components are not shown explicitly in .

In the core network includes the above noted SGSN B and potentially a number of other SGSNs as well and a GGSN B. Generally GPRS is a protocol used in GSM for routing IP packets. The GPRS core network e.g. the GGSN B and one or more SGSNs B is the centralized part of the GPRS system and also provides support for W CDMA based 3G access networks. The GPRS core network is an integrated part of the GSM core network i.e. the core network that provides mobility management session management and transport for IP packet services in GSM and W CDMA networks.

The GPRS Tunneling Protocol GTP is the defining IP protocol of the GPRS core network. The GTP is the protocol which allows end users e.g. UEs of a GSM or W CDMA network to move from place to place while continuing to connect to the Internet as if from one location at the GGSN B. This is achieved by transferring the respective UE s data from the UE s current SGSN B to the GGSN B which is handling the respective UE s session.

Three forms of GTP are used by the GPRS core network namely i GTP U ii GTP C and iii GTP GTP Prime . GTP U is used for transfer of user data in separated tunnels for each packet data protocol PDP context. GTP C is used for control signaling e.g. setup and deletion of PDP contexts verification of GSN reach ability updates or modifications such as when a subscriber moves from one SGSN to another etc. . GTP is used for transfer of charging data from GSNs to a charging function.

Referring to the GGSN B acts as an interface between a GPRS backbone network not shown and the Internet . The GGSN B extracts packet data with associated a packet data protocol PDP format e.g. IP or PPP from GPRS packets coming from the SGSN B and sends the packets out on a corresponding packet data network. In the other direction the incoming data packets are directed by the GGSN connected UE to the SGSN B which manages and controls the Radio Access Bearer RAB of a target UE served by the RAN . Thereby the GGSN B stores the current SGSN address of the target UE and its associated profile in a location register e.g. within a PDP context . The GGSN B is responsible for IP address assignment and is the default router for a connected UE. The GGSN B also performs authentication and charging functions.

The SGSN B is representative of one of many SGSNs within the core network in an example. Each SGSN is responsible for the delivery of data packets from and to the UEs within an associated geographical service area. The tasks of the SGSN B includes packet routing and transfer mobility management e.g. attach detach and location management logical link management and authentication and charging functions. The location register of the SGSN B stores location information e.g. current cell current VLR and user profiles e.g. IMSI PDP address es used in the packet data network of all GPRS users registered with the SGSN B for example within one or more PDP contexts for each user or UE. Thus SGSNs B are responsible for i de tunneling downlink GTP packets from the GGSN B ii uplink tunnel IP packets toward the GGSN B iii carrying out mobility management as UEs move between SGSN service areas and iv billing mobile subscribers. As will be appreciated by one of ordinary skill in the art aside from i iv SGSNs configured for GSM EDGE networks have slightly different functionality as compared to SGSNs configured for W CDMA networks.

The RAN e.g. or UTRAN in UMTS system architecture communicates with the SGSN B via a Radio Access Network Application Part RANAP protocol. RANAP operates over a Iu interface Iu ps with a transmission protocol such as Frame Relay or IP. The SGSN B communicates with the GGSN B via a Gn interface which is an IP based interface between SGSN B and other SGSNs not shown and internal GGSNs not shown and uses the GTP protocol defined above e.g. GTP U GTP C GTP etc. . In the embodiment of the Gn between the SGSN B and the GGSN B carries both the GTP C and the GTP U. While not shown in the Gn interface is also used by the Domain Name System DNS . The GGSN B is connected to a Public Data Network PDN not shown and in turn to the Internet via a Gi interface with IP protocols either directly or through a Wireless Application Protocol WAP gateway.

In the core network includes a plurality of Mobility Management Entities MMEs D and D a Home Subscriber Server HSS D a Serving Gateway S GW D a Packet Data Network Gateway P GW D and a Policy and Charging Rules Function PCRF D. Network interfaces between these components the RAN and the Internet are illustrated in and are defined in Table 1 below as follows 

A high level description of the components shown in the RAN and core network of will now be described. However these components are each well known in the art from various 3GPP TS standards and the description contained herein is not intended to be an exhaustive description of all functionalities performed by these components.

Referring to the MMEs D and D are configured to manage the control plane signaling for the EPS bearers. MME functions include Non Access Stratum NAS signaling NAS signaling security Mobility management for inter and intra technology handovers P GW and S GW selection and MME selection for handovers with MME change.

Referring to the S GW D is the gateway that terminates the interface toward the RAN . For each UE associated with the core network for an EPS based system at a given point of time there is a single S GW. The functions of the S GW D for both the GTP based and the Proxy Mobile IPv6 PMIP based S5 S8 include Mobility anchor point Packet routing and forwarding and setting the DiffSery Code Point DSCP based on a QoS Class Identifier QCI of the associated EPS bearer.

Referring to the P GW D is the gateway that terminates the SGi interface toward the Packet Data Network PDN e.g. the Internet . If a UE is accessing multiple PDNs there may be more than one P GW for that UE however a mix of S5 S8 connectivity and Gn Gp connectivity is not typically supported for that UE simultaneously. P GW functions include for both the GTP based S5 S8 Packet filtering by deep packet inspection UE IP address allocation setting the DSCP based on the QCI of the associated EPS bearer accounting for inter operator charging uplink UL and downlink DL bearer binding as defined in 3GPP TS 23.203 UL bearer binding verification as defined in 3GPP TS 23.203. The P GW D provides PDN connectivity to both GSM EDGE Radio Access Network GERAN UTRAN only UEs and E UTRAN capable UEs using any of E UTRAN GERAN or UTRAN. The P GW D provides PDN connectivity to E UTRAN capable UEs using E UTRAN only over the S5 S8 interface.

Referring to the PCRF D is the policy and charging control element of the EPS based core network . In a non roaming scenario there is a single PCRF in the HPLMN associated with a UE s Internet Protocol Connectivity Access Network IP CAN session. The PCRF terminates the Rx interface and the Gx interface. In a roaming scenario with local breakout of traffic there may be two PCRFs associated with a UE s IP CAN session A Home PCRF H PCRF is a PCRF that resides within a HPLMN and a Visited PCRF V PCRF is a PCRF that resides within a visited VPLMN. PCRF is described in more detail in 3GPP TS 23.203 and as such will not be described further for the sake of brevity. In the application server e.g. which can be referred to as the AF in 3GPP terminology is shown as connected to the core network via the Internet or alternatively to the PCRF D directly via an Rx interface. Generally the application server or AF is an element offering applications that use IP bearer resources with the core network e.g. UMTS PS domain GPRS domain resources LTE PS data services . One example of an application function is the Proxy Call Session Control Function P CSCF of the IP Multimedia Subsystem IMS Core Network sub system. The AF uses the Rx reference point to provide session information to the PCRF D. Any other application server offering IP data services over cellular network can also be connected to the PCRF D via the Rx reference point.

In the eHRPD RAN includes a plurality of base transceiver stations BTSs E E and E which are connected to an enhanced BSC eBSC and enhanced PCF ePCF E. The eBSC ePCF E can connect to one of the MMEs D or D within the EPS core network A over an S101 interface and to an HRPD serving gateway HSGW E over A10 and or A11 interfaces for interfacing with other entities in the EPS core network A e.g. the S GW D over an S103 interface the P GW D over an S2a interface the PCRF D over a Gxa interface a 3GPP AAA server not shown explicitly in over an STa interface etc. . The HSGW E is defined in 3GPP2 to provide the interworking between HRPD networks and EPS LTE networks. As will be appreciated the eHRPD RAN and the HSGW E are configured with interface functionality to EPC LTE networks that is not available in legacy HRPD networks.

Turning back to the eHRPD RAN in addition to interfacing with the EPS LTE network A the eHRPD RAN can also interface with legacy HRPD networks such as HRPD network B. As will be appreciated the HRPD network B is an example implementation of a legacy HRPD network such as the EV DO network from . For example the eBSC ePCF E can interface with an authentication authorization and accounting AAA server E via an A12 interface or to a PDSN FA E via an A10 or A11 interface. The PDSN FA E in turn connects to HA A through which the Internet can be accessed. In certain interfaces e.g. A13 A16 H1 H2 etc. are not described explicitly but are shown for completeness and would be understood by one of ordinary skill in the art familiar with HRPD or eHRPD.

Referring to it will be appreciated that LTE core networks e.g. and HRPD core networks that interface with eHRPD RANs and HSGWs e.g. can support network initiated Quality of Service QoS e.g. by the P GW GGSN SGSN etc. in certain cases.

While internal components of UEs such as the UEs A and B can be embodied with different hardware configurations a basic high level UE configuration for internal hardware components is shown as platform in . The platform can receive and execute software applications data and or commands transmitted from the RAN that may ultimately come from the core network the Internet and or other remote servers and networks e.g. application server web URLs etc. . The platform can also independently execute locally stored applications without RAN interaction. The platform can include a transceiver operably coupled to an application specific integrated circuit ASIC or other processor microprocessor logic circuit or other data processing device. The ASIC or other processor executes the application programming interface API layer that interfaces with any resident programs in the memory of the wireless device. The memory can be comprised of read only or random access memory RAM and ROM EEPROM flash cards or any memory common to computer platforms. The platform also can include a local database that can store applications not actively used in memory as well as other data. The local database is typically a flash memory cell but can be any secondary storage device as known in the art such as magnetic media EEPROM optical media tape soft or hard disk or the like.

Accordingly an embodiment of the invention can include a UE e.g. UE A B etc. including the ability to perform the functions described herein. As will be appreciated by those skilled in the art the various logic elements can be embodied in discrete elements software modules executed on a processor or any combination of software and hardware to achieve the functionality disclosed herein. For example ASIC memory API and local database may all be used cooperatively to load store and execute the various functions disclosed herein and thus the logic to perform these functions may be distributed over various elements. Alternatively the functionality could be incorporated into one discrete component. Therefore the features of the UEs A and B in are to be considered merely illustrative and the invention is not limited to the illustrated features or arrangement.

The wireless communication between the UEs A and or B and the RAN can be based on different technologies such as CDMA W CDMA time division multiple access TDMA frequency division multiple access FDMA Orthogonal Frequency Division Multiplexing OFDM GSM or other protocols that may be used in a wireless communications network or a data communications network. As discussed in the foregoing and known in the art voice transmission and or data can be transmitted to the UEs from the RAN using a variety of networks and configurations. Accordingly the illustrations provided herein are not intended to limit the embodiments of the invention and are merely to aid in the description of aspects of embodiments of the invention.

Referring to the communication device includes logic configured to receive and or transmit information . In an example if the communication device corresponds to a wireless communications device e.g. UE A or B one of BSs A through A one of Node Bs B through B one of eNodeBs D through D etc. the logic configured to receive and or transmit information can include a wireless communications interface e.g. Bluetooth WiFi 2G CDMA W CDMA 3G 4G LTE etc. such as a wireless transceiver and associated hardware e.g. an RF antenna a MODEM a modulator and or demodulator etc. . In another example the logic configured to receive and or transmit information can correspond to a wired communications interface e.g. a serial connection a USB or Firewire connection an Ethernet connection through which the Internet can be accessed etc. . Thus if the communication device corresponds to some type of network based server e.g. PDSN SGSN GGSN S GW P GW MME HSS PCRF the application etc. the logic configured to receive and or transmit information can correspond to an Ethernet card in an example that connects the network based server to other communication entities via an Ethernet protocol. In a further example the logic configured to receive and or transmit information can include sensory or measurement hardware by which the communication device can monitor its local environment e.g. an accelerometer a temperature sensor a light sensor an antenna for monitoring local RF signals etc. . The logic configured to receive and or transmit information can also include software that when executed permits the associated hardware of the logic configured to receive and or transmit information to perform its reception and or transmission function s . However the logic configured to receive and or transmit information does not correspond to software alone and the logic configured to receive and or transmit information relies at least in part upon hardware to achieve its functionality.

Referring to the communication device further includes logic configured to process information . In an example the logic configured to process information can include at least a processor. Example implementations of the type of processing that can be performed by the logic configured to process information includes but is not limited to performing determinations establishing connections making selections between different information options performing evaluations related to data interacting with sensors coupled to the communication device to perform measurement operations converting information from one format to another e.g. between different protocols such as .wmv to .avi etc. and so on. For example the processor included in the logic configured to process information can correspond to a general purpose processor a digital signal processor DSP an ASIC a field programmable gate array FPGA or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof designed to perform the functions described herein. A general purpose processor may be a microprocessor but in the alternative the processor may be any conventional processor controller microcontroller or state machine. A processor may also be implemented as a combination of computing devices e.g. a combination of a DSP and a microprocessor a plurality of microprocessors one or more microprocessors in conjunction with a DSP core or any other such configuration. The logic configured to process information can also include software that when executed permits the associated hardware of the logic configured to process information to perform its processing function s . However the logic configured to process information does not correspond to software alone and the logic configured to process information relies at least in part upon hardware to achieve its functionality.

Referring to the communication device further includes logic configured to store information . In an example the logic configured to store information can include at least a non transitory memory and associated hardware e.g. a memory controller etc. . For example the non transitory memory included in the logic configured to store information can correspond to RAM memory flash memory ROM memory EPROM memory EEPROM memory registers hard disk a removable disk a CD ROM or any other form of storage medium known in the art. The logic configured to store information can also include software that when executed permits the associated hardware of the logic configured to store information to perform its storage function s . However the logic configured to store information does not correspond to software alone and the logic configured to store information relies at least in part upon hardware to achieve its functionality.

Referring to the communication device further optionally includes logic configured to present information . In an example the logic configured to present information can include at least an output device and associated hardware. For example the output device can include a video output device e.g. a display screen a port that can carry video information such as USB HDMI etc. an audio output device e.g. speakers a port that can carry audio information such as a microphone jack USB HDMI etc. a vibration device and or any other device by which information can be formatted for output or actually outputted by a user or operator of the communication device . For example if the communication device corresponds to UE A or UE B as shown in the logic configured to present information can include the display A of UE A or the touchscreen display B of UE B. In a further example the logic configured to present information can be omitted for certain communication devices such as network communication devices that do not have a local user e.g. network switches or routers remote servers etc. . The logic configured to present information can also include software that when executed permits the associated hardware of the logic configured to present information to perform its presentation function s . However the logic configured to present information does not correspond to software alone and the logic configured to present information relies at least in part upon hardware to achieve its functionality.

Referring to the communication device further optionally includes logic configured to receive local user input . In an example the logic configured to receive local user input can include at least a user input device and associated hardware. For example the user input device can include buttons a touchscreen display a keyboard a camera an audio input device e.g. a microphone or a port that can carry audio information such as a microphone jack etc. and or any other device by which information can be received from a user or operator of the communication device . For example if the communication device corresponds to UE A or UE B as shown in the logic configured to receive local user input can include the keypad A any of the buttons A or B through B the touchscreen display B etc. In a further example the logic configured to receive local user input can be omitted for certain communication devices such as network communication devices that do not have a local user e.g. network switches or routers remote servers etc. . The logic configured to receive local user input can also include software that when executed permits the associated hardware of the logic configured to receive local user input to perform its input reception function s . However the logic configured to receive local user input does not correspond to software alone and the logic configured to receive local user input relies at least in part upon hardware to achieve its functionality.

Referring to while the configured logics of through are shown as separate or distinct blocks in it will be appreciated that the hardware and or software by which the respective configured logic performs its functionality can overlap in part. For example any software used to facilitate the functionality of the configured logics of through can be stored in the non transitory memory associated with the logic configured to store information such that the configured logics of through each performs their functionality i.e. in this case software execution based in part upon the operation of software stored by the logic configured to store information . Likewise hardware that is directly associated with one of the configured logics can be borrowed or used by other configured logics from time to time. For example the processor of the logic configured to process information can format data into an appropriate format before being transmitted by the logic configured to receive and or transmit information such that the logic configured to receive and or transmit information performs its functionality i.e. in this case transmission of data based in part upon the operation of hardware i.e. the processor associated with the logic configured to process information .

Generally unless stated otherwise explicitly the phrase logic configured to as used throughout this disclosure is intended to invoke an embodiment that is at least partially implemented with hardware and is not intended to map to software only implementations that are independent of hardware. Also it will be appreciated that the configured logic or logic configured to in the various blocks are not limited to specific logic gates or elements but generally refer to the ability to perform the functionality described herein either via hardware or a combination of hardware and software . Thus the configured logics or logic configured to as illustrated in the various blocks are not necessarily implemented as logic gates or logic elements despite sharing the word logic. Other interactions or cooperation between the logic in the various blocks will become clear to one of ordinary skill in the art from a review of the embodiments described below in more detail.

The various embodiments may be implemented on any of a variety of commercially available server devices such as server illustrated in . In an example the server may correspond to one example configuration of the application server described above. In the server includes a processor coupled to volatile memory and a large capacity nonvolatile memory such as a disk drive . The server may also include a floppy disc drive compact disc CD or DVD disc drive coupled to the processor . The server may also include network access ports coupled to the processor for establishing data connections with a network such as a local area network coupled to other broadcast system computers and servers or to the Internet. In context with it will be appreciated that the server of illustrates one example implementation of the communication device whereby the logic configured to transmit and or receive information corresponds to the network access ports used by the server to communicate with the network the logic configured to process information corresponds to the processor and the logic configuration to store information corresponds to any combination of the volatile memory the disk drive and or the disc drive . The optional logic configured to present information and the optional logic configured to receive local user input are not shown explicitly in and may or may not be included therein. Thus helps to demonstrate that the communication device may be implemented as a server in addition to a UE implementation as in A or B as in .

User application services involving concurrent transmission of voice and some other form of media real time or non real time are typically implemented by sending voice media as a separate independent stream. This is done for several reasons including providing positive user experience allocating preferential treatment such as QoS to voice packets. When voice packets are transmitted concurrently with real time media such as video the video stream is typically allocated a best effort BE without QoS. Thus depending upon network conditions the video packets may suffer packet loss jitter and or delay. This results in an inconsistent user experience.

As used herein a QoS channel is used to refer to a QoS communication link over any network type e.g. a GBR QoS bearer in LTE a traffic channel TCH with QoS reserved via an RonR message in EV DO etc. and a non QoS channel is used to refer to a communication link without QoS over any network type e.g. a default non GBR bearer in LTE a TCH without QoS reserved in EV DO etc. . Further in certain network configurations e.g. EV DO as in UMTS W CDMA as in etc. the RAN is responsible for assigning the QoS used by UEs and in other network configurations e.g. LTE as in etc. the core network is responsible for assigning the QoS used by UEs. Accordingly reference to a RAN core network denoted in the respective FIGS as RAN Core Network is used to refer to an entity in any network type that serves UEs and is responsible for assigning QoS thereto.

In LTE networks as an example audio data e.g. voice media is typically carried over a dedicated bearer with a certain GBR or QoS. Non audio data e.g. files or video may expect the same treatment but their bit rate requirements may be higher in this case. Thus it is unlikely that a UE can be assigned sufficient GBR to handle all of its audio and non audio data for the session. illustrates a conventional process of supporting a server arbitrated communication session with the above noted resource allocation for audio and non audio data.

Referring to the application server sets up a communication session between at least UEs 1 and 2 . In an example the communication session e.g. a PTT call a VoIP call etc. can also include one or more additional UEs in which case the communication session is a group communication session. In conjunction with setting up the communication session the RAN core network serving UE 1 allocates and or maintains a non QoS channel for supporting UE 1 s non audio data transfers for the communication session and the RAN core network serving UE 1 also allocates a QoS channel for supporting UE 1 s audio data transfers for the communication session . Similarly in conjunction with setting up the communication session the RAN core network serving UE 2 allocates and or maintains a non QoS channel for supporting UE 2 s non audio data transfers for the communication session and the RAN core network serving UE 2 also allocates a QoS channel for supporting UE 2 s audio data transfers for the communication session . In an example at or if UE 1 and or UE 2 have previously established the non QoS channel e.g. a default non GBR bearer in LTE the pre established non QoS channel can simply be maintained without any new resources being allocated to UE 1 and or UE 2 for supporting non audio data transfers for the communication session. The remainder of provides an illustration of two distinct types of non audio data i.e. video data and file data that can be sent over the non QoS channel during the communication session.

Referring to during the communication session UE 1 obtains audio and video data for transmission to UE 2 . UE 1 thereby transmits a first stream of packets carrying the audio data that is obtained at over the QoS channel allocated at and a second stream of packets carrying the video data that is obtained at over the non QoS channel from . As will be appreciated certain types of video data have a greater impact to video playback performance than other types of video data. For example intermittent I frames or I slices are relatively important video frames that help to calibrate video playback. In assume that one or more packets in the second stream of packets carrying one or more I frames are lost at some point during transfer due at least in part to the lack of QoS on the non QoS channel . Accordingly UE 2 receives the first and second streams of packets and plays the respective audio and video data contained therein . However at the loss of the I frame s from causes degradation to the quality of the video that is played by UE 2.

Referring to during the communication session UE 1 obtains an encrypted file that is configured to be decrypted or unlocked with a decryption key for transmission to UE 2 . UE 1 thereby transmits both the encrypted file and its associated decryption key within the second stream of packets over the non QoS channel from . In assume that one or more packets in the second stream of packets carrying the decryption key are lost at some point between UE 2 s serving RAN core network and the application server due at least in part to the lack of QoS on the non QoS channel . Accordingly at UE 2 receives the second stream of packets at with the encrypted file but UE 2 cannot decrypt the encrypted file because the decryption key was lost at . Thus even though the entire encrypted file was successfully transferred the loss of the decryption key renders the encrypted file unusable.

As will be appreciated while the non QoS channel is used in because sufficient QoS to handle both the audio and non audio data is typically unavailable the losses of certain higher priority non audio data e.g. decryption keys I frames header information etc. on the non QoS channel can cause a disproportionately high degradation to the user experience at the target UE as compared to lower priority non audio data e.g. B frames bulk encrypted files etc. . Accordingly embodiments of the invention are directed to identifying a subset of higher priority HP non audio data and allocating the HP non audio data to the QoS channel instead of the non QoS channel while any remaining lower priority LP non audio data if present can remain on the non QoS channel. The HP non audio data can thereby be piggybacked onto and or interleaved with the audio data being carried on the QoS channel during the communication session e.g. during times when the QoS channel is being underutilized and the HP non audio data can be marked so that the target UE can distinguish between the HP non audio data and the audio data on the QoS channel. These aspects are discussed in greater detail below with respect to .

At A the transmitting UE determines which channels are currently available for the transmission of the audio and non audio data A. In particular in the embodiment of it is assumed that the transmitting UE will have access to the non QoS channel throughout the communication session but that the QoS channel may or may not be available. For example the QoS channel may not be available for a period of time during setup of the communication session or the QoS channel may be allocated and later taken away and repurposed by the network. At A if the transmitting UE determines that it currently has access to the non QoS channel and not the QoS channel the transmitting UE sends both the audio and non audio data over the non QoS channel using a best effort BE protocol A. Otherwise at A if the transmitting UE determines that it currently has access to both the non QoS channel and the QoS channel instead of simply allocating the audio data to the QoS channel and allocating the non audio data to the non QoS channel as in the transmitting UE evaluates the non audio data to identify whether the non audio data includes a first subset of HP non audio data and optionally a second subset of LP non audio data A. With respect to the evaluation of A may identify whether the non audio data includes both the first and second subsets of LP and HP non audio data or alternatively that the non audio data includes only LP non audio data e.g. none of the non audio data is particularly important or only HP non audio data e.g. the non audio data corresponds to a file that has high priority in its entirety .

Many different types of non audio data can potentially qualify as HP non audio data. As used herein HP non audio data is defined as non audio data with sufficient priority to trigger delivery on the QoS channel instead of the non QoS channel while LP non audio data is defined as non audio data without sufficient priority to trigger delivery on the QoS channel instead of the non QoS channel even when the QoS channel is available. Table 2 below shows specific examples of non audio data that can qualify as HP non audio data in the context of the process of 

Referring to based on the evaluation at A the transmitting UE determines whether HP non audio data has been identified A. If the transmitting UE determines that no HP non audio data has been identified at A the audio data is sent over the QoS channel and the non audio data is sent over the non QoS channel A e.g. similar to and in . Alternatively if the transmitting UE determines that HP non audio data is identified at A the transmitting UE determines whether to transmit the identified HP non audio data over the QoS channel instead of the non QoS channel A.

Referring to the decision logic of A can be based on a channel allocation rule. In an example the channel allocation rule can instruct the transmitting UE to transmit all identified HP non audio over the QoS channel i.e. irrespective of a level of utilization on the QoS channel . Alternatively the channel allocation rule can instruct the transmitting UE to transmit any identified HP non audio over the QoS channel only if the QoS channel is being underutilized. For example the QoS channel is likely to be highly utilized if a user of the transmitting UE is speaking and audio is actively being transmitted by the transmitting UE to the target UE. However voice traffic is typically bursty in nature such that low utilization can be inferred if voice activity is low or if the transmitting UE does not have the floor for a half duplex call . As will be described in more detail with respect to below a decision engine can be notified with respect to the current voice activity level to facilitate the channel allocation rule that triggers HP non audio data over the QoS channel. If the transmitting UE determines not to transmit the identified HP non audio data over the QoS channel at A the audio data is sent over the QoS channel and the non audio data e.g. the HP non audio data and any LP non audio data is sent over the non QoS channel A e.g. similar to and in . Alternatively if the transmitting UE determines to transmit the identified HP non audio data over the QoS channel at A the process advances to A.

At A the transmitting UE transmits a first stream of packets over the QoS channel whereby the first stream of packets includes both i the audio data obtained at A and ii the HP non audio data identified at A. As discussed above the identified HP non audio data can be inserted into the first stream of packets during a period where the audio or voice utilization on the QoS channel is low such that the transmission of A can primarily include the audio data e.g. during high audio utilization on the QoS channel for a period of time and can primarily include the HP non audio data during another period of time e.g. during low audio utilization on the QoS channel . Thus the proportion of the audio data to the HP non audio data can vary and at certain times can be associated with an exclusive transmission of the audio data or the HP non audio data within the first stream of packets.

Further in conjunction with the transmission of A the transmitting UE can mark the packets within the first stream of packets to identify which packets carry the audio data and which packets carry the HP non audio data to permit the target UE to correctly associate the audio data and HP non audio data within the first stream of packets. For example the marking can be accomplished by adding a flag to headers of packets carrying the HP non audio data by adding a flag to headers of packets carrying the audio data or both. Also it is possible that certain hybrid packets e.g. RTP packets include some audio data and some HP non audio data. In this case the hybrid packets can be marked so as to indicate their hybrid status or alternatively the individual audio and HP non audio frames within the hybrid packets can be marked to indicate their audio or HP non audio association on a frame specific basis. In a further example the packet marking can be implemented for RTP packets by assigning a special payload type in the RTP header for non audio data e.g. video data etc. so that the non audio packets in the first stream of packets can utilize the QoS channel by using the same RTP session as the audio data while still indicating their non audio status to the target UE. In an alternative example the packet marking can be implemented for RTP packets via an RTP extension header for the non audio packets within the first stream of packets so that the non audio packets can use the same RTP session as the audio data while still indicating their non audio status to the target UE.

At A the transmitting UE also optionally transmits a second stream of packets that includes at least the second subset of LP non audio data. The transmission of A is optional because the identified HP non audio data can potentially include the entirety of the non audio data obtained at A e.g. see File Transfer Example 3 from Table 2 above . However if the HP non audio data corresponds to only a portion of the non audio data obtained at A any remaining LP non audio data can be transmitted within the second stream of packets at A. Also in a further example the HP non audio data which is transmitted in the first stream of packets over the QoS channel at A can also be redundantly transmitted over the non QoS channel at A.

Referring to an application layer B of the transmitting UE includes a non audio source B an audio source B a decision engine B a voice activity detection module B a non audio RTP packetization module B and a voice RTP packetization module B. The non audio source B e.g. a client application configured to capture video or to identify files for transfer etc. provides non audio data e.g. video files etc. to the decision engine B and also to the non audio RTP packetization module B via interface 1 e.g. as in A of . The audio source B e.g. a client application configured to capture audio or to manage audio sessions which can be the same as the non audio source B in an example provides audio data to the voice activity detection module B and also to the voice RTP packetization module B via interfaces 2 and 3 respectively e.g. as in A of .

The voice activity detection module B analyzes the audio data from interface 2 and determines if a threshold level of voice activity is present in the audio data. The voice activity detection module B notifies the decision engine B if the threshold level of voice activity is exceeded via interface 4. In the embodiment of assume that the decision engine B decides to move HP non audio data from the non QoS channel to the QoS channel based on an indication from the voice activity detection module B indicating that the threshold level of voice activity is exceeded e.g. as in A of . Accordingly the decision engine B identifies any non audio data in the non audio data received via interface 1 e.g. as in A and A of and instructs the non audio RTP packetization module B via interface 5 to forward the identified HP non audio data to the voice RTP packetization module B via interface 6. Meanwhile the decision engine B also notifies the voice activity detection module B that HP non audio data is going to be packetized along with audio data on the QoS channel and the voice activity detection module B forwards this information to the voice RTP packetization module B via interface 7. The non audio RTP packetization module B packetizes the LP non audio data and sends the packetized LP non audio data to an IP stack B via interface 8. The non audio RTP packetization module B packetizes both the audio data and the HP non audio data and sends the packetized audio data and HP non audio data to the IP stack B via interface 9. The IP stack B delivers the packetized audio data HP non audio data and LP non audio data to an uplink traffic flow template TFT module B via interface 10. The uplink TFT module B transmits a first stream of packets including the packetized audio data and HP non audio data via the QoS channel or interface 11 e.g. as in A of and the uplink TFT module B also transmits a second stream of packets including the packetized LP non audio data via the non QoS channel or interface 12 as in A of .

Referring to assume that both the non QoS channel and the QoS channel between the target and transmitting UEs are active. Under this assumption the target UE receives a first stream of packets over the QoS channel A whereby the first stream of packets corresponds to the first stream of packets transmitted by the transmitting UE at A of . The target UE also optionally receives a second stream of packets A whereby the second stream of packets corresponds to the second stream of packets optionally transmitted by the transmitting UE at A of . As noted above the reception of A is optional because the identified HP non audio data can potentially include the entirety of the non audio data obtained at A of e.g. see File Transfer Example 3 from Table 2 above . However if the HP non audio data corresponds to only a portion of the non audio data obtained at A of any remaining LP non audio data can be received within the second stream of packets at A. Also in a further example the HP non audio data received in the first stream of packets over the QoS channel at A can also be redundantly received over the non QoS channel at A as part of the second stream of packets as well.

Referring to the target UE evaluates the first set of packets to identify a first set of packets carrying the audio data for the communication session and a second set of packets carrying the HP non audio data for the communication session A. For example as discussed above with respect to the transmitting UE can mark any HP non audio packets with flags to facilitate the target UE to distinguish between the first and second sets of packets within the first stream of packets on the QoS channel. The target UE can then identify these flags at A to extract the first and second sets of packets. Accordingly at A the target UE extracts the first and second sets of packets from the first stream of packets. The target UE plays the audio from the extracted first set of packets A and the target UE also processes the HP non audio data from the extracted second set of packets along with any LP non audio data from the second stream of packets A. For example if the LP and HP non audio data correspond to video data the processing that occurs at A can include merging the LP and HP video data received on the non QoS and QoS channels respectively and then playing the merged video in conjunction with the playback of the audio at A. In another example if the LP and HP non audio data corresponds to an encrypted file and a decryption key respectively the processing that occurs at A can include decrypting the encrypted file using the encryption key.

Referring to the downlink TFT module B receives the first stream of packets with the audio data and HP non audio data via the QoS channel or interface 1 e.g. as in A of and the downlink TFT module B also receives the LP non audio data via the non QoS channel or interface 2 e.g. as in A of . The downlink TFT module B forwards the first and second streams of packets to the IP stack B via interface 3. The IP stack B forwards the first stream of packets to the voice RTP depacketization module B via interface 4 and the IP stack B forwards the second stream of packets to the non audio RTP depacketization module B via interface 5. The non audio RTP depacketization module B depacketizes the second stream of packets and forwards the depacketized second stream of packets to the non audio decoder B via interface 6. The voice RTP depacketization module B identifies within the stream of packets a first set of packets carrying the audio data and a second set of packets carrying the HP non audio data e.g. based on packet markings as discussed above . The RTP depacketization module B depacketizes the first and second sets of packets from the first stream of packets e.g. as in A of and then forwards the depacketized first set of packets to the audio decoder B via interface 7 and forwards the depacketized second set of packets to the non audio decoder B via interface 8. The audio decoder B decodes the depacketized first set of packets to facilitate playback of the audio data while the non audio decoder B decodes the depacketized second stream of packets and the depacketized first set of packets to facilitate the processing of the LP and HP non audio data e.g. as in A of .

Referring to under the assumption that the non audio data obtained at A is video data including HP video data and LP video data UE 1 executes A A A A and A of at . At this point instead of transmitting the audio data over the QoS channel and transmitting all of the video data over the non QoS channel as at and of UE 1 transmits the audio data and the HP video data over the QoS channel e.g. as in A of and UE 1 transmits the LP video data over the non QoS channel e.g. as in A of . In particular assume that the HP video data includes one or more I frames which unlike of are not lost in the process of because the HP video data is sent via the QoS channel instead of the non QoS channel . Next at UE 2 executes A through A of . UE 2 then plays the audio and video received over the first and second streams of packets on the QoS and non QoS channels e.g. as in A and A of whereby the video playback does not experience the errors discussed above with respect to of because the I frame in is transferred without error via the QoS channel.

Referring to under the assumption that the non audio data obtained at A is a file package including an encrypted file as the LP non audio data and a decryption key as the HP non audio data UE 1 executes A A A A and A of at . At this point instead of transmitting the audio data over the QoS channel and transmitting the entire file package over the non QoS channel as at of UE 1 transmits the audio data and the decryption key over the QoS channel e.g. as in A of and UE 1 transmits the encrypted file over the non QoS channel e.g. as in A of . Therefore unlike of the decryption key is not lost in the process of because the decryption key is sent via the QoS channel instead of the non QoS channel . Next at UE 2 executes A through A of . UE 2 then plays the audio received over the first stream of packets on the QoS channel e.g. as in A of and UE 2 also decrypts the encrypted file based on the decryption key e.g. as in A of .

As will be appreciated corresponds to an example implementation based upon File Transfer Example 2 from Table 2 above whereby the HP non audio data corresponds to a decryption key to decrypt an associated encrypted file. In this case both the encrypted file and the decryption key need to be conveyed to UE 2 for UE 2 to be able to decrypt the encrypted file. In an alternative to the process of can be modified slightly to conform to the thumbnail image aspect from the File Transfer Example 2 from Table 2 above whereby the HP non audio data is specifically generated based on the availability of the QoS channel and would not otherwise be sent if only the non QoS channel were available. In other words a relatively large image file can be transferred on the non QoS channel and if the QoS channel is deemed available and is not being fully utilized a thumbnail image can be generated based on the large file image and transmitted on the QoS channel as the HP non audio data so that the thumbnail image can be quickly loaded by UE 2 while UE 2 waits to receive the large image file on the slower non QoS channel. Thereby the HP non audio data does not necessarily correspond to a subset of the non audio data that is obtained for transmission on the QoS channel but in at least one embodiment can be configured to supplement or enhance the LP non audio data for scenarios where the QoS channel is available.

While are directed to implementations of the processes of whereby the HP non audio data is sent in conjunction with LP non audio data it is also possible that the non audio data in its entirety is deemed to be HP in which case the HP non audio data can be sent without any accompanying LP non audio data e.g. see File Transfer Example 3 from Table 2 above . With this in mind illustrates an example implementation of with respect to an audio conference in accordance with another embodiment of the invention. In UE 1 corresponds to the transmitting UE from and UE 2 corresponds to the target UE from . through of substantially correspond to through of and will not be described further for the sake of brevity.

Referring to under the assumption that the non audio data obtained at A is a small independent HP file package UE 1 executes A A A A and A of at . UE 1 transmits the audio data and the entirety of the small independent HP file package over the QoS channel e.g. as in A of and UE 1 does not transmit any portion of the small independent HP file package over the non QoS channel e.g. optional A of is not performed although it is possible that data unrelated to the small independent HP file package is sent over the non QoS channel and or alternatively that the small independent HP file package is sent redundantly on the non QoS channel . Next at UE 2 executes A through A of . UE 2 then plays the audio received over the first stream of packets on the QoS channel e.g. as in A of and UE 2 also processes the small independent HP file package e.g. as in A of .

While are directed to implementations of the processes of whereby the QoS channel is assumed to be available throughout the communication session it is also possible that the QoS channel is unavailable or inactive during at least a portion of the communication session. For example the QoS channel may not be available for a period of time during setup of the communication session or the QoS channel may be allocated and later taken away and repurposed by the network. Thus the status of the QoS channel can potentially be somewhat varying or unpredictable during the communication session as shown below with respect to .

Referring to assume that any of the above noted communication sessions from is active and that a current QoS session state corresponds to an active non QoS channel and an inactive QoS channel . In other words at UE 1 does not have access to the QoS channel for its communication session with UE 2. With this in mind UE 1 executes A A and A from and transmits both the audio and non audio data for the communication session over the non QoS channel . UE 2 receives both the audio and non audio data for the communication session over the non QoS channel plays the received audio data and processes the received non audio data .

At some later point during the communication session assume that UE 1 undergoes a QoS session state transition that sets the current QoS session state to an active non QoS channel and an active QoS channel . For example the QoS session state transition can be triggered by the RAN core network granting QoS to UE 1 during call setup the RAN core network granting QoS to UE 1 during an in call phase of the communication session and so on. At this point UE 1 has access to the QoS channel for its communication session with UE 2. With this in mind UE 1 executes A A A A A and A from and transmits the audio and HP non audio data for the communication session over the QoS channel . UE 1 can also execute A of so as to transmit the LP non audio data to UE 2 over the non QoS channel . UE 2 receives the audio and HP non audio data over the QoS channel and optionally receives the LP non audio data over the non QoS channel such that UE 2 executes A and A through A whereby UE 2 plays the received audio data and processes the received HP non audio data e.g. and potentially the LP non audio data as well if A and A of are executed .

At some later point during the communication session assume that UE 1 undergoes another QoS session state transition that sets the current QoS session state back to an active non QoS channel and an inactive QoS channel e.g. similar to the QoS session state from . For example the QoS session state transition can be triggered by the RAN core network withdrawing a previous allocation of QoS to UE 1 during an in call phase of the communication session and so on. In response to the QoS session state transition of the process returns to .

While the above described embodiments of the invention primarily relate to server arbitrated communication sessions it will be appreciated that other embodiments of the invention can correspond to sessions that are not arbitrated by a server that is external to the carrier network such as sessions arbitrated by the carrier network itself P2P sessions or sessions over a mesh network and so on. Further those of skill in the art will appreciate that information and signals may be represented using any of a variety of different technologies and techniques. For example data instructions commands information signals bits symbols and chips that may be referenced throughout the above description may be represented by voltages currents electromagnetic waves magnetic fields or particles optical fields or particles or any combination thereof.

Further those of skill in the art will appreciate that the various illustrative logical blocks modules circuits and algorithm steps described in connection with the embodiments disclosed herein may be implemented as electronic hardware computer software or combinations of both. To clearly illustrate this interchangeability of hardware and software various illustrative components blocks modules circuits and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application but such implementation decisions should not be interpreted as causing a departure from the scope of the present invention.

The various illustrative logical blocks modules and circuits described in connection with the embodiments disclosed herein may be implemented or performed with a general purpose processor a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof designed to perform the functions described herein. A general purpose processor may be a microprocessor but in the alternative the processor may be any conventional processor controller microcontroller or state machine. A processor may also be implemented as a combination of computing devices e.g. a combination of a DSP and a microprocessor a plurality of microprocessors one or more microprocessors in conjunction with a DSP core or any other such configuration.

The methods sequences and or algorithms described in connection with the embodiments disclosed herein may be embodied directly in hardware in a software module executed by a processor or in a combination of the two. A software module may reside in RAM memory flash memory ROM memory EPROM memory EEPROM memory registers hard disk a removable disk a CD ROM or any other form of storage medium known in the art. An exemplary storage medium is coupled to the processor such that the processor can read information from and write information to the storage medium. In the alternative the storage medium may be integral to the processor. The processor and the storage medium may reside in an ASIC. The ASIC may reside in a user terminal e.g. UE . In the alternative the processor and the storage medium may reside as discrete components in a user terminal.

In one or more exemplary embodiments the functions described may be implemented in hardware software firmware or any combination thereof. If implemented in software the functions may be stored on or transmitted over as one or more instructions or code on a computer readable medium. Computer readable media includes both computer storage media and communication media including any medium that facilitates transfer of a computer program from one place to another. A storage media may be any available media that can be accessed by a computer. By way of example and not limitation such computer readable media can comprise RAM ROM EEPROM CD ROM or other optical disk storage magnetic disk storage or other magnetic storage devices or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be accessed by a computer. Also any connection is properly termed a computer readable medium. For example if the software is transmitted from a website server or other remote source using a coaxial cable fiber optic cable twisted pair digital subscriber line DSL or wireless technologies such as infrared radio and microwave then the coaxial cable fiber optic cable twisted pair DSL or wireless technologies such as infrared radio and microwave are included in the definition of medium. Disk and disc as used herein includes compact disc CD laser disc optical disc digital versatile disc DVD floppy disk and blu ray disc where disks usually reproduce data magnetically while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer readable media.

While the foregoing disclosure shows illustrative embodiments of the invention it should be noted that various changes and modifications could be made herein without departing from the scope of the invention as defined by the appended claims. The functions steps and or actions of the method claims in accordance with the embodiments of the invention described herein need not be performed in any particular order. Furthermore although elements of the invention may be described or claimed in the singular the plural is contemplated unless limitation to the singular is explicitly stated.

