---

title: Systems and methods for performing data replication
abstract: Preparing source data to be replicated in a continuous data replication environment. Certain systems and methods populate a file name database with entries having a unique file identifier descriptor (FID), short name and a FID of the parent directory of each directory or file on a source storage device. Such information is advantageously gathered during scanning of a live file system without requiring a snapshot of the source storage device. The database can be further used to generate absolute file names associated with data operations to be replayed on a destination storage device. Based on the obtained FIDs, certain embodiments can further combine write operations to be replayed on the destination storage device and/or avoid replicating temporary files to the destination system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08745105&OS=08745105&RS=08745105
owner: CommVault Systems, Inc.
number: 08745105
owner_city: Oceanport
owner_country: US
publication_date: 20130926
---
This application is a continuation of U.S. patent application Ser. No. 13 118 294 filed on May 27 2011 U.S. Pat. No. 8 589 347 which claims the benefit of priority under 35 U.S.C. 119 e of U.S. Provisional Patent Application No. 61 349 624 filed on May 28 2010 and entitled SYSTEMS AND METHODS FOR PERFORMING DATA REPLICATION the disclosures of which are hereby incorporated by reference in their entireties.

The present disclosure relates to performing copy and or data management operations in a computer network and in particular to systems and methods for performing data replication in a storage management system.

Computers have become an integral part of business operations such that many banks insurance companies brokerage firms financial service providers and a variety of other businesses rely on computer networks to store manipulate and display information that is constantly subject to change. Oftentimes the success or failure of an important transaction may turn on the availability of information that is both accurate and current. Accordingly businesses worldwide recognize the commercial value of their data and seek reliable cost effective ways to protect the information stored on their computer networks.

To address the need to maintain current copies of electronic information certain data replication systems have been provided to continuously copy data from one or more source machines to one or more destination machines. These continuous data replication CDR systems provide several advantages for disaster recovery solutions and can substantially reduce the amount of data that is lost during an unanticipated system failure.

One drawback of such CDR systems is that during an initial synchronization phase many systems record absolute file names when scanning a source storage device in order to replicate the scanned data to a same location on a destination storage device. Moreover this scanning is generally performed while the source file system is in a fixed state such as based on a snapshot of the file system.

In view of the foregoing a need exists for improved systems and methods for preparing and transmitting source data to be replicated to a destination system. For instance there is a need for systems and methods for scanning a live file system during an initial synchronization phase between the source and destination systems.

Certain embodiments of the invention are provided for intelligent data replication. In particular embodiments of the invention include improved systems and methods scanning a source file system having data to be copied in a CDR system. In certain embodiments such scanning is performed on a live file system without requiring a snapshot of the file system data. For example file identifier descriptors FIDs which are generally not exposed to outside the kernel of files and or directories on the file system can be used to populate a file name database usable to construct an absolute file name when transmitting data to a replication system.

In certain further embodiments use of FIDs to track files on the source system and or an introduction of a slight delay in the replication process allows for more efficient data replication operations. For instance journal entries representing monitored data operations on the source file system can be stored without storing the actual data. Moreover location information in the log entries can be analyzed to determine if multiple write operations on the source system can be combined into a single write operation on the destination system. In yet other embodiments temporary files on the source system can be identified and not copied to the destination system.

According to certain embodiments a method is provided for identifying data to be copied in a data replication system. The method can include obtaining with a scanning module executing on a computing device a first file identifier descriptor FID of a first directory on a live source file system. In some cases the first FID is one of a plurality of unique identifiers corresponding to a plurality of directories and files on the source file system. The method may further include adding the first FID to a queue and can also include storing a current journal sequence number from a file system filter driver identifying a first time. In some instances the method includes following said storing accessing a current directory of the plurality of directories on the source file system that corresponds to a next FID stored in the queue. The method can additional include obtaining additional FIDs for each immediate child directory and immediate child file in the current directory. If no changes have been made to the current directory since the first time the method can include populating a file name database with the additional FIDs of each immediate child directory and immediate child file in the current directory adding the additional FIDs of each immediate child directory of the current directory to the queue and or removing the next FID from the queue. If changes have been made to the first directory since the first time the method can include repeating said storing said accessing and said obtaining the additional FIDs.

In some embodiments a system is provided for preparing data for replication from a source computing device in a network. The may include a queue configured to store a plurality of file identifier descriptors FIDs each comprising a unique identifier that corresponds to one of a plurality of directories and files on a source file system. The system can also include a scanning module executing on a computing device and configured to scan the source file system while in a live state and to populate the queue with the plurality of FIDs. In certain cases the system additionally includes a database comprising file name data that associates each of the plurality of FIDs with a short name and a parent FID. The scanning module can be further configured to populate the database with the file name data based on said scan of the source file system in the live state. The system can also include at least one database thread configured to receive a data entry identifying a data management operation associated with at least one of the plurality of directories and files on the source file system and to construct from the FID associated with the at least one directory or file an absolute file name for transmission to a destination system along with a copy of the data management operation for replying on the destination system.

According to other aspects of the disclosure a method is provided for performing data replication. The method can include monitoring a plurality of journal entries associated with writing data to a source storage device. The method may further include identifying a first journal entry of the plurality of journal entries. The first journal entry may comprise a first data write operation a first file identifier descriptor FID of a file to be modified by the first data write operation on the source storage device and a first location of a first portion of the file to be modified. The method can also include identifying a second journal entry of the plurality of journal entries the second journal entry comprising a second data write operation a second FID of a file to be modified by the second data write operation on the source storage device and a second location of a second portion of the file to be modified. In some instances the method additionally includes determining that the first and second data write operations can be combined into a single write operation. The method may also include constructing an absolute file name based on at least one of said first and second FIDs wherein neither the first nor second journal entries comprises the absolute file name. In some embodiments the method includes transmitting the single write operation and the absolute file name to a destination storage device to replay on the destination storage device the data modifications associated with the first and second write operations.

According to yet further aspects of the disclosure a system is provided for performing data replication. The system can include at least one computer application executing on a computing device and configured to generate operations associated with data on a source storage device. The system may also include a filter module disposed between the at least one computer application and the first storage device. The filter module can be configured to identify from the operations a first data modification operation a first file identifier descriptor FID of a file to be modified by the first data modification operation and a first location of a first portion of the file to be modified and a second data modification operation a second FID of a file to be modified by the second data modification operation and a second location of a second portion of the file to be modified. The system can further include a processing module configured to determine that the first and second data modification operations can be combined into a single modification operation. In some embodiments the system also includes at least one database thread configured to construct an absolute file name for replaying the single modification operation on replication data of a destination storage device based on at least one of said first and second FIDs. In some cases neither the first nor second data modification operations comprises the absolute file name.

According to other embodiments a system is provided for performing data replication. The system can include means for monitoring a plurality of journal entries associated with writing data to a source storage device. The system can further include means for identifying a first journal entry of the plurality of journal entries the first journal entry comprising a first data write operation a first file identifier descriptor FID of a file to be modified on the source storage device and a first location of a first portion of the file to be modified and for identifying a second journal entry of the plurality of journal entries the second journal entry comprising a second data write operation a second FID of a file to be modified on the source storage device and a second location of a second portion of the file to be modified. The system can also include means for determining that the first and second data write operations can be combined into a single write operation. In certain embodiments the system further includes means for constructing an absolute file name based on at least one of said first and second FIDs wherein neither the first nor second journal entries comprises the absolute file name. The system may additionally include means for transmitting the single write operation and the absolute file name to a destination storage device to replay on the destination storage device the data modifications associated with the first and second write operations.

According to additional aspects a method is provided for performing data replication. The method can include monitoring data operations associated with an application executing on a computing device the data operations operative to write data to a first storage device. The method can also include populating a log file with a plurality of data entries indicative of the data operations. In some cases the method includes identifying a first one of the plurality of data entries associated with writing data to a temporary file on the first storage device. The method may additionally include replaying to a second storage device based on a portion of the data entries a portion of the data operations to replicate data to a first location on the second storage device. The portion of the data entries according to some embodiments does not include the first one of the plurality of data entries.

According to some aspects of the disclosure a system is provided for performing data replication between two computing devices in a network. The system can include at least one computer application executing on a first computing device and configured to generate a plurality of operations associated with storing data on a source storage device the data comprising at least one temporary file and at least one non transitory file. The system may also include a log file comprising a plurality of data entries indicative of the plurality of operations. In some cases a first one of the plurality of data entries is associated with writing the at least one temporary file. According to some embodiments the system includes a processing module executing on and configured to identify a first one of the plurality of data entries associated with writing the temporary file. The processing module may be further configured to copy a portion of the entries of the log file to a second computing device in network communication with the first computing device. The portion of the data entries according to some embodiments does not include the first one of the plurality of data entries.

For purposes of summarizing the disclosure certain aspects advantages and novel features of the inventions have been described herein. It is to be understood that not necessarily all such advantages may be achieved in accordance with any particular embodiment of the invention. Thus the invention may be embodied or carried out in a manner that achieves or optimizes one advantage or group of advantages as taught herein without necessarily achieving other advantages as may be taught or suggested herein.

As will be seen from the disclosure herein certain embodiments of systems and methods are provided for intelligent data replication. In particular embodiments of the invention include improved systems and methods for scanning a source file system having data to be copied in a CDR system. In certain embodiments such scanning is performed on a live file system without requiring a snapshot of the file system data. For example FIDs of files and or directories on the file system can be used to populate a file name database usable to construct an absolute file name when transmitting data to the replication system.

In certain further embodiments use of FIDs to track files on the source system and or accumulating a group of journal entries to transmit during the replication process allows for more efficient data replication operations. For instance logs with entries representing monitored data operations on the source file system can be stored without actual data. Moreover location information in the log entries can be analyzed to determine if multiple write operations on the source system can be combined into a single write operation on the destination system. In yet other embodiments temporary files on the source system can be passed over when identifying source data to be copied to the destination system.

The features of the systems and methods will now be described with reference to the drawings summarized above. Throughout the drawings reference numbers are re used to indicate correspondence between referenced elements. The drawings associated descriptions and specific implementation are provided to illustrate embodiments of the invention and not to limit the scope of the disclosure.

Moreover embodiments of the invention can be used in combination with replication systems and methods described in U.S. Pat. No. 7 651 593 which is hereby incorporated herein in its entirety to be considered part of this specification.

The illustrated network advantageously comprises any means for communicating data between two or more systems or components. It certain embodiments the network comprises a computer network. For example the network may comprise a public network such as the Internet virtual private network VPN token ring or TCP IP based network wide area network WAN local area network LAN an intranet network point to point link a wireless network cellular network wireless data transmission system two way cable system interactive kiosk network satellite network broadband network baseband network combinations of the same or the like. In embodiments wherein the source system and destination system are part of the same computing device the network may represent a communications socket or other suitable internal data transfer path or mechanism.

As shown the source system comprises one or more applications residing on and or being executed by a computing device. For instance the applications may comprise software applications that interact with a user to process data and may include for example database applications e.g. SQL applications word processors spreadsheets financial applications management applications e commerce applications browsers combinations of the same or the like. For example in certain embodiments the applications may comprise one or more of the following MICROSOFT EXCHANGE MICROSOFT SHAREPOINT MICROSOFT SQL SERVER ORACLE MICROSOFT WORD and LOTUS NOTES.

The source system further comprises one or more processes such as filter drivers that interact with data e.g. production data associated with the applications . For instance the filter driver may comprise a file system filter driver an operating system driver a filtering program a data trapping program an application a module of the application an application programming interface API or other like software module or process that among other things monitors and or intercepts particular application requests targeted at a file system another file system filter driver a network attached storage NAS a storage area network SAN mass storage and or other memory or raw data. In some embodiments the filter driver may reside in the I O stack of the application and may intercept analyze and or copy certain data traveling from the application to a file system.

In certain embodiments the filter driver may intercept data modification operations that include changes updates and new information e.g. data writes with respect to the application s of interest. For example the filter driver may locate monitor and or process one or more of the following with respect to a particular application application type or group of applications data management operations e.g. data write operations file attribute modifications logs or journals e.g. NTFS change journal configuration files file settings control files other files used by the application combinations of the same or the like. In certain embodiments such data may also be gathered from files across multiple storage systems within the source system . Furthermore the filter driver may be configured to monitor changes to particular files such as files identified as being associated with data of the applications .

In certain embodiments multiple filter drivers may be deployed on a computing system each filter driver being dedicated to data of a particular application . In such embodiments not all information associated with the client system may be captured by the filter drivers and thus the impact on system performance may be reduced. In other embodiments the filter driver may be suitable for use with multiple application types and or may be adaptable or configurable for use with multiple applications . For example one or more instances of customized or particularizing filtering programs may be instantiated based on application specifics or other needs or preferences.

The illustrated source system further comprises a source storage device . The source storage device may include any type of media capable of storing data. For example the source storage device may comprise magnetic storage such as a disk or a tape drive or other type of mass storage. In certain embodiments the source storage device may be internal and or external to e.g. remote to the computing device s having the applications and the filter drivers .

As further illustrated in the destination system comprises a replication module and a destination storage device . In certain embodiments the replication module is configured to monitor and or manage the copying of data from the source system to the destination system such as data retrieved by the filter drivers . In yet other embodiments the replication module is a dumb server or terminal that receives and executes instructions from the source system .

The destination storage device may include any type of media capable of storing data such as replication data sent from the source system . For example the destination storage device may comprise magnetic storage such as a disk or a tape drive or other type of mass storage. In certain embodiments the destination storage device may be internal and or external to the computing device s having the replication module .

In certain embodiments the source storage device and or the destination storage device may be implemented as one or more storage volumes that include physical storage disks defining an overall logical arrangement of storage space. For instance disks within a particular volume may be organized as one or more groups of redundant array of independent or inexpensive disks RAID . In certain embodiments either or both of the storage devices may include multiple storage devices of the same or different media.

The illustrated client computer further comprises a file system for organizing files and directories accessible by the client computer . In certain embodiments the file system comprises a data structure usable to keep track of a collection of files and or directories stored on the source storage device . The file system may include for example a local file system a network file system a file server a management program or the like or may include multiple file systems accessible by an operating system. For instance in embodiments wherein the storage device is associated with multiple volumes each volume may be associated with its own file system or a single file system may span across the multiple volumes.

The illustrated client computer also comprises one or more data agents . In certain embodiments the data agent comprises a module responsible for performing data and or storage tasks related to the client computer . For example the data agent may manage and or coordinate the compilation of and or transferring of replication data from the source system . In other embodiments the data agent may provide archiving migrating and or recovery of client computer data.

In certain embodiments the client computer comprises a plurality of data agents each of which performs data management operations related to data associated with each application . In such embodiments the data agent may be aware of the various files folders registry files and or system resources that are impacted by a particular application . For instance the data agent may be programmed to detect data management requests by a particular application and determine which files folders and or system resources are associated with the data management requests.

In certain embodiments the data agent is configured to perform data management operations in accordance with one or more storage policies or other preferences. A storage policy may include a data structure or other information having a set of preferences and other storage criteria for performing a storage operation. The preferences and storage criteria may include but are not limited to information regarding storage locations relationships between system components network pathways retention policies data characteristics compression or encryption requirements preferred system components combinations of the same or the like.

In certain embodiments one or more data agents are configured to perform an initial seeding or synchronization process of a replication process. For example prior to or concurrently with data replication using one or more filter drivers the data agent may perform a scan of the source system e.g. the source storage device . For instance the data agent may evaluate the folders and or directory structure of the source system to determine which folders are used by a particular application . In certain embodiments the data agent may also identify arrange and queue necessary data of the application to provide a proper platform for replication. For example the data agent may populate source log s with application data that has already been written to the source storage database . In certain embodiments this populating is performed based on a snapshot or point in time copy of the file system. In yet other embodiments as described below the data agent is configured to scan a live file system.

In certain embodiments when the data agent is initially installed or enabled on the client computer the data agent may evaluate the application . For instance the data agent may determine the application s organizational structure which may include for example folder directory and file information. The information gathered by the data agent may be sufficient to define a complete set of information to be replicated such that suitable baseline data representing the current operational state of the application is identified. In some instances this initial process may require the examination and identification of data related to application operations occurring prior to the installation of data agent . The data agent may also be configured to identify general configuration and operational information regarding the application .

In certain embodiments the data agent may be configured to access and or monitor particular files folders directories registries preferences and or other like data structures for information to be replicated. All or a portion of the information gathered by the data agent may be copied over to the destination system as part of the initial seeding or initialization process. After the seeding process is complete data replication may occur on a substantially continuous basis based on data transfers occurring between application s and source storage device . In certain embodiments the seeding process may occur substantially concurrently with execution of the application s . For instance data operations from the application s may be temporarily stored in a queue or buffer until the seeding process or a portion thereof is complete.

As shown in the client computer communicates through the file system with the source storage device which further includes a database and database logs . In yet other embodiments the client computer may communicate with NAS or the like. In certain embodiments data intended for the source storage device may be first written to a file in the database logs and subsequently committed to the database in accordance with data management techniques for enhancing storage operation performance. Moreover although only one database and one database log are depicted in it will be understood that the source storage device may comprise additional databases database logs and or other directory and file storage structures to meet the storage needs of the client computer .

As illustrated in the filter driver is advantageously located between the application and the file system . For instance the filter driver may be deployed in the stack as an I O buffer and or process in the data path between the application and the file system . In such embodiments the filter driver may intercept snoop supervise trap process or otherwise be cognizant of some or all operations e.g. data modification operations file modification operations read operations and the like from the application to its associated location s on the source storage device .

For example in certain embodiments the filter driver may communicate with an associated data agent to determine where data for a particular application will be stored e.g. particular folders on the file system . In certain embodiments the filter driver and or the data agent may also monitor and or parse data management operations to determine if new or additional folders are affected by the production volume data of the particular application . In certain embodiments the data agent may monitor data management operations and or other data for other purposes such as for example for satisfying a query or command by a storage manager component or the like.

As further depicted in one or more of the filter drivers and associated data agent s may be grouped together as a single module such as driver module . In yet other embodiments the data agent s may be separate from the driver module .

As discussed above in certain embodiments the filter driver is preferably configured to monitor and or filter data management operations associated with a particular application . The filter driver may be further configured according to predefined criteria to cause particular data to be written to one or more source logs for subsequent replication. For instance the filter driver may be configured to intercept scrub parse and or trap data management operations and to populate the source logs with changes associated therewith.

In certain embodiments the filter driver may examine the data management operation in progress determine whether the type of operation is one of interest for replication purposes and or copy select or all data to source log . For instance as discussed above the filter driver may determine if the data management operation concerns data in one or more files determined as relevant to replication e.g. files that may store data for a particular application . In other embodiments the filter driver may generate log entries for all data management operations.

The filter driver may further process and or traverse the data and copy generate or examine other relevant information such as a log entry number time information e.g. time stamp application type data size and start field combinations of the same or the like that may be useful in the replication process. In other embodiments the filter driver may monitor files on the source storage device for modifications of data relating to the subject application . For instance as disclosed above the filter driver may monitor a select group of files which have been associated with the application or folders to detect changes to data stored therein. In certain embodiments the filter driver or other system component may detect when a data write operation of the application is made to a file or folder not in the select group. The filter driver or other system component may then determine from the properties of the data write modification if the subject folder or file should be added to the select group for subsequent monitoring .

In certain embodiments the filter driver is deployed e.g. by data agent on the client computer prior to the beginning of the replication process. In embodiments wherein the filter driver is deployed after replication begins pertinent application data already stored on the source storage device may be copied to the source logs prior to the replication process e.g. during the initial seeding process described above .

In certain embodiments the filter driver may be enabled and or disabled by the data agent . For instance enabling the filter driver may allows it to populate an associated source log with log entries from application data passed from the application to the source storage device . When the filter driver is disabled data may pass directly through to the source storage device without being copied to the source logs .

In certain embodiments the data agent monitors the storage capacity of the source logs . For instance when one or more of the source logs reach a particular memory threshold the data agent may open a socket and communicate to the destination system that a copy of the source log is ready to be transmitted. In other embodiments the data agent may be configured to copy the source log to the destination system at periodic intervals or in accordance with other predefined criteria. In yet other embodiments the source logs maintain the history of previous intercepted changes e.g. the last N gigabytes of previous changes . As just one example scenario the history of intercepted changes can be used in the event that network connectivity is temporarily lost. For example the history of intercepted changes can be accessed and any changes that were not transmitted due to the connectivity interruption can be transmitted or retransmitted to the appropriate destination. This may be particularly useful where there are multiple destination devices and where the changes are successfully transmitted to a first subset of the multiple destination devices but not a second subset of the multiple destination devices. In this case the history can be accessed to transmit or retransmit the appropriate intercepted changes to the second subset of destination devices.

In certain embodiments the source system communicates with the associated destination system to verify that the two systems are synchronized. For instance the source system may receive from the destination system an identification e.g. unique serial number of the data write operation currently being replicated by the destination system. The source system may then compare the received identification with data write operation being forwarded to the source storage device .

In certain embodiments the replication logs contain a copy of the data stored on the source logs of a client system such as the source logs of . The replication logs comprise any type of memory capable of storing data including for example cache memory. In certain embodiments the replication logs may reside on the destination system such as for example on the destination storage device or at least a portion of the replication logs may be external to the destination system . In certain embodiments once the replication logs have been populated with the data from the source logs the data on the source logs is available to be erased and or overwritten to conserve memory space.

The replication module of the destination system further comprises a replication agent and one or more processes such as threads . In certain embodiments the replication agent comprises one or more software modules that coordinate the transfer of data from the replication logs to the destination storage device .

For example in certain embodiments the replication agent instantiates an appropriate number of threads processes or routines for copying data from the replication logs to the destination storage device . In certain embodiments the number of threads is based on one or more of the following factors the number of log files sent from the source logs to the replication logs information received from the data agent s information generated by the filter driver s and the type s of application data being tracked.

In certain embodiments the replication agent further includes mapping or correlation information that determines when and to where the data from the replication logs is copied by the threads . In certain embodiments such mapping information may be based on system or user defined parameters and or may be automatically generated such as based on the status of the destination storage device .

The one or more threads or processes direct movement of data from replication logs to the appropriate location on the destination storage device . In operation in certain embodiments the threads advantageously process or traverse replication logs for particular types of data and then copy that data to certain locations on one or more replication volumes based on data paths identified by the replication agent and or associated with each thread . For example the thread s may sequentially process each entry in the replication log and write the associated data to the destination storage device .

In certain embodiments each thread is assigned to a hard coded path pair which includes i a source path identifying the location on the source storage device associated with a data management operation e.g. c Folder and ii a destination path identifying the location on the destination storage device to receive the replicated data e.g. D folder from the thread .

As detailed above in certain embodiments the filter driver preferably substantially continuously populates data relating to one or more of the applications to the source logs . As shown in the source logs further comprise a first log file and a second log file . In certain embodiments the filter driver sequentially writes log entries to the source logs and when a certain capacity of the first log file is reached the filter driver begins populating the second log file with log entries.

In yet other embodiments data relating to each application of interest may be written to a particular log file established for that application. For example with reference to the first log file may relate to a first application of interest whereas the second log file may relate to a second application of interest.

In certain embodiments each of the log files of the source logs may be established by the data agent s and or the filter driver s as part of an initial deployment or initialization process. Moreover data may be written to the source logs as determined by preferences stored on or accessed by the client computer in a preference database .

For example as further shown in the first and second log files may comprise a series of entries each having an identifier that indicates the sequence order and or type of entry being made. For instance the illustrated entry identifier L1 may indicate that the particular entry represents a first database entry in a particular order of operation. The illustrated entry identifier L2 may indicate a second database entry in a particular order of operation and so forth. The illustrated entry identifier D1 may indicate that the particular entry represents a first database commit entry in a particular order of operation. Thus in the example described above the log entries identified by L1 and L2 may correspond to modifications associated with a particular database transaction and the log entry identified by D1 may correspond to a commit command for the particular transaction.

It will be understood that although only two log files are shown in more or fewer log files may be used with embodiments of the invention. For instance multiple applications may be monitored by the filter drivers and thus additional log files may be added as necessary or desired. Moreover although in some embodiments each application and each log file in the source logs may have its own associated filter driver in other embodiments a single filter driver may be deployed and configured for use with multiple applications such that there are separate log files for each monitored application .

With continued reference to in certain embodiments of the invention the data agent and or filter driver may be advantageously configured to pause or quiesce the application during data replication. For instance the data agent may cause the application to temporarily suspend data management operations to the source storage device once the application reaches a known good stable or recoverable state. In certain embodiments such a state may be defined as when particular computing operations of the application are complete to a point such that further operation recovery and or rolling back of the application may occur based on the recorded data without the loss of critical information or computing operations needed for operation of the application . This point of referential integrity is generally referred to herein as a known good state of the application .

In certain embodiments the data agent instructs the quiescing of the application through an application programming interface API . For instance the data agent may send a command e.g. FLRSNAP.FOO to the application that causes the application to quiesce. When the application has placed itself in a known good state the application may send an acknowledgment to the data agent .

In certain embodiments once the data management operations are suspended the I O buffers in the data path of the application are flushed and or the writes in the queues are flushed and the source logs are populated. For example some or all of the pending data management operations e.g. as of the time of the suspension of the application may be allowed to complete and or percolate through the data path. The filter driver and or data agent then inserts a logical marker or tag in the source log file denoting that a consistency point or consistency recovery point has been reached. In some embodiments the consistency point indicates the time at which the application is at a known good state. For instance in certain embodiments the data agent instructs the filter driver to insert a consistency point entry into the source logs .

Notwithstanding the foregoing it will be understood that in certain embodiments although application is quiesced it need not actually pause or suspend operation during the quiescent period. Rather the application may continue to operate substantially normally but may internally queue or otherwise buffer data management operations intended for the source storage device . After the quiescent period the buffered modification operations may be allowed to complete i.e. be sent to the source storage device .

In yet other embodiments policies for the frequency of consistency point entries may be automatically generated. For instance the data agent may be configured to quiesce the application based on the status e.g. capacity of the source logs the replication logs and or the destination storage device . In yet other embodiments quiescing of the application may be performed based on an automatic reporting procedure. For instance a module of the replication system may be configured to gather receive and or analyze information associated with a failure rate and or health of applicable servers. Additional details of such status monitoring are provided in U.S. patent application Ser. No. 11 120 619 filed May 2 2005 now published as US 2006 0053261 A1 which is hereby incorporated herein by reference in its entirety. For example the frequency of consistency points may be selected or adjusted to mitigate risks detected in a storage network.

In certain embodiments one or more log entries in the source logs are preferably associated with journal sequence numbers and or time information such as for example assigned a time stamp indicative of the client system time with which the particular log entries are associated. For instance the time information may indicate the time at which the log entry is written to the source log the data management operation is generated by the application the data modification operation is committed to disk or the like. In certain embodiments not all the log entries are assigned a time stamp. Rather particular types of data such as for example consistency point markers and or database commit entries are assigned time stamps.

In certain embodiments of the invention the data agent coordinates with the replication agent to copy log files from the source logs to the replication logs . Such copying may be initiated based on any suitable factor such as for example preset copying intervals capacity thresholds reached in the source logs time lapsed since the last copy operation replication agent requests for a copy operation and or based on specific parameters or requirements associated with a particular application . For instance certain data sensitive applications may be copied more frequently than other applications in order to reduce the amount of potential data loss due to a failure occurring between copy operations.

As further illustrated in the replication logs include a first log file and a second log file . In certain embodiments each of these log files corresponds respectively to the first log file and the second log file of the source logs . For instance data may be transferred between the replication log s and the source log s such that the order in which the data was stored in the source log s is preserved. In addition the log files may be recreated in the replication log s to reflect the organization of source logs . For example the first log file and the second log file in the source logs may be transferred and recreated by the replication agent and or the data agent . In other embodiments however data may be transferred and stored in a different order without preserving source system correlations and or may be rearranged on or during transfer to or upon arrival in replication volumes A B.

The illustrated destination system further comprises an optional preference database in communication with the replication agent . In certain embodiments the preference database includes storage policies or other preferences usable by the replication agent in managing data. For instance the stored preferences may indicate the desired frequency at which the threads should copy the data from the destination logs to the replication volumes A B. The preference database may also store path information for detailing to which location s on the replication volume s A B the data in the replication log s should be copied. In yet other embodiments the preference database may include storage policies that dictate particular criteria for performing one or more data management operations on the replicated data.

With continued reference to the replication module further comprises one or more processes such as a replication set or a log processing module with a first thread A and a second thread B. In certain embodiments as discussed above the threads A B are instantiated by the replication agent to transfer data from the first and second replication logs to the first replication volume A and or the second replication volume B.

In certain embodiments the threads A B utilize time stamp or other temporal information that enables processing and or replaying of modification operations. For example based on time stamp information the threads A B may rearrange the replication data such that the data is stored on the one or more replication volumes in the proper order e.g. the order in which the data was intended to be written to the source storage device . In such embodiments the replicated data may be subsequently retrieved recalled or otherwise accessed or processed and may be used to accurately restore the state of the application as it existed at a given point in time. In yet other embodiments other data management operations e.g. searching data classification may be performed on the replicated data.

In certain embodiments instructions for the storage operations are sent from the data agent on the source system . For instance the instructions may be included in the log file entries copied from the source system . In yet other embodiments the storage operations are coordinated by the replication agent e.g. according to storage polices stored in the preference database in combination with or independent of the data agent . In yet other embodiments policies for storage operations may be stored in another system management component e.g. a storage manager module .

In certain embodiments a snapshot is taken for each volume in which data is being replicated. For instance with reference to first thread A is writing to the first replication volume A and second thread B is writing to the second replication volume B. In such embodiments when the first and second threads A B arrive at a consistency point log entry a snapshot is taken of the replicated data in each replication volume A B.

In certain preferred embodiments when the snapshot is performed at a particular consistency point the time of the snapshot is advantageously logically associated with the time that the consistency point was generated at the client system e.g. the client system time of the known good state of the application . For instance the time stamp of the consistency point may be used to logically assign a time to the snapshot of the replicated data. In such a process the snapshot of the replicated data then appears as if the snapshot was directly taken on the data in the source system at the time of the consistency point. Such a process allows for the snapshot data to be viewed as a direct copy of the production volume data for a particular application e.g. source storage device at a certain point in time e.g. the time of a known good state of an application .

While certain embodiments of storage operations have been disclosed as being usable with the replication system of a wide variety of other storage operations may also be performed on the replication data and or in conjunction with consistency point information. For example other copies of the replicated data may be performed such as but not limited to creation storage retrieval migration deletion auxiliary copies incremental copies differential copies Hierarchical Storage Management HSM copies archive copies backup copies Information Lifecycle Management ILM copies other types of copies and versions of electronic data or the like.

In certain embodiments after appropriate storage operations are performed on the replicated data a message may be sent to other system management components e.g. a snapshot manager and or optional storage manager indicating that the replication process is complete up to the time stamp associated with consistency point. At this point the replication agent may instruct copy operations associated with the threads A B to resume.

In certain embodiments the log entry is initially generated by the filter driver and is stored in the source log . For example the log entry may comprise a data word having a plurality of fields. As illustrated the log entry comprises a log entry number field a path field a time stamp field an application type field a write type field a size field a checksum field an offset field and a payload field .

The log entry number field may include information regarding the entry number assigned to the log entry for system management purposes such that entries may be tracked and reordered relative to one another if necessary. For example as mentioned herein log entries may be arranged in a temporally sequential manner based on the application write operation with which the particular log entry is associated. In certain embodiments log entry numbers or other information may be recycled over time once all the numbers in a particular range have been used. In yet other embodiments the log entry number field may be configured to store other types of identification data for labeling the log entry .

The path field may include information regarding the file path on the source storage device with which the data write operation was associated. For example a path of C DIR USER may indicate that the log entry corresponds to an operation writing data to a folder or file on the source storage device having the designated pathname. In certain embodiments the path field may include an absolute file pathname. In other embodiments the path field may include an abbreviated pathname an FID and or an inode e.g. for UNIX based systems .

Moreover the path field may include information relating to the log entry s replication volume destination and thus may be useful in establishing or confirming correlation or pairing information used by the thread s A B. For instance in certain embodiments the file path of a particular log file may be hard coded to one or more particular replication volume s .

The time stamp field may include information relating to the time when the subject data write occurred. In certain embodiments the time stamp is advantageously associated with the time of the client computer on which the application is executing. For instance the filter driver may access the source system time when generating the log entry . In other embodiments the time stamp may be provided by the filter driver and or may be relative to the replication system time.

The application type field may include information identifying the application type with which the log entry is associated e.g. MICROSOFT OUTLOOK data MICROSOFT SHAREPOINT data ORACLE data SQL data MICROSOFT WORD data MICROSOFT INTERNET EXPLORER data or the like .

The write type field may include information regarding the category of write data involved with the log entry . For instance the write type may identify if the log entry is associated with a database modification a log write a database commit command a consistency point or the like. In certain embodiments the information in the write type field is used to implement parallelism between multiple threads when performing data replication. For instance a first thread e.g. thread A may handle log write commands and a second thread e.g. thread B may handle commit database commands. In certain embodiments the data stored in the write type field may be used for prioritizing the processing of various log entries e.g. processing by the threads .

The size field may include information relating to the size e.g. the number of bytes of the data being modified by the data write operation. In yet other embodiments the size field may contain information relating to the size of other or additional segments within the log entry such as for example the size of the payload field .

The checksum field may include information relating to error checking to ensure for example that the log entry when created and subsequently transmitted contains the expected number of bits and has not been corrupted or otherwise impermissibly changed. For instance the checksum field may store data representing the arithmetic sum of some or all of the fields in the log entry .

The offset field may include information relating to the location within a file or portion of data that the data write is occurring. For instance if the subject data write operation is associated with modifying the twentieth through the thirtieth bytes of a file or piece of data fifty bytes long the offset field may store a value of twenty. In such embodiments the information in the offset field may be used jointly with the information in the size field to identify the entire portion of a file being modified. For instance in the above example the size field may store a value of eleven to indicate the length of the modified section i.e. twentieth through thirtieth bytes .

The payload field may include information relating to the data written from the application to the source storage device . This information generally represents the application data captured by the filter driver for replication and may include additional information for the ongoing operation or reconstitution of the application .

It will be understood that the illustrative filter driver log entry shown in merely represents one possible embodiment of a log entry suitable for use with embodiments of the invention and that other embodiments may be used if desired. For example in other embodiments the log entry may comprise more or fewer fields to accommodate the requirements of the particular replication or storage operation system involved and or to achieve certain data or management goals such as conserving memory increasing processing speed and increasing the amount of information in each log entry. For instance in certain embodiments wherein the path determination for a particular log file or log entry is dynamic the log entry may not include the path field . In other embodiments the log entry may include a priority field that may be used for prioritizing replication and or data management operations of data associated with the log entry .

In other embodiments the log entry may concern a file attribute change rather than a data write operation. In such embodiments the write type field may identify the log entry as being associated with a file attribute change. Furthermore the log entry may store information regarding the new file attribute but would not require offset or size values to be stored in the size field and or the offset field .

In yet other embodiments as discussed in more detail below the log entry may not have a payload portion. Such embodiments can significantly reduce the size of the log files and or increase system performance since copies of the actual data entries are not needed. Rather information stored in the log entry can be used by a file system driver e.g. filter driver to obtain a copy of the data from the source storage device when need. Such information can be obtained from the path field size field offset field and or other data identification information such as inodes FIDs or the like.

In certain embodiments the storage manager maintains an index such as a cache for storing information relating to logical relationships and associations between components of the replication system user preferences management tasks and or other useful data. For example the storage manager may use its index to track the location and timestamps of one or more snapshots of the replicated data. In certain embodiments the storage manager may track logical associations between one or more media agents not shown and or storage devices.

The storage manager may also use its index to track the status of data management operations to be performed storage patterns associated with the system components such as media use storage growth network bandwidth Service Level Agreement SLA compliance levels data protection levels storage policy information storage criteria associated with user preferences retention criteria storage operation preferences and other storage related information. The index may typically reside on the storage manager s hard disk and or other database.

As shown in the storage manager further communicates with a database . In certain embodiments the storage manager database comprises a memory for storing system management information relating to the replication of data. For instance the database may be configured to store storage and or restore policies user preferences the status or location of system components or data combinations of the same and the like. In yet other embodiments the database may be configured to store information described above with respect to the index . In yet other embodiments at least a portion of the index may be stored on the database .

Additional details of storage manager modules useful with embodiments of the replication systems described herein are described in U.S. Pat. No. 7 389 311 issued Jun. 17 2008 which is hereby incorporated herein by reference in its entirety.

The initialization process begins with Block wherein one or more data agent s are installed on the client computer . In certain embodiments the data agent may be installed remotely from other portions of the replication system based on a particular need or to conform to certain directives or resident storage policies. In other embodiments the data agent may be installed locally by a system user as desired. For instance installation of the data agent may include deployment and installation of object code files and supporting software.

In certain embodiments the data agent may be installed for each application of interest or one or more data agents may be installed for a larger number of applications . Furthermore in certain embodiments an installation guide such as a wizard or other program may recommend the appropriate number and type of data agents to install which may be performed substantially automatically based on application and system configuration information .

At Block the installed data agents may perform certain auto discovery routines in order to determine basic system and application information. In some embodiments the auto discovery routines may be considered part of the installation process. For example the data agent may begin the auto discovery process by scanning and evaluating the folder and directory structure of the client computer to determine which folders are used by a particular application . In certain embodiments such information allows the data agent to identify and locate files or other information necessary to replicate the current operating state of the application of interest.

In certain embodiments the scanning and evaluation process may involve scanning multiple physical and or logical volumes associated with the source storage device and or within a given network or enterprise to locate the data and system configuration information necessary for data replication.

After the appropriate resources have been discovered and examined the data agent may identify arrange coordinate and or queue the necessary data within various locations or instances of the application to establish a platform for proper data replication Block . In certain embodiments this process may be a precursor for performing the initial seeding or synchronization operation described above.

Next at Block the data agent communicates with the replication agent . For instance the data agent may transmit to the replication agent information regarding the replication of data. The data agent may also request information from the replication agent and or other network management components for any information that may bear on or be related to the correlation or mapping of network storage paths for replication data. For example the data agent may consult the preference database of the destination system the preference database of the source system and or a storage manager component for correlation or pairing information. Based on this information data paths may be identified for use by threads when copying data from the replication logs to the replication volumes A B. In certain embodiments one or more data paths may be dynamically coded or determined such as for example based on one or more storage policies and or preferences.

At Block the initialization process includes installing and initializing the filter drivers . In certain embodiments such installation and or initialization is based at least in part on information obtained by the data agent during the discovery or scanning process Block . For example in certain embodiments one or more filter drivers may be installed by the data agent in the I O path of the application s .

The replication process begins with Block wherein the filter driver populates the source log s with data associated with the application such as data identified by the data agent . As discussed in more detail above such data may relate to data or file modification operations being passed from the application to the source storage device . In certain embodiments the filter driver populates the source logs in a temporally sequential manner such that operations and data are recorded in time descending or ascending order e.g. first operation at the top and last operation at the bottom .

In certain embodiments the data is populated in the source logs in a format similar to the structure of the log entry of . In other embodiments the data may be populated in other suitable formats to satisfy the requirements of the particular replication system. For instance the log file format may comprise a two or multi column structure wherein the information in a first column may indicate the type of data operation performed and the log entry s position in the log file indicates the order of the operation relative to other operations in the log file. The information in a second column may indicate the payload data associated with the data operation indicated by the first column.

After or concurrently with Block the data agent or other system component pauses or quiesces the application Block . As discussed above such quiescing causes the application to temporarily suspend data modification operations to the source storage device once the application reaches a known good state.

Once new modification operations are suspended and the associated source log is populated based on the modification operations up to the known good state the data agent or other replication system component inserts a logical marker or tag in the source log Block . This consistency point denotes that the state of the data is such that the application may be recovered or that further stable operation from that point going forward is ensured. Once the consistency point is identified and established the data agent may restart the application such that data modification operations from the application to the source storage device resume.

As referenced by Block the data agent or other system management component coordinates the transfer of the data in the source logs . In certain embodiments the data agent coordinates with the replication agent to copy data from the source logs to the replication log s . For instance the replication agent and or data agent may open a network path or a communication socket between the source log s and the replication log s . The log entries of the source log s may then be transferred as described above to populate the replication log s . In certain embodiments as the replication log is populated the replication agent may also obtain configuration information from the data agent or other system management component such as for example a storage manager. Such configuration information may identify aspects of the set of information being transferred as well as identify pairing information that correlates certain types of replication data with certain replication volumes or other storage destinations.

At Block the replication process includes instantiating one or more threads to begin the transfer of data from the replication log s to certain replication volumes A B. In certain embodiments the replication agent is configured to instantiate one or more of the threads A B. In certain embodiments the threads are instantiated and or particularized based on pairing or correlation information received from a management component and or based on certain system configuration information e.g. available replication volumes data path information the type of information in the transferred data set combinations of the same and the like. For example the replication agent may instantiate one or more threads that correlate certain data types with certain data volumes and may specify primary and alternate data paths.

Once instantiated the threads process and or traverse the replication log s until a consistency point is encountered Block . In certain embodiments when reaching a consistency point the thread stops scanning the replication log and notifies the replication agent that the thread has reached the consistency point Block .

In certain embodiments once all active threads associated with traversing the replication logs have notified the replication agent that a consistency point has been reached the replication process moves to Block . At this point the replicated data stored in the replication volumes A B preferably represents a known good state of the application .

At Block the replication agent suspends further operation by the threads . For instance the replication agent may suspend data writes to the destination volumes A B. At this point the replication process proceeds with Block wherein one or more storage operations e.g. snapshots may be performed on the replicated data which are described in more detail above.

As discussed above one of the advantages of the embodiments of the data replication systems disclosed herein is that such systems are capable of translating information intercepted by a filter driver on a first source system into information that is suitable for replay e.g. replication on a second destination system. In certain embodiments however the identification of files or directories in the source system may not be suitable for use with the directory structure of the destination system.

For example in UNIX based systems such as SOLARIS and LINUX file system operations are generally identified as operations on inodes or vnodes such that files are referenced by a unique inode number and or by a combination of one or more directory inode numbers and a short name. Such systems often utilize file name or pathname translation algorithms to implement a user level hierarchical view of the file system.

Such usage of inodes and short names however is not conducive for replaying data modification operations on a second system such as occurs in the data replication systems disclosed herein. That is a path having one or more inodes and or short names does not provide a destination system with the appropriate information for performing the replicated data modification operation.

On certain operating systems e.g. SOLARIS 10 LINUX 2.6 pathname translation may sometimes be performed within the operating system kernel by traversing backwards a directory name lookup cache DNLC . Using such translation systems in the data replication environment however may yield concurrency issues if certain locking processes are not performed. For instance in order to ensure that other threads or processes do not rename one of the components of a file s absolute path between the time that the thread computes the absolute path and the time that a relevant log entry is emitted the DNLC would need to be locked against updates from other threads during that period of time. Having this central lock on the DNLC however may impose severe performance penalties on the entire operating system.

As shown the data path comprises a filter driver . In certain embodiments the filter driver is configured to monitor data management operations such as data write operations or file attribute modification operations associated with a computer application executing on a source computer. For instance such operations may comprise changes to data in a production level memory. Examples of embodiments of filter drivers usable with the data path are described in more detail herein.

The filter driver is further configured to populate a queue with log entries or raw journal entries related to detected data modification operations from the application. In certain embodiments the log entries generated by the filter driver are each associated with an inode that identifies to which directory and or file on the source storage device the associated data modification was directed. The queue is configured to store the log entries until they are processed by a driver thread or process . In certain embodiments the queue is implemented in volatile memory on the source system.

The queue forwards the log entries to the driver thread . In certain embodiments the driver thread polls the queue for newly generated log entries by the filter . The driver thread subsequently stores the log entries in a buffer . In certain embodiments the buffer may be labeled a raw buffer in that it is configured to store raw log entries which were generated by the filter driver and or which do not yet have an absolute file pathname.

In certain embodiments the buffer is a memory based queue for storing the log entries until processed by a database thread or process . In certain embodiments the buffer advantageously facilitates and or expedites the unloading of raw records from expensive driver memory to swappable application memory. For instance the buffer may comprise an application level buffer of a size between approximately 40 megabytes and approximately 60 megabytes. In certain embodiments the buffer is advantageously implemented as a first in first out buffer.

In certain embodiments the database thread is advantageously capable of performing inode to pathname translation for each of the log entries in the buffer . After performing the translation the database thread may send the log entry with the absolute file pathname instead of the inode entry to a desired destination such as a replication system for further processing.

In certain embodiments the database thread is configured to access a pathname database to enable the thread to perform pathname translation. The pathname database advantageously stores information that associates one or more inodes or short names with an absolute file pathname. In yet other embodiments the pathname database may comprise other means or data for performing pathname translation including but not limited to a flat table customized code combinations of the same or the like.

In certain embodiments of the invention accessing the pathname database introduces delay into the data path . For example at certain points in the replication process the filter driver may generate log entries at a quicker pace than the pathname translations being performed by the database thread . For instance high activity disk lookups in the database for each log entry may require more time than the generation of the log entries by the filter driver .

In such embodiments the buffer is advantageously capable of adapting itself to the speed of the database thread . For example when the lookups by the database thread are relatively fast the buffer does not introduce significant delay into the data flow e.g. relatively no performance degradation due to the buffer . Thus the buffer may be advantageously sized to be relatively transparent to the data stream e.g. has a small footprint . However when the database lookups begin to slow down the buffer is able to store multiple log entries until the database thread is able to catch up.

Other mechanisms may be used to prevent user applications from over running the data path components e.g. the queue the buffer etc. . For example in some cases user processes can generate input output so fast that such components overflow and starts swapping. In such a case the filter driver or other appropriate component such as the driver thread may throttle the input output by introducing small delays into the input output path. For example the filter driver may lengthen the delays when an in memory queue maintained by the filter driver approaches a preconfigured limit. Where the input output throttling does not remedy the situation and overflow still occurs the system may abort and reinitialize the replication process.

In certain embodiments the database lookups by the database thread may become so time intensive that the maximum storage capacity of the buffer is reached. In such embodiments the buffer is configured to provide disk swapping functionality to avoid overflow of the buffer which may result in memory problems and or aborting replication. For instance as shown in the buffer may store excess log entries in a folder in memory . In certain embodiments the memory may comprise a disk and or may be located on the storage device of the source machine.

The illustrated pathname database is configured for inode to pathname translation such as for a UNIX based system. In particular the pathname database includes three columns a directory inode or parent inode column a short name column and an entry inode column . In yet other embodiments as described in more detail below the inode information in the database can be replaced with FIDs.

In certain embodiments each inode in a UNIX based system is recorded as an entry in the pathname database . For instance illustrates a system having four inodes each having a single entry in the entry inode column and having a value of 1 through 4. The corresponding short name column identifies the short name of the file or folder associated with the particular inode. For instance entry inode 4 identifies a folder or file with the short name of user while entry inode 1 identifies a root directory. The directory inode column or parent inode column identifies the inode of the parent directory to the particular entry inode. For instance entry inode 3 which has a short name of file is a child of the folder with an inode of 2. 

As can be seen from the illustrated pathname database when the database thread receives a log entry with a particular inode the database thread is able to access the pathname database and construct an absolute file pathname using the information stored therein for transmission to the destination system.

As shown the translation process begins at Block wherein the database thread receives a log entry to be processed. For example with reference to the database thread may retrieve the log entry from a buffer . In certain embodiments the log entry preferably represents a data modification operation associated with a particular application on the source system.

At Block the database thread identifies the inode associated with the particular operation represented by the log entry. For instance the inode may represent a file or folder to which data is to be written. In other embodiments the inode in the log entry may identify a file name to be modified or other data or file modification operation.

At Block the database thread accesses the pathname database to acquire information for translating the inode to an absolute file pathname. In particular the database thread searches the entry inode column for an entry that corresponds to the value of the log entry inode. Once the corresponding inode entry is found the database thread determines and stores the associated short name from the short name column Block .

The translation process then proceeds with Block . If the subject inode does not correspond to the root directory the database thread identifies from the directory inode the inode of the parent directory Block . The database thread then searches the entry inode column for the parent directory inode Block and adds the short name associated with the parent directory inode to the absolute file pathname Block .

The translation process then returns to Block to repeat the lookups and construction of the absolute file pathname until the database thread reaches the root directory. Once the database thread reaches the root directory the database thread stores the fully translated file pathname with the associated log entry Block and the translation process terminates.

For exemplary purposes the translation process will be now be described with reference to a data write command vop write 4 DATA and the values illustrated in the pathname database of . To begin the translation process the database thread receives the log entry representing the command vop write 4 DATA Block which corresponds to writing DATA to inode 4 on the source system Block .

The database thread then accesses the pathname database and searches the entry inode column for a value of 4 Block . Upon finding 4 in the entry inode column the database thread determines from the short name column that the short name corresponding to inode 4 is user Block .

Because inode 4 does not correspond to the root directory Block the database thread identifies from the directory inode column that the parent directory inode of inode 4 is inode 2 Block . The database thread then returns to search the inode entry column for the inode value of 2 Block determines that the short name for inode 2 is dir and adds dir to the file pathname Block .

Because inode 2 does not correspond to the root directory Block the database thread identifies from the directory inode column that the parent directory inode of inode 2 is inode 1 Block . The database thread then searches the inode entry column for the inode value of 1 Block and determines that the inode 1 corresponds to the root directory Block .

Now that the database thread has encountered the root directory Block the database thread stores the translated file pathname i.e. dir user with the subject log entry and the translation process terminates.

It will be understood that the translation process may differ in other embodiments of the invention in order to suit the needs of the particular system s involved. For instance the translation process may be used to translate particular inodes into file pathnames shorter than an absolute file pathname such as for example a relative pathname. In yet other embodiments the process can use FIDs in place of inodes to construct absolute file names of files on the source system.

In certain embodiments the three column database provides significant advantages over a flat two column table e.g. with an inode column and an absolute file pathname column . For instance the three column database structure of the pathname database may use less memory than the two column table and or expedite folder rename operations. As an example when a name of a folder is modified the three column database structure allows for a single lookup and modification e.g. modifying the short name column entry associated with the entry inode column entry of the subject inode while the two column table would require multiple lookups and modifications corresponding to each entry having an absolute file pathname that includes the folder to be renamed.

As discussed above in certain embodiments the pathname database is maintained in userland e.g. an application space external to the kernel space . In such embodiments the pathname database may be advantageously managed and or accessed by userland code without impacting the resources of the operating system kernel or other applications.

In certain embodiments the pathname database may be initially populated during an initialization period. For instance a snapshot may be taken to produce a static image of the file system of the source system. The pathname database may then be populated based on the snapshot. As subsequent changes are made to file names of the source system corresponding changes are made in the pathname database in order to maintain synchronization. In yet other embodiments as discussed in more detail below the pathname database can be populated based on scan of a live source file system.

In yet other embodiments the pathname database may be specific to the files and or folders of one or more particular applications. For example the pathname database may include inodes short names and related information only for those inodes affected by a single application e.g. MICROSOFT EXCHANGE . In yet other embodiments multiple pathname databases may be used.

As can be appreciated in the translation systems and methods described with reference to it can be important for the file system on the destination system to be in a synchronized state with the pathname database otherwise the replication system can encounter a file system error e.g. a file directory does not exist when attempting to apply a journal entry on the destination system. In certain embodiments this error can result in replication failure and require a resynchronization of the both the source and destination systems via the initial seeding or synchronization process discussed above.

Moreover as discussed certain embodiments of the initial synchronization process include performing an initial file system scan and populating the pathname database . As mentioned this can be performed by scanning a file system snapshot of the source system which obtains a static image of the file system. In particular the replication system can take a consistent snapshot in which for the duration of the snapshot e.g. the file system is flushed frozen and snapped by file system driver no namespace changing operations are allowed such as renames of a directory deletes and or creates. If the replication system detects that some of these operations have occurred during the synchronization process the replication system may need to delete the snapshot and re perform the scan. In certain circumstances especially in active file systems the replication system can get trapped in a virtually infinite loop trying to take the snapshot over and over again due to the constantly changing files.

In yet other embodiments the source file system may not support the taking of consistent snapshots or may require additional drivers to be installed thereby complicating the snapshot process. Moreover taking snapshots of a root file system can introduce even further complications. For example in some cases a root file system is allocated on a file system on which a snapshot cannot be taken or on which it is difficult to take a snapshot. For instance in Linux based systems the root file system is often located on a separate partition outside of the Linux Volume Manager LVM . Moreover system directories e.g. etc. var tmp tmp are often not sub divided and are therefore all located on the same root file system. In such cases it can be difficult to take a snapshot because modifications to these directories occur on an on going continual basis.

Thus certain embodiments of the invention provide systems and methods for producing a consistent image of a live source file system in a pathname or file name translation database and or on a destination system without requiring a static image of the file system. Such embodiments can advantageously allow for changes to the source file system to occur while other portions of the file system are being scanned thereby expediting the initial seeding of the file name database and or destination system and replaying of intercepted data changes on the replication system.

For example certain embodiments of the invention in a UNIX environment utilize FIDs for performing snapless synchronization and replication in a CDR system. In certain embodiments each FID comprises a sequence of between eight and sixteen bytes that uniquely identifies a file or folder in a file system.

In certain embodiments the FIDs are introduced part of a UNIX kernel for supporting stateless implementation of Network File System NFS version 3 or below. For example the NFS 3 file system can access files and directories via handles in which the file system encodes all relevant information that it needs to later translate the FID to the corresponding file or directory inode.

In certain embodiments NFS does not interpret contents of handles but it uses the contents to directly refer to the files and directories of interest. For instance these handles can contain FIDs file directory inode numbers generation numbers or the like. Moreover file systems that are NFS compatible i.e. can be exported via NFS can support the use of FID scanning as discussed in more detail below.

The use of FIDs can provide several advantages both during scanning and during replication such as for improving writes to the destination system. For instance systems can address subdirectories and carry on with scanning even while the user makes changes to the file system including such changes as renaming parent folders. Moreover because the file system translates FID information to locate files on a storage device e.g. mapping of FIDs to vnodes the FIDs can be used to identify and access files or folders that are renamed or moved in a file system.

Another advantage is that generation IDs that are encoded into FIDs give additional robustness. For instance if a file or directory is deleted and then recreated elsewhere during scanning the file system may reuse an inode number. However file systems using FIDs generally increment the generation ID portion of the FID with each new file system object thereby resulting in an absolute unique FID. Thus FIDs are unique both in space and in time and using them can reduce the chance of accidentally confusing an old file system object with a recreated one.

As shown the system includes the file system of the source computing system. In certain embodiments the file system comprises a UNIX environment implementing NFS. In other embodiments the file system can comprise a NFS compatible file system.

In communication with the file system is a scanning module . In certain embodiments the scanning module is configured to scan a live file system e.g. file system to build a database of FIDs and associated short names that reflect the structure of the file system such as during an initial seeding or synchronization phase of data replication. For instance the scanning module can advantageously populate the database without performing a snapshot of the source file system .

In certain embodiments the scanning module can comprise one or more filter drivers such as file system drivers that execute on a computing device such as the source computing device. In certain embodiments the scanning module can comprise one or more data agents . In yet other embodiments the scanning module can comprise a plurality of modules either in software or hardware that are configured to perform the functions described herein.

In particular the scanning module maintains a FID queue to assist with producing a consistent image of the live source file system . For instance the FID queue can store a plurality of FIDs processed by the scanning module to populate a database. In certain embodiments the queue comprises a first in first out FIFO buffer or other like memory.

The system further comprises a database thread configured to translate FIDs to absolute file names for replaying operations e.g. as stored in a replication log file on a destination system. For example after performing the file name translation the database thread may send a log entry with the absolute file name instead of the FID to a desired destination such as a replication system for further processing.

In certain embodiments the database thread is configured to access a file name database to enable the thread to perform file name translation. The file name database advantageously stores information that associates one or more FIDs with short names and directory information. For instance the file name database can be similar to the pathname database illustrated in with inode information of the pathname database being replaced with FID information. In yet other embodiments the file name database may comprise other means or data for performing file name translation including but not limited to a flat table customized code combinations of the same or the like.

For example the process may be performed on the file system of the source system of . For exemplary purposes the process will be described with reference to the components of the file name translation system of .

As shown the process begins with Block by establishing an empty queue such as queue for holding FIDs during scanning of the file system . At Block the process also creates an empty file name database as described above.

The scanning module then adds the FID of the source file system s root directory to the queue Block and obtains a file descriptor by opening the root directory in read only RO mode Block . The scanning module can obtain the file descriptor by issuing an open call for example. The file descriptor may comprise an integer or other appropriate identifier in userland and can be used as a file handler or file identifier for input output. Behind the scenes in the kernel the file descriptor number can be associated with the corresponding file object. Thus when a userland application writes some data to a file descriptor the kernel is aware of what object the data should be written to. At this point after obtaining the file descriptor the process begins a recursive procedure for stepping through each of the directories in the file system and populating the database with information usable to recreate a consistent image of the file system on the destination system.

As shown at Block the scanning module obtains the next FID from the queue . In the initial pass through the process the FID will generally be the root directory FID. At Block the scanning module asks the filter driver to associate in the kernel the appropriate previously obtained file descriptor with the current FID. In certain embodiments the scanning module invokes an ioctl API e.g. FLR OPEN BY FID fd FID that receives both a pre open file descriptor and an FID. In response a file system filter driver then converts the FID to a file system vnode via a file system provided API and inserts the obtained vnode into the handler or file structure corresponding to the passed file descriptor. Once this has completed the application can then access the file or directory by making usual system calls and passing them the modified file descriptor.

At Block the scanning module scans the directory corresponding to the FID using the associated file descriptor. For example the scanning module steps into the directory associated with the current FID such as through invoking the fchdir fd command and reads each of the direct directory children such as through the opendir . and readdir commands. At Block for each detected subdirectory the scanning module appends the FID associated with the subdirectory to the end of the queue for further analysis.

At Block for each of the immediate directory children the scanning module also populates the file name database with the FID and relative short name information. For instance the scanning module may insert a row in the database that includes a parent directory FID a short name of the file or folder and the entry s FID see e.g. .

Moving to Block the scanning module determines if there are additional FIDs stored in the queue . If so the scanning module returns to Block to obtain the next FID from the queue and to step through the immediate children of the directory associated with that FID. In certain situations with the continuously changing file system and the possibility that the same directory is scanned more than once the scanning module can further resolve possible structural inconsistency problems between the scan list of direct children and the contents of the file name database Block . This technique is described in further detail below e.g. with respect to the process of .

When there are no additional FIDs stored in the queue the process concludes and monitors log or journal entries for any changes to the source file system directories Block .

As can be seen with the process the scanning module does not address directories or files by absolute file names. Rather the scanning module scans each of the file system directories individually by addressing each directory by its unique FID and by populating the database with the FIDs of children along with their relative short names. UNIX systems typically do not allow direct userland access to file system objects using FIDs. Thus the process generally constructs a dummy file descriptor that is initially associated with the root directory. The filter driver then locates desired file or directory objects e.g. children files or directories by their FID and associates those objects with the dummy file descriptor. Thus a userland application can then e.g. use a read directory operation to obtain the list of children.

One of the benefits of the snapless scanning process of is that the file system can undergo changes by the user during scanning without requiring a rescan of the entire file system. However in certain circumstances although the user s changes are intercepted by a file system driver and are appended to a change journal for further replay and although the scanning module is made less sensitive to user s changes by using the FID driven scan replication processes disclosed herein can still encounter problems in the replicating phase when trying to replay collected journal entries on the destination system and or when performing database lookups.

For instance in the case of snapshot based scanning e.g. the image on the destination system and the contents of the pathname database are populated based on a point in time replica of the entire file system i.e. the snapshot . Thus all pending journal entries can be applied to the database and or replayed on destination system following the initial scan in the order that they were generated because logically the journal entries are generated after the snapshot.

However in the case of snapless scanning e.g. each directory is scanned at a different time and likely during user modifications to different portions of the file system. Thus it becomes important to know if a particular folder or directory was scanned before or after a particular journal entry was generated. That is if a journal entry associated with the contents of a folder is generated before the scanning of the folder it may not be appropriate to apply the journal entry to the folder. Otherwise there would be a risk of introducing inconsistencies into the file name database which can require the rescanning of one or more directories in order to repopulate the database .

To address such risks of inconsistencies in certain embodiments of the invention when scanning a particular directory during an FID driven scan systems and methods can query the file system driver for its current journal sequence number. The sequence number is then stored in the file name database or other location along with the identification of the current directory s children. When the system is to apply a stream of journal entries to the database the system can ignore all journal entries that were generated before the subject directory was scanned as identified by the stored journal sequence number.

The scanning module obtains the first directory FID from the queue and associates a file descriptor with the FID such as via a ioctl call e.g. a FLR OPEN BY FID fd FID command . For example these actions may be performed in the manner described above with respect to Blocks and . At Block the routine steps into the directory of the current FID. For example as discussed above this can take place by the scanning module invoking the fchdir fd command.

At Block the scanning module obtains the current journal sequence number. In certain embodiments the sequence number is assigned by a file system filter driver to each newly generated journal entry being incremented with elementary changes made to the file system. In certain embodiments the scanning module obtains the sequence number from the filter driver and can be used as a measure of time and to advantageously coordinate the file system scan results with the stream of journal entries generated due to user s changes to file system data.

Upon obtaining the journal sequence number the scanning module begins monitoring the current directory for changes Block . For instance the scanning module can invoke an ioctl call that takes the FID of the current directory and initiates collecting statistics for the directory in the driver. In certain embodiments collecting the statistics comprises utilizing a counter of namespace changing operations applicable to the current directory and intercepted by the driver.

The scanning module then obtains the FID for each immediate child in the directory Block as discussed in more detail above with respect to Block . After processing each of the immediate children in the current directory the scanning module determines if there were any structural changes to the directory during the scan Block . For instance the scanning module could issue an ioctl call that stops the collecting of statistics e.g. by the driver and returns the number of namespace changing operations that happened since Block .

If there were structural changes to the directory during that time the routine assumes that the scan was not clean and repeats the scan process for the current directory by returning to Block . On the other hand if no changes are detected the routine continues on with the file scanning process. For example the routine proceeds to populate the file name database with the FIDs of direct children of the current directory. This can occur in the fashion described above with respect to the process of Block . In certain embodiments the process also includes storing the scan sequence number obtained at Block for the current directory. The routine in one configuration also stores the sequence number obtained at Block in the file name database . For example the stored sequence number can then be used during replication to apply collected log entries as described below with respect to .

In certain situations with the continuously changing file system and the possibility that the same directory is scanned more than once the scanning module can further resolve possible structural inconsistency problems between the scan list of direct children and the contents of the file name database . For instance the scanning module can request a rescan of suspicious file system objects by appending their FIDs to the queue and or by re parenting to null fid all database children of the current directory that are not identified on the scan list.

In certain embodiments for files that are on the scan list but are not identified in the database as children of the current directory the routine such as through the database thread can add rows corresponding to the files in the database . For subdirectories that are on the scan list but are not identified in the database as children of the current directory the routine can determined if the database already has an entry for the subdirectory. If so and the subdirectory is identified as a child of another directory the database thread can re parent the subdirectory to the current directory and request a re scan of the previous parent by obtaining its FID and appending it to the queue . Otherwise the database thread can add a new row describing that the subdirectory is the child of the current directory and can append the FID of the subdirectory to the FID queue thereby ensuring that before the scan completed the child subdirectory will be recursively scanned as well.

After the file name database is initially populated certain embodiments of the invention are configured to detect file system changes on the source system and replicate such changes to the file name database and ultimately to the destination system. illustrate a flowchart of an exemplary replication process for interleaving a stream of journal entries with the results of the live file system scan in the database such as generated by the process and or routine . For exemplary purposes the process will be described will be described with reference to the components of the file name translation system of .

In general the process includes obtaining and comparing sequence numbers of journal entries with scan sequence numbers of respective FIDs in the database . Based on this comparison the process determines whether or not to apply the journal entry to the database and destination system to discard the journal entry or to trigger an additional FID scan.

The process begins at Block by obtaining the next journal entry and its associated sequence number from the file system driver such as the filter driver or a source log . The process determines if the current journal entry is associated with a rename or move operation of a file or subdirectory from one parent directory to another Block . If not the process then determines if the current journal entry is associated with a create or remove operation of a file or subdirectory in a parent directory Block . If not the process returns to Block to obtain the next journal entry.

If it is determined at Block that the journal entry is associated with a create or remove operation the scanning module and or the database thread look up the parent directory s FID in the database to obtain the FID s scan sequence number such as discussed with respect to Block Block . If the sequence number of the journal entry is less than or equal to the FID sequence number Block the process disregards the journal entry under the assumption that the journal entry was generated before the scanning of the corresponding portion of the file system Block . The process then returns to Block to obtain the next journal entry.

At block the process updates the database to reflect or include the obtained journal entry. If the sequence number of the journal entry is greater than the FID sequence number in the database the database thread obtains from the database the absolute file names of both the parent directory and the created removed entity Block . With the information the database thread sends the journal entry for replay on the destination system Block . The process then returns to Block to obtain the next journal entry.

If at Block the journal entry is associated with a rename or move operation of a file or subdirectory from one parent directory to another the process moves to Block to obtain from the database the FIDs of both the source and destination parent directories. If the journal entry sequence number is greater than the sequence number associated with the scan of the source directory Block the process then determines if the journal entry sequence number is also greater than the sequence number associated with the scan of the destination directory Block . If it is not the database thread removes the child from the source directory in the database Block and converts the journal entry from a rename operation to a remove operation Block . The database thread then sends the journal entry to the destination system to remove the child from the source directory Block . The process then returns to Block .

If at Block the journal entry sequence number is determined to be greater than the sequence number associated with the scan of the source and destination directories the database thread applies the journal entry to the database Block . The database thread further obtains from the database the absolute file names of the involved file system objects Block and sends the journal entry to the destination system for replay Block . The process then returns to Block .

If at Block it is determined that the journal entry sequence number is less than or equal to the sequence number associated with the scan of the source directory the process further determines if the journal entry sequence number is greater than the sequence number associated with the scan of the destination directory Block . If so the process recognizes that the FID of the object being moved is not in the file name database . That is the source directory was scanned after the rename was detected and the destination directory was scanned before the rename was detected indicating that the scanning module missed the moved file system object. In this situation the process repeats the file system scan beginning with the FID of the object moved in the rename operation Block . The process then returns to Block to obtain the next journal entry.

If at Block it is determined that that the journal entry sequence number is less than or equal to the sequence number associated with the scans of the source and destination directories the process disregards the journal entry i.e. occurred before scans of both source and parent directories Block and returns to Block to obtain the next journal entry.

In certain embodiments the use of FIDs in file system scanning and or causing the associated filter driver to refer to affected file system objects by FIDs can advantageously provide for more efficient handling of write operations. For instance written data does not need to be journaled from the file system driver to userland. Rather the FID of the modified file and the offset length of the modified regions can be sent to the userland application for use in reading the data directly from the file by opening the file with the FID and by merging the modified byte ranges.

This process can provide several benefits. For instance not piping written data from the driver to the userland application or journal can provide significant improvements in performance. That is copying data first in the driver s memory then passing the data to the userland application and storing the data in the disk cache can be quite expensive. By not journaling the actual data but obtaining the data when needed directly from disk system performance can be improved several times.

In yet other embodiments the use of FIDs in combination with a slight delay in the actual transfer of data to the destination system can allow replication systems to accumulate a list of changed byte ranges in memory. This can provide further advantages in that the replication system can analyze the changed bytes and optimize and or improve replication of data to the destination system.

For example in certain embodiments inventive systems and methods can combine multiple write operations into a single write operation based on the FIDs and byte ranges associated with operations by one or more applications. For example the journal entry stream identifying the data operations intended for the source file system can be modified to refer to FIDs instead of inode numbers and to journal the offset and or length of overwritten byte ranges instead of actual data. This allows systems and methods to obtain written data directly from disk thereby achieving a significant improvement in performance.

For instance in certain embodiments the file system filter driver and or data agent s monitoring data operations can write repetitive writes to a single location. In yet other embodiments the file system driver can combine modified adjacent byte ranges into a single write operation. In further embodiments the file system driver can read non combinable byte ranges in the order of increasing file offsets thereby obtaining better performance from the file system and during subsequent replication.

In certain embodiments the file system driver and or data agent s can improve replication with respect to temporary files. The phrases temporary file or temporary data are broad terms and are used herein in their ordinary sense and include without limitation data that is created by a program or application e.g. editors and compilers for some transitory purpose but deleted later generally within a short period of time.

For instance in conventional replication systems when an application creates a temporary file the new contents of the temporary file are generally replicated from the source to destination system. A brief time later the REMOVE command is replicated from the source system that deletes the transferred data on the destination system mimicking the manner in which the file was created and removed by the application s on the source system.

By introducing the slight delay in the replication process such a by accumulated a number of journal entries inventive systems and methods may encounter an error e.g. a no such file or directory or file not found error from the file system when attempting to read the contents of a temporary file when within the delay period the temporary data has been removed from the source system. As a result the replication system does not send the temporary data across to the destination system as the file system is not able to locate the deleted file by the FID.

As shown at Block the process begins by receiving from the filter driver a first journal entry related to a modified file. In certain embodiments the file system filter driver intercepts or otherwise accesses a data modification operation sent by the application s . At Block the data agent identifies the FID of the file to be modified on the source system and the offset and length of the modified portions of the file. In certain embodiments the filter driver advantageously does not store or otherwise retain a copy of the actual data to be modified for each such data modification operation.

Also instead of immediately transmitting to the destination system the logs and data associated with replaying the data operation on the destination system the process introduces a delay in the replication of data. In certain embodiments this delay is between approximately three and four seconds in other embodiments the delay can be of a shorter or longer duration.

Due to the delay the data agent receives at least a second journal entry before the data associated with the first journal entry is accessed Block . At Block based on the second journal entry the data agent identifies the FID of the file to be modified on the source system and the offset and length of the modified portions of the file. Based on the data location information received from both the first and second journal entries the data agent determines if the data modification operations from the two journal entries are write operations for the same data Block . If so the filter driver processes only the later data write operation associated with the second journal entry and accesses the modified data portions on disk for transmission to the destination system Block . The earlier data operation of the first journal entry is ignored as being out of date.

However if the data operations are for different file regions the data agent determines if the operations concern writes to adjacent byte ranges that can be combined Block . For example the data agent can determine if the distance between the two byte ranges is larger than a predetermined threshold. For instance the threshold can be based on the size of overhead e.g. a header associated with journal entries. In certain embodiments the threshold distance is 200 bytes. In yet other embodiments the distance can be larger e.g. 1 KB or shorter and or dynamically adjusted.

If the distance between the two byte ranges is less than the threshold the process combines the separate write operations of the first and second journal entries into a single journal entry having a single write. In this case the single write operation is replayed on the destination system with both byte ranges being replicated Block .

If the byte ranges are sufficiently separated the data operations from the two journal entries cannot be combined and the process handles the journal entries separately Block . That is the data agent accesses each of the modified portions of the file s based on the information in the two journal entries. If either of the data access requests results in a particular type of file system error such as a no such file or directory or file not found error Block the process discards the journal entry associated with the request Block . For instance in certain embodiments due to the introduced delay by the time the data is requested the data may have already been deleted moved or removed such as is the case with temporary files.

Finally if no error is received when trying to access the data the process transfers the modified portions pertaining to each journal entry for replay and replication on the replication system . In certain embodiments the transfer and or replay of the journal entries can be performed in order of increasing file offsets especially with journal entries associated with the same FID.

Although the process is described with reference to particular arrangements it will be understood that other embodiments of the invention may have more or fewer blocks that those described above. For instance the data location information extracted from the second journal entry can be further compared with data location information of a third journal entry or additional journal entries Thus as can be seen the process can be repeated for each subsequent journal entry captured by the filter driver .

Embodiments of the invention have been described herein with reference to UNIX file systems and can include LINUX XFS Veritas EXT3 file systems and the like.

In certain embodiments of the invention data replication systems and methods may be used in a modular storage management system embodiments of which are described in more detail in U.S. Pat. No. 7 035 880 issued Apr. 5 2006 which is hereby incorporated herein by reference in its entirety. For example the data replication system may be part of a storage operation cell that includes combinations of hardware and software components directed to performing storage operations on electronic data. Exemplary storage operation cells usable with embodiments of the invention include CommCells as embodied in the QNet storage management system and the QiNetix storage management system by CommVault Systems Inc. Oceanport N.J. and as further described in U.S. Pat. No. 7 454 569 issued Nov. 18 2008 which is hereby incorporated herein by reference in its entirety.

Systems and modules described herein may comprise software firmware hardware or any combination s of software firmware or hardware suitable for the purposes described herein. Software and other modules may reside on servers workstations personal computers computerized tablets PDAs and other devices suitable for the purposes described herein. Software and other modules may be accessible via local memory via a network via a browser or via other means suitable for the purposes described herein. Data structures described herein may comprise computer files variables programming arrays programming structures or any electronic information storage schemes or methods or any combinations thereof suitable for the purposes described herein. User interface elements described herein may comprise elements from graphical user interfaces command line interfaces and other interfaces suitable for the purposes described herein.

Embodiments of the invention are also described above with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams may be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable memory that can direct a computer or other programmable data processing apparatus to operate in a particular manner such that the instructions stored in the computer readable memory produce an article of manufacture including instruction means which implement the acts specified in the flowchart and or block diagram block or blocks. The computer program instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operations to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide steps for implementing the acts specified in the flowchart and or block diagram block or blocks.

In addition methods and functions described herein are not limited to any particular sequence and the acts or blocks relating thereto can be performed in other sequences that are appropriate. For example described acts or blocks may be performed in an order other than that specifically disclosed or multiple acts or blocks may be combined in a single act or block.

While certain embodiments of the inventions have been described these embodiments have been presented by way of example only and are not intended to limit the scope of the disclosure. Indeed the novel methods and systems described herein may be embodied in a variety of other forms furthermore various omissions substitutions and changes in the form of the methods and systems described herein may be made without departing from the spirit of the disclosure. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of the disclosure.

