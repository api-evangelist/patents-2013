---

title: Methods and systems for robust supervised machine learning
abstract: A disclosed method may include iterating a model optimization process, the iterating including one or more iterations. The method may also include updating a classification model based on the iterating, the updating performed using training data. The method may further include generating a final version of the classification model based on a final iteration. The method may also include setting a parameter (q), the parameter corresponding to a total number of observations (Q) that are to be removed from the training data by the final iteration. The method may further include determining one or more corresponding numbers of observations to remove from the training data, where the corresponding number of observations are to be removed at some of select iterations t, and the corresponding number of observations are to be removed based on the number Q and an estimate of the number of iterations remaining until the final iteration.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09652722&OS=09652722&RS=09652722
owner: The Mathworks, Inc.
number: 09652722
owner_city: Natick
owner_country: US
publication_date: 20131205
---
The present disclosure relates generally to methods and systems for robust supervised machine learning.

In the field of machine learning one goal is to separate data into classifications based on patterns. Supervised learning is one type of machine learning in which a classification model is generated based on a data set comprising observations each observation being a data point that belongs to known classes also referred to as labels. The classification model that is generated by supervised learning can then be applied to classify other data sets to predict the labels of the observations in those data sets which are not known.

Generation of a classification model by supervised learning is based on using a specific type of model. Support vector machine SVM is one popular classification model. An SVM model is characterized by a decision boundary which is a function that defines the boundary between observations that should be classified into different classes. A generalized SVM model may have a simple decision boundary e.g. a linear decision boundary which does not classify every observation in the training data set correctly. A less generalized SVM model may have a highly non linear decision boundary e.g. a very squiggly decision boundary which classifies every observation in the training data set correctly. If a generalized SVM model consistently classifies data with an acceptable rate of classification accuracy over a range of different data sets the SVM model is robust. Less generalized SVM models may perform inconsistently over a range of different data sets because the decision boundary was generated so specifically to the training data set. These SVM models are not robust. The disclosed methods and systems are directed to generating robust SVM models based on removing outliers from the training data set.

The following detailed description refers to the accompanying drawings. Wherever possible the same reference numbers are used in the drawings and the description to refer to the same or similar parts. Also similarly named elements may perform similar functions and may be similarly designed unless specified otherwise. Numerous details are set forth to provide an understanding of the described embodiments. The embodiments may be practiced without these details. In other instances well known methods procedures and components have not been described in detail to avoid obscuring the described embodiments. While several exemplary embodiments and features are described modifications adaptations and other implementations may be possible without departing from the spirit and scope of the invention. Accordingly unless stated otherwise the descriptions relate to one or more embodiments and should not be construed to limit the inventions as a whole. Instead the proper scope of the inventions is defined by the appended claims.

In various embodiments processor may be one or more processing devices such as a microprocessor or a central processor unit CPU and may perform various methods in accordance with disclosed embodiments. Memory may include a non transitory machine readable medium such as a computer hard disk a random access memory RAM a removable storage a remote computer storage etc. In various embodiments memory stores various software programs executed by processor .

Display may be any device which provides a visual output for example a computer monitor an LCD screen etc. I O interfaces may be any device which receives inputs and or provide outputs including a keyboard a mouse an audio input device a touch screen an infrared input interface etc. In some embodiments the I O device is an interface for receiving and sending data to a user or to another system. In various embodiments the I O device includes an input interface and an output interface. The input interface is configured to receive data such as commands input parameters and training data and the output interface is configured to output information such as a classification model or classifications of data . In some embodiments the I O module may include an internet interface a wireless interface a data reader a printer etc. Network adapter may enable system to exchange information with external networks. In various embodiments network adapter may include a wireless wide area network WWAN adapter or a local area network LAN adapter.

Model optimizer may receive training data and iterate a model optimization process that improves a classification model for the training data. With successive iterations model optimizer may generate an updated classification model with improved classification accuracy. In an embodiment each iteration may be associated with an iteration index t e.g. 1 to T iterations. In one example the classification model may be a SVM model and the optimization process may be for example an iterative single data algorithm ISDA a sequential minimal optimization SMO quadratic programming optimization and or any appropriate optimization process. In another example the classification model may be something other than an SVM model such as other types of classification models known in the art.

Convergence predictor may record convergence criterions of the classification model. In an embodiment convergence predictor may record after the classification model is updated by an iteration of the model optimization process. The convergence criterion is a measure of how accurately the classification model classifies the observations in the training data. Convergence predictor may record a convergence criterion at specific iterations as discussed in further detail below.

At certain iterations convergence predictor may predict a trend of the convergence criterions with respect to the iteration index. Convergence predictor may predict the trend based on the convergence criterions recorded at previous iterations and may further predict a number of additional iterations needed before the convergence criterion of the classification model matches or is below a desired tolerance threshold.

Observation remover may remove observations from the training data after some number of iterations. For example observation remover may determine how many observations to remove based on the predictions generated by convergence predictor .

System may receive a set of training data which includes N observations. Each observation may be associated with a known classification. At step system may begin iterating a model optimization process e.g. ISDA SMO quadratic programming etc. for a classification model e.g. SVM model . Each iteration may be associated with an iteration index t e.g. the initial classification model is generated at t 1 the initial classification model is updated in the next iteration of the optimization process at t 2 etc. .

In one example in which the classification model is a SVM model the model optimization process may attempt to solve the following quadratic programming QP problem 

The KKT conditions use C as a positive box constraint on the coefficients and as non negative slack variables. After the coefficients are found by solving the QP problem a prediction SVM score at point x can be computed by Eq. 6 where b is a bias term. The predicted score f x may range continuously from Inf to Inf. Large negative values represent confident classification into class 1 large positive values represent confident classification into class 1 and values near zero imply uncertain classifications.

Several algorithms can be used to solve the QP problem shown above on large datasets such as ISDA and SMO. ISDA and SMO are based on an iterative process that inspects a subset of the data at each step. The number of iterations to convergence for the process can vary. For example the number of iterations may vary from a few dozen to a few million. One way SMO differs from ISDA is that SMO respects a linear constraint 0 Eq. 7 In some embodiments for small datasets a standard QP solver such as for example interior point methods may be used.

After an iteration of the model optimization process at step system may determine if the iteration index t equals or is greater than a burn in parameter T step . In one embodiment a user may provide Tas an input. In another embodiment Tmay be a pre set default value e.g. 1000. If the iteration index is not equal to or greater than the burn in parameter T step No system may perform another iteration of the model optimization process step and the iteration index increases by one. Until the iteration index t is equal to the burn in parameter T system may simply continue to iterate the model optimization process. This period is referred to as the burn in period.

When the iteration index t eventually equals or is greater than the burn in parameter T step Yes system may determine if the iteration index t is equal to one of predetermined indices t step . In one embodiment tof step may be set by a user by the user inputting a parameter k where k represents a constant interval. In such embodiment indices tmay be iteration indices that are spaced every k indices. For example if k 100 then tmay be the iteration indices 100 200 300 400 . . . . In another embodiment k may be pre set to a default value independent of a user s setting k. If the present iteration index t is equal to one of the tindices step Yes then system may record a convergence criterion of the current classification model as updated by the latest iteration step . If the iteration index t does not equal t step No then system may proceed to step .

After recording the convergence criterion at step system may determine whether the iteration index t is equal to or greater than t step . tof step represents the first iteration index at which observation removal from the training data may begin. In one embodiment a user may provide tas an input. In another embodiment tmay be pre set to a default value. If the iteration index is not equal to or greater than t step No then system may proceed to step . If the iteration index equals t step Yes system may remove observations from the training data step . Details of step are described in further detail with reference to .

At step system may calculate the number Mof observations to remove in the current iteration. For example there may be total number of desired observations that system is to remove by the actual final iteration tat which the criterion value meets the tolerance threshold . In one embodiment this total number may be based on a parameter q which represents a fractional number of the total number of observations in the training data that is to be removed by the final iteration t. A user may provide the parameter q or the parameter q may be a pre set default number. Sometimes to set the q parameter the user may attempt to predict the number of outliers that exist in the training data set. Outliers are observations of one class that are found within a range that is predominantly classified as the other class. q may also be optimized by the user or system for example by trying several values of q and choosing one that produces the model with the lowest generalization error. The total number of observations that are to be removed by the final iteration is represented by q N Q wherein N is the original number of observations in the training data. System may then determine the number Mof observations that should removed in the current iteration using for example Eq. 8 

At step system may calculate the gradients of all of the observations currently in the training data set. In one embodiment the gradient of each observation may be based on a summation of contributions from every other observation in the training data set. For example the gradient may be determined according to Eq. 9 1 1 Eq. 9 where 

 is the coefficient parameter corresponding to the ith observation the coefficients being what the model optimization process is attempting to optimize and updates with each iteration 

If an observation is confidently classified into its true class f x has the same sign as yand is large in magnitude its gradient is positive and large. If an observation is badly misclassified f x has the sign opposite to yand is large in magnitude its gradient is negative and large.

At step system may remove the number Mof observations with the largest magnitude gradients from the training data set In some embodiments system may remove both positive and negative gradients. In some other embodiments system may remove only the observations with the largest negative gradients i.e. the badly misclassified observations . In some embodiments system may not remove any observations from the training data if Mis zero or negative. In some embodiments system may add observations that were previously removed back into the training data if Mis negative. Subsequent iterations of the model optimization process will use the updated training data set with the observations removed instead of the full training data set.

Because in some embodiments Mis based on how quickly the convergence criterion approaches the tolerance threshold the number of observations to be removed from the training data may depend on how quickly the convergence criterion approaches the tolerance threshold . If convergence is fast system may remove many observations from the training data set at each iteration index t. If the convergence is slow system may remove few observations from the training data set at each iteration index t. If there is no convergence system may not remove any observations from the training data set or optionally may add back in observations that were previously removed from the training data set.

In some embodiments ISDA is the model optimization process being used. In such embodiments because ISDA does not respect the linear constraint of Eq. 7 removing observations from the training data may be a straightforward process of removing Mobservations. However in some embodiments in which SMO is the model optimization process being used because SMO does respect the linear constraint of Eq. 7 this linear constraint must be satisfied when observations are removed from the training data set. In an exemplary embodiment the method of includes an additional step to satisfy the condition of the linear constraint. At step if the Mobservations slated for removal satisfy the linear constraint of Eq. 7 i.e. y 0 system may remove all of the slated Mobservations from the training data. If the observations do not satisfy the linear constraint system may find a modified set of coefficients tilde over coefficients for the Mobservations such that tilde over y 0 while attempting to maximize the number of zero tilde over coefficients. The observations for which tilde over 0 are then removed from the training data set and the coefficients for the remaining observations are updated by updating with tilde over .

At step for both ISDA SMO or other classification models system may set the coefficients corresponding to the observations that were removed to zero and recalculate the gradients for all of the observations remaining in the training data to reflect the fact that the removed observations are no longer in the training data.

With reference back to at step system may determine if the convergence criterion of the classification model has reached a tolerance threshold . A user may input the tolerance threshold or the tolerance threshold may be pre set to a default value. If the convergence criterion does not meet the tolerance threshold system may return to step and perform another iteration of the model optimization process. If the convergence criterion meets the tolerance threshold then system may output the classification model generated by the last iteration of the model optimization process.

Note that when system repeats step in the situation where the tolerance threshold has not been met yet system will generate new estimates of tilde over t the estimated number of iterations until convergence is met. These new estimates will be based in part on new convergence criterions that are recorded at titerations which may change the overall estimated slope of the convergence criterions with respect to the iteration index and the extrapolation of the convergence criterion to the final iteration. Therefore although a final iteration tat which the tolerance threshold is met is not actually known system may generate estimates tilde over t of the final iteration that are periodically updated. Based on the updated estimates tilde over t the system can also periodically update its estimate of how many observations need to be removed at the titerations such that Q observations are removed by the final iteration at which the tolerance threshold is met. While in some embodiments the frequency of recording convergence criterions and removing observations may be the same e.g. every k iterations in other embodiments the frequency of recording convergence criterions and the frequency of removing observations from the training data set may be different. For example observations may be removed every k iterations and convergence criterions may be recorded every m iterations.

Normal SVM is described for example in R. E. Fan P. H. Chen and C. J. Lin Working set selection using second order information for training support vector machines. 6 1889 1918 2005. The convex concave procedure CCCP is described for example in R. Collobert F. Sinz J. Weston and L. Bottou. Trading Convexity for scalability and in In 23 ICML 06 pages 201 208 New York N.Y. USA 2006. ACM. Quantile SVM is described for example in S. Ertekin L. Bottou and C. L. Giles. Ignorance is bliss Non convex online support vector machines. 33 2 368 381 February 2011 and the ConvexRelax model is describe for example in Y. Ma L. Li X. Huang and S. Wang. Robust support vector machine using least median loss penalty. In 18 volume 18 pages 11208 11213 2011 and L. Xu K. Crammer and D. Schuurmans. Robust support vector machine training via convex outlier ablation. In 21 Volume 1 AAAI 06 pages 536 542. AAAI Press 2006.

The classification error of the model generated according to this disclosure indicated by labels is less than that of other models indicated by labels and particularly at larger R where R represents a measure of label noise. Large values of R correspond to there being many outliers in the data and small values of R correspond to there being a few outliers in the data. illustrates another graph comparing models. In the numbers of support vectors included in respective models are compared. The model generated according to the present disclosure is indicated by label . The other models are indicated by labels normal SVM CCCP Quantile SVM and ConvexRelax . In general an SVM model with a smaller number of support vectors is more desirable because such model may consume less computer memory and use less of the CPU to generate predictions for new data. In addition if a model has only a few support vectors the model may be open to easy interpretation by a human. The model generated according to the present disclosure as indicated by label includes fewer support vectors than some of the models generated using conventional techniques e.g. the models indicated by labels and . For both of the graphs shown in and values of the q parameter used to generate the models were optimally set regardless of the true fraction of outliers in the data. Table 1 compares the training times and sparsity number of support vectors of the different models for a given training dataset the true number of outliers in the dataset in the example is 0.2 . As can be seen in Table 1 at times an optimum q may be a value that differs from a value representing the actual number of outliers in the dataset.

Exemplary embodiments of the invention may include a Technical Computing Environment TCE or may be performed by TCE. TCE may include hardware or a combination of hardware and software that provides a computing environment that allows users to perform tasks related to disciplines such as but not limited to mathematics science engineering medicine business etc. more efficiently than if the tasks were performed in another type of computing environment such as an environment that required the user to develop code in a conventional programming language such as C C Fortran Pascal etc. In one implementation TCE may include a dynamically typed programming language e.g. the M language a MATLAB language a MATLAB compatible language a MATLAB like language etc. that can be used to express problems and or solutions in mathematical notations.

For example TCE may use an array as a basic element where the array may not require dimensioning. These arrays may be used to support array based programming where an operation may apply to an entire set of values included in the arrays. Array based programming may allow array based operations to be treated as high level programming that may allow for example operations to be performed on entire aggregations of data without having to resort to explicit loops of individual non array operations. In addition TCE may be adapted to perform matrix and or vector formulations that can be used for data analysis data visualization application development simulation modeling algorithm development etc. These matrix and or vector formulations may be used in many areas such as statistics image processing signal processing control design life sciences modeling discrete event analysis and or design state based analysis and or design etc.

TCE may further provide mathematical functions and or graphical tools e.g. for creating plots surfaces images volumetric representations etc. . In one implementation TCE may provide these functions and or tools using toolboxes e.g. toolboxes for signal processing image processing data plotting parallel processing etc. . Alternatively or additionally TCE may provide these functions as block sets or in another way such as via a library etc.

TCE may be implemented as a text based environment e.g. MATLAB software Octave Python Comsol Script MATRIXx from National Instruments Mathematica from Wolfram Research Inc. Mathcad from Mathsoft Engineering Education Inc. Maple from Maplesoft Extend from Imagine That Inc. Scilab from The French Institution for Research in Computer Science and Control INRIA Virtuoso from Cadence Modelica or Dymola from Dynasim etc. a graphically based environment e.g. Simulink software Stateflow software SimEvents software Simscape software etc. by The MathWorks Inc. VisSim by Visual Solutions LabView by National Instruments Dymola by Dynasim SoftWIRE by Measurement Computing WiT by DALSA Coreco VEE Pro or SystemVue by Agilent Vision Program Manager from PPT Vision Khoros from Khoral Research Gedae by Gedae Inc. Scicos from INRIA Virtuoso from Cadence Rational Rose from IBM Rhopsody or Tau from Telelogic Ptolemy from the University of California at Berkeley aspects of a Unified Modeling Language UML or SysML environment etc. or another type of environment such as a hybrid environment that includes one or more of the above referenced text based environments and one or more of the above referenced graphically based environments.

TCE may include a programming language e.g. the MATLAB language that may be used to express problems and or solutions in mathematical notations. The programming language may be dynamically typed and or array based. In a dynamically typed array based computing language data may be contained in arrays and data types of the data may be determined e.g. assigned at program execution time.

For example suppose a program written in a dynamically typed array based computing language includes the following statements 

Now suppose the program is executed for example in a TCE such as TCE. During run time when the statement A hello is executed the data type of variable A may be a string data type. Later when the statement A int32 1 2 is executed the data type of variable A may be a 1 by 2 array containing elements whose data type are 32 bit integers. Later when the statement A 1.1 2.2 3.3 is executed since the language is dynamically typed the data type of variable A may be changed from the above 1 by 2 array to a 1 by 3 array containing elements whose data types are floating point. As can be seen by this example data in a program written in a dynamically typed array based computing language may be contained in an array. Moreover the data type of the data may be determined during execution of the program. Thus in a dynamically type array based computing language data may be represented by arrays and data types of data may be determined at run time.

TCE may provide mathematical routines and a high level programming language suitable for non professional programmers and may provide graphical tools that may be used for creating plots surfaces images volumetric representations or other representations. TCE may provide these routines and or tools using toolboxes e.g. toolboxes for signal processing image processing data plotting parallel processing etc. . TCE may also provide these routines in other ways such as for example via a library local or remote database e.g. a database operating in a computing cloud remote procedure calls RPCs and or an application programming interface API . TCE may be configured to improve runtime performance when performing computing operations. For example TCE may include a just in time JIT compiler.

Processor may include a processor multiple processors microprocessors or other types of processing logic that may interpret execute and or otherwise process information contained in for example a storage device and or memory . The information may include computer executable instructions and or data that may implement one or more embodiments of the invention. Processor may comprise a variety of hardware. The hardware may include for example some combination of one or more processors microprocessors field programmable gate arrays FPGAs application specific instruction set processors ASIPs application specific integrated circuits ASICs complex programmable logic devices CPLDs graphics processing units GPUs reduced instruction set computing RISC based processing devices e.g. ARM processors or other types of processing logic that may interpret execute manipulate and or otherwise process the information. Processor may comprise a single core or multiple cores. Moreover processor may comprise a system on chip SoC system in package SiP etc.

The foregoing description of the inventions and associated embodiments has been presented for purposes of illustration only. It is not exhaustive and does not limit the inventions to the precise form s disclosed. Those skilled in the art will appreciate from the foregoing description that modifications and variations are possible in light of the above teachings or may be acquired from practicing the invention. For example the steps described need not be performed in the same sequence discussed or with the same degree of separation. Likewise various steps may be omitted repeated or combined as necessary to achieve the same or similar objectives. Steps may alternatively be performed in parallel or in series. Similarly the systems described need not necessarily include all parts described in the embodiments and may also include other parts not describe in the embodiments.

Accordingly the inventions are not limited to the above described embodiments but instead are defined by the appended claims in light of their full scope of equivalents.

