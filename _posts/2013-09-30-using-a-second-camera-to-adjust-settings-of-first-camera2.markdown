---

title: Using a second camera to adjust settings of first camera
abstract: A device may operate a first image-capture system to capture first image data of a scene. While the first image-capture system is capturing the first image data, the device may operate a second image-capture system to determine an updated value for the first image setting, and send an instruction to the first image-capture system that indicates to use the updated value for the first image setting to continue to capture the first image data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09615012&OS=09615012&RS=09615012
owner: Google Inc.
number: 09615012
owner_city: Mountain View
owner_country: US
publication_date: 20130930
---
Generally imaging may refer to capturing and representing the color and brightness characteristics of a real world environment in a digital format and or a film format e.g. in photographs and or motion video . A large variety of image capture devices exist thus providing consumers with numerous ways to capturing image data.

As image capture devices such as cameras become more popular such devices may be employed as standalone hardware devices or integrated into various other types of devices. For instance still and video cameras are now regularly included in wireless communication devices e.g. mobile phones tablet computers laptop computers video game interfaces home automation devices and even automobiles and other types of vehicles.

Some digital camera systems do not include a separate sensor system for automated functions such as auto exposure and autofocus and thus provide such functions using the same sensor system that is used for image capture. Accordingly such automated functions can lead to e.g. portions of video where the exposure settings change rapidly and or where the video goes out of focus. Accordingly example embodiments may include two or more camera systems that are oriented in the same direction such that a second camera can provide such automated functions while a first camera is capturing image data. Such a multi camera configuration may help to prevent some of the undesirable effects that result when a device is simultaneously using the same image sensor for image capture and an automated image setting adjustment process.

In one aspect a method involves a operating by a computing device a first image capture system to capture first image data of a scene wherein the first image capture system initially uses a first value for a first image setting to capture the first image data and b while the first image capture system is capturing the first image data i operating by the computing device a second image capture system to determine an updated value for the first image setting wherein the first image capture system and the second image capture system are arranged on a given device and oriented in substantially the same direction and ii sending an instruction to the first image capture system that indicates to use the updated value for the first image setting to continue to capture the first image data.

In another aspect a system includes a control system and two or more image capture systems that include at least a first and second image capture systems wherein the first and second image capture systems are oriented in substantially the same direction. The control system is configured to a initially operate the first image capture system to capture first image data using a first value for a first image setting and b while the first image capture system is capturing the first image data i operate the second image capture system to determine an updated value for the first image setting and ii cause the first image capture system to use the updated image value for the first image setting to continue to capture the first image data.

In a further aspect a non transitory computer readable medium has stored therein instructions that are executable by a computing device to cause the computing device to perform functions comprising a operating a first image capture system to capture first image data of a scene wherein the first image capture system initially uses a first value for a first image setting to capture the first image data and b while the first image capture system is capturing the first image data i operating a second image capture system to determine an updated value for the first image setting wherein the first image capture system and the second image capture system are arranged on a given device and oriented in substantially the same direction and ii sending an instruction to the first image capture system that indicates to use the updated value for the first image setting to continue to capture the first image data.

In yet a further aspect a system may include a means for operating a first image capture system to capture first image data of a scene wherein the first image capture system initially uses a first value for a first image setting to capture the first image data and b means for while the first image capture system is capturing the first image data i operating a second image capture system to determine an updated value for the first image setting wherein the first image capture system and the second image capture system are arranged on a given device and oriented in substantially the same direction and ii sending an instruction to the first image capture system that indicates to use the updated value for the first image setting to continue to capture the first image data.

These as well as other aspects advantages and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description with reference where appropriate to the accompanying drawings. Further it should be understood that the description provided in this summary section and elsewhere in this document is intended to illustrate the claimed subject matter by way of example and not by way of limitation.

Some digital cameras such as high end DSLR cameras include separate specialized components that provide automated functionality such as auto focus auto exposure and automatic white balance. These components can adjust e.g. the focus and or exposure without noticeable effects to the image data e.g. video being captured by the camera s sensor. For example many SLR cameras utilize through the lens optical AF sensors for auto focus as well as a separate sensor array for light metering.

Other digital camera systems such as the less expensive cameras that are often used in mobile phones and tablets do not include separate components for such automated functions. Accordingly such camera systems may iteratively adjust settings and analyze resulting image data to perform auto focusing and other such automated functions. This type of automated function may generally be referred to as an experiment and adjust function. As an example in a mobile phone camera auto focus may involve the camera repeatedly adjusting the focal settings capturing image data and analyzing the captured image data until the desired subject is determined to be in focus. This particular type of iterative experiment and adjust function may be referred to as focus hunting. 

When a user moves a mobile phone camera while recording a video the distance s to the subject s in the camera s field of view may change and focus hunting may occur. While focus hunting may be performed fairly quickly the segments of the video that is recorded during focus hunting may be noticeably out of focus. Accordingly example embodiments may help to provide auto focus and or other automated adjustment functions that can be used in mobile phones and other such applications without the drawbacks of focus hunting.

In particular an example device may include two or more camera systems that are oriented in the same direction. Configured as such one or more of the camera systems may perform one or more automated experiment and adjust processes to determine one or more image capture settings for another camera system is being used to capture image data. This may prevent some of the undesirable effects that may result when a device is simultaneously using the same image sensor for image capture and an automated experiment and adjust function. Further using a separate camera system for an experiment and adjust process may improve the results of such process as the experiments are not constrained by the need to capture image data with the same image sensor.

For example consider the case of mobile phones that includes two camera systems which are becoming more common for purposes of stereoscopic imaging. If the mobile phone does not use both cameras for stereoscopic imaging then the mobile phone may record video with the first camera while simultaneously using the second camera for auto focusing. As such the focal settings of the first camera may be adjusted mid recording based on the results of the auto focus process being carried out with the second camera. Further since there is no concern as to the effect of focus hunting on the resulting video the second camera system may test a wider range of settings and or test a greater number of settings than otherwise might be tested. These expanded testing capabilities may improve the results that are achieved by the auto focus process.

Herein a camera system may take the form of a camera a system within a camera a separate system that is communicatively coupled to a camera or a combination of a camera and one or more other systems. Further for sake of simplicity examples described herein may attribute certain functions and or characteristics to a camera or camera device. It should be understood that in many cases functions and or characteristics that are attributed to a camera or camera device may likewise be attributed to a camera system even when this is not stated explicitly.

The physical components of an image capture device may include an aperture through which light enters a recording surface for capturing the image represented by the light and a lens positioned in front of the aperture to focus at least part of the image on the recording surface. The aperture may be fixed size or adjustable. In an analog camera the recording surface may be photographic film. In a digital camera the recording surface may include an electronic image sensor e.g. a charge coupled device CCD or a complementary metal oxide semiconductor CMOS sensor to transfer and or store captured images in a data storage unit e.g. memory .

A shutter may be coupled to or nearby the lens or the recording surface. The shutter may either be in a closed position in which it blocks light from reaching the recording surface or an open position in which light is allowed to reach to recording surface. The position of the shutter may be controlled by a shutter button. For instance the shutter may be in the closed position by default. When the shutter button is triggered e.g. pressed the shutter may change from the closed position to the open position for a period of time known as the shutter cycle. During the shutter cycle an image may be captured on the recording surface. At the end of the shutter cycle the shutter may change back to the closed position.

Alternatively the shuttering process may be electronic. For example before an electronic shutter of a CCD image sensor is opened the sensor may be reset to remove any residual signal in its photodiodes. While the electronic shutter remains open the photodiodes may accumulate charge. When or after the shutter closes these charges may be transferred to longer term data storage. Combinations of mechanical and electronic shuttering may also be possible.

Regardless of type a shutter may be activated and or controlled by something other than a shutter button. For instance the shutter may be activated by a softkey a timer or some other trigger. Herein the term image capture may refer to any mechanical and or electronic shuttering process that results in one or more photographs being recorded regardless of how the shuttering process is triggered or controlled.

As noted previously digital cameras may be standalone devices or integrated with other devices. As an example illustrates the form factor of a digital camera device . Digital camera device may be for example a mobile phone a tablet computer or a wearable computing device. However other embodiments are possible. Digital camera device may include various elements such as a body a front facing camera a multi element display a shutter button and other buttons . Digital camera device could further include two rear facing cameras A and B. Front facing camera may be positioned on a side of body typically facing a user while in operation or on the same side as multi element display . Rear facing cameras A and B may be positioned on a side of body opposite front facing camera . Referring to the cameras as front and rear facing is arbitrary and digital camera device may include multiple cameras positioned on various sides of body .

The lenses of rear facing cameras A and B are arranged on the upper corner on the back of digital camera device and are oriented in substantially the same direction. It should be understood however that other multi camera arrangements are possible. In particular the lenses of two or more cameras which are all oriented in substantially the same direction may be arranged in different formations on a surface of the phone. For instance several other multi camera arrangements are described herein with respect to .

In particular shows a mobile device with an arrangement of four cameras A to D oriented in the same direction including two cameras A and B in the upper corner of the mobile device similar to and two additional cameras C and D that are located at the lower corners of the mobile device . Further shows another arrangement with four cameras oriented in the same direction. In particular the arrangement in includes one camera A to D in each corner of the device .

Yet further shows an arrangement with six cameras A to F facing in the same direction. In the six cameras A to F are placed on the back of the mobile device in less structured organic arrangement. Note that an arrangement with three or more cameras may provide multiple baselines between different pairs of cameras. For instance a six camera arrangement such as that shown in may provide up to 15 different baselines for e.g. stereoscopic imaging. More generally an arrangement of n cameras that are oriented in substantially the same direction may provide up to C n k baselines.

Further it is contemplated that a multi camera arrangement may include more than two cameras. In addition the lenses in a multi camera arrangement may be oriented at a different angle with respect to the surface on which the lenses are arranged. Yet further it should be understood that multi camera arrangements may be implemented on other sides of a digital camera device. Other variations on the multi camera arrangements shown in the figures are also possible.

Multi element display could represent a cathode ray tube CRT display a light emitting diode LED display a liquid crystal LCD display a plasma display or any other type of display known in the art. In some embodiments multi element display may display a digital representation of the current image being captured by front facing camera and or one or both of rear facing cameras A and B or an image that could be captured or was recently captured by any one of or any combination of these cameras. Thus multi element display may serve as a viewfinder for either camera. Multi element display may also support touchscreen and or presence sensitive functions that may be able to adjust the settings and or configuration of any aspect of digital camera device .

Front facing camera may include an image sensor and associated optical elements such as lenses. Front facing camera may offer zoom capabilities or could have a fixed focal length. In other embodiments interchangeable lenses could be used with front facing camera . Front facing camera may have a variable mechanical aperture and a mechanical and or electronic shutter. Front facing camera also could be configured to capture still images video images or both. Further front facing camera could represent a monoscopic stereoscopic or multiscopic camera. Rear facing cameras A and B may be similarly or differently arranged. Additionally front facing camera rear facing cameras A and B or any combination of these cameras may in fact be an array of one or more cameras or an array of lenses that direct light onto a common image sensor .

Any one or any combination of front facing camera and rear facing cameras A and B may include or be associated with an illumination component that provides a light field to illuminate a target object. For instance an illumination component could provide flash or constant illumination of the target object. An illumination component could also be configured to provide a light field that includes one or more of structured light polarized light and light with specific spectral content. Other types of light fields known and used to recover three dimensional 3D models from an object are possible within the context of the embodiments herein.

Any one or any combination of front facing camera and rear facing cameras A and B may include or be associated with an ambient light sensor that may continuously or from time to time determine the ambient brightness of a scene that the camera can capture. In some devices the ambient light sensor can be used to adjust the display brightness of a screen associated with the camera e.g. a viewfinder . When the determined ambient brightness is high the brightness level of the screen may be increased to make the screen easier to view. When the determined ambient brightness is low the brightness level of the screen may be decreased also to make the screen easier to view as well as to potentially save power. Additionally the ambient light sensor s input may be used to determine an exposure setting of an associated camera or to help in this determination.

Digital camera device could be configured to use multi element display and either front facing camera or one or both of rear facing cameras A and B to capture images of a target object. The captured images could be a plurality of still images or a video stream. The image capture could be triggered by activating shutter button pressing a softkey on multi element display or by some other mechanism. Depending upon the implementation the images could be captured automatically at a specific time interval for example upon pressing shutter button upon appropriate lighting conditions of the target object upon moving digital camera device a predetermined distance or according to a predetermined capture schedule.

As noted above the functions of digital camera device or another type of digital camera may be integrated into or take the form of a computing device such as a mobile phone tablet computer laptop computer and so on. For purposes of example is a simplified block diagram showing some of the components of an example computing device that may include camera components . Camera components may include multiple cameras such as cameras A and B.

By way of example and without limitation computing device may be a cellular mobile telephone e.g. a smartphone a still camera a video camera a fax machine a computer such as a desktop notebook tablet or handheld computer a personal digital assistant PDA a home automation component a digital video recorder DVR a digital television a remote control a wearable computing device or some other type of device equipped with at least some image capture and or image processing capabilities. It should be understood that computing device may represent a physical camera device such as a digital camera a particular physical hardware platform on which a camera application operates in software or other combinations of hardware and software that are configured to carry out camera functions.

As shown in computing device may include a communication interface a user interface a processor data storage and camera components all of which may be communicatively linked together by a system bus network or other connection mechanism .

Communication interface may function to allow computing device to communicate using analog or digital modulation with other devices access networks and or transport networks. Thus communication interface may facilitate circuit switched and or packet switched communication such as plain old telephone service POTS communication and or Internet protocol IP or other packetized communication. For instance communication interface may include a chipset and antenna arranged for wireless communication with a radio access network or an access point. Also communication interface may take the form of or include a wireline interface such as an Ethernet Universal Serial Bus USB or High Definition Multimedia Interface HDMI port. Communication interface may also take the form of or include a wireless interface such as a Wifi BLUETOOTH global positioning system GPS or wide area wireless interface e.g. WiMAX or 3GPP Long Term Evolution LTE . However other forms of physical layer interfaces and other types of standard or proprietary communication protocols may be used over communication interface . Furthermore communication interface may comprise multiple physical communication interfaces e.g. a Wifi interface a BLUETOOTH interface and a wide area wireless interface .

User interface may function to allow computing device to interact with a human or non human user such as to receive input from a user and to provide output to the user. Thus user interface may include input components such as a keypad keyboard touch sensitive or presence sensitive panel computer mouse trackball joystick microphone and so on. User interface may also include one or more output components such as a display screen which for example may be combined with a presence sensitive panel. The display screen may be based on CRT LCD and or LED technologies or other technologies now known or later developed. User interface may also be configured to generate audible output s via a speaker speaker jack audio output port audio output device earphones and or other similar devices.

In some embodiments user interface may include a display that serves as a viewfinder for still camera and or video camera functions supported by computing device . Additionally user interface may include one or more buttons switches knobs and or dials that facilitate the configuration and focusing of a camera function and the capturing of images e.g. capturing a picture . It may be possible that some or all of these buttons switches knobs and or dials are implemented as functions on a presence sensitive panel.

Processor may comprise one or more general purpose processors e.g. microprocessors and or one or more special purpose processors e.g. digital signal processors DSPs graphics processing units GPUs floating point units FPUs network processors or application specific integrated circuits ASICs . In some instances special purpose processors may be capable of image processing image alignment and merging images among other possibilities. Data storage may include one or more volatile and or non volatile storage components such as magnetic optical flash or organic storage and may be integrated in whole or in part with processor . Data storage may include removable and or non removable components.

Processor may be capable of executing program instructions e.g. compiled or non compiled program logic and or machine code stored in data storage to carry out the various functions described herein. Therefore data storage may include a non transitory computer readable medium having stored thereon program instructions that upon execution by computing device cause computing device to carry out any of the methods processes or functions disclosed in this specification and or the accompanying drawings. The execution of program instructions by processor may result in processor using data .

By way of example program instructions may include an operating system e.g. an operating system kernel device driver s and or other modules and one or more application programs e.g. camera functions address book email web browsing social networking and or gaming applications installed on computing device . Similarly data may include operating system data and application data . Operating system data may be accessible primarily to operating system and application data may be accessible primarily to one or more of application programs . Application data may be arranged in a file system that is visible to or hidden from a user of computing device .

Application programs may communicate with operating system through one or more application programming interfaces APIs . These APIs may facilitate for instance application programs reading and or writing application data transmitting or receiving information via communication interface receiving and or displaying information on user interface and so on.

In some vernaculars application programs may be referred to as apps for short. Additionally application programs may be downloadable to computing device through one or more online application stores or application markets. However application programs can also be installed on computing device in other ways such as via a web browser or through a physical interface e.g. a USB port on computing device .

Camera components may include but are not limited to an aperture shutter recording surface e.g. photographic film and or an image sensor lens and or shutter button. Camera components may be controlled at least in part by software executed by processor . Further camera components may include multiple camera systems which each include an aperture shutter recording surface lens image sensor processor and or shutter button. When multiple camera systems are included there may be some components that are shared between the systems and other components that are not shared. For example each camera could include its own aperture lens and image sensor while sharing other components such as a processor and a shutter button. As another example each camera could include its own lens but share the same image sensor. Alternatively each camera system s components may be utilized only for that camera system and not shared with other camera systems.

A still camera may capture one or more images each time image capture is triggered. A video camera may continuously capture images at a particular rate e.g. 24 images or frames per second as long as image capture remains triggered e.g. while the shutter button is held down . Some digital still cameras may open the shutter when the camera device or application is activated and the shutter may remain in this position until the camera device or application is deactivated. While the shutter is open the camera device or application may capture and display a representation of a scene on a viewfinder. When image capture is triggered one or more distinct digital images of the current scene may be captured. Note that example embodiments may utilize cameras with electronic shutters and or mechanical shutters.

Captured digital images may be represented as a one dimensional two dimensional or multi dimensional array of pixels. Each pixel may be represented by one or more values that may encode the respective pixel s color and or brightness. For example one possible encoding uses the YCbCr color model which may also be referred to as the YUV color model . In this color model the Y color channel may represent the brightness of a pixel and the Cb U and Cr V color channels may represent the blue chrominance and red chrominance respectively of the pixel. For instance each of these color channels may take values from 0 to 255 i.e. the tonal range that a single 8 bit byte can offer . Thus the brightness of a pixel may be represented by a 0 or a value near zero if the pixel is black or close to black and by a 255 or a value near 255 if the pixel is white or close to white. However the value of 255 is a non limiting reference point and some implementations may use different maximum values e.g. 1023 4095 etc. .

Nonetheless the YCbCr color model is just one possible color model and other color models such as a red green blue RGB color model or a cyan magenta yellow key CMYK may be employed with the embodiments herein. Further the pixels in an image may be represented in various file formats including raw uncompressed formats or compressed formats such as Joint Photographic Experts Group JPEG Portable Network Graphics PNG Graphics Interchange Format GIF and so on.

Some pixel encodings including the YCbCr color model use 8 bits to represent the brightness of each pixel. Doing so is referred to as LDR imaging. As a result only 256 levels of brightness may be supported. However real world scenes often exhibit a wider dynamic range of brightness than can be reasonably represented by LDR imaging. For example a scene of an individual standing in a dark room in front of a window may include both extremely bright regions and extremely dark regions. However use of LDR imaging to capture an image of such a scene may result in loss of detail in the bright region and or the dark region based on the exposure with which the image was captured.

The exposure of a captured image may be determined by a combination of the size of the aperture the brightness of the light entering the aperture and or the length of the shutter cycle also referred to as the shutter length or the exposure length . Additionally a digital and or analog gain may be applied to the image thereby influencing the exposure. In some embodiments the term total exposure length or total exposure time may refer to the shutter length multiplied by the gain for a particular aperture size. Herein the term total exposure time or TET should be interpreted as possibly being a shutter length an exposure time or any other metric that controls the amount of signal response that results from light reaching the recording surface. In some embodiments true exposure time may refer to the length of time of an exposure before any gain is applied.

Cameras even analog cameras may include software to control one or more camera functions and or settings such as aperture size TET gain and so on. Additionally some cameras may include software that can digitally process an image while or after capturing the image. While it should be understood that the description above refers to cameras in general it may be particularly relevant to digital cameras.

A short TET may result in a reasonably accurate representation of the bright regions of a scene but in some cases may underexpose the dark regions. Conversely a long TET may result in a reasonably accurate representation of the dark regions but in some cases may overexpose the bright regions. In the example scene introduced above if the TET is too long the features in the room may appear properly exposed but the features outside the window may appear whitewashed. But if the TET is too short the features outside the window may appear normal but the features in the room may appear darkened. Either of these outcomes is undesirable. For some scenes there may not be a single TET that results in a captured image representing the details in both bright regions and dark regions with acceptable detail.

Camera devices may support an auto exposure AE mode in which prior to output image capture the camera determines the TET based on the brightness of the scene. For example the user may observe the scene in the camera s viewfinder before triggering image capture. During this period the camera may make an initial estimate of the proper TET capture a preview image with that TET and then evaluate the pixels in the captured image. Then as one possible implementation if a majority or some other sufficient fraction of the pixels in the preview image are over exposed the camera may decrease the TET and capture another preview image. If a majority or some other sufficient fraction of the pixels in this preview image are under exposed the camera may increase the TET and capture yet another preview image.

For instance if the majority of the pixels in the captured image exhibit a brightness value above a high threshold level e.g. 240 the camera may decrease the TET. On the other hand if a majority of the pixels exhibit a brightness level below a low threshold level e.g. 96 the camera may increase the TET.

Alternatively or additionally a target average pixel value for some or all of the scene s pixels may be determined. If the actual average pixel value is above the target average pixel value the TET may be decreased and if the actual average pixel value is below the target average pixel value the TET may be increased. The target average pixel value can also be tuned differently depending on how much contrast there is in the scene. For example in a low contrast scene the target average pixel value may be bright e.g. 200 . But in a high contrast scene the target average pixel value may be lower e.g. 128 .

This process may continue until the camera determines that an image should be captured and stored e.g. the user activates the shutter button . During this process if the characteristics of the scene are relatively unchanging the camera usually converges on an estimated best TET based on the brightness of the scene. In some embodiments the image displayed on the camera s viewfinder may omit information from one or more of the captured preview images or combine information from two or more of the captured preview images.

In some cases the camera might not treat all pixels equally when determining an average brightness of the scene. Using a technique described as center weighted averaging pixels near the middle of the scene may be considered to be more important. Thus these pixels may be weighted more than pixels illustrating other areas of the scene. Alternatively pixels in other locations of an image may be given more weight. For instance if the camera detects a human face or some other object of interest in a particular location other than the center of the image the camera may give a higher weight to the associated pixels.

In this way AE algorithms may seek to determine a TET that produces a large number e.g. the largest number of properly exposed pixels. However given the range limitations of LDR imaging even images captured in AE mode may contain portions that are whitewashed or darkened. Thus as noted above some scenes there may be no single best TET.

Herein a pixel may be considered to be properly exposed if its brightness value is within a predefined range. For 8 bit brightness values this range might be for instance 32 224 16 240 96 240 128 240 and so on. A pixel is improperly exposed if its brightness value falls outside of this range i.e. the pixel is either under exposed or over exposed . However brightness values may be encoded using more or fewer bits and the pre defined range may be different than the example ranges given above.

AE algorithms may differ from the description above. For instance some may be more complex treating different colors differently considering the spatial and or structural components of a scene and or measuring contrast between regions. It is possible however that embodiments described herein may utilize or operate in conjunction with any AE algorithm now known or developed in the future.

High dynamic range HDR imaging has been proposed as a way of compensating for the deficiencies of LDR imaging. In a possible implementation HDR imaging may involve a camera capturing multiple images of a scene at various TETs and then digitally processing these captured images to make a single image that contains a reasonable representation of the details in most or all regions of the scene including those that are very bright and very dark. However determining TETs for capturing images can be problematic. In particular difficulty in adjusting TETs for a particular scene has created limitations in HDR imaging. The methods and implementations described herein may provide computational efficiency robustness to artifacts and or enhanced image quality.

In the following the term LDR image may refer to an image captured using LDR imaging and the term LDR scene may refer to a scene that has been determined to be reasonably represented using LDR imaging. Similarly the term HDR image may refer to an image captured using HDR imaging and the term HDR scene may refer to a scene that has been determined to be reasonably represented using HDR imaging. Furthermore the term LDR imaging may be used interchangeably with the term LDR image acquisition and the term HDR imaging may be used interchangeably with the term HDR image acquisition. 

Flow chart may represent a series of steps performed by digital camera device when a shutter button is triggered. Alternatively or additionally flow chart may represent steps that are continuously performed when a viewfinder of digital camera device is displaying a representation of a scene. Thus in some embodiments the features of flow chart may be performed in a way that is not apparent to the user. For instance the user may trigger the shutter once with the intention of capturing a single image. However digital camera device may capture multiple images in each of first image capture and second image capture and provide an output image that is a combination of one or more images captured during second image capture .

It should be noted that not all steps depicted in need be performed by digital camera device . Some steps such as image processing and combining images for example could be performed by a different device. For instance representations of one or more images captured during first image captures and second image capture could be transmitted from a capturing device to a remote computing device. The remote computing device could then perform image processing and combining images possibly transmitting some or all of the results thereof to the capturing device.

Additionally training image database may be included in digital camera device or alternatively training image database may be part of a separate device or system that may be accessed by digital camera device . In some embodiments training image database may include representations of training images that can be used to help determine the structure of a payload burst used in second image capture .

In first image capture a first group of images of a scene may be captured using a metering burst sweep. In a metering burst sweep each image in the group may be captured with a different TET. In some instances the metering burst sweep may capture consecutive images across a range of TETs e.g. 1 300 milliseconds 0.1 500 milliseconds or some other range . Using such ranges of TETs the metering burst sweep may capture a series of images with TETs designed to cover this range according to a linear logarithmic and or exponential distribution of TETs among other possibilities.

In some implementations a second camera of a device may perform metering burst sweeps to determine a TET that should be used for each frame of video that is being recorded by a first camera. Alternatively the second camera may perform a metering burst sweep periodically during a video recording by a first camera in order to periodically adjust the TET of the first camera. For example a metering burst sweep could be performed by the second camera once for every ten frames of a video recording by a first camera. Other examples are also possible.

As another example depicts first image capture including three digital images of a scene each captured with a different TET. The three images exhibit diverse levels of brightness due to the different TETs used to capture the images. In other examples more or fewer images may be captured during first image capture . These captured images may provide parameters for digital camera device to use when capturing subsequent images of the scene.

The metering burst sweep can be used to determine the characteristics of the scene so that a subsequent payload burst structure for second image capture can be selected. Therefore in step the images captured at step may be processed. Particularly step may include merging one or more of the images captured at step in a combined image . Step may also include forming a histogram from the merged images and then using the histogram and possibly some or all of the information in training image data to classify the scene e.g. as an LDR scene or an HDR scene determine the structure of the payload burst based on the classification of the scene and determine the TETs to use when capturing images according to the payload burst. In some embodiments the captured images shown as a result of first image capture may be downsampled prior to merging. Further the histogram may be an LDR histogram HDR histogram an log HDR histogram or some other form of histogram.

An example of this process is illustrated in . For sake of simplicity assume that the metering burst sweep consist of two images image and image . In practice anywhere from one to eight or more images may be captured in the metering burst sweep. Image was captured with a TET of 10 milliseconds while image was captured with a TET of 20 milliseconds. Thus the pixels of image might be expected to be about twice as bright as those of image . In some cases the pixels in each of images and may be tonemapped. Therefore the tonemapping process may be reverse for these images. After the tonemapping process is reversed the pixels of image may be about twice as bright as those of image . Tonemapping and reverse tonemapping are discussed in more detail below. Further note that when an image burst sweep is performed by a second camera to determine an exposure setting or settings for a first camera that is recording video the range of TETs used in the image burst sweep may vary by a greater amount than might otherwise be possible if the same camera is performing the sweep and capturing image data.

Images and may be downsampled to form images and respectively. Downsampling can be implemented by for example dividing a given image into i j pixel blocks i and j may take on the same value or different values and replacing each of these blocks by a single pixel. The value of this replacement pixel can be based on the values of the pixels in the respective i j pixel block. For instance the value of the replacement pixel may be determined by taking an average of the values of the pixels in the block resulting in a fuzzier lower resolution and smaller downsampled image. Thus as one possible example if a 1600 1200 pixel image is divided into 2 2 pixel blocks and downsampled one level the result is an 800 600 pixel image. If the 1600 1200 pixel image is downsampled two levels or if the 800 600 pixel image is downsampled one more level the result is a 400 300 pixel image and so on. Nonetheless a tile can be downsampled in other ways. For example a 4 4 an 8 8 pixel block or a 16 16 pixel block can be replaced by a single pixel and more than just one or two levels of downsampling can be performed.

In some embodiments multiple levels of downsampling may be performed for each image thus creating a pyramid of downsampled images. By using images with multiple levels of downsampling information regarding both the distribution of light levels in the image and the spatial structure of these light levels may be preserved.

Thus for instance depicts image having pixel block A in its top left corner and pixel block B in its top right corner. Additionally image has pixel block A in its top left corner and pixel block B in its top right corner. Each pixel block in each image is downsampled to an individual respective pixel in images and pixel A represents a downsampling of pixel block A pixel B represents a downsampling of pixel block B pixel A represents a downsampling of pixel block A and pixel B represents a downsampling of pixel block B.

In some cases an i j pixel block and its associated downsampled pixel may both be referred to as a paxel. Thus pixel block A and pixel A may both be referred to as a paxel.

For each location in the downsampled images e.g. pixels A and A would be considered to be in the same location the pixel with the highest value less than 255 may be selected. In some embodiments the pixel value of each color channel in the pixel may be compared to 255. If all of these pixel values are below 255 then the pixel is a candidate for selection. Out of all of the candidate pixels the one with the highest pixel value may be selected. In other embodiments a threshold different from 255 e.g. 250 245 or a value higher than 255 may be used instead.

Each selected pixel may be placed in its respective location in combined image . Thus for instance pixel A may be placed as pixel A and pixel B may be placed as pixel B in combined image . Additionally the pixels selected for combined image may be reverse tonemapped.

Tonemapping is a set of procedures that include mapping the pixel values according to a pre determined function. Thus for instance some camera devices map pixel values from a linear space wherein an increase or decrease of k units in pixel value represents a proportional increase or decrease in brightness to a non linear space. Tonemapping may be automatically performed for artistic purposes such as brightening mid range pixel values. Regardless in order to reverse tonemap the selected pixels back to linear space the inverse of the tonemapping function may be applied to the respective pixel values.

Additionally the pixel values may be divided by the respective TET with which the pixels were captured. Doing so may normalize the pixel values that were captured using the various TETs to a particular range. Thus for pixel A the R G B values may be 25 10 15 while for pixel B the R G B values may be 10 5 5. On the other hand for TETs under 1 millisecond e.g. TETs of 0.25 milliseconds 0.5 milliseconds and so on dividing by the TET may increase the pixel values in combined image . In some cases this may result in these pixel values being greater than 255 and therefore combined image may be an HDR representation of the scene.

Further a histogram may be created from combined image . While there are many ways in which the histogram can be formed some embodiments may include evaluating a function of the color channels of each pixel. This function may be a maximum or some other function for instance.

Moreover the logarithm of this function s output may also be taken and the resulting value plotted on histogram . The distribution of light levels in the real world is logarithmic. Thus by using a log scale there is a roughly uniform coverage of that range. For histograms based on linear light levels more histogram bins might be used. Further in some embodiments a weight may be applied to the logarithm of the function s output before placing this output in histogram . For instance a center weighted average technique may be used to apply a higher weight for pixels that are closer to the center of the captured image and a lower weight might for pixels that are further from the center of the image.

Regardless histogram may represent the distribution of pixel values in combined image . The vertical axis of the histogram may indicate the number of pixels of each pixel value and the horizontal axis may represent the range of pixel values. The pixel values may be within the range 0 255 or some other range may be used. For instance an HDR histogram may include pixel values above 255. In some embodiments an HDR histogram may represent 15 bit pixel values i.e. from 0 to 32 767. Thus the log values appearing in the HDR histogram may be in the range of 0 to log 32 767 4.52.

As an example histogram plots most of the pixels on the extreme ends of its horizontal axis. This indicated that most of histogram plots image is a shade of black or white. However since histogram also plots data points in the middle of the horizontal axis combined image may contain pixels with mid range brightness as well.

Still as part of image processing histogram may be compared to one or more histograms of images in training image database . As depicted in training image database may contain a set of histograms of reference scenes and or associated parameters. The histograms of the reference scenes may be HDR histograms as described above.

In some embodiments the percentage of pixels clipped e.g. pixels having a pixel value of 255 or higher in the downsampled combined single image may be stored in target percentage of clipped pixels . Clipping may occur when the TET used to capture the image was either too high or too low and some very dark or very bright details of the scene were lost in the image capture process. In some cases only pixels clipped at one end of the range may be included in the percentage of pixels clipped. For instance the percentage of pixels clipped may include only pixels with a pixel value of 255 or higher and the average pixel value of unclipped pixels may be calculated over all other pixels e.g. all pixels with a pixel value of 0 254 .

Further an indication of whether the scene was LDR or HDR may be stored in scene type . In some implementations scene type may take on a value of zero when the scene type is LDR and a value of one when the scene type is HDR. Alternatively scene type may be within a range e.g. from 0 to 1. In this case a value less than a threshold e.g. 0.5 might indicate an LDR scene type and any number greater than or equal to the threshold value might indicate an HDR scene type.

Target pixel value s may be one or more pixel values that have been determined to be desirable for the associated scene. If the scene is LDR target pixel value s may include a single pixel value or a range of pixel value s . This single pixel value or range may have been selected so that images of the scene with an average pixel value matching the target pixel value would be reasonably well exposed. Thus the target pixel value s may be a target average pixel value. Additionally the target pixel value s may be LDR values e.g. from 0 to 255 .

Further in an embodiment where a second camera performing a method such as method A to adjust the TET for a video being recorded by a first camera target pixel value s may include a single pixel value for use by the first camera. In particular a second camera may repeat a method such as that described herein to determine a single pixel value for each frame of video that is being captured by a first camera. Alternatively the second camera could be used to periodically determine a single pixel value for use by a first camera that is capturing video e.g. once every four frames .

If the scene is HDR target pixel value s may include a short exposure target pixel value or range a long exposure target pixel value or range and possibly a fallback exposure target pixel value or range . These pixel values or ranges may have been selected so that HDR images with the short exposure target pixel value for the short exposures and long exposure target pixel value for the long exposures may be reasonably well exposed. The fallback target pixel value may be used if HDR imaging fails e.g. as discussed below and a single TET is used to capture the HDR scene.

In some cases target pixel value s target percentage of clipped pixels and scene type may be determined manually by examining several synthetic exposures of the captured images and selecting the pixel value s and or range s that appear most pleasing to the eye. Alternatively target pixel value s target percentage of clipped pixels and scene type may be determined algorithmically or automatically.

Further in an embodiment where a second camera performing a method such as method A to adjust the TET for a video being recorded by a first camera target pixel value s may include a short exposure target pixel value or range for use by the first camera. In particular a second camera may repeat such a method using neighboring frames of video in order to determine the short exposure target pixel value or range for each frame of video that is being captured by a first camera. This may help to provide noise reduction and HDR in a video. Alternatively the second camera could be used to simply determine a single exposure TET e.g. a single pixel value for each frame of video being captured by a first camera.

Training image database may also contain similar entries for histogram target pixel value s target percentage of clipped pixels and scene type as well as histogram target pixel value s target percentage of clipped pixels and scene type . In some embodiments one or more parameters such as the target percentage of clipped pixels may be omitted from training image database . Additionally other parameters may be included in training image database . Training image database may store as few as one or several tens hundreds or thousands of such entries each entry possibly relating to a different scene.

As discussed above in the context of an image captured during first image capture multiple levels of downsampling may be performed for each image in training image database thus creating a pyramid of downsampled versions of each image. By using images with multiple levels of downsampling information regarding both the distribution of light levels in the image and the spatial structure of these light levels may be preserved. Histograms for each of these downsampled images may be included in training image data .

The information in training image database may be stored therein during calibration testing and or other pre shipment evaluations before commercial operation among other possibilities. Alternatively the information may also be stored on various other devices and systems capable of managing training image database . Regardless the information may be substantially static in nature though the information may be modified through firmware and or software updates or other installations.

By comparing histogram and or a similarly derived histograms based on downsampled versions of images and with the histograms in training image data the dynamic range of the scene represented in images and may be estimated. This estimated dynamic range may be used to determine obtain or otherwise select a payload burst structure for second image capture .

For example each histogram may be compared to each histogram stored in training image data . A pair of histograms can be compared in various ways any of which may be used. In some embodiments the earth mover s distance EMD between the pair of histograms may be calculated. The EMD is 0 when the histograms are identical and increases with the differences between the histograms. Thus a lower EMD indicates a good match between the two histograms and a higher EMD indicates a poor match between the two histograms.

A weight may be derived from the EMD for a particular pair of histograms. For example the weight may be inversely proportional to its associated EMD value. In some situations the weight w may be derived as 

For each pair of histograms where a first histogram is histogram and a second histogram is from an entry in training image database the weight w may be applied to the respective scene type associated with the second histogram s entry in training image database . The result may be averaged over the pairs of histograms to determine an HDR ness of the scene. For instance if the resulting HDR ness is at or above 0.5 on a scale of 0 to 1 then the scene may be designated for HDR processing but if the resulting HDR ness is below 0.5 on the same scale then the scene may be designated for LDR processing.

It should be understood that the embodiment described above is just one possible way of determining the brightness and HDR ness of a scene. Other techniques could be used instead and some of these other techniques may be based on comparing the parameters e.g. percentage of clipped pixels and average value of unclipped pixels of the images captured during first image capture with the same or similar parameters of the images represented in training image database . Further techniques may include comparing the respective downsampled image pyramids of the first and second histograms.

In order to determine the TETs for the payload burst the following example procedure may be used. However other procedures may be used instead.

For each pair of histograms again where the first histogram is histogram and a second histogram is from an entry in training image database the weight w may be applied to the respective target pixel value associated with the second histogram s entry in training image database . The result may be averaged to determine a target average pixel value for the scene represented by histogram . If the scene type is HDR two or more target average pixel values may be determined.

For each target average pixel value interval halving may be used to search for a particular TET value such that if the scene were to be captured with the particular TET value the resulting image would have the target average pixel value or about the target average pixel value. One possible method for determining a particular TET value based on a target average pixel value is shown in the pseudocode below.

At lines 1 and 2 of Table 1 initial low and high TET values lo tet and hi tet respectively are defined. These values may be chosen at or near the extreme ends of the range in which the ultimate TET value mid tet is expected to fall. In some embodiments a broader range or a narrower range may be used.

Lines 3 11 depict a loop that may be iterated t times. The value of t may be selected so that mid tet converges after t iterations of the loop. In some embodiments t may be as low as 2 or 3 but in other embodiments t may be 5 10 20 50 100 or some other value. Nonetheless at line 4 mid tet is set to be the average midpoint of lo tet and hi tet.

At line 5 the pixel value of the image at mid tet is determined. One possible way of making this determination is to synthetically expose the HDR image as if the image were captured using a TET of mid tet. The resulting image which may be an HDR image may be tonemapped and the average pixel value of the tonemapped image pixel value at mid tet which may be an LDR value may be determined.

Synthetic exposure is one way of obtaining an LDR image from an HDR image. Suppose that an HDR image was captured using a TET of T. This HDR image can be synthetically exposed to a TET of p times T by multiplying the pixel values of each pixel in the HDR image by p p may be greater than or less than one . In the resulting image all pixel values above 255 are clipped to 255. This process simulates the appearance of the scene as if it were captured using a TET of p times T with LDR imaging. Alternatively a non log HDR histogram of an HDR image with or without center weighted averaging applied may be used. After this step whatever processing would normally be applied to the linear image such as tonemapping may be emulated in order to produce a synthetic LDR image. The average value in that image applying center weighted averaging if desired may be taken and compared to the target pixel value.

At lines 6 11 if this resulting average pixel value is greater than the target average pixel value then mid tet is too high and hi tet is set to be mid tet in order to reduce mid tet in the next iteration. On the other hand if the resulting average pixel value is less than or equal to the target average pixel value then the mid tet is too low and lo tet is set to be mid tet in order to increase mid tet in the next iteration.

The process illustrated by Table 1 may be repeated for each TET value that may be used in the payload burst structure. Thus if the scene is determined to be an LDR scene the process of Table 1 may be carried out for one TET. However if the scene is determined to be an HDR scene the process of Table 1 may be carried out for two or more TETs e.g. the short TET the long TET and or the fallback TET all three of which may have different target average LDR pixel values.

In step the second group of images may be captured. The number of images captured and the arrangement of TETs used to capture these images may be referred to as a payload burst. For example in second image capture includes three images of a scene each captured with a TET identified in step . It should be understood that the TETs identified in step may be the same or different than the TETs used to capture images in step . Additionally it is possible that all three images in second image capture are captured with the same or similar TETs.

In step images from the second group of images may be combined. Combining images may include aligning two or more of the images. In some instances images may be aligned globally i.e. aligning whole images as opposed to portions of images locally i.e. aligning portions of images or possibly both globally and locally. Further combining two or more images may also include merging them to form an output image . This merging may be carried out in accordance with any image fusion technique now known or developed in the future.

Merging the images in the second group of images may result in output image being sharper and or better exposed than any of the individual images in the second group. For instance if some of the images in second image capture are captured with the same or similar TETs these images may be merged to reduce noise in one or more sections of the images. Alternatively or additionally if the images in second image capture are captured with two or more different TETs at least some images with different exposure times may be merged according to HDR procedures. Regardless the output image may be stored on a computer readable medium and or displayed on an output medium such as the multi element display of .

In some embodiments the arrangements of various possible payload burst structures may be determined based on the TETs determined in step as well as an understanding of combining images step . While numerous arrangements of payload burst structures may be possible three examples are described herein.

Table 2 illustrates these examples. In the first example the scene type is LDR. In this example the payload burst structure includes four images captured sequentially and may be referred to as an LDR burst structure. Each T in the Payload Burst Structure column of Table 2 may represent a captured image. Each of these images may be captured using the same or a similar TET that was determined in step . In some embodiments fewer or more images may be captured in an LDR payload burst. For example as few as one or as many as ten or more images may be included.

Regardless of the number of images captured some of these images may be aligned and combined in step . For instance if m images are captured in the payload burst the sharpest one of these images may be selected as a primary image and the remaining m 1 images may be considered secondary images. In some implementations the sharpness of an image may be measured by the image s resolution and or boundaries between zones of different tones and or colors in the image. Alternatively or additionally other sharpness measurements may be used.

Further zero or more of the m 1 secondary images may then be aligned and merged with the sharpest image. For instance alignment may be attempted between each of the secondary images and the sharpest image respectively. If the alignment fails for parts of a respective secondary image those parts may be discarded and not combined with the primary image. In this way the sharpest image may be denoised with information from some or all of the secondary images.

In the second example the scene type is HDR. In this example the payload burst structure includes ten images captured according to a pattern of long and short TETs and may be referred to as an HDR burst structure. In the Payload Burst Structure column of Table 2 each L may represent an image captured with the long TET each S may represent an image captured with the short TET. Thus the pattern of L S L L S L L S L L may indicate that the first image of the payload burst is captured using the long TET the second image is captured using the short TET the third and fourth images are captured using the long TET the fifth image is captured using the short TET the sixth and seventh images are captured using the long TET the eighth image is captured using the short TET the ninth image is captured using the long TET and the tenth image is captured using the long TET.

The long and short TETs may be determined based on the results of image processing . Thus the long and short TETs may be selected so that the resulting images captured with these TETs can be combined using HDR procedures. The long TET may be used to capture the details in dark sections of the scene while the short TET may be used to capture the details in bright sections of the scene.

Examples of short TET values may include TETs of 1 millisecond 2 milliseconds and or 8 milliseconds while examples of long TET values may include TETs of 20 milliseconds 40 milliseconds and or 80 milliseconds. However short and long TETs may take on different values.

Despite the payload burst in the second example having a particular structure in Table 1 other structures may be used. For example payload burst structures of L S L S L S L S L S or L L S L L S L L S L could potentially provide suitable patterns of long and short TETs. Further some payload burst structures may include medium TETs denoted by an M . Thus additional example payload bursts may include S M L L L S M L L L or S M L S M L S M L L structures.

In some embodiments a payload burst structure may include more or fewer than ten images. Generally speaking the determining the length of the payload burst structure involves a tradeoff. On one hand a long payload burst i.e. a payload burst with a large number of image captures is desirable because the likelihood of one or more of the captured image being well exposed and sharp is increased. On the other hand if the payload burst is too long the likelihood of ghosting due to movement in the scene is also increased. Additionally darker scenes may benefit from images captured using a longer TET so that more light can reach the recording surface. Therefore the payload burst structure may be based possibly in part on these considerations.

In the third example the scene type is also HDR. However in this example the associated payload burst structure which also may be referred to as an HDR burst structure includes seven images captured according to a pattern of long and short TETs followed by three fallback TETs. Each F may represent an image captured with the fallback TET and the fallback TET may take on a value different from both the long and short TETs.

Regardless of the type of payload burst structure the images of an HDR scene may be aligned and combined. Images captured using the short TET may be referred to as short images for convenience and images captured with the long TET may be referred to as long images for convenience.

In some embodiments the sharpest short image may be selected from the short images as the primary short image. Zero or more of the remaining secondary short images may then be aligned and merged with the primary short image. For instance alignment may be attempted between each of the secondary short images and the primary short image respectively. If the alignment fails for parts of the respective secondary short image those parts may be discarded and not combined with the primary short image. In this way the sharpest short image may be denoised with information from some of the secondary short images.

The same or a similar process may be undertaken for the long images. For example the sharpest long image may be selected from the long images as the primary long image. Zero or more of the remaining secondary long images may then be aligned and merged with the primary long image. Alignment may be attempted between each of the secondary long images and the primary long image respectively. If the alignment fails for parts of the respective secondary long image those parts may be discarded and not combined with the primary long image.

The resulting combined short image e.g. the sharpest short image possibly denoised by information from zero or more secondary short images and the resulting combined long image e.g. the sharpest long image possibly denoised by information from zero or more secondary long images may then be aligned. If the alignment succeeds these two images e.g. both LDR images may be combined according to HDR procedures. For instance they may be combined into an HDR image and the HDR image may then be tonemapped so that its brightness falls within a range commensurate with the display abilities of convention video output devices e.g. pixel values between 0 and 255 inclusive . The resulting tonemapped HDR image may be designated as output image . In some embodiments if the signal to noise ratio of part of all of output image is still lower than a threshold value a de noising procedure may be applied to further reduce noise. Additionally output image may also be sharpened possibly after applying the de noising procedure. In general various types of HDR fusion algorithms such as Exposure Fusion or Local Laplacian Filters may be used to merge short and long images. If medium TETs are used in the payload burst structure these HDR fusion algorithms may be applied to one or more medium images as well.

If the alignment fails between the combined short image and the combined long image then the HDR processing fails. However if fallback images were captured one or more of the fallback images may be used to form output image . For instance the sharpest fallback image may be selected. Zero or more of the remaining secondary fallback images may be aligned and combined with the sharpest fallback image carried out in a similar fashion as the processes described above for the short and long images. For payload burst structures without fallback images in which alignment fails the combined long or short image may be used to form output image .

Example payload burst structures are further illustrated in . Payload burst structure may represent a payload burst for an LDR scene payload burst structure may represent a payload burst for an HDR scene and payload burst structure may represent a payload burst for an HDR scene with fallback. Each burst structure shows the approximate duration of the TET for each image capture. For example for burst structures and short TETs are of a lesser duration than long TETs and the duration of fallback TETs are between that of short and long TETs.

In the vertical dotted lines depict epochs in time at which an image capture can begin. Some electronic image sensors operate at a particular frequency such as 30 hertz. A sensor operating at this frequency can capture 30 images per second or approximately one image every 33.33 milliseconds. Thus the duration between the epochs in may be 33.33 milliseconds. Nonetheless for low light scenes the exposure time may be longer than 33.33 milliseconds and the operating frequency of the image sensor may be adjusted downward accordingly. In this example a TET longer than 30 milliseconds can be achieved by increasing the gain component of the TET.

For some sensors image capture may be activated only at the end of such an epoch. Thus as shown in each image capture ends at the edge of a respective epoch. However due to their varying TETs some image captures may begin at different times. Alternatively for some image sensors image capture may be activated only at the beginning of an epoch.

It should be understood that various other techniques procedures and or algorithms may be used determine a payload burst structure. Thus the discussion above accompanying Table 2 and above merely provides some possible payload burst structures. Other techniques may be used without departing from the scope of the embodiments herein.

For example in some embodiments the average pixel value of the tonemapped image e.g. step 5 in Table 1 may be calculated using either an RMS or SMR method based on the length of the TET. The RMS average of n values may be calculated as 

For shorter TETs e.g. the short TETs and or fallback TETs of an HDR payload burst or possibly the TETs of an LDR burst it may be desirable to attempt to increase or maximize the brightness of the pixels without clipping them. Thus taking the RMS average of the pixel values puts more weight on the brighter pixels. For longer TETs e.g. the long TETs of an HDR payload burst it may be desirable to emphasize the darker parts of the scene. Thus taking the SMR average of the pixel values puts more weight on the darker pixels.

Another variation is illustrated in the context of burst structure of . This variation involves selecting the sharpest short image. For sake of argument assume that the sharpest long image is long image . Then instead of selecting the sharpest short image as the primary short image the short image that was captured closest in time to the sharpest long image may be selected as the primary short image. This could be for instance short image which immediately precedes long image . Then the remaining secondary short images may be aligned with and or combined into as the alignments permit the primary short image. Alternatively if the image sensor captures images at the beginning of the epoch a short image following perhaps immediately following the sharpest long image may be selected as the primary short image. As an alternative a temporally adjacent short image long image pair may be selected so that together these images maximize a combined sharpness metric.

In some embodiments the payload burst structure may be based on whether the image sensor captures images with an exposure time of less than the readout time of the images sensor s pixels referred to herein as sub readout exposure times at the beginning or end of an image capture epoch. If the image sensor captures sub readout exposure times at the end of the image capture epoch the payload burst structure may include one or more two TET subsequences of a short TET immediately followed by a long TET. If the image sensor captures sub readout exposure times at the beginning of the image capture epoch the payload burst structure may include one or more two TET subsequences of a long TET immediately followed by a short TET.

One possible advantage of selecting the primary long image in this fashion is to reduce motion blur or ghosting effects. For example if a scene contains motion merging multiple images captured from the scene may result in the motion appearing in the merged image as a blur. In general the greater the difference in time between when the images are captured the greater this blurring. By selecting primary long and short images that are close to one another in time the ghosting may be reduced

It should be noted that the steps illustrated by flow chart may be carried out by various types of cameras and or computing devices such as those exemplified by digital camera device and or computing device . Further it may be possible to distribute aspects of some individual steps between multiple cameras and or computing devices. For example first image capture and second image capture may occur on digital camera device . Further image processing and combining images may occur on a different computing device. Other combinations of distributing individual steps may also exist.

At step based at least on the first plurality of images a TET sequence for capturing images of the scene may be determined. Determining the TET sequence may involve determining a scene histogram based on at least one of the images in the first plurality of images of the scene. The scene histogram may be based on downsampling and combining the images in the first plurality of images of the scene.

In some embodiments additional histograms may be stored in a training image database. These histograms may be based on at least two images captured of respective scenes. The histograms may be associated with respective dynamic range parameters where the respective dynamic range parameters indicate whether the respective scenes are LDR or HDR. Determining the TET sequence may further involve comparing the scene histogram to at least one histogram in the training image database and based on an outcome of the comparison determining a dynamic range parameter for the scene from the respective dynamic range parameters where the TET sequence is further based on the dynamic range parameter for the scene.

If the dynamic range parameter for the scene indicates that the scene is LDR then determining the TET sequence may involve selecting a single TET value to use in the TET sequence. If the dynamic range parameter for the scene indicates that the scene is HDR then determining the TET sequence may involve selecting a short TET value and a long TET value to use in the TET sequence. If the scene is HDR determining the TET sequence may also involve selecting a fallback TET value to use in the TET sequence.

In some embodiments the histograms in the training database may also be associated with respective target pixel values. In these embodiments determining the TET sequence may further involve determining one or more target pixel values for the scene based on the respective target pixel values in the training database selecting one or more TET values to use in the TET sequence based on the one or more determined target pixel values.

At step a second plurality of images of the scene may be captured by the image sensor. The images in the second plurality of images may be captured using the TET sequence. At step based at least on the second plurality of images an output image of the scene may be constructed.

Determining the TET sequence may involve determining that the scene is an LDR scene and defining a common value for TETs in the TET sequence. Constructing the output image of the scene may involve aligning and combining one or more of the images in the second plurality of images captured using the common value.

Alternatively or additionally determining the TET sequence may involve determining that the scene is an HDR scene and defining a short TET value and a long TET value. Constructing the output image of the scene may involve aligning and combining i one or more of the images in the second plurality of images captured with the short TET value and ii one or more of the images in the second plurality of images captured with the long TET value.

Alternatively or additionally determining the TET sequence may involve determining that the scene is an HDR scene and defining a short TET value a long TET value and a fallback TET value. Constructing the output image of the scene may involve attempting to align i one or more of the images in the second plurality of images captured with the short TET value with ii one or more of the images in the second plurality of images captured with the long TET value. Constructing the output image of the scene may further involve determining that alignment of i the one or more of the images in the second plurality of images captured with the short TET value and ii the one or more of the images in the second plurality of images captured with the long TET value has failed. Constructing the output image of the scene may also involve in response to determining that the alignment has failed aligning and combining one or more of the images in the second plurality of images captured with the fallback TET value to form the output image.

At step a second plurality of images of the scene may be captured by the image sensor wherein the images in the second plurality of images are captured sequentially in an image sequence using a sequence of TETs corresponding to the TET sequence. The image sequence may include a three image sub sequence of a first long TET image captured using the long TET followed by a short TET image captured using the short TET followed by a second long TET image captured using the long TET. Alternatively the image sequence may include a three image sub sequence of a first long TET image captured using the long TET followed by a second long TET image captured using the long TET followed by a short TET image captured using the short TET. The pattern in the subsequence may be chosen based on characteristics of the camera device e.g. whether a rolling shutter is used and or other image sensor characteristics and or characteristics of the scene e.g. average pixel value or some other metric . Regardless the sub sequence may repeat one or more times through the payload burst structure.

At step based on one or more images in the image sequence an output image may be constructed. In some embodiments the image sequence may include a two image sub sequence of a primary short TET image followed by a primary long TET image. The image sequence may also include one or more secondary short TET images and one or more secondary long TET images. The primary short TET image and the secondary short TET images may be captured using the short TET and the primary long TET image and the secondary long TET images may be captured using the long TET. Constructing the output image may involve forming a combined short TET image where the combined short TET image includes the primary short TET image and at least part of the one or more secondary short TET images forming a combined long TET image where the combined long TET image includes the primary long TET image and at least part of the one or more secondary long TET images and forming the output image where the output image includes at least part of the combined short TET image and at least part of the combined long TET image.

Alternatively or additionally the image sequence may include a long TET image captured using the long TET. Constructing the output image may involve determining that the long TET image is a sharpest image of all images in the image sequence captured using the long TET. Based on the long TET image being the sharpest image of all images in the image sequence captured using the long TET selecting the long TET image as a primary long TET image and selecting as a primary short TET image an image captured using the short TET that is adjacent to the primary long TET image in the image sequence. Constructing the output image may further involve forming a combined short TET image where the combined short TET image includes the primary short TET image and at least part of one or more images from the image sequence that were captured using the short TET forming a combined long TET image where the combined long TET image includes the primary long TET image and at least part of one or more images from the image sequence that were captured using the long TET and forming the output image where the output image includes at least part of the combined short TET image and at least part of the combined long TET image.

In some examples the primary short TET image may immediately precede the primary long TET image in the image sequence. The short TET image that immediately precedes the primary long TET image in the image sequence may be selected as the primary short TET image based on image capture ending at the edge of respective image capture epochs.

In other examples the primary short TET image may immediately follow the primary long TET image in the image sequence. The short TET image that immediately follows the primary long TET image in the image sequence may be selected as the primary short TET image based on image capture beginning at the edge of respective image capture epochs.

The steps depicted in may be carried out by a camera device such as digital camera device a computing device such as computing device and or by two or more distinct devices. For instance in some embodiments the image capture steps may be performed by an image sensor and the remaining steps may be performed by a separate computing device. Other arrangements are possible. Further the flow charts depicted in may be modified according to the variations disclosed in this specification and or the accompanying drawings.

The term sequence as used herein may refer to one or more TETs arranged sequentially and or one or more images captured sequentially. However in some embodiments a camera device may include multiple image sensors e.g. an image sensor array and these image sensors may capture images sequentially in parallel or using some combination of both. For sake of convenience the terms sequence or sequential may also be used to refer to capturing at least some images of a sequence in parallel.

For instance depicts split paxel downsampling. As noted above an i j pixel block and its associated downsampled pixel may both be referred to as a paxel. Pixel block is an example of a paxel a 3 2 pixel block containing pixel values. These pixel values may be one or more of the R G B values of the respective pixels averages of the R G B values of the respective pixels the luma Y values of the pixels in the YCbCr color space or some other representation of the pixels. Regardless the average of these six pixel values is 96.67.

In addition to determining the average pixel value as part of downsampling the six pixels of pixel block the average value may also be used to split the pixels in the paxel into two groups. The first group consists of the pixels with a pixel value greater than or equal to the average pixel value and the second group consists of the pixels with a pixel value less than the average pixel value. The average of the first group of pixel values may be calculated to determine high average paxel which has a value of 180. The average of the second group of pixel values may be calculated to determine low average paxel which has a value of 55. Additionally a percentage or fraction of high pixels may be determined by calculating the number of pixels in the first group divided by the total number of pixels in the paxel. In some embodiments but not shown in a percentage or fraction of low pixels may be determined by calculating the number of pixels in the second group divided by the total number of pixels in the paxel. Alternatively the percentage or fraction of low pixels may be determined by subtracting the percentage or fraction of high pixels from 1.

Each paxel in a downsampled image may be represented by these three or four values. In this way information about the structure of the paxel such as the variance of the pixel values may be retained. Additionally whether the distribution is more heavily weighted above the average value or below the average value may also be represented by percentage or fraction of high pixels .

Not unlike the process described above in the context of images and may be reverse tonemapped and combined into combined high average image . Thus for each location in the downsampled high average images e.g. pixels A and A would be considered to be in the same location the pixel with the highest value less than 255 may be selected. Possibly independently images and may be reverse tonemapped and combined into combined high average image . Thus for each location in the downsampled low average images e.g. pixels A and A would be considered to be in the same location the pixel with the highest value less than 255 may be selected. In other embodiments a threshold pixel value different from 255 e.g. 250 245 or a threshold pixel value higher than 255 may be used instead.

Each pixel selected from one of high average images and may be placed in its respective location in combined high average image . Thus for instance pixel A may be placed as pixel A and pixel B may be placed as pixel B. Additionally the pixels selected for combined high average image may be reverse tonemapped. Similarly each pixel selected from one of low average images and may be placed in its respective location in combined high average image . Thus for instance pixel A may be placed as pixel A and pixel B may be placed as pixel B. The pixels selected for combined high average image may also be reverse tonemapped

Further the pixel values placed in each of combined high average image and combined low average image may be divided by the respective TET with which the non downsampled pixels in their respective paxels were captured. Doing so may normalize the pixel values that were captured using various TETs to a particular range for instance the pixels of the various images may be placed into the same brightness units.

A histogram may be created from combined high average image and combined low average image . Histogram may be an LDR histogram HDR histogram log HDR histogram or some other type of histogram. While there are many ways in which the histogram can be formed some embodiments may include evaluating a function of the pixel values of each pixel. For instance if the pixel values are represented in the R G B color space the function may take the average or maximum of the values of the R G B color channels.

Alternatively multiple histograms may be constructed. For example in the case of the R G B color space one histogram for the R channel one for the G channel and one for the B channel may be created. If image pyramids are used one histogram pyramid per color channel may be constructed.

Moreover the logarithm of this function s output may also be taken and the result may be used to locate a bin on the x axis of the histogram. The quantity added to this bin may be based on the pixels associated percentage or fraction of high pixels or percentage or fraction of low pixels. For example suppose that pixel A is associated with a percentage or fraction of high pixels of 0.73. Suppose further that pixel A is placed as pixel A. Then once the histogram bin is determined for this pixel the weight of 0.73 may be added to that bin. Similarly suppose that pixel A is associated with a percentage or fraction of low pixels of 0.49. Suppose further that pixel A is placed as pixel A. Then once the histogram bin is determined for this pixel the weight of 0.49 may be added to that bin. In some embodiments this weight may be further modified perhaps by center weighted averaging or by some other technique.

In some embodiments the processes of downsampling and merging the pixel information into histogram may be performed together rather than sequentially. For instance the pixel values for a particular pixel location in both combined high average image and combined low average image may be determined and the associated weights may be added to histogram before considering the next pixel location. Doing so might allow for further normalization of the weights if the high average pixel in combined high average image was taken from one image the low average pixel in combined low average image was taken from another image and the sum of their respective weights is not 1. Other orderings of the procedures illustrated by are also possible and may be included in alternative embodiments.

Histogram may be compared to one or more reference histograms to determine TET values for a subsequent payload burst. For example histogram may be compared to each histogram stored in training image data . A pair of histograms can be compared in various ways such as calculating the EMD between the pair of histograms. As discussed above a weight w perhaps taking on a value between 0 and 1 may be derived from the EMD for a particular pair of histograms.

For each pair of histograms where a first histogram is histogram and a second histogram is from an entry in training image database the weight w may be applied to the respective scene type associated with the second histogram s entry in training image database . The result may be averaged over the pairs of histograms to determine an HDR ness of the scene. For instance if the resulting HDR ness is at or above 0.5 on a scale of 0 to 1 then the scene may be designated for HDR processing but if the resulting HDR ness is below 0.5 on the same scale then the scene may be designated for LDR processing. The embodiment described above is just one possible way of determining the brightness and HDR ness of a scene. Other techniques could be used instead. For instance the short and long TETs for a scene may be determined. If the difference between the short and long TETs or the ratio of the long TET to the short TET is less than or equal to a threshold value the scene may be considered an LDR scene. If the difference between the short and long TETs or the ratio of the long TET to the short TET is greater than the threshold value the scene may be considered an HDR scene.

In order to determine the TETs for the payload burst the following example procedure may be used. However other procedures may be used instead.

For each pair of histograms again where the first histogram is histogram and a second histogram is from an entry in training image database the weight w may be applied to the respective target pixel value associated with the second histogram s entry in training image database . The result may be averaged to determine a target average pixel value for the scene represented by histogram . Then the procedure depicted in Table 1 or some other procedure may be used to determine the TET to be used for subsequent image captures. If the scene type is HDR two or more target average pixel values and two or more respective TETs may be determined. In some embodiments the respective TETs may include a fallback TET. However images might not be captured using the fallback TET unless the merge of short and long images captured during second image capture fails. Alternatively only the images captured during first image capture may be used to form output image .

In particular several variations may be supported by the embodiments herein. In one possible variation a short TET and a long TET may be determined during image processing . Then during second image capture short and long images may be captured using the short and long TETs respectively. If the alignment and or merge of the combined short image and combined long image fails either the combined short image or the combined long image may be provided as or as at least part of output image .

In another possible variation a short TET a long TET and a fallback TET may be determined during image processing . Then during second image capture short long and fallback images may be captured using the short long and fallback TETs respectively. If the alignment and or merge of the combined short image and combined long image fails one of the fallback images or a combined fallback image may be provided as or as at least part of output image . The combined fallback image may be constructed in a similar fashion as the combined short image and or the combined long image.

In yet another possible variation a short TET a long TET and a fallback TET may be determined during image processing . Then during second image capture only short and long images may be captured using the short and long TETs respectively. If the alignment and or merge of the combined short image and combined long image fails either the combined short image or the combined long image may be selected and provided as or as at least part of output image . However in this variation if the fallback TET was greater than the TET of the selected combined image then digital gain may be applied e.g. a multiplication operation on the pixel values to adjust the brightness of output image to a brightness corresponding to the fallback TET. Thus output image may exhibit brightness commensurate with that of an image captured using the fallback TET.

At step the pixel value histogram may be compared to one or more reference pixel value histograms. Comparing the pixel value histogram to the one or more reference pixel value histograms may involve determining for the pixel value histogram and each of the one or more reference pixel value histograms respective similarity metrics and determining respective weights based on inverses of the similarity metrics. The one or more reference pixel value histograms may be associated with respective target average pixel values and the payload TET may be based on a sum of the respective weights applied to the respective target average pixel values.

At step a payload TET may be determined based on comparing the pixel value histogram to the one or more reference pixel value histograms. At least one additional image of the scene may be captured using the payload TET.

In some embodiments downsampling the images in the plurality of images may involve forming a plurality of high average downsampled images and a second plurality of low average downsampled images. Additionally constructing the pixel value histogram based on pixel values of the downsampled images may involve constructing the pixel value histogram based on pixel values of the plurality of high average downsampled images and the plurality of low average downsampled images.

Forming each image of the plurality of high average downsampled images may involve dividing each image in the plurality of high average downsampled images into respective non overlapping matrices of paxels calculating average pixel values of each paxel and calculating high average pixel values of each paxel. Each paxel in the non overlapping matrices of paxels may represent at least a pixel tile of the respective image in the plurality of images with a dimension of 1 2 2 1 2 2 or greater. The average pixel values may be respective average values of all pixels within the respective paxel and the high average pixel values may be respective average values of all pixels within the respective paxel with values greater than or equal to the average pixel value of the respective paxel.

Forming each image of the plurality of low average downsampled images may involve calculating low average pixel values of each paxel. The low average pixel values may be respective average values of all pixels within the respective paxel with values less than the average pixel value of the respective paxel.

Downsampling the images in the plurality of images may further involve calculating respective high pixel fractions for each paxel and respective low pixel fractions for each paxel. The high pixel fractions may be respective ratios of i pixels within the respective paxel with values greater than or equal to the average pixel value of the respective paxel to ii total pixels within the respective paxel. The low pixel fractions may be respective ratios of i pixels within the respective paxel with values less than the average pixel value of the respective paxel to ii total pixels within the respective paxel.

Constructing the pixel value histogram based on the pixel values of the downsampled images may involve combining images from the plurality of high average downsampled images into a combined high average downsampled image and combining images from the plurality of low average downsampled images into a combined low average downsampled image. Combining images from first plurality of high average downsampled images into the combined high average downsampled image may involve for each pixel location in the combined high average downsampled image selecting a high average pixel value from the same pixel location in one of the high average downsampled images. Combining images from the second plurality of low average downsampled images into the combined low average downsampled image may involve for each pixel location in the combined low average downsampled image selecting a low average pixel value from the same pixel location in one of the low average downsampled images.

Constructing the pixel value histogram based on the pixel values of the downsampled images may further include adding to the pixel value histogram a first set of quantities representing each high average pixel value in the combined high average downsampled image and a second set of quantities representing each low average pixel value in the combined low average downsampled image. The quantities in the first set of quantities are based on the respective associated high pixel fractions. The quantities in the second set of quantities are based on the respective associated low pixel fractions.

The steps depicted in may be carried out by a camera device such as digital camera device a computing device such as computing device and or by two or more distinct devices. For instance in some embodiments the image capture step s may be performed by an image sensor and the remaining steps may be performed by a separate computing device. Other arrangements are possible. Further the flow chart depicted in may be modified according to the variations disclosed in this specification and or the accompanying drawings.

Accordingly an illustrative device such those shown in may use a first camera system to record video or for viewfinding while using a second camera system to implements an auto exposure process that includes or is based on the method illustrated in . For example camera A of digital camera device may be used to record video of a scene. While camera A is recording video of the scene a digital camera device may implement an auto exposure process using camera B to build up an HDR histogram of the scene and compare the histogram to reference data in order to provide mid recording adjustments to the exposure used by camera A.

Generally an autofocus AF system may include a sensor of some kind a control system that automatically determines focus settings and a motor to adjust the mechanical components of the camera e.g. the lens according to the focus settings. The data provided by the sensor may be used to evaluate the manner in which the environment is or will be recorded by an image sensor and to responsively control an electro mechanical system that can change the focus of camera e.g. by using the motor to move components of the lens and or changing the size of the aperture . Various types of autofocus techniques may be utilized by an image capture device such as digital camera device .

Most consumer cameras include passive autofocus systems which focus the lens on a subject by passively analyzing the image that is entering the optical system e.g. they do not direct controlled beams of light on the subject in order to focus . Typical passive autofocus techniques include phase detection autofocus PD AF and contrast detection autofocus CD AF which may also be referred to as contrast measurement autofocus.

Passive autofocus processes typically involves a computing system e.g. a processor operating a mechanical lens system to adjust the focal settings of the lens e.g. to change the focal distance and then analyzing whether or not the resulting image from an autofocus sensor is in focus. If the resulting image is not satisfactorily in focus then the computing system again adjusts the focal settings and evaluates the focal characteristics in the resulting image. In some implementations each adjustment to the focal settings may be determined based on some measure of how out of focus the image is or how out of focus a particular portion of the image is . In other implementations the adjustments may be predetermined. In either case this process may be repeated until the resulting image is deemed to be satisfactorily in focus.

As noted above some cameras such as DSLRs may include dedicated autofocus systems which may include one or more sensors that are dedicated to autofocus. Such cameras typically do not use the image sensor which is used to capture images for purposes of autofocus. Further such cameras typically include PD AF system in which light received through the lens is split into a pair of images. Both of the images in the pair may then be directed onto the autofocus sensor and analyzed to determine whether or not the lens is in focus.

One common system PD AF system is a second image registration SIR through the lens phase detection system. An SIR PD AF system utilizes a beam splitter to direct incoming light towards an autofocus sensor. More specifically micro lenses that are located on opposite sides of the lens may direct light from coming from the opposite sides of the lens towards the autofocus sensor which effectively creates a rangefinder with two images being projected onto the autofocus sensor. The images formed by the two micro lenses are then compared to determine a separation error which is evaluated to determine whether the lens is focused correctly. If the separation error indicates that the subject is out of focus e.g. if the separation error is not zero or within some threshold from zero then an adjustment to the focus settings may be calculated based on the separation error and the lens may be moved according to the adjusted settings.

When size and or cost of components are significant in the design of a device the device may utilize a camera system that does not include a separate autofocus system. Such is the case with many mobile phones and or tablet computers which often include camera systems that use the same image sensor for both autofocus and image capture. In many cases cameras in portable devices such as mobile phones and tablets use CD AF for purposes of focusing.

While CD AF systems can use a separate sensor that is dedicated to autofocus most CD AF systems use the same image sensor for both image capture and autofocus. CD AF systems determine whether or not a subject is in focus by measuring the contrast in the image that is detected on the sensor. To do so a CD AF system may evaluate the change in contrast at various points in the image with higher contrast being interpreted as an indication of a sharper image.

More specifically the difference in intensity between adjacent pixels of a sensor is generally greater when the subject captured in the adjacent pixels is in focus as compared to when image subject is out of focus. Further a CD AF system may measure the contrast at specific pixels or determine the average over certain groups of pixels. In either case a CD AF system may then adjust focus settings until a threshold contrast is detected and possibly until a maximum contrast is detected . For example an illustrative CD AF system may pass image data through a high pass filter and adjusts the focus of the lens until the output from the filter exceeds a threshold and possibly until the output of the filter is at its highest level .

The process of capturing multiple images of a scene with different focus settings may involve a focus sweep. When capturing a still photograph a camera system may perform a focus sweep once just before taking picture. When recording video the camera system may perform focus sweeps continuously or at least periodically. Notably when a CD AF system that utilizes the main image sensor performs a focus sweep while recording video this can result in short segments of video that are noticeably out of focus. Note that the process of capturing multiple images of a scene with different focus settings may also involve minor focus hunting in one direction or the other.

Accordingly an illustrative device such those shown in may use a first camera system to record video while using a second camera system to implement CD AF on behalf of the first camera system. For example camera A of digital camera device may be used to record video of a scene. While camera A is recording video of a scene the digital camera device may implement a CD AF process using camera B in order to determine and make mid recording adjustments to the focus settings of camera A.

In another aspect when a device includes three or more cameras the device may use two or more cameras for image capture while using the other camera or cameras to provide auto focus auto exposure and or automatic white balance for all of the cameras that are being used for image capture.

Further method may be implemented for various types of image capture. For example a second camera may be used to update an image setting that is used by a first camera to capture video. As another example a second camera may be used to update an image setting that is used by a first camera to generate image data for display in a viewfinder. Other examples are also possible.

In a further aspect note that one or more cameras that are determining settings on another camera s behalf could operate in a lower power mode while determining the settings. For example a secondary camera that is performing an autofocus process may use cropping to conserve power while doing so. As another example a secondary camera that is performing an auto exposure process may use techniques for binning or skipping of pixels to conserve power. Other examples of low power techniques for determining various camera settings are also possible.

In some implementations methods and may provide an autofocus process which uses a second camera to adjust the focus for a first camera while the first camera is recording video or viewfinding. For example in order to provide autofocus while a first camera of a mobile device is capturing a video of a scene a second camera of the mobile device may be used to perform one or more focal sweeps of the scene such that the focal settings of the first camera can be adjusted. In some implementations the focal sweeps may sweep through a larger number and or range focal settings than if the same camera were performing the focal sweeps and capturing the video. In turn the results of the autofocus process may improve. For example when a second camera to perform autofocus for a first camera the autofocus process may be more responsive to sudden or drastic changes in the scene. Further this configuration may help to reduce and possibly eliminate the visible effects of focus hunting in video that is captured by the first camera.

In some embodiments it may be assumed that the distance between the lens of the second image capture system and the subject being focused on is the same as the distance between the lens of the first image capture system and the subject or at least close enough for purposes of autofocus . In such an embodiment the first image capture system may simply use the focus setting that is determined by the second image capture system e.g. the setting s corresponding to the subject being in focus .

In other embodiments the focus setting e.g. the focal distance that is determined using the second image capture system may be adjusted for use by the first image capture system. For example the focal setting determined using a second camera may be adjusted for a first camera according to the baseline between the first and second cameras i.e. the distance between the two cameras . To do so the distance from the lens of the first camera to the subject may be determined based on a the distance and angle between the lens of the second camera and the subject and b the baseline between the first and second camera. The focal distance used by the second camera may then be adjusted according to the ratio of the distance from the first camera s lens to the subject as compared to the distance from the second camera s lens to the subject in order to determine the distance between the first camera s lens and the subject. The focus setting s for the first camera may then be set such that the focal distance of the first camera aligns with the distance between the subject and the first camera s lens. Other examples are also possible.

Additionally or alternatively methods and may provide an auto white balance process which uses a second image capture system to determine white balance settings for the first image capture system while the first image capture system is recording video or viewfinding. In such embodiments the second image capture system may use any white balancing process that is now known or later developed.

Further methods and may provide an auto exposure process which uses a second image capture system to determine the exposure for the first image capture system. For example a second camera may implement an auto exposure process that adjusts the TET upwards or downwards until the average pixel value is equal to or deviates by less than a predetermined amount from a target average pixel value. Other examples are also possible.

In addition or in the alternative to iterative experiment and adjust processes a computing device may use image data from a second camera system to make model based updates to the settings for a first camera that is capturing image data. For instance a second camera may be used for an auto exposure process while a first camera is recording video.

As a specific example illustrates a method that may be implemented by a device to use a second camera to provide data for a model based auto exposure process. Method may be implemented for example at block of method . Further method may be implemented repeatedly while a first camera of a device is recording a video of a scene to use a second camera of the device to provide an auto exposure process for the first camera.

By implementing method a mobile device s second camera may be used to capture image data from which the mobile device or a remote device in communication with the mobile device can progressively build an HDR histogram of the scene which is updated as new image data with different exposure settings is captured by the second camera. The progressively built histogram may be periodically or possibly even continuously compared to reference data in order to provide mid recording adjustments to the exposure of the first camera.

As shown by block method involves a computing device operating the second image capture system to capture a plurality of second images of a scene where each image of the plurality of images is captured using a different value for an exposure setting. At block the computing device downsamples the plurality of second images. The computing device may then construct a pixel value histogram based on pixel values of the downsampled images as shown by block . Further the computing device may compare the pixel value histogram to one or more reference pixel value histograms as shown by block . The computing device may then determine an updated value for the exposure setting based on the comparison of the pixel value histogram to the one or more reference pixel value histograms as shown by block .

In some embodiments block may involve determining a TET for use by the first image capture system to continue to capture the first image data. Accordingly method may be repeated a number of times during a video recording by a first camera in order that the TET used by the first camera to capture the video may be adjusted a number of times throughout the recording. Similarly method may be repeated a number of times while a first camera is capturing image data for presentation in a viewfinder such that the TET of the first camera may be adjusted a number of times while capturing image data for the viewfinder.

In a further aspect at block a comparison of the pixel value histogram to the one or more reference pixel value histograms may be accomplished using various techniques. For example the comparison of the pixel value histogram to the one or more reference pixel value histograms may involve determining for the pixel value histogram and each of the one or more reference pixel value histograms respective similarity metrics and determining respective weights based on inverses of the similarity metrics. The one or more reference pixel value histograms may be associated with respective target average pixel values and the TET may be determined at block based on a sum of the respective weights applied to the respective target average pixel values. The TET for the first camera may then be updated accordingly.

In some embodiments downsampling the images in the plurality of images may involve forming a plurality of high average downsampled images and a second plurality of low average downsampled images. Additionally constructing the pixel value histogram based on pixel values of the downsampled images may involve constructing the pixel value histogram based on pixel values of the plurality of high average downsampled images and the plurality of low average downsampled images.

Forming each image of the plurality of high average downsampled images may involve dividing each image in the plurality of high average downsampled images into respective non overlapping matrices of paxels calculating average pixel values of each paxel and calculating high average pixel values of each paxel. Each paxel in the non overlapping matrices of paxels may represent at least a pixel tile of the respective image in the plurality of images with a dimension of 1 2 2 1 2 2 or greater. The average pixel values may be respective average values of all pixels within the respective paxel and the high average pixel values may be respective average values of all pixels within the respective paxel with values greater than or equal to the average pixel value of the respective paxel.

Forming each image of the plurality of low average downsampled images may involve calculating low average pixel values of each paxel. The low average pixel values may be respective average values of all pixels within the respective paxel with values less than the average pixel value of the respective paxel.

Downsampling the images in the plurality of images may further involve calculating respective high pixel fractions for each paxel and respective low pixel fractions for each paxel. The high pixel fractions may be respective ratios of i pixels within the respective paxel with values greater than or equal to the average pixel value of the respective paxel to ii total pixels within the respective paxel. The low pixel fractions may be respective ratios of i pixels within the respective paxel with values less than the average pixel value of the respective paxel to ii total pixels within the respective paxel.

Constructing the pixel value histogram based on the pixel values of the downsampled images may involve combining images from the plurality of high average downsampled images into a combined high average downsampled image and combining images from the plurality of low average downsampled images into a combined low average downsampled image. Combining images from first plurality of high average downsampled images into the combined high average downsampled image may involve for each pixel location in the combined high average downsampled image selecting a high average pixel value from the same pixel location in one of the high average downsampled images. Combining images from the second plurality of low average downsampled images into the combined low average downsampled image may involve for each pixel location in the combined low average downsampled image selecting a low average pixel value from the same pixel location in one of the low average downsampled images.

Constructing the pixel value histogram based on the pixel values of the downsampled images may further include adding to the pixel value histogram a first set of quantities representing each high average pixel value in the combined high average downsampled image and a second set of quantities representing each low average pixel value in the combined low average downsampled image. The quantities in the first set of quantities are based on the respective associated high pixel fractions. The quantities in the second set of quantities are based on the respective associated low pixel fractions.

Note that the split paxel technique above which utilizes high average and low average pixel values is just one way of building the pixel value histogram. In other cases the split paxel technique described above may be omitted and a histogram may be created directly from the downsampled image. Other techniques for generating a pixel value histogram are also possible.

Note that the use of a second camera for an auto exposure process such as method may allow for significant experimentation with the exposure which may improve the results of the auto exposure process. Further since the histogram of the scene may be progressively built during a video recording the histogram may improve as the video recording progresses. Therefore the results of the auto exposure process may also improve the longer a recording of a scene lasts.

In some embodiments a device may use an example method such as method to adjust two or more image settings. Method or portions thereof may be implemented for each of two or more different types of image settings. As such all of the two or more different types of image settings may be updated for a first image capture system while the first image capture system is capturing the first image data of the scene.

As a specific example one or more second cameras on a device may be used to concurrently update both a focal setting and an exposure setting for a first camera on the same device while the first camera device is capturing image data e.g. recording video or capturing a burst of still images . As another specific example one or more second cameras on a device may be used to update a near field focus setting and a far field focus setting for a first camera on the same device while the first camera device is capturing image data. For example a second camera could perform focus sweeps or focus hunting at distances that are further from a device and a third camera could perform focus sweeps or focus hunting at distances that are nearer to the device while a first camera is capturing video.

Further note that one or more second cameras on a device could also be used to concurrently update multiple settings e.g. a focal setting and an exposure setting for two or more other cameras on the same device that are concurrently capturing image data. For example a first camera could perform an AF process and a second camera could simultaneously perform an auto exposure process in order to adjust the focus and exposure settings for both a third camera and a fourth camera that are capturing stereo video e.g. to create 3D video footage . Other examples are also possible.

In some embodiments the device may use one secondary camera to update multiple image settings for a first camera that is recording video. For instance method may further involve the computing device alternating between a operating the second image capture system to update the first image setting for the first image capture system and b operating the second image capture system to update the second image setting.

As an example while the first camera is recording a video a computing device may switch back and forth between a using a second camera to determine a focal setting for the first camera and b using the same second camera to determine an exposure setting for the first camera.

As another example while the first camera is recording a video a computing device may switch back and forth between a using a second camera to determine a focal setting for the first camera and b using the same second camera to determine a white balance setting for the first camera.

And as yet another example while the first camera is recording a video a computing device may rotate between a using a second camera to determine a focal setting for the first camera b using the same second camera to determine a white balance setting for the first camera and c using the same second camera to determine an exposure setting for the first camera. Other examples are also possible.

In some embodiments example methods may be implemented by a device that includes three or more cameras. In such a device two or more additional cameras can be used to update simultaneously update different image settings for a first camera that is recording video. As such method may further involve while the first image capture system is capturing the first image data a operating by the computing device the third image capture system to determine an updated value for the second image setting and b sending an instruction to the first image capture system that indicates to use the updated value for the second image setting to continue to capture the first image data.

As an example when the first camera is recording video a second camera may be used to determine focus settings for the first camera e.g. for autofocusing while the third camera may be simultaneously used to determine exposure settings for the first camera e.g. for auto exposure . As another example while the first camera is recording a video a computing device may simultaneously a use a second camera to update the focus of the first camera b use a third camera to update white balancing for the first camera and c use a fourth camera to update the exposure of the first camera. Other examples are also possible. Further note that such functionality may be implemented by a set of cameras in an array configuration which are all capturing image data. However other configurations are also possible.

Further when a device includes three or more cameras one camera may determine a setting or settings for use by two or more of the other cameras. For example a first camera on a device may perform an autofocus process an auto exposure process and or an auto white balance process on behalf of two or more other cameras on the device. Other examples are also possible.

The above detailed description describes various features and functions of the disclosed systems devices and methods with reference to the accompanying figures. In the figures similar symbols typically identify similar components unless context indicates otherwise. The illustrative embodiments described in the detailed description figures and claims are not meant to be limiting. Other embodiments can be utilized and other changes can be made without departing from the scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure as generally described herein and illustrated in the figures can be arranged substituted combined separated and designed in a wide variety of different configurations all of which are explicitly contemplated herein.

With respect to any or all of the message flow diagrams scenarios and flow charts in the figures and as discussed herein each step block and or communication may represent a processing of information and or a transmission of information in accordance with example embodiments. Alternative embodiments are included within the scope of these example embodiments. In these alternative embodiments for example functions described as steps blocks transmissions communications requests responses and or messages may be executed out of order from that shown or discussed including in substantially concurrent or in reverse order depending on the functionality involved. Further more or fewer steps blocks and or functions may be used with any of the message flow diagrams scenarios and flow charts discussed herein and these message flow diagrams scenarios and flow charts may be combined with one another in part or in whole.

A step or block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein described method or technique. Alternatively or additionally a step or block that represents a processing of information may correspond to a module a segment or a portion of program code including related data . The program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique. The program code and or related data may be stored on any type of computer readable medium such as a storage device including a disk drive a hard drive or other storage media.

The computer readable medium may also include non transitory computer readable media such as computer readable media that stores data for short periods of time like register memory processor cache and or random access memory RAM . The computer readable media may also include non transitory computer readable media that stores program code and or data for longer periods of time such as secondary or persistent long term storage like read only memory ROM optical or magnetic disks and or compact disc read only memory CD ROM for example. The computer readable media may also be any other volatile or non volatile storage systems. A computer readable medium may be considered a computer readable storage medium for example or a tangible storage device.

Moreover a step or block that represents one or more information transmissions may correspond to information transmissions between software and or hardware modules in the same physical device. However other information transmissions may be between software modules and or hardware modules in different physical devices.

While various aspects and embodiments have been disclosed herein other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting with the true scope being indicated by the following claims.

