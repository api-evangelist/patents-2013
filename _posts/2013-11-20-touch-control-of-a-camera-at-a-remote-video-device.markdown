---

title: Touch control of a camera at a remote video device
abstract: A device, computer program product, and method control the operation of one or more cameras associated with a video conference endpoint in a video conference call. A camera control unit (remote control) includes a touch enabled end user component that presents to the user graphical objects representing video conference endpoints currently connected in a conference call, and/or one or more live video feeds currently captured by the video conference endpoint's cameras, and allows the user to manipulate a camera's pan, tilt and zoom using a touch screen display embedded in said camera control unit.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09172911&OS=09172911&RS=09172911
owner: Cisco Technology, Inc.
number: 09172911
owner_city: San Jose
owner_country: US
publication_date: 20131120
---
The present application is a continuation application of and claims the benefit of priority under 35 U.S.C. 120 from U.S. application Ser. No. 12 894 751 filed Sep. 30 2010 which claims the benefit of the earlier filing date of U.S. Provisional Patent application Ser. No. 61 251 509 filed in the USPTO on Oct. 14 2009 the entire contents of each of which is incorporated herein by reference.

The present invention relates to a touch screen control device for a video conferencing system and more specifically a method computer program product and device for controlling a camera of a video conferencing system.

Conventional video conferencing systems include a number of end points communicating real time video audio and or data often referred to as duo video streams over and between various networks such as WAN LAN and circuit switched networks.

In most high end video conferencing systems high quality cameras with pan tilt and zoom capabilities are used to capture a view of the meeting room and the participants in the conference. The cameras typically have a wide field of view FOV and high mechanical zooming capability. This allows for both good overview of a meeting room and the possibility of capturing close up images of participants. The video stream from the camera is compressed and sent to one or more receiving sites in the video conference. All sites in the conference receive live video and audio from the other sites in the conference thus enabling real time communication with both visual and acoustic information.

Video conferences vary a great deal when it comes to purpose the number of participants layout of conference rooms etc. Each meeting configuration typically requires an individual adjustment of the camera in order to present an optimal view. Adjustments to the camera may be required both before and during the video conference. For example in a video conference room seating up to 16 persons it is natural that the video camera is preset to capture all of the 16 available seat locations. However if only 2 or 3 participants are present the wide field of view camera preset will give the receiving end a very poor visual representation since most of the screen will display unoccupied seating areas.

Adjustments to the camera are typically done using a standard input device such as a keypad on a remote control or a mouse either by manually controlling the camera s pan tilt and zoom or by choosing between a set of pre defined camera positions. Typically a traditional IR remote control like that shown in with standard push buttons is used to adjust the camera. A standard setup is a set of four arrow keys to control the pan and tilt and a zoom in and zoom out button to control the zoom.

Finding the optimal camera adjustment on known systems often requires several iterations of pushing buttons on a remote control or an on screen menu system which makes it cumbersome distractive and not very intuitive.

Even though the camera s pan tilt mechanism include small step motors allowing high resolution movement the video conferencing system is often programmed to move the camera in steps to spare the user from excessive key pushing. This works as intended when the camera is in a wide FOV however it may cause trouble when the camera is zoomed in since the steps then become quite large.

In addition if a user wants to control the camera of a remote endpoint the user needs to navigate through a number of on screen menus to access far end camera control settings and then if more than one remote endpoint is connected in the conference choose the correct remote endpoint from a list of connected endpoints. This process is cumbersome and for inexperienced user it may be difficult to do successfully.

Hence as recognized by the present inventors the presently known methods for controlling the camera in a video conference are cumbersome and confusing therefore the camera is often left in a sub optimal setting in a video conference resulting in a degraded video experience.

Today users of technical installations are accustomed to and demand systems which are easy to use and provide flexibility in ways of customization of graphical environments and collaboration between devices.

Traditional video conferencing systems are not very flexible. Further traditional video conferencing systems are operated using on screen menu systems controlled by a keypad on an IR remote control device allowing for limited flexibility and cumbersome user experience.

It is an object of the present invention to provide a system computer program product and method that addresses the above identified and other drawbacks of conventional approaches.

In the following the present invention will be discussed by describing various embodiments and by referring to the accompanying drawings. However people skilled in the art will realize other applications and modifications within the scope of the invention in light of the present teachings.

The presented invention relates to a system and method for controlling the operation of one or more cameras associated with a video conference endpoint in a video conference call. A camera control unit remote control according to the present invention is a touch enabled end user component that presents to the user graphical objects representing video conference endpoints currently connected in a conference call and or one or more live video feeds currently captured by the video conference endpoint s cameras and allows the user to manipulate a camera s pan tilt and zoom using a touch screen display embedded in the camera control unit.

Illustrated in is an example of a communications network in which the present embodiment may be applied. Each user is connected to the communications network through an endpoint apparatus . The endpoint apparatus is described in more detail below. The communications network is an internet protocol IP network . However users may also be connected to a different communications network such as the Integrated Digital Services Network ISDN or other digital communications network including a packet switched network or a circuit switched network. An MCU Multipoint Control Unit may also be connected to a communications network over which the video conference call can be provided. Alternatively one of the endpoint apparatuses includes an embedded MCU for providing multipoint services. A number of endpoint apparatuses residing at different sites may participate in the same conference through one or more MCU s performing e.g. switching and mixing functions to allow the audiovisual terminals to intercommunicate properly. Each endpoint and hence each audiovisual stream is provided with its own unique endpoint identifier ID .

If users from more than one type of communications network wish to participate in a video conference call the two communications networks may be linked using a gateway . The gateway allows translation of data transmitted using different protocols into the protocols appropriate for transmission of data across each type of network. A wireless touch panel controller e.g. IPAD or PHONE for example may also connect to the network via BLUETOOTH or WIFI for example. The wireless touch panel controller is used to control the camera settings or used as the video conference endpoint. Touch panel and touch screen display are used interchangeably herein.

The endpoint apparatus is a device connectable to a communications network for video and or audio conferencing which enables a user to send information across and receive information from the communications network. For a video conference the endpoint apparatus may be a traditional telephone handset if the user is only connecting to receive audio data. However more commonly the endpoint apparatus will enable video data to be received from and transmitted across the communications network and also be displayed to the user and is hereafter referred to as a video conference endpoint. Examples are traditional endpoints or portable wireless endpoints .

Reference is now made to . With particular reference to the typical video conference endpoint is designated by the reference numeral . The endpoint includes a camera and a microphone supplying video and audio signals to a CODEC which supplies audio video signals to one or more monitor s . The CODEC may also be supplied with signals from external multimedia sources e.g. VCR s DVD players document cameras personal computers etc. As understood by the double headed arrow between the CODEC and the external source s the CODEC may receive data signals e.g. video audio still images etc. from the external sources to be displayed on the monitor s and or the signals from the external sources may be sent to other video endpoints connected to the video endpoint via a network .

The video conference endpoint may be an H.323 or SIP endpoint if it is connected to an IP network or an H.320 endpoint if it is connected to an ISDN network. Incidentally H.323 and H.320 are standards defined by the International Telecommunications Union.

The video endpoint may also be provided with a far end camera control FECC facility. The FECC facility is traditionally accessed by selecting buttons and on a user input device such as the IR remote control illustrated in and shown in . The FECC commands travel down the same communications link as the audio and video data and act to control another video endpoint. For example they allow a user to cause another camera other than their own to pan zoom tilt or focus. The commands may be sent from the controlling video conference endpoint to the other video conference endpoint using a signaling protocol message such as ITU T H.281 or H.323 Annex Q the entire contents of each of which being incorporated herein by reference.

The CODEC or computer on behalf of the CODEC has an API that allows users to programmatically change the pan tilt and or zoom of an endpoint camera using a camera control unit . API is an abbreviation for Application Programming Interface . With further reference to API communications between the camera control unit or remote control and the CODEC are exchanged via a port in the CODEC and a port in the camera control unit . This part may optionally be configured as a wireless link such as BLUETOOTH or WIFI and the control unit may be a mobile device see . This interface accessed via the port permits the presently described system to communicate with the CODEC so that the camera control unit according to the present embodiment can provide the CODEC with a desired series of commands and receive responses from the CODEC .

With reference to the components of a system according to the present embodiment are generally designated by the reference numeral and are seen to include a touch screen display unit a personal computer and a graphics generator . As seen the personal computer or processor of mobile device has a port to which is coupled a communication link coupled wired or wirelessly to the API communications port of the CODEC .

The touch screen display includes an LCD screen or other video display technology CRT OLED Plasma etc. that can be of varying size. In addition to the display screen the touch screen contains hardware that overlays the display screen with a matrix of x and y coordinate detectors. When an object finger stylus etc. applies pressure touch to the touch screen display it transmits a command to the computer . The command at least includes the x and y coordinates of the point where the pressure was applied. The graphical interface may be implemented with a downloadable app that is executed on the mobile device . The downloadable app is a software program that is executable on the mobile device .

The camera control unit communicates with the CODEC using the previously mentioned API. The communication between the CODEC and the camera control unit include information from the CODEC and commands to the Codec . When a video conference call is started the CODEC sends information to the camera control unit at least identifying the number of participants and endpoint identifier for each participant.

According to one exemplary embodiment of the present invention the CODEC includes a video converter . The video converter receives video streams video conference streams from the near end camera endpoint and or one or more video conference streams from a far end video conference endpoint processes the video streams video conference streams and outputs a corresponding set of video streams with reduced resolution and or bit rate. According to one exemplary embodiment of the invention the video converter generates one or more video streams by coding the received video streams according to one of a number of standards for coding video for real time applications such as ITU I H.261 H.262 H.263 H.264 or similar video coding standards. According to another exemplary embodiment of the present invention the video converter takes snapshots of the received video streams at predefined intervals and then resizes the snapshots before it outputs the series of snapshots as a video stream. The video streams are resized because the touch screen display is normally of a much smaller size than the size of a video conference endpoint monitor for which the received video streams are intended for. Typical intervals of such an embodiment can be in the range of 30 snapshots per second to 1 snapshot every 5 seconds.

In response to receiving the information from the CODEC the computer generates a graphical user interfaces GUI to be displayed on the touch screen display . The GUI is used for controlling operation of one or more cameras such as a near end camera or a far end camera. The graphical user interface can also include a number of touch controls menus and dialogs that can be displayed on the touch screen display for a user to operate and control the endpoint X. For example the GUI can allow the user to place or answer a video conference call disconnect one or more video conference calls control a near end camera control a far end camera and other typical video conference actions. Once again the GUI and control functions may be implemented by way of an app on the mobile device .

According to one embodiment when a video conference call is started the CODEC sends a signal to the camera control unit indicating that a conference is started and identifying at least the unique conference ID of the video conference endpoint participating in the conference call. In response to receiving the signal from the CODEC a graphics generator under control of the personal computer generates and provides to the touch screen via a port a graphical user interface for allowing operation of one or more cameras associated with one or more of the video conference endpoints connected in the video conference call. Alternatively the graphical user interface for allowing operation of one or more cameras are only generated and provided to the touch screen upon a request from the user.

As illustrated in the graphical user interface is composed of a scene where the scene is a defined area including graphical objects . The scene may or may not be outlined with a distinct border and if the scene is not outlined the scene will appear to be part of the background. Each graphical object represents one of the video conference endpoints connected in the ongoing conference call. According to another exemplary embodiment the graphical user interface also include a menu area in addition to the scene.

The scene is a graphical representation of the conference call and visualizes to the user the different participants in the call. The scene may or may not include a graphical object representing the local video conference endpoint to which the camera control unit is connected.

According to one embodiment of the present invention the graphical objects are images identifying the video conference endpoints that the graphical objects represent respectively. The image may be an outline of one or more persons avatars a computer generated image a photograph text describing the video conference endpoint e.g. name of participant name of video conferencing system e.g. endpoint ID the name of the location where the video conference system is located etc. or a combination of two or more of the above.

According to another embodiment the graphical objects are live video feeds from the connected video conferencing endpoints. The live video feeds are provided by the video converter described above.

As illustrated in when a user touches the symbol represents a touch registered by the touch screen system the screen of the touch screen system with an object e.g. finger or stylus the x and y coordinates corresponding to the location of the touched point are transmitted to the computer via the port the conductor and a port on the computer . If a user touches a coordinate within an area of the screen displaying one of the graphical objects as in the computer compares the touched x and y coordinate with the content of the GUI and interprets the user s action as a selection of the touched graphical object and hence a selection of the video conference system associated with that graphical object. The user may then operate the camera associated with the video conferencing endpoint represented by the selected graphical object. The selected graphical object may be highlighted to illustrate to the user that the camera control unit remote control has registered the user s selection. Alternatively the selected graphical object is enlarged. According to one embodiment the selected graphical object is enlarged while the other graphical objects are reduced in size. According to another embodiment the selected graphical object are enlarged so as to cover the entire scene.

The various graphical camera view options shown in are associated with corresponding fixed camera settings that correspond with camera pan tilt and zoom setting that correspond with the GUI option. The fixed camera settings can be used for an initial rough adjustment that can be further refined through user interaction with the touch screen. For example the top single person option displayed in the touch screen system has a pre set pan tilt zoom setting for that particular far end endpoint which would reflect the physical layout at that particular location. Moreover the physical layout would reflect a distance to the camera for that particular seat height of the seat and angle of the seat relative to the camera. The camera or the endpoint may save these settings in computer readable memory. The bottom left display showing two avatars at a distance would reflect a different camera setting. Likewise the object reflects yet another pan tilt zoom camera setting for that physical location.

The operation of the camera s is performed using the touch screen. The camera control unit will provide the user with visual audible and or tactile feedback when the user operates the camera control. The feedback ensures the user that the camera control unit has registered the users command. Examples of visual feedback are that the arrow keys change form size or color when pressed. One example of tactile feedback is that the camera control unit include a vibrating device and that the vibrating device is activated when a camera control e.g. arrows keys are pressed. An example of audible feedback is that the camera control unit includes a loudspeaker and that the camera control unit emits a sound via the loudspeaker when a camera control is pressed or camera control gesture is performed.

According to one embodiment the operation of the camera involves displaying camera controls on the touch screen display and receiving touch data from a user with respect to those camera controls to control operation of the camera. When generating the GUI the computer designates at one area of the GUI for each touch control up down left right zoom inn zoom out etc. . When the computer receives touch data coordinates from the touch screen display the computer compares the coordinates with the designated areas and associates the received touch data with the camera control instruction designated for the touched area.

According to one exemplary embodiment the displayed camera controls includes a set of arrow keys or similar buttons indicating direction for controlling up down left and right movement of the camera. Camera controls for operating the camera s zoom function include a sliding bar or alternatively two buttons representing zoom in and zoom out respectively. The user may operate the camera by pressing the desired arrow key or zoom key repeatedly or by pressing and holding until the camera has moved to the desired position. One exemplary camera control as described above is illustrated in .

According to one embodiment the operation of the camera involves applying a tactile finger gesture in general or on certain areas of the GUI. The computer receives the finger gestures as a series of consecutive touch coordinates. The computer designates a finger gesture for each touch control up down left right zoom in zoom out etc. either as a gesture performed anywhere on the screen or as a gesture performed in connection with a certain area. When the computer receives touch data coordinates from the touch screen display and if a series of consecutive coordinates imply a gesture the computer compares the coordinates with the designated gestures and associates the received touch data with the camera control instruction designated for the performed gesture.

According to one exemplary embodiment the displayed camera controls include a virtual joystick as illustrated in and i.e. . The virtual joystick can e.g. include a set of arrow keys and a central body or joystick icon . By dragging the central body in a desired direction the user is able to control the pan tilt camera more freely and intuitively without the restrictions of the stepwise movement of 4 directions. In this embodiment the camera may be moved in any direction with one movement or touch. Additionally the speed of the camera pan tilt may be determined by the distance between the current position and the initial position of the body. When the user releases the central body the camera stops moving.

According to yet another embodiment when the graphical objects are live video feeds the camera may be operated by interacting with the graphical object directly. Since the user now can see the actual image captured by a camera on the touch screen display in the GUI the user can operate the camera s pan tilt and zoom movements by performing finger gesture on the live video stream in the GUI as illustrated in . The user may control a cameras pan tilt by performing a drag and drop finger gesture on the video stream as shown in . In this embodiment the video stream from the currently controlled camera is constantly updated during the drag and drop procedure. In this way the GUI allows the user to drag the image around on the touch screen display until a desired view is found just as a physical object can be moved on a desk or table surface. Further the user can zoom by performing certain predefined gestures on the displayed video stream e.g. by performing pinching movements of two or more fingers while continuously applying pressure to the touch screen for zooming in and zooming out as illustrated in . Alternatively the zoom function may be activated by pressing and holding for a predetermined period of time e.g. 0.5 2 seconds . After the predetermined period of time a zoom icon and or a sliding bar may appear to illustrate that zoom function is activated. Then the zoom may be operated by sliding the finger in a horizontal or vertical direction as shown in .

According to yet another embodiment when the graphical objects are displaying live video streams the user may zoom in on a participant of choice by tapping on or double tapping or any other suitable gesture such as a simultaneous separation of two fingers that are in contact with the touch screen on a participant in the live video feed as illustrated in . When double tapping on a participant the camera control unit will instruct the camera associated with the relevant video stream via the local CODEC to zoom in on that person. For fixed installations e.g. conference rooms with fixed seating and constant distances between camera and participants this can easily be performed by panning tilting and zooming in to a predefined camera position preset . If the associated conference room is not a fixed installation face detection algorithms may be applied to determine the position and size of a face closest to the touched x and y coordinate. If a face is detected and position and size of the face is obtained the camera control unit calculates an appropriate pan tilt and zoom position to frame the participant of choice and sends the camera control instructions to the CODEC which in turn instructs the relevant video endpoint and camera using the Far End Camera Control FECC protocol. Exemplary face detection systems and methods applicable for camera control use herein are found in US Patent Publication 20090015658 the entire contents of which being incorporated herein by reference.

According to yet another embodiment when the graphical objects are live video feeds the user can zoom in on one or more participant s or any other area of the live video feed by framing the area of interest using gestures recognized by the computer . As illustrated in the area of interest can be defined by navigating a finger or object on the touch screen around the area of interest or by defining at least two of the corners of the area of interest by tapping or double tapping two or more fingers on the touch screen as shown in .

Next when the user has touched the desired camera control button in the graphical user interface displayed on the touch screen or applied a finger gesture associated with a camera control according to one or more of the embodiments above the computer receives the touch data from the touch screen display and associates them with camera control instructions understood by the CODEC.

The computer then transmits a command or signal including the endpoint ID of the selected video conference endpoint and the camera control instructions. The relevant endpoint ID identifies which of the connected video conferencing system the selected video stream originates from and hence to which video conference system the FECC instructions should be sent to or if the camera control instructions should be sent to its own camera. The command is sent to the CODEC via the port the communication link and a port on the CODEC . If the received endpoint ID is the ID of the receiving CODEC the CODEC sends control instructions to the camera connected to the CODEC . If the received endpoint ID is the ID of one of the remote video conference systems connected to the CODEC the CODEC generates FECC instructions based on the received command and sends to the identified video conference system which in turn sends a control instruction to its camera.

According to one embodiment the camera control unit is a dedicated device. The dedicated device may be a default part of the video conferencing system or be an add on device acquired separately.

According to another embodiment a portable computing device such as a personal digital assistant mobile phone laptop computer tablet computer or similar portable computing device having a touch screen interface and a communication interface supported by the video composing Server e.g. TCP IP may be utilized as the camera control unit. A client software program e.g. camera control client app may be downloaded and or installed on such a portable computing device enabling the portable computing device as a camera control unit.

The computer in the camera control unit may in addition to a processor include a memory medium s on which one or more computer programs or software components according to one embodiment of the present invention may be stored. For example the graphics generator to be deployed may be stored on the memory medium of the computer . Also the memory medium may store a graphical programming development application used to create the graphics generator as well as software operable to convert and or deploy the graphics generator on the portable computing device. The memory medium may also store operating system software as well as other software for operation of the computer system.

In more detail the computer is capable of executing logical instructions written in a computer programming language. The computer controls operation of the CODEC via a PCI or other appropriate bus physically installed in the computer with the CODEC via a communication link schematically represented in . According to one exemplary embodiment the communication between the computer and the CODEC is exchanged using TCP IP over communication link . Communication link may be a wired or wireless link such as PAN CAN MAN LAN WLAN WAN etc. The communication between the CODEC and the computer are commands that go back and forth between the port of the computer and the port of the CODEC . As should be understood these communications conform to the manufacturer s Application Programming Interface API of the video composing Server .

Communications also occur using the touch screen via a communications link shown in and designated by the reference numeral for the conductor and and for the ports. Images from the computer graphics generator is transferred to and displayed on the touch screens display via port conductor and port .

With reference to communication between the CODEC and the computer follows a similar procedure. When predefined events occur in step the CODEC sends in step a command signal to the computer at least including a set of Endpoint ID s identifying the video conference endpoints currently connected to the CODEC directly or via an MCU in a conference call. The predefined event may include when a video conference is started when a new video conference endpoint joins an ongoing conference when a camera control unit connects to the CODEC during a conference on request from the camera control unit etc. For example using a video conferencing endpoint C as illustrated in a user C calls participant A on a video conferencing endpoint A and a participant B on a video conferencing endpoint B . The video conferencing endpoint C is already connected to a camera control unit according to an embodiment of the present invention and when the conference starts all the endpoints are connected the CODEC of video conference endpoint C sends a command to the camera control unit s computer indicating that a conference is ongoing and identifying at least the Endpoint ID s of each of the video conference endpoints in the present call in this case the Endpoint ID of video conference systems A B and C .

Next in step the computer graphics generator creates a graphical user interface GUI having at least a scene . The Scene is a dedicated area of the GUI dedicated for illustrating the currently connected endpoints of the ongoing video conference. Based on the received Endpoint ID s the computer generates a set of graphical objects one for each received Endpoint ID. The graphical objects occupy parts of or the entire dedicated area of the scene. Even though one graphical object is generated for each video conference endpoint the user can control the setup of the scene e.g. the graphical object representing the local video conference endpoint may be omitted. In its simplest form the graphical objects comprise one or more avatars and a text field containing information associated with the relevant video conference system. The text in the text field may be the Endpoint ID itself or information associated with the Endpoint ID stored in a memory or network device accessible to the computer such as the name of the participant name or address of the video conference system Endpoint ID etc. In another exemplary embodiment the graphical objects comprise live video feed from the respective video conference systems. The computer sends the GUI image to the touch screen display via the port communication link and port on the touch screen display .

A user now has a very readily understandable graphical representation of all the participants in the video conference call. Next in step to control a camera the user selects a camera or video conference endpoint to control by touching one of the graphical objects in the Scene . In response to the user s selection touch the touch screen transmits the x and y coordinates of the touched area or areas to the computer via port communication link and port . The computer registers the users selection of camera video conference endpoint to control and the computer and graphics generator process the information from the touch screen and update the GUI Scene by e.g. outlining or enlarging the selected graphical object and hence the image displayed on the touch screen live.

In step a user controls a selected video conference endpoint s camera by manipulating the GUI e.g. graphical objects control menus buttons gestures by touching the touch screen and or performing finger gestures on the touch screen as described in more detail above. The user s touch data is sent to the computer via link in the form of x and y coordinates.

Next in step upon receiving the touch coordinates or set of consecutive touch coordinates gesture the computer compares the touch coordinates with a set of areas of the GUI and or finger gestures designated as camera control instructions. The designation of at least one area of the GUI and or at least one finger gesture as at least one camera control instruction is performed either as a preconfigured designation stored in a memory accessible to the computer or the designation are made when generating the GUI and temporarily stored in a memory accessible to the computer . If a positive match is found the computer associates the received touch coordinates with a camera control command.

Next in step if the computer associates the received touch coordinates with a touch command the computer sends a command to the CODEC at least comprising an Endpoint ID and the associated control command. The CODEC analyses the Endpoint ID and the associated control command and either generates a FECC command and sends it to the relevant video conference endpoint or sends the camera control commands to its local camera if the Endpoint ID identifies said local video conference endpoint .

In a final step the CODEC then sends an action completed signal to the computer via the port communication link and port . Once the action has been completed in the manner described above the computer awaits indication of the next touch of the screen by the user either in the form of a new camera control instruction or a new camera selection or a change in number of connected video conference endpoints in the conference call.

The computer system also includes a disk controller coupled to the bus to control one or more storage devices for storing information and instructions such as a magnetic hard disk and a removable media drive e.g. floppy disk drive read only compact disc drive read write compact disc drive compact disc jukebox tape drive and removable magneto optical drive . The storage devices may be added to the computer system using an appropriate device interface e.g. small computer system interface SCSI integrated device electronics IDE enhanced IDE E IDE direct memory access DMA or ultra DMA .

The computer system may also include special purpose logic devices e.g. application specific integrated circuits ASICs or configurable logic devices e.g. simple programmable logic devices SPLDs complex programmable logic devices CPLDs and field programmable gate arrays FPGAs .

The computer system may also include a display controller coupled to the bus to control a display such as a cathode ray tube CRT or LCD display for displaying information to a computer user. The computer system includes input devices such as a keyboard and a pointing device for interacting with a computer user and providing information to the processor . The pointing device for example may be a mouse a trackball or a pointing stick for communicating direction information and command selections to the processor and for controlling cursor movement on the display . In addition a printer may provide printed listings of data stored and or generated by the computer system .

The computer system performs a portion or all of the processing steps in an embodiment of the invention in response to the processor executing one or more sequences of one or more instructions contained in a memory such as the main memory . Such instructions may be read into the main memory from another computer readable medium such as a hard disk or a removable media drive . One or more processors in a multi processing arrangement may also be employed to execute the sequences of instructions contained in main memory . In alternative embodiments hard wired circuitry may be used in place of or in combination with software instructions. Thus embodiments are not limited to any specific combination of hardware circuitry and software.

As stated above the computer system includes at least one computer readable medium or memory for holding instructions programmed according to the teachings of the invention and for containing data structures tables records or other data described herein. Examples of computer readable storage media are compact discs hard disks floppy disks tape magneto optical disks PROMs EPROM EEPROM flash EPROM DRAM SRAM SDRAM or any other magnetic medium compact discs e.g. CD ROM or any other optical medium punch cards paper tape or other physical medium with patterns of holes.

Stored on any one or on a combination of computer readable storage media the embodiments of the present invention include software for controlling the computer system for driving a device or devices for implementing the invention and for enabling the computer system to interact with a human user. Such software may include but is not limited to device drivers operating systems development tools and applications software.

The computer code devices of the present invention may be any interpretable or executable code mechanism including but not limited to scripts interpretable programs dynamic link libraries DLLs Java classes and complete executable programs. Moreover parts of the processing of the present invention may be distributed for better performance reliability and or cost.

The term computer readable storage medium as used herein refers to any physical medium that participates in providing instructions to the processor for execution. A computer readable storage medium may take many forms including but not limited to non volatile media and volatile media. Non volatile media includes for example optical magnetic disks and magneto optical disks such as the hard disk or the removable media drive . Volatile media includes dynamic memory such as the main memory .

Various forms of computer readable storage media may be involved in carrying out one or more sequences of one or more instructions to processor for execution. For example the instructions may initially be carried on a magnetic disk of a remote computer. The remote computer can load the instructions for implementing all or a portion of the present invention remotely into a dynamic memory and send the instructions over a telephone line using a modem. A modem local to the computer system may receive the data on the telephone line and use an infrared transmitter to convert the data to an infrared signal. An infrared detector coupled to the bus can receive the data carried in the infrared signal and place the data on the bus . The bus carries the data to the main memory from which the processor retrieves and executes the instructions. The instructions received by the main memory may optionally be stored on storage device or either before or after execution by processor .

The computer system also includes a communication interface coupled to the bus . The communication interface provides a two way data communication coupling to a network link that is connected to for example a local area network LAN or to another communications network such as the Internet. For example the communication interface may be a network interface card to attach to any packet switched LAN. As another example the communication interface may be an asymmetrical digital subscriber line ADSL card an integrated services digital network ISDN card or a modem to provide a data communication connection to a corresponding type of communications line. Wireless links may also be implemented. In any such implementation the communication interface sends and receives electrical electromagnetic or optical signals that carry digital data streams representing various types of information.

The network link typically provides data communication through one or more networks to other data devices. For example the network link may provide a connection to another computer through a local network e.g. a LAN or through equipment operated by a service provider which provides communication services through a communications network . The local network and the communications network use for example electrical electromagnetic or optical signals that carry digital data streams and the associated physical layer e.g. CAT 5 cable coaxial cable optical fiber etc. . The signals through the various networks and the signals on the network link and through the communication interface which carry the digital data to and from the computer system may be implemented in baseband signals or carrier wave based signals. The baseband signals convey the digital data as unmodulated electrical pulses that are descriptive of a stream of digital data bits where the term bits is to be construed broadly to mean symbol where each symbol conveys at least one or more information bits. The digital data may also be used to modulate a carrier wave such as with amplitude phase and or frequency shift keyed signals that are propagated over a conductive media or transmitted as electromagnetic waves through a propagation medium. Thus the digital data may be sent as unmodulated baseband data through a wired communication channel and or sent within a predetermined frequency band different than baseband by modulating a carrier wave. The computer system can transmit and receive data including program code through the network s and the network link and the communication interface . Moreover the network link may provide a connection through a LAN to a mobile device such as a personal digital assistant PDA laptop computer or cellular telephone.

Numerous modifications and variations of the present invention are possible in light of the above teachings. It is therefore to be understood that within the scope of the appended claims the invention may be practiced otherwise than as specifically described herein.

