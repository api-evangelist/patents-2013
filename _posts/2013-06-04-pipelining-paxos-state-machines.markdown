---

title: Pipelining Paxos state machines
abstract: Paxos transactions are pipelined in a distributed database formed by a plurality of replica servers. A leader server is selected by consensus of the replicas, and receives a lock on leadership for an epoch. The leader gets Paxos log numbers for the current epoch, which are greater than the numbers allocated in previous epochs. The leader receives database write requests, and assigns a Paxos number to each request. The leader constructs a proposed transaction for each request, which includes the assigned Paxos number and incorporates the request. The leader transmits the proposed transactions to the replicas. Two or more write requests that access distinct objects in the database can proceed simultaneously. The leader commits a proposed transaction to the database after receiving a plurality of confirmations for the proposed transaction from the replicas. After all the Paxos numbers have been assigned, inter-epoch tasks are performed before beginning a subsequent epoch.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09230000&OS=09230000&RS=09230000
owner: GOOGLE INC.
number: 09230000
owner_city: Mountain View
owner_country: US
publication_date: 20130604
---
This application claims priority to U.S. Provisional Application Ser. No. 61 655 430 filed Jun. 4 2012 entitled Pipelining Paxos State Machines which is incorporated by reference herein in its entirety.

The disclosed embodiments relate generally to database management systems and more specifically to reducing latency for database access in a distributed database.

For large scale databases a key feature is fault tolerance. Loss of access to data for a day an hour or even a few minutes may not be acceptable. For example an email user whose data is stored at a distant server expects access to the email at all times.

One way to address fault tolerance is to maintain multiple copies of data at distinct geographic locations. This is commonly called a distributed database. Even if one copy of the data is unavailable e.g. due to a power outage or hardware failure one of the other replicas can seamlessly provide the data. However maintaining multiple copies of data at distinct sites introduces the potential problem of having different data at different replicas.

One technique introduced to keep multiple databases synchronized is the Paxos consensus algorithm. This methodology can successfully keep replicas synchronized under most typical scenarios but is slower than desired especially for large scale databases. One reason for the slow speed is that requests to write to the database within each Paxos group are single threaded.

Disclosed embodiments improve performance of database writes by processing multiple writes in parallel. This is sometimes referred to as pipelining because there can be multiple write transactions in the pipeline at the same time. To enable pipelining of writes disclosed embodiments modify the basic Paxos algorithm in multiple ways. First writes to the database are grouped into epochs with a certain number of writes processed in each epoch. For example some embodiments process write transactions per epoch. Second a single database server or group of servers is designated as the leader for each epoch and the leader holds a lock on leaderhood for the duration of the epoch. In this way only the one leader proposes writes during the epoch so the database management system does not have to address conflicting write requests from multiple independent servers. The leader is able to prevent proposing conflicting writes based on granular tracking of what database objects each transaction will modify. For example if two write requests intend to update the same column of the same row there is an inherent conflict so the two write requests cannot proceed in parallel. However if two write requests do not have an inherent conflict the leader can create write transactions for the requests that proceed in parallel.

Epochs provide a checkpoint for transactions. Prior to beginning a new epoch all of the transactions in the previous epoch must be resolved. For example each outstanding write transaction is either already applied to the database or is applied to the database now or is killed. In the absence of a failure at the leader all of the write transactions are generally committed. This process essentially serializes epochs rather than serializing individual transactions within a Paxos group. There is freedom to parallelize transactions within an epoch but all transactions within an epoch must be committed before proceeding to the next epoch.

Epochs provide additional benefits. Another benefit of epochs is that it places an upper bound on unresolved transactions. This can be particularly important if the leader becomes unavailable e.g. hardware failure power outage network outage at or near the leader etc. . When this occurs a new leader must be selected and the new leader must resolve all of the transactions proposed by the previous leader. Because of the parallel processing of transactions there can be many proposed transactions that are uncommitted. The new leader resolves each of the outstanding proposed transactions committing the transactions that already have a quorum of confirmations i.e. acceptances by the replicas that the proposed transaction is okay reproposing some proposed transactions and replacing some proposed operations with NO OPs no operation . Because all transactions for each epoch are resolved at the end of an epoch a new leader after a failure only needs to review the transactions for the current epoch.

At an epoch boundary the database management system can also change the size of the next epoch and or elect a new leader.

Pipelining is different from batching or boxcarring transactions. Pipelined transactions process independently and in parallel. Batching on the other hand groups multiple write requests together essentially as a composite transaction. Batching generally reduces the overhead of processing individual transactions thereby increasing throughput but can actually increase latency by holding some transactions until the batch is full. Pipelining on the other hand does not increase throughput but decreases latency because write requests do not spend time waiting unnecessarily. Although the present disclosure addresses pipelining some embodiments implement pipelining in conjunction with batching to achieve higher throughput and lower latency.

While pipelining increases the parallelism between distinct write transactions disclosed embodiments also increase the parallelism between database reads and writes by maintaining more granular data on when objects were last modified. Each item e.g. column in a multiversion database is versioned and stored at a server assigned timestamp. Old versions of an item can be read but are typically subject to garbage collection limits. Timestamps are causally consistent so that reads of data at old timestamps are guaranteed to reflect a causally consistent view of the database. For example if transaction T completes before transaction T starts then the timestamp of T must be less than the timestamp of T even if the transactions are on separate machines and do not overlap in terms of the data they access. Moreover transaction T is guaranteed to see the effects of T and any transaction that sees T will also see T. A read transaction with a timestamp T between T and T will see the effects of T but not T regardless of when the transaction T is issued. In other words two reads of the same data with the same specified timestamp will return the same data.

When a client reads data from a multiversion database the read can either specify a timestamp or allow the database management system to select the read timestamp within a specified bound on staleness. Selecting a timestamp within a staleness bound requires locking and or blocking in order to prevent ambiguous staleness calculations.

Multiversion databases enable the calculation of a read timestamp by tracking the last time any change was made to a row of data. However when the database tracks only the last time each row was modified the algorithm for selecting a read timestamp locks the row which will conflict with other operations on the same row even if the other operation is not accessing the same columns or objects within the row. This methodology artificially limits concurrent access to a single row.

Rather than keeping a per row or per shard record of last timestamp written disclosed embodiments keep track of the last timestamp written for each object e.g. column or field within a row . Because of this detail read timestamps can be selected and reads can proceed without being blocked by concurrent writes as long as the ongoing write does not have locks on the specific objects required by the read operation.

In large scale computer systems a single server computer is typically unable to handle all of the received requests e.g. requests for web pages requests for data etc. . Therefore arrays of server computers are networked together typically with load balancing software or a dedicated load balancing server computer to process all of the received requests. The array may include 10 50 100 or 1000 server computers as needed to accommodate the volume of requests. From the viewpoint of a client computer the array appears to be a single server. Accordingly as used in this disclosure and the corresponding claims the term server should be read to mean one or more server computers operating jointly or in parallel to perform related tasks.

In accordance with some embodiments a computer implemented method executes at a plurality of replica servers at a plurality of distinct geographic locations each replica server having one or more processors and memory. The memory stores one or more programs for execution by the one or more processors. The method pipelines Paxos transactions in a distributed database formed by the plurality of replica servers. The method selects a leader server from the plurality of replica servers based on a consensus of the replica servers. The method allocates a finite sequence of Paxos log numbers for a current epoch. The lowest log number in the finite sequence is greater than a highest log number allocated in previous epochs. The method establishes a leadership lock assigned to the leader server for the current epoch. The leadership lock precludes any non leader replica server from constructing proposed write transactions. The leader server receives a first database write request and assigns a first Paxos log number to the first database write request. The first Paxos log number is selected from the finite sequence of Paxos log numbers for the current epoch. The leader server constructs a first proposed write transaction that includes the first Paxos log number and incorporates the first received write request and transmits the first proposed write transaction to at least a plurality of the non leader replica servers. The leader server also receives a second database write request. The second write request and the first write request access distinct objects in the distributed database. The leader server assigns a second Paxos log number to the second database write request. The second Paxos log number is selected from the finite sequence of Paxos log numbers for the current epoch and is distinct from the first Paxos log number. The leader server constructs a second proposed write transaction that includes the second Paxos log number and incorporates the second received write request then transmits the second proposed write transaction to at least a plurality of the non leader replica servers prior to committing the first proposed write transaction. The leader server commits the first and second proposed write transactions after receiving a plurality of confirmations for the first and second proposed write transactions from the replica servers. After all of the finite sequence of Paxos log numbers have been assigned to Paxos write transactions the method performs one or more inter epoch tasks prior to allocating additional Paxos log numbers for a subsequent epoch.

In accordance with some embodiments a database management system comprises a plurality of replica servers at a plurality of distinct geographic locations each replica server having one or more processors and memory. The memory stores one or more programs for execution by the one or more processors. The database management system pipelines Paxos transactions in a distributed database formed by the plurality of replica servers. The database management system selects a leader server from the plurality of replica servers based on a consensus of the replica servers. The database management system allocates a finite sequence of Paxos log numbers for a current epoch. The lowest log number in the finite sequence is greater than a highest log number allocated in previous epochs. The database management system establishes a leadership lock assigned to the leader server for the current epoch. The leadership lock precludes any non leader replica server from constructing proposed write transactions. The leader server receives a first database write request and assigns a first Paxos log number to the first database write request. The first Paxos log number is selected from the finite sequence of Paxos log numbers for the current epoch. The leader server constructs a first proposed write transaction that includes the first Paxos log number and incorporates the first received write request and transmits the first proposed write transaction to at least a plurality of the non leader replica servers. The leader server also receives a second database write request. The second write request and the first write request access distinct objects in the distributed database. The leader server assigns a second Paxos log number to the second database write request. The second Paxos log number is selected from the finite sequence of Paxos log numbers for the current epoch and is distinct from the first Paxos log number. The leader server constructs a second proposed write transaction that includes the second Paxos log number and incorporates the second received write request then transmits the second proposed write transaction to at least a plurality of the non leader replica servers prior to committing the first proposed write transaction. The leader server commits the first and second proposed write transactions after receiving a plurality of confirmations for the first and second proposed write transactions from the replica servers. After all of the finite sequence of Paxos log numbers have been assigned to Paxos write transactions the database management system performs one or more inter epoch tasks prior to allocating additional Paxos log numbers for a subsequent epoch.

In accordance with some embodiments a computer readable storage medium stores one or more programs configured for execution by a plurality of replica servers at a plurality of distinct geographic locations each replica server having one or more processors and memory storing one or more programs for execution by the one or more processors. The one or more programs include instructions for pipelining Paxos transactions in a distributed database formed by the plurality of replica servers. The programs are configured to select a leader server from the plurality of replica servers based on a consensus of the replica servers. The programs are configured to allocate a finite sequence of Paxos log numbers for a current epoch. The lowest log number in the finite sequence is greater than a highest log number allocated in previous epochs. The programs are configured to establish a leadership lock assigned to the leader server for the current epoch. The leadership lock precludes any non leader replica server from constructing proposed write transactions. The leader server receives a first database write request and assigns a first Paxos log number to the first database write request. The first Paxos log number is selected from the finite sequence of Paxos log numbers for the current epoch. The leader server constructs a first proposed write transaction that includes the first Paxos log number and incorporates the first received write request and transmits the first proposed write transaction to at least a plurality of the non leader replica servers. The leader server also receives a second database write request. The second write request and the first write request access distinct objects in the distributed database. The leader server assigns a second Paxos log number to the second database write request. The second Paxos log number is selected from the finite sequence of Paxos log numbers for the current epoch and is distinct from the first Paxos log number. The leader server constructs a second proposed write transaction that includes the second Paxos log number and incorporates the second received write request then transmits the second proposed write transaction to at least a plurality of the non leader replica servers prior to committing the first proposed write transaction. The leader server commits the first and second proposed write transactions after receiving a plurality of confirmations for the first and second proposed write transactions from the replica servers. The programs are configured to perform one or more inter epoch tasks after all of the finite sequence of Paxos log numbers have been assigned to Paxos write transactions and prior to allocating additional Paxos log numbers for a subsequent epoch.

In accordance with some embodiments a computer implemented method executes at one or more server systems each with one or more processors and memory. The memory stores one or more programs for execution by the one or more processors. The programs include instructions for reading and writing data from a database. The method includes creating a database table that has a plurality of rows. Each row has a primary key and a plurality of non key columns. Each non key column has one or more column values each having an associated timestamp. The timestamp associated with a column value identifies when the column value was stored. Accordingly the timestamps associated with the column values in each non key column provide a unique order for the column values that comprise the non key column. The method includes initiating a write transaction to update a first row in the database table which includes placing a lock on a first non key column of the first row. Prior to completion of the write transaction the method initiates a read transaction to read from a second non key column of the first row in the database table. The second non key column is distinct from the first non key column. The first transaction and the second transaction execute at the same time operating on distinct columns of the same row. Completing the write transaction includes selecting a first timestamp that is greater than the timestamps associated with existing column values for the first non key column of the first row and inserting a new column value into the first non key column of the first row of the database table. The new column value is associated with the first timestamp.

In accordance with some embodiments a database management system includes one or more server systems each with one or more processors and memory. The memory stores one or more programs for execution by the one or more processors. The programs include instructions for reading and writing data from a database. The database management system creates a database table that has a plurality of rows. Each row has a primary key and a plurality of non key columns. Each non key column has one or more column values each having an associated timestamp. The timestamp associated with a column value identifies when the column value was stored. Accordingly the timestamps associated with the column values in each non key column provide a unique order for the column values that comprise the non key column. The database management system initiates a write transaction to update a first row in the database table which includes placing a lock on a first non key column of the first row. Prior to completion of the write transaction the database management system initiates a read transaction to read from a second non key column of the first row in the database table. The second non key column is distinct from the first non key column. The first transaction and the second transaction execute at the same time operating on distinct columns of the same row. The database management system completes the write transaction by selecting a first timestamp that is greater than the timestamps associated with existing column values for the first non key column of the first row and inserting a new column value into the first non key column of the first row of the database table. The new column value is associated with the first timestamp.

In accordance with some embodiments a computer readable storage medium stores one or more programs configured for execution by one or more server computers in a database management system each server computer having one or more processors and memory storing one or more programs for execution by the one or more processors the one or more programs comprising instructions for reading and writing data from a database. The computer readable storage medium includes instructions to create a database table that has a plurality of rows. Each row has a primary key and a plurality of non key columns. Each non key column has one or more column values each having an associated timestamp. The timestamp associated with a column value identifies when the column value was stored. Accordingly the timestamps associated with the column values in each non key column provide a unique order for the column values that comprise the non key column. The computer readable storage medium includes instructions to initiate a write transaction to update a first row in the database table which includes placing a lock on a first non key column of the first row. The computer readable storage medium includes instructions that are configured to execute prior to completion of the write transaction which initiate a read transaction to read from a second non key column of the first row in the database table. The second non key column is distinct from the first non key column. The instructions included in the computer readable storage medium are configured to execute the first transaction and the second transaction at the same time operating on distinct columns of the same row. The computer readable storage medium includes instructions to complete the write transaction by selecting a first timestamp that is greater than the timestamps associated with existing column values for the first non key column of the first row and inserting a new column value into the first non key column of the first row of the database table. The new column value is associated with the first timestamp.

Disclosed embodiments thus increase concurrency between multiple writes to a database as well as concurrency between reads and writes.

Other aspects of this disclosure may be advantageous for generating globally synchronized timestamps without incurring various types of network uncertainty inherent in explicit synchronization. The globally synchronized timestamps can be used by various services e.g. to validate local timestamps and clocks or to provide causality respecting timestamps for database updates. By introducing techniques to track calculate and record time data relative to a number of reliable time references an easy to manage and low cost time base may be provided for host machines in a distributed system.

One aspect of the disclosure provides a method that includes receiving an initial local timestamp receiving reference timestamps transmitted from a plurality of time references determining transmission delays associated with the time references and calculating using a processor time offset intervals corresponding to the time references. Each time offset interval includes a set of time offsets that represent differences between a given reference timestamp and the initial local timestamp. In this regard the set of time offsets compensate for the transmission delay associated with the corresponding time reference. The method also includes selecting at least one of the time offset intervals based on an occurrence of that time offset interval among the calculated time offset intervals. In one example the selected time offset interval has a highest occurrence among the calculated time offset intervals. In another example determining transmission delays includes estimating an amount of time associated with receiving reference timestamps from a given time reference. The method may also include determining a timing error in local timestamps generated based on a local clock by comparing the local timestamps to the reference timestamps and adjusting the local timestamps with the time offsets from the selected time offset interval. In one example adjusting the local timestamps includes waiting for a period of time to pass.

Another aspect of the disclosure provides a system that includes a memory a local clock a receiver module adapted to receive reference timestamps from a plurality of time references and a processor coupled to the memory. The processor may be configured to receive an initial local timestamp based on the local clock register in memory reference timestamps received by the receiver module determine transmission delays associated with the time references and calculate time offset intervals corresponding to the time references. Each time offset interval includes a set of time offsets that represent differences between a given reference timestamp and the initial local timestamp. In this regard the set of time offsets compensates for the transmission delay associated with the corresponding time reference. The processor is also configured to select at least one of the time offset intervals based on an occurrence of that time offset interval among the calculated time offset intervals.

Yet another aspect of the disclosure provides a tangible computer readable storage medium that includes instructions that when executed by a processor cause the processor to perform a method. The method includes receiving an initial local timestamp receiving reference timestamps transmitted from a plurality of time references determining transmission delays associated with the time references and calculating using a processor time offset intervals corresponding to the time references. Each time offset interval includes a set of time offsets that represent differences between a given reference timestamp and the initial local timestamp. In this regard the set of time offsets compensates for the transmission delay associated with the corresponding time reference. The method also includes selecting at least one of the time offset intervals based on an occurrence of that time offset interval among the calculated time offset intervals.

Reference will now be made in detail to embodiments examples of which are illustrated in the accompanying drawings.

In this disclosure systems and methods are provided for generating globally coherent timestamps. This technology may allow distributed systems to causally order transactions without incurring various types of communication delays inherent in explicit synchronization. By globally deploying a number of time masters that are based on various types of time references e.g. Global Positioning Systems GPS and atomic clocks the masters may serve as primary time references for the distributed systems. Techniques described herein may be used to request track calculate and record data relative to each time master. This data can be cross checked against a number of time masters in order to assess which time masters may be malfunctioning and which time masters may provide validated causal timestamps to clients.

A server may contain a processor memory and other components typically present in general purpose computers. The memory may store information accessible by the processor including instructions that may be executed by the processor . Memory may also include data that can be retrieved manipulated or stored by the processor . The memory may be a type of non transitory computer readable medium capable of storing information accessible by the processor such as a hard drive memory card ROM RAM DVD CD ROM write capable and read only memories. The processor may be a well known processor such as processors from Intel Corporation or Advanced Micro Devices. Alternatively the processor may be a dedicated controller such as an ASIC.

The instructions may be a set of instructions executed directly such as machine code or indirectly such as scripts by the processor . In this regard the terms instructions steps and programs may be used interchangeably herein. The instructions may be stored in object code format for direct processing by the processor or other types of computer language including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance. Functions methods and routines of the instructions are explained in more detail below.

The data may be retrieved stored or modified by the processor in accordance with the instructions . For instance although the system and method is not limited by a particular data structure the data may be stored in computer registers in a relational database as a table having a plurality of different fields and records or XML documents. The data may also be formatted in a computer readable format such as but not limited to binary values ASCII or Unicode. Moreover the data may comprise information sufficient to identify relevant information such as numbers descriptive text proprietary codes pointers references to data stored in other memories including other network locations or information that is used by a function to calculate relevant data.

Although functionally illustrates the processor and memory as being within the same block it will be understood by those of ordinary skill in the art that the processor and memory may actually comprise multiple processors and memories that may or may not be stored within the same physical housing. For example some of the instructions and data may be stored on a removable CD ROM and others within a read only computer chip. Some or all of the instructions and data may be stored in a location physically remote from yet still accessible by the processor . Similarly the processor may actually comprise a collection of processors which may or may not operate in parallel.

As shown in servers and may also include a time master module . The time master module may be operable in conjunction with a receiver for receiving time signals. Time masters may serve as accurate time references in a distributed system for 1 synchronizing machines and devices and 2 validating and monitoring that synchronization. This may include serving a reference time to a core network of machines and their supporting infrastructure. For example an incoming time query from a client may receive a timestamp determined by the time master . The timestamps may be based on various types of systems known in the arts for providing an accurate and reliable time reference e.g. a GPS system .

According to aspects servers and may consist of middleware software that may manage and integrate the time master module with internal components of the servers and . In one aspect the middleware may consist of a set of services that allow multiple processes running from the time master module to interact with the processor memory instructions and data . In some aspects it may be neither desirable nor possible to run other types of NTP services on a host server in conjunction with a time master module . For example system conflicts can occur over ports assigned to the NTP service severely impacting a time master s reliability.

Servers and may be at a node of network and capable of directly and indirectly communicating with other nodes of the network . For example the servers and may comprise a web server that may be capable of communicating with client device via network such that it uses the network to transmit information to a client application. Servers and may also comprise a plurality of computers e.g. a load balanced server farm that exchange information with different nodes of a network for the purpose of receiving processing and transmitting data to client devices. In this instance the client computer will typically still be at different nodes of the network than the computers comprising servers and . Although only a few servers are depicted in it should be appreciated that a typical system can include a large number of connected servers with each being at a different node of the network .

Each client may be configured similarly to servers and with a processor memory instructions and data . Each client may be a personal computer intended for use by a person having all the internal components normally found in a personal computer such as a central processing unit CPU an optional display device for example a monitor having a screen a projector a touch screen a small LCD screen a television or another device such as an electrical device that can be operable to display information processed by the processor CD ROM hard drive user input for example a mouse keyboard touch screen or microphone speakers modem and or network interface device telephone cable or otherwise and all of the components used for connecting these elements to one another. Moreover computers in accordance with the systems and methods described herein may comprise devices capable of processing instructions and transmitting data to and from humans and other computers including general purpose computers PDAs network computers lacking local storage capability set top boxes for televisions and other networked devices.

Although the client may comprise a full sized personal computer the systems and methods of this disclosure may also be used in connection with mobile devices capable of wirelessly exchanging data over a network such as the Internet. By way of example only a client may be a wireless enabled PDA tablet PC or a cellular phone capable of sending information via the Internet. The user may input information for example using a small keyboard a keypad or a touch screen.

As shown in the client may include an application interface module . The application interface module may be used to access a service made available by a server such as servers and . The application interface module may be a software module operable in conjunction with several types of operating systems known in the arts. For example the client may be connected to a SQL Structured Query Language database server that may operate in conjunction with the application interface module for saving and retrieving information data. Memory coupled to a client may store data accessed by the application module . The data can also be stored on a removable medium such as a disk tape SD Card or CD ROM which can be connected to client .

Servers and and client may be capable of direct and indirect communication such as over network . For example using an Internet socket a client may connect to a service operating on remote servers and through an Internet protocol suite. Servers and may set up listening sockets that may accept an initiating connection for sending and receiving information.

The network and intervening nodes may comprise various configurations and protocols including the Internet World Wide Web intranets virtual private networks wide area networks local networks private networks using communication protocols proprietary to one or more companies Ethernet WiFi such as 802.11 802.11b g n or other such standards and HTTP and various combinations of the foregoing. Such communication may be facilitated by a device capable of transmitting data to and from other computers such as modems e.g. dial up cable or fiber optic and wireless interfaces.

Although certain advantages are obtained when information is transmitted or received as noted above other aspects of the system and method are not limited to a particular manner of transmission of information. Yet further although some functions may be indicated as taking place on a single server having a single processor various aspects of the system and method may be implemented by a plurality of servers for example communicating information over network .

As previously discussed each host server may be connected to a receiver such as a GPS receiver for receiving time signals. For resilience it may be preferable to employ several varieties of GPS receivers e.g. SPECTRACOM Model TSync PCIe SYMMETRICOM Model bc637PCI V2 and MEINBERG. The GPS receivers may require roof mounted antennas and which may be located on the roof above data centers and . Host servers may be housed in server racks located in the data centers and . As such conduits may be installed to route antenna cables from a host server to the roof top antennas. It may be possible to share one antenna across several receivers. This can be achieved for example with an antenna splitter. However antenna sharing may be undesirable because of the low possibility of an antenna failure.

It is conceivable that a GPS receiver may fail. For example possible scenarios may include latent bugs in GPS receivers software and satellite software upgrades and a global catastrophe. In the case of a GPS failure the ability of a time master to freewheel e.g. run without a GPS time reference for several months may allow for enough time to make alternate arrangements. This type of specially configured time master implemented for emergency situations is described in greater detail with respects to .

As shown in the server may include a Recovery master module which may replace a time master module. In this configuration the Recovery master may be frequency locked to a very stable oscillator instead of for example a GPS system. The stability of the oscillator may determine how long and how accurately a Recovery master can serve time. For example based on empirical data an oven controlled crystal oscillator OCXO may have a short term frequency stability of 50 ppb parts per billion 10 9 with an aging rate of instability at 5 ppb per day and a rubidium oscillator may have a frequency stability of 1 ppb with an aging rate of instability at 0.03 ppb per day. It should be noted that time instability errors can accumulate linearly with the short term stability and quadratically with the aging rate.

According to aspects a Recovery master can be calibrated against a fleet of time masters and over a network . During re calibration all available ordinary time masters may participate. Each time master and may be synched for example to an individual time reference such as a GPS feed. Although only one Recovery master is depicted in it should be appreciated that a typical system can include a number of Recovery masters with each being at a different node of the network . According to aspects re calibrations may be staggered across several Recovery masters to avoid injecting undetected failures e.g. GPS signal failures into multiple Recovery masters.

An initial re calibration interval between a Recovery master and the ordinary time masters and may be short to achieve a relatively rapid approximate synchronization. To achieve an increasingly accurate calibration the interval may be doubled in each subsequent re calibration until it reaches a configured constant e.g. 60 days for Rubidium based Recovery master and 2 days for OXCO based Recovery master . Calibration parameters may be stored in a persistent file so that the parameters survive software restarts and server re boots. For example a calibrations file can be loaded into memory whenever a server starts. The calibrations file may contain a reference number corresponding to time master and and corresponding calibration parameters. The re calibrations may be discarded whenever a frequency reference is disturbed e.g. when a time reference is power cycled .

In addition to the components described above and illustrated in the figures various operations will now be described. It should be understood that the following operations do not have to be performed in the precise order described below. Rather various steps may be handled in a different order or simultaneously. Steps may also be added or omitted unless otherwise stated.

According to aspects to ensure that timestamps computed in a disturbed system are trustworthy a subroutine referred to herein as a time synchronization daemon may be executed. Time synchronization daemons may be employed on one or a number of host machines simultaneously e.g. client and time master host machines . By querying a fleet of time masters to determine a current timestamp the time synchronization daemons may periodically compare the host s machines notion of time against the fleet of time masters. Once the fleet time masters have been queried for the current time the time synchronization daemon may track the round trip query delay associated with querying each time master. In some aspects the time synchronization daemon can calculate synchronization offset intervals that may be relative to a time master e.g. an interval between local time and a time master s notion of current time .

To detect and reject offset intervals derived from possibly malfunctioning time masters e.g. time masters that have failed but are still producing timestamps a voting method may be used such as a variant of Marzullo s algorithm. As a result of the voting method the time synchronization daemon may accept output from validated time masters or reject output from certain time masters for example malfunctioning time masters.

In block an initial local timestamp may be retrieved. For example method may access a registry for storing a host machine s notion of a current date and time. This may reflect time from an internal time clock on the host machine. The timestamps can be encoded in various time formats used to describe instants of time such as Coordinated Universal Time UTC Unix epoch and the unambiguous International Atomic Time epoch TAI .

In block time queries may be sent to a pre determined number of time masters e.g. 5 to 10 time masters . For example an application interface may be utilized to make a connection to a time master for sending and receiving information. In one aspect the time masters may operate at Stratum 1 or Stratum 2. It is also possible for time masters to operate at any arbitrary number such as from 1 . . . K . The basic definition of a Stratum 1 time master is that it may be directly linked e.g. not over a network connection to a reliable source of time such as a GPS receiver. A Stratum 2 time master may be connected to one or more Stratum 1 time masters over for example a network connection. In this example a Stratum 2 time master may get its time via a network request to one or more Stratum 1 time master. In this regard if a Stratum master is fed by a master operating at stratum K or less it may be described as a Stratum K 1 .

In block responses may be received from the time masters queried in block . For example as a result of the time request a time master may generate a timestamp. According to aspects time masters may publish timestamps over a network connection in a distributed system. In one aspect the application interface used to request a timestamp in block may also be used to retrieve timestamp responses. Various other techniques for transmitting data over a network socket can be used to publish and receive timestamp responses e.g. Transmission Control Protocol Internet Protocol TCP IP World Wide Web s Hypertext Transfer Protocol HTTP File Transfer Protocol FTP Telnet Telnet protocols and other types of communication protocols.

In block a received time query response may be associated with a current local timestamp. For example the current local timestamp may denote respective events of reception of the time query response according to a local time scale. In some aspects associating the current local time with the time query response may be accomplished by storing both in memory a data structure or by inserting both in a suitable computer readable medium capable of storing information accessible by a processor.

In block a time offset interval may be calculated for each time masters that was queried in block . The offset interval s width e.g. a length of time duration may represent transmission delays associated with a time request to and from a particular time master. The timestamps from each non malfunctioning time master may correspond to an instant of time somewhere between the initial local timestamp taken in block and the local timestamp associated with a time master s response in block .

A level of uncertainty e.g. transmission delays may also affect the time offset interval calculation. In this regard to calculate the offset interval for each time master the following equations may be used 0 2 0 2

In the above equations D m represents the time offset relative to time master m T m represents the timestamp provided by master m U m represents an uncertainty related to a time master m L m represents the local time captured at the time when a time query response was received from time master m and L 0 represents a local timestamp taken prior to dispatching the time queries to the time masters. Thus with respects to a time master m local time may be in error by an offset D m plus or minus the uncertainty calculation U m or in other words an interval range of D m U m to D m U m .

In block it may be continually tested whether the total number of time query responses has been reached. If the number has been reached then method may proceed to block . Otherwise method may repeat block and receive another time query response.

In block an agreement algorithm may be employed to analyze an aggregate of the time master offset intervals calculated in block . Because time masters may occasionally fail an agreement algorithm e.g. Marzullo s algorithm may determine a smallest time offset interval consistent among a selected group of time masters. In some aspects the agreement algorithm may be employed more than once.

In an initial run of the agreement algorithm a group of local time offsets calculated in block may be selected. The selection may be based on a corresponding time master s configuration such as whether a time master is operating at Stratum 1. Each calculated offset may represent the local clock error relative to a particular time master. A determination may be made by method for a maximal set of intersecting local time offsets from the group. In one example if no two offset intervals intersect method may stop. Method may also stop if more offsets in the group disagree than agree. If at least two offset intervals intersect than an agreement validated offset interval e.g. a smallest interval containing all points lying in the intersection of at least k 1 of the k intervals in the group may be assigned for the group.

In one aspect a level of uncertainty may be reflected in a group of offsets in an interval e.g. the width of the group from smallest to largest offset . This level of uncertainty may be relatively small because typically several time masters may be nearby. Thus the agreement validated offset determined in block should be relatively accurate plus or minus a level of group uncertainty.

According to some aspects the agreement algorithm may be employed a second time. In this iteration of the agreement algorithm local time offsets calculated in block may be selected from a group of time masters configured differently than the first group. For example this selection may be based on time masters not operating at Stratum 1. The offset intervals corresponding to the non Stratum 1 time masters may be clipped against the Stratum 1 offset interval. The second round may be used to steer local clocks when there is no nearby Stratum 1 time master thus improving consistency among nearby hosts.

The second iteration of the agreement algorithm may be then applied to the clipped non Stratum 1 offsets. This second iteration may yield a final validated offset interval which may be accurate plus or minus a level of uncertainty related to the second group of time masters. Typically this final validated offset interval should be within the uncertainty range resulting from the first run of the agreement algorithm. In one example the second iteration results may be discarded if the results are outside of the uncertainty range of the first iteration.

In block local time on a host machine may be disciplined or validated depending on the machine s configuration. For example if the machine employing method is a GPS fed time master the offset interval from block may be used to validate time published by the GED fed master. If the machine is a client host or a Recovery master the offset interval from block may be used to adjust the host machine s local clock. For example if the offset interval is 2 1 the local clock can be somewhere between 1 and 2 seconds ahead of the masters. If the interval is 1 2 the local clock can be 1 to 2 seconds behind the masters. If the interval is 1 1 the local clock can be somewhere between 1 second behind to 1 second ahead of the master.

In this regard a host server s local system clock may be disciplined to help keep precise time. For example small adjustments e.g. the validated offset from block may be applied to the system clock periodically. These adjustments may be applied using various utilities for accessing a system s local clock such as the utility application adjtimex. In one aspect adjtimex may be used in PLL mode phase locked loop . In this example PLL constants and offset clamps values may be chosen to bind the local clock rate error to approximately 1000 ppm. Some applications for example distributed lease protocols may depend on time progressing at a similar rate on each host. For such applications to work properly clock rates need to be controlled.

In block a request for a timestamp may be received. For example a client may open a socket connection to time master s host server to initiate a connection for sending and receiving information. Through this connection the time master may act as a NTP server that transmits timestamps to the client.

In block time signals may be provided from a reliable time reference. For example GPS timestamps may be provided by a GPS disciplined oscillator e.g. a SPECTRACOM PCI Express Model TSync PCIe connected to a host server In some aspects timestamps may be served directly from the system s oscillator. According to aspects this may help avoid several sources of time corruption such as time keeping bugs related to operating systems and Time Stamp Control TSC deficiencies that plague many microprocessors.

In block GPS timestamps may be adjusted based on a leap second. A leap second may be a positive or negative one second adjustment to a time scale that may keep it close to mean solar time. In some aspects leap seconds may be amortized over a period of ten hours on either side of the leap second thus rendering the leap second invisible to a client requesting a timestamp. Several techniques may be used to amortize leap seconds for example they may be linearly amortized over a window of time e.g. 10 hours on either side . A standard leap file disseminated by the National Institute of Standards and Technology NIST may govern leap second insertions.

In block possible system failure conditions may be continually monitored. For example a failure can be a discovered disagreement between the NIST leap file and leap seconds advertised by a connected GPS system. Other possible failures may include when the NIST file is about to expire poor satellite signal reception a shorted antenna cable parity errors etc. If a failure condition is detected method may raise an alert at block . Otherwise it may proceed to block .

In block a failure alert may be raised. Once a failure condition has been detected a time master may block or discard incoming time queries until the alert has been resolved. Typically many failures may require some type of repair to be preformed. For example some alerts may be resolved by making a request to a service for an updated NIST file for adjusting leap seconds. In this example method may optionally repeat block to adjust leap seconds based on the newly provisioned NIST file. Other failures may be resolved by repairing a GPS receiver or other hardware.

In block timestamps may be synchronized with an external reference. For example timestamps from a secondary device such as a High Precision Event Timer HPET attached to a time master host may be used as a sanity check. According to aspects on a server running an Intel processor the HPET may be a fairly reliable timing device. HPET accesses can impact processing time by several microseconds. This however may be an acceptable impact in performance.

In block timestamp spikes may be detected. If a timestamp lies outside of a determined range it may be considered a spike. The spikes may be logged but not transmitted to clients. In one example spikes may be detected by planting guideposts to demark an acceptable range for the timestamps. In this example a guidepost can be correlated GPS and HPET timestamps planted periodically. Each GPS timestamp may be checked against the bounds since the last guidepost was planted. This check may provide a measure of protection against for example GPS system faults and satellite signal spoofing attacks. The spike detection process employed in block is further described with respects to .

As illustrated emanating from the initial guidepost are rays and positioned at angles. A shaded region e.g. the cone of uncertainty between the rays reflects an uncertainty about the precision of the HPET frequency plus or minus a predetermined tolerance level. According to aspects if a subsequent reading of a GPS timestamp intersects the shaded region it may be accepted as a valid timestamp. If it does not intersect the shaded region it may then be declared a spike because it violates the HPET frequency error bounds and .

The guidepost may be advanced at a rate determined by roughly balancing the uncertainty induced by HPET frequency errors the GPS system and HPET read latencies. In one example guideposts are advanced approximately every 100 milliseconds. A new guidepost such as guidepost may be planted when the previous guidepost expires and a new valid GPS timestamp is obtained for verification. The newly obtained GPS timestamp may also become an anchor in the new guidepost. For example plot points and of guidepost are determined by a subsequent pair of GPS and HPET time stamp readings. In one aspect the guidepost may be associated with three timestamps taken in the following order a 3rd HPET timestamp a 2nd GPS timestamp and a 4th HPET timestamp.

According to aspects spikes with high frequency synchronization errors that exceed approximately 20 microseconds may be detected. As well as low frequency wander spikes exceeding the worst case HPET variation. In some aspects it may be desirable to use a number of spike detectors each advancing at a different rate. In this regard a spike detector advancing at a rapid rate may be better at detecting high speed spikes while a detector advancing at a slower rate may be better at detecting gradual frequency shifts in a time master.

According to aspects time masters and may serve as primary time references in a distributed system. As previously discussed the time masters internal clocks may be synchronized with e.g. a GPS signal atomic clock or other types of accurate timekeeping technologies known in the arts. The time masters may be designed for high reliability and may be deployed at multiple sites throughout a distributed system. Although only two time masters are depicted in it should be appreciated that a typical system can include a large number of time masters communicating with each other over e.g. a network connection .

The time synchronization daemon may periodically query multiple time masters and for a current timestamp. The daemons may calculate synchronization offset intervals relative to each time master and by tracking round trip delays related to a time query. Sets of time master offsets may be validated against each other using e.g. clock synchronizing techniques as described with respects to . As previously discussed the clock synchronizing techniques may employ an agreement algorithm to detect and reject offsets from malfunctioning time masters. An intersection of surviving offset intervals may be used to determine a client s local time synchronization error which may have occurred at the time a query was made to a time master and . Accordingly the client s local clock rate may be updated based on the surviving offset intervals.

A TrueTime library interface may be employed to transmit validated timestamps requested by client applications. For example a client may use an application programming interface API that may be operable in conjunction with the TrueTime library in order to communicate with modules of system . The TrueTime library may also calculate a local time offset interval e.g. a difference between local time and a time reference based on such factors as a local clock known rate errors related to the local clock and an offset interval determined by the time synchronization daemon at the last time the time masters and were polled.

In some instances it may be difficult to determine an order of events in a distributed system because system clocks are not always synchronized. However event ordering can be determined using timestamps generated with the TrueTime library rather than using a machine local approximation of current time. In one aspect timestamps determined by employing the TrueTime library can be used as a basis for making causally dependant information available to host clients in a distributed system. For example this information can be used by an application associated with a host client to causally order event transactions.

In block an initial local timestamp may be retrieved. For example a host client s notion of a current date and time may be accessed and stored. This may reflect time from an internal time clock memory registry or other means of tracking time on the host machine.

In block a time interval may be determined e.g. a difference between the local timestamp and a time reference . For example a software module operable in conjunction with a TrueTime library interface may request a time interval provided by the TrueTime library. As previously discussed the TrueTime library may provide a time interval interface to host clients requiring meaningful globally coherent timestamps. The TrueTime library may determine the time interval from a time synchronization daemon employed locally on a host client. As described with respect to host clients may employ a time synchronization daemon which may track a correlation between local clocks and causal time by computing uncertainty bounds on that correlation. The time synchronization daemon may derive a causal time by querying a number of time masters deployed throughout a distributed system. In some aspects the time synchronization daemon may determine a time interval consistent among a selected group of time masters by employing an agreement algorithm on the selected group.

In block a timestamp from the time interval may be recorded. For example the latest timestamp in the time interval determined in block may be selected. According to aspects this timestamp may be larger than the timestamp of other causal predecessors. In one aspect the timestamp may be recorded and maintained in a predetermined format. For example the timestamp can be encoded in a time format used to describe instants of time such as UTC. Consistent time formatting may for example allow for comparison of the timestamp with local timestamps from one or more different systems.

In block a period of time may pass after a timestamp is recorded from the time interval. The time period for waiting may be chosen so that after the waiting period the time of recorded timestamp has passed. According to aspects waiting may preserve the causal order of events by ensuring that causal successors e.g. logically ordered transactions will be assigned respectively larger timestamps. For example if the current time is between 3 50 PM and 4 00 PM and a 10 minute waiting period is chosen then after the waiting period it will be later than 4 00 PM and thus any future chosen timestamps will be later than 4 00 PM. It will be appreciated that in some aspects other types of client computations can occur in block while the time period is passing.

In block the recorded timestamp may be provided. For example a programming interface may be used by a host client to access the recorded timestamp from block . The timestamp may also be returned as a result of a call to a computer program library class function or other types of programming techniques known in the arts. The host client may then associate the timestamp e.g. with host client events thereby ensuring a causal order of event transactions for services accessed at the client.

The above described aspects of the technology may be advantageous for generating globally synchronized timestamps without incurring various types of network uncertainty inherent in explicit synchronization. The globally synchronized timestamps can be used by various services e.g. to validate local timestamps and clocks or to provide causality respecting timestamps for database updates. By introducing techniques to track calculate and record time data relative to a number of reliable time references an easy to manage and low cost time base may be provided for host machines in a distributed system. Moreover the various techniques and parameters disclosed within may be further reconfigured to increase overall timestamp accuracy.

As noted above in large scale computer systems a single server computer is typically unable to handle all of the incoming requests so arrays of server computer are networked together to form a single virtual server. The array may include many server computers as needed to accommodate the volume of requests. Because the array appears as a single server it is convenient to refer to such an array as a server. This disclosure and the corresponding claims thus use the term server broadly to mean one or more server computers operating jointly or in parallel to perform related tasks.

Replica includes database server s and data store similar to replica and replica includes database server s that access data store again like replica . In some embodiments one or more of the replicas or is designated as read only. A read only replica is kept synchronized with the other replicas but cannot be designated to respond to client requests to write data only client read requests .

As illustrated in client computers and can access the data from the database management system by sending the requests over a communications network such as the Internet. The requests are sent from a software application executing on a client computer or which may be a web application that runs in a web browser . The client requests to read or write data are received by a front end server which directs the requests to an appropriate replica of the database. In some embodiments the front end server includes a load balancing module to spread out the incoming requests among the replicas. Although illustrates only a single front end server many embodiments include a plurality of front end servers such as 10 20 or 50 servers.

Referring to a database server generally includes one or more processing units CPUs one or more network or other communications interfaces memory and one or more communication buses for interconnecting these components. The communication buses may include circuitry sometimes called a chipset that interconnects and controls communications between system components. A database server may optionally include a user interface for instance a display and a keyboard . Memory may include high speed random access memory such as DRAM SRAM DDR RAM or other random access solid state memory devices and may include non volatile memory such as one or more magnetic disk storage devices optical disk storage devices flash memory devices or other non volatile solid state storage devices. Memory may include mass storage that is remotely located from the central processing unit s . Memory or alternately the non volatile memory device s within memory comprises a computer readable storage medium. In some embodiments memory or the computer readable storage medium of memory stores the following programs modules and data structures or a subset thereof 

In many embodiments there are multiple database servers such as 100 or 1000 each accessing data from the data store . The database servers and illustrated in are similar to database server .

Referring to a front end server generally includes one or more processing units CPUs one or more network or other communications interfaces memory and one or more communication buses for interconnecting these components. The communication buses may include circuitry sometimes called a chipset that interconnects and controls communications between system components. A front end server may optionally include a user interface for instance a display and a keyboard . Memory may include high speed random access memory such as DRAM SRAM DDR RAM or other random access solid state memory devices and may include non volatile memory such as one or more magnetic disk storage devices optical disk storage devices flash memory devices or other non volatile solid state storage devices. Memory may include mass storage that is remotely located from the central processing unit s . Memory or alternately the non volatile memory device s within memory comprises a computer readable storage medium. In some embodiments memory or the computer readable storage medium of memory stores the following programs modules and data structures or a subset thereof 

Although A and B illustrate various client and server computers these figures are intended more as functional illustrations of the various features that may be present in a single computer or set of servers rather than a structural schematic of the embodiments described herein. In practice and as recognized by those of ordinary skill in the art items shown separately could be combined and some items could be separated. For example some items shown separately in such as database server and data store could be implemented on a single server and single items could be implemented by one or more servers. The actual number of servers used to implement a database management system and how features are allocated among them will vary from one embodiment to another and may depend in part on the amount of data traffic that the system must handle during peak usage periods as well as during average usage periods.

Each of the methods described herein may be performed by instructions that are stored on a computer readable storage medium. The instructions are executed by one or more processors of one or more servers or clients. Each of the operations shown in A and B may correspond to instructions stored in a computer memory or computer readable storage medium.

Unlike an ordinary SQL database an individual column of an individual row in a multiversion database comprises a set of values rather than a single value. For example in the Address column of the row comprises a set of values . This set of values is illustrated in . Each Address Value has an associated Address Timestamp which specifies when the Address Value was written to the database table . In the illustrated embodiment timestamps are stored with an accuracy of 1 10000 of a second but other embodiments have greater or lesser precision. illustrates that the customer with Customer ID has had three addresses beginning with 123 Main St. as of Jan. 2 2009 at about 1 12 PM . This timestamp indicates when the address was saved in the database table not when the customer actually began living at 123 Main Street . The timestamps are not user entered effective dates that appear in some software applications. 

Having multiple versions of data allows a read in the past that sees the data as it was at that time. For example a read of row for customer 322257018 at a read timestamp of Jan. 1 2010 will return the Address Value 123 Main Street whereas a read of the same row at a read timestamp of Jan. 1 2012 will return 2388 First St. 12 . Of course an actual read timestamp also specifies the time of day but the time of day would not make a difference in the above two examples. Due to storage space limits of the data store old versions of column values are subject to garbage collection based on user defined criteria. For example entries can be subject to removal when the number of entries for a column exceeds a designated threshold or the entries are older than a threshold staleness.

As illustrated by the sequence of three values for the customer addresses for customer 312257018 the timestamps for the values create a unique order for the values. The database management system guarantees that the timestamps for the values are monotonically increasing even when new values are inserted in rapid succession.

Although the Database Management System supports multiple versions for the values in non key columns multiple versions are not required. For example people do not generally change their first names and thus each customer row would typically have only a single value in the First Name column . The timestamp for the single value in this instance would be the timestamp when the row was inserted.

As illustrated in The Orders column has values that are tables. illustrates the subtable of orders for customer 312257018. In this illustration the subtable has only two orders and but there can be any number of orders. The abbreviated subtable illustrated in has four columns including an Order ID as the primary key and three non key columns Order Date Shipping Address and Order Items . Like the root table the values for the non key columns have specified data types which can be number strings dates Boolean values protocol buffers or subtables.

Although Order Date and Shipping Address in the subtable can store multiple values there would generally be a single value for each of these columns. However additional values would be added if the customer corrects an invalid entry. For example the customer might recognize that the shipping address for order QA1997233 was mistyped or used an old address. When the customer corrects the error a new shipping address value would be added but the previous value would still be in the set of values . This also illustrates the point that a user cannot change history. Once a value is saved it will continue to exist in the database until some time later when it is removed by garbage collection.

The order QA1997233 includes a subtable that specifies the Order Items within the order. This illustrates that subtables may be nested inside other subtables. illustrates an abbreviated set of columns to identify the items within subtable . Similar to the root table and subtable the subtable has a primary key Item ID and some non key columns. The non key columns include a Quantity and Price . illustrates a subtable with two rows and but there could be any number of rows. In row the Item ID is 7752 81517 the Quantity is a value in the set of quantity values and the price is a value in the set of price values . In general there would be a single value for the quantity and price but there would be multiple values if the data changed after it was originally saved. E.g. the customer decides to get three of an item rather than two. 

As illustrates the last write timestamp is known for each column in the database table . This granularity enables selection of read and write timestamps without blocking other transactions that access different columns of the same row.

When a database server receives a read request the read is assigned a timestamp that is greater than last write timestamp of the accessed columns and less than MNNWT . This serializes the read after the last write and before the next new write. The read can proceed in parallel with any new writes that are received by the database server because the database maintains multiple versions of column values the read accesses the existing data and any new writes will create new versions of data with timestamps greater than or equal to MNNWT.

When the database server receives a write request the write transaction is assigned a timestamp greater than or equal to MNNWT typically greater than and increases MNNWT to be greater than the assigned timestamp.

When a client computer needs to perform a read at a time that is consistent across multiple database servers the client may specify a read timestamp. As long as that timestamp is less than the MNNWT of each database server the read may safely proceed at each database server and the read results are guaranteed to be stable if the read is repeated at the same timestamp.

In some embodiments there are one or more database replicas e.g. replica that are capable of serving writes but not reads or vice versa. In some of these embodiments the limited replica periodically receives a MNNWT value from another replica. A replica with knowledge of MNNWT may independently serve a read at a timestamp less than MNNWT without having to communicate with other database servers. In the absence of writes a database server periodically increases the value of MNNWT so that replica entities may serve reads at increasing timestamps.

If the read request does not include a read timestamp the database management system selects a read timestamp that is greater than the last write timestamp of the values of the objects to be read and that is less than MNNWT . MNNWT is guaranteed to be greater than all of the timestamps of the previous writes so it is always possible to select a read timestamp meeting these criteria.

Whether the read timestamp is specified by the client or selected by the database management system a database server proceeds to read the desired data at the specified timestamp and returns the data to the client .

The database server processing the write transaction locks the appropriate object and proceeds with the write. This is described in more detail above with respect to . In this illustration the read request is received a moment after the database management system receives the write request. As illustrated in FIG. B a database server selects a read timestamp and verifies that there are no locks on the objects to be read. Then the database server performs the write in parallel with the read and performs the read in parallel with the write. In this illustration the read completes first and provides the read data to the requestor. Afterwards the database server completes the write and confirms the write with the write requestor. In some instances the write request would complete prior to completion of the read.

Each row of the database table comprises a primary key which uniquely identifies the row and a plurality of non key columns. In some embodiments each of the rows of the database table comprises one or more shards. This was described in greater detail above with respect to . In some embodiments the shards for each row form a partition of the non key columns in the respective row. In some embodiments the shards for each row form a partition of the non key columns whose values are not subtables. This is described in greater detail above with respect to .

As described in greater detail above with respect to each non key column has one or more column values. Each column value has an associated timestamp that identifies when the associated column value was stored. This enables the database management system to read data from the database table as it appeared at any point in the past. The timestamps associated with the column values in each non key column provide a unique order for the column values that comprise the non key column. When a new column value is inserted its associated timestamp represents when the new column value is stored and thus the associated timestamp is greater than all of the previous timestamps for the previously existing column values for the same column.

The process initiates a write transaction to update a first row in the database table . In some embodiments a single write transaction can update a plurality of root rows. The write transaction places a lock on a first non key column of the first row. This first non key column represents a column that the write transaction will update.

Prior to completion of the write transaction the process initiates a read transaction to read from a second non key column of the first row in the database table . The second non key column is distinct from the first non key column. Because the timestamps are stored for each individual non key column and the columns accessed by the read and write transactions are distinct they do not block each other. Therefore the first transaction and the second transaction execute at the same time. In embodiments where database tables can be sharded the first and second non key columns can be in the same shard of the first row or can be in different shards of the first row.

Completing the write transaction includes selecting a first timestamp that is greater than the timestamps associated with the existing values for the first non key column of the first row and inserting a new column value into the first non key column of the first row of the database table . The new column value is associated with the first timestamp.

In some instances the read transaction completes before the write transaction completes. In other instances the write transaction completes before the read transaction completes.

In some instances the first row of the root database table includes one or more non key columns that comprise column values that are subtables. Each subtable comprises a plurality of subrows i.e. rows of the subtable . Each subrow comprises a subkey that uniquely identifies the subrow within the subtable and one or more non key subcolumns i.e. a column within the subtable . Each non key subcolumn comprises one or more subcolumn values i.e. values for the subcolumn . Furthermore each subcolumn value has an associated subtimestamp that identifies when the associated subcolumn value was stored in the database. The subtimestamps associated with the subcolumn values in each non key subcolumn provide a unique order for the subcolumn values that comprise the non key subcolumn.

The leader determines whether the write request conflicts with any pending proposals. Conflicts arise when the write request will update an object that is not already committed to the database. E.g. the write request and an existing pending request will update the same column of the same root row. When there is a conflict some embodiments reject the write request informing the client of the rejection. Because the request conflicts with a pending write to the same data the request may have been made based on a version of the data that will no longer be accurate. In other embodiments when there is a conflict the new write request is postponed until the previous write is committed.

If the write request does not conflict with a pending write the leader assigns a Paxos log number see to the write and changes the status of the assigned Paxos log number to Proposed. The leader builds a write proposal that includes the Paxos number and identifies the proposed changes . This is illustrated in below. The leader then transmits the proposal to at least a plurality of the replicas. There must be a plurality of confirmations i.e. acceptances for the proposal before it is committed.

The replicas receive the proposal and confirm the proposal. In general the replicas are in distinct geographical locations so the network latency between the leader and the non leader replicas varies. In addition one or more of non leader replicas may not receive the proposal due to a network outage corruption of the proposal message en route or other failure at the replica site. When a plurality of the replicas leader plus non leader replicas accept the write proposal the write is logically committed. When the leader is notified of the plurality of confirmations the leader commits the write. As illustrated in below some embodiments commit a write transaction by changing the status of the corresponding Paxos number to Committed. Once the proposal is committed the leader applies the write to the database and notifies the non leader replicas that the write has been committed. Each of the non leader replicas then applies the write to its copy of the database.

Within an epoch such as the current epoch Paxos numbers are generally assigned sequentially to write requests as they arrive. Each implementation specifies the number of write transactions that can be pending at a single time which is referred to as the depth of the pipeline. The pipeline depth cannot be greater than the total number of Paxos numbers allocated to an epoch e.g. 500 but is generally a smaller number such as 5 10 or 20 transactions.

Certain processing occurs at the end of each epoch in the inter epoch periods and prior to the beginning of the next epoch. One important activity in an inter epoch period or is to resolve all transactions from the epoch that just ended. In a standard non pipelined Paxos implementation all of the individual transactions are serialized so no new transaction is proposed until the prior transaction is committed. Therefore a leader for a Paxos group would not have more than a single pending transaction for the group and any point in time. When Paxos transactions are pipelined however there can be as many pending transactions as the depth of the pipeline. All of the pending transactions are resolved prior to beginning the next epoch. Resolving the transaction includes committing all proposed transaction for which there is a plurality of confirmations among the replicas. In some embodiments the leader re proposes transactions for which there is not yet a plurality of confirmations. In some embodiments transactions for which a plurality of confirmations has not been received in a predefined amount of time will be replaced with a no op no operation . In some embodiments resolving the transactions for an epoch includes applying all of the committed transactions to the database.

In some inter epoch periods a new election is held to determine a new leader for the upcoming epoch. In some embodiments the existing leader tracks data regarding the health and performance of the replicas during each epoch. Using this data the leader makes a recommendation to hand off leadership to another replica that can process the data more efficiently more timely with greater throughput lower latency or is preferable for various other criteria. Based on the tracked data about replicas the leader may also determine that it should renew its leadership for the next epoch. In some embodiments an election for the new leader is held regardless of the current leader s recommendation but the recommendation may carry some weight in the voting process.

Although illustrates epochs with the same number of write transactions in each epoch this is not required. During an inter epoch period the size of the next epoch can be changed.

During each inter epoch period such as period there will be no more Unassigned Paxos numbers and any transactions that are still in a Proposed state are resolved. In some embodiments Committed transactions are considered resolved because there is no ambiguity that it will be applied to the database. In some embodiments all outstanding transactions from the epoch are Applied to the database prior to beginning the next epoch.

Each of the zones Zone A Zone B Zone C and Zone D is in a geographically distinct location so that an outage is unlikely to affect two distinct zones simultaneously. The tablets A B and C for Group 1 are located in zones A B and C . The leader for Group 1 is at Zone A so write proposals for the shards in Group 1 originate from Zone A . In this illustration Zone A also has the leadership role for Group 2. For Group 2 there is a tablet A in Zone A as well as tablet B in Zone B and tablet D in Zone D . The tablets A C and D are spread across zones A C and D with leadership at Zone C . Finally tablets B C and D are spread across zones B C and D with leadership at Zone B .

The process selects a leader replica from the plurality of replicas for each Paxos group as previously illustrated in . To simply the discussion of process the discussion will address a single Paxos group. The same description applies to pipelining of transactions within each of the Paxos groups.

The process allocates a finite sequence of Paxos log numbers for a current epoch. The Paxos log numbers are also referred to as Paxos numbers or log numbers. In some embodiments the Paxos log numbers are non negative integers. In some embodiments the finite sequence is a contiguous set of 500 integers such as 0 499 or 500 999. The Paxos log numbers for each epoch are greater than the Paxos log numbers used in previous epochs. In particular the lowest log number in the current epoch is greater than the highest log number used in previous epochs.

The process also establishes a leadership lock assigned to the leader replica for the current epoch . The leadership lock precludes any non leader replica from constructing proposed write transactions. This is different from the standard Paxos algorithm in which multiple distinct replicas can propose write transactions which can potentially conflict. In the disclosed embodiments there is a single leader for each Paxos group so the non leader replicas i.e. all of the replicas other than the leader are not attempting to propose their own distinct transactions.

The leader receives a first database write request and assigns a first Paxos log number to the request. The leader selects the first Paxos log number from the finite sequence of Paxos log numbers for the current epoch. In preferred embodiments the selected Paxos log number is the smallest log number in the finite sequence that has not already been assigned to another write request. The leader then constructs a first proposed write transaction that includes the first Paxos log number and incorporates the first received write request. This was illustrated above with respect to .

The leader transmits the first proposed write transaction to at least a plurality of the non leader replicas. In some embodiments the leader transmits the proposal to all of the non leader replicas.

The leader also receives a second database write request. The first and second write requests seek to access distinct objects in the distributed database. For example the two requests may seek to access different root rows or different columns within the same root row. The leader assigns a second Paxos log number to the second database write request. The leader selects the second Paxos log number from the finite sequence of Paxos log numbers for the current epoch . The second Paxos log number is distinct from the first Paxos log number. In preferred embodiments the second Paxos log number is greater than the first Paxos log number when the second write request arrives at the leader after the first write request. In preferred embodiments the second Paxos log number is selected as the smallest unassigned Paxos log number in the finite sequence of log numbers for the current epoch .

The leader constructs a second proposed write transaction which includes the second Paxos log number and incorporates the second received write request. The leader then transmits the second proposed write transaction to at least a plurality of the non leader replicas. In some instances the leader transmits the second proposed write transaction prior to committing the first database write transaction. The leader commits the first and second proposed write transactions after receiving a plurality of confirmations for the first and second proposed write transactions from the replicas. The first and second proposed write transactions are not necessarily committed in the order in which the write requests were received each proposed write transaction is committed after there is a plurality of confirmations for the proposed write transaction. Each write transaction is applied to the database at some point after the transaction is committed.

After of the finite sequence of Paxos log numbers has been assigned to Paxos write transactions the process performs one or more inter epoch tasks prior to beginning the next epoch. In some embodiments an epoch begins by allocating a sequence of Paxos log numbers for the epoch. In these embodiments the inter epoch tasks are performed prior to allocating additional Paxos log numbers for the subsequent epoch.

In some embodiments the one or more inter epoch tasks includes resolving each of the finite sequence of Paxos log numbers for the current epoch. In some embodiments resolving a Paxos log number includes confirming that the respective assigned write transaction has been committed or committing the respective assigned write transaction or designating the Paxos log number as a NO OP no operation .

In some embodiments the one or more inter epoch tasks includes identifying one or more Paxos log numbers assigned to write transactions that have not been committed but for which there is a plurality of confirmations from replica servers. When such Paxos numbers are identified the one or more inter epoch tasks includes committing the write transactions corresponding to the identified Paxos log numbers.

In some embodiments the one or more inter epoch tasks includes electing a new leader from among the replicas and assigning a new finite sequence of Paxos log numbers to the new leader.

In some embodiments the leader receives N database write requests where N is a positive integer greater than 1 and each of the N database write requests seeks to access distinct objects in the distributed database. The relevant scenario here is when the N requests arrive at the leader about the same time so that they are processed in parallel. The leader assigns a unique Paxos log number to each of the N database write requests. Each of the Paxos log numbers is selected from the finite sequence of log numbers for the current epoch that were previously unassigned.

The leader constructs substantially in parallel N proposed write transactions where each of the N proposed write transactions incorporates one of the N received write requests and includes the unique Paxos number assigned to the respective write request. In some embodiments occurring substantially in parallel means that there is a point in time at which all N write requests have been received but none of them have been committed.

The leader transmits substantially in parallel the N proposed write transactions to at least a plurality of the non leader replicas. In some embodiments transmitting substantially in parallel means that all N proposed write transactions are transmitted to the replicas before any of them is committed. Subsequently the leader commits each of the N proposed write transactions after receiving a plurality of confirmations for the respective proposed write transaction from the replicas. In some embodiments the leader is one of the replicas in other embodiments the leader manages the process of receiving the write requests making the proposed write transactions and committing the transactions as they receive a plurality of confirmations but does not have a copy of the tablet for the relevant Paxos group. Note that committing the N proposed write transactions can occur in any order depending on when a plurality of confirmations is attained for each proposed write transaction.

In some circumstances an unexpected event takes the leader out of service. This can happen due to a hardware failure at the leader a power outage at the leader a network failure at or near the leader or any other event that disconnects the leader from the rest of the Internet. In any of these circumstances the leader is now unavailable. The other replicas which are generally in other geographic regions are still available. To provide seamless fault tolerant access to the distributed database the database management system must select a new leader resolve whatever transactions are in progress from the previous leader and begin handling new write requests.

In some circumstances the leader becomes unavailable prior to assigning all of the Paxos log numbers for the current epoch to write transactions. A statistically less probably event is having the leader become unavailable after assigning all of the Paxos log numbers to transactions but before the inter epoch tasks have begun. In this case the process holds an election to select another of the replicas as the new leader. The new leader is selected by consensus of the replicas that are available. In some embodiments consensus requires a plurality of the replicas to agree on the new leader in other embodiments consensus requires that one potential leader receives greater support than all others but the support may be less than a plurality. In some embodiments the replicas track various statistics that identify qualities potentially relevant to leadership selection e.g. latency in response to messages between replicas and use the statistics to assign preferential weights to potential leaders. In some of these embodiments consensus is determined by summing preferential weights.

The leadership lock previously assigned to the former leader is relinquished and the process establishes a new leadership lock assigned to the new leader for the current epoch. As before the leadership lock precludes any non leader replica from constructing proposed write transactions.

The new leader must first resolve each of the finite sequence of Paxos log numbers for the current epoch. Because the former leader is unavailable the status of some of the write transactions may not even be known. The new leader polls each of the replicas and resolves each of the Paxos numbers according to the aggregate information from the polling. In some embodiments any transaction identified as committed by any of the replicas will be marked as committed by the new leader this would include transactions that have been committed and applied at one or more replicas . In some embodiments any proposed write transaction that has a plurality of confirmations i.e. acceptances from replicas is committed by the new leader if not already committed. In some embodiments any proposed write transaction that has less than a plurality of confirmations will be re proposed by the new leader as long as there are not two distinct outstanding proposed write transactions with the same Paxos log number. When a Paxos log number is assigned to two or more distinct proposed write transactions and none has a plurality of confirmations some embodiments designate the Paxos log number as a NO OP no operation .

In some circumstances the former leader may have received one or more write requests and assigned Paxos log numbers to the requests but encountered the outage before transmitting corresponding proposals to any other replicas. In this case no replica is aware of the writes so the Paxos log numbers previously assigned by the former leader are identified as unassigned by the new leader. The new leader is thus free to assign these Paxos numbers to future write requests. If the former leader comes back online quickly this can lead to Paxos numbers assigned to two distinct proposed transactions. However the relevant Paxos log numbers are likely to be included in new transactions quickly and once confirmed by a plurality of replicas these numbers are part of committed transactions. 

In some embodiments the new leader resolves each Paxos log number by determining that the Paxos log number has not been assigned to any write transaction that is known by any of the replicas or confirming that the Paxos log number is assigned to a write transaction that has been committed or determining that the Paxos log number is assigned to a respective write transaction and committing that write transaction or determining that the Paxos log number is assigned to a respective write transaction and re proposing that write transaction or designating the Paxos log number as a NO OP.

After resolving all outstanding Paxos transactions from the former leader the new leader can begin processing new write requests. Of course the front end server s need to direct the write requests to the new leader.

The new leader receives a third database write request and assigns a third Paxos log number to the third database write request. The third Paxos log number is selected from the finite sequence of Paxos log numbers for the current epoch that have not been previously assigned to a proposed write transaction. The new leader constructs a third proposed write transaction that includes the third Paxos log number and incorporates the third received write request. The new leader transmits the third proposed write transaction to at least a plurality of the replicas and commits the third proposed write transaction after receiving a plurality of confirmations for the third write transaction from the replicas. Subsequently the leader and the non leader replicas apply the third write transaction to the database.

Standard Paxos won t accept a proposal unless it has committed the preceding proposal. In other words there are no holes in the log. Preventing holes greatly simplifies the role of a recovering potential leader because for all but the last log position there exists a quorum that knows what was committed. Disclosed embodiments allow log holes in order to reduce latency of writes. Otherwise a lagging replica can prevent progress in a healthy quorum. The presence of log holes however means that a recovering leader may need to use Paxos to roll forward many log entries that are in progress.

Pipelining does not introduce any throughput benefit relative to boxcarring multiple user writes into each round of Paxos writes. However pipelining does reduce latency. The latency benefit comes from removing the propose quorum delay that a write would have to wait in an un pipelined system.

Pipelined writes require several types of locking First the leader holds a synchronous time based exclusive lock on leadership. This exclusive lock on leadership for a Paxos group allows the leader to dispense read leases and to complete writes unimpeded by contenders. While that lock is valid no competing leader is issuing conflicting proposals. Standard Paxos in contrast is fully asynchronous and cannot guarantee that a leader is exclusive. Therefore an asynchronous leader can never rule out the possibility of writes competing with its own.

Second transaction locks guarantee that writes concurrently submitted to the Paxos leader do not conflict. Transaction locks weed out conflicting writes before they are submitted to Paxos as a transaction cannot be reverted once proposals are issued. Strong writes acquire exclusive locks before they are submitted to the leader s Paxos state machine. Although a Paxos write can be acknowledged as inevitable after a quorum of Propose messages is acknowledged write locks must be held until the write is applied to the database.

Third the leader respects any restrictions imposed by read leases. A holder of a read lease can serve strong reads without consulting other replicas. To render this safe the leaseholder must know about any writes that have potentially been acknowledged or applied. The Paxos leader honors read leases by ensuring that all leaseholders have acknowledged a proposal before it can be considered committed.

The set of leases is known for certain only by the leader who dispensed them. A read lease term must be shorter than the leader s and be nullified on any incoming message from a higher numbered leader.

A simple implementation has a replica relinquish its entire read lease when it receives a proposal only regaining it when that proposal has been committed and applied. This shortens the duty cycle of strong reads on that replica. For the entire period between propose and commit reads on that replica must block even for unrelated data items.

Disclosed embodiments perform better by utilizing fine grained transaction locks. Consider a read lease as analogous to a shared lock on the entire directory though none is physically held . On receipt of a proposal a Paxos replica acquires an exclusive lock for all cells identified in the proposal before acknowledging it to the leader. If the leader does not commit until all leaseholders have acknowledged a write it ensures no locked reads will be served during its leadership that neglect to reflect a committed write. If the leader s lease expires and some new leader completes the write all read leases have expired in the mean time and the write can be safely committed.

As illustrated in individual shards are grouped together in Paxos groups. Over time the sizes of the shards change generally getting bigger so some shards have to be reallocated to other Paxos groups or create new Paxos groups . If a shard changes groups around the same time that the current group leader has an outage there is some complexity about what group the shard belongs to and how the new leader can recover.

When a new leader takes over in a new epoch or after an outage of the previous leader the new leader may need to resolve the previous leader s incomplete writes. The Paxos rules that ensure that possibly committed writes get recommitted require the leader to re propose the highest numbered proposal returned by a quorum. To do so the leader must know what replicas can form a quorum at the log sequence number in question. Since a directory s set of replicas its Paxos group is changed via Paxos writes to that directory there is a problem about which replicas to consult.

Some embodiments address this issue by allowing group changes only in the last log entry of an epoch. For example with Paxos blocks of 500 numbers group changes could occur at Paxos log sequence 499 or 999 and so on. This restriction allows concurrently catching up log entries within a group epoch so long as the preceding epoch was entirely applied. Leader catch up will only be pipelined one group epoch at a time.

The Paxos algorithm itself prevents conflicting transactions from being decided at a single log sequence number. In unpipelined Paxos any quorum knows the global high water mark of proposed sequence numbers. It is at most one position beyond the last committed sequence number.

In pipelined Paxos however each time a new leader takes over it determines from a quorum the high water mark prior leaders have committed to operate within. Via catch up Paxos on each outstanding sequence number it kills or commits all proposals that may have been written by prior leaders. That is all proposals up to and including that high water mark. In particular if any sequence numbers up to that high water mark went unused then the new leader must fill them with no ops. It can then safely issue transaction locks and pipelined proposals. Future leaders catching up from any quorum will at worst re propose values already known to this leader. The pseudocode in identifies the catch up process used in some embodiments.

Thus following catch up a leader may only issue new proposals at the sequence number following the lowest high water mark known to any quorum. Before putting multiple writes in flight simultaneously it must notify the quorum of an extended high water mark. Whenever the leader has failed to secure a window of sequence numbers in which to issue concurrent proposals it reverts to keeping a single proposal in flight.

A small state machine is maintained for each log position with states roughly corresponding to those in a single position unpipelined implementation. Exemplary states for each Paxos log number are illustrated in . For each state the description identifies how the state is used and how it transitions to other states. The PENDING  states are used in some embodiments to allow greater control over the size and number of outgoing messages. There is no mention of applying in these states because applying committed proposals is handled independently.

When a leader is caught up it receives incoming writes from clients. Incoming writes are assigned to empty slots in the Paxos pipeline as they arrive. Some embodiments introduce a small artificial delay during periods of heavy load so that many writes can be bundled into a single outgoing propose round i.e. boxcarring . When boxcarring multiple writes preferred embodiments impose a limit on the size or number of writes in a single batch.

Writes are not allowed to span a group epoch boundary. Once an epoch boundary is reached the flow is the same as in leader catch up the pipeline drains and then the leader seeks re election or passes off leadership to a newly elected leader.

The foregoing description for purpose of explanation has been described with reference to specific embodiments. However the illustrative discussions above are not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications to thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated.

