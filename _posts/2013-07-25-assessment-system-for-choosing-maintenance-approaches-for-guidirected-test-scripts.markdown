---

title: Assessment system for choosing maintenance approaches for GUI-directed test scripts
abstract: A graphical user interface (GUI) tool analysis system helps determine whether to purchase or license automated testing tools. The system provides guidance, e.g., to test managers, for making decisions on expenditures for the automated test tools. As a result, the test managers need not make purchasing decisions ad hoc, based on their own personal experience and perceived benefits of implementing a tool based automatic testing approach versus a manual testing approach.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09009672&OS=09009672&RS=09009672
owner: Accenture Global Services Limited
number: 09009672
owner_city: Dublin
owner_country: IE
publication_date: 20130725
---
This application is a continuation application of U.S. patent application Ser. No. 12 813 300 filed Jun. 10 2010 the entire contents of which is hereby expressly incorporated by reference and which claims the priority benefit of Provisional Patent Application Ser. Nos. 61 186 331 filed on Jun. 11 2009 and 61 186 366 filed Jun. 11 2009 both of which are incorporated herein by reference in their entireties.

This disclosure relates to black box testing of Graphical User Interface GUI based Applications GAPs .

Manual black box testing of GAPs is tedious and laborious since nontrivial GAPs contain hundreds of GUI screens and thousands of GUI objects. Test automation plays a key role in reducing the high cost of testing GAPs. In order to automate this process test engineers write programs using scripting languages e.g. JavaScript and VBScript and these programs test scripts mimic users by performing actions on GUI objects of these GAPs using some underlying testing frameworks. Extra effort put in writing test scripts pays off when these scripts are run repeatedly to determine if GAPs behave as desired.

Unfortunately releasing new versions of GAPs with modified GUIs breaks their corresponding test scripts thereby obliterating the benefits of test automation. Consider a situation of a list box replaced with a text box in the successive release of some GAP. Test script statements that select different values in this list box will result in exceptions when executed on the text box. This simple modification may invalidate many statements in test scripts that reference this GUI object. Maintaining test scripts involves changing its code to keep up with changes to their corresponding GAPs.

This and many other similar modifications are typical between successive releases of different GAPs including such well known GAPs as Adobe Acrobat Reader and Microsoft Word. As many as 74 of the test cases become unusable during GUI regression testing and some evaluations of automated testing have shown that even simple modifications to GUIs result in 30 to 70 changes to test scripts. To reuse these scripts test engineers should fix them. For example scores of test engineers need to be employed to fix test scripts both manually and using different testing tools. The annual cost of manual maintenance and evolution of test scripts is enormous and may run into the tens or hundreds of millions of dollars in large organizations.

Currently there are two main modes of maintaining test scripts tool based and manual. Existing testing tools detect exceptions in test scripts at runtime i.e. test engineers run these scripts in order to execute statements that reference modified GUI objects. Exceptions interrupt continuous testing and they require human intervention to fix them.

Unlike compilers that check unit tests against the program code test scripts are based on different type systems than GAPs that they test. As it turns out multiple disparate type systems make GUI testing very difficult. Existing regression testing approaches work in settings where test harnesses are written in the same language and use the same type system as the programs that these harnesses test e.g. JUnit test harnesses are applied to Java programs . In contrast when testing GAPs two type systems are involved the type system of the language in which the source code of the GAP is written and the type system of the language in which test scripts are written. When the type of the GUI object is modified the type system of the test script does not know that this modification occurred thereby aggravating the process of maintaining and evolving test scripts.

As a result tool based approaches provide maintenance modes that allow testers to find broken statements in test scripts by executing these statements line by line against GAPs. The presence of loops in test scripts make them run for a long time in order to reach statements that should be checked. Test engineers comment out loops but their modifications may change testing logic and mask broken statements. Finally commercial testing tools are expensive e.g. a license for one of the flagship industry tools costs more than 10 000 .

On the other hand manual maintenance of test scripts is popular among test professionals. During manual maintenance testers determine differences between successive release of GAPs and they locate and fix statements in test scripts that are affected by these changes. Since the sizes of test scripts are much smaller than the GAPs that these scripts act upon e.g. many scripts are smaller than 1KLOC it is feasible for testers to understand and fix them. In addition testers are perceived to do a more thorough job of understanding and fixing scripts if they do not rely heavily on tool support. However some test engineers lack time and necessary skills to understand and fix old scripts especially if these scripts were created by other engineers.

Currently testers run test scripts that are written for the previous releases of a GAP on the successive releases of this GAP to determine if these scripts can be reused. The testers may use existing tools that include a script debugger e.g. QTP from Hewlett Packard company . Once a statement that accesses a modified GUI object is reached the testing platform generates an exception and terminates the execution of the script. The engineer analyzes the exception fixes the statement and reruns the script again. This process is repeated until the script runs without throwing any exceptions.

Often it takes a long time until statements that reference changed GUI objects are executed. Test scripts contain loops branches and fragments of code that implement complicated testing logic in addition to statements that access GUI objects. Consider a test script that contains a loop with code that reads in and analyzes data from files computes some result from this data and inserts it in a GUI object. Computing this result may take hours depending on the sizes of the files. Test scripts often contain multiple computationally intensive loops that are interspersed with statements that access GUI objects. Each time an exception is thrown because of a failure the results of the execution are discarded and the script should be rerun after engineers fix this failure. Commenting out loops when possible speeds up execution but it changes the logic of test scripts and subsequently the quality of repairs.

In addition existing testing tools provide little information about how to fix failures in test scripts. When a test script is executed against a new version of the GAP existing tools have no information about changes between GUI objects that lead to exceptions. As a result test engineers must analyze GUIs manually to obtain this information and relate it to the exceptions and this is a laborious and intellectually intensive process.

When fixing failures in test scripts manually testers examine GUIs of two consecutive releases of some GAP to determine what GUI objects are modified. In addition testers with advanced skills as well as programmers study the source code of GAPs if it is available to understand these changes in depth. Learning how GAPs are modified between released versions and relating these changes to statements and operations in test scripts may have a beneficial learning effect on testers. Without relying on tool support testers are thought to do a more thorough job of finding and fixing failures in test scripts.

It is not clear if the manual approach has definite benefits over the tool based approach. On one hand testing tools are expensive and may take a long time to execute scripts to determine what statements are broken because of changes made to GUI objects between successive releases of GAPs. On the other hand the manual approach requires testers to go over each statement and operation in test scripts to understand what GUI objects they refer to and it is laborious and expensive.

What is needed is a sound and complete approach. A sound approach ensures the absence of failures in test scripts if it reports that no failures exist or if all reported failures do in fact exist and a complete approach reports all failures or no failures for correct scripts. Both manual and tool based approaches allow testers to detect some failures that result from modifications of GUI objects however it is unclear with what degree of precision.

A GUI testing analysis system system determines which approach manual versus tool based is more effective for test personnel and GUI developers to find failures in test scripts. The system may employ a large scale case study in one example with forty five professional programmers and test engineers to empirically assess the productivities for maintaining GUI directed test scripts using a tool based versus a manual approach. The system may process and generate statistical evidence that demonstrates that users find more broken test script statements due to changes in GUI objects between successive releases of GAPs and report fewer false positives in test scripts with an automated approach than with a manual approach. The system may calculate a cost benefit ratio based on the assessment. The system may also process and generate statistical evidence that suggests significantly higher productivity with programmers but similar results with experienced test engineers when using the testing tool compared to those who maintained test scripts using the manual approach. The statistical evidence may include a plurality of statistical variables and the correlation among the statistical variables determined as a result of the empirical assessment.

The system may also generate for any particular organization a recommendation whether the organization supply programmers with testing tools that help them to fix test scripts faster so that these scripts can unit test software. Based on the correlation determined by the empirical assessment the system may determine the optimum values of statistical variables through an optimization algorithm. The system may utilize these variables as a basis for determining the recommendation.

Other systems methods features and advantages will be or will become apparent to one with skill in the art upon examination of the following figures and detailed description. It is intended that all such additional systems methods features and advantages be included within this description be within the scope of the invention and be protected by the following claims.

Cost benefit analysis of the tool based approach versus the manual approach is complicated. Licenses for tools are expensive. At the same time the time of testers is also expensive. If using testing tools does not result in significant time savings the cost of tool licenses will increase the cost of testing. It is imperative to understand when using test tools is effective so that managers can plan test automation for software projects.

Besides test engineers GUI developers test their code by creating and eventually maintaining test scripts. Since these developers spend major part of their time writing source code of GAPs rather than testing them managers often think that the purchasing licenses of expensive testing tools for developers will not result in significantly increased savings of their time. On the other hand the developer s time is more expensive and it is desirable that they spend their time writing code than fixing test scripts. Testing tools may make developer s work more productive. Thus the system may analyze during the cost benefit analysis what approach is more effective for programmers as well as testers.

Leasing tools for a short period of time results in a higher cost per hour of usage of these tools however the overall cost of ownership is low. If GAPs did not evolve their test scripts would likely to stay unchanged once created and leasing testing tools for a short period of time to create these scripts would be economic. However since test scripts should be maintained on a regular basis tool licenses should be purchased to reduce the overall cost of ownership. Purchasing more licenses than it is economically required is detrimental for the cost of software projects.

To run test scripts only one tool license is required however if testers maintain test scripts on a regular basis many tool licenses are needed one per tester for maintenance tasks. When testers maintain scripts manually tool license can be leased for a short period of time to run scripts and the cost of the ownership of the tool is minimal. On the other hand it requires significant investment for each tester to maintain scripts with the help of testing tool since it would mean a purchasing many licenses of testing tools. A trade off between the cost of the tool licenses and the increase in testing productivity justifies using manual versus tool based approaches.

In practice there is no consensus which approach testers and GUI developers should use to maintain test scripts. Test managers make their decisions ad hoc based on their personal experience and perceived benefits of the tool based approach versus the manual. Currently testers use tool based approaches on an ad hoc basis while programmers rarely use GUI directed test tools for unit testing. As few as only 35 of testers used automated testing tools one year after the tool installation.

The objectives of test automation are among other things to reduce the human resources needed in the testing process and to increase the frequency at which software can be tested. Traditional capture replay tools provide a basic test automation solution by recording mouse coordinates and user actions as test scripts which are replayed to test GAPs. Since these tools use mouse coordinates test scripts break even with the slightest changes to the GUI layout.

Modern capture replay tools e.g. Quick Test Professional QTP Abbot Selenium and Rational Functional Tester RFT avoid this problem by capturing values of different properties of GUI objects rather than only mouse coordinates. This method is called testing with object maps and its idea is to reference GUI objects by using unique names in test scripts. Test engineers assign unique names to collections of the values of the properties of GUI objects and they use these names in test script statements to reference these objects.

In testing with object maps the pairs uname where is the set of the pairs of values v of the properties p of a GUI object are collected during capture and stored in object repositories ORs under the unique name uname. During playback the references to uname in scripts are translated into operations that retrieve from ORs and the referenced GUI object is identified on the screen by matching the retrieved values against some or all of its properties. This extra level of indirection adds some flexibility since cosmetic modifications to GUI objects may not require changes to test scripts. Changing the GUI object property values in the OR ensures that the corresponding GUI objects will be identified during playback.

However many changes still break scripts for example changing the type of a GUI object from the list box to the text box. The system defines test script statements that access and manipulate GUI objects as failures if these statements are broken because of modifications made to the referenced GUI objects in the successive releases of GAPs. Test engineers put a lot of efforts in detecting and understanding failures so that they can fix test scripts to make them work on modified versions of GAPs.

A test automation model that illustrates interactions between test scripts and GAPs is shown in . Statements of test scripts are processed by the scripting language interpreter that is supplied with a testing platform. When the interpreter encounters statements that access and manipulate GUI objects it passes the control to the testing platform that translates these statements into a series of instructions that are executed by the underlying GUI framework and the operating system.

At an abstract level the system can view the layers between test scripts and GAPs as a reflective connector. A connector is a channel that transmits and executes operations between test scripts and GAPs. Reflection exposes the type of a given GUI object and it enables test scripts to invoke methods of objects whose classes were not statically known before the GAP is run. This model combines a connector between scripts and GAPs with reflection so that test scripts can access and manipulate GUI objects at run time.

Each statement in test scripts which accesses and manipulates GUI objects includes the following operations 1 navigate to some destination GUI object and 2 invoke methods to perform actions on this object including getting and setting values. Using implementations of the concepts of reflection and connector statements in test scripts can navigate GUI objects in GAPs and perform operations on these objects. This is the essence of the current implementations of test automation tools.

Several fundamental problems make it difficult to maintain and evolve test scripts. First specifications for GUI objects are often not available and these objects are created dynamically in the GAPs processes and the contexts of the underlying GUI frameworks e.g. Windows or Java SWT . With black box testing obtaining information about GUI objects from the source code of GAPs is not an option. Therefore test engineers have to use capture replay tools to extract values of properties of GUI objects so that these objects can be later identified on GUI screens by matching these prerecorded values with the properties of GUI objects that are created at runtime. Because complete specifications of GUI objects are not available it is difficult to analyze statically how GUI objects are accessed and manipulated by test script statements.

Another problem is that test scripts are run on testing platforms externally to GAPs and therefore cannot access GUI objects as programming objects that exist within the same programs. Using Application Programming Interface API calls exported by testing platforms is a primary mode of accessing and manipulating GUI objects and these API calls lead to various run time errors in test scripts especially when their corresponding GAPs are modified.

Consider a test script statement that is written using VBScript of QTP VbWindow Login .VbButton Dolt .Click. The API calls VbWindow and VbButton are exported by the QTP testing framework. Executing these API calls identifies a window whose property values match those stored in some OR under the name Login and this window contains a button whose property values match those stored in an OR under the name Dolt . By calling the method Click this button is pressed. Since API calls take names of the property values of GUI objects as string variables and GUI objects are identified only at runtime it is impossible to apply effective sound checking algorithms. These problems exacerbate the process of detecting and understanding failures in test scripts making maintenance and evolution of these scripts expensive and prohibitive. These fundamental problems are inherent for most existing open source and commercial automated testing tools.

The selected statistical measure of performance may include correctly identified failures CIFs false positives FPs and missed failures MFs . The statistical analysis parameters may also include a pre specified level of significance and the statistical analysis program may be implemented to include instructions that when executed evaluates the primary null hypothesis and the alternative null hypotheses at the pre specified level of significance . In addition the instructions of the statistical analysis program may further include variance analysis and t test analysis .

The memory may further include a statistical analysis result an optimization program statistical variables and statistical constraint equations . The statistical analysis result may store the results generated by the statistical analysis program . The statistical variables may be identified by an external entity such as a program testing manager or may be preconfigured in the system and may be further divided into input variables and output variables . The statistical constraint equations may include one or more of the statistical variables and may define mathematical relationships among the statistical variables. The statistical constraint equations may be determined by the system based on the statistical analysis result may be preconfigured in the system or may be identified by an external entity such as the manager. The manager may for example identify the statistical constraint equations based on the statistical analysis result .

The optimization program may further include an optimization algorithm and the evaluation function . The optimization program may be implemented as instructions which cause the processor to receive values of the input variables and determine an optimized value of the statistical variables based on the statistical constraint equations and the values for the input variables by executing the optimization algorithm and the evaluation function .

The memory may also include a recommendation representing a recommendation the system may generate based on the results of the optimization program . The recommendation may indicate whether manual testing or automated testing is recommended for testing the GUI as one example. The recommendation may also indicate whether manual maintenance or tool based maintenance is recommended in case an automated testing is recommended. Further if tool based maintenance is recommended the recommendation may indicate whether purchasing or leasing the tools for the tool based maintenance is recommended.

The system may further include a display coupled to the processor and memory . The display may be for example a computer monitor a portable device or another local or remote display. The statistical analysis program may generate a user interface on the display . The user interface may display statistical analysis results such as the reports and graphs. The manager may reference the statistical analysis results in identifying the statistical constraint equations . The user interface may also display the recommendations .

The system considers the following null and alternative hypotheses to evaluate how close the means are for the CIFs FPs and MFs for control and treatment groups . Unless specified otherwise participants of the treatment group uses the automated tool and participants of the control group use the manual approach. The system may seek to evaluate the following hypotheses at a 0.05 level of significance or any other significance level specified as a parameter by the system operator .

HThe primary null hypothesis is that there is no difference in the numbers of CIFs FPs and MFs between participants who attempt to locate failures in test scripts manually or using QTP.

HAn alternative hypothesis to His that there is statistically significant difference in the numbers of CIFs FPs and MFs between participants who attempt to locate failures in test scripts manually or using QTP.

Once the system tests the null hypothesis H the system may determine or analyze the directionality of means of the results of control and treatment groups. The system may compare the effectiveness of the tool QTP versus the baseline manual approach with respect to CIFs MFs and FPs.

In addition the system may determine whether the performance of the participants who have testing experience differs from those who do not have any testing experience . The categorical variables are testing experience and reported CIFs FPs and MFs. Hypothesis H10 considers correlation between the hypotheses H4 and H7 hypothesis H11 considers correlation between the hypotheses H5 and H8 and the hypothesis H12 considers correlation between the hypotheses H6 and H9.

H10 Independence of testing experience from CIFs the testing categorical variable is independent from the variable CIF the alternative is that they are associated.

H11 Independence of testing experience from FPs the testing categorical variable is independent from the variable FP the alternative is that they are associated.

H12 Independence of testing experience from MFs the testing categorical variable is independent from the variable MFs the alternative is that they are associated.

In one implementation the system analyzed the results of testing against the following four open source subject GAPs based on the following criteria easy to understand domain limited size of GUI less than 200 GUI objects and two successive releases of GAPs with modified GUI objects . Twister versions 2.0 and 3.0.5 is a real time stock quote downloading programming environment that allows users to write programs that download stock quotes. mRemote versions 1.0 and 1.35 enables users to manage remote connections in a single place by supporting various protocols e.g. SSH Telnet and HTTP S . University Directory versions 1.0 and 1.1 allows users to obtain data on different universities. Finally Budget Tracker versions 1.06 and 2.1 is a program for tracking budget categories budget planning for each month and keeping track of expenses. Most of these applications are nontrivial they are highly ranked in Sourceforge with the activity over 95 .

In performing the tests test scripts were obtained for each of the GAPs that contained both GUI and non GUI related code e.g. setting values of environment variables and reading and manipulating directories contents . The scripts were also modified to include generated statements that referenced GUI objects in the subject GAPs. The scripts were also interspersed with the generated statements replicated throughout the test scripts. Information on subject GAPs and test scripts can be found in Table 1.

Table 1 shows the subject GAPs and test scripts. Column Size contains five subcolumns reporting the numbers of lines of code LOC in test scripts the number of GUI objects that are referenced in the script numbers of added and deleted GUI objects and the numbers of API calls that reference GUI objects. The column Total Failures show the number of failures that are injected in test scripts.

The system analyzes a cross validation study design in a cohort of 45 participants who were randomly divided into two blocks labeled using different color labels. The system sectioned the study into two experiments in which each block was given a different approach manual and automated to apply to the subject GAPs . Thus each participant used each approach on different GAPs in the process of the case study. The system randomly distributed participants so that each block had approximately the same number of participants with and without testing experience . Before the study the participants were given two one hour tutorials on using each of these approaches. The example GAP mRemote in these tutorials was not used during the experiments thereby eliminating the knowledge of the GAP as a possible confounding factor .

The participants have different backgrounds experience and belong to different groups within a large consulting organization. Out of 45 participants 23 had prior testing experience ranging from three weeks to ten years and 18 participants reported prior experience with writing programs in scripting languages including test scripts. Seven participants reported prior experience with the automated tool six participants reported prior experience with other GUI testing tools. Twenty nine participants have bachelor degrees and ten have master degrees in different technical disciplines.

Sources of variation are all things that could cause an observation to have a different value from another observation. In designing the experiment the sources of variations are identified including the prior experience of the participants with tools GAPs and test scripts used in this study the amount of time they spend on learning how to use tools and different computing environments which they use during the case study. The latter is extremely sensitive since some participants who use slow laptops with limited form factor are likely to be less effective than other participants who use much better computing systems.

The system controls the experiment design to drastically reduce the effects of covariates e.g. nuisance factors in order to normalize sources of variations. Using the cross validation design the experiment normalized variations to a certain degree since each participant uses all three approaches on different subject GAPs . The system selected participants who had no prior knowledge of the subject GAPs. At the same time subject GAPs belong to domains that are easy to understand and these GAPs have similar complexity so variations between them are negligent. However different computing environments and prior experience of users with testing scripts and subject GAPs are major covariates.

The study analyzed by the system eliminated the effect of the computing environments by providing all participants with Dell Latitude D630 laptops with Intel Core 2 Duo Processor 2.4 GHz with 4 MB L2 Cache 1 Gb RAM and 14.1 WXGA displays. The same standard Windows XP based image was installed on the laptops. The installed GAPs scripts and tools were installed in a virtual machine that runs on top of the Microsoft Virtual PC thereby allowing participants to obtain a common environment of the entire experimental setup.

The system applied the one way Analysis of Variance ANOVA t tests for paired two samples for means and Xto evaluate the hypotheses. These tests are based on an assumption that the population is normally distributed. The law of large numbers states that if the population sample is sufficiently large between 30 to 50 participants then the central limit theorem applies even if the population is not normally distributed. Since the study included 45 participants the central limit theorem applies and the above mentioned tests have statistical significance.

Since seven participants reported prior experience with the automated tool this case study can be viewed as biased towards the automated tool versus the manual approach and Manual. To reduce this bias the study included a comprehensive tutorial on the automated tool for all participants of the study and given the large number of participants it is expected that this bias is negligent. However the results of this study show that participants who had prior experience with the automated tool performed better with the manual approach. In addition prior testing experience of the participants remains a source of variation.

The study distinguished between participants who have testing experience testers and those who do not have any testing experience programmer as defined in hypotheses as non tester based on the data that these participants reported in their resumes and questionnaires . This division does not account for large variations in different skills that may affect the results of this case study. Specifically it is desired to conduct an independent evaluation of the skills of the participants using programming tasks which was not done.

Other considerations the study used nontrivial test scripts that contained code written by different test engineers there are no metrics of how representative these scripts are of those used to test GAPs no data was available to report the percentage of coverage of GUI objects by test scripts and subject test scripts contain references to GUI objects that are located on one GUI screen per GAP . Other case studies may be created by extending the case study to support test scripts whose statements reference objects on different GUI screens.

Below are reported the results of the case study and detailed evaluation of the null hypotheses by the system. The system applied statistical tests such as one way ANOVA t tests for paired two samples for means and Xto evaluate the hypotheses stated above in the Hypotheses section . However the system may apply other statistical tests and hypothesis depending on the implementation.

The system may analyze as a main independent variable the approaches manual or automated that participants use to find failures in test scripts. The other independent variable that the system may analyze is the participants testing experience. Dependent variables that the system may analyze are the numbers of correctly identified failures CIFs false positives FPs and missed failures MFs . The study minimizes the effects of other variables GAPs test scripts prior knowledge are minimized by the design of the case study.

The system applied statistical test such as ANOVA to evaluate the null hypothesis Hthat the variation in an experiment is no greater than that due to normal variation of individuals characteristics and error in their measurement . The results of ANOVA with respect to CIFs confirm that there are large differences between the groups for CIF with F 4 12 F 3 97 with p

Similarly the results of ANOVA with respect to FPs confirm that there are large differences between the groups for CIF with F 12 1 F 3 97 with p

Finally the results of ANOVA with respect to MFs confirm that there are large differences between the groups for MF with F 4 43 Fcrit 3 97 with p

To test the null hypotheses H1 to H9 the system applies two t tests for paired two samples for means for CIFs FPs and MFs for participants who used the automated approach and the manual approach . The results of this test for CIFs and for FPs are shown in Table 2. Results of t tests of hypotheses H for paired two sample for means for two tail distribution for dependent variables specified in the columns Var either CIF FP and MF and Test Experience either all participants testers or programmers all minus testers whose measurements are reported in the following columns. Extremal values Median Means variance s2 degrees of freedom DF and the pearson correlation coefficient C are reported along with the results of the evaluation of the hypotheses i.e. statistical significance p and the T statistics. The column Samples shows that 37 to 41 out of a total of 45 participants participated in all experiments several participants missed one or two experiments . Based on these results the system rejects the null hypotheses H2 H7 H8 and H9 and the system accepts the alternative hypotheses that say that participants without testing experience programmers who use the automated approach report fewer false positives correctly identify more failures and miss fewer failures in test scripts than those who use the manual approach.

Alternatively the system accepts null hypotheses H1 H3 H4 H5 and H6 and reject the corresponding alternative hypotheses that say that participants with testing experience testers who use the automated approach report approximately the same numbers of false positives and correctly identify the same number of failures in test scripts than those who use the manual approach.

One explanation of this result uses the key differentiator between testers and programmers testing experience. Programmers who do not have testing experience and whose goal is to write code rather than test it rely on testing tools more than testers who understand test scripts and know how to fix them. Testers know the limitations of testing tools and they can work without them as effectively as when using these tools.

Based on the results of the case study and considering the high cost of the programmers time and the lower cost of the time of test engineers and considering that programmers often modify GAP objects in the process of developing software the system recommendation is to supply programmers with testing tools that enable them to fix test scripts faster so that these scripts can unit test software. The other side of the system recommendation is that experienced test engineers are likely to be as productive with the manual approach as with the tool based approach when fixing test scripts and the system may consequently recommend that organizations do not need to provide each tester with an expensive tool license to fix test scripts.

The system measures the qualities of both approaches using two ratios precision and recall and . The precision ratio is computed as CIF CIF FP and the recall ration is computed as CIF TF where TF is the total number of failures in test scripts. The precision is the ratio of correctly recovered failures and the recall ratio shows how mistaken participants are when analyzing failures in test scripts.

One benefit of computing the precision and recall is to evaluate the difference between CIFs and FPs. If all identified failures are in fact real failures and not FPs i.e. FP 0 then precision 1. If all identified failures are FP i.e. CIF 0 then precision 0 and recall 0. The system may generate graphs for precision and recall for the manual approach and automated approach such as graphs and shown in correspondingly. The system may also generate and display histograms of precision and recall values for these approaches such as shown histogram manual histogram automatic and histogram manual histogram automatic correspondingly. From these graphs one can see that precision and recall is somewhat better with automated approach than with the manual approach.

The system may construct a contingency table to establish a relationship between CIFs and FPs for participants with and without testing experience as shown in Table 3. To test the null hypotheses H5 and H6 that the categorical variables CIFs and FPs are independent from the categorical variable testing experience the system may perform statistical tests such as applying two X tests Xand Xfor CIFs and FPs respectively . The system obtains X 21 3 for p

The system may generate the statistical analysis results based on the analysis of the statistical analysis program and generate the user interface on the display device to display the statistical analysis results . The statistical analysis results may include some or all of the information shown above in Tables 1 3 and in .

The system may include statistical variables stored in the memory . The statistical variables may be categorized into input variables and output variables . The system may also include the statistical constraint equations stored in the memory . The statistical constraint equations may be preconfigured in the memory defined by the manager based on the statistical analysis result or obtained in other ways. The statistical constraint equations may include one or more of the statistical variables and constrain the mathematical relationships among the included statistical variables . The statistical variables may include but is not limited to the variables shown in Table 4.

The values for the input variables may be work place dependent and may be received from a manager of a work place preconfigured in the memory e.g. based on prior experience or obtained in other ways. MTR represents the cost for hiring a manual tester. The cost for hiring the manual tester may include for example the hourly rate paid to the manual tester. MTR may be represented as units of monetary value over time such as dollars hour. MTT represents the time required for a manual tester to conduct a manual GUI testing. MTT may be represented as units of time such as hours. ER represents the number of revisions a tester goes through in writing a GUI testing script for automated testing. ER may be represented as a number of revisions. NMT represents the number of manual testers needed for conducting a manual GUI testing. NMT may be represented as number of persons. ETER represents the cost for hiring an experienced test engineer. The cost for hiring the experienced test engineer may include for example the hourly rate paid to the experienced test engineer. ETER may be represented as monetary value over time such as dollars hour. NTER represents the cost for hiring a novice test engineer. The cost for hiring the novice test engineer may include for example the hourly rate paid to the novice test engineer. NTER may be represented as units of monetary value over time such as dollars hour. TLC represents the cost for licensing a test tool for an automated GUI testing. TLC may be represented as units of monetary value such as dollars or other units of currency. TPC represents the cost for purchasing a test tool for an automated GUI testing. TPC may be represented as units of monetary value such as dollars or other units of currency.

The output variables may be dependent on the values received for the input variables and the statistical constraint equations . Given values of the input variables the system may determine the output variables in the manner noted below. TSCT represents the time it takes to compose the test scripts for an automated GUI testing. TSCT may be represented in units of time such as hours. TSRT represents the time it takes to run the test scripts for an automated GUI testing. TSRT may be represented in units of time such as hours. TSMT represents the time it takes to maintain the test scripts for an automated GUI testing. TSMT may be represented in units of time such as hours. MS represents the factor by which a tool based maintenance quicker than a manual maintenance of GUI testing tools. MS may be represented as a real number without any units. NTL represents the number of test tool licenses needed to run an automated GUI testing. NTL may be represented in units of number of licenses. NTEN represents the number of test engineers needed to run an automated GUI testing. NTEN may be represented in units of number of test engineers. TP represents whether a manual or an automated testing is used. A value of 0 may represent manual and a value of 1 may represent automated. MP represents whether a manual or a tool based test script maintenance is used in the event an automated testing is used. A value of 0 may represent manual maintenance and a value of 1 may represent tool based maintenance. TUP represents whether a maintenance tool is purchased or leased in the event an automated testing used for GUI testing and tool based maintenance is used for test script maintenance. A value of 0 may represent purchasing the tool and a value of 1 may represent leasing the tool. TC represents the total cost for conducting a GUI test. TC may be represented in monetary units such as dollars .

Based on the statistical variables and the statistical constraint equations the system may generate a recommendation by executing the optimization program . The recommendation generated by the system may be largely related to one of the following three approaches a manual testing of the GUI b automated testing of the GUI with manual test script maintenance and c automated testing of the GUI with tool based test script maintenance. For each of these types of approaches the system may consider various factors related to each approach in generating the recommendation . For manual testing a the system may consider the rate at which the manual testers may conduct the testing and the time it takes for the manual testers to conduct the test. For automated approaches b and c the system may consider factors related to composing the test scripts number of test engineers needed number of licenses required for the test tools and associated costs running the test scripts test script running time and factors associated with maintaining the test scripts test engineer pay rate and manual maintenance time for approach b test engineer pay rate additional tool license number and cost and tool speed up factor for approach c .

The statistical variables may include variables which represent the factors discussed above which the system may consider in generating the recommendations. Each of the approaches that the recommendation may possibly make may be represented as a combination of values of some or all of the output variables . For example the recommendation may be represented by the combination of values of output variables TP and MP. The variables TP and MP may each hold either a value of 0 or a value of 1 . For TP a 0 may represent a manual test plan and a 1 may represent an automatic test plan. For MP a 0 may represent a manual maintenance and a 1 may represent a tool based maintenance. If the recommendation generated by the system represents the approach b the recommendation may be for example a value 1 for TP representing automatic test plan and an MP with the value 0 representing manual maintenance.

The statistical constraint equations and the values to which they are set may vary widely. The statistical constraint equations and the values may be obtained from controlled experiments on prior test script testing projects for example. However they may also be preconfigured in the memory or obtained in other ways. After the statistical variables and the statistical constraint equations are configured or identified the system may then receive from values for each of the input variables . The values for the input variables may be workplace dependent and may be input by managers of at the workplace. However the values may also be preconfigured in the memory received from other external entities or obtained in many other ways.

Next the system applies the statistical constraint equations to a parameter space defined by the output variables to define a curve in the parameter space . The curve represents all the possible values of the output variables as constrained by the values of the input variables and the statistical constraint equations .

Next the system determines the optimum point on the curve defined in step by executing the optimization algorithm . The optimization algorithm may determine an optimum point based on the value of the evaluation function at a given point on the curve. While the optimum point may be a local maximum or minimum in most cases the optimum point will be a global maximum or minimum. The system may search for other types of optimum results however. The evaluation function may be defined to return values representing different goals depending on the purpose of the optimization program . For example the goal of the optimization program may be to determine the most efficient method of testing a GUI. In such a case the evaluation function may be defined to return the value representing efficiency. The goal of the optimization program may also be set to determine the least time consuming method of testing a GUI. In such a case the evaluation function may be defined to return the value representing time. The optimum point determined by the optimization algorithm in this case will be a point on the curve associated with values of the statistical variables which represents a testing approach which requires the least or one of the least amount of time to test the GUI and maintain the GUI testing tools.

Based on the optimum point determined by the optimization algorithm the system may generate the recommendation . The system may display the recommendation on the user interface of the display . The recommendation may include values of some or all of the output variables corresponding to the optimum point determined by the optimization algorithm . For example as discussed above the recommendation may include values for the output variables TP MP at the optimum point. The recommendation may further include value of the variable TUP representing whether to purchase 0 or lease 1 the GUI testing tools.

In one embodiment the goal of the optimization program may be set to determine the point on the curve which represents the least amount of cost for testing and maintaining the GUI. In this embodiment the evaluation function may be defined to represent the cost of conducting a GUI test given the values of the statistical variables at a given point on the curve. shows an exemplary mathematical equation of the evaluation function . The equation includes three optimization components. The first optimization component represents the cost of purely manual testing. The second optimization component represents the cost of automated testing with manual maintenance of the test script. The third optimization component represents the cost of automated testing with tool based maintenance of the test script. Thus overall the equation represents the cost for performing the GUI testing manually optimization component for automated with manual test script maintenance optimization component and for automated with tool based test script maintenance optimization component and the optimization algorithm finds the minimum cost according to the three optimization components and that define the costs for the various testing scenarios. The equations may include additional different or fewer testing scenarios by modifying the optimization components appropriately.

Thus the system helps reach a consensus regarding what approach testers should use to maintain test scripts. The system may help test managers avoid making their decisions ad hoc based on their personal experience and perceived benefits of the tool based approach versus the manual. As described above the system analyzes a case study e.g. with forty five professional programmers and test engineers to experimentally assess the automated approach for maintaining GUI directed test scripts versus the manual approach. Based on the results of the case study and considering the high cost of the programmers time and the lower cost of the time of test engineers and considering that programmers often modify GAP objects in the process of developing software the system generates a recommendation. In particular the system may identify statistical variables including input and output variables determine statistical constraint equations and determine an evaluation function to be optimized. Based on the statistical constraint equations and values received for the input variables the system may generate recommendations by executing an optimization program with respect to the evaluation function The recommendation may indicate as an example that organizations supply programmers with testing tools that enable them to fix test scripts faster so that these scripts can unit test software. The other side of the recommendation may indicate that experienced test engineers are likely to be as productive with the manual approach as with the automated approach when fixing test scripts and the system may therefore recommend that organizations do not need to provide each tester with an expensive tool license to fix test scripts.

The implementation discussed above is exemplary. Other implementations may vary any of the equations variables parameters or characteristics noted above. For example other implementations may use a different number of test GAPs types of test scripts types and number of hypotheses and tests used for hypotheses evaluation number of test subjects experience level of test subjects confidence levels or other statistical analysis parameters types or sequences of statistical tests applied types of report outputs types of statistical variables statistical constraint equations types of statistical variables included in the recommendation or other characteristics. The statistical analysis program operates on any particular set of statistical analysis parameters obtained by running the tests and analyzes them to generate results. The optimization program also operates on any particular set of statistical variables and determines recommendations based on the statistical constraint equations and values received for the input variables.

The system described above may be implemented in any combination of hardware and software. For example programs provided in software libraries may provide the functionality that forms the hypotheses obtains the input data executes the statistical tests generates output reports obtains values for input variables identifies the statistical variables determines the statistical constraint equations determine recommendations or other functions. Such software libraries may include dynamic link libraries DLLs or other application programming interfaces APIs . The logic described above may be stored on a computer readable medium such as a CDROM hard drive ROM or RAM floppy disk flash memory or other computer readable medium. In addition the system may be implemented as a particular machine. For example the particular machine may include a CPU GPU and software library for carrying out the functionality that forms the hypotheses obtains the input data executes the statistical tests generates output reports obtains values for input variables identifies the statistical variables determines the statistical constraint equations determine recommendations or other functions noted above. Thus the particular machine may include a CPU a GPU and a memory that stores the logic described above.

While various embodiments of the invention have been described it will be apparent to those of ordinary skill in the art that many more embodiments and implementations are possible within the scope of the invention. Accordingly the invention is not to be restricted except in light of the attached claims and their equivalents.

