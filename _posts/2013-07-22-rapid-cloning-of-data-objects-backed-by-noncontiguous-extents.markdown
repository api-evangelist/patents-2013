---

title: Rapid cloning of data objects backed by non-contiguous extents
abstract: A request is received to clone a source data object. A source block range of the source data object in a source logical storage unit is determined. An empty data object in the destination logical storage unit is created. A destination block range of the empty data object in the destination logical storage unit is determined. The source block range is mapped to the destination block range. The source data object is cloned based on the mapping.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09483202&OS=09483202&RS=09483202
owner: NetApp, Inc.
number: 09483202
owner_city: Sunnyvale
owner_country: US
publication_date: 20130722
---
This application is a continuation of co pending U.S. application Ser. No. 12 965 760 filed Dec. 10 2010.

A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever. The following notice applies to the software and data as described below and in the drawings hereto Copyright 2010 NetApp Inc. All Rights Reserved.

This invention relates generally to storage systems and more particularly to rapid cloning of virtual machines in a storage system.

A virtual machine is the representation of a physical machine by software. A virtual machine has its own set of virtual hardware e.g. random access memory RAM central processing unit CPU network interface card NIC hard disks etc. upon which an operating system and applications are loaded. The virtual machine operating system sees a consistent normalized set of hardware regardless of the actual physical hardware components. In a virtualized environment a physical host machine e.g. a computer runs virtualization software such as a hypervisor and abstracts physical hardware e.g. processors memory storage and networking resources etc. to be provisioned to one or more virtual machines. Storage on a storage system is mapped to the physical host machine such that the physical host machine can use the storage.

A guest operating system e.g. Windows etc. may be installed on each of the virtual machines. The virtualization software presents the physical hardware of the host machine as virtual hardware to the guest operating system and applications running in the guest operating system. A user may access the virtual machine to perform computing tasks as if it were a physical machine. For example a user may want to rapidly clone a file or data object.

The storage system includes an operating system such as NetApp Data ONTAP . The storage system operating system provides single instance storage sis clone functionality which can be used to create a clone of an entire Logical Unit Number LUN . The storage system operating system can provide the capability to perform sub LUN cloning by providing as input a logical block address LBA range to be cloned and a block range of the destination to store the clone. However when the entity to be cloned is a file present on a file system such as New Technology File System NTFS the LBA range of the file is not known. Furthermore the cloned blocks in the destination are not recognized as a file by the destination NTFS.

Rapid cloning capabilities provided by virtual machine managers such as Windows System Center Virtual Machine Manager SCVMM use Windows Background Transfer Service BITS technology and do not provide significant performance gain over traditional file copy. Moreover virtual machine managers rapid cloning is time intensive and uses a significant amount of memory.

Rapid cloning of virtual machines is performed by receiving a request to clone a source data object virtual machine . A source block range of the source data object in a source logical storage unit is determined. An empty data object in the destination logical storage unit is created. A destination block range of the empty data object in the destination logical storage unit is determined. The source block range is mapped to the destination block range. The source data object is cloned based on the mapping.

In the following detailed description of embodiments of the invention reference is made to the accompanying drawings in which like references indicate similar elements and in which is shown by way of illustration specific embodiments in which the invention may be practiced. These embodiments are described in sufficient detail to enable those skilled in the art to practice the invention and it is to be understood that other embodiments may be utilized and that logical mechanical electrical functional and other changes may be made without departing from the scope of the present invention. The following detailed description is therefore not to be taken in a limiting sense and the scope of the present invention is defined only by the appended claims.

Embodiments are described for a rapid cloning of virtual machines on LUNs. References in this specification to an embodiment . one embodiment or the like mean that the particular feature structure or characteristic being described is included in at least one embodiment of the present invention. Occurrences of such phrases in this specification do not necessarily all refer to the same embodiment nor are they necessarily mutually exclusive.

Rapid cloning of a source data object is performed by determining a source block range of the source data object creating an empty data object in a destination logical storage unit determining a destination block range of the empty data object in the destination logical storage unit and mapping the source block range to the destination block range of the empty data object.

Cloning a file does not require a physical copy of data involved and is therefore a fast and time efficient process. Furthermore the cloning of a file is performed off host and therefore does not consume any resources from the host itself. In one embodiment a user requests a clone of a virtual machine running on LUNs mapped to a Windows host. In an alternate embodiment a user requests provisioning virtual machines e.g. desktops using a virtual machine template. In another embodiment a user requests conversion of a physical machine to a virtual machine. In yet another embodiment a user requests a virtual machine to be moved from one LUN to another LUN. Still in yet another alternate embodiment a user requests restoring a subset of files from a LUN in a snapshot back to an original LUN.

Referring to host machine has access to storage system which provides access to data stored on storage devices and . A user of host machine may request rapid cloning of a file or data object stored on a storage device such as file on storage device . A clone of the file is created and stored on a storage device such as cloned file on storage device .

Referring to rapid cloning environment includes physical host machine and network storage system . In one embodiment network storage system is a storage area network SAN . Network storage system may include storage server configured to store and retrieve data. Network storage system operates to store and manage shared storage objects e.g. files in a set of mass storage devices such as magnetic or optical disks or tapes or flash drives. The mass storage devices may be organized into one or more volumes of a Redundant Array of Inexpensive Disks RAID . In a SAN context storage server provides block level access to stored data rather than file level access.

Host machine includes host machine operating system such as for example Microsoft Windows Server 2008 R2. Host machine operating system may include rapid cloning software for cloning of a file or data object. For example host machine operating system may clone source file to cloned file . Host machine operating system may include a hypervisor such as for example Hyper V in the Microsoft Windows Server 2008 R2 operating system. Host machine operating system enables host machine to host one or more virtual machines not shown each running its own operating system. In one embodiment host machine is also running virtualization software which may enable the provisioning of storage from network storage system through the virtual machines. In one embodiment virtualization software includes NetApp SnapDrive for Windows developed by NetApp Inc. of Sunnyvale Calif.

Network storage system includes one or more Logical Unit Numbers LUNs or logical storage units. Storage server operating system running on storage server of network storage system creates LUN and maps it to host machine . Mapping a LUN to host machine may include writing an identifier of LUN into a data structure representing the storage resources associated with host machine .

In network environment a user may request to clone a file such as source file . Rapid cloning software determines block range of source file in LUN . Cloned file is created in host machine . Block range of cloned file is determined. In one embodiment block range of cloned file is in the same LUN LUN as the block range of source file . In an alternate embodiment block range of cloned file is in a different LUN than LUN . Once block range of source file and block range of cloned file are determined block range of source file is mapped to block range of cloned file . Source file is cloned to cloned file using the mapping of block range of source file to block range of cloned file . Once cloned file is cloned it is accessible to host machine operating system .

Block range determiner data object creator and data cloner are used by operating system to implement rapid cloning of virtual machines on LUNs. Block range determiner determines a block range in memory for source data object and a block range in memory for cloned data object . Data object creator creates an empty data object such as cloned data object . Data cloner clones source data object and stores the cloned data object in cloned data object .

A request to clone a data object is received at block . The request may be received from a user or administrator that selects one or more source data objects in a first system running a first operating system e.g. Windows etc. . The source data objects are logically stored in a source logical storage unit e.g. LUN in a storage system running a storage operating system e.g. NetApp Data ONTAP . The source logical storage unit in the storage system is mapped to the first system running the first operating system. The request may include a number of clones that the user desires to create of the source data object. In one embodiment the request includes only the source data object and does not include a destination logical storage unit for the clones of the source data object. In this embodiment the destination logical storage unit for the clone s of the source data object is set to the same logical storage unit e.g. LUN as the source logical storage unit of the source data object. In an alternate embodiment the user may request a specific destination logical storage unit for the clone s of the source data object. The user may specify that the destination logical storage unit be mapped to a second system. In this embodiment the destination logical storage unit can be in the same storage system as the first system. The clone s of the source data object are stored in the destination logical storage unit.

At block the method determines if the host of the source data object is the same as the host of the destination for the clone. The determination may be made by 1 obtaining the host information for the host of the source logical storage unit storing the source data object 2 obtaining the host information for the destination logical storage unit to store the clone and 3 performing a comparison. In one embodiment Windows Management Instrumentation WMI is used to obtain the host information. In an alternate embodiment an API may be used obtain the host information. In one embodiment the results of the comparison are stored for future use. In an alternate embodiment the results of the comparison are not stored. If the host of the source data object is not equivalent to the host of the destination for the clone the destination logical storage unit is mapped to the host of the data object to be cloned the source logical storage unit at block . In one embodiment prior to mapping the destination logical storage unit to the host of the source logical storage unit the destination logical storage unit is unmapped from a host of the destination logical unit. If the host of the source data object is equivalent to the host of the destination for the clone the method does not need to perform a mapping and the method proceeds to block .

At block the method determines one or more block ranges of the source data object in the source logical storage unit. The determination is made as described below in conjunction with . The determination provides an array of source block ranges that includes a starting logical block address LBA and a block count number of blocks occupied by the source data object starting from the starting LBA for each element in the array each source block range .

An empty destination data object is created in the destination logical storage unit at block . In one embodiment the empty data object has a larger storage allocation than the data object to be cloned in order to provide storage for metadata associated with the cloned object. In an alternate embodiment the size of the empty data object is equivalent to the size of the source data object.

Once the empty destination data object is created the method determines a block range of the empty destination data object in the destination logical storage unit at block . The determination is made as described below in conjunction with . The determination provides an array of destination block ranges that includes a destination LBA and a block count number of blocks occupied by the destination data object starting from the destination LBA for each element in the array each destination block range .

Mapping the source block range to the destination block range is performed at block . The data object to be cloned or the empty destination data object may be fragmented in the source and destination file system e.g. NTFS and the source and destination fragments may be of unequal sizes. Therefore the fragments of the data object to be cloned may be mapped to the fragments of the empty destination data object. The mapping is performed as described below in conjunction with .

At block the blocks of the source data object in the source logical storage unit are cloned to the empty data object in the destination logical storage unit based on the mapping determined at block .

At block the method determines if the host of the source logical storage unit of the data object to be cloned is the same as the host of the destination for the clone. In one embodiment the determination may be made by 1 obtaining the host information for the host of the source logical storage unit storing the data object to be cloned 2 obtaining the host information for the destination logical storage unit to store the clone and 3 performing a comparison. In one embodiment Windows Management Instrumentation WMI is used to obtain the host information. In an alternate embodiment an API may be used obtain the host information. In an alternate embodiment a previously made determination stored in memory is obtained.

If the host of the source data object is not equivalent to the host of the destination for the clone the destination logical storage unit is mapped to the desired destination host for the clone at block . In one embodiment the destination logical storage unit is mapped to the desired destination host for the clone by using a Zephyr API ZAPI a proprietary API of NetApp and a WMI or Windows Virtual Disk Service API. As part of the mapping the destination logical storage unit may first be disconnected from the host of the data object to be cloned the source logical storage unit . If the host of the source data object is equivalent to the host of the destination for the clone the method ends.

The cloning described above may be performed by using a cloning API. In one example a Zephyr API ZAPI for Single Instance Storage SIS clone a proprietary API of NetApp is used to clone the data. In this example an API such as ZAPI may provide for marshalling of API name and input parameters using XML extensible markup language with input parameters being typed and the contents of the XML being independent of the programming language and architecture on both client and server sides of a transaction and with the server returning values from the invocation of the API marshaled in the same format as the input. The SIS clone ZAPI provides a method of reducing storage device e.g. disk space by eliminating duplicate data blocks on a flexible volume. Only a single instance of each unique data block is stored. The SIS clone ZAPI requires as input an array of block ranges to be cloned. Each block range contains an LBA for a source block an LBA for a destination block and a block count a number of blocks to be cloned .

Referring to the method determines or acquires one or more extents associated with the data object in the host file system at block . An extent is a contiguous area of storage in a file system that is reserved for a data object. In one embodiment the determination is made by calling a file system control operation such as FSCTL GET RETRIEVAL POINERS. The call for the file system control operation may require a file handle in which case the data object may be passed as an input to the operation. The file handle can be obtained using an API. For example the CreateFile API can be used to obtain the file handle with the path of the data object as input. The file system control operation returns one or more extents associated with the data object. Each extent includes a logical cluster number and a size for each of the extents. The extents are relative to the beginning of a volume device seen by a host operating system.

At block the method determines the starting offset on the storage device starting storage device offset where the volume device containing the data object begins. In one embodiment the starting sector offset on the storage device is obtained by calling a file system control operation such as IOCTL VOLUME GET VOLUME DISK EXTENTS. The call for the file system control operation may require a file handle in which case the data object may be passed as an input to the operation. The file handle can be obtained using an API. For example the CreateFile API can be used to obtain the file handle with the path of the data object as input. The method executes a loop to determine an LBA block count beginning at block ending at block and performing the processes represented by blocks and .

At block a product is generated e.g. multiply of the logical cluster number for an extent and a number of sectors per cluster and the product is added to the starting sector offset to determine the start LBA of that extent. The start LBA may be stored in an array of start LBAs. In one embodiment the number of sectors per cluster is acquired from the file system by calling a file system control operation.

At block the block count for the extent is determined. In one embodiment the size of the extent determined at block is in units of clusters. The block count is calculated by converting the size of the extent from clusters into sectors. In one embodiment this conversion is made by obtaining the sectors per cluster for the logical storage unit and generating a product of the size of the extent in sector and the sectors per cluster. In one embodiment the sectors per cluster may be obtained by calling a file system control operation such as the Windows API GetDiskFreeSpace. The product is the number of blocks occupied by the data object beginning at each start LBA. The number of blocks for each start LBA may be stored in the array of start LBAs.

Referring to the method performs an initialization at block . The initialization is performed by initializing a source range flag a destination range flag a current source range a current destination range a number of source bytes a number of destination bytes and a list of cloning ranges to be cloned. The initialization can include setting a source range flag to be complete and setting a destination range flag to be complete. The initialization can include initializing a current source range to the first source LBA element in a source block ranges array e.g. array of source block ranges of generated at block . The initialization can include initializing a current destination range to the first destination LBA element in a destination block range array e.g. array of destination block ranges of generated at block . The initialization can include initializing a variable for the number of source bytes to the first block count element in the source block ranges array e.g. array of source block ranges of . The initialization can include initializing a number of destination bytes to the first block count element in the destination block ranges array e.g. array of destination block ranges of . The initialization can include initializing a list of ranges to be cloned to empty.

The method executes a loop to map source ranges to destination ranges by determining if there are more source ranges in source block ranges array or more destination ranges in destination block ranges array beginning at block ending at block and performing the processes represented by blocks through .

At block a cloning block range is computed. The cloning block range consists of a source LBA a destination block LBA and a number of blocks to clone. The source LBA can be determined by determining if the source range flag is set to be complete. If the source range flag is set to be complete the source LBA is set to be the current source range. If the source range is not marked as complete the source LBA is the sum of the current source range and a number of blocks to clone of the previous cloning range to be cloned. The destination LBA can be determined by determining if the destination range flag is set to be complete. If the destination range flag is set to be complete the destination LBA is set to be the current destination range. If the destination range is not set to be complete the destination LBA is the sum of the current destination LBA and a number of blocks to clone of the previous cloning range to be cloned. The number of blocks to clone is the minimum of the source bytes and the number of destination bytes.

The computed cloning block range computed at block is included in a list of cloning block ranges to be cloned at block . The method compares the number of source bytes to the number of destination bytes at block .

If the number of source bytes is less than the number of destination bytes the source bytes from more than one source range e.g. current and next range must be mapped to the current destination range. If the number of source bytes is less than the number of destination bytes the method sets the current source range to be the next source LBA element in the source block ranges array at block .

The source bytes and the destination bytes are updated at block . The source bytes are set to be the current block count element in the source block ranges array corresponding to the current source LBA element . The destination bytes are determined by subtracting the number of blocks to clone from the current value of the destination bytes. The source range flag is marked complete and the destination range flag is marked incomplete at block .

If the number of source bytes is equal to the number of destination bytes the source bytes must be mapped to the destination bytes and the method sets the current source range to be the next source LBA element in the source block ranges array and the current destination range to be the next destination LBA element in the destination block ranges array at block .

The source bytes and the destination bytes are updated at block . The source bytes are determined by subtracting the number blocks to clone from the current value of the source bytes. The destination bytes are updated by subtracting the number of blocks to clone from the current value of the destination bytes. The source range flag is marked complete and the destination range flag is marked complete at block .

If the number of source bytes for the current source range is greater than the number of destination bytes for the current destination range the source bytes must be mapped to more than one destination range. If the number of source bytes is greater than the number of destination bytes the method sets the current destination range to be the next destination LBA element in the destination block ranges array at block .

The source bytes and the destination bytes are updated at block . The source bytes are determined by subtracting the number blocks to clone from the current value of the source bytes. The destination bytes are set to the current block count element in the destination block ranges array corresponding to the current destination LBA element . The source range flag is marked incomplete and the destination range flag is marked complete at block .

Storage of data in storage units is managed by storage servers which receive and respond to various read and write requests from clients directed to data stored in or to be stored in storage units . Storage units constitute mass storage devices which can include for example flash memory magnetic or optical disks or tape drives illustrated as disks A B . The storage devices can further be organized into arrays not illustrated implementing a Redundant Array of Inexpensive Disks Devices RAID scheme whereby storage servers access storage units using one or more RAID protocols known in the art.

Storage servers can provide file level service such as used in a network attached storage NAS environment block level service such as used in a storage area network SAN environment a service which is capable of providing both file level and block level service or any other service capable of providing other data access services. Although storage servers are each illustrated as single units in a storage server can in other embodiments constitute a separate network element or module an N module and disk element or module a D module . In one embodiment the D module includes storage access components for servicing client requests. In contrast the N module includes functionality that enables client access to storage access components e.g. the D module and may include protocol components such as Common Internet File System CIFS Network File System NFS or an Internet Protocol IP module for facilitating such connectivity. Details of a distributed architecture environment involving D modules and N modules are described further below with respect to and embodiments of an D module and an N module are described further below with respect to .

In yet other embodiments storage servers are referred to as network storage subsystems. A network storage subsystem provides networked storage services for a specific application or purpose. Examples of such applications include database applications web applications Enterprise Resource Planning ERP applications etc. e.g. implemented in a client. Examples of such purposes include file archiving backup mirroring etc. provided for example on archive backup or secondary storage server connected to a primary storage server. A network storage subsystem can also be implemented with a collection of networked resources provided across multiple storage servers and or storage units.

In the embodiment of one of the storage servers e.g. storage server A functions as a primary provider of data storage services to client . Data storage requests from client are serviced using disks A organized as one or more storage objects. A secondary storage server e.g. storage server B takes a standby role in a mirror relationship with the primary storage server replicating storage objects from the primary storage server to storage objects organized on disks of the secondary storage server e.g. disks B . In operation the secondary storage server does not service requests from client until data in the primary storage object becomes inaccessible such as in a disaster with the primary storage server such event considered a failure at the primary storage server. Upon a failure at the primary storage server requests from client intended for the primary storage object are serviced using replicated data i.e. the secondary storage object at the secondary storage server.

It will be appreciate that in other embodiments network storage system may include more than two storage servers. In these cases protection relationships may be operative between various storage servers in system such that one or more primary storage objects from storage server A may be replicated to a storage server other than storage server B not shown in this figure . Secondary storage objects may further implement protection relationships with other storage objects such that the secondary storage objects are replicated e.g. to tertiary storage objects to protect against failures with secondary storage objects. Accordingly the description of a single tier protection relationship between primary and secondary storage objects of storage servers should be taken as illustrative only.

Nodes may be operative as multiple functional components that cooperate to provide a distributed architecture of system . To that end each node may be organized as a network element or module N module A B a disk element or module D module A B and a management element or module M host A B . In one embodiment each module includes a processor and memory for carrying out respective module operations. For example N module may include functionality that enables node to connect to client via network and may include protocol components such as a media access layer Internet Protocol IP layer Transport Control Protocol TCP layer User Datagram Protocol UDP layer and other protocols known in the art.

In contrast D module may connect to one or more storage devices via cluster switching fabric and may be operative to service access requests on devices . In one embodiment the D module includes storage access components such as a storage abstraction layer supporting multi protocol data access e.g. Common Internet File System protocol the Network File System protocol and the Hypertext Transfer Protocol a storage layer implementing storage protocols e.g. RAID protocol and a driver layer implementing storage device protocols e.g. Small Computer Systems Interface protocol for carrying out operations in support of storage access operations. In the embodiment shown in a storage abstraction layer e.g. file system of the D module divides the physical storage of devices into storage objects. Requests received by node e.g. via N module may thus include storage object identifiers to indicate a storage object on which to carry out the request.

Also operative in node is M host which provides cluster services for node by performing operations in support of a distributed storage system image for instance across system . M host provides cluster services by managing a data structure such as a RDB RDB A RDB B which contains information used by N module to determine which D module owns services each storage object. The various instances of RDB across respective nodes may be updated regularly by M host using conventional protocols operative between each of the M hosts e.g. across network to bring them into synchronization with each other. A client request received by N module may then be routed to the appropriate D module for servicing to provide a distributed storage system image.

It should be noted that while shows an equal number of N and D modules constituting a node in the illustrative system there may be different number of N and D modules constituting a node in accordance with various embodiments of rapid cloning. For example there may be a number of N modules and D modules of node A that does not reflect a one to one correspondence between the N and D modules of node B. As such the description of a node comprising one N module and one D module for each node should be taken as illustrative only.

Memory includes storage locations addressable by processor network adapter and storage adapter for storing processor executable instructions and data structures associated with rapid cloning. Storage operating system portions of which are typically resident in memory and executed by processor functionally organizes the storage server by invoking operations in support of the storage services provided by the storage server. It will be apparent to those skilled in the art that other processing means may be used for executing instructions and other memory means including various computer readable media may be used for storing program instructions pertaining to the inventive techniques described herein. It will also be apparent that some or all of the functionality of the processor and executable software can be implemented by hardware such as integrated currents configured as programmable logic arrays ASICs and the like.

Network adapter comprises one or more ports to couple the storage server to one or more clients over point to point links or a network. Thus network adapter includes the mechanical electrical and signaling circuitry needed to couple the storage server to one or more client over a network. Each client may communicate with the storage server over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

Storage adapter includes a plurality of ports having input output I O interface circuitry to couple the storage devices e.g. disks to bus over an I O interconnect arrangement such as a conventional high performance FC or SAS link topology. Storage adapter typically includes a device controller not illustrated comprising a processor and a memory for controlling the overall operation of the storage units in accordance with read and write commands received from storage operating system . As used herein data written by a device controller in response to a write command is referred to as write data whereas data read by device controller responsive to a read command is referred to as read data. 

User console enables an administrator to interface with the storage server to invoke operations and provide inputs to the storage server using a command line interface CLI or a graphical user interface GUI . In one embodiment user console is implemented using a monitor and keyboard.

When implemented as a node of a cluster such as cluster of the storage server further includes a cluster access adapter shown in phantom having one or more ports to couple the node to other nodes in a cluster . In one embodiment Ethernet is used as the clustering protocol and interconnect media although it will apparent to one of skill in the art that other types of protocols and interconnects can by utilized within the cluster architecture.

Multi protocol engine includes a media access layer of network drivers e.g. gigabit Ethernet drivers that interface with network protocol layers such as the IP layer and its supporting transport mechanisms the TCP layer and the User Datagram Protocol UDP layer . A file system protocol layer provides multi protocol file access and to that end includes support for the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A VI layer implements the VI architecture to provide direct access transport DAT capabilities such as RDMA as required by the DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the storage server. In certain cases a Fibre Channel over Ethernet FCoE layer not shown may also be operative in multi protocol engine to receive and transmit requests and responses to and from the storage server. The FC and iSCSI drivers provide respective FC and iSCSI specific access control to the blocks and thus manage exports of luns to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing blocks on the storage server.

The storage operating system also includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on storage devices. Information may include data received from a client in addition to data accessed by the storage operating system in support of storage server operations such as program application data or other system data. Preferably client data may be organized as one or more logical storage objects e.g. volumes that comprise a collection of storage devices cooperating to define an overall logical arrangement. In one embodiment the logical arrangement may involve logical volume block number vbn spaces wherein each volume is associated with a unique vbn.

File system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustrated as a SCSI target module . SCSI target module is generally disposed between drivers and file system to provide a translation layer between the block lun space and the file system space where luns are represented as blocks. In one embodiment file system implements a WAFL write anywere file layout file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using a data structure such as index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . File system uses files to store metadata describing the layout of its file system including an inode file which directly or indirectly references points to the underlying data blocks of a file.

Operationally a request from a client is forwarded as a packet over the network and onto the storage server where it is received at a network adapter. A network driver such as layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to file system . There file system generates operations to load retrieve the requested data from the disks if it is not resident in core i.e. in memory . If the information is not in memory file system accesses the inode file to retrieve a logical vbn and passes a message structure including the logical vbn to the RAID system . There the logical vbn is mapped to a disk identifier and device block number disk dbn and sent to an appropriate driver of disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the storage server. Upon completion of the request the node and operating system returns a reply to the client over the network.

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the storage server adaptable to the teachings of the invention may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by the storage server in response to a request issued by a client. Moreover in another alternate embodiment of the invention the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the storage server. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

When implemented in a cluster data access components of the storage operating system may be embodied as D module for accessing data stored on disk. In contrast multi protocol engine may be embodied as N module to perform protocol termination with respect to a client issuing incoming access over the network as well as to redirect the access requests to any other N module in the cluster. A cluster services system may further implement an M host e.g. M host to provide cluster services for generating information sharing operations to present a distributed file system image for the cluster. For instance media access layer may send and receive information packets between the various cluster services systems of the nodes to synchronize the replicated databases in each of the nodes.

In addition a cluster fabric CF interface module CF interface modules A B may facilitate intra cluster communication between N module and D module using a CF protocol . For instance D module may expose a CF application programming interface API to which N module or another D module not shown issues calls. To that end CF interface module can be organized as a CF encoder decoder using local procedure calls LPCs and remote procedure calls RPCs to communicate a file system command to between D modules residing on the same node and remote nodes respectively.

Rapid cloning of virtual machines in LUNs requires translating the host machine s view of a file to an LBA range used for cloning by the storage system and may be performed by rapid cloner in file system . A source block range in a source logical storage unit in RAID system is determined. An empty data object is created in a destination logical storage unit in RAID system . A destination block range of the empty data object is determined. The source data object is cloned to the empty data object.

Although the present invention is shown herein to implement rapid cloning within the storage operating system it will be appreciated that rapid cloning may be implemented in other modules or components of the storage server in other embodiments. In addition rapid cloning may be implemented as one or a combination of a software executing processor hardware or firmware within the storage server. As such rapid cloning may directly or indirectly interface with modules of the storage operating system in accordance with teachings of the present invention.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write anywhere file system the teachings of the present invention may be utilized with any suitable file system including conventional write in place file systems.

In practice the methods and may constitute one or more programs made up of computer executable instructions. Describing the methods with reference to the flowchart in enables one skilled in the art to develop such programs including such instructions to carry out the operations acts represented by logical blocks until until and until on suitably configured computers the processor of the computer executing the instructions from computer readable media . The computer executable instructions may be written in a computer programming language or may be embodied in firmware logic or in hardware circuitry. If written in a programming language conforming to a recognized standard such instructions can be executed on a variety of hardware platforms and for interface to a variety of operating systems. In addition the present invention is not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings of the invention as described herein. Furthermore it is common in the art to speak of software in one form or another e.g. program procedure process application module logic . . . as taking an action or causing a result. Such expressions are merely a shorthand way of saying that execution of the software by a computer causes the processor of the computer to perform an action or produce a result. It will be further appreciated that more or fewer processes may be incorporated into the method illustrated in without departing from the scope of the invention and that no particular order is implied by the arrangement of blocks shown and described herein.

Rapid cloning of virtual machines on LUNs has been described. Although specific embodiments have been illustrated and described herein it will be appreciated by those of ordinary skill in the art that any arrangement which is determined to achieve the same purpose may be substituted for the specific embodiments shown. This application is intended to cover any adaptations or variations of the present invention.

Moreover the description of is intended to provide an overview of computer hardware and other operating components suitable for performing the methods of the invention described above but is not intended to limit the applicable environments. One of skill in the art will immediately appreciate that the invention can be practiced with other computer system configurations. The invention can also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network.

It will be readily apparent to one of skill that input output devices such as a keyboard a pointing device and a display may be coupled to the storage server. These conventional features have not been illustrated for sake of clarity.

The term memory as used herein is intended to encompass all volatile storage media such as dynamic random access memory DRAM and static RAM SRAM . Computer executable instructions can be stored on non volatile storage devices such as magnetic hard disk an optical disk and are typically written by a direct memory access process into memory during execution of software by a processor. One of skill in the art will immediately recognize that the term computer readable storage medium includes any type of volatile or non volatile storage device that is accessible by a processor.

Therefore it is manifestly intended that this invention be limited only by the following claims and equivalents thereof.

