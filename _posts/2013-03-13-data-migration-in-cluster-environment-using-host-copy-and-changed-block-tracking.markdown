---

title: Data migration in cluster environment using host copy and changed block tracking
abstract: Data is non-disruptively migrated from a source LUN to a target LUN in an active-active cluster with coordination of a host-based copy sweep with duplicating of ongoing writes of all hosts, using iteration and changed block tracking (CBT). During a first iteration, one host performs the copy sweep, and the locations of new application writes at other hosts are recorded in respective CBT records. In each subsequent iteration, the data blocks of the CBT-recorded writes are copied to the target LUN while the locations of new writes are recorded in new CBT records. A final iteration is done with I/O suspended to completely synchronize the target LUN with the source LUN, and the migration then proceeds to a committed state in which the target LUN is used in place of the source LUN. The one host may use write cloning or CBT recording like the other hosts.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09400611&OS=09400611&RS=09400611
owner: EMC Corporation
number: 09400611
owner_city: Hopkinton
owner_country: US
publication_date: 20130313
---
The present invention relates to migration of data from a source data storage device to a target data storage device in a data processing system.

Data migration techniques are used to move or migrate data from one storage device or logical unit to another for any of a variety of purposes such as upgrading storage hardware or information lifecycle management. Generally migration involves synchronizing the target device to the source device i.e. achieving an operating state in which the target device stores the same data as the source device and then switching operation so that subsequent accesses of the data are directed to the target device instead of the source device. Once the switching is successfully accomplished the source device can be taken out of service or put to some other use.

Non disruptive migration is performed while there is ongoing application level access to the data stored on the source storage device. In non disruptive migration there are two parts to achieving synchronization a copy sweep by which all existing data on the source device is copied to the target device and cloning duplicating of ongoing application writes for the source device to the target device. Non disruptive migration also requires a non disruptive mechanism for switching operation to the target device. Example descriptions of non disruptive migration can be found in the following US patents whose entire contents are incorporated by reference herein 

Clustering is a technique used in computer systems to provide certain desirable functionality and characteristics from the perspective of external users. Advantages include increased performance and availability over non clustered systems. Two general types of clusters are failover and active active or parallel clusters. In a failover cluster all cluster nodes may be aware of a given storage device accessible in the cluster but in general a given storage device is accessed by only one node during operation. In the event of node failure a failover mechanism causes ownership of the storage device to be transferred to a new node that has assumed responsibility for the workload of the failed node. Due to the single node access there is no need for synchronizing accesses among the hosts. In active active clusters storage devices may be actively accessed from all nodes in the cluster and the operating software e.g. application software of the nodes is responsible for synchronizing access to shared storage resources.

It is desirable to support non disruptive data migration in clustered computer systems including active active clusters but such systems present certain challenges that must be addressed. In general all hosts of an active active cluster may be accessing a source device or LUN logical unit of storage during a migration and writes from the hosts to the source LUN must be duplicated to the target LUN. This activity must somehow be coordinated with the copying operation used in migration in order to maintain data consistency.

A method is disclosed for non disruptively migrating data from a source LUN to a target LUN in a cluster based data processing system where the data is being accessed concurrently by application programs of different host computers. The copy sweep part of the migration is performed by one of the host computers referred to as the host copy or HC host herein. This operation is coordinated with the duplicating of ongoing application writes of the other or non HC host computers of the cluster to maintain data consistency throughout the system.

In particular coordination is achieved using a sequence of iterated operations and a technique known as changed block tracking or CBT. During a first iteration the HC host performs the copy sweep and new application writes occurring at the other hosts are applied to the source LUN but not immediately cloned to the target LUN. Rather each of the other hosts uses CBT to keep a record of all the application writes to the source LUN occurring during the iteration. In each subsequent iteration the data blocks of the CBT recorded writes from the previous iteration are copied to the target LUN while new writes are recorded in new CBT records. This process continues to a desired point of completion and then a final iteration is done with I O suspended to bring the target LUN into complete synchronization with the source LUN. The migration then proceeds to a committed state in which the target LUN is used in place of the source LUN.

For the application writes occurring at the HC host the HC host may immediately clone these writes to the target LUN as they occur or it may use CBT like the other hosts. If the HC host uses cloning then at each iteration the CBT records of all other hosts are supplied to the HC host which then copies the corresponding blocks to the target LUN while it continues to clone its own newly generated writes. These operations are coordinated at the HC host to avoid data corruption. If the HC host uses CBT then each host can copy the blocks of the recorded writes from its own CBT records to the target LUN while recording newly generated writes in new CBT records to be applied in the next iteration.

The technique achieves desired non disruptive migration in an active active cluster in which a source LUN is accessed by multiple hosts concurrently. Moreover because it relies on host copy and a host executed coordination mechanism for the cloned writes CBT and iteration it does not require any specific type of storage system or other external components e.g. external write duplication functionality and thus may be generally deployed in a variety of types of data processing systems.

The interconnect includes one or more storage oriented networks providing pathways for data transfer among the hosts and devices . An example of the interconnect is a FibreChannel storage area network SAN either by itself or in conjunction with Ethernet or other network components. The devices are logical units of storage allocated for uses such as storing databases file systems etc. used by application programs executing on the hosts . Generally the devices are visible to the hosts as block oriented storage devices.

The LUNs include a source LUN S and a target LUN T participating in a migration operation by which the target LUN T functionally replaces the source LUN S in the system. It is assumed that prior to migration the source LUN S stores a data resource that is accessed by applications executing on the hosts using an association mechanism that specifically associates the application visible data resource with the source LUN S. Specific examples of such association mechanisms are described below. A migration operation moves the data resource to the target LUN T and changes the association mechanism so that future application accesses of the data resource are directed to the target LUN T rather than to the source LUN S. Reasons for such migration of storage resources include a desire for additional storage capacity or improved performance or to upgrade to more current and well supported hardware for example. In some cases the source LUN S is to be removed from the system although in other cases it may be retained and reused for other purposes.

In the active active cluster there may be applications executing simultaneously on different hosts having access to the source LUN S. The migration operation occurs while the applications continue to execute and thus it is in that sense non disruptive. One aspect of the migration operation is to coordinate certain operations of the hosts to ensure that there is no data loss or data incoherency created which could have any of several deleterious effects as generally known in the art. These aspects of the migration operation are described below.

For migration operations the system includes an actor referred to as a migration controller MIG CTRLLR . As indicated by lines during a migration the migration controller communicates with the drivers . In some embodiments the migration controller may be implemented by software executing on one of the hosts and in other embodiments it may be realized by software executing on a separate physical or virtual device in the system referred to below as a migration appliance . The functionality of the migration controller may also be divided between one of the hosts and an external appliance where the on host portion provides a user interface and high level control and the on appliance portion communicates with and coordinates the actions of the hosts based on higher level commands from the on host portion.

One of the hosts plays two important roles in the migration operation described herein. First it includes functionality for bulk copying of existing data from the source LUN S to the target LUN T which is also referred to as a copy sweep herein. Second it may play a key role in duplicating ongoing writes to the source LUN S from all the hosts . In the following description this one host is referred to as the host copy or HC host while the other hosts are referred to as the non HC or other hosts . In embodiments having the user interface and high level control parts of the migration controller on a host it may be advantageous to use the HC host for this purpose.

The copying copy sweep is done by the HC host as a unitary background process proceeding sequentially through all blocks of the source LUN S and copying them to corresponding locations on the target LUN T.

Write duplication is somewhat more complicated because it must be done for all writes occurring at all hosts and it must be coordinated in some fashion with the copy sweep to maintain consistency between the source LUN S and the target LUN T. As described more below writes are duplicated using a technique known as changed block tracking or CBT. During the copy sweep when the HC host is copying a set of data blocks from the source LUN S to the target LUN T each other host keeps a CBT record identifying blocks or regions that have been changed by writes to the source LUN S. Once the copy sweep is complete the blocks or regions recorded in the CBT records are copied from the source LUN S to the target LUN T and at the same time new CBT records are created to record changed blocks regions for new writes occurring during this copying. As mentioned writes at the HC host may be cloned or the affected blocks regions may be similarly recorded into a CBT record to be written in the next iteration. This basic set of operations is iterated as necessary to obtain a fully synchronized relationship between the source LUN S and the target LUN T. The last of these iterations is performed in the later commit operation as described more fully below and thus at the completion of the synchronize operation the target LUN T is synchronized with the source LUN S up to the point of a most recent set of writes which will be applied in that final iteration.

During the later Commit operation the target LUN T becomes fully synchronized to the source LUN S and the hosts begin using the target LUN T to the exclusion of the source LUN S. In particular this includes suspending application I O and applying access control to prevent application access to the source LUN S copying the last set of CBT recorded blocks regions from the source LUN S to the target LUN T changing the association mechanism to newly associate the application visible data with the target LUN T and then resuming application I O. Future application writes to the data are automatically directed to the target LUN T rather than to the source LUN S. After any necessary cleanup the source LUN S can be retired or put to other use. Details of this process are also described below.

CBT is a mechanism that tracks all changes to blocks of a storage volume that have occurred since a point in time. In the present context the point in time for a first iteration is just before the start of a copy sweep when transitioning to a Synchronizing state . For subsequent iterations the point in time is the end of the preceding iteration. The mechanism does not keep a copy of the changed data rather just a log identifying blocks that have been the subject of writes. In general such a log may be maintained in any of a variety of ways. It is preferably memory based for low latency. In one embodiment the log may be structured using a bitmap in which each bit represents a corresponding block or region of the source LUN S. Changed blocks regions are then tracked by setting the corresponding bits in the bit map.

CBT tracking may be done with respect to blocks or regions of some fixed size of at least 512 bytes. For the present technique CBT may be done with respect to larger blocks or regions such as 4 KiB 8 KiB or even bigger because the read write step can be performed more efficiently with a larger block size. Note that these are conventional references to corresponding power of 2 values i.e. 4 Ki means 4 096 2 etc. The block size will be a multiple of 512 bytes the unit of writing in the SCSI storage protocol. As an example the source LUN S may have a size of 1 GiB which is 28 KiB blocks. The CBT record for this LUN can be held in a bit map of four 4 KiB pages of memory with one bit representing each 8 Ki block of storage. So a write to anywhere on the LUN between byte addresses 0 and 8191 would cause the first bit in the bit map to be set to 1. If desired a larger block size can be chosen to be represented by a bit to prevent the bitmap from becoming too large.

The log method might keep the offset into the LUN and the number of bytes written. Depending on how long the iteration is and how many writes are being done the log could keep growing and might require allocating more memory in the middle of an application write. It is believed that such conditions would be rare.

A dual structure approach might be used to support the switching between CBT records when transitioning between iterations. For example when bitmaps are used a pointer might be switched to a new zeroed bitmap when a new iteration is started while the filled in bit map for the iteration just ending is used to guide the block copying process. Another approach would be to copy the just completed bitmap and send the copy elsewhere for processing in some cases to a different host then zero out the bitmap to be ready for the next iteration. A similar thing can be done with a log based tracking mechanism.

It is assumed that prior to the process both the source and target LUNs S T have become configured for use by the hosts . The contents data of the source LUN S constitute a data resource for operating software of the hosts and there is an association mechanism that associates this data resource with the source LUN S as the location of the resource. Different association mechanisms have different properties and may require different handling during a migration operation. One aspect of an association mechanism is whether it is location dependent or location independent i.e. whether the association mechanism includes physical device identifying information. Another aspect of an association mechanism is whether it involves a name used by applications to identify the location of application data to lower level components such as the drivers or rather a signature stored with application data on a device and used by operating software for purposes such as detection of duplication or other integrity consistency checking. In the process of it is assumed that whatever association mechanism is used there is a way to non disruptively make a change to the association mechanism as required to complete migration. Different specific association mechanisms and the manner in which they are handled in non disruptive as well as disruptive migrations are discussed below.

In response to the Setup command the migration controller verifies that the target LUN T is a suitable target such as checking that its size capacity is at least the size of the source LUN S. Each host applies access control to prevent application access to the target LUN T. The migration enters the Setup state during which all hosts access the source LUN S.

In response to the Synchronize command the migration controller commands certain kernel level components of the driver of the hosts to prevent application access to the target LUN T and for the synchronizing operation to begin. For each of the non HC hosts this includes enabling changed block tracking for subsequent writes to the source LUN S by that host . This is described more below. For the HC host it includes beginning the copy sweep. The HC host also begins duplicating cloning application writes that it generates. Thus when an application on the HC host writes to the source LUN S the driver of the HC host will perform the write operation on the source LUN S and perform the same write operation on the target LUN T. This operation will continue throughout synchronization.

It should be noted that each host may need to temporarily suspend input output to the source and target LUNs S T to transition into this operating mode target access disabled and CBT write cloning enabled but in general it is not required that such suspension occur across all hosts at the same time. However it is necessary that each host has transitioned into this operating mode before the copy sweep starts. If any of the hosts is offline for some reason when synchronization is begun it must begin operation in this operating mode when it comes back online to ensure that all writes are recorded for later duplication to the target LUN T.

At this point the migration advances to the Synchronizing state during which the contents of the target LUN T are made almost identical to the contents of the source LUN S. The contents only become fully identical when a last set of changed blocks recorded by CBT is copied from source LUN S to the target LUN T as described below. Getting to this almost identical state is achieved in the Synchronizing state by the combination of 1 the copy sweep in which all the existing previously written data of the source LUN S is copied to the target LUN T and 2 duplicating the ongoing application writes performed on the source LUN S to the target LUN T.

Duplication is performed using multiple iterations of a concurrent set of operations performed by the hosts . In operations of the HC host during each iteration are shown at left while operations of the other hosts are shown at right.

During the first iteration the HC host does the copy sweep. At the same time for new writes to the source LUN S by applications executing on the HC host the HC host clones those writes to the target LUN T. Cloning requires coordination with the copy sweep as explained below. Also during the first iteration each other host records all changed blocks caused by new application writes to the source LUN S by that host in local CBT records.

The second iteration commences when the copy sweep by the HC host in iteration is completed. This transition may occur automatically or it may require involvement of the migration controller and potentially the user directing the migration. In one embodiment the migration controller may monitor the progress of the copy sweep or at least become notified of its completion. For monitoring the migration controller may periodically or upon user command query the HC host to obtain a progress or completion indicator for the copy sweep. In one embodiment the HC host returns a value representing the number of blocks of the source LUN S that have been copied. The migration controller knows the size of the source LUN S and can compute a percentage or fraction of completion. When the migration controller is in a separate appliance this query and response will typically employ explicit messages across the interconnect . When the migration controller resides in the HC host this query and response will use communication mechanisms within the HC host as generally known in the art.

In the second iteration the HC host obtains the CBTs from the other hosts and copies blocks regions to the target LUN T accordingly i.e. it copies the identified blocks regions of the source LUN S to the target LUN T . For this operation the migration controller might gather the CBT records from the other hosts and then provide them to the HC host . This operation could be done in a host serial manner i.e. first obtaining the CBT records for one other host and copying those blocks then repeating the process for each remaining other host . In another embodiment the CBT records of all hosts may be obtained first and merged in some manner to create a merged set of changed block records and then the blocks for this merged set are all copied. When bitmaps are used the merging can be a logical OR operation across all bitmaps. This method may have advantages of minimizing wasteful duplication as described more below.

As shown each non HC host first closes the CBT records from the preceding iteration and provides the closed CBTs to the HC host as part of its gather copy process for the current iteration in this case . Each non HC host also immediately starts new CBT records for new writes it will receive during the current iteration. The transition from the existing CBT records to the new CBT records must be done atomically i.e. without any operating gap that would cause any intervening write to be unrecorded. Techniques for such atomic operations are generally known in the art. As indicated above each non HC host might provide its CBT record to the migration controller which then provides the CBT information from all non HC hosts e.g. merged or non merged to the HC host .

Iterating continues to a certain point. As a general matter the amount of data being copied and hence the time required at each iteration decreases over a set of iterations. The first iteration may have a very long duration because it involves copying all the previously existing data. Each subsequent iteration lasts sufficiently long to copy all the blocks of all writes that were subject to CBT in the immediately preceding iteration . At the end of each iteration there remains a set of most recently written blocks recorded in the CBT records of the hosts that still need to be copied to the target LUN T. In general iterating should continue until the number of remaining writes is desirably small because for a final iteration N it is necessary to suspend application I O which represents a performance penalty that is preferably minimized. Different criteria or mechanisms may be used to stop the iterating in different embodiments. Specific examples are discussed below.

Returning to operation in the first iteration the cloning of application writes at the HC host must be coordinated with the copy sweep. In one embodiment the storage space of the source LUN S is divided into an array of fixed size segments such as 256 KB segments. The copy sweep is done on a segment by segment basis proceeding serially from a first segment at one end of the address range of the source LUN S through successive segments to the other end of the address range. Any application writes to a segment that is currently being copied are held not performed until the copy of that segment completes. This avoids potential data corruption that could by caused by the copy sweep overwriting newly written data. It should be noted that this same constraint applies to the subsequent iterations . The HC host holds application writes that fall within a region being updated based on a CBT record from another host and performs those writes only when the CBT based updating is complete.

During the synchronization state device faults may occur for either or both the source LUN S and the target LUN T. Certain effects of faults and responses to the occurrence of faults are described below.

Once synchronization is completed to the above described point where the contents of the target LUN T are almost identical to the contents of the source LUN S i.e. identical except for remaining writes in current CBT records the user can issue a Commit command . This causes the following operations by the driver of each host under control of the migration controller 

The above operations require proper sequencing. In particular they are performed in sequential phases each of which must be complete for every host of the cluster before the step s of the next phase can be initiated 

The migration controller may take action to initiate each phase in sequence and leave any intra phase sequencing to each host . Features may be incorporated that protect the integrity of the process in the event of a host reboot during phases and . In some embodiments I O suspension does not survive a reboot and therefore without such protective features there is a possibility of inconsistency between the source LUN S and the target LUN T at the point of commitment. The protective features are preferably instituted at the end of Phase 1 and removed at the beginning of Phase 4. An example of such protective features is given in U.S. patent application Ser. No. 13 575 740 filed Jun. 28 2012. In an embodiment in which I O suspension survives a reboot such protective mechanisms may not be necessary.

As mentioned above for step 3 the second approach each host copying the blocks for its own CBTs may be more efficient when there are only a small number of writes in the CBT records. There are trade offs. Performing the updates on every node allows it to happen in parallel which should take less wall clock time. Minimizing the time required is important because IO is suspended. However if all the nodes modify a very similar set of blocks then some blocks regions may be copied multiple times. Such duplication can be avoided by gathering the CBTs at the migration controller or HC host and first merging the records before copying the corresponding blocks regions.

Once the above operations have been performed the migration enters the Committed state . In this state application I O is automatically directed to the target LUN T and not to the source LUN S by virtue of the change to the association mechanism. The target LUN T will store all newly written data and no synchronization is maintained with the source LUN . It is generally not possible to revert to operating with the source LUN S.

As mentioned the above operations are initiated by a Commit command from a user in one embodiment. The migration controller may be used to monitor progress and provide information that synchronization is sufficiently complete to move into the Committed state . In alternative embodiments the operations may be initiated automatically. As mentioned above the series of iterations will generally be of successively shorter durations and some automatic mechanism may be used to stop the iterating and then initiate the final iteration N and transition to the Committed state as described above. In one embodiment this may be achieved by monitoring the number of blocks being copied to the target LUN T in each iteration and testing whether it has diminished below some threshold. Alternatively there may be monitoring of the time required for each iteration and initiating the above steps when an iteration is of shorter duration than some threshold. In yet another approach there may be monitoring for a sufficiently small difference between the sizes or durations of successive iterations indicating that no efficiency is to be gained by additional iterating. Yet another approach may be to always perform a predetermined number of iterations where this number may be fixed or in some manner programmable or configurable. In a system such as that of the control logic for such monitoring and initiation of the commitment steps may be located in the migration controller . In an embodiment providing for automatic transition to the Committed state there may or may not also be support for user initiated transition such as through use of a Commit command . There may also be a way for a user to temporarily override an automatic mechanism so that operation in the synchronizing state can continue indefinitely until the override is cancelled.

A cleanup command initiates a cleanup operation to remove any remaining metadata associating the source LUN S with the storage resource that has been migrated. At that point the source LUN S may be removed from the system or it may be re configured for another use in the system. One important task performed during cleanup is to erase any information on the source device S that might cause it to be identified mistakenly as the storage resource that has been migrated to the target device T. Earlier in the migration access control prevents this mistaken identity . Along with erasure the source LUN S may have its contents replaced by a known pattern such as all zeros for security or other operational reasons.

The process of includes an abort path leading from the Synchronizing state back to the Setup state . Aborting may occur by user command or by automatic operation when problems are encountered during the process. For example if either the source LUN S or target LUN T fails or otherwise becomes unavailable during the process such failure may be detected either manually or automatically and lead to aborting the migration.

More particularly with respect to failures a device fault is a write failure to either the source or target LUNs S T. When the HC host uses write cloning all writes of the HC host are duplicated and migration can only proceed if both writes original and clone succeed. If one succeeds and the other fails the migration must be aborted. In this case the migration will go into a target device faulted state at this point and the user will have to execute the abort and start over perhaps first curing whatever problem caused the fault. A fault could also happen when updating the target LUN T based on the CBT records and this would also result in a fault condition that would require an abort. The copy sweep operation could also fail due to a read failure on the source LUN S or a write failure on the target LUN T. This is not a device fault but it will cause the synchronization to stop. An explanation of the handling of device faults in a non cluster environment can be found in the above referenced U.S. Pat. No. 7 770 053.

Device fault handling in the cluster environment may be generally similar to that described in the 053 patent but there are specific differences. For example in the non cluster environment as described in the 053 patent there are additional operating states including a source selected state and a target selected state. In the target selected state reads are redirected to the target device while writes are still being duplicated and synchronization is being maintained. If the system should be shut down unexpectedly upon restart the non selected side is faulted because there s no guarantee that all writes made it to both sides. Thus the source device is faulted if this occurs during operation in the target selected state. Also if a fault happens during normal I O then the side that fails the write will be faulted so the source side is faulted when operating in the source selected state and a write to the source device fails. In contrast in the cluster environment as described herein operation proceeds directly from the Synchronizing state to the Committed state or a Committed and Redirected state described below there is no source selected state or target selected state. Only the target LUN T is faulted when a write fails no matter which side the write fails on.

One possibility with the present migration technique is that a cloned write by the HC host that experiences a fault in one iteration could potentially be added to a CBT record at the HC host and retried in the next iteration . If the fault condition has cleared the retried write may succeed and thus the migration need not be aborted. In some embodiments retrying may continue until the final iteration N but in other embodiments it may be limited to a smaller limit such as one retry.

When the HC host employs CBT instead of cloning the CBT based updating of each iteration is done in parallel on all the hosts and without suspending application I O at any host . Additionally none of the hosts needs to coordinate writes in the special manner described above for HC host in the process of . In the first iteration A the location of all the new application writes are held in CBT records and used to update the target LUN T at these locations only in the second iteration A after the initial copy sweep is complete. Thus the copy sweep cannot overwrite the data at these locations. Similarly in subsequent iterations A the location of all the new application writes are likewise held in CBT records and used to update the target LUN T in the next iteration A after all updates in the preceding iteration A have been applied. The updating of data for locations from CBT records cannot overwrite the data of current writes.

The purpose of the Committed and Redirected state is to support a separate task of changing the association mechanism that associates the data resource with a source LUN S so that the data resource can be newly associated with the target LUN T. This is the point at which disruption occurs where for example one or more applications of the hosts may be stopped reconfigured as necessary to create the new association and then restarted. Once the change is made the system can operate correctly using the new association with the target LUN T so that redirection is no longer required.

Once the application is stopped redirection and changing the association mechanism can generally be done in either order as redirection has no effect when the application is stopped. In some cases the association mechanism may be changed while redirection is still in effect. With some association mechanisms it may be necessary to stop redirection prior to updating the association mechanism. In either case prior to restarting normal operation e.g. restarting any applications that are being reconfigured the Undo Redirect command is used to advance the migration state to Committed . Normal operation is then resumed. It should be noted that at least some control communications such as SCSI Inquiry commands are not redirected so that each LUN remains directly accessible for purposes of receiving or providing corresponding control related information.

As mentioned above there are several specific cases of association mechanisms that associate a data resource with the location LUN of the data resource. Specific cases may differ along one or more of the following dimensions 

The application s executed by the user VMs may be conventional user level applications such as a web server database application simulation tool etc. These access data of so called virtual disks that are presented by the hypervisor . The hypervisor itself employs the FS LVM and devices of as the underlying real physical storage. The MP DM plugin is a component working in conjunction with other storage device driver code not shown of the hypervisor to carry out the reading and writing of data from to the devices in operation of the FS LVM . The MP DM plugin may provide specialized and or enhanced input output functionality with respect to the devices . For example it may include multipathing functionality and an ability to access individual LUNs via multiple paths using the paths for increased performance and or availability. An example of such multipathing functionality is that provided by the PowerPath VE product sold by EMC Corporation.

The migration application is a specialized application providing the functionality of the migration controller described above. In particular the migration application carries out higher level logic and user facing functionality of migration. For example it may provide the above mentioned command line interface or application programming interface API for interacting with a human or machine user that exercises control over a migration process. In operation it communicates with the MP DM plugin of each host to cause the MP DM plugin of each host to perform lower level operations pertaining to migration. One example is the above discussed duplication of writes used to maintain synchronization between the source LUN S and the target LUN T. Another is to prevent access to the target LUN T prior to the migration becoming committed as well as preventing access to the source LUN S once the migration has become committed. The migration VM of a given host may call directly to the other hosts through a so called common information model object manager or CIMOM. More generically a call may be made through a listener employing a migration specific component that handles function invocations used to provide commands to the MP DM plugin at each migration step. Overall communication is done by the migration VM invoking each kernel action on each host as needed through each individual host s listener.

The hypervisor in a host such as shown in may employ a construct known as a datastore in its use of storage devices . This construct is used by ESX for example. A datastore is an organized container usable by the FS LVM to store files which implement virtual disks presented to the user VMs . Each datastore is assigned a unique signature which includes information identifying the device on which the datastore resides. A signature is a set of information metadata written on the device itself. For ESX it includes a device identifier associated with the device by the physical storage system array and which is returned in response to a SCSI extended inquiry command for vital product data VPD page 0x83 for example. ESX writes this SCSI property onto the device on which the datastore resides. When ESX accesses the datastore it checks the signature included in the datastore against the device identifier as returned by the SCSI extended inquiry command for VPD page 0x83 for the device on which the datastore resides. The values must match in order for the access to be handled normally. If there is a mismatch ESX treats it as an error condition and does not complete the access. This is done to prevent a replica of the datastore being used in place of the actual datastore.

It will be appreciated that the datastore signature as described above is a location dependent association mechanism associating the datastore with the device where the datastore is located. When a datastore is copied from a source LUN S to a target LUN T the existing signature is also copied so that the signature for the datastore as residing on the target LUN T identifies the source LUN S instead of the target LUN T. If the datastore were accessed from the target LUN T in this condition the signature checking would fail because the signature having the device identifier for the source LUN S does not match the device identifier for the target LUN T on which the datastore now resides. In order for the migration to be fully completed the signature must be changed to include the device identifier of the target LUN T instead of the device identifier of the source LUN S.

In one embodiment the disruptive process of may be used along with a separate resignaturing process that is performed while the migration is in the Committed and Redirected state . More comprehensively the following sequence of actions may be performed 

In other embodiments another process may be used to effect resignaturing without requiring the stopping and starting of the pertinent VMs . In such a case the non disruptive process of or may be used. It should also be noted that in other types of systems notably the Windows operating system from Microsoft signatures are also employed but they do not include physical device identifying information and thus are location independent. In this case also a non disruptive process such as that of or may be used because the signature is copied to the target LUN T as part of the migration.

While the above description focuses on use of signatures in particular in the system of in alternative embodiments other associating mechanisms may be used and are handled in a corresponding manner. For example the system may use device names such as described below and in that case operations the same or similar to those described below may be used.

A filter driver is a component working in conjunction with a standard device driver not shown as part of an operating system that implements the system calls reading and writing data from to the user devices via host bus adapters HBAs as requested by the applications . The filter driver may provide specialized and or enhanced input output functionality with respect to the user devices . For example in one embodiment the filter driver may be a multipathing driver having an ability to access individual LUNs via multiple paths and it manages the use of the paths for increased performance and or availability. An example of a multipathing driver is the above mentioned PowerPath driver.

The migration tool contains functionality for data migration operations. The user level part carries out higher level logic under control of the migration controller . A kernel level part of the migration tool performs lower level operation such as that provided by MP DM plugin . The kernel level part may be realized in one embodiment as an extension component having a defined interface to a basic or core set of components of the filter driver .

It should be noted that the applications may be virtual machines that contain user applications. Also referring back to although not shown there is a kernel space and there may be a user space.

As previously indicated the migration controller is a single point of control for migrations. It can reside in an off host computerized device appliance or in one of the hosts in the cluster . When the migration controller is realized in an off host migration appliance then a migration is initiated at the appliance and the commands executed on each host are subordinate commands. The above described flows may be augmented as follows. During setup a cluster flag may be added to the Setup command that lists the hosts in the cluster . This command is executed on the appliance. The appliance in turn calls Setup cluster on each host with another flag sub subordinate . This is a system level command for communications between the appliance and hosts not available to users. Alternatively the API on the host may be invoked with the same information. The sub flag indicates to the host receiving the command that the host is only doing host specific setup actions and not setting up the overall migration which happens only once from the appliance. For the Synchronize command the user runs it on the appliance. The appliance in turn invokes the Synchronize command or API on each host . Each host sets up the host specific state for the synchronization. Once all hosts have successfully performed these tasks the appliance code invokes the technology specific code to start the copy operation. The commit operation may require multiple staged operations at the hosts i.e. the four commit phases discussed above with reference to . As for the other commands during the Commit command the appliance is the only place that technology specific code is called. Cleanup follows the same pattern as synchronize but there s no technology specific code. The Undo Redirect command if used also works like synchronize because it is assumed that the application using the target LUN T is not running when that command executes.

The above referenced U.S. Pat. No. 7 904 681 provides two examples of association mechanisms that associate application visible data with a particular LUN. In one case applications including a file system or logical volume manager are each configured with a native name of the source LUN S and each uses this name in all I O commands for the associated data e.g. database records . In this case the minimally disruptive process of may be used for migration and during the Committed and Redirected state all applications using such a native name are stopped and reconfigured with the native name of the target LUN T. These application are then restarted after the Undo Redirect command is received and the migration is in the Committed state . In another case applications are configured with a so called pseudoname that is mapped by lower level driver components to a particular LUN. In the present context this mapping will initially associate the pseudoname with an identifier of the source LUN S. The non disruptive process of or can be used and the mapping changed as part of the transition into the Committed state as described above. While these association mechanisms and operations are described with reference to a host similar to host they may also be used in a host of the type shown in .

While various embodiments of the invention have been particularly shown and described it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined by the appended claims.

