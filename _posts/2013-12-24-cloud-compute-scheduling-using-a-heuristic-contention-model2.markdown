---

title: Cloud compute scheduling using a heuristic contention model
abstract: Technologies for contention-aware cloud compute scheduling include a number of compute nodes in a cloud computing cluster and a cloud controller. Each compute node collects performance data indicative of cache contention on the compute node, for example, cache misses per thousand instructions. Each compute node determines a contention score as a function of the performance data and stores the contention score in a cloud state database. In response to a request for a new virtual machine, the cloud controller receives contention scores for the compute nodes and selects a compute node based on the contention score. The cloud controller schedules the new virtual machine on the selected compute node. The contention score may include a contention metric and a contention score level indicative of the contention metric. The contention score level may be determined by comparing the contention metric to a number of thresholds. Other embodiments are described and claimed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09614779&OS=09614779&RS=09614779
owner: Intel Corporation
number: 09614779
owner_city: Santa Clara
owner_country: US
publication_date: 20131224
---
This application is a national stage entry under 35 USC 371 b of International Application No. PCT US2013 077659 which was filed Dec. 24 2013.

Cloud computing refers to highly scalable networked computing systems capable of delivering elastic computing performance to numerous users. Cloud computing typically involves clusters of densely packed computing servers called nodes with each node potentially executing dozens of virtual machines. Typically each node includes a hypervisor or other virtualization framework and the entire cloud computing cluster includes one or more cloud controllers that manage instantiation of virtual machines on the particular compute nodes. OpenStack is one example of such a cloud computing framework.

In a multi tenant cloud computing environment the virtual machines on a particular node may be controlled by different customers. Thus the resources of the node such as processor network and storage resources must be shared among the virtual machines and thus among different customers. Cloud computing customers typically expect and may be sold a predictable performance level for each virtual machine in the cloud computing cluster. However the performance of a virtual machine may be impacted by the behavior of other virtual machines on the same node. This is sometimes called the noisy neighbor problem that is some virtual machine workloads have side effects that impact the performance of other virtual machines executing on the same node. In addition to the outright reduction in performance the noisy neighbor problem may cause virtual machine performance to be unpredictable which can be particularly undesirable for cloud customers and providers.

While the concepts of the present disclosure are susceptible to various modifications and alternative forms specific embodiments thereof have been shown by way of example in the drawings and will be described herein in detail. It should be understood however that there is no intent to limit the concepts of the present disclosure to the particular forms disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives consistent with the present disclosure and the appended claims.

References in the specification to one embodiment an embodiment an illustrative embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may or may not necessarily include that particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is submitted that it is within the knowledge of one skilled in the art to effect such feature structure or characteristic in connection with other embodiments whether or not explicitly described. Additionally it should be appreciated that items included in a list in the form of at least one of A B and C can mean A B C A and B A and C B and C or A B and C . Similarly items listed in the form of at least one of A B or C can mean A B C A and B A and C B and C or A B and C .

The disclosed embodiments may be implemented in some cases in hardware firmware software or any combination thereof. The disclosed embodiments may also be implemented as instructions carried by or stored on one or more transitory or non transitory machine readable e.g. computer readable storage media which may be read and executed by one or more processors. A machine readable storage medium may be embodied as any storage device mechanism or other physical structure for storing or transmitting information in a form readable by a machine e.g. a volatile or non volatile memory a media disc or other media device .

In the drawings some structural or method features may be shown in specific arrangements and or orderings. However it should be appreciated that such specific arrangements and or orderings may not be required. Rather in some embodiments such features may be arranged in a different manner and or order than shown in the illustrative figures. Additionally the inclusion of a structural or method feature in a particular figure is not meant to imply that such feature is required in all embodiments and in some embodiments may not be included or may be combined with other features.

Referring now to in an illustrative embodiment a system for contention aware cloud compute scheduling includes a number of compute nodes a cloud controller and a cloud state database all in communication over a network . In use as discussed in more detail below each compute node instantiates one or more virtual machines to perform computational tasks. Each compute node monitors its performance including cache contention and generates a contention score. The contention scores for each compute node are stored in the cloud state database . The cloud controller schedules new virtual machines based on the reported contention scores of each of the compute nodes . Contention aware cloud scheduling may allow for improved throughput by scheduling compatible workloads onto the same physical node. Additionally contention aware cloud scheduling may improve the predictability and or stability of performance for cloud workloads. Although the illustrative embodiments involve monitoring socket level cache contention it should be understood that the technologies of this disclosure may apply to other contention domains of the compute nodes such as socket contention core contention disk contention or network contention.

Referring now to typical cloud compute systems may suffer from the so called noisy neighbor problem. In particular performance of a virtual machine may be reduced by contention for non partitionable resources of the compute node from other virtual machines executing on the same compute node. Chart illustrates performance degradation that may occur due to multiple virtual machines executing on the same compute node. Curve illustrates instructions per cycle for a virtual machine executing on a compute node which is a measure of throughput. Curve illustrates cache misses per thousand instructions MPKI for the compute node which is a measure of cache contention. In the illustration eight virtual machines each running an identical computational benchmark are instantiated sequentially with each virtual machine pinned to a particular processor core. It would be expected that the throughput of each virtual machine would remain unchanged at least until the number of virtual machines exceeds the number of physical cores in the compute node . However in the illustrative example as each virtual machine is instantiated throughput drops. As shown by illustrative curve throughput steps down as each virtual machine is instantiated. Curve illustrates that cache contention also steps up as each virtual machine is instantiated. After all eight virtual machines are instantiated the compute node reaches steady state and the curves level off. Note that throughput for each virtual machine has been significantly reduced compared to execution of a single virtual machine by itself. Next the eight virtual machines are stopped sequentially and as each virtual machine is stopped throughput steps up as shown in curve and contention steps down as shown in curve . Therefore in a traditional cloud compute system throughput achieved by a virtual machine may be affected by workloads executing in a different virtual machine on a different physical core of the same compute node.

Referring back to each compute node may be embodied as any type of computation or computer device capable of performing the functions described herein including without limitation a computer a multiprocessor system a server a rack mounted server a blade server a laptop computer a notebook computer a network appliance a web appliance a distributed computing system a processor based system and or a consumer electronic device. As shown in each compute node illustratively includes two processors an input output subsystem a memory a data storage device and communication circuitry . Of course the compute node may include other or additional components such as those commonly found in a server device e.g. various input output devices in other embodiments. Additionally in some embodiments one or more of the illustrative components may be incorporated in or otherwise form a portion of another component. For example the memory or portions thereof may be incorporated in one or more processor in some embodiments.

Each processor may be embodied as any type of processor capable of performing the functions described herein. Each illustrative processor is a multi core processor however in other embodiments each processor may be embodied as a single or multi core processor s digital signal processor microcontroller or other processor or processing controlling circuit. Each processor illustratively includes four processor cores and an uncore . Each of the processor cores is an independent processing unit capable of executing programmed instructions. Each processor core includes a performance monitoring unit PMU . Each PMU may be embodied as a number of performance counters capable of recording and monitoring the flow of instructions through the respective processor core . For example each PMU may be capable of counting clock cycles instructions issued instructions retired cache misses or similar events. The PMUs may be programmed to monitor particular performance statistics using model specific registers of the processor core . In one embodiment each PMU may include four fully programmable hardware counters and three fixed function hardware counters. Software may access the PMUs using a kernel interface such as the perf subsystem of the Linux kernel. Although each of the illustrative compute nodes includes two processors having four processor cores each compute node may include one two or more processors having one two or more processor cores each in other embodiments. In particular this disclosure is also applicable to uniprocessor or single core compute nodes .

Each processor also includes an uncore . In the illustrative embodiment each uncore includes any part of the particular processor not included in the processor cores e.g. all components of the particular processor except for the processor cores themselves . For example the uncore of each illustrative processor includes a PMU and cache memory . Similar to the PMUs of the processor cores the PMU monitors performance statistics of the uncore and may include a number of programmable or fixed function hardware performance counters. The cache memory may be a last level cache shared by the processor cores . In some embodiments the PMU may monitor accesses to the cache memory including recording cache misses amounts of data transferred and other cache information. Although not illustrated the uncore may additionally include typical components of a processor or a system on a chip. For example each uncore may include a memory controller processor graphics input output controllers power management circuitry or other components of the processor .

The memory may be embodied as any type of volatile or non volatile memory or data storage capable of performing the functions described herein. In operation the memory may store various data and software used during operation of the compute node such as operating systems applications programs libraries and drivers. The memory is communicatively coupled to the processor via the I O subsystem which may be embodied as circuitry and or components to facilitate input output operations with the processor the memory and other components of the compute node . For example the I O subsystem may be embodied as or otherwise include memory controller hubs input output control hubs firmware devices communication links i.e. point to point links bus links wires cables light guides printed circuit board traces etc. and or other components and subsystems to facilitate the input output operations. In some embodiments the I O subsystem may form a portion of a system on a chip SoC and be incorporated along with the processor the memory and other components of the compute node on a single integrated circuit chip.

The data storage device may be embodied as any type of device or devices configured for short term or long term storage of data such as for example memory devices and circuits memory cards hard disk drives solid state drives or other data storage devices. The data storage device may store performance statistics monitored by the compute node .

The communication circuitry of the compute node may be embodied as any communication circuit device or collection thereof capable of enabling communications between the compute node the cloud controller the cloud state database and or other remote devices over the network . The communication circuitry may be configured to use any one or more communication technology e.g. wired or wireless communications and associated protocols e.g. Ethernet Bluetooth Wi Fi WiMAX etc. to effect such communication.

The cloud controller is configured to manage virtual machines or other compute instances distributed among the compute nodes of the system . The cloud controller may be embodied as any type of server computing device or collection of devices capable of performing the functions described herein. As such the cloud controller may be embodied as a single server computing device or a collection of servers and associated devices. For example in some embodiments the cloud controller may be embodied as a virtual server formed from multiple computing devices distributed across the network and operating in a public or private cloud. Accordingly although the cloud controller is illustrated in as embodied as a single server computing device it should be appreciated that the cloud controller may be embodied as multiple devices cooperating together to facilitate the functionality described below. Illustratively the cloud controller includes a processor an I O subsystem a memory a data storage device communication circuitry and or other components and devices commonly found in a server or similar computing device. Those individual components of the cloud controller may be similar to the corresponding components of the compute nodes the description of which is applicable to the corresponding components of the cloud controller and is not repeated herein so as not to obscure the present disclosure.

The cloud state database stores information that is synchronized across the system including performance statistics. The cloud state database may be embodied as a dedicated database server distributed data storage or any other data storage system capable of maintaining consistent state for the system . As such copies or portions of the cloud state database may be stored in the data storage of each compute node and or the data storage of the cloud controller . Updated cloud state information may be transferred between the compute nodes the cloud controller and or the cloud state database using any communication protocol. In some embodiments cloud state information may be transferred asynchronously using a message bus for example a message bus implementing the advanced message queuing protocol AMQP such as rabbitmq.

As discussed in more detail below the compute nodes the cloud controller and the cloud state database may be configured to transmit and receive data with each other and or other devices of the system over the network . The network may be embodied as any number of various wired and or wireless networks. For example the network may be embodied as or otherwise include a wired or wireless local area network LAN a wired or wireless wide area network WAN a cellular network and or a publicly accessible global network such as the Internet. As such the network may include any number of additional devices such as additional computers routers and switches to facilitate communications among the devices of the system .

Referring now to in an illustrative embodiment each compute node establishes an environment during operation. The illustrative environment includes a number of virtual machines a data collection module a contention score determination module and a communication module . The various modules of the environment may be embodied as hardware firmware software or a combination thereof.

Each virtual machine is configured to perform a cloud computing workload on the compute node . Each virtual machine may be pinned assigned or otherwise allocated to one or more of the processor cores . In some embodiments each virtual machine may specify a number of desired virtual CPUs and the virtual machine may be assigned to that number of processor cores . In a multi tenant cloud computing environment each virtual machine may be controlled by a different entity and therefore additionally may execute a workload having different performance characteristics. In particular each virtual machine may exert different pressure on the cache memory of the compute node . As described below the virtual machines are instantiated and otherwise managed by the cloud controller .

The data collection module is configured to collect performance data for the compute node . The data collection module may collect data indicative of cache contention on the compute node for example data measuring cache misses in the cache memory . The data collection module may include individual data collectors for each processor and or processor core of the compute node or for each virtual machine . The data collection module may filter the collected data to remove noise. The data collectors of the data collection module may communicate with each other asynchronously for example using a message bus such as the zeromq message bus.

The contention score determination module is configured to calculate a contention score as a function of the performance data collected by the data collection module . The contention score may include both a contention metric and a contention score level. The contention metric may include aggregated data describing cache misses for all processors of the compute node . The contention score level may be generated by a heuristic algorithm and may be selected from levels for low contention medium contention and high contention e.g. via a form of quantization . Calculation of the contention metric and the contention score level is further described below.

The communication module is configured to transmit the calculated contention score to the cloud state database . As described below the contention score is then accessible to the cloud controller through the cloud state database . The communication module may transmit the contention score as a tuple including the contention metric and the contention score level. The communication module may use any communication method to interface with the other members of the system . For example the communication module may be embodied as a message bus.

Still referring to each compute node communicates with the cloud controller which each may establish an environment during operation. The illustrative environment includes a compute service module a scheduler module and a communication module . The various modules of the environment may be embodied as hardware firmware software or a combination thereof.

The compute service module is configured to receive requests to instantiate a new virtual machine and to schedule the new virtual machine for execution on a compute node selected by the scheduler module . The compute service module may receive commands from any appropriate source. For example the compute service module may receive commands from a cloud system administrator or from a cloud computing customer through a web based control interface. As another example in some embodiments the compute service module may receive a command for a new virtual machine based on demand for computing services for example to scale to respond to application demand.

The scheduler module is configured to select the appropriate compute node based on the contention score data. In some embodiments the scheduler module may be embodied as a filter scheduler that selects a compute node having the lowest reported contention score level. When two or more compute nodes have the lowest reported contention score level the scheduler module may select one randomly or by using any other available selection criteria. For example in some embodiments the scheduler module may further sort the compute nodes and select the compute node having the lowest contention metric.

The communication module is configured to receive contention score data from the cloud state database for all compute nodes within the system . The communication module may receive a tuple from the cloud state database for each compute node containing the contention metric and the contention score level. The communication module may use any communication method to interface with the other members of the system . For example the communication module may be embodied as a message bus.

Referring now to in use each compute node may execute a method for collecting performance data. The method begins with block in which the compute node initializes performance data collection. The compute node may prepare data structures counters timers or other mechanisms to enable monitoring instruction flow and cache contention within the processors . In some embodiments in block the compute node may initialize the PMUs to begin monitoring performance data. For example in some embodiments performance data collection may be initialized for each virtual machine . That is the PMUs and or any other performance data collection systems may be initialized for every virtual machine instantiated on the compute node . To initialize the performance counters the compute node may use a software interface such as the Linux performance monitoring subsystem perf. 

In block the compute node collects performance data. The compute node may collect information on instruction flow within the processors and the processor cores including the number of instructions issued and or retired and the number of clock cycles. The compute node may also collect information on cache contention including the number of cache misses in the cache memory . In some embodiments in block the compute node may monitor performance by reading the per core PMUs and or the per processor PMUs . Additionally in some embodiments in block the compute node may associate collected performance data with the appropriate virtual machine executing on the compute node . As described above a typical PMU may include a limited number of hardware counters such as four programmable counters and three fixed function counters. Thus the compute node may multiplex the available hardware counters among several processes for example by sampling performance data for each process in discrete timeslices and storing the hardware counts in memory and or data storage when not monitoring. Associating performance data with the virtual machines may for example allow the compute node to disregard performance data for processor cores that are not executing virtual machines . The compute node may associate performance data with the appropriate virtual machine by tracking the lifecycle of the virtual machines i.e. when each virtual machine is started stopped suspended etc. . In some embodiments the compute node may track virtual machine lifecycle using a virtualization library such as libvirt.

In some embodiments in block the compute node may filter the collected performance data to remove noise. For example as described above the compute node may collect performance data by sampling the PMUs over relatively small timeslices. Thus sampling error may be introduced into the performance data. The sampling error typically occurs for one or two sample periods. The compute node may apply a quartile based outlier detection algorithm or other filtering algorithm to remove the erroneous samples and thereby clean the collected performance data. Additionally to prevent data corruption from random tasks other than virtual machines executing on processor cores the compute node may set performance data such as memory cache misses to zero for all processor cores that have not been allocated a virtual machine .

In block the compute node determines the contention score for the compute node based on the collected performance data. The contention score measures the cache memory contention experienced on the compute node . The contention score may be embodied as a tuple including the cache misses per some reference number of instructions e.g. per thousand instructions as well as a contention score level e.g. high medium or low contention . In block the compute node determines cache misses per thousand instructions MPKI . The compute node may determine MPKI by dividing the number of misses to the last level cache read from a PMU by the number of instructions retired also read from that PMU and multiplying the result by one thousand. The compute node may calculate MPKI on a per core or a per processor basis depending on the performance data available from the PMUs . In block the compute node aggregates MPKI for all physical sockets. That is the compute node combines the MPKI values for all processors and or processor cores to determine an aggregate MPKI value for the compute node . In some embodiments aggregation of MPKI values may be performed by a performance monitoring subsystem such as the perf subsystem. Although the illustrative embodiment calculates MPKI in other embodiments the compute node may calculate the cache misses per some other reference number of instructions e.g. cache misses per instruction cache misses per hundred instructions and so on . In block the compute node determines a contention score level e.g. high medium or low for the aggregated MPKI value based on a set of threshold values. In other words the compute node quantizes the MPKI value to one of a number of preset contention score levels. The threshold values may be determined for example based on a heuristic contention model of the compute node historical performance data or any other criteria. One embodiment of a method for determining the contention score level is described below in connection with .

In block the compute node stores the contention score in the cloud state database . The compute node may use any data connection or data protocol to update the contention score. In some embodiments in block the compute node transmits a tuple containing the MPKI value and the contention score level to the cloud state database . The compute node may transmit the tuple over a message bus for example in some embodiments the rabbitmq message bus. After storing the contention score the method loops back to block to continue collecting performance data for the compute node .

Referring now to in use each compute node may execute a method for determining a contention score level. The method may be used as method for determining the contention score level as described above with respect to block of .

The method begins in block in which the compute node determines whether the measured cache misses per thousand instructions MPKI value or per other reference number of instructions is less than a low threshold. The low threshold may be determined through for example empirical modeling of the behavior of virtual machines on the compute node . In some embodiments the low threshold may be 3 MPKI. If the MPKI value is greater than or equal to the low threshold the method branches to block described below. If the MPKI value is less than the low threshold the method branches to block in which the contention score level is set to a low contention level. After assigning the contention level the method is completed.

As described above if the MPKI value is greater than or equal to the low threshold the method branches to block . In block the compute node determines whether the measured MPKI value is greater than or equal to a high threshold. Similar to the low threshold the high threshold may be determined for example through empirical modeling of the behavior of virtual machines on the compute node . In some embodiments the high threshold may be 50 MPKI. If the MPKI value is greater than or equal to the high threshold the method branches to block described below. If the MPKI value is less than the high threshold the method branches to block in which the contention score level is set to a medium contention level. After assigning the contention level the method is completed.

Referring back to block if the MPKI value is greater than or equal to the high threshold the method branches to block . In block the contention score level is set to a high contention level. After assigning the contention level the method is completed. Although the method illustrates selecting between three content score levels i.e. high medium and low contention it should be understood that in other embodiments different granularities may be used to quantize or otherwise set the content score levels. For example some embodiments may select between two content score levels e.g. low and high contention some embodiments may select between five content score levels and so on.

Referring now to in use the cloud controller may execute a method for contention aware compute scheduling. The method begins in block in which the cloud controller monitors for requests for new virtual machine instances. The requests for virtual machine instances may be received through a cloud compute application programming interface and may be generated by a user request or by an automated request. For example new virtual machine instances may be requested by a cloud customer or a cloud administrator to meet increased demand for an application service. In block the cloud controller determines whether a new virtual machine has been requested. If not the method loops back to block to continue monitoring for virtual machine requests. If a new virtual machine has been requested the method advances to block to schedule the new virtual machine .

In block the cloud controller identifies all available compute nodes in the cloud system . The cloud controller may use any technique available to identify available nodes. For example the cloud controller may query the cloud state database for information on available nodes.

In some embodiments in block the cloud controller may filter out any compute nodes that lack capacity for a new virtual machine . For example the cloud controller may filter out compute nodes on which a threshold number of virtual machines and or virtual CPUs have already been instantiated. The threshold number of virtual machines and or virtual CPUs may be set to the number of processor cores included in the compute node or to the number of processor cores plus a predefined overcommit factor. In some embodiments the cloud controller may filter out compute nodes that have fewer available processor cores than the number of virtual CPUs requested by the new virtual machine . Filtering may be performed by a filter scheduler of a cloud compute framework such as nova.

In block the cloud controller obtains a contention score for each remaining compute node from the cloud state database . As described above the contention score may be embodied as a tuple including a cache miss per thousand or other reference number instructions MPKI value and a contention score level. The cloud controller may use any available technique to query the cloud state database . For example the cloud controller may communicate with the cloud state database over a message bus such as the rabbitmq message bus.

In block the cloud controller selects a compute node based on contention score level. In some embodiments in block the cloud controller may select a compute node having the lowest reported contention score level. For example the cloud controller may select any compute node having the low contention level. When no compute nodes are available with the low contention level the cloud controller may select any compute node having the medium contention level and so on. The cloud controller may use any criteria to select from among compute nodes having the lowest available contention score level such as random selection round robin selection or other selection criteria. In some embodiments in block the cloud controller may sort the compute nodes to select the compute node having the lowest MPKI value that is the lowest measured cache contention.

In block the cloud controller schedules a new virtual machine on the selected compute node . The cloud controller may use any protocol or technique to schedule the new virtual machine . For example the cloud controller may schedule the virtual machine using a cloud computing framework such as nova. In some embodiments the cloud controller may communicate with a hypervisor or other virtualization framework of the selected compute node to instantiate the new virtual machine for example through an application programming interface. After scheduling the new virtual machine the method loops back to block to monitor for additional requests to schedule virtual machines .

Illustrative examples of the technologies disclosed herein are provided below. An embodiment of the technologies may include any one or more and any combination of the examples described below.

Example 1 includes a cloud controller of a cloud computing cluster the cloud controller comprising a compute service module to receive a request for a new virtual machine a communication module to receive a contention score for each compute node of a plurality of compute nodes of the cloud computing cluster the contention score to indicate cache memory contention of the compute node and a scheduler module to select a compute node based on the contention score wherein the compute service module is further to schedule the new virtual machine on the selected compute node.

Example 2 includes the subject matter of Example 1 and wherein the contention score comprises a contention metric the contention metric being a function of a number of cache misses of the compute node.

Example 3 includes the subject matter of any of Examples 1 and 2 and wherein the contention metric comprises a rate of cache misses per reference number of instructions of the compute node.

Example 4 includes the subject matter of any of Examples 1 3 and wherein the rate of cache misses per reference number of instructions comprises the rate aggregated for all processor cores of the compute node.

Example 5 includes the subject matter of any of Examples 1 4 and wherein to select the compute node comprises to sort the plurality of compute nodes based on the contention metric to select a compute node having the lowest contention metric.

Example 6 includes the subject matter of any of Examples 1 5 and wherein the contention score comprises a contention score level indicative of a contention metric of the compute node.

Example 7 includes the subject matter of any of Examples 1 6 and wherein the contention core level comprises a contention score level being a quantization of the contention metric.

Example 8 includes the subject matter of any of Examples 1 7 and wherein the contention score level comprises a contention score level being determined by a comparison of a rate of cache misses of the compute node per reference number of instructions of the compute node to one or more threshold values.

Example 9 includes the subject matter of any of Examples 1 8 and wherein the contention score level comprises a low contention level indicative of the rate of cache misses per reference number of instructions being less than a low contention threshold value a medium contention level indicative of the rate of cache misses per reference number of instructions being greater than or equal to the low contention threshold value and less than a high contention threshold value or a high contention level indicative of the rate of cache misses per reference number of instructions being greater than or equal to the high contention threshold value.

Example 10 includes the subject matter of any of Examples 1 9 and wherein the low contention threshold value comprises 3 cache misses per thousand instructions and wherein the high contention threshold value comprises 50 cache misses per thousand instructions.

Example 11 includes the subject matter of any of Examples 1 10 and wherein to select the compute node comprises to select a compute node having the lowest contention score level of the plurality of compute nodes.

Example 12 includes the subject matter of any of Examples 1 11 and wherein the scheduler module is further to filter the plurality of compute nodes to remove any compute nodes lacking capacity for the new virtual machine prior to receipt of the contention score.

Example 13 includes a compute node of a cloud computing cluster the compute node comprising a data collection module to collect performance data indicative of cache memory contention of the compute node a contention score determination module to determine a contention score as a function of the performance data the contention score to include a contention metric and a contention score level indicative of the contention metric and a communication module to store the contention score in a cloud state database the cloud state database accessible by a cloud controller of the cloud computing cluster.

Example 14 includes the subject matter of Example 13 and further including a processor core including a performance monitoring unit wherein to collect the performance data comprises to read the performance monitoring unit of the processor core.

Example 15 includes the subject matter of any of Examples 13 and 14 and further including a processor including an uncore the uncore including a performance monitoring unit wherein to collect the performance data comprises to read the performance monitoring unit of the uncore of the processor.

Example 16 includes the subject matter of any of Examples 13 15 and wherein to collect the performance data comprises to associate the performance data with one or more virtual machines executed by the compute node.

Example 17 includes the subject matter of any of Examples 13 16 and wherein the data collection module is further to filter the performance data to remove noise prior to determination of the contention score.

Example 18 includes the subject matter of any of Examples 13 17 and wherein to determine the contention score comprises to determine the contention metric as a function of a number of cache misses of the compute node.

Example 19 includes the subject matter of any of Examples 13 18 and wherein to determine the contention metric comprises to determine a rate of cache misses per reference number of instructions.

Example 20 includes the subject matter of any of Examples 13 19 and wherein to determine the contention metric further comprises to aggregate the rate of cache misses per reference number of instructions for all processor cores of the compute node.

Example 21 includes the subject matter of any of Examples 13 20 and wherein to determine the contention score comprises to determine the contention score level by quantization of the contention metric.

Example 22 includes the subject matter of any of Examples 13 21 and wherein to determine the contention score comprises to determine the contention score level by comparison of the rate of cache misses per reference number of instructions to one or more threshold values.

Example 23 includes the subject matter of any of Examples 13 22 and wherein to determine the contention score level further comprises to compare the rate of cache misses per reference number of instructions to a low contention threshold value set the contention score level to a low contention level in response to a determination that the rate is less than the low contention threshold value compare the rate of cache misses per reference number of instructions to a high contention threshold value set the contention score level to a medium contention level in response to a determination that the rate is greater than or equal to the low contention threshold value and less than the high contention threshold value and set the contention score level to a high contention level in response to a determination that the rate is greater than or equal to the high contention threshold value.

Example 24 includes the subject matter of any of Examples 13 23 and wherein the low contention threshold value comprises 3 cache misses per thousand instructions and wherein the high contention threshold value comprises 50 cache misses per thousand instructions.

Example 25 includes a method for cloud compute scheduling the method comprising receiving by a cloud controller of a cloud computing cluster a request for a new virtual machine receiving by the cloud controller a contention score for each compute node of a plurality of compute nodes of the cloud computing cluster the contention score indicative of cache memory contention of the compute node selecting by the cloud controller a compute node based on the contention score and scheduling by the cloud controller the new virtual machine on the selected compute node.

Example 26 includes the subject matter of Example 25 and wherein receiving the contention score comprises receiving a contention metric the contention metric being a function of a number of cache misses of the compute node.

Example 27 includes the subject matter of any of Examples 25 and 26 and wherein receiving the contention metric comprises receiving a rate of cache misses per reference number of instructions of the compute node.

Example 28 includes the subject matter of any of Examples 25 27 and wherein receiving the rate of cache misses per reference number of instructions of the compute node comprise receiving the rate aggregated for all processor cores of the compute node.

Example 29 includes the subject matter of any of Examples 25 28 and wherein selecting the compute node comprises sorting by the cloud controller the plurality of compute nodes based on the contention metric to select a compute node having the lowest contention metric.

Example 30 includes the subject matter of any of Examples 25 29 and wherein receiving the contention score comprises receiving a contention score level indicative of a contention metric of the compute node.

Example 31 includes the subject matter of any of Examples 25 30 and wherein receiving the contention core level comprises receiving a contention score level being a quantization of the contention metric.

Example 32 includes the subject matter of any of Examples 25 31 and wherein receiving the contention score level comprises receiving a contention score level the contention score level being determined by comparing a rate of cache misses of the compute node per reference number of instructions of the compute node to one or more threshold values.

Example 33 includes the subject matter of any of Examples 25 32 and wherein receiving the contention score level comprises receiving a low contention level indicating the rate of cache misses per reference number of instructions is less than a low contention threshold value receiving a medium contention level indicating the rate of cache misses per reference number of instructions is greater than or equal to the low contention threshold value and less than a high contention threshold value or receiving a high contention level indicating the rate of cache misses per reference number of instructions is greater than or equal to the high contention threshold value.

Example 34 includes the subject matter of any of Examples 25 33 and wherein the low contention threshold value comprises 3 cache misses per thousand instructions and wherein the high contention threshold value comprises 50 cache misses per thousand instructions.

Example 35 includes the subject matter of any of Examples 25 34 and wherein selecting the compute node comprises selecting a compute node having the lowest contention score level of the plurality of compute nodes.

Example 36 includes the subject matter of any of Examples 25 35 and further including filtering by the cloud controller the plurality of compute nodes to remove any compute nodes lacking capacity for the new virtual machine prior to receiving the contention score.

Example 37 includes a method for contention monitoring for cloud computing the method comprising collecting by a compute node of a cloud computing cluster performance data indicative of cache memory contention of the compute node determining by the compute node a contention score as a function of the performance data the contention score including a contention metric and a contention score level indicative of the contention metric and storing by the compute node the contention score in a cloud state database the cloud state database accessible by a cloud controller of the cloud computing cluster.

Example 38 includes the subject matter of Example 37 and wherein collecting the performance data comprises reading a performance monitoring unit of a processor core of the compute node.

Example 39 includes the subject matter of any of Examples 37 and 38 and wherein collecting the performance data comprises reading a performance monitoring unit of an uncore of a processor of the compute node.

Example 40 includes the subject matter of any of Examples 37 39 and wherein the vehicle configuration management module to determine a vehicle identifier for the vehicle and retrieve the user vehicle configuration settings from the data storage wherein collecting the performance data comprises associating the performance data with one or more virtual machines executed by the compute node.

Example 41 includes the subject matter of any of Examples 37 40 and further including filtering by the compute node the performance data to remove noise prior to determining the contention score.

Example 42 includes the subject matter of any of Examples 37 41 and wherein determining the contention score comprises determining the contention metric as a function of a number of cache misses of the compute node.

Example 43 includes the subject matter of any of Examples 37 42 and wherein determining the contention metric comprises determining a rate of cache misses per reference number of instructions.

Example 44 includes the subject matter of any of Examples 37 43 and wherein determining the contention metric further comprises aggregating the rate of cache misses per reference number of instructions for all processor cores of the compute node.

Example 45 includes the subject matter of any of Examples 37 44 and wherein determining the contention score comprises determining the contention score level by quantizing the contention metric.

Example 46 includes the subject matter of any of Examples 37 45 and wherein determining the contention score comprises determining the contention score level by comparing the rate of cache misses per reference number of instructions to one or more threshold values.

Example 47 includes the subject matter of any of Examples 37 46 and wherein determining the contention score level further comprises comparing the rate of cache misses per reference number of instructions to a low contention threshold value setting the contention score level to a low contention level in response to determining that the rate is less than the low contention threshold value comparing the rate of cache misses per reference number of instructions to a high contention threshold value setting the contention score level to a medium contention level in response to determining that the rate is greater than or equal to the low contention threshold value and less than the high contention threshold value and setting the contention score level to a high contention level in response to determining that the rate is greater than or equal to the high contention threshold value.

Example 48 includes the subject matter of any of Examples 37 47 and wherein the low contention threshold value comprises 3 cache misses per thousand instructions and wherein the high contention threshold value comprises 50 cache misses per thousand instructions.

Example 49 includes a computing device comprising a processor and a memory having stored therein a plurality of instructions that when executed by the processor cause the computing device to perform the method of any of Examples 25 48.

Example 50 includes one or more machine readable storage media comprising a plurality of instructions stored thereon that in response to being executed result in a computing device performing the method of any of Examples 25 48.

Example 51 includes a computing device comprising means for performing the method of any of Examples 25 48.

Example 52 includes a cloud controller of a cloud computing cluster the cloud controller including means for receiving a request for a new virtual machine means for receiving a contention score for each compute node of a plurality of compute nodes of the cloud computing cluster the contention score indicative of cache memory contention of the compute node means for selecting a compute node based on the contention score and means for scheduling the new virtual machine on the selected compute node.

Example 53 includes the subject matter of Example 52 and wherein the means for receiving the contention score comprises means for receiving a contention metric the contention metric being a function of a number of cache misses of the compute node.

Example 54 includes the subject matter of any of Examples 52 and 53 and wherein the means for receiving the contention metric comprises means for receiving a rate of cache misses per reference number of instructions of the compute node.

Example 55 includes the subject matter of any of Examples 52 54 and wherein the means for receiving the rate of cache misses per reference number of instructions of the compute node comprise means for receiving the rate aggregated for all processor cores of the compute node.

Example 56 includes the subject matter of any of Examples 52 55 and wherein the means for selecting the compute node comprises means for sorting the plurality of compute nodes based on the contention metric to select a compute node having the lowest contention metric.

Example 57 includes the subject matter of any of Examples 52 56 and wherein the means for receiving the contention score comprises means for receiving a contention score level indicative of a contention metric of the compute node.

Example 58 includes the subject matter of any of Examples 52 57 and wherein the means for receiving the contention core level comprises means for receiving a contention score level being a quantization of the contention metric.

Example 59 includes the subject matter of any of Examples 52 58 and wherein the means for receiving the contention score level comprises means for receiving a contention score level the contention score level being determined by comparing a rate of cache misses of the compute node per reference number of instructions of the compute node to one or more threshold values.

Example 60 includes the subject matter of any of Examples 52 59 and wherein the means for receiving the contention score level comprises means for receiving a low contention level indicating the rate of cache misses per reference number of instructions is less than a low contention threshold value means for receiving a medium contention level indicating the rate of cache misses per reference number of instructions is greater than or equal to the low contention threshold value and less than a high contention threshold value or means for receiving a high contention level indicating the rate of cache misses per reference number of instructions is greater than or equal to the high contention threshold value.

Example 61 includes the subject matter of any of Examples 52 60 and wherein the low contention threshold value comprises 3 cache misses per thousand instructions and wherein the high contention threshold value comprises 50 cache misses per thousand instructions.

Example 62 includes the subject matter of any of Examples 52 61 and wherein the means for selecting the compute node comprises means for selecting a compute node having the lowest contention score level of the plurality of compute nodes.

Example 63 includes the subject matter of any of Examples 52 62 and further means for comprising filtering the plurality of compute nodes to remove any compute nodes lacking capacity for the new virtual machine prior to receiving the contention score.

Example 64 includes a compute node of a cloud computing cluster the compute node including means for collecting performance data indicative of cache memory contention of the compute node means for determining a contention score as a function of the performance data the contention score including a contention metric and a contention score level indicative of the contention metric and means for storing the contention score in a cloud state database the cloud state database accessible by a cloud controller of the cloud computing cluster.

Example 65 includes the subject matter of Example 64 and wherein the means for collecting the performance data comprises means for reading a performance monitoring unit of a processor core of the compute node.

Example 66 includes the subject matter of any of Examples 64 and 65 and wherein the means for collecting the performance data comprises means for reading a performance monitoring unit of an uncore of a processor of the compute node.

Example 67 includes the subject matter of any of Examples 64 66 and wherein the means for collecting the performance data comprises means for associating the performance data with one or more virtual machines executed by the compute node.

Example 68 includes the subject matter of any of Examples 64 67 and further comprising means for filtering the performance data to remove noise prior to determining the contention score.

Example 69 includes the subject matter of any of Examples 64 68 and wherein the means for determining the contention score comprises means for determining the contention metric as a function of a number of cache misses of the compute node.

Example 70 includes the subject matter of any of Examples 64 69 and wherein the means for determining the contention metric comprises means for determining a rate of cache misses per reference number of instructions.

Example 71 includes the subject matter of any of Examples 64 70 and wherein the means for determining the contention metric further comprises means for aggregating the rate of cache misses per reference number of instructions for all processor cores of the compute node.

Example 72 includes the subject matter of any of Examples 64 71 and wherein the means for determining the contention score comprises means for determining the contention score level by quantizing the contention metric.

Example 73 includes the subject matter of any of Examples 64 72 and wherein the means for determining the contention score comprises means for determining the contention score level by comparing the rate of cache misses per reference number of instructions to one or more threshold values.

Example 74 includes the subject matter of any of Examples 64 73 and wherein the means for determining the contention score level further comprises means for comparing the rate of cache misses per reference number of instructions to a low contention threshold value means for setting the contention score level to a low contention level in response to determining that the rate is less than the low contention threshold value means for comparing the rate of cache misses per reference number of instructions to a high contention threshold value means for setting the contention score level to a medium contention level in response to determining that the rate is greater than or equal to the low contention threshold value and less than the high contention threshold value and means for setting the contention score level to a high contention level in response to determining that the rate is greater than or equal to the high contention threshold value.

Example 75 includes the subject matter of any of Examples 64 74 and wherein the low contention threshold value comprises 3 cache misses per thousand instructions and wherein the high contention threshold value comprises 50 cache misses per thousand instructions.

