---

title: Automated root cause analysis
abstract: Various aspects of the performance of computing resources, such as storage volumes, are measured and used to train a probability model. The probability model is used in a query engine that is able to respond receive queries about a computing resource's state. The queries may specify a state of the computing resource and provide a set of measurements of the computing resource's performance. The query engine may use the probability model, which may be in the form of a contingency table, to provide information that indicates one or more most likely causes of the state.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09104572&OS=09104572&RS=09104572
owner: Amazon Technologies, Inc.
number: 09104572
owner_city: Reno
owner_country: US
publication_date: 20130211
---
Modern computer systems are often made of multiple components that work collectively each playing a role in a system s overall operation. The components may include servers data storage devices networking devices software components and other pieces of hardware and or software that enable the system to operate. These components may be distributed throughout a data center and or even throughout multiple data centers. The transfer of information from one component to the other can involve the participation of multiple other components. For instance an electronic communication from a client to a server may pass from one programming module to another within a computer system over a network and or from one programming module to another on another computer system. While on the network the electronic communication may pass through numerous components on the network such as firewalls routers switches and or other networking devices.

In many contexts computing devices often send and receive numerous communications to and from other computing devices over various time periods. An application on a server may for instance send numerous input output operation requests to a data storage device located in a different location on a network. The application s effective operation may require effective communication with the data storage device. At the same time a degradation in performance by a device participating in the communications may cause the effectiveness of the communications to decrease or cause the communications to cease. A malfunctioning router for example may cause a decrease or halt to the ability of a computer system to communicate with a data storage device. Diagnosing and repairing such problems can be very difficult given the complexities of the various devices involved.

In the following description various embodiments will be described. For purposes of explanation specific configurations and details are set forth in order to provide a thorough understanding of the embodiments. However it will also be apparent to one skilled in the art that the embodiments may be practiced without the specific details. Furthermore well known features may be omitted or simplified in order not to obscure the embodiment being described.

Various embodiments of the present disclosure relate to the diagnosis of issues for computing resources. In some examples the computing resources are storage volumes in a distributed computing environment although the techniques described and suggested herein may be modified to be used in connection with computing resources of different types. The issues may relate to the ability of computer system instances to communicate with the storage volumes to perform input output operations per second IOPS . In many environments IOPS are complex processes involving numerous sub processes sub operations including the transmission of communications within a physical computing device the transmission of communications over a network and the performance of other operations such as read and or write operations on a data storage medium. As a result a decreased ability to perform IOPS may have multiple possible causes rendering diagnosis of the decreased ability a complex task.

To improve the ability to diagnose issues relating to the operation of computing resources embodiments of the present disclosure include measuring the various processes involved in the operation of the computing resources. With the example of storage volumes various communication latencies and operational latencies may be measured and the measurements may be collected. In addition data regarding the overall performance of a computing resource may also be collected. For example IOPS may involve numerous individual communications between devices on a network and other operations such as transmitting requests to perform IOPS from a data storage client to a data storage server performing the requested operations and transmitting acknowledgments of the requested operations having been performed. The time it takes to transmit a request perform the requested operation and receive the acknowledgment can be measured as an overall indication of the ability to perform IOPS. This measurement may be taken numerous times over a time period and be normalized averaged and or otherwise statistically processed.

Measurements of the overall performance of a computing resource can be used to determine a health state of the computing resource. The health state may be a state from a finite set of predetermined health states each corresponding to a level of operational ability. For example the health states available for a computing resource may be healthy degraded severely degraded or inoperable. In some embodiments however the health of a computing resource may be determined in other ways. For example the health state may be a value on a continuum of health states expressed for example as a percentage of expected performance.

In various embodiments the measurements that are taken and collected are used to train a probability model of the computing resource s operation. The probability model may manifest itself in various ways such as a contingency table that is usable to determine the probability that a computing resource is in a particular health state given a particular measurement being within a particular range. For example the contingency table may indicate that there is a 10 probability that a storage volume is in a degraded state given that a first latency value is between 8 and 10 milliseconds but that there is a 50 probability that the storage volume is in a degraded state given that a second latency value is between 8 and 10 milliseconds. The first and second latency values may be related to different parts of a network and or different operations involved in IOPS and their association with the degraded state is defined either from simulations benchmarks with failure mode injection or from historical data on the service. In this manner the probability model can be used to determine more likely causes of performance degradations. For instance referring to the specific example in this paragraph if the storage volume is in a degraded state with the first latency value and the second latency value both between 8 and 10 milliseconds it is more likely to be beneficial to investigate potential causes of the second latency value being in the range given the judgment implied by the probability model.

Various embodiments of the present disclosure also allow the use of probability models in convenient diagnostic tools. For instance in some embodiments a probability model is used in a query engine that allows technicians and or automated processes to submit queries to determine potential causes of performance degradations. The queries may specify for example a health state for a computing resource and measurements that have been taken regarding the computing resource s operation. The query engine may use the information specified by a query to determine a response that guides diagnosis of the performance degradation and or its repair. The query engine may for example provide a ranked list of potential causes where the ranking is based at least in part on the probability model. The ranked list may have the causes themselves or information indicative of the causes such as the measurements corresponding to the causes and the probability that the given measurements are associated with the given degradation. In this manner the query response can be used to address the most likely causes of the performance degradation before addressing others that are less likely to be the cause.

In an embodiment the data storage volumes are block level storage devices which may be physically attached block level data storage devices e.g. drives with spinning magnetic media or solid state drives or may be virtual block level data storage devices or a combination of the two. A virtual block level storage volume may be implemented in various ways. For example the underlying data comprising the block level storage volume may be physically stored on multiple data storage devices even though the block level storage volume may be logically represented as a single device. The data storage volumes may be used in various ways. For example customers of a computing resource provider may format the data storage devices with a file system and mount the file system. Data storage volumes may be physically and or logically attached to a physical or virtual computer system.

In an embodiment the clients are implemented using computer systems hosted by the computing resource provider. The clients may be applications of computer systems hosted by the computing resource provider. The client may be an application executing on a computer system such that other applications such as operating system are able to communicate with a corresponding data storage volume for the purpose of data storage. The client may be configured such that from the point of view of the operating system the operating system communicates with the client as if it is communicating with a locally attached broad data storage device.

In an embodiment clients are separated from corresponding data storage volumes by a network which is not illustrated explicitly in the figure. Accordingly also not illustrated data storage volumes may be implemented using one or more data storage servers. As data flows between clients and data storage volumes an overall system implementing the clients and data storage volumes may measure various aspects of the communications between the clients and the data storage volume. Such measurements are described in more detail below but generally in an embodiment measurements that are relevant to the ability of a client to communicate with its corresponding data storage volumes and also the rates of individual communication steps are collected. Such performance measurements including health states of storage volumes during which the measurements were taken may be collected and used to configure a probabilistic query engine . The probabilistic query engine may be a computer system or programming module of a computer system configured to receive and respond to queries such as described in more detail below. As illustrated in a query to the probabilistic query engine may relate to a particular data storage volume and may include information identifying a health state of the volume and one or more performance metrics measured for the volume. In various embodiments the query may be submitted by different entities. For example a technician may submit the query using a computer system operated by the technician. As another example an automated process of a computer system may cause the computer system to transmit the query.

The probabilistic query engine may then process the query and provide a potential root cause or multiple potential root causes of the health state identified in the query . The probabilistic query engine may determine the root cause in a probabilistic way based at least in apart on performance measurements that have been used to configure the probabilistic query engine . For example in an embodiment the probabilistic query engine uses a probability model to determine which potential root causes are more likely than others for particular health states. The probability model may be encoded by the probabilistic query engine in various ways. For instance in some embodiments the probability model is encoded using one or more contingency tables that associate health states with probabilities of certain performance measurements having values in predetermined ranges. Alternative embodiments use more sophisticated probability models e.g. Gaussian models or Logistic models describing the probability of degraded performance given one or more values of the given metrics. In this manner because measurements are tied to computing resources being measured more likely root causes may be identified using the probability model. The probability model may be generated using measurements over a past time period for a fleet of computing resources e.g. a fleet of storage volumes or from simulation data.

Turning now to an example embodiment of a data storage system environment is shown. A virtual computer system instance is attached to a primary volume in a first storage server . The virtual computer system instance or virtual machine may read and write data to the primary volume . As information is written to the primary volume the written information may be sent to a slave volume in a second storage server . By storing the written information of the primary volume the slave volume may provide redundancy for the primary volume. The ellipsis between the primary volume and other volume indicates that the first storage server may support any suitable number of volumes. The ellipsis between the slave volume and other volume indicates that the second storage server may support any suitable number of volumes. The ellipsis between the first storage server and the second storage server indicates that there may be any suitable number of storage servers. While the communication with the slave volume is shown through the virtual computer system instance it should be recognized that the written information may also be relayed from other systems such as the first storage server . It should also be noted that embodiments describing virtualization is provided for the purpose of illustration but that components described as virtual may be actual hardware devices. As such the primary volume could be attached to a physical computer system in a different configuration.

As illustrated in the environment may include a storage monitoring system and a storage management system which may include a provisioning system . The storage monitoring system may receive information and from the host of a virtual computer system instance as well as information about the storage servers and and or storage server volumes and . The storage monitoring system may then use the information and for the collection of various performance measurements to be used in probability models such as described below. The information collected by the storage monitoring system may also be used in other ways such as to determine one or more indicators that represent whether the current placement of the primary volume and slave volume are satisfactory. Indicators may include or be based in part on historical current and or predicted characteristics and or information. If one or more of the indicators jointly or individually indicate that the primary volume and or slave volume should be moved the storage monitoring system may send one or more packets to the provisioning system that cause the provisioning system to determine a new placement of one or both of the primary volume and slave volume . The provisioning system may also use information and to determine the effect transferring a volume would have on potential new hosts for the primary volume and or slave volume . Information about potential hosts may also be used to determine if they are appropriate available implementation resources for one or both of the volumes and .

For example a storage monitoring system server in a low latency data storage system may receive information and indicating a high utilization of a first storage server . For example the storage monitoring system may use indicators of memory usage storage capacity network bandwidth operating system utilization read write operations etc. to determine if one or more indicators of server health are at or lower than a threshold level. Using the indicators the storage monitoring system determines that the utilization of the first storage server is too high and must be reduced. The storage monitoring system may then determine that moving volume should sufficiently reduce the utilization of the first storage server . The storage monitoring system may request a provisioning system to transfer the volume to a different storage server. Provisioning system may examine one or more available implementation resources such as free space or volumes in other storage servers in light of information and associated indicators and indicators associated with volume and make a placement decision. The provisioning system may then cause the volume to be transferred to the available implementation resource.

In at least one embodiment one or more aspects of the environment may incorporate and or be incorporated into a computing resource service. depicts aspects of an example computing resource service in accordance with at least one embodiment. The computing resource service provides virtualized computing services including a virtual computer system service and a virtual data store service with a wide variety of computing resources interlinked by a relatively high speed data network. Such computing resources may include processors such as central processing units CPUs volatile storage devices such as random access memory RAM nonvolatile storage devices such as flash memory hard drives and optical drives servers such as the Web server and the application server described above with reference to one or more data stores such as the data store of as well as communication bandwidth in the interlinking network. The computing resources managed by the computing resource service are not shown explicitly in because it is an aspect of the computing resource service to emphasize an independence of the virtualized computing services from the computing resources that implement them.

The computing resource service may utilize the computing resources to implement the virtualized computing services at least in part by executing one or more programs program modules program components and or programmatic objects collectively program components including and or compiled from instructions and or code specified with any suitable machine and or programming language. For example the computing resources may be allocated and reallocated as necessary to facilitate execution of the program components and or the program components may be assigned and reassigned as necessary to the computing resources. Such assignment may include physical relocation of program components for example to enhance execution efficiency. From a perspective of a user of the virtualized computing services the computing resource service may supply computing resources elastically and or on demand for example associated with a per resource unit commodity style pricing plan.

The computing resource service may further utilize the computing resources to implement a service control plane configured at least to control the virtualized computing services. The service control plane may include a service administration interface . The service administration interface may include a Web based user interface configured at least to enable users and or administrators of the virtualized computing services to provision de provision configure and or reconfigure collectively provision suitable aspects of the virtualized computing services. For example a user of the virtual computer system service may provision one or more virtual computer system instances . The user may then configure the provisioned virtual computer system instances to execute the user s application programs. The ellipsis between the virtual computer system instances and indicates that the virtual computer system service may support any suitable number e.g. thousands millions and more of virtual computer system instances although for clarity only two are shown.

The service administration interface may further enable users and or administrators to specify and or re specify virtualized computing service policies. Such policies may be maintained and enforced by a service policy enforcement component of the service control plane . For example a storage administration interface portion of the service administration interface may be utilized by users and or administrators of the virtual data store service to specify virtual data store service policies to be maintained and enforced by a storage policy enforcement component of the service policy enforcement component . Various aspects and or facilities of the virtual computer system service and the virtual data store service including the virtual computer system instances the low latency data store the high durability data store and or the underlying computing resources may be controlled with interfaces such as application programming interfaces APIs and or Web based service interfaces. In at least one embodiment the control plane further includes a workflow component configured at least to interact with and or guide interaction with the interfaces of the various aspects and or facilities of the virtual computer system service and the virtual data store service in accordance with one or more workflows.

In at least one embodiment service administration interface and or the service policy enforcement component may create and or cause the workflow component to create one or more workflows that are then maintained by the workflow component . Workflows such as provisioning workflows and policy enforcement workflows may include one or more sequences of tasks to be executed to perform a job such as provisioning or policy enforcement. The workflow component may modify further specify and or further configure established workflows. For example the workflow component may select particular computing resources of the computing resource service to execute and or be assigned to particular tasks. Such selection may be based at least in part on the computing resource needs of the particular task as assessed by the workflow component . As another example the workflow component may add additional and or duplicate tasks to an established workflow and or reconfigure information flow between tasks in the established workflow. Such modification of established workflows may be based at least in part on an execution efficiency analysis by the workflow component . For example some tasks may be efficiently performed in parallel while other tasks depend on the successful completion of previous tasks.

The virtual data store service may include multiple types of virtual data store such as a low latency data store and a high durability data store . For example the low latency data store may maintain one or more data sets which may be read and or written collectively accessed by the virtual computer system instances with relatively low latency. The ellipsis between the data sets and indicates that the low latency data store may support any suitable number e.g. thousands millions and more of data sets although for clarity only two are shown. For each data set maintained by the low latency data store the high durability data store may maintain a set of captures . Each set of captures may maintain any suitable number of captures and of its associated data set respectively as indicated by the ellipses. Each capture and may provide a representation of the respective data set and at a particular moment in time. Such captures and may be utilized for later inspection including restoration of the respective data set and to its state at the captured moment in time. Although each component of the computing resource service may communicate utilizing the underlying network data transfer between the low latency data store and the high durability data store is highlighted in because the contribution to utilization load on the underlying network by such data transfer can be significant.

For example the data sets of the low latency data store may be virtual disk files i.e. file s that can contain sequences of bytes that represent disk partitions and file systems or other logical volumes. The low latency data store may include a low overhead virtualization layer providing access to underlying data storage hardware. For example the virtualization layer of the low latency data store may be low overhead relative to an equivalent layer of the high durability data store . Systems and methods for establishing and maintaining low latency data stores and high durability data stores in accordance with at least one embodiment are known to those of skill in the art so only some of their features are highlighted herein. In at least one embodiment the sets of underlying computing resources allocated to the low latency data store and the high durability data store respectively are substantially disjointed. In a specific embodiment the low latency data store could be a Storage Area Network target or the like. In this example embodiment the physical computer system that hosts the virtual computer system instance can send read write requests to the SAN target.

The low latency data store and or the high durability data store may be considered non local and or independent with respect to the virtual computer system instances . For example physical servers implementing the virtual computer system service may include local storage facilities such as hard drives. Such local storage facilities may be relatively low latency but limited in other ways for example with respect to reliability durability size throughput and or availability. Furthermore data in local storage allocated to particular virtual computer system instances may have a validity lifetime corresponding to the virtual computer system instance so that if the virtual computer system instance fails or is de provisioned the local data is lost and or becomes invalid. In at least one embodiment data sets in non local storage may be efficiently shared by multiple virtual computer system instances . For example the data sets may be mounted by the virtual computer system instances as virtual storage volumes.

Data stores in the virtual data store service including the low latency data store and or the high durability data store may be facilitated by and or implemented with a block data storage BDS service at least in part. The BDS service which may include the storage management system of may facilitate the creation reading updating and or deletion of one or more block data storage volumes such as virtual storage volumes with a set of allocated computing resources including multiple block data storage servers. A block data storage volume and or the data blocks thereof may be distributed and or replicated across multiple block data storage servers to enhance volume reliability latency durability and or availability. As one example the multiple server block data storage systems that store block data may in some embodiments be organized into one or more pools or other groups that each has multiple physical server storage systems co located at a geographical location such as in each of one or more geographically distributed data centers and the program s that use a block data volume stored on a server block data storage system in a data center may execute on one or more other physical computing systems at that data center.

The BDS service may facilitate and or implement local caching of data blocks as they are transferred through the underlying computing resources of the computing resource service including local caching at data store servers implementing the low latency data store and or the high durability data store and local caching at virtual computer system servers implementing the virtual computer system service . In at least one embodiment the high durability data store is an archive quality data store implemented independent of the BDS service . The high durability data store may work with sets of data that are large relative to the data blocks manipulated by the BDS service . The high durability data store may be implemented independent of the BDS service . For example with distinct interfaces protocols and or storage formats.

Each data set may have a distinct pattern of change over time. For example the data set may have a higher rate of change than the data set . However in at least one embodiment bulk average rates of change insufficiently characterize data set change. For example the rate of change of the data set may itself have a pattern that varies with respect to time of day day of week seasonally including expected bursts correlated with holidays and or special events and annually. Different portions of the data set may be associated with different rates of change and each rate of change signal may itself be composed of independent signal sources for example detectable with Fourier analysis techniques. Any suitable statistical analysis techniques may be utilized to model data set change patterns including Markov modeling and Bayesian modeling.

Computer system hardware may be abstracted using virtualization techniques to simultaneously operate a plurality of guest operating systems. illustrates an environment utilizing one such technique using a virtual machine monitor or hypervisor. The hardware of a host in some embodiments may be used to implement various computer systems such as computer systems described above. For example the host illustrated in may be used to implement a computer system to which a storage volume is attached. The hardware of the host in various embodiments interfaces with a virtual machine monitor or hypervisor running directly on the hardware e.g. a bare metal or native hypervisor. Examples of such hypervisors include Xen Hyper V and the like. Hypervisors typically run at a higher more privileged processor state than any other software on the machine and provide services such as memory management and processor scheduling for dependent layers and or domains. The most privileged of such layers and or domains resides in the service domain layer which may include an administrative operating system for configuring the operation and functionality of the hypervisor as well as that of domains of lower privilege such as guest domains including guest operating systems which may be heterogeneous e.g. running different operating systems than each other . The service domain may have direct access to the hardware resources of the host by way of the hypervisor while the user domains may not. Techniques illustratively described herein are also applicable to code running within or upon other types and subtypes of virtualization such as hardware virtualization software virtualization hardware assisted virtualization such as virtualization techniques utilizing Intel VT x VT i and or AMD AMD V implementations full virtualization paravirtualization partial virtualization and any variants and or subtypes thereof.

As illustrated in a guest operating system of the host may implement a client of a data storage volume. As illustrated in the volume may be implemented using a master volume which may be a primary volume such as described above in connection with and a slave volume. As shown in the host according to the operation of a client operating on a guest operating system communicates with the master to perform input output operations. While not illustrated in the figure for the purpose of simplification of illustration communications with the master may be through a storage server. Similarly communications with the slave may be through a separate storage server. Further as illustrated in the figure when the master is used for input output operations communication between the master and slave may follow such as to synchronize the slave with the master should the slave become needed such as for failover and or to being used for read operations in parallel with the master.

As indicated by the clocks in various processes involved with the operation of a storage volume or generally of a computing resource can be measured. For example in the environment illustrated in a read or write operation can involve multiple sub processes sub operations . For example as noted above a storage volume may be logically attached to a computer system instance implemented by the host in a guest operating system . A data operation e.g. read write update or delete operation can involve transmitting a request to the storage volume e.g. to a storage server serving the storage volume and receiving an acknowledgment of the request. Also as noted above the operation may also involve communication between master and slave for the purpose of synchronization. Transmitting the request itself may include various sub processes. For instance a guest operating system may transmit the request through a more privileged domain e.g. a domain in which the administrative operating system operates which may instruct a network card to transmit the request over a network. The time it takes the more privileged domain to process the request may be timed as may the time it takes the network card to transmit the request to the network. The time it takes the request to travel over the network to the data storage server may also be timed. Network hops between the client and server may also be timed. At the server the time the request sits in a request queue as well as the length of the request queue may be measured. The time it takes a physical storage device to perform a read write or other operation may be measured. As with the request travelling from the client to the server various aspects of a communication of an acknowledgment of an operation being performed may also be measured such as various latencies and processing times.

Other performance measurements may also be utilized. For example the total time spent doing work on the server in connection with processing a request may be measured. This may include operations from reading in the request on the socket to sending out the reply after the I O is completed. A local disk time may also be measured. The local disk time may be a time a physical storage device spends performing an I O operation. A network request receipt time the time a request spends travelling across a network from client to server may be measured. This metric may an approximation of the network time. In some embodiments for example the timer is started the I O request is written to the socket sock sendmsg and the timer is stopped. While TCP NODELAY may be set on the socket to disable Nagel s algorithm for small requests e.g. read requests this in some instances may measure the speed of the transmission control protocol TCP buffer. For larger requests write requests the buffer is more likely to fill and this will observe the time from the first byte written until the last byte is placed on the network. It will return once the last byte is handed to the TCP stack.

Similar to the network request receipt time the time the block device response spends traveling across the network from the server to the client may also be measured. In order to determine when to start the timer the client may block on a read sock recvmsg for a global network block device GNBD GNBD header e.g. 16 bytes . Once it receives the header it may start the timer read the remainder of the response and then stop the timer. A queue wait time may also be measured. The queue wait time may be the time a request spends in a data storage server queue after the request is pulled off the socket and before the request is serviced as an I O request.

Various performance metrics involving other services may also be measured. For example in some embodiments a block data storage service communicates with a high durability data storage service for various purposes such as for storing snapshots of storage volumes. Latencies communication with the other services may for example be measured. The latencies may include times to receive responses from the other service times for the other service to complete an operation and times for obtaining data from the other storage service such as snapshots.

As noted above various metrics involving communications between a master and slave may also be measured. For example in an embodiment the time spent sending writes to the slave may be measured. Times from the start of sending a mirror write to the slave to receipt of a response from the slave by the master may also be measured. Times for receipt of acknowledgments from a slave to the master may be measured. Another metric may come into play when the master sends a mirror write to the slave and the slave responds by asking for the full chunk to be sent over. This metric may involve a measurement of the time for the master to read the entire chunk from disk before sending it to the slave. As another example the time it takes to send a full chunk write to the slave may be measured. Such an operation may occur when the slave responds to a mirror write asking the master to send the full chunk.

Other metrics that may be used include for a storage volume acting as a master the time to read the request in the time to send reply back to client on the network the time to read from local disk time to read data from another storage service e.g. a high durability storage service the time waiting for another storage service to free up the time a read request spends in a request queue the time a write request spends in a request queue the time waiting for a throttling component to release a request the total time an operation took to complete the time to read a write request from a network the time to send a reply to network the time to write to local disk the time to prepare the mirror slave for sending data the time to send to mirror slave the time to send to get an acknowledgment from mirror slave the time to read a full chunk to send to the mirror the time to send a full chunk to the mirror the time waiting to read data off a socket to enqueue and or other metrics. Metrics may also involve measurements of others activity on shared resources. For example in an environment where a data storage server serves storage volumes for multiple entities e.g. multiple customers of a computing resource provider for a particular storage volume served by a storage server measurements involving others activity in connection with the storage server may also be measured. The activity may be for example IOPS by the others and or any of the measurements discussed above.

The metrics may be collected in various ways. For example as illustrated in various measurements may be made in different locations of a network. Various devices in the network are involved in the timing of various operations involved in input output. Such devices may be configured to transmit measurements over the network to an entity configured to collect and or process measurements. For example in many examples measurements are made in connection with a request in transit. Measurements may be piggybacked onto the communications involved in the request such as in annotations to the requests and or acknowledgments. For example as illustrated in the host receives measurements that have been added to communications while in transit and included in an acknowledgment. The host may transmit the measurements or data based at least in part on the measurements to a BDS statistics data store i.e. to a server serving the BDS statistics data store which may be a component of a storage monitoring system such as described above in connection with . It should be noted however that measurements may be sent to a collecting entity in other ways and not necessarily through the host .

In an embodiment the performance of a volume is categorized into one of a plurality of health states. The health states may each correspond to a particular level of performance measured in terms of the number of IOPS that it is able to provide. That is the number of IOPS a computer system logically or otherwise attached to the volume is able to achieve while using the volume. accordingly shows an illustrative example of a table for classifying the health state of a volume. The table in this example includes four different health states. In other embodiments a different number of states may be used. In an embodiment the health state of a volume corresponds to some range of performance relative to a desired performance where the desired performance may be measured in various ways such as a number of IOPS.

In an embodiment health states are based at least in part on a desired performance. For storage volumes hosted by a computing resource provider the desired performance may be based at least in part on a service level agreement with customers. Further the health state may be based at least in part on an average performance over a period of time such as a minute or hour or other time period. In the embodiment illustrated in the volume health state is designated as healthy if the measured performance is above 90 of a desired performance. For example if the desired performance of the volume is 100 IOPS if the performance of the volume is measured to be over 90 that is over 90 IOPS then the volume would be designated by the table as being healthy. If the volume is able to provide a measured performance that is above 50 of the desired performance but less than or equal to 90 of the desired performance the volume may be designated as having a health state of degraded where degraded may correspond to a level of volume health where the volume is not operating as desired. If the volume has a measured performance that is nonzero but less than or equal to 50 of the desired performance in an embodiment the volume is designated as having a health state of severely degraded. Similarly if the volume s measured performance is equal to 0 of the desired performance the volume may be designated as having a health state of operable that is the volume is unable to be used for its intended purpose.

It should be noted that while discussing the performance of the volume the volume s performance may be affected by some aspect of a larger distributed system and not necessarily a storage device used to implement the volume. For example as noted above various embodiments of the present disclosure utilize environments where computer systems which may be virtual computer systems are logically attached to a volume where the volume is served by a data storage server separate from the computer system. In this manner communications between the computer system and the volume travel over a network. The malfunction of a device in the network through which communications travel may be a reason for a volume s inability to perform at a certain level. It should be noted that the number of health states and the ranges corresponding to the health states are provided for the purpose of illustration and that different ranges and or numbers of health states may be used. In some embodiments the health state may not fall into discrete categories but may be a value on a continuum. For example in some embodiments the health state is simply the percentage of the measured performance relative to the desired performance.

Multiple other columns of the database in an embodiment correspond to various measurements taken in connection with the volume s operation. The measurements may be for example timing measurements such as described above in connection with . While not illustrated as such other columns may also be included in the database such as columns for various characteristics of a system in which the volume is used. For example a volume may be implemented using one of several different hardware types. For instance some volumes may be implemented by magnetic storage devices and other volumes may be implemented using solid state drives SSDS thus a column in the database may correspond to that hardware type. Similarly a virtual computer system to which the volume is logically attached may be one of several different types of virtual computer system available where the type generally corresponds to some type of hardware capacity reserved for the virtual computer system. Accordingly a column in the database may correspond to the type of virtual machine to which the volume is logically attached or a type of physical machine to which the volume is attached.

Generally any characteristic of a computer system in which the volume operates may be represented by a column in the database . As noted above each row in the database corresponds to measurements taken in connection with the volume s performance during a particular time identified in the time stamp column for a particular volume identified in the volume ID column.

As illustrated in a single volume may have information stored in multiple rows of the database where each row corresponds to a different time. In an embodiment for a period of time for each row in the database is one minute although other times may be used. Further the information corresponding measurements in a row may be averages of actual measurements made during the corresponding time period. For example looking to the row for Volume ID 3141 having the time stamp 1 i.e. the first minute for which the measurements were taken for the volume having ID 3141 a measurement identified as measurement 1 has a value of 0.5 milliseconds. During the minute for which the row corresponds many measurements corresponding to measurement 1 may have been taken and their average may be 0.5 milliseconds.

As rules are provided to the learner the learner may calculate the probabilities based at least in part on the per volume data that the learner has received. Accordingly in an embodiment the learner provides rules and parameters . The rules and parameters may be sets of probabilities that a volume is in a particular health state given various ranges of a metric. The rules and parameters may be included in any suitable manner such as a contingency table or collection of contingency tables that each has entries on one axis corresponding to health states of a volume and another axis corresponding to ranges of a metric. In this manner when a volume is measured to have a particular performance metric the rules and parameters may be used to find the most likely root cause.

As illustrated in per volume data may also be provided to a classifier which like the learner may be a computer system or program module operating on a computer system that is configured to receive per volume data as input and provide route cause analysis . Per volume data provided to the classifier may be data for a particular volume whose performance has been measured. For example referring to per volume data may correspond to a row in the database . It should be noted that the learner and classifier may be part of the same computer system or different computer systems. The root cause analysis provided by the classifier may be information corresponding to a most likely root cause of a performance health state of a volume based at least in part on the rules and parameters provided to the classifier . The information in the root cause analysis may be provided in any suitable manner. For example the root cause analysis may include information corresponding to a most likely cause of the volume health state and or a ranking of possible causes of the health state where the ranking is based at least in part on probabilities calculated by the learner .

Various embodiments of the present disclosure utilize graphical probability models for the purpose of analysis both in learning probabilities of various causes for volume states and for analyzing current performance data for a particular volume. shows an illustrative example of a graphical probability model . In an embodiment a graphical probability model is a directed graph with nodes corresponding to various performance metrics. In this particular example a service time node depends directly from four different performance metrics labeled M1 M2 M3 and M4. Accordingly each of M1 M2 M3 and M4 are represented by a separate node in the graph. An edge connecting each of M1 M2 M3 and M4 to the service time node is a directed edge pointing to the service time node . For example a node labeled M1 has an edge directed to the service time node thereby indicating that service time depends from M1. As illustrated in root causes such as a root cause labeled R1 are represented by a node labeled R1 in the graph and may be modeled by various performance metrics. For instance in a root cause e.g. a device malfunction has an effect on both the metrics M1 and M2.

The graphical probability model can be generated in any suitable way. For example the relationships between nodes can be specified by a user and or learned by a computer system based at least in part on historical data. Further the model may be used in various ways. For instance referring to the particular graphical probability model in the root cause R1 is modeled to be able to cause changes to M1 and M2. If analysis of contingency tables in connection with a volume having a particular health state indicates the probability of being in the particular health state being higher given M1 and or M2 being within a range in which measured values fall for the volume the graphical probability model can be used to indicate R1 as the root cause of the particular health state. It should be noted that information output by a computer system operating in accordance with various embodiments described herein may output different types of information. For example the information output may indicate explicitly or implicitly the root cause. For instance referring to output of a system may specifically indicate R1 or may implicitly indicate R1 by indicating explicitly M1 and or M2. Other variations are also considered as being within the scope of the present disclosure.

In an embodiment the process includes two stages model creation and model application. During model creation the process may include obtaining performance measurements of storage volumes. The performance measurements may be obtained in any suitable manner such as in manners described above. Further as noted above the measurements may be statistically processed. For example the measurements may be averages over a time period and may be normalized to take into account different types of use of storage volumes. It should be noted that obtaining performance measurements may include obtaining other information relevant to analysis in accordance with various embodiments such as other measurements and or characteristics of respective storage volumes and health states for the volumes at a time when the measurements were taken. It should also be noted that while the process is described in connection with storage volumes the techniques are applicable to other computing resources.

Also part of the model creation state the process includes using the obtained measurements to generate a probability model for data storage volumes. The probability model may comprise a set of contingency tables each of which associates probabilities with health states and ranges for a corresponding metric. For example one contingency table may organize contingent probabilities for each of a plurality of health states. For each health state for instance the table may encode a probability that a volume is in the health state given a measurement falling within a particular range for each of a plurality of ranges. Other probability models may also be used. For example instead of tables probability functions may be interpolated or otherwise determined from the obtained measurements. Instead of discrete ranges and or health states the probability may be a continuous function of a measurement representing a health state e.g. percentage of desired IOPS achieved and one or more measurements.

Turning to the model application stage of the process in an embodiment the process includes receiving a query for a particular volume. The query may have been submitted by a technician or by an automated process of a computer system involved in the monitoring and or diagnosis of storage volumes. The query may specify or otherwise indicate a health state and one or more measurements of the volume taken while in the indicated health state. The process may include querying a probability model to determine probabilities for various root causes. For example in some embodiments the measurements and health state indicated in the query may be looked up in one or more contingency tables that encode the generated probability model. As noted above other ways of encoding a probability model may be used and the process may be modified accordingly. Returning to the illustrated embodiment the root causes or other associated information such as metrics corresponding to root causes may be ranked using the probability such as according to lookups in one or more contingency tables. For example metrics associated with higher probabilities may be ranked higher.

Once the metrics or other information has been ranked the process may include providing a response to the query based at least in part on the ranked root causes or other information . For example a list of the root causes may be provided according to the ranking. A graphical probability model may be used to identify root causes that are associated by the graphical probability model with metrics ranked in accordance with the probability model. One or more such root causes may be provided in a ranking according to the raking of associated metrics. The information may be provided in any way. For example if the query was submitted from another computer system different from the computer system processing the query providing the response may include transmitting one or more electronic messages that encode the response over a network to the requestor. If the query was submitted from the computer system that processes the query providing the response may include causing a display device e.g. monitor or touchscreen to display the response or information based thereupon. In some embodiments a system is configured with the ability to submit queries receive responses and address root causes according to the responses. For example referring to the storage management system may be able to receive a response to the query and take corresponding action. Depending on the root cause the corresponding action may be one that the storage management system is able to address such as by causing the provisioning system to reprovision a computing resource causing a computer system instance to reboot updating routing tables to cause a rerouting of network traffic and or in other ways. Generally the information may be provided in any suitable manner.

The illustrative environment includes at least one application server and a data store . It should be understood that there can be several application servers layers or other elements processes or components which may be chained or otherwise configured which can interact to perform tasks such as obtaining data from an appropriate data store. As used herein the term data store refers to any device or combination of devices capable of storing accessing and retrieving data which may include any combination and number of data servers databases data storage devices and data storage media in any standard distributed or clustered environment. The application server can include any appropriate hardware and software for integrating with the data store as needed to execute aspects of one or more applications for the client device handling a majority of the data access and business logic for an application. The application server provides access control services in cooperation with the data store and is able to generate content such as text graphics audio and or video to be transferred to the user which may be served to the user by the Web server in the form of HyperText Markup Language HTML Extensible Markup Language XML or another appropriate structured language in this example. The handling of all requests and responses as well as the delivery of content between the client device and the application server can be handled by the Web server. It should be understood that the Web and application servers are not required and are merely example components as structured code discussed herein can be executed on any appropriate device or host machine as discussed elsewhere herein.

The data store can include several separate data tables databases or other data storage mechanisms and media for storing data relating to a particular aspect. For example the data store illustrated includes mechanisms for storing production data and user information which can be used to serve content for the production side. The data store also is shown to include a mechanism for storing log data which can be used for reporting analysis or other such purposes. It should be understood that there can be many other aspects that may need to be stored in the data store such as page image information and access right information which can be stored in any of the above listed mechanisms as appropriate or in additional mechanisms in the data store . The data store is operable through logic associated therewith to receive instructions from the application server and obtain update or otherwise process data in response thereto. In one example a user might submit a search request for a certain type of item. In this case the data store might access the user information to verify the identity of the user and can access the catalog detail information to obtain information about items of that type. The information then can be returned to the user such as in a results listing on a Web page that the user is able to view via a browser on the user device . Information for a particular item of interest can be viewed in a dedicated page or window of the browser.

Each server typically will include an operating system that provides executable program instructions for the general administration and operation of that server and typically will include a computer readable storage medium e.g. a hard disk random access memory read only memory etc. storing instructions that when executed by a processor of the server allow the server to perform its intended functions. Suitable implementations for the operating system and general functionality of the servers are known or commercially available and are readily implemented by persons having ordinary skill in the art particularly in light of the disclosure herein.

The environment in one embodiment is a distributed computing environment utilizing several computer systems and components that are interconnected via communication links using one or more computer networks or direct connections. However it will be appreciated by those of ordinary skill in the art that such a system could operate equally well in a system having fewer or a greater number of components than are illustrated in . Thus the depiction of the system in should be taken as being illustrative in nature and not limiting to the scope of the disclosure.

The various embodiments further can be implemented in a wide variety of operating environments which in some cases can include one or more user computers computing devices or processing devices which can be used to operate any of a number of applications. User or client devices can include any of a number of general purpose personal computers such as desktop or laptop computers running a standard operating system as well as cellular wireless and handheld devices running mobile software and capable of supporting a number of networking and messaging protocols. Such a system also can include a number of workstations running any of a variety of commercially available operating systems and other known applications for purposes such as development and database management. These devices also can include other electronic devices such as dummy terminals thin clients gaming systems and other devices capable of communicating via a network.

Most embodiments utilize at least one network that would be familiar to those skilled in the art for supporting communications using any of a variety of commercially available protocols such as Transmission Control Protocol Internet Protocol TCP IP Open System Interconnection OSI File Transfer Protocol FTP Universal Plug and Play UpnP Network File System NFS Common Internet File System CIFS and AppleTalk. The network can be for example a local area network a wide area network a virtual private network the Internet an intranet an extranet a public switched telephone network an infrared network a wireless network and any combination thereof.

In embodiments utilizing a Web server the Web server can run any of a variety of server or mid tier applications including Hypertext Transfer Protocol HTTP servers FTP servers Common Gateway Interface CGI servers data servers Java servers and business application servers. The server s also may be capable of executing programs or scripts in response requests from user devices such as by executing one or more Web applications that may be implemented as one or more scripts or programs written in any programming language such as Java C C or C or any scripting language such as Perl Python or TCL as well as combinations thereof. The server s may also include database servers including without limitation those commercially available from Oracle Microsoft Sybase and IBM .

The environment can include a variety of data stores and other memory and storage media as discussed above. These can reside in a variety of locations such as on a storage medium local to and or resident in one or more of the computers or remote from any or all of the computers across the network. In a particular set of embodiments the information may reside in a storage area network SAN familiar to those skilled in the art. Similarly any necessary files for performing the functions attributed to the computers servers or other network devices may be stored locally and or remotely as appropriate. Where a system includes computerized devices each such device can include hardware elements that may be electrically coupled via a bus the elements including for example at least one central processing unit CPU at least one input device e.g. a mouse keyboard controller touch screen or keypad and at least one output device e.g. a display device printer or speaker . Such a system may also include one or more storage devices such as disk drives optical storage devices and solid state storage devices such as random access memory RAM or read only memory ROM as well as removable media devices memory cards flash cards etc.

Such devices also can include a computer readable storage media reader a communications device e.g. a modem a network card wireless or wired an infrared communication device etc. and working memory as described above. The computer readable storage media reader can be connected with or configured to receive a computer readable storage medium representing remote local fixed and or removable storage devices as well as storage media for temporarily and or more permanently containing storing transmitting and retrieving computer readable information. The system and various devices also typically will include a number of software applications modules services or other elements located within at least one working memory device including an operating system and application programs such as a client application or Web browser. It should be appreciated that alternate embodiments may have numerous variations from that described above. For example customized hardware might also be used and or particular elements might be implemented in hardware software including portable software such as applets or both. Further connection to other computing devices such as network input output devices may be employed.

Storage media and computer readable media for containing code or portions of code can include any appropriate media known or used in the art including storage media and communication media such as but not limited to volatile and non volatile removable and non removable media implemented in any method or technology for storage and or transmission of information such as computer readable instructions data structures program modules or other data including RAM ROM Electrically Erasable Programmable Read Only Memory EEPROM flash memory or other memory technology Compact Disc Read Only Memory CD ROM digital versatile disk DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the a system device. Based on the disclosure and teachings provided herein a person of ordinary skill in the art will appreciate other ways and or methods to implement the various embodiments.

The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. It will however be evident that various modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims.

Although Other variations are within the spirit of the present disclosure. Thus while the disclosed techniques are susceptible to various modifications and alternative constructions certain illustrated embodiments thereof are shown in the drawings and have been described above in detail. It should be understood however that there is no intention to limit the invention to the specific form or forms disclosed but on the contrary the intention is to cover all modifications alternative constructions and equivalents falling within the spirit and scope of the invention as defined in the appended claims.

The use of the terms a and an and the and similar referents in the context of describing the disclosed embodiments especially in the context of the following claims are to be construed to cover both the singular and the plural unless otherwise indicated herein or clearly contradicted by context. The terms comprising having including and containing are to be construed as open ended terms i.e. meaning including but not limited to unless otherwise noted. The term connected is to be construed as partly or wholly contained within attached to or joined together even if there is something intervening. Recitation of ranges of values herein are merely intended to serve as a shorthand method of referring individually to each separate value falling within the range unless otherwise indicated herein and each separate value is incorporated into the specification as if it were individually recited herein. All methods described herein can be performed in any suitable order unless otherwise indicated herein or otherwise clearly contradicted by context. The use of any and all examples or exemplary language e.g. such as provided herein is intended merely to better illuminate embodiments of the invention and does not pose a limitation on the scope of the invention unless otherwise claimed. No language in the specification should be construed as indicating any non claimed element as essential to the practice of the invention.

Preferred embodiments of this disclosure are described herein including the best mode known to the inventors for carrying out the invention. Variations of those preferred embodiments may become apparent to those of ordinary skill in the art upon reading the foregoing description. The inventors expect skilled artisans to employ such variations as appropriate and the inventors intend for the invention to be practiced otherwise than as specifically described herein. Accordingly this invention includes all modifications and equivalents of the subject matter recited in the claims appended hereto as permitted by applicable law. Moreover any combination of the above described elements in all possible variations thereof is encompassed by the invention unless otherwise indicated herein or otherwise clearly contradicted by context.

All references including publications patent applications and patents cited herein are hereby incorporated by reference to the same extent as if each reference were individually and specifically indicated to be incorporated by reference and were set forth in its entirety herein.

