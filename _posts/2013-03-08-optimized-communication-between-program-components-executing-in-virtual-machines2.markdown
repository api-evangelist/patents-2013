---

title: Optimized communication between program components executing in virtual machines
abstract: Communication between program components executing in different virtual machines on the same physical computer may be optimized utilizing various mechanisms. A virtual machine manager may be configured to route network communications between virtual machines on the same physical host through a memory buffer. The virtual machine manager might also be configured to provide a shared memory and/or a shared data structure for enabling data communication between program components executing in different virtual machines on the same physical computing device. Mechanisms might also be implemented in order to prevent inconsistent read and/or write operations from being performed on the shared memory and/or the shared data structure. Mechanisms might also be implemented to minimize copying of a memory buffer, shared memory, and/or shared data structure.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09588788&OS=09588788&RS=09588788
owner: Amazon Technologies, Inc.
number: 09588788
owner_city: Seattle
owner_country: US
publication_date: 20130308
---
This patent application is a continuation in part of U.S. patent application Ser. No. 13 592 922 entitled OPTIMIZED DEPLOYMENT AND EXECUTION OF PROGRAMS IN A DISTRIBUTED COMPUTING ENVIRONMENT which was filed on Aug. 23 2012 the entirety of which is expressly incorporated herein by reference.

Distributed computing environments can provide computing resources configured according to a service oriented architecture SOA for executing distributed computer programs on a permanent or an as needed basis. The computing resources provided by such distributed computing environments may include various types of resources such as data processing resources data storage resources and others. Each type of computing resource may be general purpose or may be available in a number of specific configurations. For example data processing resources may be available as configurable virtual machine instances for executing various program components of a distributed program.

Distributed computing environments such as those described above have evolved over time to provide a significant number of services and other facilities for executing distributed programs. Along with this increased functionality however it has also become increasingly more difficult for developers to create deploy and execute distributed programs in distributed computing environments in a robust efficient and scalable manner.

The following detailed description is directed to technologies for automated optimized deployment and execution of programs in a distributed computing environment. Utilizing the technologies described herein a developer can more easily create deploy and execute a distributed program in a distributed computing environment in a reliable scalable performant and cost efficient manner.

In order to provide the functionality briefly described above components are provided for execution within or in conjunction with a distributed computing environment that can configure deploy and execute the program components of a distributed program in an optimized way with minimal input from a developer of the distributed program. As utilized herein the term optimize means to optimize at least one aspect of the operation of at least one component utilized by a distributed program. Following the optimization of the distributed program the distributed program may operate in some manner more optimally than it did prior to the application of the optimization techniques presented herein. The term optimize is not intended to mean that a distributed program is fully optimized. For instance in embodiments only one or more program components utilized in the operation of a distributed program might be optimized utilizing the concepts presented herein. Other program components utilized by the distributed program might not be optimized.

According to one embodiment a software developer creates program logic that performs the substantive computations for one or more program components of a distributed program. The developer of the distributed program need not however specify all of the runtime environment operating characteristics for the components of the distributed program. Rather the various mechanisms disclosed herein can determine optimized execution mechanisms for the distributed program identify optimal locations in a distributed environment for execution of the components of the program deploy the components of the distributed program to the identified locations bind the components of the program to frameworks for optimized inter program communication and execute the programs in an efficient low latency scalable and distributed manner. Other types of optimizations might also be identified and implemented.

Because the substantive logic of the distributed program may be abstracted from the actual implementation of the logic within the distributed computing environment in which it will execute the developer of the distributed program may be relieved of the burden of determining an optimized way to configure deploy and execute the logic in the distributed execution environment. For example the optimization components described herein might configure several program components for execution on different physical machines because the program components do not scale well when executed on the same physical machine. As another example the optimization components described herein might place different components of a distributed program on the same physical or virtual machine because the components exchange a significant volume of data. In this way the developer may be relieved of having to determine an optimal configuration for execution of the components of a distributed program.

According to another embodiment communication between program components executing in different virtual machines on the same physical computer might be optimized. For example in one implementation a virtual machine manager is configured to route network communications between virtual machines on the same physical host through a memory buffer. In this way routing the network traffic over a network can be avoided. In other implementations the virtual machine manager may be configured to provide a shared memory or a shared data structure for enabling data communication between program components executing in different virtual machines on the same physical device. Mechanisms might also be implemented in order to prevent inconsistent read and or write operations from being performed on the shared memory and or the shared data structure. Mechanisms might also be implemented to minimize copying of the memory buffer shared memory and or shared data structure.

The optimization components described herein might also perform other types of optimizations based upon the characteristics of the distributed program with little or no involvement of the developer of the distributed program. These mechanisms may make it easier for a developer to create an efficient scalable distributed program even in a highly complex distributed computing environment.

Additional details regarding the various components and processes described above for optimized configuration deployment and execution of programs in a distributed computing environment will be presented below with regard to .

It should be appreciated that the subject matter presented herein may be implemented as a computer process a computer controlled apparatus a computing system or an article of manufacture such as a computer readable storage medium. While the subject matter described herein is presented in the general context of program modules that execute on one or more computing devices those skilled in the art will recognize that other implementations may be performed in combination with other types of program modules. Generally program modules include routines programs components data structures and other types of structures that perform particular tasks or implement particular abstract data types.

Those skilled in the art will also appreciate that the subject matter described herein may be practiced on or in conjunction with other computer system configurations beyond those described herein including multiprocessor systems microprocessor based or programmable consumer electronics minicomputers mainframe computers handheld computers personal digital assistants e readers cellular telephone devices special purposed hardware devices network appliances and the like. The embodiments described herein may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment program modules may be located in both local and remote memory storage devices.

In the following detailed description references are made to the accompanying drawings that form a part hereof and that show by way of illustration specific embodiments or examples. The drawings herein are not drawn to scale. Like numerals represent like elements throughout the several figures which may be referred to herein as a FIG. or FIGS. .

The program components that make up the distributed program may be implemented in many different ways using many different computer languages and technologies. For instance the program components might be implemented using the JAVA programming language from ORACLE CORP. referred to herein as JAVA the JAVASCRIPT scripting language also from ORACLE CORP. referred to herein as JAVASCRIPT the C programming language the PERL programming language and other compiled and interpreted languages. The program components might also be execution environments capable of executing other programs such as virtual machine instances containerized environments for executing JAVA or JAVASCRIPT programs and other types of execution environments. The program components might also call one another utilizing different frameworks and standards.

The program components may be but are not necessarily tied to the frameworks of the distributed computing environment in which they are executed. Nonetheless and as will be described in greater detail below the execution of the distributed program that utilizes the various program components can be optimized utilizing the concepts and technologies disclosed herein.

As also illustrated in many different independent developers A C which might be referred to individually as a developer or collectively as the developers might author and provide the program components of a distributed program . For instance in the example shown in a developer A has provided the program component A a developer B has provided the program component B and a developer C has provided the program component C. The distributed program might be configured to make use of each of the program components even though different independent developers using different technologies have authored the program components .

The term developer as used herein should be read to encompass any entity that contributes a portion of a distributed program including but not limited to an individual a team or an entity such as a company or corporation. The developers might operate independently to create and contribute the various program components that make up a distributed program . In this regard it should be appreciated that the optimization of the distributed program described herein does not impact the independence that each of the developers has to independently author and deploy components using the technologies of their own choosing.

As shown in a developer A of a distributed program might also provide a program meta description referred to herein as a meta description of the distributed program . The meta description is a high level description of the execution of the various program components that make up the distributed program . The meta description might be expressed using a standardized meta programming language such as extensible markup language XML syntax for representing graphs. Other languages and mechanisms might also be utilized to create a meta description that describes the operation of the program components of a distributed program .

In some embodiments the meta description is required in order to optimize the deployment and execution of the distributed program . In other embodiments however the meta description is optional. The deployment and execution of the distributed program may be optimized based upon an execution graph of the distributed program even without a developer provided meta description . However provision of the meta description may allow additional and or deeper optimization of the distributed program to be performed.

A graphical user interface GUI might also be provided through which a developer can define the meta description for the distributed program . In other implementations a Web application programming interface API is provided through which a developer can submit the meta description for a distributed program . Other mechanisms might also be provided for defining and submitting the program meta description .

As will be described in greater detail below with regard to the meta description might describe the program components that make up the distributed program and how the program components have been defined e.g. C JAVA JAVASCRIPT virtual machine image etc. . The meta description might also define the other program components that each program component calls in what order and how the program calls the other components . The meta description might also describe other components that call each of the program components and how the program components are called.

As illustrated in the developer A of the distributed program might also specify a runtime optimization policy for the distributed program . The runtime optimization policy might specify one or more optimization metrics referred to herein as metrics that are consumed by an optimization component when determining how to optimize aspects of the deployment and execution of the distributed program .

For instance through an appropriate interface the developer A can define a runtime optimization policy that specifies the metrics for which the distributed program should be optimized. Metrics for which a distributed program should be optimized for include but are not limited to latency speed of execution reliability guaranteed execution availability cost scalability memory or other resource usage and accuracy.

The runtime optimization policy might also specify one or more constraints that are considered by the optimization component when optimizing the deployment and execution of the distributed program . As an example the runtime optimization policy might specify the maximum number of virtual machine instances or other types of computing resources that can be utilized in the deployment and execution of the distributed program . As another example the runtime optimization policy might specify a limit on the costs that can be incurred in executing the distributed program . In this way the developer A can specify that optimization be performed in a manner that optimizes deployment and execution of the program components of the distributed program in view of the metrics and constraints specified in the runtime optimization policy .

In some embodiments the runtime optimization policy also specifies preferences regarding how the deployment and execution of the distributed program should be optimized. For example the developer A might specify data indicating how the developer would prefer the various components of the distributed program to be executed. The preferences might also specify other build time deployment time and or runtime preferences that might be utilized to optimize the execution of one or more program components of the distributed program . The optimization component might utilize some all or none of the preferences specified in the runtime optimization policy .

It should be appreciated that in some embodiments a developer A provides the runtime optimization policy prior to the build deployment and or runtime of the distributed program . In other embodiments however the runtime optimization policy might be generated in an automated fashion at build time deployment time and or runtime based upon operational characteristics of the distributed program . The runtime optimization policy might also be modified during execution of the distributed program . For example instrumentation data might be received regarding various aspects of the execution of the distributed program . This data might be utilized to dynamically modify the runtime optimization policy while the distributed program is executing. Other factors might also be utilized to create and or modify the runtime optimization policy at build time deployment time and or execution time of the distributed program .

In some embodiments a developer is required to provide a runtime optimization policy . In other embodiments however the runtime optimization policy is not required. If a developer does not provide a runtime optimization policy one or more assumptions may be made about the metrics and constraints that should be utilized to optimize the deployment and execution of the distributed program . For instance in the absence of a developer supplied runtime optimization policy metrics and constraints might be selected that minimize network latency and maximize execution speed while maintaining a reasonable cost of operation. Various factors might be utilized to generate a runtime optimization policy on behalf of the developer . If a runtime optimization policy is required but not provided a query might also be transmitted to the responsible developer requesting that the developer provide the runtime optimization policy .

A Web service API command line interface Web based user interface or other type of interface might be provided through which a developer can specify the runtime optimization policy . For instance a Web based user interface might be provided through which a developer can specify that the distributed program should be optimized for speed of execution and set constraints on the costs that might be incurred during optimized execution. Such an interface might also provide functionality for specifying other types of metrics constraints and preferences regarding the optimization of the deployment and execution of a distributed program .

The runtime optimization policy and the meta description if provided may be utilized to optimize aspects of the distributed program at build time of the program components of the distributed program deployment time of the program components of the distributed program and or execution time of the program components of the distributed program . In one embodiment the program meta description and the runtime optimization policy are provided to an optimization component that is configured to determine an optimized configuration and deployment for the various components of the distributed program in view of the meta description and the specified metrics constraints and execution preferences. Optimization of the distributed program results in an optimized distributed program .

As will be described in greater detail herein the optimization component operates in conjunction with other components in order to build deploy and execute the optimized distributed program in a distributed computing environment . The distributed computing environment provides facilities for the deployment and execution of the program components and the optimized distributed program . The distributed computing environment provides a SOA in one embodiment disclosed herein. It should be appreciated however that the technologies disclosed herein might also be utilized with distributed programs executing in other types of distributed computing environments. Additional details regarding the configuration and operation of a distributed computing environment utilized in one embodiment are described below with regard to .

As will be described below with regard to the optimizations identified and implemented by the optimization component might be performed at many different levels of an operating stack of the distributed computing environment . For instance program components might be containerized into execution environments and re located to different data centers or server computers within the distributed computing environment in order to optimize their execution. For instance a containerized JAVA virtual machine JVM might be utilized to move JAVA programs around within the distributed computing environment for optimized execution. Similarly a containerized JAVASCRIPT engine might be utilized to move JAVASCRIPT programs around within the distributed computing environment for optimized execution.

Other types of optimizations that might be performed include but are not limited to automatically binding containers to appropriate frameworks moving processes closer to data that the processes operate upon placing two related components that communicate with one another in the same data center server computer virtual machine or execution environment placing components in different data centers server computers virtual machines or execution environments to improve scalability optimizing inter component communication resource and dependency sandboxing auto deployment of program components auto establishment of service endpoints auto scaling of virtual machine instances automatic parallelization pre computation map reduce and multi branch evaluation. Other types of optimizations might also be utilized.

Appropriate execution strategies might also be selected by the optimization component and utilized to compile and or execute the program components of a distributed program in an optimized manner. Execution strategies include but are not limited to sequential synchronous execution greedy parallel single host synchronous execution heuristically parallel single host synchronous execution parallel multi host synchronous execution sequential asynchronous execution parallel asynchronous execution caching execution pre computing execution eager branch execution and multiple call single result execution. Other types of execution mechanisms and or strategies might also be utilized.

It should be appreciated that the optimizations described above might also be used individually or in combination. These optimizations might also be performed in view of relevant service level agreements SLAs entered into by the owner and or operator of the distributed computing environment . New execution strategies and or optimization techniques might also be placed into service as they are developed and utilized to further optimize existing distributed programs . Optimizations might also be modified over time in response to changes in the behavior of a distributed program . These types of dynamic runtime optimizations can be performed without additional effort by a developer of a distributed program . Details regarding these and other mechanisms for optimizing a distributed program are provided below.

It should be appreciated that the configuration shown in and the other figures presented herein has been greatly simplified for discussion purposes and that many more hardware and software components might be utilized in order to provide the functionality described herein. In this regard it should also be appreciated that the functionality described herein as provided by the optimization component might be provided by a single component or multiple components including various combinations of components operating within or external to the distributed computing environment . Other implementations will become apparent to those skilled in the art.

It should be appreciated however that it is not necessary for the optimization component to optimize the deployment and execution of all of the program components that make up a distributed program . Only a subset of the program components utilized in a distributed program might be optimized. For instance in the example shown in the optimization component is optimizing aspects of the deployment and or execution of the program components A and B thereby creating an optimized portion of the optimized distributed program . In this example the optimization component has not however performed any optimization upon the program component C thereby resulting in an un optimized portion of the optimized distributed program .

Optimizing only a subset of the program components in a distributed program permits the optimization of existing distributed programs that include program components implemented using many different technologies all of which may not be appropriate for optimization. As an example if a distributed program utilizes a program component that cannot be optimized for some reason then the remainder of the program components within the distributed program may still be optimized. The optimization component might be initially configured to optimize only components that have been implemented utilizing JAVA for instance. Those program components of a distributed program that have been implemented utilizing C would not be optimized. However functionality could later be added to the optimization component for optimizing the deployment and execution of C programs. Other types of optimization features might also be added to the optimization component at a later time.

The distributed computing environment can provide computing resources for executing distributed programs on a permanent or an as needed basis. The computing resources provided by the distributed computing environment may include various types of resources such as data processing resources data storage resources data communication resources and the like. Each type of computing resource may be general purpose or may be available in a number of specific configurations. For example data processing resources may be available as virtual machine instances. The instances may be configured to execute programs including Web servers application servers media servers database servers and other types of components. Data storage resources may include file storage devices block storage devices and the like.

Each type or configuration of computing resource may be available in different sizes such as large resources consisting of many processors large amounts of memory and or large storage capacity and small resources consisting of fewer processors smaller amounts of memory and or smaller storage capacity. Customers may choose to allocate a number of small processing resources as Web servers and or one large processing resource as a database server for example.

The computing resources provided by the distributed computing environment are furnished by server computers and other components operating in one or more data centers A D which may be referred to herein singularly as a data center or collectively as the data centers . The data centers are facilities utilized to house and operate computer systems and associated components for providing a distributed computing environment. The data centers typically include redundant and backup power communications cooling and security systems. The data centers might also be located in geographically disparate locations. One illustrative configuration for a data center that implements aspects of the concepts and technologies disclosed herein for optimized deployment and execution of programs in a distributed computing environment will be described below with regard to .

Users of the distributed computing environment may access the computing resources provided by the data centers over a wide area network WAN . Although a WAN is illustrated in it should be appreciated that a local area network LAN the Internet or any other networking topology known in the art that connects the data centers to remote customers and other users may be utilized. It should also be appreciated that combinations of such networks might also be utilized.

The distributed computing environment might provide various interfaces through which aspects of its operation may be configured. For instance various application programming interfaces API may be exposed by components operating in the distributed computing environment for configuring various aspects of its operation. Other mechanisms for configuring the operation of components in the distributed computing environment might also be utilized.

According to embodiments disclosed herein the capacity of resources provided by the distributed computing environment can be scaled in response to demand. In this regard scaling refers to the process of instantiating which may also be referred to herein as launching or creating or terminating which may also be referred to herein as de scaling instances of computing resources in response to demand.

Auto scaling is one mechanism for scaling computing resources in response to increases or lulls in demand for the resources. Auto scaling allows customers of the distributed computing environment to configure the environment to scale their purchased computing resources according to conditions defined by the customer. For instance rules may be defined for scaling up capacity in a particular manner in response to the occurrence of specified conditions such as a spike in demand. Similarly rules might also be defined to scale down capacity in a particular manner in response to the occurrence of other conditions such as a lull in demand.

The distributed computing environment might also be configured with a deployment component to assist customers in the deployment of new instances of computing resources. The deployment component may receive a configuration from a customer that includes data describing how new instances should be configured. For example the configuration might specify one or more applications or software components that should be installed in new instances provide scripts and or other types of code to be executed in new instances provide cache warming logic specifying how an application cache should be prepared and other types of information. The deployment component utilizes the customer provided configuration and cache warming logic to launch configure and prime new instances of computing resources. Additional details regarding the functionality provided by the data centers will be provided below with regard to .

The server computers may be standard tower or rack mount server computers configured appropriately for executing a distributed program . For example the server computers might be configured to execute the various program components of the distributed program and to enable data communication between the program components and other computing systems.

The server computers might execute the program components directly. For instance the server computers might execute an operating system and execute the program components directly on the operating system. Compiled C programs for instance might be executed in this manner. The server computers might also be configured to execute a virtual machine manager VMM on top of an executing operating system. The VMM might be a hypervisor or another type of program configured to enable and manage the execution of multiple instances on a single server for example. Compiled and other types of programs might be executed in the virtual machine instances.

Programs that need other types of execution environments to execute might also be executed within execution environments executing on the server computers or within virtual machine instances. For example JAVA and JAVASCRIPT programs might be executed in this manner. Other types of execution environments might be provided on the server computers for executing procedural programs scripts compiled or interpreted programs and other types of programs. Additionally each server computer might be configured to provide multiple execution environments. For example a single server might provide functionality for executing virtual machine instances JAVA programs JAVASCRIPT programs and other types of programs.

As will be discussed in detail below the program components may be containerized in various embodiments to provide flexibility in locating the program components in the distributed execution environment for efficient execution. Containerization refers to a process of integrating one or more program components within an appropriate and re locatable execution environment. For instance a JAVASCRIPT program might be containerized within an execution environment for executing the JAVASCRIPT program. The containerized JAVASCRIPT program can then be deployed to a server computer within the distributed computing environment identified for efficient execution. Additional details regarding this process will be provided below with regard to .

The data center A shown in also includes a server computer F reserved for executing software components for managing the operation of the data center A and the server computers . In particular the server computer F might execute the optimization component described briefly above. As mentioned previously the optimization component utilizes a meta description and a runtime optimization policy to optimize the execution of a distributed program .

Once the optimization component has determined an optimized configuration for the distributed program the optimization component might utilize other components to deploy and execute the components of the optimized distributed program on the servers in the data center A and potentially other data centers . For example the optimization component might utilize a deployment component to assist in the deployment of the program components to server computers in the data center A or other data centers within the distributed computing environment.

In embodiments the optimization component also utilizes an auto scaling component to auto scale instances of computing resources like virtual machine instances in response to fluctuations in demand for the resources. The auto scaling component and the deployment component may execute on a single server computer F or in parallel across multiple server computers in the data center A.

In addition the auto scaling component and the deployment component may consist of a number of subcomponents executing on different server computers F or other computing devices in the distributed computing environment. The auto scaling component and the deployment component may be implemented as software hardware or any combination of the two. The auto scaling component may monitor available computing resources in the distributed computing environment over an internal management network for example.

It should be appreciated that the architecture of the server computers shown in has been simplified for discussion purposes. In this regard the components illustrated as executing on the server computer F are merely illustrative. Many other software and hardware components might also be utilized to build configure deploy and execute the program components of the distributed program in an optimized way.

In the example data center A shown in an appropriate LAN is utilized to interconnect the server computers . The LAN is also connected to the WAN illustrated in . It should be appreciated that the network topology illustrated in has also been greatly simplified for discussion purposes and that many more networks and networking devices may be utilized to interconnect the various computing systems disclosed herein. Appropriate load balancing devices or software modules might also be utilized for balancing a load between each of the data centers between each of the server computers in each data center and between instances executing within the distributed computing environment. These network topologies and devices should be apparent to those skilled in the art.

It should also be appreciated that the data center A described in is merely illustrative and that other implementations might be utilized. In particular functionality described herein as being performed by the optimization component the auto scaling component and the deployment component might be performed by one another might be performed by other components or might be performed by a combination of these or other components. Additionally it should be appreciated that this functionality might be implemented in software hardware or a combination of software and hardware. Other implementations should be apparent to those skilled in the art.

The program components might be created by the developers utilizing different technologies suitable for use in creating applications that are executable in a distributed computing environment that implements a SOA. For instance in the example shown in the program component A has been created using JAVA the program component B has been created using JAVASCRIPT the program component C has been created using C and the program component D has been created using PERL.

The distributed program might be configured to make use of each of the program components shown in even though these program components have been authored by different independent developers using different technologies. As mentioned briefly above the optimization of the distributed program as described herein does not impact the independence that each of the developers has to independently author and deploy program components using the technologies of their own choosing.

As also shown in and described briefly above a developer E of the distributed program might generate a meta description for the distributed program . The meta description is a high level description of the execution of the various program components that make up the distributed program . The meta description might be expressed using a standardized meta programming language such as XML syntax for representing graphs. Other languages and mechanisms might also be utilized to describe create a meta description that describes the configuration and operation of the program components of a distributed program .

In one implementation the meta description includes data A that identifies the program components that make up the distributed program . The meta description might also include for each of the program components data B that defines the consumable interfaces exposed by the program components . The consumable interfaces are interfaces exposed by the program components that can be called by other program components . The data B might also identify other program components that call the consumable interfaces.

In embodiments the meta description also includes for each of the program components data C that defines the dependency adapters called by the program components . The dependency adapters are interfaces that the program components might call. Additional details regarding the form and content of the meta description utilized in one embodiment disclosed herein are provided below with regard to .

The left hand side of specifies the various consumable interfaces exposed by the program components of the distributed program . For instance various interfaces might be exposed by a program component such as but not limited to a Simple Object Access Protocol SOAP interface a RESTful interface a command line interface and or a JAVA or library interface. Other types of interfaces might also be exposed by the program components of a distributed program .

The right hand side of specifies one or more dependency adapters utilized by each program component for binding to dependencies. For instance and without limitation program components might call JAVA JAVASCRIPT SOAP RESTful and command line interfaces exposed by other program components. The program components might also call other types of adapters not illustrated in .

The optimization component might determine that one or more program components of the distributed program should be containerized in order to optimize for the metrics specified by the developer of a distributed program . In response thereto the optimization component might work in conjunction with other components to perform the containerization. The optimization component might then operate with the deployment component to deploy the containerized program component to a server computer selected to optimize the execution of the distributed program that utilizes the containerized program component . The server computer to which the JVM is deployed might be selected to maximize the efficiency of execution of the JAVA programs A and B in the context of the entire distributed program and the metrics and constraints specified by the developer of the distributed program .

In a similar fashion the JAVASCRIPT programs A and B which ordinarily execute in separate JAVASCRIPT engines A and B respectively might be containerized into the containerized JAVASCRIPT engine . Containerizing in this fashion provides significant flexibility in selecting a server for executing the JAVASCRIPT programs. It should be appreciated however that containerization of program components in the manner presented above is not limited to JAVA and JAVASCRIPT programs.

For instance as shown in executable C programs A and B that typically execute in separate virtual machines A and B might be containerized into a containerized virtual machine for execution. Similarly as shown in other types of executable programs A and B might be containerized into other types of deployment packages for deployment and execution on a physical computer or a virtual machine . In this regard it should be appreciated that the examples shown in are merely illustrative and that many other types of program components might be containerized into other types of containers for execution within a distributed computing environment.

As also mentioned briefly above once the program components have been containerized the containers can be deployed to locations in the distributed computing environment in order to optimize for the metrics and constraints specified by the developer of the distributed program . For instance containers may be located close together i.e. on the same server in the same rack of servers or in the same data center when possible to reduce latency and data movement between the containerized programs. Similarly containerized programs might be placed close together or close to dependent resources in order to eliminate or reduce remote network calls. Other types of optimizations might also be performed on containerized and non containerized program components . Other types of optimizations will be described below with regard to .

For example the optimization component might optimize execution by co locating two or more service endpoints A on a network F in order to optimize their execution in the context of the distributed program . Similarly the optimization component might configure two or more virtual machine images B for execution by the same VMM E in order to optimize their execution. The optimization component might also cause an operating system O S deployment environment C to be deployed on various O S platforms D in order to optimize for the metrics and constraints specified by the developer of the distributed program .

In a similar fashion two or more JAVA programs D might be executed on the same JVM C in order to optimize their execution. Containerized JAVASCRIPT programs E might also be deployed to an appropriate JAVASCRIPT execution environment B and placed at an appropriate location in the distributed computing environment in order to optimize their execution. Other program components optimizing at other layers might also be containerized and executed at different locations within the distributed computing environment in order to optimize their execution in the context of the distributed program and the metrics and constraints set forth by the developer of the distributed program .

As discussed above the meta description and the runtime optimization policy may be utilized to determine appropriate optimizations for the program components of the distributed program . is a stack diagram illustrating this process further. In particular illustrates that the runtime optimization policy and the program meta description may be utilized to perform optimizations from the top of the SOA stack which includes the JAVASCRIPT execution environment B in one embodiment all the way to the network F layer at the bottom of the stack.

The optimization component can utilize the runtime optimization policy and the meta description of the distributed program to determine appropriate optimizations at the various levels of the stack. The optimization component can then operate in conjunction with other programs and or components within or external to the distributed execution environment to cause the distributed program to be built deployed and executed in the determined manner. Additional details regarding this process will be performed below with regard to .

The implementation of the various components described herein is a matter of choice dependent on the performance and other requirements of the computing system. Accordingly the logical operations described herein are referred to variously as operations structural devices acts or modules. These operations structural devices acts and modules may be implemented in software in firmware in special purpose digital logic and any combination thereof. It should also be appreciated that more or fewer operations may be performed than shown in the Figures and described herein. These operations may also be performed in parallel or in a different order than those described herein.

The routine begins at operation where the developers provide the program components of a distributed program . As discussed above the program components might be developed independently and utilizing different technologies.

From operation the routine proceeds to operation where a developer such as the developer A provides a meta description of a distributed program that makes use of one or more of the program components . As discussed above with regard to the meta description might identify the various program components utilized by the distributed program and for each of the program components specify the consumable interfaces exposed and the dependency adapters utilized. The meta description might also specify other aspects regarding the construction configuration and or operation of the distributed program .

From operation the routine proceeds to operation where the developer A provides the runtime optimization policy . As discussed above the runtime optimization policy specifies one or more metrics for which the deployment and execution of the distributed program should be optimized. The runtime optimization policy might also specify one or more constraints that should also be considered when optimizing the distributed program . The runtime optimization policy might also include preferences regarding the manner in which the distributed program is executed. The runtime optimization policy might also include other types of instructions to be considered by the optimization component when determining an optimal deployment and execution strategy for the distributed program .

From operation the routine proceeds to operation where the optimization component and or other components utilize the meta description and or the runtime optimization policy to perform build time optimization on the program components of the distributed program . As used herein the term build time refers to a time prior to the deployment of the program components within the distributed execution environment. For example build time might encompass the time at which the program components are compiled and or linked. Additional details regarding the build time optimization performed by the optimization component will be provided below with regard to .

From operation the routine proceeds to operation where the optimization component and or other components utilize the meta description and or the runtime optimization policy to perform deployment time optimization on the program components of the distributed program . As used herein the term deployment time refers to the time at which the program components are deployed to the distributed execution environment by the deployment component or another component. Additional details regarding the deployment time optimization performed by the optimization component will be provided below with regard to .

From operation the routine proceeds to operation where the optimization component and or other components utilize the meta description and or the runtime optimization policy to perform runtime optimization on the program components of the distributed program . As used herein the term runtime refers to the time at which one or more of the program components of the distributed program are executed within the distributed execution environment. As will be described in greater detail below with regard to optimization might be dynamically performed in a continuous fashion as the distributed program is executed. In this way new optimizations can be applied to the execution of the distributed program as they are developed. Additional details regarding this process and other aspects of the runtime optimization performed by the optimization component will be provided below with regard to .

The right hand side of illustrates some of the possible execution plans that might be selected by the optimization component . In particular sequential synchronous execution refers to executing a series of instructions one by one sequentially where maximum latency is capped but successful completion is not guaranteed. Greedily parallel single host synchronous execution refers to executing a series of instructions with independent instructions each placed into their own execution thread. Heuristically parallel single host synchronous execution refers to executing a series of instructions and giving input output I O instructions a dedicated thread for non blocking I O. Sequences of computational instructions are also assigned to a thread pool no larger than the number of cores on the host computer.

Parallel multi host synchronous execution refers to execution by finding the N largest or most expensive independent sub parts of a program. Each of the identified sub parts might be registered as private methods on all service workers in a fleet of machines or virtual machine instances. When a request comes in for the initial program parallel remote requests are made for each method implementing one of its sub parts. This type of execution may be tuned to only make a sub part of a program remote when it is more computationally expensive than the serialization deserialization required to call that part of the program.

In sequential asynchronous execution a series of instructions are executed one by one. All of the executed instructions are guaranteed to be completed but the maximum amount of time this may take is unbounded asynchronous . In parallel asynchronous execution a series of instructions are executed with independent instructions started in sequence. Execution may wait until each independent execution produces a result to continue.

In caching execution the result for every function call made by a program e.g. JAVA method service operation etc. is cached in association with the function inputs. The cached value is returned on subsequent calls to the same function with the same input. In precomputing execution functions are called before they are needed e.g. on system startup every N minutes reactively on a change to an input value etc. and results are cached for some set of the most common inputs to each function.

In eager branch execution both sides of a conditional branch are executed in parallel. When the predicate of the conditional branch returns the execution of the appropriate thread is continued and the other thread is aborted. In multiple call single result execution for any remote service call or any call that may fail or have widely variable latency independently of input the call is made N times in parallel. The first result that is returned is utilized. This technique might optimize both reliability and latency at the cost of total cost of ownership.

It should be appreciated that the execution plans described above are merely illustrative and that other plans might be utilized. Additionally new execution plans might be added and utilized in the future as they are developed.

From operation the routine proceeds to where program components might be containerized if appropriate to support optimized execution. Utilizing the meta description and or the runtime optimization policy the optimization component can determine which of the program components if any should be containerized prior to deployment to the distributed computing environment.

From operation the routine proceeds to operation . As mentioned above the optimization component might determine optimal mechanisms for enabling inter component communication within the distributed computing environment thereby freeing the developer of the distributed program from specifying all of the implementation details of the distributed program . At operation the optimization component utilizes the meta description and the runtime optimization policy to determine appropriate frameworks for inter component communication and binds the program components of the distributed program to the identified frameworks.

From operation the routine proceeds to operation where the optimization component might perform other build time optimizations on the program components of the distributed program to optimize for the metrics and constraints specified by the developer of the distributed program . The routine then proceeds to operation where it returns to operation of where deployment time optimizations are performed in the manner described below with regard to .

From operation the routine proceeds to operation where the optimization component deploys the program components and any containerized program components to the selected locations within the distributed computing environment. For instance a component might be deployed to a particular data center to a rack of server computers to a particular server computer in a rack of server computers to a virtual machine executing in a certain location or to another execution environment or locality based upon the location selected at operation for optimizing the execution of the distributed program in view of the metrics and constraints specified in the runtime optimization policy .

From operation the routine proceeds to operation where the optimization component configures the program components for inter component communication according to the frameworks to which the program components were bound at operation described above. For example the internet protocol IP address port number and other information might be set for a program component that is based upon the location to which the program component was deployed at operation . Other types of configuration of the deployed program components might also be performed at operation . Once the configuration has been completed the routine proceeds to operation where the routine returns to operation of the routine where runtime optimization is performed in the manner described below with regard to .

From operation the routine proceeds to operation where the execution of the distributed program is monitored. For instance resource utilization inter process communication latency and other aspects of the operation of the distributed program may be monitored. The execution of the distributed program may then be further optimized based upon the results of the monitoring. For example the execution plan for the components of the distributed program might be modified based upon the actual execution characteristics of the distributed program . Similarly program components might be relocated within the distributed computing environment depending upon the manner in which they are executing.

New execution strategies might also be applied to the execution of the distributed program that were not available at the build time or deployment time of the distributed program . For example new mechanisms for optimization might be added in order to take advantage of advances in hardware or software. This process may continue indefinitely while the program components of the distributed program are executing.

When the optimization component encounters a configuration as shown in the optimization component might cause the JAVA programs A and B to be containerized for execution within the same JVM. Additionally the optimization component might bind the JAVA programs A and B for communication by way of a shared memory rather than an IP network.

Other mechanisms might also be utilized to optimize communication between the JAVA programs A and B. In this regard network calls from one JAVA program A to the other JAVA program B may be transparently resolved to local calls or to the shared memory . Local call optimization can include simple IP optimization local redirect and shared memory optimized calls that do not require serialization and de serialization. These types of optimizations can greatly reduce latency. It should be appreciated that the concepts illustrated in might also be applied to other types of program components such as but not limited to JAVASCRIPT programs executing in a containerized JAVASCRIPT execution environment.

In order to optimize network communication between the program components A and B at the network layer the VMM A can determine that the destination IP address for network traffic originating from one of the program components A or B is another program component A or B executing in a virtual machine managed by the VMM A. In this case the VMM A can route the network traffic directly between the virtual NICs A and B assigned to each of the virtual machines A and B. In this way the VMM A can optimize network communication between the program components A and B executing in the virtual machines A and B by not sending traffic over the network . This may reduce latency and also decreases transmission time.

In order to route network traffic originating at one of the virtual machines A or B to another of the virtual machines A or B the VMM A utilizes a memory buffer buffer in one implementation. The memory address of the buffer may be handed to one of the virtual machines A or B that is transmitting data. Once the transmitting virtual machine A or B has placed the outgoing message s into the buffer the memory address of the buffer might then be provided to the virtual machine A or B that is the destination for the transmitted messages. In this way network messages can be transmitted between program components A and B executing on different virtual machines A and B respectively without making a copy of the transmitted data.

In some implementations the VMM A is also configured to implement one or more mechanisms to prevent one of the virtual machines A or B from overwriting the buffer while the buffer is in use by another one of the virtual machines A or B. For example the VMM A is configured in some embodiments to implement a copy on write mechanism for mediating access to the buffer . In order to implement the copy on write mechanism the VMM A may redirect write operations generated by one of the virtual machines A or B to a copy of the buffer to prevent a virtual machine A or B from overwriting data placed into the buffer by another virtual machine A or B. This process is transparent to the program components A and B and the virtual machines A and B.

In some implementations the VMM A is configured to implement an execution scheduling management mechanism to prevent one of the virtual machines A or B from overwriting the buffer while in use by another one of the virtual machines A or B. In order to implement this mechanism the VMM A schedules the execution of the virtual machines A and B in order to prevent conflicting write operations to the buffer . For example the VMM A might block the execution of one of the virtual machines A or B until such time as the program component executing in the other virtual machine A or B has completed its processing of the buffer .

In some implementations an operating system driver for the virtual NICs A and B might also be configured to utilize received data directly from the buffer rather than making a local copy of the received data. In these embodiments the operating system driver might verify the integrity of the data stored in the buffer but not make a local copy of the data. In this way the operating system driver can provide a further enhancement of prior solutions that would require the operating system driver to make a local copy of the contents of the buffer .

From operation the routine proceeds to operation where the VMM A implements the memory buffer for passing network messages between the program components A and B executing on the virtual machines A and B respectively. As discussed above the VMM A may enable the program components A and B to communicate by way of the buffer in a manner that minimizes copying of the buffer .

As also discussed above the VMM A might implement one or more mechanisms to prevent the program components A and B from overwriting the contents of the buffer when the buffer is in use by another one of the program components A and B and or for minimizing copying of the buffer . For example at operation the VMM A may implement the copy on write mechanism described above for minimizing copying of the buffer and for preventing buffer access conflicts. At operation the VMM A implements the execution scheduling management mechanism described above to avoid buffer access conflicts. It should be appreciated that the copy on write and execution scheduling management mechanisms described above might be implemented separately or together. It should also be appreciated that the VMM A might implement other mechanisms in other embodiments to prevent conflicting read and or write operations from being performed on the buffer and to minimize copying of the buffer . From operation the routine proceeds to operation where it ends.

In another implementation illustrated in a shared memory is utilized by both of the programs A and B in order to share messages and other data. In this example the programs A and B are configured for communicating with each other across a network but are actually communicating through the shared memory . This communication is managed by the VMM A.

In the example shown in when communication is between the two virtual machines A and B the shared memory controlled by the VMM A is utilized rather than traversing the entire IP stack. The program components A and B executing on each virtual machine A and B place messages into and retrieve messages from the shared memory .

In some embodiments an application programming interface API library or other type of program code is utilized to provide an interface to the program components A and B for accessing the shared memory . For example the API a library or another type of program code might implement various mechanisms for permitting the program components A and B to write to and read from the shared memory . The API library or other component might also provide mechanisms to prevent access conflicts such as the copy on write and execution scheduling management mechanisms described above. The VMM A might also provide this functionality in other embodiments. Mechanisms such as the copy on write mechanism described above might also be utilized to minimize copying of the shared memory . Additionally in some embodiments file level primitives might be utilized to access the shared memory .

From operation the routine proceeds to operation where communication is enabled between the program components A and B executing in the virtual machines A and B respectively by way of the shared memory . As mentioned above the program components A and B might utilize an API library or another type of component in order to read and write data to and from the shared memory .

From operation the routine proceeds to operation where one or more mechanisms are implemented in order to prevent inconsistent read and write operations from being performed on the shared memory . For example and as described above a copy on write mechanism and or an execution scheduling management mechanism might be utilized. Other mechanisms might also be utilized in order to prevent conflicting operations from being performed on the shared memory . Mechanisms such as copy on write might also be implemented in order to minimize copying of the shared memory . From operation the routine proceeds to operation where it ends.

In another implementation illustrated in the VMM A utilizes a shared memory to provide a shared data structure . The shared data structure is shared between two or more program components A and B executing in different virtual machines A and B. The VMM A mediates access to the shared data structure by the program components A and B.

In order to mediate access to the shared data structure an API library or other type of program component might be provided that the program components A and B can utilize to write to read from and potentially perform other types of operations on the shared data structure . In contrast to the embodiments disclosed above with regard to the mechanism disclosed in may not require the program components A to serialize data prior to storing the data in the shared data structure . Similarly the mechanism disclosed in does not require the program components A to de serialize data after reading the data from the shared data structure . The mechanism shown in might also not require serialization and deserialization of data placed into the shared memory .

According to embodiments the VMM A or another component might provide mechanisms to prevent destructive modifications to the shared data structure. For example the VMM A might implement the copy on write mechanism and or the execution scheduling management mechanism described above in order to prevent the program components A and B from destructively modifying the contents of the shared data structure. In other embodiments the VMM A or another component might implement a record or element level locking mechanism to prevent inconsistent read and write operations from being performed on the shared data structure . Through such a mechanism the VMM A can lock records or elements of the shared data structure while the records or elements are being modified. Other mechanisms might also be utilized in order to prevent conflicting operations from being performed on the shared data structure and potentially to minimize copying of the shared data structure . These mechanisms might be utilized individually or in combination.

From operation the routine proceeds to operation where communication is enabled between the program components A and B executing in the virtual machines A and B respectively by way of the shared data structure . As mentioned above the program components A and B might utilize an API library or another type of component in order to read and write data to and from the shared data structure .

From operation the routine proceeds to operation where one or more mechanisms are implemented in order to prevent inconsistent read and write operations from being performed on the shared data structure . For example and as described above a copy on write mechanism and or an execution scheduling management mechanism might be utilized. Additionally a record or element level locking mechanism might be implemented in other embodiments. Other mechanisms might also be utilized in order to prevent conflicting operations from being performed on the shared data structure and to minimize copying of the shared data structure . From operation the routine proceeds to operation where it ends.

In some embodiments the programs A and B executing in the virtual machines A and B respectively might be permitted to control access to the shared memory or the shared data structure . For example the program A might inform the program B that the shared memory or the shared data structure is available for writing. When the program B has completed writing to the shared memory or the shared data structure the program B might return control of the shared memory or shared data structure to the program A. In one embodiment an out of band synchronization protocol is utilized between the programs A and B to exchange these types of synchronization messages. For example synchronization messages such as these might be transmitted between the programs A and B over a network channel.

It should be appreciated that the mechanisms described above with regard to can be used to optimize communications when two or more program components are executing on the same physical computer but in different virtual machines. Other mechanisms might also be utilized to optimize the communication between two programs executing in different virtual machines managed by the same VMM A. In this way optimizations may be implemented in the VMM A to eliminate communication over a network such as the WAN .

Communication between program components A and B that execute in different data centers A and B respectively might also be optimized in various ways as shown in . For example the optimization component might cause the program components A and B to be deployed and executed within the same data center A as shown in rather than in different data centers as shown in . In this way network traffic over a WAN may be reduced or even eliminated. In a similar fashion the optimization component might cause program components that communicated with one another to be deployed to the same rack of server computers to server computers connected to the same subnet or deployed in other ways likely to reduce LAN or WAN traffic between the components. Other types of optimizations might also be performed.

It should be appreciated that the various optimizations described above are merely illustrative. In other implementations the optimization component might cause other types of optimizations to be implemented based upon the metrics constraints and preferences set forth in the runtime optimization policy and the description of the distributed program set forth in the meta description .

The computer includes a baseboard or motherboard which is a printed circuit board to which a multitude of components or devices may be connected by way of a system bus or other electrical communication paths. In one illustrative embodiment one or more central processing units CPUs operate in conjunction with a chipset . The CPUs may be standard programmable processors that perform arithmetic and logical operations necessary for the operation of the computer .

The CPUs perform operations by transitioning from one discrete physical state to the next through the manipulation of switching elements that differentiate between and change these states. Switching elements may generally include electronic circuits that maintain one of two binary states such as flip flops and electronic circuits that provide an output state based on the logical combination of the states of one or more other switching elements such as logic gates. These basic switching elements may be combined to create more complex logic circuits including registers adders subtractors arithmetic logic units floating point units and the like.

The chipset provides an interface between the CPUs and the remainder of the components and devices on the baseboard. The chipset may provide an interface to a random access memory RAM used as the main memory in the computer . The chipset may further provide an interface to a computer readable storage medium such as a read only memory ROM or non volatile RAM NVRAM for storing basic routines that help to startup the computer and to transfer information between the various components and devices. The ROM or NVRAM may also store other software components necessary for the operation of the computer in accordance with the embodiments described herein.

The computer may operate in a networked environment using logical connections to remote computing devices and computer systems through a network such as the local area network . The chipset may include functionality for providing network connectivity through a NIC such as a gigabit Ethernet adapter. The NIC is capable of connecting the computer to other computing devices over the network . It should be appreciated that multiple NICs may be present in the computer connecting the computer to other types of networks and remote computer systems.

The computer may be connected to a mass storage device that provides non volatile storage for the computer. The mass storage device may store system programs application programs other program modules and data which have been described in greater detail herein. The mass storage device may be connected to the computer through a storage controller connected to the chipset . The mass storage device may consist of one or more physical storage units. The storage controller may interface with the physical storage units through a serial attached SCSI SAS interface a serial advanced technology attachment SATA interface a fiber channel FC interface or other type of interface for physically connecting and transferring data between computers and physical storage units.

The computer may store data on the mass storage device by transforming the physical state of the physical storage units to reflect the information being stored. The specific transformation of physical state may depend on various factors in different implementations of this description. Examples of such factors may include but are not limited to the technology used to implement the physical storage units whether the mass storage device is characterized as primary or secondary storage and the like.

For example the computer may store information to the mass storage device by issuing instructions through the storage controller to alter the magnetic characteristics of a particular location within a magnetic disk drive unit the reflective or refractive characteristics of a particular location in an optical storage unit or the electrical characteristics of a particular capacitor transistor or other discrete component in a solid state storage unit. Other transformations of physical media are possible without departing from the scope and spirit of the present description with the foregoing examples provided only to facilitate this description. The computer may further read information from the mass storage device by detecting the physical states or characteristics of one or more particular locations within the physical storage units.

In addition to the mass storage device described above the computer may have access to other computer readable storage media to store and retrieve information such as program modules data structures or other data. It should be appreciated by those skilled in the art that computer readable storage media can be any available media that provides for the storage of non transitory data and that may be accessed by the computer .

By way of example and not limitation computer readable storage media may include volatile and non volatile removable and non removable media implemented in any method or technology. Computer readable storage media includes but is not limited to RAM ROM erasable programmable ROM EPROM electrically erasable programmable ROM EEPROM flash memory or other solid state memory technology compact disc ROM CD ROM digital versatile disk DVD high definition DVD HD DVD BLU RAY or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium that can be used to store the desired information in a non transitory fashion.

The mass storage device may store an operating system utilized to control the operation of the computer . According to one embodiment the operating system comprises the LINUX operating system. According to another embodiment the operating system comprises the WINDOWS SERVER operating system from MICROSOFT Corporation. According to further embodiments the operating system may comprise the UNIX or SOLARIS operating systems. It should be appreciated that other operating systems may also be utilized. The mass storage device may store other system or application programs and data utilized by the computer such as the optimization component the runtime optimization policy the program meta description and or the other software components and data described above.

In one embodiment the mass storage device or other computer readable storage media is encoded with computer executable instructions which when loaded into the computer transforms the computer from a general purpose computing system into a special purpose computer capable of implementing the embodiments described herein. These computer executable instructions transform the computer by specifying how the CPUs transition between states as described above. According to one embodiment the computer has access to computer readable storage media storing computer executable instructions which when executed by the computer perform the routines and described with regard to respectively.

The computer may also include an input output controller for receiving and processing input from a number of input devices such as a keyboard a mouse a touchpad a touch screen an electronic stylus or other type of input device. Similarly the input output controller may provide output to a display such as a computer monitor a flat panel display a digital projector a printer a plotter or other type of output device. It will be appreciated that the computer may not include all of the components shown in may include other components that are not explicitly shown in or may utilize an architecture completely different than that shown in .

Based on the foregoing it should be appreciated that technologies for optimized communication between program components executing in virtual machines on the same host computer have been presented herein. Although the subject matter presented herein has been described in language specific to computer structural features methodological acts and computer readable media it is to be understood that the invention defined in the appended claims is not necessarily limited to the specific features acts or media described herein. Rather the specific features acts and mediums are disclosed as example forms of implementing the claims.

The subject matter described above is provided by way of illustration only and should not be construed as limiting. Furthermore the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure. Various modifications and changes may be made to the subject matter described herein without following the example embodiments and applications illustrated and described and without departing from the true spirit and scope of the present invention which is set forth in the following claims.

