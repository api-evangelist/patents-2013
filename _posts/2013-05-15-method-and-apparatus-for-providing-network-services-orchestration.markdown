---

title: Method and apparatus for providing network services orchestration
abstract: A method and apparatus for providing network services orchestration are described herein. The apparatus comprises a network controller that runs a network services orchestration module. The network services orchestration module has a service management northbound application programming interface (API), an instance management submodule, an autoscaling and power management submodule, and an instance location southbound API. A steering module is also described herein that includes a steering northbound API and an instance location northbound API.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09491063&OS=09491063&RS=09491063
owner: Telefonaktiebolaget LM Ericsson (publ)
number: 09491063
owner_city: Stockholm
owner_country: SE
publication_date: 20130515
---
The present invention relates to providing network services orchestration. More particularly the present invention relates to handling conflicting needs of network services for performance versus scale.

Cloud computing has introduced the concept of service orchestration. Service orchestration involves the following functions 

The services orchestrated by cloud orchestration tools are typically end user services like Web applications ecommerce applications etc.

Another class of services is network services. Network services perform operations on flows of packets in real time prior to the delivery of these packets to the end user or to end user services. Examples of such applications include load balancers for ensuring traffic loads are spread among service VMs deep packet inspection DPI to check for flows that have security problems firewalls to exclude particular kinds of traffic media gateways for audio video traffic and Web proxies of various sorts etc. These services often have real time performance requirements since they interpose on the delivery of packets between the source and destination so a person is typically waiting for a response on the destination side. Often a chain of such services must be run on a packet flow and different services must be run on different packet flows. For example if a packet flow is identified as a YouTube video DPI need not be run on it since the content is known and does not pose a security risk but perhaps an ad insertion service might.

The problem with the first solution is that it does not allow for multiple services in the service chain. If the orchestration is tied to a single service like load balancing it becomes very difficult to include other services in the service chain. Complex routing solutions using access control lists ACLS must be programmed into the switches and routers and if another service is a hardware service rearranging the routing to reach the location of the hardware service may require manual intervention.

The problem with the second solution is that some network services are either not of interest to the designer of an end user service or need to be inserted by default. For example the designer of an end user service should not need to be aware that DPI is running on their traffic and therefore should not need to be responsible for including DPI in their service orchestration. The DPI service is both to protect the end user service and to protect the network operator.

The problem with the third solution is that it does not allow for hardware implementations for network services nor for implementations that run on the bare metal with system software optimized for high performance packet forwarding because the services must run in VMs and in some instances VMs packaged with specific agents. Because the services must be packaged as VMs hardware based services or services running on bare metal with optimized system software for packet processing are not usable.

What is needed therefore is a clean network services orchestration solution that can optimally place and manage a chain of network services the ordering of which can differ depending on the particular flows that need processing and can arrange for flows to be routed through the services in the proper order.

A method and apparatus for providing network services orchestration is disclosed. In one embodiment a network services orchestration module runs on a network controller. Service data is specified for a network operator using a service management northbound application programming interface API . The specified service data can include at least one of service instance pools service instances and performance types.

Virtual machines VMs and specialized APIs are managed in response to operator requests from the service management northbound API using an instance management submodule. The hypervisor can be used to manage VMs implementing virtual appliances. A specialized API can be used to manage service instances. The managed service instances can be hardware based service instances or instances implemented on bare metal servers.

Statistics are monitored and service instances are affected using an autoscaling and power management submodule. Statistics can be monitored from hypervisors running virtual service appliances and switches. Affecting service instances can include scheduling new instances and shutting down instances having no load.

Changes in deployed network services instance availability are pushed to a steering module using an instance location southbound API. The instance location southbound API pushes the changes to an instance location northbound API of the steering module.

A service instance can be added to a pool of instances using the instance management submodule and load monitoring can be set up using the autoscaling and power management submodule.

Returned statistics can be compared to established upper and lower limits on a service pool using the autoscaling and power management submodule.

When a service instance is marked as overloaded the instance management submodule can end an instance that is a hardware or bare metal instance. When the instance is a virtual appliance the instance management submodule can start a new virtual appliance or increase a number of virtual machines.

The instance management module can delete a service instance when a number of flows is below a lower limit.

Also disclosed is a steering module. In one embodiment the steering module includes a steering northbound API and an instance location northbound API. The steering northbound API allows subscriber based and policy based rules to be pushed down to the steering module. The instance location northbound API allows changes in deployed network services instance availability to be pushed down to the steering module by the network services orchestration module. The instance location northbound API provides identity topological location and performance type service data to the steering module. A flow steering southbound API is coupled to the steering module and pushes the rules and deployed network services instance availability to one or more switches.

In the following description numerous specific details are set forth. However it is understood that embodiments of the invention may be practiced without these specific details. In other instances well known circuits structures and techniques have not been shown in detail in order not to obscure the understanding of this description. It will be appreciated however by one skilled in the art that the invention may be practiced without such specific details. Those of ordinary skill in the art with the included descriptions will be able to implement appropriate functionality without undue experimentation.

References in the specification to one embodiment an embodiment an example embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is submitted that it is within the knowledge of one skilled in the art to effect such feature structure or characteristic in connection with other embodiments whether or not explicitly described.

In the following description and claims the terms coupled and connected along with their derivatives may be used. It should be understood that these terms are not intended as synonyms for each other. Coupled is used to indicate that two or more elements which may or may not be in direct physical or electrical contact with each other co operate or interact with each other. Connected is used to indicate the establishment of communication between two or more elements that are coupled with each other.

As used herein a network element e.g. a router switch bridge is a piece of networking equipment including hardware and software that communicatively interconnects other equipment on the network e.g. other network elements end stations . Some network elements are multiple services network elements that provide support for multiple networking functions e.g. routing bridging switching Layer 2 aggregation session border control Quality of Service and or subscriber management and or provide support for multiple application services e.g. data voice and video . Subscriber end stations e.g. servers workstations laptops netbooks palm tops mobile phones smartphones multimedia phones Voice Over Internet Protocol VOIP phones user equipment terminals portable media players tablets GPS units gaming systems set top boxes access content services provided over the Internet and or content services provided on virtual private networks VPNs overlaid on e.g. tunneled through the Internet. The content and or services are typically provided by one or more end stations e.g. server end stations belonging to a service or content provider or end stations participating in a peer to peer service and may include for example public webpages e.g. free content store fronts search services private webpages e.g. username password accessed webpages providing email services and or corporate networks over VPNs. Typically subscriber end stations are coupled e.g. through customer premise equipment coupled to an access network wired or wirelessly to edge network elements which are coupled e.g. through one or more core network elements to other edge network elements which are coupled to other end stations e.g. server end stations .

Different embodiments of the invention may be implemented using different combinations of software firmware and or hardware. Thus the techniques shown in the figures can be implemented using code and data stored and executed on one or more electronic devices e.g. an end station a network element . Such electronic devices store and communicate internally and or with other electronic devices over a network code and data using computer readable media such as non transitory computer readable storage media e.g. magnetic disks optical disks random access memory read only memory flash memory devices phase change memory and transitory computer readable transmission media e.g. electrical optical acoustical or other form of propagated signals such as carrier waves infrared signals digital signals . In addition such electronic devices typically include a set of one or more processors coupled to one or more other components such as one or more storage devices non transitory machine readable storage media user input output devices e.g. a keyboard a touchscreen and or a display and network connections. The coupling of the set of processors and other components is typically through one or more busses and bridges also termed as bus controllers . Thus the storage device of a given electronic device typically stores code and or data for execution on the set of one or more processors of that electronic device.

SDN inline services and forwarding architecture uses two different types of switches. The Perimeter Open Flow OF Switches are placed on the perimeter of the service delivery network. These switches classify the incoming traffic and steer this traffic towards the next service in the chain. OF switches are the switches to which services or gateway nodes are connected. The Inner Switches forward the traffic using efficient Layer 2 L2 switching. Inner switches are only connected to other switches. Inner switches may or may not be OF switches.

Some embodiments of the present disclosure will be discussed as using the OpenFlow protocol but could be implemented with other types of Software Defined Networking SDN . OpenFlow is a communications protocol that gives access to the forwarding plane of a network switch or router over the network. OpenFlow 1.1 supports multiple tables and a metadata field to exchange information between tables. The present disclosure takes advantage of these features to reduce the number of rules by avoiding cross products that occur when flattening multi step classifications.

In a service network an operator is able to define service policies that specify traffic classes and the chain of services that each class must traverse. These policies are translated by the controller into rules that are programmed on the switches in the service network. These rules steer the network traffic through the ordered chain of services as specified by the policies.

Embodiments of the present invention provide flexibility as they support the integration of existing and third party services with no modifications. Service instances can be located and chained in an arbitrary fashion by the operator and each service instance can be part of multiple service chains. The ability to steer traffic at the granularity of subscribers and traffic types is also provided.

The approach as discussed herein provides scalability in three distinct manners. First it reduces the number of rules required to be stored in a switch by avoiding rule cross product and instead using multiple tables combined with metadata to communicate information between tables. Second the load is distributed across a network of switches instead of using a single centralized router or load balancer while still maintaining central control. Third expensive forwarding operations such as classification and header rewriting are pushed to the perimeter of the service network which can be beneficial in many ways. These operations need to be performed only once between services regardless of the number of switch hops between them. Additionally the need for aggregated throughput is often less at the perimeter of the network where the traffic has been distributed onto a plurality of switches. The present invention combined with the use of virtual appliances running on commodity servers enables pushing all expensive operations onto the software switch running on the virtual machine monitor.

A forwarding plane can be designed that uses multiple tables to reduce the total number of rules needed to support a given set of service policies.

An encoding of the service path in a metadata field can be designed that supports a large number of service chains and supports multiple instances per service. The encoding can be flexible and allow each service to be scaled independently.

A network organization can be provided so that expensive operations such as classification and header rewriting only need to be done once between services regardless of the number of switch hops between them.

The traffic steering mechanism as described herein makes the following assumptions about the configuration of the network and the type of traffic that traverses it. 1 Every service is connected to a switch using two ports. Similar to routers and bridges inline services are by definition traversed by traffic so this is a natural requirement. The services need to have a clear notion of upstream and downstream traffic and require the use of two ports. 2 The Service Network is bounded by a single gateway on each end. A single router connects the access network to the Service Network and a single router connects the Service Network to the Internet. 3 All services are addressable at the Ethernet layer. Some services may behave like bridges and may violate this assumption. 4 All traffic going through the Service Network is subscriber traffic. 5 Terminating services such as Internet Protocol Security IPSec gateways and Content Delivery Network CDN servers which are communication endpoints are located on a separate subnet connected to one of the gateway nodes.

Perimeter switches can have two types of input output ports node ports and transit ports. Services and routers are connected to node ports. Transit ports connect to other perimeter switches or to inner switches. In the exemplary service network each perimeter switch has at least one upstream facing node port at least one downstream facing node port and at least one transit port. Each service node S S S and S is connected to a perimeter switch. Perimeter switches are connected via inner switch .

Inner switches such as include transit ports and simply forward traffic based on their destination Media Access Control MAC address. These switches could therefore be implemented with plain Ethernet switches. Optionally there can be advantages to using OpenFlow switches in the inner service network to enable features such as multi path support.

Incoming traffic either coming in from a gateway node such as routers R and R or coming back from a service always enters the service network via a perimeter switch and through a node port. Packets arriving through node ports are processed and steered towards the next node which can be a service or a gateway in their assigned service paths. Packets arriving on transit ports are simply forwarded using their destination MAC address.

Router can connect the service network to user equipment and . Router can connect the service network to an internal network and or the Internet .

Traffic steering is a two step process. The first step classifies incoming packets and assigns them a service path based on predefined policies e.g. subscriber application and ordering policies. The second step forwards packets to a next service based on its current position along its assigned service path. This two step traffic steering process only needs to be performed once between any two nodes service or router regardless of the number of switches between them.

The traffic steering process described herein supports three types of service policies subscriber based policies application based policies and flow based policies. These policies can be specified by the operator and pushed to the relevant switches by a centralized controller e.g. controller .

Subscriber based policies are policies that are defined on a per subscriber basis. These policies specify the IP address of the subscriber and the set of services that each particular subscriber s traffic should traverse.

An application represents an end user Internet application such as Youtube a type of traffic such as Hypertext Transfer Protocol HTTP or a combination of both. These types of policies are defined either in terms of an IP address block and or a User Datagram Protocol UDP Transmission Control Protocol TCP port. They are specified on a per application basis and apply to all subscribers. Application based policies refine subscriber based policies by adding or removing services from the set of services specified in the subscriber based policies.

Flow based policies are policies specific to a single flow or IP 5 tuple i.e. source IP address destination IP address protocol source port destination port . They are used to dynamically override subscriber and application policies for specific flows. The forwarding rules derived from these policies can be pushed dynamically by the controller even mid flow effectively re steering a flow towards a different set of services.

Additionally service ordering policies can be supported. Service ordering policies are different than the three types of service policies described above. They do not specify a mapping between traffic and services but instead specify the relative ordering between services for each traffic direction upstream and downstream . The controller can transform these relative orderings into a global ordering and can use this ordering to convert the sets of services specified in the service policies into ordered service chains.

The datapath that implements the steering mechanism of embodiments of the present invention involves a number of table lookups. Additionally information about services for example identity topological location and performance type can be provided to a steering module e.g. using a northbound application programming interface API of the steering module.

End user services orchestration in cloud computing is built on a single hardware base e.g. a server blade. Network services tend to be less well characterized by clean orchestration solutions because they consist of a mix of virtual appliances implementations on optimized server hardware and specialized hardware dedicated to a particular service. The present disclosure provides orchestration of network services taking into account a heterogeneous hardware and software base.

The steering module uses OpenFlow to steer packets between services. This removes the need for specialized code in the service implementations. The steering module includes a northbound API that provides the identity topological location and performance type of services. Previously the steering module implementations assumed that services were at fixed locations and read the locations from a file. The network services orchestration module itself supports a northbound API that allows the network operator to specify that services should be started or stopped so the operator can control them. Within the network services orchestration module a service instance management submodule is responsible for managing the service based on the operator s instructions provided by the API given the existing hardware bare metal software and virtual appliance base. The service instance management submodule uses libvirt or the hypervisor control API to manage virtual appliances and specialized hardware APIs for the hardware and bare metal server instances. An autoscaling and power management submodule handles scaling up services that are experiencing increasing load and scaling down services where the load is decreasing. Since the present disclosure provides for management of both virtual and physical instances a scale up for an idled hardware instance can take place by initiating power up of the hardware instance utilizing a virtual appliance for the first few flows while the hardware instance is booting up then moving traffic to the hardware instance when it power up is complete.

Steering module includes a steering northbound API and an instance location northbound API . The steering northbound API allows subscriber based and policy based rules to be pushed down to the steering module. The instance location northbound API allows changes in deployed network services instance availability to be pushed down to the steering module by the network services orchestration module . The instance location northbound API provides identity topological location and performance type service data to the steering module . A flow steering southbound API is coupled to the steering module and pushes the rules and deployed network services instance availability to one or more switches.

Network services orchestration module includes an instance location southbound API a service management northbound API an instance management submodule and an autoscaling and power management submodule . The network service orchestration module implements network service startup and shutdown and handles autoscaling of services.

The instance location southbound API pushes changes in deployed network services instance availability down to the steering module . The instance location southbound API provides identity topological location and performance type service data to the steering module .

A service management northbound API specifies service data for a network operator. The service management northbound API provides the network operator with a convenient API for specifying service instance pools service instances and their performance types. The service management API allows the operator to specify services for management. The API has the following calls manage service unmanage service modify pool scalability parameters and pool maximum reached.

An instance management submodule in response to operator requests incoming from the service management northbound API manages VMs and specialized APIs. The instance management submodule utilizes libvirt or the hypervisor control API to manage VMs implementing virtual appliances and specialized APIs for managing hardware based service instances and instances implemented on bare metal servers.

The Instance Management API communicates the availability of service instances between the Network Services Orchestration module and the Flow Steering Module. The API has three different calls add delete and withdraw.

The withdraw API call withdraws instance name from the instances eligible for further flow steering but does not remove existing flows. The instance is currently at maximum capacity.

The performance class parameter in the add call can be defined in a number of ways depending on the available hardware and data center support. Examples of some possible performance classes are 

An autoscaling and power management submodule monitors statistics and affects service instances. Affecting service instances includes scheduling new instances and shutting down instances having no load. This module monitors statistics from the OpenFlow switches and from the hypervisors running virtual service appliances.

A virtual instance management module is responsible for starting and stopping virtual machines VMs on the hypervisors.

A hardware bare metal server instance management module is responsible for starting and stopping hardware instances and instances implemented directly on bare metal servers. The southbound APIs are hardware specific.

The OpenFlow flow steering module and statistics module handle the OpenFlow protocol to the switches both flow steering and statistics.

At block statistics are monitored and service instances are affected using an autoscaling and power management submodule. In one embodiment statistics are monitored from hypervisors running virtual service appliances and switches. In one embodiment affecting service instances includes scheduling new instances and shutting down instances having no load.

At block changes in deployed network services instance availability are pushed to a steering module using an instance location southbound API. The instance location southbound API pushes the changes to an instance location northbound API of the steering module.

In one embodiment a service instance is added to a pool of instances using the instance management submodule and load monitoring is set up using the autoscaling and power management submodule. This embodiment is explained in further detail in .

In one embodiment returned statistics are compared to established upper and lower limits on a service pool using the autoscaling and power management submodule. This embodiment is explained in further detail in .

In one embodiment when a service instance is marked as overloaded the instance management submodule ends an instance that is a hardware or bare metal instance. When the instance is a virtual appliance the instance management submodule starts a new virtual appliance or increases a number of virtual machines. This embodiment is explained in further detail in .

In one embodiment the instance management module deletes a service instance when a number of flows is below a lower limit. This embodiment is explained in further detail in .

Virtual machines VMs are started and stopped on the hypervisors running on the servers using the virtual instance management module.

Hardware instances and instances implemented directly on bare metal servers are started and stopped using a hardware bare metal server instance management module. The southbound APIs are hardware specific.

OpenFlow protocol to the switches both flow steering and statistics is handled using an OpenFlow flow steering module and an OpenFlow statistics module respectively.

The Autoscaling and Power Management Submodule sets up load monitoring at block using either OpenFlow statistics if the instance is a hardware bare metal server appliance or through collecting hypervisor statistics. If the instance is a hardware bare metal server appliance autopolling is set up for Openflow switch statistics at block . Otherwise autopolling is set up for hypervisor statistics at block . The autopolling functions periodically collect statistics from the instance and determine whether it is overloaded.

The computer system includes a bus es that is coupled with a processing system a power supply volatile memory e.g. double data rate random access memory DDR RAM single data rate SDR RAM nonvolatile memory e.g. hard drive flash memory Phase Change Memory PCM . The processing system may be further coupled to a processing system cache . The processing system may retrieve instruction s from the volatile memory and or the nonvolatile memory and execute the instruction to perform operations described above. The bus es couples the above components together and further couples a display controller one or more input output devices e.g. a network interface card a cursor control e.g. a mouse trackball touchscreen touchpad etc. a keyboard etc. . In one embodiment the display controller is further coupled to a display device .

As described herein instructions may refer to specific configurations of hardware such as application specific integrated circuits ASICs configured to perform certain operations or having a predetermined functionality or software instructions stored in memory embodied in a non transitory computer readable medium. Thus the techniques shown in the figures can be implemented using code and data stored and executed on one or more electronic devices e.g. an end station a network element . Such electronic devices store and communicate internally and or with other electronic devices over a network code and data using computer readable media such as non transitory computer readable storage media e.g. magnetic disks optical disks random access memory read only memory flash memory devices phase change memory and transitory computer readable communication media e.g. electrical optical acoustical or other form of propagated signals such as carrier waves infrared signals digital signals . In addition such electronic devices typically include a set of one or more processors coupled to one or more other components such as one or more storage devices non transitory machine readable storage media user input output devices e.g. a keyboard a touchscreen and or a display and network connections. The coupling of the set of processors and other components is typically through one or more buses and bridges also termed as bus controllers . Thus the storage device of a given electronic device typically stores code and or data for execution on the set of one or more processors of that electronic device. Of course one or more parts of an embodiment of the invention may be implemented using different combinations of software firmware and or hardware.

While the flow diagrams in the figures show a particular order of operations performed by certain embodiments of the invention it should be understood that such order is exemplary e.g. alternative embodiments may perform the operations in a different order combine certain operations overlap certain operations etc. .

While the invention has been described in terms of several embodiments those skilled in the art will recognize that the invention is not limited to the embodiments described. The description is thus to be regarded as illustrative instead of limiting.

