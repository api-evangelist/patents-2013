---

title: Methods and systems for identifying changed data in an expandable storage volume
abstract: An embodiment of the invention provides an apparatus and method for presenting differences in a file system of a clustered storage system. In an embodiment, the clustered storage system receives a request from a host, where the request is for a listing of changes in the file system within a time interval. A comparison unit in each storage node of the clustered storage system determines each metadata container associated with the file system which has changed within the time interval. The application programming interface buffers at least one identifier that corresponds to a metadata container associated with the file system which has changed within the time interval. The application programming interface packages and transmits the at least one identifier to the host. The at least one identifier is readable on the host.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09015123&OS=09015123&RS=09015123
owner: NetApp, Inc.
number: 09015123
owner_city: Sunnyvale
owner_country: US
publication_date: 20130116
---
At least one embodiment of the present invention pertains to data storage systems and more particularly to methods and systems for presenting differences in a file system.

In storage technology a storage server is a class of computer that provides services relating to the organization and storage of information or data on storage devices such as disk drives disks . In other words a storage server is adapted to store and retrieve data on behalf of one or more client processing systems clients or hosts in response to external requests received from the hosts. A storage server can provide clients with file level access to data stored in the storage devices. A storage server can also provide clients with block level access to stored data or with both file level access and block level access. For convenience a storage server will be described herein for the most part in terms of storage devices that provide file level access though the description herein will also have application to storage devices that provide block level access as will be apparent to those of ordinary skill in the art in light of the description that follows. Examples of such storage servers include a file server or another type of computing device that provides storage services using a file system to respond to file oriented data access requests filer . A storage server includes a storage operating system that implements a file system to logically organize the information as a hierarchical structure of directories and files on the disks. Each file stored on a disk or other type of storage device may be implemented as a set of data structures e.g. disk blocks which are configured to store information.

A disk based storage for a storage server typically has one or more storage volumes which are a collection of physical storage disks and which define an overall logical arrangement of storage space. In other words a storage volume is a logical container that includes a collection of disks. Therefore the collection of disks is grouped assimilated into the storage volume. Each storage volume is generally associated with its own file system.

A software application can seek access to a file system in order to determine the changes that have occurred for files or directories in the file system. In one instance such determined changes of the file system could be utilized by the software application to create a backup of a storage server the file system is associated with. However current methods for discovering the changes in a file system would require the software application to traverse through each directory and through each branch from a directory in a file system a process known as a tree walk since the file system is typically tree structured and to examine each file in each directory to determine which files have been modified added or accessed. The software application which is external to a storage server that stores a file system is required to perform multiple reads to the file system by use of the tree walk process across the directories in the file system store the results of the multiple reads to the file system and then determine the changes in the file system based on the results of these multiple reads. Such a process creates a huge latency. The tree walk process is further inefficient because of additional latency when data read from the directories have been stored on disk in a non sequential and random placement. Such non sequential and random placement of data results in a longer time to access the data.

Accordingly the current technology is limited in its capabilities and suffers from at least the above constraints and deficiencies.

The technique introduced here provides an apparatus and method for identifying and or presenting differences in a file system of a clustered storage system. The file system maintains a hierarchical representation of the logical containers of data e.g. files and directories stored in the clustered storage system. A clustered storage system includes multiple networked storage nodes to store the files and directories. In one embodiment of the clustered storage system one of the storage nodes is used as a namespace storage node interconnected while the remaining storage nodes are used as data storage nodes. In some instances the namespace storage node is used to maintain the file system of the clustered storage system. The data storage nodes are used to store the data associated with each of the files and directories present in the file system of the clustered storage system. This separation i.e. the separation of the file system of the clustered storage system from the data associated with the files and directories of the file system results in any content modification to the data of the files and directories stored in the data storage nodes to happen independent of the file system of the clustered storage system. Further in some embodiments each data storage node maintains a local file system independent of the file system of the clustered storage system to organize and maintain the data of the files and directories present in the file system of the clustered storage system.

In an illustrative embodiment of the technique introduced here an application programming interface associated with for example the namespace storage node in the clustered storage system receives a request e.g. via a network from a host to list the changes in the file system of the clustered storage system within a time interval. The application programming interface forwards the request to a comparison unit in each of the storage nodes. The comparison units determine the changes to the file system within the time interval. Here the comparison unit in the namespace storage node identifies each metadata container e.g. Inode that has changed i.e. added deleted renamed or moved in the file system of the clustered storage system within the time interval.

Similarly comparison units in each of the data storage nodes identify each metadata container e.g. Inode that has been modified in the time interval as indicated by their respective local file systems. Given that the modified metadata containers in the data storage nodes correspond to modified data of files or directories in the file system of the clustered storage system the application programming interface utilizes the file system of the clustered storage system i.e. the file system stored in the namespace storage node to identify and buffer at least one identifier for each modified metadata container. The application programming interface packages and transmits the identifiers to the host where the identifiers serve as indicators of the changes to the file system of the clustered storage system.

This brief summary has been provided so that the nature of this disclosure may be understood quickly. A more complete understanding of the disclosure can be obtained by reference to the following detailed description of the various embodiments thereof in connection with the attached drawings.

As a preliminary note the terms component module system and the like as used in this disclosure are intended to refer to a computer related entity either software executing general purpose processor hardware firmware and a combination thereof. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer.

By way of illustration both an application running on a server and the server can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers. Also these components can execute from various computer readable media having various data structures stored thereon. The components may communicate via local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems via the signal .

Computer executable components can be stored for example on non transitory computer readable medium including but not limited to an application specific integrated circuit ASIC compact disc CD digital video disk DVD read only memory ROM floppy disk hard disk electrically erasable programmable read only memory EEPROM memory stick flash memory device or any other non volatile memory device or any other storage device in accordance with the claimed subject matter.

Referring to a network data storage environment is shown. The storage environment includes a plurality of client systems . .N a storage server system and a network connecting the client systems . .N and the storage server system . As shown in the storage server system includes at least one storage server a switching fabric and a number of mass storage devices within a mass storage subsystem such as conventional magnetic disks optical disks such as CD ROM or DVD based storage magneto optical MO storage flash memory storage device or any other type of non volatile storage devices suitable for storing structured or unstructured data. The examples disclosed herein may reference a storage device as a disk but the adaptive embodiments disclosed herein are not limited to disks or any particular type of storage media device in the mass storage subsystem .

The storage server or servers may be for example one of the storage server products available from NetApp Inc. the assignee of the present application. The client systems . .N may access the storage server via network which can be a packet switched network for example a local area network LAN wide area network WAN or any other type of network.

The storage server maybe connected to the storage devices via the switching fabric which can be a fiber distributed data interface FDDI network for example. It is noted that within the network data storage environment any other suitable numbers of storage servers and or mass storage devices and or any other suitable network technologies may be employed. While implies in some embodiments a fully connected switching fabric where storage servers can see all storage devices it is understood that such a connected topology is not required. In some embodiments the storage devices can be directly connected to the storage servers such that no two storage servers see a given storage device.

The storage server can make some or all of the storage space on the storage devices available to the client systems . .N in a conventional manner. For example each storage device can be implemented as an individual disk multiple disks e.g. a RAID group or any other suitable mass storage device s . The storage server can communicate with the client systems . .N according to well known protocols such as the Network File System NFS protocol or the Common Internet File System CIFS protocol to make data stored at storage devices available to users and or application programs.

The storage server can present or export data stored at storage device as volumes also referred to herein as storage volumes to each of the client systems . .N. A volume is an abstraction of physical storage combining one or more physical mass storage devices e.g. disks or parts thereof into a single logical storage object the volume and which is managed as a single administrative unit such as a single file system. A file system is a structured e.g. hierarchical set of stored logical containers of data e.g. volumes logical unit numbers LUNs directories files . Note that a file system does not have to include or be based on files per se as its units of data storage.

Various functions and configuration settings of the storage server and the mass storage subsystem can be controlled from a management console coupled to the network .

The storage environment includes a plurality of client systems . .M a clustered storage system and a network connecting the client systems and the clustered storage server system . As shown in the clustered storage server system includes a plurality of server nodes may also be referred to as nodes . .N a cluster switching fabric and a plurality of mass storage devices . .N similar to storage devices . Note that more than one mass storage device can be associated with each node .

Each of the nodes is configured to include several modules including an N module a D module and an M host each of which can be implemented by using a separate processor executable module and an instance of a replicated database RDB . Specifically node . includes an N module . a D module . and an M host . node .N includes an N module .N a D module .N and an M host .N and so forth. The N modules . .N include functionality that enables nodes . .N respectively to connect to one or more of the client systems over the network while the D modules . .N provide access to the data stored at storage devices . .N respectively. The M hosts provide management functions for the clustered storage server system including a system for replicating the Infinite Volume described below in detail. Accordingly each of the server nodes in the clustered storage server arrangement provides the functionality of a storage server.

In one embodiment RDB is a database that is replicated throughout the cluster i.e. each node includes an instance of the RDB . The various instances of the RDB are updated regularly to bring them into synchronization with each other. The RDB provides cluster wide storage for information used by nodes including a volume location database VLDB not shown . The VLDB is a database that indicates the location within the cluster of each volume in the cluster i.e. the owning D module for each volume and is used by the N modules to identify the appropriate D module for any given volume to which access is requested.

A switched virtualization layer including a plurality of virtual interfaces VIFs is provided between the respective N modules . .N and the client systems . .M allowing the storage . .N associated with the nodes . .N to be presented to the client systems as a single shared storage pool.

The clustered storage system can be organized into any suitable number of virtual servers also referred to as vservers in which each vserver represents a single storage system namespace with separate network access. Each vserver has a user domain and a security domain that are separate from the user and security domains of other vservers. Moreover each vserver is associated with one or more VIFs and can span one or more physical nodes each of which can hold one or more VIFs and storage associated with one or more vservers. Client systems can access the data on a vserver from any node of the clustered system but only through the VIFs associated with that vserver. It is noteworthy that the embodiments described herein are not limited to the use of vservers.

The nodes are interconnected by a cluster switching fabric which can be embodied as a Gigabit Ethernet switch for example. The N modules and D modules cooperate to provide highly scalable distributed storage system architecture of a clustered computing environment implementing exemplary embodiments of the technique introduced here. Note that while there is shown an equal number of N modules and D modules in there may be differing numbers of N modules and or D modules in accordance with various embodiments of the technique described here. For example there need not be a one to one correspondence between the N modules and D modules. As such the description of a node comprising one N module and one D module should be understood to be illustrative only.

The storage controller further includes a memory a network adapter a cluster access adapter and a storage adapter all interconnected by an interconnect . Interconnect may include for example a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire or any other system.

The cluster access adapter includes a plurality of ports adapted to couple the node to other nodes of the cluster. In the illustrated embodiment Ethernet is used as the clustering protocol and interconnect media although other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternative embodiments where the N modules and D modules are implemented on separate storage systems or computers the cluster access adapter is utilized by the N module and or D module for communicating with other N modules and or D modules of the cluster.

The storage controller can be embodied as a single or multi processor storage system executing a storage operating system that preferably implements a high level module such as a storage manager to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks at the storage devices. Illustratively one processor can execute the functions of the N module on the node while another processor executes the functions of the D module .

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing processor executable code and data structures associated with the present disclosure. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system portions of which is typically resident in memory and executed by the processors s functionally organizes the storage controller by among other things configuring the processor s to invoke storage operations in support of the storage service provided by the node . It will be apparent to those skilled in the art that other processing and memory implementations including various computer readable storage media may be used for storing and executing program instructions pertaining to the technique introduced here.

The network adapter includes a plurality of ports to couple the storage controller to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus can include the mechanical electrical and signaling circuitry needed to connect the storage controller to the network . Illustratively the network can be embodied as an Ethernet network or a Fibre Channel FC network. Each client can communicate with the node over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system to access information requested by the clients . The information may be stored on any type of attached array of writable storage media such as magnetic disk or tape optical disk e.g. CD ROM or DVD flash memory solid state disk SSD electronic random access memory RAM micro electro mechanical and or any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is stored on storage devices . The storage adapter includes a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance Fibre Channel FC link topology.

Storage of information on storage devices can be implemented as one or more storage volumes that include a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number VBN space on the volume s . The storage devices can be organized as a RAID group. One or more RAID groups together form an aggregate. An aggregate can contain one or more volumes file systems.

The storage operating system facilitates clients access to data stored on the storage devices . In certain embodiments the storage operating system implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by storage devices . In certain embodiments a storage manager logically organizes the information as a hierarchical structure of named directories and files on the storage devices . Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the storage manager to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers LUNs .

In the illustrative embodiment the storage operating system is a version of the Data ONTAP operating system available from NetApp Inc. and the storage manager implements the Write Anywhere File Layout WAFL file system. Additional details of an example storage operating system are disclosed in for example commonly assigned U.S. patent application Ser. No. 10 836 090. However other storage operating systems are capable of being enhanced or created for use in accordance with the principles described herein.

In the illustrated embodiment the storage operating system includes multiple functional layers organized to form an integrated network protocol stack or more generally a multi protocol engine that provides data paths for clients to access information stored on the node using block and file access protocols. The multiprotocol engine in combination with underlying processing hardware also forms the N module . The multi protocol engine includes a network access layer which includes one or more network drivers that implement one or more lower level protocols to enable the processing system to communicate over the network such as Ethernet Internet Protocol IP Transport Control Protocol Internet Protocol TCP IP Fibre Channel Protocol FCP and or User Datagram Protocol Internet Protocol UDP IP . The multiprotocol engine also includes a protocol layer which implements various higher level network protocols such as NFS CIFS Hypertext Transfer Protocol HTTP Internet small computer system interface iSCSI etc. Further the multiprotocol engine includes a cluster fabric CF interface module A which implements intra cluster communication with D modules and with other N modules.

In addition the storage operating system includes a set of layers organized to form a backend server that provides data paths for accessing information stored on the storage devices of the node . The backend server in combination with underlying processing hardware also forms the D module . To that end the backend server includes a storage manager module that manages any number of storage volumes a RAID system module and a storage driver system module .

The storage manager primarily manages a file system or multiple file systems and serves client initiated read and write requests. The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with a RAID redundancy protocol such as RAID 4 RAID 5 or RAID DP while the storage driver system implements a disk access protocol such as SCSI protocol or FCP.

The backend server also includes a CF interface module B to implement intra cluster communication with N modules and or other D modules. The CF interface modules A and B can cooperate to provide a single file system image across the D modules in the cluster. Thus any network port of an N module that receives a client request can access any data container within the single file system image located on any D module of the cluster.

The CF interface modules A B implement the CF protocol to communicate file system commands among the modules of cluster over the cluster switching fabric . Such communication can be effected by a D module exposing a CF application programming interface API to which an N module or another D module issues calls. To that end a CF interface module can be organized as a CF encoder decoder. The CF encoder of e.g. CF interface A on N module can encapsulate a CF message as i a local procedure call LPC when communicating a file system command to a D module residing on the same node or ii a remote procedure call RPC when communicating the command to a D module residing on a remote node of the cluster. In either case the CF decoder of CF interface B on D module de encapsulates the CF message and processes the file system command.

In operation of a node a request from a client is forwarded as a packet over the network and onto the node where it is received at the network adapter . A network driver of layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the storage manager . At that point the storage manager generates operations to load retrieve the requested data from storage device if it is not resident in memory . If the information is not in memory the storage manager indexes into a metadata file to access an appropriate entry and retrieve a logical virtual block number VBN . The storage manager then passes a message structure including the logical VBN to the RAID system the logical VBN is mapped to a disk identifier and disk block number DBN and sent to an appropriate driver e.g. SCSI of the storage driver system . The storage driver accesses the DBN from the specified storage device and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client over the network .

The data request response path through the storage operating system as described above can be implemented in general purpose programmable hardware executing the storage operating system as software or firmware. Alternatively it can be implemented at least partially in specially designed hardware. That is in an alternate embodiment of the technique introduced here some or all of the storage operating system is implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC for example.

The N module and D module each can be implemented as processing hardware configured by separately scheduled processes of storage operating system however in an alternate embodiment the modules may be implemented as processing hardware configured by code within a single operating system process. Communication between an N module and a D module is thus illustratively effected through the use of message passing between the modules although in the case of remote communication between an N module and D module of different nodes such message passing occurs over the cluster switching fabric . A known message passing mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF API.

The techniques introduced here generally relate to a content repository implemented in a network storage server system such as described above. illustrates the overall architecture of the content repository according to one embodiment. The content repository includes a distributed object store an object location subsystem OLS a presentation layer and a management subsystem . Normally there will be a single instance of each of these components in the overall content repository and each of these components can be implemented in any one server node or distributed across two or more server nodes . The functional elements of each of these units i.e. the OLS presentation layer and management subsystem can be implemented by specially designed circuitry or by programmable circuitry programmed with software and or firmware or a combination thereof. The data storage elements of these units can be implemented using any known or convenient form or forms of data storage device.

The distributed object store provides the actual data storage for the data objects in the server system and includes multiple data constituent volumes may interchangeably be referred to as distinct single node object stores . A single node object store or data constituent volume is an object store that is implemented entirely within one node. Each data constituent volume is a logical non physical container of data such as a data constituent volume or a logical unit LUN . Some or all of the data constituent volumes that make up the distributed object store can be implemented in separate server nodes . Alternatively all of the data constituent volumes that make up the distributed object store can be implemented in the same server node. Any given server node can access multiple data constituent volumes and can include multiple data constituent volumes .

The distributed object store provides location independent addressing of data objects i.e. data objects can be moved among data constituent volumes without changing the data objects addressing with the ability to span the object address space across other similar systems spread over geographic distances. Note that the distributed object store has no namespace the namespace for the server system is provided by the presentation layer .

The term namespace as used herein refers to a virtual hierarchical collection of unique volume names or identifiers and directory paths to the volumes in which each volume represents a virtualized container storing a portion of the namespace descending from a single root directory. For example each volume associated with a namespace can be configured to store one or more data containers scripts word processing documents executable programs and others.

The presentation layer provides access to the distributed object store . It is generated by at least one presentation module i.e. it may be generated collectively by multiple presentation modules one in each multiple server nodes . The presentation module can be in the form of specially designed circuitry or programmable circuitry programmed with software and or firmware or a combination thereof.

The presentation layer receives client requests translates them into an internal protocol and sends them to the appropriate D module . The presentation layer provides two or more independent interfaces for accessing stored data e.g. a conventional NAS interface and a Web Service interface . The NAS interface allows access to the object store via one or more conventional NAS protocols such as NFS and or CIFS. Thus the NAS interface provides a file system like interface to the content repository.

The Web Service interface allows access to data stored in the object store via either named object access or raw object access also called flat object access . Named object access uses a namespace e.g. a file system like directory tree interface for accessing data objects as does NAS access whereas raw object access uses system generated global object IDs to access data objects as described further below. The Web Service interface allows access to the object store via Web Service as defined by the W3C using for example a protocol such as Simple Object Access Protocol SOAP or a RESTful REpresentational State Transfer ful protocol over HTTP.

The presentation layer further provides at least one namespace may also be referred to as namespace volume for accessing data via the NAS interface or the Web Service interface. In one embodiment this includes a Portable Operating System Interface POSIX namespace. The NAS interface allows access to data stored in the object store via the namespace s . The Web Service interface allows access to data stored in the object store via either the namespace s by using named object access or without using the namespace s by using raw object access . Thus the Web Service interface allows either named object access or raw object access and while named object access is accomplished using a namespace raw object access is not. Access by the presentation layer to the object store is via either a fast path or a slow path as discussed further below.

The function of the OLS is to store and provide valid location IDs and other information such as policy IDs of data objects based on their global object IDs these parameters are discussed further below . This is done for example when a client requests access to a data object by using only the global object ID instead of a complete object handle including the location ID or when the location ID within an object handle is no longer valid e.g. because the target data object has been moved . Note that the system thereby provides two distinct paths for accessing stored data namely the fast path and the slow path . The fast path provides data access when a valid location ID is provided by a client e.g. within an object handle . The slow path makes use of the OLS and is used in all other instances of data access. The fast path is so named because a target data object can be located directly from its valid location ID whereas the slow path is so named because it requires a number of additional steps relative to the fast path to determine the location of the target data object.

The management subsystem includes a content management component and an infrastructure management component . The infrastructure management component includes logic to allow an administrative user to manage the storage infrastructure e.g. configuration of nodes storage devices volumes LUNs etc. . The content management component is a policy based data management subsystem for managing the lifecycle of data objects and optionally the metadata stored in the content repository based on user specified policies. It can execute actions to enforce defined policies in response to system defined trigger events and or user defined trigger events e.g. attempted creation deletion access or migration of an object .

The specified policies may relate to for example system performance data protection and data security. Performance related policies may relate to for example which logical container a given data object should be placed in migrated from or to when the data object should be migrated or deleted etc. Data protection policies may relate to for example data backup and or data deletion. Data security policies may relate to for example when and how data should be encrypted who has access to particular data etc. The specified policies can also include polices for power management storage efficiency data retention and deletion criteria. The policies can be specified in any known convenient or desirable format and method. A policy in this context is not necessarily an explicit specification by a user of where to store what data when to move data etc. Rather a policy can be a set of specific rules regarding where to store what when to migrate data etc. derived by the system from the end user s SLOs i.e. a more general specification of the end user s expected performance data protection security etc. For example an administrative user might simply specify a range of performance that can be tolerated with respect to a particular parameter and in response the management subsystem would identify the appropriate data objects that need to be migrated where they should get migrated to and how quickly they need to be migrated.

In one embodiment the distributed object store is implemented by providing at least one data constituent volume in each of at least two D modules in the system any given D module can include zero or more single node object stores . Also implemented in each of at least two D modules in the system are an OLS store that contains mapping data structures used by the OLS including valid location IDs and policy IDs and a policy store e.g. a database that contains user specified policies relating to data objects note that at least some policies or policy information may also be cached in the N module to improve performance .

The presentation layer is implemented at least partially within each N module . In one embodiment the OLS is implemented partially by the N module and partially by the corresponding M host as illustrated in . More specifically in one embodiment the functions of the OLS are implemented by a special daemon in the M host and by the presentation layer in the N module .

In one embodiment the management subsystem is implemented at least partially within each M host . Nonetheless in some embodiments any of these subsystems may also be implemented at least partially within other modules. For example at least a portion of the content management component of the management subsystem can be implemented within one or more N modules to allow for example caching of policies in such N modules and or execution application of policies by such N module s . In that case the processing logic and state information for executing applying policies may be contained in one or more N modules while processing logic and state information for managing policies is stored in one or more M hosts . Administrative users can specify policies for use by the management subsystem via a user interface provided by the M host to access the management subsystem .

As noted above the distributed object store enables both path based access to data objects as well as direct access to data objects. For purposes of direct access the distributed object store uses a multilevel object handle as illustrated in . When a client creates a data object it receives an object handle as the response to creating the object. This is similar to a file handle that is returned when a file is created in a traditional storage system. The first level of the object handle is a system generated globally unique number called a global object ID that is permanently attached to the created data object. The second level of the object handle is a hint which includes the location ID of the data object and in the illustrated embodiment the policy ID of the data object. Clients can store this object handle containing the global object ID location ID and policy ID .

When a client attempts to read or write the data object using the direct access approach the client includes the object handle of the object in its read or write request to the server system . The server system first attempts to use the location ID within the object handle which is intended to be a pointer to the exact location within a volume where the data object is stored. In the common case this operation succeeds and the object is read written. This sequence is the fast path for I O see .

If however an object is moved from one location to another for example from one volume to another the server system creates a new location ID for the object. In that case the old location ID becomes stale invalid . The client may not be notified that the object has been moved or that the location ID is stale and may not receive the new location ID for the object at least until the client subsequently attempts to access that data object e.g. by providing an object handle with an invalid location ID . Or the client may be notified but may not be able or configured to accept or understand the notification.

The current mapping from global object ID to location ID is stored reliably in the OLS . If during fast path I O the server system discovers that the target data object no longer exists at the location pointed to by the provided location ID this means that the object must have been either deleted or moved. Therefore at that point the server system will invoke the OLS to determine the new valid location ID for the target object. The server system then uses the new location ID to read write the target object. At the same time the server system invalidates the old location ID and returns a new object handle to the client that contains the unchanged and unique global object ID as well as the new location ID. This process enables clients to transparently adapt to objects that move from one location to another for example in response to a change in policy .

An enhancement of this technique is for a client never to have to be concerned with refreshing the object handle when the location ID changes. In this case the server system is responsible for mapping the unchanging global object id to location ID. This can be done efficiently by compactly storing the mapping from global object ID to location ID in for example cache memory of one or more N modules .

As noted above the distributed object store enables path based access to data objects as well and such path based access is explained in further detail in the following sections.

In a traditional storage system a file is represented by a path such as u foo bar file.doc . In this example u is a directory under the root directory foo is a directory under u and so on. Therefore a file is uniquely identified by a single path. However since file handles and directory handles are tied to a location in a traditional storage system an entire path name is tied to a specific location e.g. as indicated by an inode of the file making it difficult to move files around without having to rename them.

An inode is a data structure e.g. a 128 byte structure which is used to store information such as metadata about a data container. Examples of data containers as may be used herein include files directories etc. The metadata contained in an inode may include data information e.g. ownership of a file access permission for the file size of the file file type location of the file on disk etc. as is described in more detail below. The file system uses a file handle i.e. an identifier that includes an inode number to retrieve an inode from a storage disk.

Now refer to which illustrates a mechanism that allows the server system to break the tight relationship between path names and location. As illustrated in the example of path names of data objects in the server system are stored in association with a namespace e.g. a directory namespace . The directory namespace maintains a separate directory entry e.g. for each data object stored in the distributed object store . A directory entry as indicated herein refers to an entry that describes a name of any type of data object e.g. directories files other such logical containers of data etc. . Each directory entry includes for example a path name e.g. NAME 1 i.e. a logical address of the data object and a pointer e.g. REDIRECTOR POINTER 1 shown as stub 1 pointer for mapping the directory entry to the data object.

In a traditional storage system the pointer e.g. an inode number directly maps the path name to an inode associated with the data object. On the other hand in the illustrated embodiment shown in the pointer of each data object points to a stub file or a redirector file used interchangeably in this specification associated with the data object. A redirector file as indicated herein refers to a file that maintains an object locator of the data object. The object locator of the data object could either be the multilevel object handle or just the global object ID of the data object. In the illustrated embodiment the redirector file e.g. redirector file for data object 1 is also stored within the directory namespace . In addition to the object locator data the redirector file may also contain other data such as metadata about the location of the redirector file etc.

As illustrated in for example the pointer included in the directory entry of data object 1 points to a redirector file for data object 1 instead of pointing to for example the inode of data object 1 . The directory entry does not include any inode references to data object 1. The redirector file for data object 1 includes an object locator i.e. the object handle or the global object ID of data object 1. As indicated above either the object handle or the global object ID of a data object is useful for identifying the specific location e.g. a physical address of the data object within the distributed object store . Accordingly the server system can map the directory entry of each data object to the specific location of the data object within the distributed object store . By using this mapping in conjunction with the OLS i.e. by mapping the path name to the global object ID and then mapping the global object ID to the location ID the server system can mimic a traditional file system hierarchy while providing the advantage of location independence of directory entries.

By having the directory entry pointer of a data object point to a redirector file containing the object locator information instead of pointing to an actual inode of the data object the server system introduces a layer of indirection between i.e. provides a logical separation of directory entries and storage locations of the stored data object. This separation facilitates transparent migration i.e. a data object can be moved without affecting its name and moreover it enables any particular data object to be represented using multiple path names thereby facilitating navigation. In particular this allows the implementation of a hierarchical protocol such as NFS on top of an object store while at the same time allowing access via a flat object address space wherein clients directly use the global object ID to access objects and maintaining the ability to do transparent migration.

In one embodiment instead of using a redirector file for maintaining the object locator i.e. the object handle or the global object ID of a data object the server system stores the global object ID of the data object directly within the directory entry of the data object. An example of such an embodiment is depicted in . In the illustrated example the directory entry for data object 1 includes a path name and the global object ID of data object 1. In a traditional server system the directory entry would contain a path name and a reference to an inode e.g. the inode number of the data object. Instead of storing the inode reference the server system stores the global object ID of data object 1 in conjunction with the path name within the directory entry of data object 1. As explained above the server system can use the global object ID of data object 1 to identify the specific location of data object 1 within the distributed object store . In this embodiment the directory entry includes an object locator i.e. a global object ID instead of directly pointing to the inode of the data object and therefore still maintains a layer of indirection between the directory entry and the physical storage location of the data object. As indicated above the global object ID is permanently attached to the data object and remains unchanged even if the data object is relocated within the distributed object store .

In the file mbox is created and stored by the storage operating system under the pathname . usr kiran mbox provided by one of the clients . through .N in InfiniteVol . The storage operating system creates in the file system the various directories and sub directories in the pathname such as directory usr and sub directory kiran under directory usr if the directories sub directories weren t already present in the file system and associates an inode file with each of the directories. The inode file shown in contains the metadata associated with each of the directories sub directories. One of the purposes of the inode as discussed above is to store metadata about a particular directory such as a pointer to the data blocks associated with the directory the size e.g. in kilobytes of the directory the number of data blocks the link count number of references to that directory permissions that are associated with the directory creation time date of the directory and access time date to the directory.

Further a root directory of the file system maintains a mapping between the files directories under the root . of the file system and their corresponding inode files. Additionally in the file system the inode file container number 6 is associated with directory usr and inode file container number 29 is associated with directory kiran . The inode files container number 6 and container number 29 include pointers to data blocks associated with the directories usr and kiran respectively. The data blocks associated with directories such as usr and kiran contain a lookup table mapping filenames to the inode numbers of the various files directories under the directories. Here in the lookup table in data block 132 is associated with directory usr and the lookup table in data block 406 is associated with directory kiran . In addition the file mbox is created and stored under sub directory kiran in the pathname . usr kiran mbox . The inode file container number 60 is created and associated with the regular file mbox as opposed to directory type files such as usr and kiran where the inode file container number 60 stores all the meta data associated with the file mbox including the data blocks associated with the file mbox . The inode files container number 60 includes pointers to data blocks associated with the file mbox .

As discussed earlier unlike a traditional file system where the data blocks associated with regular files contain the data associated with the regular files in the InfiniteVol namespace the data blocks associated with regular files contain an object locator of the data associated with the regular files. Here the data blocks stored in block 518 associated with the mbox regular file contain a multilevel object handle returned by one of the data storage nodes . .N after storing the data associated with the mbox file within itself. Therefore any modification to the data associated with the mbox file such as addition or deletion of content from the data will be carried out in the data storage node . .N where the mbox file s data is stored in without any modification to the file system or the associated inode files stored in the namespace storage node .

For example when a client sends a request to modify the content of file mbox under pathname . usr kiran mbox in one embodiment the storage operating system could utilize the directory namespace to lookup the stub file pointer e.g. inode container number of stub file to access and retrieve the multilevel object handle that is returned by the data storage node . after storing the data associated with the mbox file within its storage node .. In one embodiment to perform the content modification requested by the client the storage operating system sends the client s content modification request along with the retrieved multilevel object handle to the data storage node . to carry out the request.

In another embodiment when a client sends a request to modify the content of file mbox under pathname . usr kiran mbox the storage operating system first retrieves the root directory lookup table of the file system and identifies the inode container number of the directory usr as inode container number 6 . The file system next retrieves the inode container number 6 and identifies the pointer 132 to the data block storing the data associated with directory usr . In this case given that usr is a directory the data block associated with pointer 132 contains a lookup table mapping filenames to the inode numbers of the various files directories under the directory usr . The storage operating system next searches for the inode container number of the sub directory kiran based on client provided pathname . usr kiran mbox in the lookup table associated with data block of the directory usr and identifies the inode container number of the subdirectory kiran as inode container number 29 . The storage operating system retrieves the inode container number 29 and identifies the pointer 406 to the data block storing the data associated with subdirectory kiran . In this case given that kiran is a directory the data block associated with pointer 406 contains a lookup table mapping filenames to the inode numbers of the various files directories under the directory kiran .

The storage operating system next searches for the inode container number of the regular file mbox based on client provided pathname . usr kiran mbox in the lookup table associated with data block of the directory kiran and identifies the inode container number of the file mbox as inode container number 60 . The storage operating system retrieves the inode container number 60 and identifies the pointer 518 to the data block storing the data associated with regular file mbox . In this case given that mbox is a regular file the data block associated with pointer 518 contains a multilevel object handle returned by the data storage node . to the data associated with the mbox file that is stored within the node ..

In one embodiment to perform the content modification requested by the client the storage operating system retrieves the multilevel object handle and sends the client s content modification request along with the retrieved multilevel object handle to the data storage node . to carry out the request. Therefore any modification to the data associated with the mbox file such as adding or deleting content from the data will be carried out in the data storage node . .N where the mbox file s data is stored in without any modification to the file system or the associated inode files stored in the namespace storage node . On the other hand when a file directory is deleted or created in the file system inode files associated with the file directory are accordingly deleted or created in the file system . discuss in detail how changes to metadata files e.g. inode files between a given time interval can be used to determine the files in the file system that have changed i.e. created deleted modified etc. within the given time interval.

In one instance the file name for the newly created file could be the inode container number of the mbox file in the namespace storage node i.e. inode container number 60 . The storage operating system creates a new file 60 in its file system under pathname . sys vol1 where in one instance the pathname is determined by the storage operating system internally creates and associates a new inode file container number 7 with the new file 60 and returns a file handle to the namespace storage node . The file handle includes a location ID that incorporates the pathname . sys vol1 60 and the number of the data storage node ..

The next time the client wants to modify the file mbox in the clustered storage server the storage operating system retrieves the file handle stored in the stub file associated with the mbox file and sends a request to the data storage node . along with the retrieved file handle and the modified data from the client .. In the data storage node . the storage operating system retrieves the location ID from the file handle where the location ID includes the pathname . sys vol1 60 of the file to be modified. The storage operating system accesses a root directory lookup table of the file system and identifies the inode container number of the directory sys as inode container number 8 .

The storage operating system next retrieves the inode container number 8 from the file system and identifies the pointer 142 to the data block storing the data associated with directory sys . In this case given that sys is a directory the data block associated with pointer 142 contains a lookup table mapping filenames to the inode numbers of the various files directories under the directory sys . The storage operating system next searches the file system for the inode container number of the sub directory vol1 based on received pathname . sys vol1 60 in the lookup table associated with data block of the directory sys and identifies the inode container number of the subdirectory vol1 as inode container number 25 . The storage operating system retrieves the inode container number 25 and identifies the pointer 306 to the data block storing the data associated with subdirectory vol1 . In this case given that vol1 is a directory the data block associated with pointer 306 contains a lookup table mapping filenames to the inode numbers of the various files directories under the directory vol1 .

The storage operating system next searches for the inode container number of the regular file 60 based on client . provided pathname . sys vol1 60 in the lookup table associated with data block of the directory vol1 and identifies the inode container number of the file 60 as inode container number 7 . The storage operating system retrieves the inode container number 7 and identifies the pointer 418 to the data block storing the data associated with regular file 60 . In this case given that 60 is a regular file the data block associated with pointer 418 contains the data associated with the file 60 . The file system retrieves the content stored in the data block and modifies the content in the data block as per the client s request. In the event additional data blocks are needed to store additional data from the client . the file system allocates new blocks to store the additional data and stores the pointers to the new blocks in the inode container number 7 .

Similarly in the event some of the data blocks assigned to the file 60 are freed after some of the previously stored data are deleted by the client . the file system removes the references to the pointers to the freed blocks from the inode container number 7 . Once the data access request is completed the data storage node . informs the namespace storage node of the completion status which can in turn inform the client . of the completion of data access request. Thus the file system and its associated inode files of the namespace storage node remains unchanged from a modification of an existing file while the modification is reflected in the file system and its associated inode files of the data storage node .

When a session is established between the client . and the InfiniteVol the software application can send a request to the InfiniteVol . In an embodiment of the technique introduced here the application programming interface API in the InfiniteVol receives the request . The request is a request to determine the changes that have occurred to the files or and directories within a given time interval in the file system of InfiniteVol which is presented to any of the clients . through .N. As discussed above with reference to the file system of the namespace storage node is visible to the client while the file system of the data storage node is not visible to the client.

For purposes of the discussion herein a modification or change in a file system can include adding deleting moving renaming or modifying a file or directory in a file system of the InfiniteVol . The request includes a field that indicates identifies the data subset e.g. volume that will be checked for modified files or and directories in the file system. The request also includes a field that indicates the data subset e.g. base persistent point in time image or snapshot to be used as the base snapshot or base persistent point in time image PPTI which is defined below. A snapshot is a specified subset of data that is maintained by the InfiniteVol . Typically this specified subset of data is for example a volume of data.

Although snapshots are discussed herein as examples of the above mentioned data subset it is within the scope of embodiments of the technique introduced here that the data subset can be any suitable type of persistent point in time image PPTI which is a point in time representation of data e.g. file system that is stored on a storage device e.g. disk . As discussed earlier associated with each file in a dataset is a set of metadata for that file such as a pointer to the file the file size the number of blocks included in the file permissions etc.

This set of metadata is stored in a unit of storage called a metadata container see . One example of a metadata container is an inode which is shown as example inode in .

Each file in a dataset has a separate metadata container e.g. inode which contains the file s metadata. The main purpose of an inode is to store metadata about a particular data file including a pointer to the tree structure of the data file the size e.g. in kilobytes of the data file the number of data blocks in the data file the link count number of references to that data file in the dataset permissions that are associated with the data file creation time date of the data file and access time date to the data file. An inode may also include other metadata that are not mentioned herein. Whenever an actual data block in a file is modified added deleted or renamed at least some of the metadata in that file s inode will necessarily change. Therefore by comparing the contents of an inode in one PPTI e.g. snapshot with the contents of the corresponding inode in another PPTI e.g. snapshot it is possible to determine whether the associated file changed from one PPTI to the other PPTI. If the contents of the two corresponding inodes are different then the file has changed. If the inode contents are identical then the file has not changed.

As discussed above in reference to in the InfiniteVol the data storage nodes . .N and the namespace storage node each maintain an independent file system and associated inode files. In the InfiniteVol the namespace storage node maintains the files and directories in the file system which includes the associated inode files which are visible to the clients . .M while the data associated with the files and directories in the file system are stored in data storage nodes . .N where each data storage nodes . .N maintains an independent file system e.g. file system and associated inode files. Therefore any modification to the data associated with the files in the file system such as adding or deleting content from the stored data will be carried out in the data storage node . .N where at least some of the metadata in the stored data s inode will necessarily change in the data storage node . .N. Such modification of the data associated with the files in the file system will not change the metadata associated with the files in the namespace storage node .

On the other hand when a file is deleted or created in the file system inode files associated with the file are accordingly deleted or created in the file system . Therefore by comparing the contents of an inode in one PPTI e.g. snapshot of the namespace storage node with the contents of the corresponding inode in another PPTI e.g. snapshot of the namespace storage node it is possible to determine files that have been newly created deleted renamed or moved within a given time interval. Further by comparing the contents of an inode in one PPTI e.g. snapshot of a data storage node . .N with the contents of the corresponding inode in another PPTI e.g. snapshot of the data storage node . .N it is possible to determine files that have been modified within a given time interval. Here given that the names of modified files in the data storage nodes . .N are different from the corresponding names of the modified files in the file system of the namespace storage node see discussion relating to a snapshot of the namespace storage node corresponding to snapshot of the data storage node . .N is used to generate filenames and identifiers corresponding to the modified files. explains in detail how file names corresponding to inodes that have changed are determined in the InfiniteVol .

As an example time T2 may be the latest or current time that has occurred when the request is received by the API . Therefore base snapshot can be a data subset e.g. a volume in the file system at a given start time T1 and the difference snapshot can be the same data subset e.g. the same volume at a given end time T2. Therefore the user can indicate typically via software application a time T1 in the field and this time T1 will correspond to an appropriate base snapshot. The user can optionally indicate typically via software application a time T2 in field of the request and this time T2 will correspond to an appropriate difference snapshot . Alternatively the difference snapshot will correspond to the snapshot of the given dataset at the current time when the request is received by the API and in this case the field in the request is not used or does not include a value. Similar to the base snapshot and difference snapshot of the namespace storage node the base snapshot and difference snapshot of the data storage node corresponds to the start time T1 and end time T2.

For each request from a software application the API will forward each request to a comparison unit included in each of the storage nodes . through .N . In an embodiment of the technique introduced here shown in the API will forward each request to the comparison unit of the namespace storage node and to the comparison unit of the data storage node . Based on the contents in the fields and in the requests each comparison unit determines the metadata containers e.g. inodes of files and or directories that have changed between the time interval from T1 to T2 in the namespace storage node . Given that the InfiniteVol stores the data associated with the files in the data storage node while maintaining the namespace associated with the files in the file system of the namespace storage node a comparison of metadata in the base and difference snapshot of the namespace storage node would identify any newly created deleted renamed or moved files or directories in the InfiniteVol . However as discussed above any modifications to the data of the files or directories will not be identifiable based on the comparison of metadata in the base and difference snapshot of the namespace storage node . Any modification to the data associated with the files in the file system of the namespace storage node will be determined by a comparison of metadata in the base and difference snapshot stored within the data storage node .

In the namespace storage node the comparison unit compares the metadata containers e.g. inodes in the base snapshot of the namespace storage node with the same metadata containers e.g. inodes in the difference snapshot of the namespace storage node in order to determine which metadata containers of the namespace storage node have that changed from the time T1 to time T2. In the namespace storage node a change to a metadata container corresponds to a file or directory that has been created deleted renamed or moved in the file system between time intervals T1 and T2. For each metadata container e.g. inode corresponding to a file or directory that has been created deleted renamed or moved in the file system between time intervals T1 and T2 the comparison unit determines the name of the newly created deleted renamed or moved file or directory in the file system and forwards the determined name to the API . illustrates in detail how file names corresponding to inodes that have changed are determined in the InfiniteVol .

In another embodiment the comparison unit will assign and transmit an identifier for each metadata container e.g. inode corresponding to a file or directory that has been created deleted renamed or moved in the file system between time intervals T1 and T2. As an example an identifier can be bit values that are unique for each metadata container. Also each changed metadata container that are listed in the listing will list a corresponding metadata container number e.g. inode number that identifies the directory or file for that corresponds to the changed metadata container. Additional details of the comparison between the snapshots data subsets and is discussed in an example below with reference to . An example of a suitable comparison unit is disclosed in commonly assigned U.S. patent application Ser. No. 11 093 074.

Similar to the functions of the comparison unit the comparison unit of the data storage node compares the metadata containers e.g. inodes in the base snapshot of the data storage node with the same metadata containers e.g. inodes in the difference snapshot of the data storage node in order to determine which metadata containers of the data storage node that have changed from the time T1 to time T2. In the data storage node a change to a metadata container corresponds to a file that has been created deleted or modified in the file system between time intervals T1 and T2. Given that the comparison unit identified all the files that have been created or deleted in the file system the comparison unit filters out the metadata containers corresponding to the files that have been created or deleted and processes only the metadata containers corresponding to the files that have been modified between time intervals T1 and T2. For each metadata container e.g. inode in the data storage node that corresponds to a file that has been modified between time intervals T1 and T2 the comparison unit determines the filename of the modified file as referenced in the file system and forwards the determined name to the API .

Here given that the names of modified files in the data storage node are different from the corresponding names of the modified files in the file system of the namespace storage node see discussion relating to the comparison unit utilizes the base and difference snapshots of the namespace storage node that correspond to the base and difference snapshot of the data storage node to generate filenames and identifiers for the modified files that correspond to the filenames and identifiers for the modified files in the file system . The comparison unit includes a namespace snapshot sync unit that copies the appropriate namespace storage node snapshots for time intervals T1 and T2 before the comparison unit performs the filename and identifier generation. The comparison unit cross references the file names of the modified files in the file system of the data storage node with that of the file names of the modified files in the file system of the namespace storage node to generate filenames and identifiers for the modified files that correspond to the filenames and identifiers for the modified files in the file system . explains in detail how file names corresponding to inodes that have changed are determined in the InfiniteVol .

In an embodiment the request may include a maxdiff value threshold amount which indicates the maximum number of files names or directory names or identifiers of changed metadata containers e.g. inodes that will be contained in a single response from the API . For example if the maxdiff value is set at 50 where maxdiff is typically set by a user via software application then each response from the API will indicate up to 50 identifiers of changed file directory pathnames or metadata containers that correspond to files and or directories that have changed in a given data subset e.g. volume between time T1 and time T2.

A response is sent from the API to the software application until all file directory pathnames or identifiers of metadata containers of changed files changed directories are reported in response to the previous request . When all file directory pathnames or identifiers of metadata containers of changed files changed directories have been reported via the response s the software application can typically send a request to end the session between the host and the storage server .

The listing of all file directory pathnames or metadata containers of changed files directories is useful in for example creating a catalog of information of a file system of the storage server . The listing may include information such as for example a listing of file name changes or directory name changes where these name changes are determined by the metadata container comparison by the comparison unit metadata container numbers which identifies a file or directory that corresponds to the metadata container in file system of the namespace storage node access time date creation time date modification time date and or changes in file size. Therefore the identifier can also include these other information.

As one example of a benefit that is provided by the generation of the listings the listings advantageously permit a faster update of a standard file system index which can be in a memory of a host or can be in a storage space e.g. disk that is accessible to the host . As known to those skilled in the art generally a file system index may be any data structure that organizes a collection of metadata according to some aspect or attribute and permits the host to query the content of a file system based on the aspect or attribute that is indexed in the index . Various software applications for creating and updating a file system index are commercially available from various vendors e.g. Microsoft Corporation . As an example an index may list a name of all files in a file system or can sort the file system content based on associated user content creation or modification time or other attributes. Since the list indicates only the files or directories that have been changed in a file system a user can use a standard software application for creating updating a file system index in order to update the attributes contents in the index based on the listing . Therefore a faster update of the index is possible because only the attributes that are identified in the list are updated by a user in corresponding attributes entries in the index . As mentioned above a user can use any commercially available suitable software application for updating the file system index .

The API can implement other functionalities such as for example the functionalities in the Zephyr API which is a proprietary API that is provided by NetApp Inc. The API typically includes software components and operates with standard hardware components in order to perform the various functions that are discussed herein. The software components in the API are represented by the software module which can be programmed by use of suitable known software programming languages e.g. C C or other languages and by use of known programming techniques.

The transmissions of requests and responses between the software application and API can use for example XML extensible markup language . As known to those skilled in the art XML is commonly used on the Internet to create simple methods for the exchange of data among diverse clients or hosts. However different transport mechanisms that are known to those skilled in the art can alternatively be used for the transmissions of the requests and responses . The functions calls in a transport mechanism may require modification depending on for example if transmissions are being sent via a socket fibre channel SCSI or via TCP IP.

As shown in the example of the structure of a PPTI i.e. data subset includes a metadata container file which has information about all metadata containers e.g. inodes for a given dataset. As an example the base snapshot of the data storage node has a corresponding metadata container file which stores information about all metadata containers e.g. inodes for a given data subset such as e.g. a volume along with the state of the data subset taken at time T1. The metadata container file has a hierarchical structure with a root node . The root node has fields that each contains pointers to another node in the metadata container file . This other node can be an indirect block not shown in which points to another node or a direct block and a direct block as shown in the example of . The direct blocks and includes metadata containers of files for the given data subset. For example the metadata container includes a pointer to a corresponding data file which has the data of a file that corresponds to the metadata container . The metadata container also includes metadata that relates to the data file . The data file is also in a hierarchical structure and includes a root node with fields that contain pointers to direct data blocks . The direct data blocks contain a portion of the actual data of the file that corresponds to the metadata container

The difference snapshot has a corresponding metadata container file which stores information about all metadata containers for the same given data subset with the state of the data subset taken at time T2. Therefore root node is the root node at time T2. The comparison unit compares each metadata container in the metadata container file with the same metadata container in the metadata container file in order to determine if a file or directory corresponding to the metadata container has changed i.e. modified added deleted or accessed between the time T1 and time T2. For example the comparison unit compares the content in a particular field in the metadata container at time T1 with the content in the same particular field in the same metadata container shown as metadata container at time T2. If the contents in the field have changed between time T1 and time T2 then the metadata container is a changed metadata container. The fields in a metadata container would indicate for example a data file pointer file size number of blocks for the file link count permissions creation time date and access time date.

The comparison unit compares the fields with the data file pointer file size number of blocks for the file link count permissions creation time date and access time date in the metadata container with corresponding fields in the metadata container in order to determine if a metadata container has been modified between time T1 and time T2. These fields were previously discussed above with reference to . Since a metadata container file is sequentially accessed by the comparison unit the speed of determining the changed metadata containers is increased. Note that a third party software application which is external to a storage server is not able to access and not able to read the metadata container files in the file system. Furthermore the API and the comparison unit advantageously eliminates the use of the above discussed previous techniques where an external software application is required to perform multiple reads in a file system to determine the changes in the file system.

For a directory metadata container e.g. directory inode the comparison unit can read the directory metadata container blocks in parallel for faster speed. The contents of a directory metadata container blocks are the names and references for the metadata containers in that directory. The comparison unit can simultaneously read two 2 or more directory metadata container blocks and compare them to corresponding directory metadata container blocks at time T2 in order to determine changes in the directories in the file system. This metadata container comparison process is further described in for example the above cited U.S. application Ser. No. 11 093 074.

Similar to the comparison unit the comparison unit of the namespace storage node compares the metadata in the metadata container files of the base and difference snapshot in order to determine if a file or directory corresponding to the metadata container has changed i.e. added deleted or accessed between the time T1 and time T2. In addition as discussed earlier in reference to and the comparison unit of the data storage node filters the metadata container files that have changed between the time T1 and time T2 and processes only those metadata container files that correspond to a modified file or directory.

Another embodiment of the technique introduced here can have the optional feature of providing access security by the API so that only authorized requests are processed by the API . In this case of using authorized requests as an example the session request would include a password field that would be authenticated by the API before a session is established between the host and storage server . Another embodiment of the technique introduced here can have the optional feature where the API can send a progress indicator to the application software to indicate the progress of the above discussed metadata container comparison process.

In the namespace storage node the comparison unit utilizes the metafile to determine the name of the file or directory associated with the changed inode file i.e. metadata container file using the inode container number of the changed inode file. Further in one embodiment the comparison unit determines the full pathnames of all the files or directories for which differences were recorded. By full pathname what is meant is the filename and the names of any directories and subdirectories in which the file is located from the root of the volume to the file itself in human readable form. For example assume a file named China exports is stored in a subdirectory called world trade which is stored in a directory called economics in a storage volume named vol1 in that case the full pathname of the file would be . vol1 economics world trade China exports. The full pathnames can be determined simply by walking the snapshot trees starting from the root nodes and recording the various directories and subdirectories along the path to each changed file. A technique for quickly and efficiently walking a hierarchical data set to identify full pathnames of changed files and directories which is suitable for this purpose is described in co pending U.S. patent application Ser. No. 10 954 381 of S. Manley et al. filed on the Sep. 29 2004 and entitled Method and Apparatus for Generating User Level Difference Information About Two Data Sets the Manley technique which is incorporated herein by reference.

In the data storage nodes . .N the comparison units perform a similar function to the comparison unit of the namespace storage node to determine the names of files and directories that correspond to metadata container files e.g. inodes that have been modified in the InfiniteVol . illustrates a metafile created and maintained by the storage operating system as part of the file system in the data storage node .. As discussed with reference to when the data associated with file mbox is stored in the data storage node . under file name 60 where 60 refers to the inode number associated with file mbox in file system the storage operating system created an inode file with container number 7 and associated with the file name 60 . Further the storage operating system created an entry with inode contained number 7 mapping to file name 60 in the metafile . In the data storage node . the comparison unit similar to the comparison unit utilizes the metafile to determine the name of the file or directory associated with the changed inode file i.e. metadata container file using the inode container number of the changed inode file.

However unlike the namespace storage node in the data storage node . the name of the file or directory associated with the changed inode file corresponds to an internal name that is not visible to the clients . .M making the request. For example the data associated with file mbox in the data storage node . is stored under the name 60 . So when the name of the file associated with a modified inode file with container number 7 is looked up in the metafile the filename associated with a modified inode file shows up as 60 which is an internal name for the data associated with the file mbox . In one embodiment the comparison units of the data storage nodes perform an additional lookup utilizing the internal file name e.g. filename 60 . The comparison unit utilizes the internal file name e.g. filename 60 and the metafile associated with the namespace storage node to look up the file or directory name associated with the inode container number 60 where the internal file name 60 is now used as the inode container number . This is possible because when the data associated with file mbox was stored in data storage node . the storage operating system deliberately provided the inode container number of the file here container number 60 whose data was stored in the data storage node . enabling the data storage node . to do the reverse lookup in metafile to identify the filename mbox associated with the modified file 60 .

In metafile the inode container 60 corresponds to the filename mbox . Once the filename and the associated inode container number 60 is available the comparison unit similar to the comparison unit determines the full pathnames of all the files or directories for which differences were recorded. The full pathnames can be determined simply by walking the namespace snapshot trees that were previously copied locally by the namespace snapshot sync unit starting from the root nodes and recording the various directories and subdirectories along the path to each changed file.

In block the comparison unit in the data storage node compares metadata containers e.g. inodes in the file system at a start time T1 with the same corresponding metadata container at a subsequent time T2 in order to determine if a file corresponding to a metadata container in the file system has been modified between time T1 and time T2. In block API buffers an identifier e.g. filename with its full pathname for each metadata container that corresponds to a file that has been modified.

In block when the number of buffered identifiers buffered in buffer has reached a maxdiff value maximum different value the API will package and transmit the identifiers in a response to the external software application request . When the API stops receiving any identifier from the comparison unit the API will package and transmit any remaining buffered identifiers in the buffer via the response that is transmitted to the external software application .

It is also within the scope of an embodiment of the technique introduced here to implement a program or code that can be stored in a machine readable medium to permit a computer to perform any of the methods described above. The above description of illustrated embodiments of the invention including what is described in the Abstract is not intended to be exhaustive or to limit the invention to the precise forms disclosed. While specific embodiments of and examples for the invention are described herein for illustrative purposes various equivalent modifications are possible within the scope of the invention as those skilled in the relevant art will recognize. These modifications can be made to the invention in light of the above detailed description. The terms used in the following claims should not be construed to limit the invention to the specific embodiments disclosed in the specification and the claims. Rather the scope of the invention is to be determined entirely by the following claims which are to be construed in accordance with established doctrines of claim interpretation.

The techniques introduced above can be implemented by programmable circuitry programmed or configured by software and or firmware or entirely by special purpose circuitry or in a combination of such forms. Such special purpose circuitry if any can be in the form of for example one or more application specific integrated circuits ASICs programmable logic devices PLDs field programmable gate arrays FPGAs etc. Software or firmware for implementing the techniques introduced here may be stored on a machine readable storage medium and may be executed by one or more general purpose or special purpose programmable microprocessors. A machine readable medium as the term is used herein includes any mechanism that can store information in a form accessible by a machine a machine may be for example a computer network device cellular phone personal digital assistant PDA manufacturing tool any device with one or more processors etc. . For example a machine accessible medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. etc.

Thus an apparatus and method for presenting differences in a file system of a clustered storage system have been described. Note that references throughout this specification to one embodiment or an embodiment mean that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the technique introduced here. Therefore it is emphasized and should be appreciated that two or more references to an embodiment or one embodiment or an alternative embodiment in various portions of this specification are not necessarily all referring to the same embodiment. Furthermore the particular features structures or characteristics being referred to may be combined as suitable in one or more embodiments of the invention as will be recognized by those of ordinary skill in the art.

While the present disclosure is described above with respect to what is currently considered its preferred embodiments it is to be understood that the disclosure is not limited to that described above. To the contrary the disclosure is intended to cover various modifications and equivalent arrangements within the spirit and scope of the appended claims.

