---

title: Methods and systems for enhancing television applications using 3D pointing
abstract: Systems and methods according to the present invention provide a control framework for organizing, selecting and launching media items including graphical user interfaces coupled with an optional 3D control device for collection of the basic control primitives of point, click, scroll, hover and zoom which permit for easy and rapid selection of media items, e.g., movies, songs etc., from large or small collections. The remote control maps natural hand movements and gestures into relevant commands while the graphical display uses images, zooming for increased/decreased levels of detail and continuity of GUI objects to provide easy organization, selection and navigation among the media items by a user.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09436359&OS=09436359&RS=09436359
owner: HILLCREST LABORATORIES, INC.
number: 09436359
owner_city: Rockville
owner_country: US
publication_date: 20130415
---
This application is a continuation of U.S. patent application Ser. No. 13 448 907 filed on Apr. 17 2012 which is a continuation of U.S. patent application Ser. No. 12 951 669 filed on Nov. 22 2010 and now issued as U.S. Pat. No. 8 169 405 which is a continuation of U.S. patent application Ser. No. 11 354 329 filed on Feb. 14 2006 and now issued as U.S. Pat. No. 7 839 385 which is related to and claims priority from U.S. Provisional Patent Application No. 60 653 229 entitled Methods and Systems for Enhancing Television Applications Using Free Space Pointing filed on Feb. 14 2005 the disclosure of which is incorporated here by reference.

The present invention describes a framework for organizing selecting and launching media items. Part of that framework involves the design and operation of graphical user interfaces with the basic building blocks of point click scroll hover and zoom and more particularly to graphical user interfaces associated with media items which can be used with a 3D pointing remote.

Technologies associated with the communication of information have evolved rapidly over the last several decades. Television cellular telephony the Internet and optical communication techniques to name just a few things combine to inundate consumers with available information and entertainment options. Taking television as an example the last three decades have seen the introduction of cable television service satellite television service pay per view movies and video on demand. Whereas television viewers of the 1960s could typically receive perhaps four or five over the air TV channels on their television sets today s TV watchers have the opportunity to select from hundreds and potentially thousands of channels of shows and information. Video on demand technology currently used primarily in hotels and the like provides the potential for in home entertainment selection from among thousands of movie titles. Digital video recording DVR equipment such as offered by TiVo Inc. 2160 Gold Street Alviso Calif. 95002 further expand the available choices.

The technological ability to provide so much information and content to end users provides both opportunities and challenges to system designers and service providers. One challenge is that while end users typically prefer having more choices rather than fewer this preference is counterweighted by their desire that the selection process be both fast and simple. Unfortunately the development of the systems and interfaces by which end users access media items has resulted in selection processes which are neither fast nor simple. Consider again the example of television programs. When television was in its infancy determining which program to watch was a relatively simple process primarily due to the small number of choices. One would consult a printed guide which was formatted for example as series of columns and rows which showed the correspondence between 1 nearby television channels 2 programs being transmitted on those channels and 3 date and time. The television was tuned to the desired channel by adjusting a tuner knob and the viewer watched the selected program. Later remote control devices were introduced that permitted viewers to tune the television from a distance. This addition to the user television interface created the phenomenon known as channel surfing whereby a viewer could rapidly view short segments being broadcast on a number of channels to quickly learn what programs were available at any given time.

Despite the fact that the number of channels and amount of viewable content has dramatically increased the generally available user interface and control device options and framework for televisions has not changed much over the last 30 years. Printed guides are still the most prevalent mechanism for conveying programming information. The multiple button remote control with simple up and down arrows is still the most prevalent channel content selection mechanism. The reaction of those who design and implement the TV user interface to the increase in available media content has been a straightforward extension of the existing selection procedures and interface objects. Thus the number of rows and columns in the printed guides has been increased to accommodate more channels. The number of buttons on the remote control devices has been increased to support additional functionality and content handling e.g. as shown in . However this approach has significantly increased both the time required for a viewer to review the available information and the complexity of actions required to implement a selection. Arguably the cumbersome nature of the existing interface has hampered commercial implementation of some services e.g. video on demand since consumers are resistant to new services that will add complexity to an interface that they view as already too slow and complex.

In addition to increases in bandwidth and content the user interface bottleneck problem is being exacerbated by the aggregation of technologies. Consumers are reacting positively to having the option of buying integrated systems rather than a number of segregable components. A good example of this trend is the combination television VCR DVD in which three previously independent components are frequently sold today as an integrated unit. This trend is likely to continue potentially with an end result that most if not all of the communication devices currently found in the household being packaged as an integrated unit e.g. a television VCR DVD internet access radio stereo unit. Even those who buy separate components desire seamless control of and interworking between them. With this increased aggregation comes the potential for more complexity in the user interface. For example when so called universal remote units were introduced e.g. to combine the functionality of TV remote units and VCR remote units the number of buttons on these universal remote units was typically more than the number of buttons on either the TV remote unit or VCR remote unit individually. This added number of buttons and functionality makes it very difficult to control anything but the simplest aspects of a TV or VCR without hunting for exactly the right button on the remote. Many times these universal remotes do not provide enough buttons to access many levels of control or features unique to certain TVs. In these cases the original device remote unit is still needed and the original hassle of handling multiple remotes remains due to user interface issues arising from the complexity of aggregation. Some remote units have addressed this problem by adding soft buttons that can be programmed with the expert commands. These soft buttons sometimes have accompanying LCD displays to indicate their action. These too have the flaw that they are difficult to use without looking away from the TV to the remote control. Yet another flaw in these remote units is the use of modes in an attempt to reduce the number of buttons. In these moded universal remote units a special button exists to select whether the remote should communicate with the TV DVD player cable set top box VCR etc. This causes many usability issues including sending commands to the wrong device forcing the user to look at the remote to make sure that it is in the right mode and it does not provide any simplification to the integration of multiple devices. The most advanced of these universal remote units provide some integration by allowing the user to program sequences of commands to multiple devices into the remote. This is such a difficult task that many users hire professional installers to program their universal remote units.

Some attempts have also been made to modernize the screen interface between end users and media systems. Electronic program guides EPGs have been developed and implemented to replace the afore described media guides. Early EPGs provided what was essentially an electronic replica of the printed media guides. For example cable service operators have provided analog EPGs wherein a dedicated channel displays a slowly scrolling grid of the channels and their associated programs over a certain time horizon e.g. the next two hours. Scrolling through even one hundred channels in this way can be tedious and is not feasibly scalable to include significant additional content deployment e.g. video on demand. More sophisticated digital EPGs have also been developed. In digital EPGs program schedule information and optionally applications system software is transmitted to dedicated EPG equipment e.g. a digital set top box STB . Digital EPGs provide more flexibility in designing the user interface for media systems due to their ability to provide local interactivity and to interpose one or more interface layers between the user and the selection of the media items to be viewed. An example of such an interface can be found in U.S. Pat. No. 6 421 067 to Kamen et al. the disclosure of which is incorporated here by reference. depicts a GUI described in the 067 patent. Therein according to the Kamen et al. patent a first column 190 lists program channels a second column 191 depicts programs currently playing a column 192 depicts programs playing in the next half hour and a fourth column 193 depicts programs playing in the half hour after that. The baseball bat icon 121 spans columns 191 and 192 thereby indicating that the baseball game is expected to continue into the time slot corresponding to column 192. However text block 111 does not extend through into column 192. This indicates that the football game is not expected to extend into the time slot corresponding to column 192. As can be seen a pictogram 194 indicates that after the football game ABC will be showing a horse race. The icons shown in can be actuated using a cursor not shown to implement various features e.g. to download information associated with the selected programming. Other digital EPGs and related interfaces are described for example in U.S. Pat. Nos. 6 314 575 6 412 110 and 6 577 350 the disclosures of which are also incorporated here by reference.

However the interfaces described above suffer from among other drawbacks an inability to easily scale between large collections of media items and small collections of media items. For example interfaces which rely on lists of items may work well for small collections of media items but are tedious to browse for large collections of media items. Interfaces which rely on hierarchical navigation e.g. tree structures may be more speedy to traverse than list interfaces for large collections of media items but are not readily adaptable to small collections of media items. Additionally users tend to lose interest in selection processes wherein the user has to move through three or more layers in a tree structure. For all of these cases current remote units make this selection processor even more tedious by forcing the user to repeatedly depress the up and down buttons to navigate the list or hierarchies. When selection skipping controls are available such as page up and page down the user usually has to look at the remote to find these special buttons or be trained to know that they even exist.

Accordingly it would be desirable to provide organizing frameworks techniques and systems which simplify the control and screen interface between users and media systems as well as accelerate the selection process while at the same time permitting service providers to take advantage of the increases in available bandwidth to end user equipment by facilitating the supply of a large number of media items and new services to the user. Moreover it would be desirable to provide interfaces which supply an easy and fast selection experience regardless of the size s of the media item collection s being browsed.

Systems and methods according to the present invention address these needs and others by providing a total control framework for organizing selecting and launching media items including an user interface framework which then provides for easy and rapid selection of media items. Control of the framework can employ a 3D pointing device that includes a minimal set of buttons and scroll wheel for pointing clicking and scrolling through selections on an associated graphical user interface. This exemplary graphical user interface GUI provides feedback to the user through the use of an on screen pointer graphical animations when the pointer hovers over selections and zooming into and out of selections to smoothly navigate between overview and detail screens. Exemplary embodiments of the present invention employ images zooming for increased decreased levels of detail and continuity of GUI objects which permit easy navigation by a user. Graphical user interfaces according to the present invention organize media item selections on a virtual surface. Similar selections can be grouped together. Initially the interface presents a zoomed out view of the surface and in most cases the actual selections will not be visible in full detail at this level. As the user zooms progressively inward more details are revealed concerning the media item groups or selections. At different zoom levels different controls are available so that the user can play groups of selections individual selections or go to another part of the virtual surface to browse other related media items.

According to one exemplary embodiment of the present invention a method for using a 3D pointing device with a software application that displays output on a television screen includes the steps of displaying a first set of information on the television screen associated with the software application wherein the software application includes at least one of an electronic program guide television screen controls a photo album a home video collection and a shopping interface moving the 3D pointing device processing data associated with movement of the 3D pointing device to compensate for unintended movement of the 3D pointing device moving a cursor displayed on the television screen based on the processed data to a new position on the television screen and changing the display of the first set of information on the television screen in response to the new position of the cursor.

According to another exemplary embodiment of the present invention a system includes a 3D pointing device a television a processor for running a software application that displays output on the television wherein the processor initially displays a first set of information on the television associated with at least one of an electronic program guide television screen controls a photo album a home video collection and a shopping interface further wherein in response to movement of the 3D pointing device the processor compensates data output from the 3D pointing device for unintended movement and then moves a cursor displayed on the television based on the compensated data to a new position on the television and further wherein said display of the first set of information on the television screen is changed in response to the new position of the cursor.

According to yet another exemplary embodiment of the present invention a system for using a 3D pointing device with a software application that displays output on a television screen includes means for displaying a first set of information on the television screen associated with the software application wherein the software application includes at least one of an electronic program guide television screen controls a photo album a home video collection and a shopping interface means for moving the 3D pointing device means for processing data associated with movement of the 3D pointing device to compensate for unintended movement of the 3D pointing device means for moving a cursor displayed on the television screen based on the processed data to a new position on the television screen and means for changing the display of the first set of information on the television screen in response to the new position of the cursor.

According to yet another exemplary embodiment of the present invention a software application for displaying information on a television screen in response to movement of a 3D pointing device includes means for processing movement data to remove unintended movement therefrom means for repositioning a cursor on the television screen in response to the processed movement data and means for changing a display of information on the television screen solely responsive to the repositioning of the cursor.

The following detailed description of the invention refers to the accompanying drawings. The same reference numbers in different drawings identify the same or similar elements. Also the following detailed description does not limit the invention. Instead the scope of the invention is defined by the appended claims.

In order to provide some context for this discussion an exemplary aggregated media system in which the present invention can be implemented will first be described with respect to . Those skilled in the art will appreciate however that the present invention is not restricted to implementation in this type of media system and that more or fewer components can be included therein. Therein an input output I O bus connects the system components in the media system together. The I O bus represents any of a number of different of mechanisms and techniques for routing signals between the media system components. For example the I O bus may include an appropriate number of independent audio patch cables that route audio signals coaxial cables that route video signals two wire serial lines or infrared or radio frequency transceivers that route control signals optical fiber or any other routing mechanisms that route other types of signals.

In this exemplary embodiment the media system includes a television monitor a video cassette recorder VCR digital video disk DVD recorder playback device audio video tuner and compact disk player coupled to the I O bus . The VCR DVD and compact disk player may be single disk or single cassette devices or alternatively may be multiple disk or multiple cassette devices. They may be independent units or integrated together. In addition the media system includes a microphone speaker system video camera and a wireless I O control device . According to exemplary embodiments of the present invention the wireless I O control device is a media system remote control unit that supports 3D pointing has a minimal number of buttons to support navigation and communicates with the entertainment system through RF signals. For example wireless I O control device can be a 3D pointing device which uses a gyroscope or other mechanism to define both a screen position and a motion vector to determine the particular command desired. A set of buttons can also be included on the wireless I O device to initiate the click primitive described below as well as a back button. In another exemplary embodiment wireless I O control device is a media system remote control unit which communicates with the components of the entertainment system through IR signals. In yet another embodiment wireless I O control device may be an IR remote control device similar in appearance to a typical entertainment system remote control with the added feature of a track ball or other navigational mechanisms which allows a user to position a cursor on a display of the entertainment system .

The entertainment system also includes a system controller . According to one exemplary embodiment of the present invention the system controller operates to store and display entertainment system data available from a plurality of entertainment system data sources and to control a wide variety of features associated with each of the system components. As shown in system controller is coupled either directly or indirectly to each of the system components as necessary through I O bus . In one exemplary embodiment in addition to or in place of I O bus system controller is configured with a wireless communication transmitter or transceiver which is capable of communicating with the system components via IR signals or RF signals. Regardless of the control medium the system controller is configured to control the media components of the media system via a graphical user interface described below.

As further illustrated in media system may be configured to receive media items from various media sources and service providers. In this exemplary embodiment media system receives media input from and optionally sends information to any or all of the following sources cable broadcast satellite broadcast e.g. via a satellite dish very high frequency VHF or ultra high frequency UHF radio frequency communication of the broadcast television networks e.g. via an aerial antenna telephone network and cable modem or another source of Internet content . Those skilled in the art will appreciate that the media components and media sources illustrated and described with respect to are purely exemplary and that media system may include more or fewer of both. For example other types of inputs to the system include AM FM radio and satellite radio.

Generation and control of a graphical user interface according to exemplary embodiments of the present invention to display media item selection information is performed by the system controller in response to the processor executing sequences of instructions contained in the memory . Such instructions may be read into the memory from other computer readable mediums such as data storage device s or from a computer connected externally to the media system . Execution of the sequences of instructions contained in the memory causes the processor to generate graphical user interface objects and controls among other things on monitor . In alternative embodiments hard wire circuitry may be used in place of or in combination with software instructions to implement the present invention. As mentioned in the Background section conventional interface frameworks associated with the television industry are severely limited in their ability to provide users with a simple and yet comprehensive selection experience. Accordingly control frameworks described herein overcome these limitations and are therefore intended for use with televisions albeit not exclusively. It is also anticipated that the revolutionary control frameworks graphical user interfaces and or various algorithms described herein will find applicability to interfaces which may be used with computers and other non television devices. In order to distinguish these various applications of exemplary embodiments of the present invention the terms television and TV are used in this specification to refer to a subset of display devices whereas the terms GUI GUI screen display and display screen are intended to be generic and refer to television displays computer displays and any other display device. More specifically the terms television and TV are intended to refer to the subset of display devices which are able to display television signals e.g. NTSC signals PAL signals or SECAM signals without using an adapter to translate television signals into another format e.g. computer video formats . In addition the terms television and TV refer to a subset of display devices that are generally viewed from a distance of several feet or more e.g. sofa to a family room TV whereas computer displays are generally viewed close up e.g. chair to a desktop monitor .

Having described an exemplary media system which can be used to implement control frameworks including zoomable graphical interfaces according to the present invention several examples of such interfaces will now be described. According to exemplary embodiments of the present invention a user interface displays selectable items which can be grouped by category. A user points a remote unit at the category or categories of interest and depresses the selection button to zoom in or the back button to zoom back. Each zoom in or zoom back action by a user results in a change in the magnification level and or context of the selectable items rendered by the user interface on the screen. According to exemplary embodiments each change in magnification level can be consistent i.e. the changes in magnification level are provided in predetermined steps. Exemplary embodiments of the present invention also provide for user interfaces which incorporate several visual techniques to achieve scaling to the very large. These techniques involve a combination of building blocks and techniques that achieve both scalability and ease of use in particular techniques which adapt the user interface to enhance a user s visual memory for rapid re visiting of user interface objects.

The user interface is largely a visual experience. In such an environment exemplary embodiments of the present invention make use of the capability of the user to remember the location of objects within the visual environment. This is achieved by providing a stable dependable location for user interface selection items. Each object has a location in the zoomable layout. Once the user has found an object of interest it is natural to remember which direction was taken to locate the object. If that object is of particular interest it is likely that the user will re visit the item more than once which will reinforce the user s memory of the path to the object. User interfaces according to exemplary embodiments of the present invention provide visual mnemonics that help the user remember the location of items of interest. Such visual mnemonics include pan and zoom animations transition effects which generate a geographic sense of movement across the user interface s virtual surface and consistent zooming functionality among other things which will become more apparent based on the examples described below.

Organizing mechanisms are provided to enable the user to select from extremely large sets of items while being shielded from the details associated with large selection sets. Various types of organizing mechanisms can be used in accordance with the present invention and examples are provided below.

Referring first to an exemplary control framework including a zoomable graphical user interface according to an exemplary embodiment of the present invention is described for use in displaying and selecting musical media items. Various alternative and or additional GUI screens associated with interfaces according to the present invention can also be found in the later Figures. portrays the zoomable GUI at its most zoomed out state. Therein the interface displays a set of shapes . Displayed within each shape are text and or a picture that describe the group of media item selections accessible via that portion of the GUI. As shown in the shapes are rectangles and text and or picture describe the genre of the media. However those skilled in the art will appreciate that this first viewed GUI grouping could represent other aspects of the media selections available to the user e.g. artist year produced area of residence for the artist length of the item or any other characteristic of the selection. Also the shapes used to outline the various groupings in the GUI need not be rectangles. Shrunk down versions of album covers and other icons could be used to provide further navigational hints to the user in lieu of or in addition to text and or picture within the shape groupings . A background portion of the GUI can be displayed as a solid color or be a part of a picture such as a map to aid the user in remembering the spatial location of genres so as to make future uses of the interface require less reading. The selection pointer cursor follows the movements of an input device and indicates the location to zoom in on when the user presses the button on the device not shown in .

According to one exemplary embodiment of the present invention the input device can be a wireless mouse e.g. the wireless mouse manufactured by Gyration Inc. 12930 Saratoga Avenue Bldg. C Saratoga Calif. 95070 coupled with a graphical user interface that supports the point click scroll hover and zoom building blocks which are described in more detail below. One feature of this exemplary input device that is beneficial for use in conjunction with the present invention is that it has only two buttons and a scroll wheel i.e. three input actuation objects. One of the buttons can be configured as a ZOOM IN select button and one can be configured as a ZOOM OUT back button. Compared with the conventional remote control units e.g. that shown in the present invention simplifies this aspect of the GUI by greatly reducing the number of buttons etc. that a user is confronted with in making his or her media item selection. An additional preferred but not required feature of input devices according to exemplary embodiments of the present invention is that they provide 3D pointing capability for the user. The phrase 3D pointing is used in this specification to refer to the ability of a user to freely move the input device in three or more dimensions in the air in front of the display screen and the corresponding ability of the user interface to translate those motions directly into movement of a cursor on the screen. Thus 3D pointing differs from conventional computer mouse pointing techniques which use a surface other than the display screen e.g. a desk surface or mousepad as a proxy surface from which relative movement of the mouse is translated into cursor movement on the computer display screen. Use of 3D pointing in control frameworks according to exemplary embodiments of the present invention further simplifies the user s selection experience while at the same time providing an opportunity to introduce gestures as distinguishable inputs to the interface. A gesture can be considered as a recognizable pattern of movement over time which pattern can be translated into a GUI command e.g. a function of movement in the x y z yaw pitch and roll dimensions or any subcombination thereof. Those skilled in the art will appreciate however that any suitable input device can be used in conjunction with zoomable GUIs according to the present invention. Other examples of suitable input devices include but are not limited to trackballs touchpads conventional TV remote control devices speech input any devices which can communicate translate a user s gestures into GUI commands or any combination thereof. It is intended that each aspect of the GUI functionality described herein can be actuated in frameworks according to the present invention using at least one of a gesture and a speech command. Alternate implementations include using cursor and or other remote control keys or even speech input to identify items for selection.

A user may then select one of the artist groups for further review and or selection. shows a further zoomed in view in response to a user selection of Artist 3 via positioning of cursor and actuation of the input device in which images of album covers come into view. As with the transition from the GUI screen of and the unselected adjacent artists artists 2 6 and 7 in this example are shown towards the side of the zoomed in display and the user can click on these with selection pointer to pan to these artist views. In this portion of the interface in addition to the images of album covers artist information can be displayed as an item in the artist group. This information may contain for example the artist s picture biography trivia discography influences links to web sites and other pertinent data. Each of the album images can contain a picture of the album cover and optionally textual data. In the case that the album image includes a user created playlist the graphical user interface can display a picture which is selected automatically by the interface or preselected by the user.

Finally when the user selects an album cover image from within the group the interface zooms into the album cover as shown in . As the zoom progresses the album cover can fade or morph into a view that contains items such as the artist and title of the album a list of tracks further information about the album a smaller version of the album cover and controls to play back the content modify the categorization link to the artists web page or find any other information about the selection. Neighboring albums are shown that can be selected using selection pointer to cause the interface to bring them into view. As mentioned above alternative embodiments of the present invention can for example zoom in to only display the selected object e.g. album 5 and omit the clipped portions of the unselected objects e.g. albums 4 and 6. This final zoom provides an example of semantic zooming wherein certain GUI elements are revealed that were not previously visible at the previous zoom level. Various techniques for performing semantic zooming according to exemplary embodiments of the present invention are provided below.

As illustrated in the and the description this exemplary embodiment of a graphical user interface provides for navigation of a music collection. Interfaces according to the present invention can also be used for video collections such as for DVDs VHS tapes other recorded media video on demand video segments and home movies. Other audio uses include navigation of radio shows instructional tapes historical archives and sound clip collections. Print or text media such as news stories and electronic books can also be organized and accessed using this invention.

As will be apparent to those skilled in the art from the foregoing description zoomable graphical user interfaces according to the present invention provide users with the capability to browse a large or small number of media items rapidly and easily. This capability is attributable to many characteristics of interfaces according to exemplary embodiments of the present invention including but not limited to 1 the use of images as all or part of the selection information for a particular media item 2 the use of zooming to rapidly provide as much or as little information as a user needs to make a selection and 3 the use of several GUI techniques which combine to give the user the sense that the entire interface resides on a single plane such that navigation of the GUI can be accomplished and remembered by way of the user s sense of direction. This latter aspect of GUIs according to the present invention can be accomplished by among other things linking the various GUI screens together geographically by maintaining as much GUI object continuity from one GUI screen to the next e.g. by displaying edges of neighboring unselected objects around the border of the current GUI screen. Alternatively if a cleaner view is desired and other GUI techniques provide sufficient geographic feedback then the clipped objects can be omitted. As used in this text the phrase GUI screen refers to a set of GUI objects rendered on one or more display units at the same time. A GUI screen may be rendered on the same display which outputs media items or it may be rendered on a different display. The display can be a TV display computer monitor or any other suitable GUI output device.

Another GUI effect which enhances the user s sense of GUI screen connectivity is the panning animation effect which is invoked when a zoom is performed or when the user selects an adjacent object at the same zoom level as the currently selected object. Returning to the example of as the user is initially viewing this GUI screen his or her point of view is centered about point . However when he or she selects Genre 3 for zooming in his or her point of view will shift to point . According to exemplary embodiments of the present invention the zoom in process is animated to convey the shifting the POV center from point to . This panning animation can be provided for every GUI change e.g. from a change in zoom level or a change from one object to another object on the same GUI zoom level. Thus if for example a user situated in the GUI screen of selected the leftmost unselected genre Genre 2 a panning animation would occur which would give the user the visual impression of moving left or west. Exemplary embodiments of the present invention employ such techniques to provide a consistent sense of directional movement between GUI screens enables users to more rapidly navigate the GUI both between zoom levels and between media items at the same zoom level.

Various data structures and algorithms can be used to implement zoomable GUIs according to the present invention. For example data structures and algorithms for panning and zooming in an image browser which displays photographs have been described for example in the article entitled Quantum Treemaps and Bubblemaps for a Zoomable Image Browser by Benjamin B. Bederson 20013 2 pp. 71 80 the disclosure of which is incorporated here by reference. However in order to provide a GUI for media selection which can at a high level switch between numerous applications and at a lower level provide user controls associated with selected images to perform various media selection functions additional data structures and algorithms are needed.

Zoomable GUIs can be conceptualized as supporting panning and zooming around a scene of user interface components in the view port of a display device. To accomplish this effect zoomable GUIs according to exemplary embodiments of the present invention can be implemented using scene graph data structures. Each node in the scene graph represents some part of a user interface component such as a button or a text label or a group of interface components. Children of a node represent graphical elements lines text images etc. internal to that node. For example an application can be represented in a scene graph as a node with children for the various graphical elements in its interface. Two special types of nodes are referred to herein as cameras and layers. Cameras are nodes that provide a view port into another part of the scene graph by looking at layer nodes. Under these layer nodes user interface elements can be found. Control logic for a zoomable interface programmatically adjusts a cameras view transform to provide the effect of panning and zooming.

Rendering the scene graph can be accomplished as follows. Whenever the display needs to be updated e.g. when the user triggers a zoom in from the view of to the view of a repaint event calls the camera node attached to the display to render itself. This in turn causes the camera node to notify the layer node to render the area within the camera s view port. The layer node renders itself by notifying its children to render themselves and so on. The current transformation matrix and a bounding rectangle for the region to update is passed at each step and optionally modified to inform each node of the proper scale and offset that they should use for rendering. Since the scene graphs of applications operating within zoomable GUIs according to the present invention may contain thousands of nodes each node can check the transformation matrix and the area to be updated to ensure that their drawing operations will indeed be seen by the user. Although the foregoing example describes a scene graph including one camera node and one layer node it will be appreciated that exemplary embodiments of the present invention can embed multiple cameras and layers. These embedded cameras can provide user interface elements such as small zoomed out maps that indicate the user s current view location in the whole zoomable interface and also allow user interface components to be independently zoomable and pannable.

When using a zoomable interface to coordinate the operation of multiple applications e.g. like the exemplary movie browser described below with respect to the memory and resource requirements for each application may exceed the total memory available in the media system. This suggests that applications unload some or all of their code and data when the user is no longer viewing them. However in zoomable GUIs according to the present invention it can be desirable to provide the appearance that some or all of the applications appear active to the user at all times. To satisfy these two competing objectives the applications which are off screen from the user s point of view can be put into a temporarily suspended state. To achieve this behavior in zoomable GUIs according to exemplary embodiments of the present invention events are sent to applications to indicate when they enter and exit a view port. One way to implement such events is to add logic to the code that renders a component so that it detects when the user enters a view port. However this implies that the notification logic gets invoked at every rendering event and more importantly that it cannot easily detect when the user has navigated the view port away from the component. Another method for sending events to applications is to incorporate the notification logic into the GUI navigation elements such as hyperlinks and buttons so that they send notifications to the component when they change the view port of a camera to include the component of interest. However this requires the programmer to vigilantly add notification code to all possible navigation UI elements.

According to one exemplary embodiment a computationally efficient node watcher algorithm can be used to notify applications regarding when GUI components and or applications enter and exit the view of a camera. At a high level the node watcher algorithm has three main processing stages 1 initialization 2 view port change assessment and 3 scene graph change assessment. The initialization stage computes node quantities used by the view port change assessment stage and initializes appropriate data structures. The view port change assessment stage gets invoked when the view port changes and notifies all watched nodes that entered or exited the view port. Finally the scene graph change assessment stage updates computations made at the initialization stage that have become invalid due to changes in the scene graph. For example if an ancestor node of the watched node changes location in the scene graph computations made at initialization may need to be recomputed.

Of these stages view port change assessment drives the rest of the node watcher algorithm. To delineate when a node enters and exits a view port the initialization step determines the bounding rectangle of the desired node and transforms it from its local coordinate system to the local coordinate system of the view port. In this way checking node entrance does not require a sequence of coordinate transformations at each view port change. Since the parents of the node may have transform matrices this initialization step requires traversing the scene graph from the node up to the camera. As described below if embedded cameras are used in the scene graph data structure then multiple bounding rectangles may be needed to accommodate the node appearing in multiple places.

Once the bounding rectangle for each watched node has been computed in the view port coordinate system the initialization stage adds the bounding rectangle to the view port change assessment data structures. The node watcher algorithm uses a basic building block for each dimension in the scene. In zoomable interfaces according to some exemplary embodiments this includes an x dimension a y dimension and a scale dimension. As described below however other exemplary implementations may have additional or different dimensions. The scale dimension describes the magnification level of the node in the view port and is described by the following equation 

When the view port changes the following processing occurs for each dimension. First the View Left Side and View Right Side pointers are checked to see if they need to be moved to include or exclude a Transition Block. Next if one or both of the pointers need to be moved they are slid over the Transition Block list to their new locations. Then for each Transition Block passed by the View Left Side and View Right Side pointers the node watcher algorithm executes the Transition Block notification code described below. This notification code determines if it is possible that its respective node may have entered or exited the view port. If so that node is added to a post processing list. Finally at the end of this processing for each dimension each node on the post processing list is checked that its view port status actually did change as opposed to changing and then changing back . If a change did occur then the algorithm sends an event to the component. Note that if the view port jumps quickly to a new area of the zoomable interface that the algorithm may detect more spurious entrance and exit events.

The Transition Block notification code can be implemented as a table lookup that determines whether the node moved into or out of the view port for the dimension being checked. An exemplary table is shown below.

When the output of the table indicates enter or exit the node watcher algorithm adds the node to the post processing list. The output columns of Table 1 are populated based on the following rules. If the node intersects in all dimensions then an enter notification will be sent in the post processing step. If the node was in the view and now one or more dimensions have stopped intersecting then an exit notification will be sent. To reduce the number of nodes in the post processing list the Transition Block notification code checks for intersection with other dimensions before adding the node to the list. This eliminates the post processing step when only one or two dimensions out of the total number of dimensions e.g. three or more intersect. When a user interface object e.g. an application wants to be notified of its view port status in the GUI it registers a function with the node watcher algorithm. When the application goes into or out of the view the node watcher algorithm calls that application s registered function with a parameter that indicates what happened. Alternatively notification can be performed using message passing. In this case each application has an event queue. The application tells the node watcher algorithm how to communicate with its event queue. For example it could specify the queue s address. Then when the node watcher detects a transition it creates a data structure that contains the cause of the notification and places it in the application s queue.

In addition to using node watcher notifications for application memory management this algorithm can also be used for other functions in zoomable GUIs according to the present invention. For example the node watcher algorithm can be used to change application behavior based on the user s view focus e.g. by switching the audio output focus to the currently viewed application. Another application for the node watcher algorithm is to load and unload higher resolution and composite images when the magnification level changes. This reduces the computational load on the graphics renderer by having it render fewer objects whose resolution more closely matches the display. In addition to having the node watcher algorithm watch a camera s view port it is also useful to have it watch the navigation code that tells the view port where it will end up after an animation. This provides earlier notification of components that are going to come into view and also enables zoomable GUIS according to exemplary embodiments of the present invention to avoid sending notifications to nodes that are flown over due to panning animations.

To better understand operation of the node watcher algorithm an example will now be described with reference to . depict a portion of a zoomable GUI at two different magnification levels. At the lower magnification level of three nodes are visible a circle a triangle and an ellipse. In the view has been zoomed in so much that the ellipse and circle are only partially visible and the triangle is entirely outside of the view. These nodes may for example represent applications or user interface components that depend on efficient event notification and therefore are tracked by the node watcher algorithm according to exemplary embodiments of the present invention. In this example the bounding rectangles for each node are explicitly illustrated in although those skilled in the art will appreciate that the bounding rectangles would not typically be displayed on the GUI. Each side of each of the bounding rectangles has been labeled in and these labels will be used to show the correspondence between the bounding rectangle sides and the transition block data structure which were described above.

Given these exemplary data structures and GUI scenes the associated processing within the node watcher algorithm while the zoom transition occurs can be described as follows. Starting with the left side of the view the node watcher algorithm moves the view left side pointer to the right until the transition block that is just outside of the view on the left side is reached. As shown in the view left side pointer first passes the Ctransition block. For this example assume that the circle node represents an application or other user interface object associated with the zoomable GUI that requires a notification when it is not fully visible in the view. Given these inputs to the node watcher algorithm Table 1 indicates that the circle node should receive an exit notification for the horizontal dimension. Of course the node watcher algorithm will typically aggregate notifications from all dimensions before notifying the node to avoid sending redundant exit notifications. Next the view left side pointer passes the left side of the triangle T. If the triangle node has requested notifications for when it completely leaves the view then the node watcher algorithm indicates per Table 1 that no notification is necessary. However when the view pointer passes T Table 1 indicates that the triangle has exited the view entirely and should be notified. The view pointer stops here since the right side of the circle s bounding rectangle C is still visible in the view.

From the right side the node watcher algorithm s processing is similar. The view right side pointer moves left to the ellipse s right side E. Depending on whether the ellipse has requested full or partial notifications the node watcher algorithm will or will not send a notification to the ellipse pursuant to Table 1. The vertical dimension can be processed in a similar manner using similar data structures and the top and bottom boundary rectangle values. Those skilled in the arts will also appreciate that a plurality of boundary rectangles can be used to approximate non rectangular nodes when more precise notification is required. Additionally the present invention contemplates that movement through other dimensions can be tracked and processed by the node watcher algorithm e.g. a third geometrical depth or scale dimension as well as non geometrical dimensions such as time content rating adult PG 13 etc. and content type drama comedy etc. . Depending on the number of dimensions in use the algorithm more accurately detects intersections of boundary segments rectangles and n dimensional hypercubes.

In addition to the node watcher algorithm described above exemplary embodiments of the present invention provide resolution consistent semantic zooming algorithms which can be used in zoomable GUIs according to exemplary embodiments of the present invention. Semantic zooming refers to adding removing or changing details of a component in a zoomable GUI depending on the magnification level of that component. For example in the movie browser interface described below when the user zooms close enough to the image of the movie it changes to show movie metadata and playback controls. The calculation of the magnification level is based on the number of pixels that the component uses on the display device. The zoomable GUI can store a threshold magnification level which indicates when the switch should occur e.g. from a view without the movie metadata and playback controls to a view with the movie metadata and playback controls.

Television and computer displays have widely varying display resolutions. Some monitors have such a high resolution that graphics and text that is readable on a low resolution display is so small to become completely unreadable. This also creates a problem for applications that use semantic zooming especially on high resolution displays such as HDTVs. In this environment semantic zooming code that renders based on the number of pixels displayed will change the image before the more detailed view is readable. Programmatically modifying the threshold at which semantic zooming changes component views can only work for one resolution.

The desirable result is that semantic zooming occurs consistently across all monitor resolutions. One solution is to use lower resolution display modes on high resolution monitors so that the resolution is identical on all displays. However the user of a high resolution monitor would prefer that graphics would be rendered at their best resolution if semantic zooming would still work as expected. Accordingly exemplary embodiments of the present invention provide a semantic zooming technique which supports displays of all different solutions without the previously stated semantic viewing issues. This can be accomplished by for example creating a virtual display inside of the scene graph. This is shown in by using an embedded virtual camera node and adding logic to compensate for the display resolution. The virtual camera node defines a view port whose size maps to the user s view distance and monitor size. For example a large virtual camera view port indicates that a user is either sitting close enough to the monitor or has a large enough monitor to resolve many details. Alternately a small view port indicates that the user is farther away from the monitor and requires larger fonts and image. The zoomable GUI code can base the semantic zooming transitions on the magnification level of components seen on this virtual camera and using the user s preferred viewing conditions.

The main camera node that is attached to the display device has its view port configured so that it displays everything that the virtual camera is showing. Since graphics images and text are not mapped to pixels until this main camera no loss of quality occurs from the virtual camera. The result of this is that high definition monitors display higher quality images and do not trigger semantic zooming changes that would make the display harder to read.

According to one exemplary embodiment of the present invention the process works as follows. Each camera and node in the scene graph has an associated transform matrix Tto T . These matrices transform that node s local coordinate system to that of the next node towards the display. In the figure Ttransforms coordinates from its view port to display coordinates. Likewise Ttransforms its local coordinate system to the camera s view port. If the leaf node needs to render something on the display it computes the following transform matrix This calculation can be performed while traversing the scene graph. Since the component changes to support semantic zooming are based on the virtual camera the following calculation is performed Typically Tto Tcan be determined ahead of time by querying the resolution of the monitor and inspecting the scene graph. Determining B from A is therefore accomplished by inverting these matrices and multiplying as follows For the case when calculating Tto Tahead of time is problematic e.g. if a graphics API hides additional transformations logic can be added to the virtual camera to intercept the transformation matrix that it would have used to render to the display. This intercepted transformation is then inverted and multiplied as above to compute the semantic zooming threshold.

One strength of zoomable interfaces according to exemplary embodiments of the present invention is the ability to maintain context while navigating the interface. All of the interface components appear to exist in the zoomable world and the user just needs to pan and zoom to reach any of them. The semantic zooming technique described above changes the appearance of a component depending on the zoom or magnification level. provide an example of semantic zooming for a component where the zoomed out version of the component is a picture and the zoomed in version includes the same picture as well as some controls and details. Some more detailed examples of this are provided below. One challenge associated with semantic zooming is that changes between views can occur abruptly and transition techniques such as alpha blending do not provide visually pleasing results when transitioning between two such views.

Accordingly exemplary embodiments of the present invention provide for some common image or text in all views of a component to provide a focal point for a transition effect when a semantic zoom is performed. For example in the common element is the picture. The transition effect between the zoomed out version and the zoomed in version can be triggered using for example the above described node watcher algorithm as follows. First a registration with the node watcher can be performed to receive an event when the main camera s view port transitions from the magnification level of the zoomed out version of the component to the zoomed in version. Then when the event occurs an animation can be displayed which shows the common element s shrinking and translating from their location in the zoomed out version to their location in the zoomed in version. Meanwhile the camera s view port continues to zoom into the component.

These capabilities of graphical user interfaces according to the present invention will become even more apparent upon review of another exemplary embodiment described below with respect to . Therein a startup GUI screen displays a plurality of organizing objects which operate as media group representations. The purely exemplary media group representations of home video movies TV sports radio music and news could of course include different more or fewer media group representations. Upon actuation of one of these icons by a user the GUI according to this exemplary embodiment will then display a plurality of images each grouped into a particular category or genre. For example if the movie icon in was actuated by a user the GUI screen of can then be displayed. Therein a large number e.g. 120 or more selection objects are displayed. These selection objects can be categorized into particular group s e.g. action classics comedy drama family and new releases. Those skilled in the art will appreciate that more or fewer categories could be provided. In this exemplary embodiment the media item images can be cover art associated with each movie selection. Although the size of the blocks in is too small to permit detailed illustration of this relatively large group of selection item images in implementation the level of magnification of the images is such that the identity of the movie can be discerned by its associated image even if some or all of the text may be too small to be easily read.

The cursor not shown in can then be disposed over a group of the movie images and the input device actuated to provide a selection indication for one of the groups. In this example the user selects the drama group and the graphical user interface then displays a zoomed version of the drama group of images as seen in . As with the previous embodiment a transition effect can also be displayed as the GUI shifts from the GUI screen of to the GUI screen of e.g. the GUI may pan the view from the center of the GUI screen of to the center of the drama group of images during or prior to the zoom. Note that although the zoomed version of the drama group of only displays a subset of the total number of images in the drama group that this zoomed version can alternatively contain all of the images in the selected group. The choice of whether or not to display all of the images in a selected group in any given zoomed in version of a GUI screen can be made based upon for example the number of media items in a group and a minimum desirable magnification level for a media item for a particular zoom level. This latter characteristic of GUIs according to the present invention can be predetermined by the system designer service provider or can be user customizable via software settings in the GUI. For example the number of media items in a group and the minimum and or maximum magnification levels can be configurable by either or both of the service provider or the end user. Such features enable those users with for example poor eyesight to increase the magnification level of media items being displayed. Conversely users with especially keen eyesight may decrease the level of magnification thereby increasing the number of media items displayed on a GUI screen at any one time and decrease browsing time.

One exemplary transition effect which can be employed in graphical user interfaces according to the present invention is referred to herein as the shoe to detail view effect. When actuated this transition effect takes a zoomed out image and simultaneously shrinks and translates the zoomed out image into a smaller view i.e. the next higher level of magnification. The transition from the magnification level used in the GUI screen of to the greater magnification level used in the GUI screen of results in additional details being revealed by the GUI for the images which are displayed in the zoomed in version of . The GUI selectively reveals or hides details at each zoom level based upon whether or not those details would display well at the currently selected zoom level. Unlike a camera zoom which attempts to resolve details regardless of their visibility to the unaided eye exemplary embodiments of the present invention provide for a configurable zoom level parameter that specifies a transition point between when to show the full image and when to show a version of the image with details that are withheld. The transition point can be based upon an internal resolution independent depiction of the image rather the resolution of TV Monitor . In this way GUIs according to the present invention are consistent regardless of the resolution of the display device being used in the media system.

In this exemplary embodiment an additional amount of magnification for a particular image can be provided by passing the cursor over a particular image. This feature can be seen in wherein the cursor has rolled over the image for the movie Apollo 13 . Although not depicted in such additional magnification could for example make more legible the quote Houston we have a problem which appears on the cover art of the associated media item as compared to the corresponding image in the GUI screen of which is at a lower level of magnification. User selection of this image e.g. by depressing a button on the input device can result in a further zoom to display the details shown in . This provides yet another example of semantic zooming as it was previously described since various information and control elements are present in the GUI screen of that were not available in the GUI screen of . For example information about the movie Apollo 13 including among other things the movie s runtime price and actor information is shown. Those skilled in the art will appreciate that other types of information could be provided here. Additionally this GUI screen includes GUI control objects including for example button control objects for buying the movie watching a trailer or returning to the previous GUI screen which could also be accomplished by depressing the ZOOM OUT button on the input device . Hyperlinks can also be used to allow the user to jump to for example GUI screens associated with the related movies identified in the lower right hand corner of the GUI screen of or information associated with the actors in this movie. In this example some or all of the film titles under the heading Filmography can be implemented as hyperlinks which when actuated by the user via the input device will cause the GUI to display a GUI screen corresponding to that of for the indicated movie.

A transition effect can also be employed when a user actuates a hyperlink. Since the hyperlinks may be generated at very high magnification levels simply jumping to the linked media item may cause the user to lose track of where he or she is in the media item selection map . Accordingly exemplary embodiments of the present invention provide a transition effect to aid in maintaining the user s sense of geographic position when a hyperlink is actuated. One exemplary transition effect which can be employed for this purpose is a hop transition. In an initial phase of the transition effect the GUI zooms out and pans in the direction of the item pointed to by the hyperlink. Zooming out and panning continues until both the destination image and the origination image are viewable by the user. Using the example of once again if the user selects the hyperlink for Saving Private Ryan then the first phase of the hyperlink hop effect would include zooming out and panning toward the image of Saving Private Ryan until both the image for Saving Private Ryan and Apollo 13 were visible to the user. At this point the transition effect has provided the user with the visual impression of being moved upwardly in an arc toward the destination image. Once the destination image is in view the second phase of the transition effect gives the user the visual impression of zooming in and panning to e.g. on the other half of the arc the destination image. The hop time i.e. the amount of time both phases one and two of this transition effect are displayed can be fixed as between any two hyperlinked image items. Alternatively the hop time may vary e.g. based on the distance traveled over the GUI. For example the hop time can be parameterized as HopTime A log zoomed in scale level hop apex scale level B distance between hyperlinked media items C where A B and C are suitably selected constant values.

The node watcher algorithm described above with respect to can also be used to aid in the transition between the zoom level depicted in the exemplary GUI screen of and the exemplary GUI screen of . The rendering of GUI screens containing text and or control elements which are not visible in other zoom level versions of the selected image may be more computationally and or memory intensive than the images at lower magnification levels. Accordingly the node watcher algorithm can be used in exemplary embodiments of the present invention to aid in pre loading of GUI screens such as that shown in by watching the navigation code of the GUI to more rapidly identify the particular media item being zoomed in on.

Included in exemplary implementations of the present invention are screen location and semantically based navigation controls. These control regions appear when the user positions the cursor near or in a region associated with those controls on a screen where those controls are appropriate as shown in . For example when playing a movie the so called trick functions of Fast Forward Rewind Pause Stop and so on are semantically appropriate. In this exemplary embodiment the screen region assigned to those functions is the lower right corner and when the cursor is positioned near or in that region the set of icons for those trick functions appear. These icons then disappear when the function engaged is clearly completed or when the cursor is repositioned elsewhere on the screen. The same techniques can also be used to cover other navigational features like text search and home screen selection. In this exemplary implementation these controls are semantically relevant on all screens and the region assigned to them is the upper right corner. When the cursor is positioned near or in that region the set of icons for those navigational controls appear. These icons then disappear when the function is activated or the cursor is repositioned elsewhere on the screen. Note that for user training purposes the relevant control icons may initially optionally appear briefly e.g. 5 seconds on some or all of the relevant screens in order to alert the inexperienced user to their presence.

Having provided some examples of zoomable graphical user interfaces according to the present invention exemplary frameworks and infrastructures for using such interfaces will now be described. provides a framework diagram wherein zoomable interfaces associated with various high level applications e.g. movies television music radio and sports are supported by primitives referred to in the Figure as atoms . In this exemplary embodiment primitives include POINT CLICK ZOOM HOVER and SCROLL although those skilled in the art will appreciate that other primitives may be included in this group as well e.g. PAN and DRAG. As described above the POINT and CLICK primitives operate to determine cursor location and trigger an event when for example a user actuates the ZOOM IN or ZOOM OUT button on the handheld input device. These primitives simplify navigation and remove the need for repeated up down right left button actions. As illustrated above the ZOOM primitive provides an overview of possible selections and gives the user context when narrowing his or her choices. This concept enables the interface to scale to large numbers of media selections and arbitrary display sizes. The SCROLL primitive handles input from the scroll wheel input device on the exemplary handheld input device and can be used to for example accelerates linear menu navigation. The HOVER primitive dynamically enlarges the selections underneath the pointer and or changes the content of the selection to enable the user to browse potential choices without committing. Each of the aforedescribed primitive operations can be actuated in GUIs according to the present invention in a number of different ways. For example each of POINT CLICK HOVER SCROLL and ZOOM can be associated with a different gesture which can be performed by a user. This gesture can be communicated to the system via the input device whether it be a 3D pointer trackball touchpad etc. and translated into an actuation of the appropriate primitive. Likewise each of the primitives can be associated with a respective voice command.

Between the lower level primitives and the upper level applications reside various software and hardware infrastructures which are involved in generating the images associated with zoomable GUIs according to the present invention. As seen in such infrastructures can include a handheld input device pointer application program interfaces APIs zoomable GUI screens developers tools etc.

The foregoing exemplary embodiments are purely illustrative in nature. The number of zoom levels as well as the particular information and controls provided to the user at each level may be varied. Those skilled in the art will appreciate that the present invention provides revolutionary techniques for presenting large and small sets of media items using a zoomable interface such that a user can easily search through browse organize and play back media items such as movies and music. Graphical user interfaces according to the present invention organize media item selections on a virtual surface such that similar selections are grouped together. Initially the interface presents a zoomed out view of the surface and in most cases the actual selections will not be visible at this level but rather only their group names. As the user zooms progressively inward more details are revealed concerning the media item groups or selections. At each zoom level different controls are available so that the user can play groups of selections individual selections or go to another part of the virtual surface to browse other related media items. Zooming graphical user interfaces according to exemplary embodiments of the present invention can contain categories of images nested to an arbitrary depth as well as categories of categories. The media items can include content which is stored locally broadcast by a broadcast provider received via a direct connection from a content provider or on a peering basis. The media items can be provided in a scheduling format wherein date time information is provided at some level of the GUI. Additionally frameworks and GUIs according to exemplary embodiments of the present invention can also be applied to television commerce wherein the items for selection are being sold to the user.

The conventional television application assumes that the user has to consciously and deliberately select from a set of relevant screen regions or objects in order to make a selection. Typically this is done with up down left right arrows with the screen highlighting the region of interest. When the user gets there she or he presses e.g. the OK button to proceed. By way of contrast exemplary embodiments of the present invention allow the user to directly point to whatever on the screen is of interest. This makes selection much more intuitive and quick. It also permits enhanced information to be displayed as part of pointing something referred to herein as semantic hover . One example of semantic hover in exemplary embodiments of the present invention is that hovering over a program title in the TV application causes extended information about that program to be displayed. Additionally it permits actions to be relevant to the region of interest. The user can simply point at a region of the screen that may or may not contain a specific object of interest and engage an action that has no relevance anywhere else on the screen. One example of this in applications according to the present invention is that moving the cursor to the left part of the screen engages a channel selection feature. The user can then use a scroll wheel to change channels or just point at the slider bar to set the corresponding channel. None of this is easily accomplished with a conventional navigation scheme.

Another unique feature associated with integrating a 3D device with an on screen navigation system according to exemplary embodiments of the present invention is the ability to cope with both tremor and unwanted movement. This is very different than the situation with a desktop mouse and a PC screen. In that case the mouse is resting on a flat surface and the unwanted movement is naturally filtered out thanks to friction and the ability to rest. However with a 3D device it is held in the user s hand at arbitrary angles resulting in a very different control environment. Either the algorithm associated with processing the motion data of the 3D device or the application which receives that data or both have to deal with tremor and unwanted movement. Another aspect of such combinations according to the present invention is precision. In general the pointing resolution of a desktop mouse is superior to that of a 3D device. This means that the desktop mouse can point at smaller screen objects than the 3D device can.

Systems according to exemplary embodiments of the present invention do all three of these things. In the algorithm itself tremor and desired motion are modeled and processed so that only the desired motion is used. Additionally movement of the 3D device that is generated solely because the user is depressing a button or rolling a scroll wheel of the 3D pointing device is modeled and compensated for. Additionally in the downstream application which receives the processed movement data associated with the user s movement of the 3D pointing device knowledge of 3D resolution and movement characteristics are included in the design of the application screens which also provide or semantic feedback to make the system easier to control.

The advantages of 3D pointing in combination with on screen navigation also include new features and functions that are possible in various applications because of this pairing. Some examples are listed below categorized by their different applications. Note that all of the GUI screens illustrated in can be navigated to using the afore described techniques.

The user is allowed to just point at the screen to indicate what is desired. The gesture itself can optionally be considered as part of the applications input. By adjusting the application both to take account of e.g. tremor unintended movement screen resolutions and take advantage of e.g. semantic hover zoom screen image relevant controls the 3D navigation a truly powerful interactive TV application is possible. An additional discussion of exemplary techniques for handling unintended movement of a 3D pointing device is found in U.S. patent application Ser. No. 11 119 987 entitled Methods and Devices for Removing Unintentional Movement in 3D Pointing Devices filed on May 2 2005 the disclosure of which is incorporated here by reference.

The above described exemplary embodiments are intended to be illustrative in all respects rather than restrictive of the present invention. Thus the present invention is capable of many variations in detailed implementation that can be derived from the description contained herein by a person skilled in the art. All such variations and modifications are considered to be within the scope and spirit of the present invention as defined by the following claims. No element act or instruction used in the description of the present application should be construed as critical or essential to the invention unless explicitly described as such. Also as used herein the article a is intended to include one or more items.

