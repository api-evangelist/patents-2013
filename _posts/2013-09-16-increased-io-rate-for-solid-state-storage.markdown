---

title: Increased I/O rate for solid state storage
abstract: The storage device receives a write request from a disk controller to write data to a storage array. The storage device determines that one or more blocks are marked for deletion. In response to receiving the write request and determining that blocks are marked for deletion, the storage device issues a write command on a first media access channel for a first location of the storage array, and issues an erase command on a second media access channel for a different storage location of the storage array. Thus, the commands are issued concurrently on different channels.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09111598&OS=09111598&RS=09111598
owner: NetApp, Inc.
number: 09111598
owner_city: Sunnyvale
owner_country: US
publication_date: 20130916
---
Embodiments described are related generally to storage device access and embodiments described are more particularly related to a storage device providing concurrent write and erase commands in response to a write request.

Portions of the disclosure of this patent document can contain material that is subject to copyright protection. The copyright owner has no objection to the reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever. The copyright notice applies to all data as described below and in the accompanying drawings hereto as well as to any software described below. Copyright 2013 NetApp Inc. All Rights Reserved.

The longer a storage device is used the amount of free unwritten space decreases. The decrease of free unwritten space is especially true in storage devices used in data centers or other network accessed shared storage architectures. When the storage device is a solid state drive SSD the decrease in free unwritten space causes the performance of the device to decrease. The performance decrease is accounted for because the SSD must erase the space necessary for the new data prior to writing the data. The extra delay in writing the data reduces throughput performance.

Traditionally a write to an SSD or other storage device that requires erasing prior to writing includes a host system sending data to a disk controller or comparable controller device. The disk controller separates the data into blocks for storage on the storage devices and issues a write command to the storage device itself. The storage device executes local firmware to determine how to store the data on the physical medium of the storage device. Traditional firmware includes the following conditions 

If the data is not edited data and there are adequate free pages on the storage device the local controller writes the data directly to the media 

If the data is not edited data and there are not adequate free pages on the storage device the local controller obtains address es of block s marked for deletion and erases the block s . Only after the completing the erase function the local controller writes the new data to the newly erased pages 

If the data is edited data and there are adequate free pages on the storage device the local controller reads the original data modifies the original data with the new data and writes the modified data to the free pages on the media. The local controller then marks the old pages for deletion and

If the data is edited data and there are not adequate free pages on the storage device the local controller reads the original data and modifies the original data with the new data. The local controller obtains address es of block s marked for deletion and erases the block s . Only after completing the erase function the local controller writes the modified data to the newly erased pages. The local controller then marks the old pages for deletion.

The delay created by the requirement to erase pages prior to performing a write when there are not adequate free pages can become a significant performance bottleneck in a data storage subsystem having a heavy access load such as a data center.

Descriptions of certain details and embodiments follow including a description of the figures which can depict some or all of the embodiments described below as well as discussing other potential embodiments or implementations of the technology presented herein.

A storage device generates parallel or concurrent write and erase commands in response to a received write request. The storage device can write the data in response to the request and erase blocks marked for deletion to free storage space for subsequent write requests. The erase operation does not need to immediately precede the write operation. The concurrent erasure of blocks marked for deletion increases the likelihood that a storage device will have free storage space available whenever a write request is received.

The storage device receives a write request from a disk controller or comparable controller device to write data to a storage array. The storage device determines that one or more blocks are marked for deletion. In response to receiving the write request and determining that blocks are marked for deletion the storage device issues a write command on a first media access channel for a first location of the storage array where the first location is either unwritten space or erased space. The storage device also issues an erase command on a second media access channel for a different storage location of the storage array to erase the block s marked for deletion. Thus the commands are issued concurrently on different channels.

It will be understood that concurrent commands or parallel commands refers to commands that do not require completion of each other prior to being executed. In traditional systems where a block is required to be erased prior to being written the local controller is required to suspend a write command until completion of an erase command of the address space on which the write command will execute. As described in more detail below a local controller does not have to wait for completion of either the write or erase command prior to executing the other command. It will be understood that a write command refers to a command by a local controller to commit data to the physical media a storage array of the storage device to which the local controller belongs. An erase command refers to a command by a local controller to reset a state of a location e.g. block or blocks of the physical media. For example an erase command can include setting all bits of a particular address range to zeros. The commands are generated within the local controller in response to a request or command from external to the storage device by an external controller device e.g. a disk controller .

The use of concurrent commands allows a storage device to avoid delay in a situation that would otherwise incur delay to erase a block prior to writing new data to the previously written blocks. Writing new data to a previously written block can be referred to as overwriting the block which can include dual operations of first clearing the block of previous data and then writing the new data to it. A common example of a storage device that requires erasing prior to overwriting is a solid state storage device or solid state drive SSD . A host system such as a user computer device a server device a storage server or other host system includes a device controller e.g. a disk controller or comparable controller device to issue access requests to a storage device . The device controller sends a write request to the storage device which determines if one or more blocks are marked for deletion and issues a write command in response to the write request. The storage device also issues an erase request as a concurrent command to erase a location already marked for deletion which is not a location to which the write command will write data. Thus the two command operate on different blocks which allows the write command to proceed without having to wait for completion of the erase command.

In system clients access the storage system including storage server and associated storage devices over network . Network can include any internetworking technology to remotely access storage. Network can include any combination of private and public network. In one embodiment network includes a fibre channel SAN storage area network . In one embodiment network includes an iSCSI small computer system interface over internet network. In an alternative embodiment clients can be executed as different processes e.g. different applications on a host computer device which host device can be directly attached directly to storage devices e.g. a JBOD or just a bunch of drives implementation .

Storage server represents a device that interfaces via network to process and requests to a storage subsystem. The storage subsystem includes multiple storage devices . In one embodiment storage subsystem includes multiple SSDs . It will be understood that SSDs could be referred to as storage devices of the storage subsystem. SSDs are separately identified in system as one example of a storage device that requires erasing previously written blocks prior to writing new data to the previously written blocks.

As the storage devices of the storage subsystem are used longer the amount of free space decreases until the storage device eventually performs writes by freeing up storage space prior to writing. As mentioned above traditionally the storage device performs the erasing or freeing up of storage space immediately prior to performing the write. However the erase operation decreases the performance of the storage device with respect to throughput or data input output I O rates. In contrast in system SSDs perform write and erase operations concurrently on separate access channels to different locations of the physical media in response to a write request. Thus a write request results in the desired writing of data and can also free up space for a subsequent write.

In one embodiment storage server includes a driver that divides data into 512 KB chunks for writing to the storage subsystem. In one embodiment SSDs erase 256 KB blocks as soon as a complete block is marked for deletion and will erase 512 KB blocks when available as soon as they are marked for deletion. In one embodiment each SSD includes a local controller that determines when blocks are marked for deletion. Thus the SSD marks blocks for deletion as soon as they are ready for deletion. Additionally the local controller can issue concurrent commands to cause a write on one channel of an SSD while issuing an erase on a different channel of the SSD . It will be understood that such concurrent operation can be performed independently by the storage devices themselves without request or command by an external device controller.

User device includes application app which represents an end user application one with a user interface or a system level application that generates a data access request. A data access request can include a write request. Application generates a write request to store new data to the storage devices and or to modify data already stored on a storage device. In one embodiment user device sends a data access request to storage server for access to storage device or drive . In one embodiment user device sends the data access request to storage server over a network connection.

Storage server includes operating system OS . OS represents control software to manage the storage server and its operations. OS includes filesystem which manage how data is stored in the connected storage devices. Filesystem can be a block based filesystem which stores and manages data in blocks which are not necessarily contiguously stored within storage device . In one embodiment storage server includes HBA host bus adapter which allows storage server to interface with storage device . Storage server includes a hardware interface to drive and a driver to operate the hardware interface. HBA can represent the driver used to interface with storage device . HBA can separate a write operation generated by application into multiple packets of operations for access to storage device . In one embodiment HBA provides the packets of data to device controller .

Storage server includes device controller which represents a disk controller or comparable device. Device controller is an external controller from the perspective of storage device . Device controller generates data access requests to storage device . Device controller can include driver components and physical interface components to send data access requests to storage device . Device controller includes hardware interface components to connect to storage device and storage device includes hardware interface components to connect to device controller .

Storage device includes controller which is a local controller or control logic local to the storage device. Controller performs operations in response to receiving requests from device controller . More particularly local controller issues commands within the storage device to provide the requested access for application . For a write request controller issues one or more commands necessary to execute the write operation as well as issuing a command to perform an erase operation on and blocks currently marked for deletion. The one or more commands to execute the write operation can depend on a state of the storage within storage device . For example a single command may be sufficient to execute a write of new data when free space is available whereas multiple commands may be required for a modification of existing data e.g. read the data modify the data and then write the data .

In one embodiment storage device includes write buffer where controller can store data for a write request. Write buffer can be or include any type of storage device register or buffer used to temporarily cache data. Thus data received from device controller in one or more write requests can be stored in write buffer until stored on the physical media of storage device . Storage array represents the physical media of storage device . Typically the storage space in an SSD is arranged in rows and columns and thus is a storage array. Those of skill in the art are familiar with the structure and accessing of the physical media and details will not be provided herein. Storage device includes multiple channels to access storage array as shown by N channels CHO through CH N 1 . Each of the N channels is a storage access channel or a channel over which local controller can access storage array .

In operation application generates and or modifies data. The generation or modification of data triggers filesystem to generate a write request which it forwards to HBA . HBA processes the data associated with the request into packets of data to transmit to storage device . HBA forwards the data packets to device controller . Device controller transports the data to storage device . In one embodiment device controller transports the data as SCSI write commands. In one embodiment storage device via controller breaks down a single write command received from device controller into multiple blocks for delivery to the physical media storage array . The multiple blocks can be for example 512 KB byte blocks delivered to the media.

In one embodiment storage device stores data arranged in blocks of 4 KB pages. In one embodiment storage device uses 64 pages per block making the size of the block 256 KB. In one embodiment storage device uses 128 pages per block making the size of the block 512 KB. Storage device writes the data to storage array using free pages. The free pages can be pages that are unwritten until there are no more unwritten pages after which storage device writes the data to erased pages. If data that is already stored to storage device is later edited controller first reads the data from storage array modifies the data and rewrites the data to a different location of the physical media. The new location is typically mapped to the same logical address e.g. logical block address LBA as the original location from which the data was read. Controller also generates a concurrent erase of blocks marked for deletion on a different channel. Thus the erase is performed as a separate process asynchronously with respect to the write. The controller further marks the blocks at the original location for deletion.

In one embodiment the number N of data access channels is two. In such an implementation a storage device would issue a write command on one channel and issue an erase command on the other channel. In one embodiment the number N of data access channels is a multiple of two higher than two. In such an implementation in one embodiment the local controller can issue pairs of write and erase commands in parallel on different pairs of data access channels.

The operating system transfers the data to a host bus adapter HBA or comparable host interface block . The HBA driver places the data in I O packets for transmission to a device controller e.g. disk controller block . The device controller includes a driver that further processes the data for storage by the storage device. The device controller breaks the data packets up into blocks for storage on the storage device s block . In one embodiment the device controller prepares the data blocks for a RAID implementation as appropriate block . The device controller then sends the data blocks to the storage device s block .

The storage device s receive the data from the device controller and internally perform concurrent write and erase operations in response to receiving the data block . The internal nature of the concurrent write and erase operations refers to the fact that the storage device s manage the concurrent operations internally and the concurrent commands are not managed by the device controller. For example the storage device includes an internal controller that can generate a data write operation on a first channel e.g. channel A and an erase operation on a second channel e.g. channel B . The operations are discussed below in more detail with respect to respectively.

A local controller of the storage device determines when a write operation is required and in response to the need for a write operation initiates process of to execute a concurrent write on a first channel. The local controller also checks for blocks marked for deletion in response to receiving a write request and initiates process of to execute a concurrent erase on a second channel. It will be understood that the local controller issues the write command to a first location of the storage media and issues the erase command to a second location of the storage media. Thus the storage device writes to one location and erases a different location in parallel.

If there are adequate free pages on the storage device to fulfill the write request block YES branch the local controller determines if the write request is for a new data or if it is an edit of existing data block . The data edit will identify the storage location of the data to be modified. If the write operation is not for a data edit block NO branch the local controller can obtain the address es of available blocks block . The available blocks are blocks that are either unwritten or erased and therefore reset to an unwritten state. The local controller issues a command to write the data to selected available blocks block . The local controller can return a command complete signal to the device controller after completing the write operation block .

If the write operation is for a data edit block YES branch the local controller reads the existing data corresponding to the data to modify from a specified address for the data to modify block . The local controller modifies the existing data with the new data block and writes the modified data to new page s on the storage media block . The local controller marks the pages that contain the original data for deletion. The local controller can send a command complete signal to the device controller after completing the write operation block .

If there are blocks marked for deletion block YES branch the local controller issues an erase command to the block or blocks marked for deletion block . It will be understood that a block will be marked for deletion from a previous operation that deleted and or moved data. A delete operation marks a block for deletion. A modification of data includes modifying the data and performing a delete operation on the original data location.

If the erase operation does not complete successfully block NO branch the local controller can send an error condition to the device controller block . If the erase operation completes successfully block YES branch the local controller can send a command complete signal to the device controller block . The local controller then terminates the block deletion operation block .

Storage of data in storage units is managed by storage servers which receive and respond to various read and write requests from clients directed to data stored in or to be stored in storage units . Storage units constitute mass storage devices which can include for example flash memory magnetic or optical disks or tape drives illustrated as disks disk A B . Storage devices can further be organized into arrays not illustrated implementing a Redundant Array of Inexpensive Disks Devices RAID scheme whereby storage servers access storage units using one or more RAID protocols known in the art.

Storage servers can provide file level service such as used in a network attached storage NAS environment block level service such as used in a storage area network SAN environment a service which is capable of providing both file level and block level service or any other service capable of providing other data access services. Although storage servers are each illustrated as single units in a storage server can in other embodiments constitute a separate network element or module an N module and disk element or module a D module . In one embodiment the D module includes storage access components for servicing client requests. In contrast the N module includes functionality that enables client access to storage access components e.g. the D module and the N module can include protocol components such as Common Internet File System CIFS Network File System NFS or an Internet Protocol IP module for facilitating such connectivity. Details of a distributed architecture environment involving D modules and N modules are described further below with respect to and embodiments of a D module and an N module are described further below with respect to .

In one embodiment storage servers are referred to as network storage subsystems. A network storage subsystem provides networked storage services for a specific application or purpose and can be implemented with a collection of networked resources provided across multiple storage servers and or storage units.

In the embodiment of one of the storage servers e.g. storage server A functions as a primary provider of data storage services to client . Data storage requests from client are serviced using disks A organized as one or more storage objects. A secondary storage server e.g. storage server B takes a standby role in a mirror relationship with the primary storage server replicating storage objects from the primary storage server to storage objects organized on disks of the secondary storage server e.g. disks B . In operation the secondary storage server does not service requests from client until data in the primary storage object becomes inaccessible such as in a disaster with the primary storage server such event considered a failure at the primary storage server. Upon a failure at the primary storage server requests from client intended for the primary storage object are serviced using replicated data i.e. the secondary storage object at the secondary storage server.

It will be appreciated that in other embodiments network storage system can include more than two storage servers. In these cases protection relationships can be operative between various storage servers in system such that one or more primary storage objects from storage server A can be replicated to a storage server other than storage server B not shown in this figure . Secondary storage objects can further implement protection relationships with other storage objects such that the secondary storage objects are replicated e.g. to tertiary storage objects to protect against failures with secondary storage objects. Accordingly the description of a single tier protection relationship between primary and secondary storage objects of storage servers should be taken as illustrative only.

In one embodiment storage devices include respective local controllers controller A B . The local controller receives a write request and determines if an erase command can be performed concurrently with a write command to service the write request. If the erase and write can be performed concurrently controller generates a write command to service or fulfill the write request on one media access channel and generates an erase command to erase blocks marked for deletion on another media access channel. The write operation and the erase operation are directed to different locations on the physical media.

Nodes can be operative as multiple functional components that cooperate to provide a distributed architecture of system . To that end each node can be organized as a network element or module N module A B a disk element or module D module A B and a management element or module M host A B . In one embodiment each module includes a processor and memory for carrying out respective module operations. For example N module can include functionality that enables node to connect to client via network and can include protocol components such as a media access layer Internet Protocol IP layer Transport Control Protocol TCP layer User Datagram Protocol UDP layer and other protocols known in the art.

In contrast D module can connect to one or more storage devices via cluster switching fabric and can be operative to service access requests on devices . In one embodiment the D module includes storage access components such as a storage abstraction layer supporting multi protocol data access e.g. Common Internet File System protocol the Network File System protocol and the Hypertext Transfer Protocol a storage layer implementing storage protocols e.g. RAID protocol and a driver layer implementing storage device protocols e.g. Small Computer Systems Interface protocol for carrying out operations in support of storage access operations. In the embodiment shown in a storage abstraction layer e.g. file system of the D module divides the physical storage of devices into storage objects. Requests received by node e.g. via N module can thus include storage object identifiers to indicate a storage object on which to carry out the request.

Also operative in node is M host which provides cluster services for node by performing operations in support of a distributed storage system image for instance across system . M host provides cluster services by managing a data structure such as a relational database RDB RDB A B which contains information used by N module to determine which D module owns services each storage object. The various instances of RDB across respective nodes can be updated regularly by M host using conventional protocols operative between each of the M hosts e.g. across network to bring them into synchronization with each other. A client request received by N module can then be routed to the appropriate D module for servicing to provide a distributed storage system image.

Similar to what is described above storage devices and or of system include respective local controllers controller A B . Typically each storage device includes a separate local controller although controller B is shown within storage B which includes multiple disks B. The local controller receives a write request and determines if an erase command can be performed concurrently with a write command to service the write request. If the erase and write can be performed concurrently controller generates a write command to service or fulfill the write request on one media access channel and generates an erase command to erase blocks marked for deletion on another media access channel. The write operation and the erase operation are directed to different locations on the physical media.

It will be noted that while shows an equal number of N and D modules constituting a node in the illustrative system there can be different number of N and D modules constituting a node in accordance with various embodiments. For example there can be a number of N modules and D modules of node A that does not reflect a one to one correspondence between the N and D modules of node B. As such the description of a node comprising one N module and one D module for each node should be taken as illustrative only.

Memory includes storage locations addressable by processor network adapter and storage adapter for storing processor executable instructions and data structures associated with a multi tiered cache with a virtual storage appliance. A storage operating system portions of which are typically resident in memory and executed by processor functionally organizes the storage server by invoking operations in support of the storage services provided by the storage server. It will be apparent to those skilled in the art that other processing means can be used for executing instructions and other memory means including various computer readable media can be used for storing program instructions pertaining to the technology described herein. It will also be apparent that some or all of the functionality of the processor and executable software can be implemented by hardware such as integrated currents configured as programmable logic arrays ASICs and the like.

Network adapter comprises one or more ports to couple the storage server to one or more clients over point to point links or a network. Thus network adapter includes the mechanical electrical and signaling circuitry needed to couple the storage server to one or more client over a network. Each client can communicate with the storage server over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

Storage adapter includes a plurality of ports having input output I O interface circuitry to couple the storage devices e.g. disks to bus over an I O interconnect arrangement such as a conventional high performance FC or SAS Serial Attached SCSI Small Computer System Interface link topology. Storage adapter typically includes a device controller not illustrated comprising a processor and a memory for controlling the overall operation of the storage units in accordance with read and write commands received from storage operating system . As used herein data written by a device controller in response to a write command is referred to as write data whereas data read by device controller responsive to a read command is referred to as read data. 

User console enables an administrator to interface with the storage server to invoke operations and provide inputs to the storage server using a command line interface CLI or a graphical user interface GUI . In one embodiment user console is implemented using a monitor and keyboard.

Computing device includes storage adapter to interface with storage devices that include respective local controllers such as controllers . The local controller receives a write request and determines if an erase command can be performed concurrently with a write command to service the write request. If the erase and write can be performed concurrently the local controller generates a write command to service or fulfill the write request on one media access channel and generates an erase command to erase blocks marked for deletion on another media access channel. The write operation and the erase operation are directed to different locations on the physical media.

When implemented as a node of a cluster such as cluster of the storage server further includes a cluster access adapter shown in phantom having one or more ports to couple the node to other nodes in a cluster. In one embodiment Ethernet is used as the clustering protocol and interconnect media although it will be apparent to one of skill in the art that other types of protocols and interconnects can by utilized within the cluster architecture.

Multi protocol engine includes a media access layer of network drivers e.g. gigabit Ethernet drivers that interface with network protocol layers such as the IP layer and its supporting transport mechanisms the TCP layer and the User Datagram Protocol UDP layer . The different instances of access layer IP layer and TCP layer are associated with two different protocol paths or stacks. A file system protocol layer provides multi protocol file access and to that end includes support for the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A VI virtual interface layer implements the VI architecture to provide direct access transport DAT capabilities such as RDMA as required by the DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the storage server. In certain cases a Fibre Channel over Ethernet FCoE layer not shown can also be operative in multi protocol engine to receive and transmit requests and responses to and from the storage server. The FC and iSCSI drivers provide respective FC and iSCSI specific access control to the blocks and thus manage exports of luns logical unit numbers to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing blocks on the storage server.

The storage operating system also includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on storage devices. Information can include data received from a client in addition to data accessed by the storage operating system in support of storage server operations such as program application data or other system data. Preferably client data can be organized as one or more logical storage objects e.g. volumes that comprise a collection of storage devices cooperating to define an overall logical arrangement. In one embodiment the logical arrangement can involve logical volume block number vbn spaces wherein each volume is associated with a unique vbn.

File system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustrated as a SCSI target module . SCSI target module is generally disposed between drivers and file system to provide a translation layer between the block lun space and the file system space where luns are represented as blocks. In one embodiment file system implements a WAFL write anywhere file layout file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using a data structure such as index nodes or indirection nodes inodes to identify files and file attributes such as creation time access permissions size and block location . File system uses files to store metadata describing the layout of its file system including an inode file which directly or indirectly references points to the underlying data blocks of a file.

Operationally a request from a client is forwarded as a packet over the network and onto the storage server where it is received at a network adapter. A network driver such as layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to file system . There file system generates operations to load retrieve the requested data from the disks if it is not resident in core i.e. in memory . If the information is not in memory file system accesses the inode file to retrieve a logical vbn and passes a message structure including the logical vbn to the RAID system . There the logical vbn is mapped to a disk identifier and device block number disk dbn and sent to an appropriate driver of disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the storage server. Upon completion of the request the node and operating system returns a reply to the client over the network.

It will be understood that the software path through the storage operating system layers described above can alternatively be implemented in hardware. Thus any path needed to perform data storage access for a client request received at the storage server can be implemented in hardware and or software. A storage access request data path can be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . Such a hardware embodiment increases the performance of the storage service provided by the storage server in response to a request issued by a client. Moreover in another alternate embodiment the processing elements of adapters can be configured to offload some or all of the packet processing and storage access operations respectively from processor to increase the performance of the storage service provided by the storage server. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware and or software.

When implemented in a cluster data access components of the storage operating system can be embodied as D module for accessing data stored on disk. In contrast multi protocol engine can be embodied as N module to perform protocol termination with respect to a client issuing incoming access over the network as well as to redirect the access requests to any other N module in the cluster. A cluster services system can further implement an M host e.g. M host to provide cluster services for generating information sharing operations to present a distributed file system image for the cluster. For instance media access layer can send and receive information packets between the various cluster services systems of the nodes to synchronize the replicated databases in each of the nodes.

In addition a cluster fabric CF interface module CF interface modules A B can facilitate intra cluster communication between N module and D module using a CF protocol . For instance D module can expose a CF application programming interface API to which N module or another D module not shown issues calls. To that end CF interface module can be organized as a CF encoder decoder using local procedure calls LPCs and remote procedure calls RPCs to communicate a file system command between D modules residing on the same node and remote nodes respectively.

In one embodiment disk drive system includes disk controller which is shown separately for simplicity in description. Disk controller interfaces operating system with one or more storage devices. The storage devices each include a local controller . The local controller receives a write request and determines if an erase command can be performed concurrently with a write command to service the write request. If the erase and write can be performed concurrently controller generates a write command to service or fulfill the write request on one media access channel and generates an erase command to erase blocks marked for deletion on another media access channel. The write operation and the erase operation are directed to different locations on the physical media.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and can implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

As used herein instantiation refers to creating an instance or a copy of a source object or source code. The source code can be a class model or template and the instance is a copy that includes at least some overlap of a set of attributes which can have different configuration or settings than the source. Additionally modification of an instance can occur independent of modification of the source.

Flow diagrams as illustrated herein provide examples of sequences of various process actions. Although shown in a particular sequence or order unless otherwise specified the order of the actions can be modified. Thus the illustrated embodiments should be understood only as an example and the process can be performed in a different order and some actions can be performed in parallel. Additionally one or more actions can be omitted in various embodiments thus not all actions are required in every embodiment. Other process flows are possible.

Various operations or functions are described herein which can be described or defined as software code instructions configuration and or data. The content can be directly executable object or executable form source code or difference code delta or patch code . The software content of the embodiments described herein can be provided via an article of manufacture with the content stored thereon or via a method of operating a communications interface to send data via the communications interface. A machine readable medium or computer readable medium can cause a machine to perform the functions or operations described and includes any mechanism that provides i.e. stores and or transmits information in a form accessible by a machine e.g. computing device electronic system or other device such as via recordable non recordable storage media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices or other storage media or via transmission media e.g. optical digital electrical acoustic signals or other propagated signal . A communication interface includes any mechanism that interfaces to any of a hardwired wireless optical or other medium to communicate to another device such as a memory bus interface a processor bus interface an Internet connection a disk controller. The communication interface can be configured by providing configuration parameters and or sending signals to prepare the communication interface to provide a data signal describing the software content.

Various components described herein can be a means for performing the operations or functions described. Each component described herein includes software hardware or a combination of these. The components can be implemented as software modules hardware modules special purpose hardware e.g. application specific hardware application specific integrated circuits ASICs digital signal processors DSPs etc. embedded controllers hardwired circuitry etc.

Besides what is described herein various modifications can be made to the disclosed embodiments and implementations without departing from their scope. Therefore the illustrations and examples herein should be construed in an illustrative and not a restrictive sense.

