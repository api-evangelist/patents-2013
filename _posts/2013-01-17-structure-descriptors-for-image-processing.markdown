---

title: Structure descriptors for image processing
abstract: A structure descriptor for an m×n pixel block of an image may be determined. The m×n pixel block may contain a primary pixel having a primary pixel value and a plurality of secondary pixels having respective secondary pixel values. The structure descriptor may include a plurality of structure indicators each associated with a respective secondary pixel. The respective structure indicators may be based on the primary pixel value and the respective secondary pixel value of the associated secondary pixel. Based on the structure descriptor, a structure value for the m×n pixel block may be determined. Based on the structure value, image processing may be applied to the m×n pixel block.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08995784&OS=08995784&RS=08995784
owner: Google Inc.
number: 08995784
owner_city: Mountain View
owner_country: US
publication_date: 20130117
---
Existing image processing techniques attempt to account for structure in images by analyzing relatively large sections of the images. As a result these techniques are computationally intensive and are often inappropriate for applications that have access to limited computational resources e.g. mobile applications . Furthermore existing image processing techniques typically do not enforce the preservation of image structure so the processing they apply can lead to objectionable changes in image structure between the input and output images.

The structure of a section of a digital image may be evaluated. Based on the evaluation a structure descriptor for the section may be determined. The structure descriptor may represent relationships between one or more pixels or groups of pixels in the section. Based on the structure descriptor image processing may be applied to the image.

In a first example embodiment a structure descriptor for an m n pixel block of an image may be determined. The m n pixel block may contain a primary pixel having a primary pixel value and a plurality of secondary pixels having respective secondary pixel values. The structure descriptor may include a plurality of structure indicators each associated with a respective secondary pixel. Each respective structure indicator may be based on the primary pixel value and the respective secondary pixel value of its associated secondary pixel. Based on the structure descriptor a structure value for the m n pixel block may be determined. Based on the structure value image processing may be applied to the m n pixel block.

A second example embodiment may include means for determining a structure descriptor for an m n pixel block of an image wherein the m n pixel block contains a primary pixel having a primary pixel value and a plurality of secondary pixels having respective secondary pixel values wherein the structure descriptor includes a plurality of structure indicators each associated with a respective secondary pixel and wherein each respective structure indicator is based on the primary pixel value and the respective secondary pixel value of its associated secondary pixel. The second example embodiment may also include means for determining a structure value for the m n pixel block based on the structure descriptor and means for applying image processing to the m n pixel block based on the structure value.

A third example embodiment may include a non transitory computer readable storage medium having stored thereon program instructions that upon execution by a computing device cause the computing device and or its peripherals to perform operations in accordance with the first and or second example embodiments.

A fourth example embodiment may include a computing device comprising at least a processor an image sensor and data storage. The data storage may contain program instructions that upon execution by the processor cause the computing device operate in accordance with the first and or second example embodiments.

These as well as other aspects advantages and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description with reference where appropriate to the accompanying drawings. Further it should be understood that the description provided in this summary section and elsewhere in this document is intended to illustrate the claimed subject matter by way of example and not by way of limitation.

The structure of digital images may refer to representations of lines edges and or transitions between objects represented in the images. For instance a digital image of a black and white chess board may represent many edges between the black and white squares on the chessboard. These edges may be considered to be part of the structure of the image. Accordingly such a black and white chess board may exhibit more structure than a random assortment of black and white pixels.

When using image processing techniques such as de noising sharpening and de blurring one possible challenge is distinguishing between structure and random noise in an image. Having this ability may allow the manipulation of images without introducing undesirable artifacts such as a dark bright edge that gets reversed to become a bright dark edge or an edge that gets blurred or lost entirely.

As one example of such artifacts image sharpening techniques can lead to random noise being exaggerated and can even introduce unnatural ringing artifacts around edges. As another example image de noising techniques can cause the loss of fine image detail by removing a significant amount of structure that would be present in the ideal noise free image. In both cases better understanding of local structure which helps separate structure from noise can improve the quality of processed images and avoid the at least some objectionable artifacts.

As noted above existing image processing techniques may be unsuitable for mobile applications. In particular mobile applications may have limited access to computational resources e.g. processing and or storage resources . Further it may be desirable for mobile applications to limit their use of available computational resources so that the battery life of mobile devices can be conserved. Thus it is desirable for de noising sharpening and de blurring techniques to effectively differentiate between image structure and noise but to do so in a computationally efficient fashion.

Accordingly the embodiments herein may operate on image capture devices such as the digital camera devices illustrated in . However these embodiments may also operate on other types of computing devices including computing devices without image capture capabilities. Thus a digital camera device may capture an image and process that image in accordance with the embodiments herein. Alternatively or additionally a digital camera device may capture an image the image may be transmitted or transferred to another computing device and that computing device may process the image in accordance with the embodiments herein.

In the following image capture devices and computing devices will be described in detail and then image processing techniques that can operate efficiently on such devices will be presented.

Image capture devices such as cameras may be employed as standalone hardware devices or integrated into various other types of devices. For instance still and video cameras are now regularly included in wireless communication devices e.g. mobile phones tablet computers laptop computers video game interfaces home automation devices and even automobiles and other types of vehicles.

The physical components of a camera may include an aperture through which light enters a recording surface for capturing the image represented by the light and a lens positioned in front of the aperture to focus at least part of the image on the recording surface. The aperture may be fixed size or adjustable. In an analog camera the recording surface may be photographic film. In a digital camera the recording surface may include an electronic image sensor e.g. a charge coupled device CCD or a complementary metal oxide semiconductor CMOS sensor to transfer and or store captured images in a data storage unit e.g. memory .

A shutter may be coupled to or nearby the lens or the recording surface. The shutter may either be in a closed position in which it blocks light from reaching the recording surface or an open position in which light is allowed to reach to recording surface. The position of the shutter may be controlled by a shutter button. For instance the shutter may be in the closed position by default. When the shutter button is triggered e.g. pressed the shutter may change from the closed position to the open position for a period of time known as the shutter cycle. During the shutter cycle an image may be captured on the recording surface. At the end of the shutter cycle the shutter may change back to the closed position.

Alternatively the shuttering process may be electronic. For example before an electronic shutter of a CCD image sensor is opened the sensor may be reset to remove any residual signal in its photodiodes. While the electronic shutter remains open the photodiodes may accumulate charge. When or after the shutter closes these charges may be transferred to longer term data storage. Combinations of mechanical and electronic shuttering may also be possible.

Regardless of type a shutter may be activated and or controlled by something other than a shutter button. For instance the shutter may be activated by a softkey a timer or some other trigger. Herein the term image capture may refer to any mechanical and or electronic shuttering process that results in one or more photographs being recorded regardless of how the shuttering process is triggered or controlled.

The exposure of a captured image may be determined by a combination of the size of the aperture the brightness of the light entering the aperture and the length of the shutter cycle also referred to as the shutter length or the exposure length . Additionally a digital or analog gain may be applied to the image thereby influencing the exposure. In some embodiments the term total exposure length or total exposure time may refer to the shutter length multiplied by the gain for a particular aperture size. Herein the term total exposure time or TET should be interpreted as possibly being a shutter length an exposure time or any other metric that controls the amount of signal response that results from light reaching the recording surface.

A still camera may capture one or more images each time image capture is triggered. A video camera may continuously capture images at a particular rate e.g. 24 images or frames per second as long as image capture remains triggered e.g. while the shutter button is held down . Some digital still cameras may open the shutter when the camera device or application is activated and the shutter may remain in this position until the camera device or application is deactivated. While the shutter is open the camera device or application may capture and display a representation of a scene on a viewfinder. When image capture is triggered one or more distinct digital images of the current scene may be captured.

Cameras even analog cameras may include software to control one or more camera functions and or settings such as aperture size TET gain and so on. Additionally some cameras may include software that digitally processes images during or after these images are captured. While it should be understood that the description above refers to cameras in general it may be particularly relevant to digital cameras.

As noted previously digital cameras may be standalone devices or integrated with other devices. As an example illustrates the form factor of a digital camera device . Digital camera device may be for example a mobile phone a tablet computer or a wearable computing device. However other embodiments are possible. Digital camera device may include various elements such as a body a front facing camera a multi element display a shutter button and other buttons . Digital camera device could further include a rear facing camera . Front facing camera may be positioned on a side of body typically facing a user while in operation or on the same side as multi element display . Rear facing camera may be positioned on a side of body opposite front facing camera . Referring to the cameras as front and rear facing is arbitrary and digital camera device may include multiple cameras positioned on various sides of body .

Multi element display could represent a cathode ray tube CRT display a light emitting diode LED display a liquid crystal LCD display a plasma display or any other type of display known in the art. In some embodiments multi element display may display a digital representation of the current image being captured by front facing camera and or rear facing camera or an image that could be captured or was recently captured by either or both of these cameras. Thus multi element display may serve as a viewfinder for either camera. Multi element display may also support touchscreen and or presence sensitive functions that may be able to adjust the settings and or configuration of any aspect of digital camera device .

Front facing camera may include an image sensor and associated optical elements such as lenses. Front facing camera may offer zoom capabilities or could have a fixed focal length. In other embodiments interchangeable lenses could be used with front facing camera . Front facing camera may have a variable mechanical aperture and a mechanical and or electronic shutter. Front facing camera also could be configured to capture still images video images or both. Further front facing camera could represent a monoscopic stereoscopic or multiscopic camera. Rear facing camera may be similarly or differently arranged. Additionally front facing camera rear facing camera or both may be an array of one or more cameras.

Either or both of front facing camera and rear facing camera may include or be associated with an illumination component that provides a light field to illuminate a target object. For instance an illumination component could provide flash or constant illumination of the target object. An illumination component could also be configured to provide a light field that includes one or more of structured light polarized light and light with specific spectral content. Other types of light fields known and used to recover three dimensional 3D models from an object are possible within the context of the embodiments herein.

Either or both of front facing camera and rear facing camera may include or be associated with an ambient light sensor that may continuously or from time to time determine the ambient brightness of a scene that the camera can capture. In some devices the ambient light sensor can be used to adjust the display brightness of a screen associated with the camera e.g. a viewfinder . When the determined ambient brightness is high the brightness level of the screen may be increased to make the screen easier to view. When the determined ambient brightness is low the brightness level of the screen may be decreased also to make the screen easier to view as well as to potentially save power. Additionally the ambient light sensor s input may be used to determine a TET of an associated camera or to help in this determination.

Digital camera device could be configured to use multi element display and either front facing camera or rear facing camera to capture images of a target object. The captured images could be a plurality of still images or a video stream. The image capture could be triggered by activating shutter button pressing a softkey on multi element display or by some other mechanism. Depending upon the implementation the images could be captured automatically at a specific time interval for example upon pressing shutter button upon appropriate lighting conditions of the target object upon moving digital camera device a predetermined distance or according to a predetermined capture schedule.

As noted above the functions of digital camera device or another type of digital camera may be integrated into a computing device such as a wireless communication device tablet computer laptop computer and so on. For purposes of example is a simplified block diagram showing some of the components of an example computing device that may include camera components .

By way of example and without limitation computing device may be a cellular mobile telephone e.g. a smartphone a still camera a video camera a fax machine a computer such as a desktop notebook tablet or handheld computer a personal digital assistant PDA a home automation component a digital video recorder DVR a digital television a remote control a wearable computing device or some other type of device equipped with at least some image capture and or image processing capabilities. It should be understood that computing device may represent a physical camera device such as a digital camera a particular physical hardware platform on which a camera application operates in software or other combinations of hardware and software that are configured to carry out camera functions.

As shown in computing device may include a communication interface a user interface a processor data storage and camera components all of which may be communicatively linked together by a system bus network or other connection mechanism .

Communication interface may function to allow computing device to communicate using analog or digital modulation with other devices access networks and or transport networks. Thus communication interface may facilitate circuit switched and or packet switched communication such as plain old telephone service POTS communication and or Internet protocol IP or other packetized communication. For instance communication interface may include a chipset and antenna arranged for wireless communication with a radio access network or an access point. Also communication interface may take the form of or include a wireline interface such as an Ethernet Universal Serial Bus USB or High Definition Multimedia Interface HDMI port. Communication interface may also take the form of or include a wireless interface such as a Wifi BLUETOOTH global positioning system GPS or wide area wireless interface e.g. WiMAX or 3GPP Long Term Evolution LTE . However other forms of physical layer interfaces and other types of standard or proprietary communication protocols may be used over communication interface . Furthermore communication interface may comprise multiple physical communication interfaces e.g. a Wifi interface a BLUETOOTH interface and a wide area wireless interface .

User interface may function to allow computing device to interact with a human or non human user such as to receive input from a user and to provide output to the user. Thus user interface may include input components such as a keypad keyboard touch sensitive or presence sensitive panel computer mouse trackball joystick microphone and so on. User interface may also include one or more output components such as a display screen which for example may be combined with a presence sensitive panel. The display screen may be based on CRT LCD and or LED technologies or other technologies now known or later developed. User interface may also be configured to generate audible output s via a speaker speaker jack audio output port audio output device earphones and or other similar devices.

In some embodiments user interface may include a display that serves as a viewfinder for still camera and or video camera functions supported by computing device . Additionally user interface may include one or more buttons switches knobs and or dials that facilitate the configuration and focusing of a camera function and the capturing of images e.g. capturing a picture . It may be possible that some or all of these buttons switches knobs and or dials are implemented as functions on a presence sensitive panel.

Processor may comprise one or more general purpose processors e.g. microprocessors and or one or more special purpose processors e.g. digital signal processors DSPs graphics processing units GPUs floating point units FPUs network processors or application specific integrated circuits ASICs . In some instances special purpose processors may be capable of image processing image alignment and merging images among other possibilities. Data storage may include one or more volatile and or non volatile storage components such as magnetic optical flash or organic storage and may be integrated in whole or in part with processor . Data storage may include removable and or non removable components.

Processor may be capable of executing program instructions e.g. compiled or non compiled program logic and or machine code stored in data storage to carry out the various functions described herein. Therefore data storage may include a non transitory computer readable medium having stored thereon program instructions that upon execution by computing device cause computing device to carry out any of the methods processes or functions disclosed in this specification and or the accompanying drawings. The execution of program instructions by processor may result in processor using data .

By way of example program instructions may include an operating system e.g. an operating system kernel device driver s and or other modules and one or more application programs e.g. camera functions address book email web browsing social networking and or gaming applications installed on computing device . Similarly data may include operating system data and application data . Operating system data may be accessible primarily to operating system and application data may be accessible primarily to one or more of application programs . Application data may be arranged in a file system that is visible to or hidden from a user of computing device .

Application programs may communicate with operating system through one or more application programming interfaces APIs . These APIs may facilitate for instance application programs reading and or writing application data transmitting or receiving information via communication interface receiving and or displaying information on user interface and so on.

In some vernaculars application programs may be referred to as apps for short. Additionally application programs may be downloadable to computing device through one or more online application stores or application markets. However application programs can also be installed on computing device in other ways such as via a web browser or through a physical interface e.g. a USB port on computing device .

Camera components may include but are not limited to an aperture shutter recording surface e.g. photographic film and or an image sensor lens and or shutter button. Camera components may be controlled at least in part by software executed by processor . However it should be understood that the embodiments herein can operate on computing devices that do not include camera components. For instance variations of computing device without camera components may represent personal computing devices e.g. desktop personal computer laptop tablet and or server devices .

Captured digital images may be represented as a one dimensional two dimensional or multi dimensional array of pixels. Each pixel may be represented by one or more values that may encode the respective pixel s color and or brightness. For example one possible encoding uses the YCbCr color model. In this color model the Y channel may represent the brightness of a pixel and the Cb and Cr channels may represent the blue chrominance and red chrominance respectively of the pixel. For instance each of these channels may take values from 0 to 255 i.e. the tonal range that a single 8 bit byte can offer . Thus the brightness of a pixel may be represented by a 0 or a value near zero if the pixel is black or close to black and by a 255 or a value near 255 if the pixel is white or close to white. However the value of 255 is a non limiting reference point and some implementations may use different maximum values e.g. 1023 4095 etc. .

Nonetheless the YCbCr color model is just one possible color model and other color models such as a red green blue RGB color model or a cyan magenta yellow key CMYK may be employed with the embodiments herein. Further the pixels in an image may be represented in various file formats including raw uncompressed formats or compressed formats such as Joint Photographic Experts Group JPEG Portable Network Graphics PNG Graphics Interchange Format GIF and so on.

Most if not all image capture devices introduce noise to images while capturing these images. Often the introduced noise is Gaussian in nature. Thus each pixel in the image may be changed from its original value by some amount and across multiple pixels this amount can exhibit a Gaussian normal distribution. But other noise distributions are possible. For example the noise at different pixels may be correlated. In addition to noise introduced by the image capture process captured images of a scene may exhibit some degree of blurring due to improper focus movement in the scene and or movement of the image capture device.

Both noise and blurring are undesirable in images as they are aesthetically unpleasing to the human eye and may obscure details of the image. Noise and blurring may particularly impact representations of lines edges and or transitions between objects in the image. These representations of lines edges and or transitions may be referred to as structure and some images may exhibit more structure than others.

In order to reduce the impact of noise and blurring image processing techniques such as de noising sharpening and or de blurring may be employed. However in order for these techniques to be effective they may need to differentiate between structure and noise in an image. Without the ability to distinguish structure from noise these techniques may introduce undesirable artifacts to images such as dark bright edges that gets reversed to become bright dark edges edges that gets blurred or lost entirely and or the exaggeration of noise in the images. Further image processing techniques can cause the loss of fine image detail by removing structure that would be present in an ideal noise free image.

As one example consider an image of a black and white chess board. Ideally the image should represent the transitions between the white and black squares as sharp distinct lines. On one side of such a line all of the pixels should be white and on the other side of the line the pixels should be black. However the noise introduced by an image capture device might distort this line by placing some amount of black pixels on the white side of the line and some amount of white pixels on the black side of the line. This may result in the line appearing blurred and or the black and white sections appearing peppered with the other color. Alternatively or additionally instead of all pixels being either black or white the image capture device may represent some pixels along the line as gray which may reduce the sharpness of the line.

To improve the quality of captured images image processing techniques such as de noising sharpening and or de blurring may be employed. There are many de noising sharpening and de blurring techniques any of which may potentially be used with the embodiments herein. Some of these techniques may be configurable to operate at different level of aggressiveness. For instance a de noising technique operating at a lower level of aggressiveness may apply a limited amount of de noising to a captured image. However the same de noising technique operating at a higher level of aggressiveness may de noise the captured image less conservatively. Thus at the higher level of aggressiveness the de noising technique may perform more iterations of de noising on the captured image and or attempt to de noise parts of the image that it would not when set at the lower level of aggressiveness.

A better understanding and representation of the local structure of images may improve the quality of processed images and avoid the creation of at least some undesirable artifacts. One way of considering local structure during image processing is to process an image in sections. For example each section might be an m n block of pixels. The size of the pixel block may be small perhaps containing as few as 1 100 pixels. However the pixel block might be larger than 100 pixels. In some cases m and n might take on the same value resulting in a square pixel block.

By analyzing one or more pixel blocks of an image the local structure represented by the pixel block may be differentiated from noise. Particularly a structure descriptor containing a characterization of an associated pixel block may be determined. Image processing of the pixel block may be based at least to some extent on the pixel block s associated structure descriptor.

To derive structure descriptor the value of the center pixel of pixel block may be compared to its neighbors. Each cell in structure descriptor may take on a value of either 0 or 1 based on the outcome of this comparison. The value of a cell in structure descriptor takes on a value of 0 if the value of its corresponding pixel in pixel block is less than or equal to the value of the center pixel of pixel block and 1 if the value of its corresponding pixel in pixel block is greater than the value of the center pixel of pixel block .

Therefore since the center pixel of pixel block has a pixel value of 13 all pixels in pixel block with pixel values of 13 or less are assigned a 0 in their associated cells of structure descriptor . On the other hand all pixels in pixel block with pixel values of 14 or more are assigned a 1 in their associated cells of structure descriptor . As depicted in the center pixel of pixel block is surrounded by lower pixel values above and to the left and by higher pixel values below and to the right. Thus the pixels of may represent an edge between two objects or colors. Structure descriptor reflects this structure.

As depicted in structure descriptor may be determined based on pixel block . Since the center pixel of pixel block has a pixel value of 14 all pixels in pixel block with pixel values of 14 or less are assigned a 0 in their associated cells of structure descriptor . Also all pixels in pixel block with pixel values of 15 or more are assigned a 1 in their associated cells of structure descriptor . The center pixel of pixel block is surrounded by both lower and higher pixel values. Thus the pixels of may represent a noisy part of a scene. Structure descriptor reflects this structure.

In some embodiments the structure descriptor may take the form of an 8 bit sequence e.g. one byte wherein each bit represents one of the non center binary values. For instance structure descriptor may be represented as 00101011 43 in decimal and structure descriptor may be represented as 00110001 49 in decimal . In each of these representations the center pixel of the pixel block is not included. For larger structure descriptors more bits may be used. For instance a 5 5 structure descriptor may be represented with a 3 byte 24 bit value.

Regardless there are several ways in which a structure value representing the structure of an m n pixel block can be determined from an associated structure descriptor. In one technique the number of contiguous vertical or horizontal 0 0 and 1 1 pairs in the structure descriptor may be determined. For a 3 3 pixel block the result is a number between 0 and 12 where 0 indicates little or no structure and high noise while 12 indicates high structure and little or no noise. Generally the higher this value the greater the amount of structure in the pixel block.

In another technique the size of the least connected component of a structure descriptor may be determined. The least connected component may be defined as the smallest contiguous e.g. vertically or horizontally adjacent set of either 0 s or 1 s in the structure descriptor. For a 3 3 pixel block the result is a number between 1 and 9 where 1 indicates less structure and 9 indicates more structure. Generally the higher this value the greater the amount of structure in the pixel block.

In yet another technique each possible value of the 8 bit representation of the structure descriptor e.g. for a 3 3 pixel block these values may be between 0 and 255 inclusive may be mapped to a structure value in a given range. For instance suppose that the given range is 1 to 10 where a 1 indicates less structure and a 10 indicates more structure. Then a structure descriptor of 0 may be mapped to a structure value of 10 a structure descriptor of 90 may be mapped to a structure value of 1 and so on. These mappings may be pre determined or derived using some form of machine learning procedure.

Once a structure value is determined it can be used during image de noising and image sharpening. In image de noising pixel blocks with less structure may be de noised more aggressively than those with more structure. In this way fine image details may be preserved. In image sharpening pixel blocks with more structure may be sharpened more than pixel blocks with less structure. In image de blurring pixel blocks with more structure may be de blurred more than pixel blocks with less structure.

As an example of image sharpening suppose that a pixel block contains an edge that is defined between a set of pixels with pixel values of 100 and an adjacent set of pixels with pixel values of 110. If the structure value indicates a low or medium degree of structure the pixel values may be changed to 90 and 120 respectively. However if the structure value indicates a high degree of structure the pixel values may be changed to 80 and 130 respectively. Thus contrast may be added between the two sets of pixels.

Additionally the structure descriptor may be compared before and after image processing to evaluate how well local structure is preserved by image processing and to limit the introduction of artifacts. The before structure descriptor may be determined prior to applying image process to an pixel block and the after structure descriptor may be determined after applying image processing to the pixel block. The difference between the before and after structure descriptors may be used to determine whether new structures have been created or original structures have been eliminated. This difference can be defined as the number of different bits in the before and after structure descriptors or by some other calculation. In some implementations this difference may be the Hamming distance between the before and after structure descriptors. For example if the difference is large it is very likely that image artifacts have been introduced by the image processing.

The pseudo code in Table 1 is an example of an image sharpening using structure descriptors. At line 1 an image I is sharpened by convolving it with an unsharpen mask formed from image I . For instance the unsharpen mask for I may be created by applying Gaussian noise to a I and then comparing the result to I. If the difference is greater than a pre defined threshold the images are combined e.g. subtracted to form I . The effect of this operation may be to emphasize high frequency components of I.

At line 2 the structure descriptors of I and I D I and D I respectively are determined. At line 3 V I the structure value of I is determined. At line 4 I and I are merged. As the difference between D I and D I grows the weight given to I in the merging is increased. For instance if the difference is above a threshold more weight might be given to I and or less weight might be given to I . The weight given to I is also increased as V I grows.

It should be understood that Table 1 illustrates an example embodiment and that other embodiments may be used instead. Some of these embodiments may encompass using de noising and or de blurring techniques instead of or combined with the illustrated sharpening technique.

At step a structure value for the m n pixel block may be determined based on the structure descriptor. At step image processing may be applied to the m n pixel block based on the structure descriptor.

In some embodiments the structure descriptor may contain m n structure indicators one of which is associated with the primary pixel and m n 1 of which are the plurality of structure indicators.

Alternatively or additionally determining the structure descriptor may involve setting a respective structure indicator for a secondary pixel to 0 if the secondary pixel has a secondary pixel value less than or equal to the primary pixel value. Determining the structure descriptor may also involve setting the respective structure indicator for the secondary pixel to 1 if the secondary pixel has a secondary pixel value greater than the primary pixel value.

In some embodiments the structure descriptor may be an m n block. Determining the structure value may involve calculating a number of 0 0 and 1 1 vertically adjacent or horizontally adjacent structure indicator pairs in the structure descriptor. In other embodiments the structure descriptor may be an m n block and determining the structure value may involve calculating a size of a least connected component in the structure descriptor. The least connected component may be a smallest contiguous group of structure indicators in the structure descriptor with a common value. Alternatively the structure descriptor may take on any form and determining the structure value may involve mapping the structure descriptor to the structure value using a lookup table that associates structure descriptors to structure values.

In some cases m n and or m is an odd number. The primary pixel may be a center pixel of the m n pixel block. As one possible example m 3 and the structure descriptor is represented by an 8 bit sequence that includes the structure indicators associated with each secondary pixel of the m n pixel block.

Applying image processing to the m n pixel block may involve applying a first degree of de noising if the structure value is less than or equal to a structure value threshold and or applying a second degree of de noising if the structure value is greater than the structure value threshold wherein the first degree of de noising involves more de noising than the second degree of de noising. Alternatively or additionally applying image processing to the m n pixel block may involve applying a first degree of sharpening if the structure value is greater than a structure value threshold and or applying a second degree of sharpening if the structure value is less than or equal to the structure value threshold wherein the first degree of sharpening involves more sharpening than the second degree of sharpening.

In some embodiments applying image processing to the m n pixel block forms a second m n pixel block of the image. Then a second structure descriptor for the second m n pixel block may be determined. Based on the second structure descriptor a second structure value for the second m n pixel block may be determined. Based on the structure value and the second structure value the m n pixel block and the second m n pixel block may be merged. Merging the m n pixel block and the second m n pixel block may involve determining an absolute difference between the first structure value and the second structure value determining that the absolute difference is greater than a threshold difference and based on the absolute difference being greater than the threshold difference assigning the m n pixel block more weight than the second m n pixel block during the merging. The absolute difference may be for instance the absolute value of the difference between the first structure value and the second structure value.

The steps depicted in may be carried out by a camera device such as digital camera device a computing device such as computing device and or by two or more distinct devices. Nonetheless other arrangements are possible. Further the flow chart depicted in may be modified according to the variations disclosed in this specification and or the accompanying drawings.

The above detailed description describes various features and functions of the disclosed systems devices and methods with reference to the accompanying figures. The illustrative embodiments described in the detailed description figures and claims are not meant to be limiting. Other embodiments can be utilized and other changes can be made without departing from the scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure as generally described herein and illustrated in the figures can be arranged substituted combined separated and designed in a wide variety of different configurations all of which are explicitly contemplated herein.

With respect to any or all of the message flow diagrams scenarios and flow charts in the figures and as discussed herein each step block and or communication may represent a processing of information and or a transmission of information in accordance with example embodiments. Alternative embodiments are included within the scope of these example embodiments. In these alternative embodiments for example functions described as steps blocks transmissions communications requests responses and or messages may be executed out of order from that shown or discussed including in substantially concurrent or in reverse order depending on the functionality involved. Further more or fewer steps blocks and or functions may be used with any of the message flow diagrams scenarios and flow charts discussed herein and these message flow diagrams scenarios and flow charts may be combined with one another in part or in whole.

A step or block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein described method or technique. Alternatively or additionally a step or block that represents a processing of information may correspond to a module a segment or a portion of program code including related data . The program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique. The program code and or related data may be stored on any type of computer readable medium such as a storage device including a disk drive a hard drive or other storage media.

The computer readable medium may also include non transitory computer readable media such as computer readable media that stores data for short periods of time like register memory processor cache memory and or random access memory RAM . The computer readable media may also include non transitory computer readable media that store program code and or data for longer periods of time such as secondary or persistent long term storage like read only memory ROM optical or magnetic disks and or compact disc read only memory CD ROM for example. The computer readable media may also be any other volatile or non volatile storage systems. A computer readable medium may be considered a computer readable storage medium for example or a tangible storage device.

Moreover a step or block that represents one or more information transmissions may correspond to information transmissions between software and or hardware modules in the same physical device. However other information transmissions may be between software modules and or hardware modules in different physical devices.

While various aspects and embodiments have been disclosed herein other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting with the true scope being indicated by the following claims.

