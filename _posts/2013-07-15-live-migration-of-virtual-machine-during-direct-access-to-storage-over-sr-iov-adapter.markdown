---

title: Live migration of virtual machine during direct access to storage over SR IOV adapter
abstract: A method is provided to migrate a virtual machine from a source computing machine to a destination computing machine comprising: suspending transmission of requests from a request queue disposed in source computing machine memory associated with the VM from the request queue to a VF; while suspending the transmission of requests, determining when no more outstanding responses to prior requests remain to be received; in response to a determination that no more outstanding responses to prior requests remain to be received, transferring state information that is indicative of locations of requests inserted to the request queue from the VF to a PF and from the PF to a memory region associated with a virtualization intermediary of the source computing machine. After transferring the state information to source computing machine memory associated with a virtualization intermediary, resuming transmission of requests from locations of the request queue indicated by the state information to the PF; and transmitting the requests from the PF to the physical storage.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09086904&OS=09086904&RS=09086904
owner: VMware, Inc.
number: 09086904
owner_city: Palo Alto
owner_country: US
publication_date: 20130715
---
This application is a continuation of the U.S. patent application Ser. No. 12 856 399 filed on Aug. 13 2010 entitled Live Migration of Virtual Machine During Direct Access to Storage over SR IOV Adapter. 

A host computer system may run multiple virtual machines VMs that share common resources such as physical storage. Physical storage used by the VMs typically is emulated so as to present virtual storage resources to the VMs. A virtualization intermediary manages interaction between VMs and physical storage. Some prior virtualization intermediaries trap intercept virtual storage requests issued by individual VMs and to thereby intervene in the storage access requests so as to redirect the requests from virtual targets to physical targets. Such earlier virtualization intermediary uses trap handlers during emulation to redirect IO commands to prevent storage access violations. However this emulation can be expensive in terms of instructions processed. Overall performance may decline when many VMs seek to access physical storage at the same time. The many storage requests can result in data access delays due to the compute cycles required by the virtualization intermediary to trap and translate simultaneous data requests from many VMs.

One solution to this problem has been proposed in the Single Root Virtualization I O and Sharing Specification Revision 1.0 Sep. 11 2007 PCI SIG SR IOV specification. The PCI SIG SR IOV specification proposes providing each of one or more VMs with direct access to physical storage through its own storage adapter instance as a designated virtual function VF running on a physical storage adapter. This avoids the need for heavy intervention by the virtualization intermediary to gain access to physical storage.

Live migration typically involves the ability to move a VM from a source physical machine to a destination physical machine possibly one with different IO adapter hardware at almost any point in a VM s work flow without affecting correctness of processing by the VM and without significantly affecting the availability of the VM. In general migration involves a virtualization intermediary intervening to suspend a VM s execution at any instruction boundary and to control the flow of the VM s IO during the migration process. The virtualization intermediary also tracks and quiesces IO initiated by the VM on the source migration host and resumes the IO on the destination migration host. However direct access by a VM to physical storage that bypasses intervention of the virtualization intermediary complicates live migration of a virtual machine across physical machines since the virtualization intermediary does not intervene in VM IO operations.

In one aspect a virtual machine VM migrates from a source computing machine to a destination computing machine. The source computing machine is configured to implement the VM and a virtualization intermediary. The source computing machine includes a physical storage adapter that is configured to implement a physical function PF and a virtual function VF to support IO operations between the VM and physical storage. The migration occurs while the VM runs an application that involves the VM inserting IO requests into an IO request queue associated with the VM and that involves the VF de queuing the IO requests from the IO request queue and transmitting the IO requests to physical storage. The VM s running the application also involves the VF inserting one or more IO responses received from the physical storage to an IO response queue associated with the VM. State information that is indicative of the request queue and of the response queue is provided within a memory region of the VF. In the course of migration de queuing of requests from the request queue is suspended. While the de queuing is suspended state information is transferred from the VF memory region to a memory region associated with the virtualization intermediary within the source computing machine. Subsequently the state information is transferred from the second memory region within the source computing machine to the destination machine.

In another aspect prior to VM migration during normal IO access involving direct access to storage i.e. substantially without virtualization intermediary intervention an IO request queue includes one or more requests produced by a guest operating system of the VM. State information that is indicative of one or more locations of requests within the request queue is transmitted from the VM to the VF. Requests are de queued from the request queue to the VF from locations indicated by the first state information and are then transmitted from the VF to physical storage. The VF receives responses to the transmitted requests from physical storage. During VM migration de queuing of requests from the request queue to the VF is suspended. In response to a determination that no more outstanding responses to previous requests remain to be received the first state information is transferred from the VF to the PF and from the PF to a memory region associated with the virtualization intermediary of the source computing machine. After the first state information has been transferred to the source computing machine memory the VM resumes normal operation but with virtualization intermediary intervention in IO access. One or more IO requests are de queued from locations in the request queue indicated by the first state information to the PF. The requests then are transmitted from the PF to the physical storage.

Thus in yet another aspect prior to VM migration during normal IO access involving direct access to storage i.e. substantially without virtualization intermediary intervention one or more requests are transmitted from a VF to a physical storage. The VF receives responses to the transmitted requests from physical storage. Second state information is provided within the VF that is indicative of one or more locations in a response queue disposed within source computing machine memory associated with the source VM. IO responses received by the VF are inserted to one or more locations in the response queue indicated by the second state information. During VM migration the transmission of responses is suspended. In response to a determination that no more outstanding responses to previous requests remain to be received the second state information is transferred from the VF to the PF and from the PF to a memory region of the source computing machine. After transferring the second state information to source computing machine memory the VM resumes normal operation but with virtualization intermediary intervention in IO access. Requests are transmitted from the PF to the physical storage. Responses are received by the PF from the physical storage. Responses received by the PF are inserted to one or more locations in the response queue indicated by the second state information.

These and other features and advantages will be apparent from the following description of embodiments in conjunction with the illustrative drawings.

The following description is presented to enable a person skilled in the art to create and use a computer system configured for use with an SR IOV adapter in which a virtual machine using a direct data path access to storage can migrate between different host machines. Various modifications to the preferred embodiments will be readily apparent to those skilled in the art and the generic principles defined herein may be applied to other embodiments and applications without departing from the spirit and scope of the invention. Moreover in the following description numerous details are set forth for the purpose of explanation. However one of ordinary skill in the art will realize that the invention might be practiced without the use of these specific details. In other instances well known structures and processes are shown in block diagram form in order not to obscure the description of the invention with unnecessary detail. Thus the present invention is not intended to be limited to the embodiments shown but is to be accorded the widest scope consistent with the principles and features disclosed herein.

In this description reference is sometimes made to a virtual machine a hypervisor kernel virtual machine monitors VMMs a virtualization intermediary or some other virtualized component taking some action. Persons skilled in the art will appreciate that a hypervisor kernel VMMs and a virtualization intermediary comprise one or more software layers that run on a host system which comprises hardware and software. In order to perform any given action a virtual machine virtualization intermediary or other virtualized component configures physical resources of the host machine to perform the given action. For example a virtualization intermediary may configure one or more physical processors according to machine readable program code stored in machine readable storage device.

Each virtual machine VM to VMn typically will have both guest system software and virtual system hardware which typically includes one or more virtual CPUs VCPUs to virtual system memory at least one virtual disk and one or more virtual devices . The virtual hardware components of the virtual machine may be implemented in software using known techniques to emulate the corresponding physical components. The guest system software includes guest operating system OS and virtual drivers as needed for the various virtual devices .

As is well known to persons skilled in the art a typical device driver is a computer program based component that runs on i.e. configures a machine e.g. host computer and acts as a translator between a physical device and the applications or operating systems that use the device. A device driver typically accepts generic high level commands and breaks them into a series of low level device specific commands as required by the device being driven. A virtual driver is a computer program based component that that runs on a host computer and performs the same role as a physical driver except that it configures a machine e.g. host computer to provide translation between a virtual device that provides hardware emulation and that runs within a VMM and or the guest operating system for example. Furthermore drivers virtual or physical can provide a level of security as they can run in kernel mode thereby protecting the operating system from applications running in user mode.

In many cases software applications running on a virtual machine e.g. VM will function as they would if run on a real computer even though the applications are running at least partially indirectly that is via guest OS and virtual processor s . Executable files will be accessed by the guest OS from virtual disk or virtual memory which will correspond to portions of an actual physical disk or storage on the SAN or memory allocated to that virtual machine.

A software component referred to herein as a virtualization intermediary serves as an interface between the guest software within a virtual machine and the various hardware components and devices in the underlying hardware platform. The virtualization intermediary may include VMMs a hypervisor also referred to as a virtualization kernel or some combination thereof. Because virtualization terminology has evolved over time and has not yet become fully standardized these three terms do not always provide clear distinctions between the software layers and components to which they refer. In some systems some virtualization code is included in at least one superior virtual machine to facilitate the operations of other virtual machines. Furthermore specific software support for virtual machines may be included in the host OS itself. For example the term hypervisor often is used to describe both a VMM and a kernel together either as separate but cooperating components or with one or more VMMs incorporated wholly or partially into the hypervisor itself to serve as a virtualization intermediary. However the term hypervisor also is sometimes used instead to mean some variant of a VMM alone which interfaces with some other software layer s or component s to support the virtualization.

A hypervisor portion of a virtualization intermediary comprises a software layer implemented to manage physical resources process creation and I O stacks and includes physical device drivers only one shown that serve as an interface to host system devices such as the IO storage adapter . Under such an implementation the hypervisor would manage the selections of physical devices and their temporary assignments to virtual devices. A host computer system may run multiple virtual machines VMs that share common resources such as physical storage. A virtualization intermediary manages interaction between VMs and physical storage. Some prior virtualization intermediaries trap intercept virtual storage requests issued by individual VMs and redirect the requests from virtual targets to physical targets. Such earlier virtualization intermediaries use trap handlers during emulation to redirect IO commands to prevent storage access violations.

For example a hypervisor kernel portion of a virtualization intermediary would manage the mapping between VM VMn and their virtual processors to virtual memory and the physical hardware devices that are selected to implement these virtual devices. More particularly when a virtual processor is dispatched by a VM a physical processor such as one of the physical processors would be scheduled by the hypervisor to perform the operations of that virtual processor. In contrast in the context of such implementation the virtual machine monitors VMM VMMn would be responsible for actually executing commands on physical processors performing binary translation BT or programming of virtual hardware for example. Note that in some embodiments the VMM is instanced meaning that a separate instance of the VMM portion of the virtualization intermediary is created for each VM. Thus although in this example such a hypervisor and a VMM may be distinct they would work together as a virtualization intermediary. Unless otherwise indicated the term virtualization intermediary encompasses any combination of VMM and hypervisor or hypervisor kernel that provides a virtualization layer between a guest OS running on VMs and the host hardware.

In the system of the virtual machine monitors VMM to VMMn are shown as separate entities from the hypervisor kernel software that run within VM to VMn respectively. The VMMs of the system of emulate virtual system hardware. While the hypervisor kernel is shown as a software layer located logically between all VMs and the underlying hardware platform and or system level host software it would be possible to implement at least part of the hypervisor layer in specialized hardware. The illustrated embodiments are given only for the sake of simplicity and clarity and by way of illustration since as mentioned above the distinctions are not always so clear cut. Again unless otherwise indicated or apparent from the description it is to be assumed that one or more components of the virtualization intermediary can be implemented anywhere within the overall structure of such virtualization intermediary and may even be implemented in part with specific hardware support for virtualization.

The various virtualized hardware components of the VM such as VCPU s to virtual system memory virtual disk and virtual device s are shown as being emulated within VMM which runs within virtual machine VM. One advantage of such an arrangement is that the virtual machine monitors VMM to VMMn may be set up to expose generic devices which facilitate VM migration and hardware platform independence. For example the VMM may be set up to emulate a standard Small Computer System Interface SCSI disk so that the virtual disk appears to the VM to be a conventional SCSI disk connected to a conventional SCSI adapter whereas the underlying actual physical disk may be something else. The term disk as used herein signifies persistently stored data addressed in sequence typically from address zero to address max capacity 1. A SCSI driver typically would be installed into the guest OS as one of the virtual drivers . A virtual device within the VMM then would provide an interface between VM and a physical device driver within the hypervisor kernel that serves as an interface to a physical device e.g. device that is part of the host system and would handle disk operations for the VM.

Different systems may implement virtualization to different degrees virtualization generally relates to a spectrum of definitions rather than to a bright line and often reflects a design choice with respect to a trade off between speed and efficiency on the one hand and isolation and universality on the other hand. For example full virtualization is sometimes used to denote a system in which no software components of any form are included in the guest OS other than those that would be found in a non virtualized computer thus the guest OS could be an off the shelf commercially available OS with no components included specifically to support use in a virtualized environment.

In contrast another term which has yet to achieve a universally accepted definition is para virtualization. As the term implies a para virtualized system is not fully virtualized but rather the guest is specially configured in some way to provide certain features that facilitate virtualization. For example the guest in some para virtualized systems is designed to avoid hard to virtualize operations and configurations such as by avoiding certain privileged instructions certain memory address ranges etc. As another example some para virtualized systems include an interface within the guest that enables explicit calls to other components of the virtualization software.

For some the term para virtualization implies that the guest OS in particular its kernel is specifically designed to support such an interface. Others define the term para virtualization more broadly to include any guest OS with any code that is specifically intended to provide information directly to any other component of the virtualization software. According to this view loading a module such as a driver designed to communicate with other virtualization components renders the system para virtualized even if the guest OS as such is an off the shelf commercially available OS not specifically designed to support a virtualized computer system. Unless otherwise indicated or apparent embodiments are not restricted to use in systems with any particular degree of virtualization and are not to be limited to any particular notion of full or partial para virtualization.

In addition to the sometimes fuzzy distinction between full and partial para virtualization two arrangements of intermediate system level software layer s are in general use a hosted configuration and a non hosted configuration. In a hosted virtualized computer system an existing general purpose operating system forms a host OS that is used to perform certain input output I O operations alongside and sometimes at the request of the VMM.

The system of is an example of a non hosted configuration in which VMMs are deployed on top of a software layer hypervisor kernel constructed specifically to provide an interface for the virtual machines. Kernel also may handle any other applications running on it that can be separately scheduled as well as a console operating system that in some architectures is used to boot the system and facilitate certain user applications used to interact with the virtualization software.

Many modern computing devices employ input output IO adapters and buses that utilize some version or implementation of the Peripheral Component Interconnect PCI standard which specifies a computer bus for attaching peripheral devices to a computer motherboard. PCI Express PCIe is an implementation of the PCI computer bus that uses existing PCI programming concepts but bases the computer bus on a different and much faster serial physical layer communications protocol. In addition to the PCI and PCIe specifications the PCI SIG has defined input output virtualization IOV standards for defining how to design an IO adapter that can be shared by several virtual machines.

The term function is used in the PCI context to signify a device with access controlled by a PCI bus. A PCI function is identified within a single PCI root complex by its PCI or PCIe bus device and slot identifiers. A PCI function includes a configuration space which includes both device dependent and device independent regions used by host software to support device relocation on the PCI bus flexible device to interrupt binding device identification and device configuration. A function also includes memory space which is identified by Barrier Address Registers in configuration space and provides a memory mapped IO interface for host IO initiated from host to the device. A PCIe function also includes message space which can be identified by MSI and MSI X capabilities in configuration space and provides either or both MSI MSI X message based interrupt generation. Many network e.g. Ethernet and storage e.g. disk adapters are implemented as PCI or PCIe compliant adapters and are recognized by a machine s PCI sub system as a single PCI function. Multi port PCI or PCIe adapters simply appear to a host PCI sub system as multiple PCI functions.

The illustrative system includes VMs and each independently running a separate and possibly different guest operating system. A virtualization intermediary layer runs between the virtual machines and and a host machine . Device driver of VM and device driver of VM each drives a physical function PF with intervention by the virtualization intermediary . Device driver of VM drives a virtual function VF without intervention by the virtualization intermediary . The device driver communicates with IO MMU logic disposed on the host machine in the course of data access to data with mass storage not shown . A device manager within virtualization intermediary manages the allocation and de allocation of VFs for the SR IOV adapter . The IOV adapter provides a memory mapped input output interface for IO and provides an interface for controlling VFs.

A typical IOV adapter includes processor memory and network interface resources not shown to implement the PF and one or more virtual functions VFs. A PF is a PCIe function that supports the SR IOV capabilities defined in the PCI SR IOV specification. A PF is used to control the physical services of the device and to manage individual VFs.

A VF is a PCIe function which is associated with a particular physical function and shares physical PCI adapter resources e.g. ports memory with that physical function and other virtual functions located on the same physical adapter. A virtual function has its own PCI configuration space memory space and message space separate from other physical or virtual functions on that same adapter. A physical function such as PF in this example that is associated with a virtual function is responsible for allocating resetting and de allocating that virtual function and the PCI resources required by that virtual function. In general a VF can either be accessed via a virtualization intermediary or bypass the virtualization intermediary to be directly accessed by a guest OS. In the example system VMs respectively access PF via the virtualization intermediary and VM accesses VF directly i.e. without intervention by the virtualization intermediary . Thus a VF can be a direct sink for I O and memory operations of a VM i.e. without run time intervention by a virtualization intermediary. The VF also can be a source of Direct Memory Access DMA completion and interrupt operations to a VM.

The International Committee for Information Technology Standards INCITS T10 Technical Committee has adopted a layered approach that divides the Small Computer System Interface SCSI into multiple layers of standards. The lowest layer refers to physical interfaces that are sometimes referred to as physical transports. The next layer up pertains to transport protocols usually directly associated with one physical transport standard. The top layer consists of command sets associated with specific devices such as disk drives or tape drives for example. See J. Lohmeyer SCSI Standards Architecture Business Briefing Data Management Storage Technology 2003. A result of this layered approach to the SCSI standard is that there are over 30 SCSI standards. In general only a few of these standards apply to a given product. As used herein the term SCSI signifies compliance with one or more of these SCSI standards.

A SCSI command is a request describing a unit of work to be performed by a device server. A SCSI command descriptor block CDB is a structure used to communicate commands from an application client to a device server. The SCSI command set assumes an underlying request response protocol. The fundamental properties of the request response protocol are defined in SCSI Architecture Model SAM 3 Revision 14. Action on SCSI commands is not be deemed completed until a SCSI response is received. For example a SCSI IO request may include a Read command or a Write command. If successful the SCSI request results in the reading or writing of data. A SCSI IO response provides status information concerning the data transfer if successful or concerning error if unsuccessful. The SCSI IO response ordinarily includes a status that indicates the final disposition of the command. See SCSI Primary Commands 3 SPC 3 Revision 23 Section 4.2 The request response model May 4 2005 American National Standards for Information Systems InterNational Committee for Information Technology Standards. hereinafter SPC 3 Revision 23 

In some embodiments the PCI bus protocol is compliant with both the PCI specification and the PCIe SR IOV extension specification and SCSI commands are used with one or more SCSI transport protocols such as iSCSI SAS or Fibre Channel to directly communicate IO access requests Read Write with persistent physical storage such as SAN storage for example. More particularly the system is configured to allow a virtual machine to access physical storage via IOV direct access for certain SCSI Read Write CDBs and to allow access to physical storage via a hypervisor kernel for other SCSI CDBs. Specifically certain frequently occurring SCSI I O commands such as SCSI 6 10 12 and 16 byte SCSI read and write commands are directed over a direct IOV data path coupling between the virtual machine and a virtual function substantially without involvement of the hypervisor kernel . The direct IOV data path is referred to as direct because it does not involve the hypervisor kernel and has been referred to as the fast path because it allows for faster access to storage since it does not involve the hypervisor kernel . A set of control plane IO request response queues shared with the virtualization intermediary are used for all other SCSI I O SCSI task management operations and all other forms of non SCSI fast path related communication between the virtual adapter and the hypervisor kernel .

The storage adapter includes adapter resources such as processor and memory devices e.g. RAM EPROM FLASH or Disk and network protocol translation and interface resources which will be readily understood by persons skilled in the art to implement a physical function PF and the virtual function VF . Adapter resources are configured to implement the physical function PF . The virtual function VF is controlled e.g. created deleted enabled disabled quiesced checkpointed and restored through interaction between the hypervisor kernel and the PF via the PF driver . For example in some embodiments the PF provides access to local physical and virtual ports of the physical storage adapter . Moreover in some embodiments physical storage adapter management tasks e.g. configuration firmware download core upload diagnostics hard reset monitoring and SCSI task management take place through the PF .

The PF driver communicates information between the PF and the hypervisor kernel . In the illustrative embodiment the VF is associated with virtual machine . A guest VF driver communicates information with both the VF and a hybrid storage adapter HSA instantiated within a VMM within the virtual machine . Although only one VM and one corresponding VF are shown and described herein it will be appreciated that the host system may host multiple VMs and the adapter may implement multiple corresponding VFs and the description herein would apply to each such combination of VM and VF. Multiple VFs only one shown may be instantiated within the adapter and each respective virtual machine only one shown may be associated with a different respective VF to create respective IOV data paths for certain frequently occurring Read and Write SCSI IO commands.

The HSA appears to be a physical PCI device i.e. a physical storage adapter from the perspective of the virtual machine . The HSA acts as the virtual machine s interface to the physical storage world i.e. to the physical storage adapter . The hybrid storage adapter is an emulated PCI storage adapter within the VM which encapsulates a PCI SR IOV virtual function of an SR IOV compliant physical storage adapter presented by the hypervisor kernel within a protected memory space of the virtual machine. A PCI configuration space of the virtual function is copied to the HSA s PCI configuration space so as to provide a memory mapped interface to a first HSA PCI memory space that supports direct access to physical memory. The HSA through the first PCI memory space provides a direct access path to the guest OS of the virtual machine with the capability to issue IO requests directly to the physical adapter virtual function VF without intervention by the hypervisor kernel . Although the HSA is shown resident within a protected memory space of the virtual machine it will be appreciated that it could instead reside within the hypervisor kernel . The HSA is referred to as a hybrid herein because it has two memory mapped interfaces and .

In some embodiments the HSA s PCI configuration space also maps to a second HSA PCI memory mapped interface that supports fully emulated processing of certain SCSI commands using the virtualization intermediary . Specifically a virtual target emulation mapping not shown comprising first mapping metadata not shown associated with a VF to map virtual SCSI logical units i.e. virtual disks to portions of physical SCSI logical units and second mapping metadata not shown to map virtual SCSI address two tuple information i.e. virtual SCSI target virtual SCSI LUN to virtual disks allocated to a given virtual machine. Commonly assigned patent application Ser. No. 12 689 152 invented by Goggin et al. filed Jan. 18 2010 entitled Virtual Target Addressing During Direct Data Access Via VF of IO Storage Adapter which is expressly incorporated herein by this reference discloses virtual target emulation mapping associated with the VF . Commonly assigned patent application Ser. No. 12 689 162 invented by Goggin et al. filed Jan. 18 2010 entitled Configuring VM and IO Storage Adapter VF for Virtual Target Addressing During Direct Data Access which is expressly incorporated herein by this reference discloses a method to configure such virtual target emulation mapping with a VF that is associated with a VM.

In other alternative embodiments for each physical SCSI disk exposed to the VF on the physical SAN there may be multiple physical SCSI paths existing to the physical SCSI disk of which only a subset may be active at any point in time. Commonly assigned patent application Ser. No. 12 731 265 invented by Aswani et al. filed Mar. 25 2010 entitled Virtualization Intermediary Virtual Machine Guest Operating System Collaborative SCSI Path Management which is expressly incorporated herein by this reference discloses a path management layer in a storage stack not shown within the hypervisor kernel that performs several functions. The path management layer performs functions such as discovery of the physical paths to SCSI devices collapsing multiple physical paths to a SCSI device keeping track of the active path to a SCSI device ensuring that IO coming from the software layers above it is routed to the correct path for a SCSI device and changing the active path based on certain path selection and or load balancing policies for example.

In some embodiments the hypervisor kernel is implemented as the ESX hypervisor produced by VMware Inc. having a place of business in Palo Alto Calif. The ESX hypervisor serves as a virtualization intermediary having both VMM and hypervisor kernel functionality. Each VM e.g. virtual machine runs on top of ESX. In an ESX environment a portion of each VM comprises a VMM. That is VMMs are embedded in the VM s address space albeit in a protected region. In some embodiments the hybrid storage adapter also resides in a protected memory space of the VM and more particularly runs within the context of the VMM that is embedded in that VM s memory space. If a given VM has multiple virtual processors VCPUs then each virtual processor has an associated VMM. In an ESX environment the VMM hypervisor virtualization intermediary serves as the primary memory management component to manage multiple simultaneously running VMs.

The guest VF driver is savvy as to the hybrid nature of the HSA and as such is a para virtual device driver. The guest VF driver directs certain SCSI IO operations such as certain data IO operations to the first HSA PCI memory space for direct access to physical storage via the VF . As explained above the guest VF driver directs other SCSI operations such as certain control IO operations to the second HSA PCI memory space for fully emulated processing by the hypervisor kernel . More particularly in some embodiments during runtime operation the VF driver directs only certain SCSI storage access e.g. Read Write commands to the first HSA memory space and directs all other SCSI commands to the second HSA memory space .

The VF driver allocates a portion of host memory to one or more first IO queues referred to herein as the first queue that contain SCSI requests responses that are communicated with the VF driver over the first HSA memory space which provides direct access to physical storage . Typically the first queue comprises one or more circular SCSI IO request queues and one or more circular SCSI IO response queues . The VF driver also allocates a portion of host memory to one or more second queues referred to herein as the second queue that contain information that is directed by the VF driver to the second HSA memory space for emulated access to physical storage . Generally the second IO queue comprises one or more circular request queues and one or more circular response queues that are used for SCSI IO operations other than certain SCSI IO Reads and Writes. Requests in the first or second IO queues comprise queued requests to be sent to storage . Responses in the first or second IO queues comprise responses received from storage that are queued for processing by the guest OS .

More particularly the first HSA memory space enables access to the first request queue and the first response queue through a storage device memory within the VF configured to implement VF registers that contain queue metadata state information indicative of the structure and state of these queues respectively. For example in some embodiments the first SCSI IO request queue includes a sequence of first slots having slot addresses ADX to ADXn each of which may contain a SCSI IO request and the first SCSI IO response queue includes a sequence of second slots having slot addresses ADY to ADYn each of which may contain a SCSI IO response. In some embodiments first metadata includes a first index into a sequence of locations ADX ADXn of a register that correspond to and therefore are indicative of first slot addresses locations in the first request queue . A current value of the first index is indicative of where a next request to be de queued by the VF is located. Similarly in some embodiments second metadata includes a second index into a sequence of locations ADY ADYn of a register that correspond to and therefore are indicative of second slot address locations in the first response queue . A current value of the second index is indicative of where a location in the second request queue where a next response received from storage is to be placed by the VF .

In some embodiments when the VF driver issues a SCSI IO Read or Write request it adds the request to the next open slot in the first SCSI IO request queue . The VF driver informs the VF of the new request via the first HSA memory space and the VF increments the first index to the indicate the first slot location of the newly added IO request in the first request queue that is to be processed by the VF . Conversely when the VF receives a SCSI IO response directed to the first response queue for example VF fills the response to a second slot indicated by the second index . The VF then increments the second index so that a next received response will be added to a next slot in the first response queue . Thus in some embodiments the first HSA memory space maps to queue metadata state information and within the VF that is indicative of the structure and state of the first SCSI IO request queue and of the first SCSI IO response queue respectively and thereby provides a direct path i.e. a path that is not intermediated by the hypervisor kernel between the first queue associated with the VM and storage .

In some embodiments dispatch of an IO request from the first request queue to storage occurs as follows. An application running on the virtual machine issues a Read Write access request to the guest OS . The VF driver assembles a SCSI IO request. In some embodiments the SCSI IO request comprises an IO Dispatch Control Block containing amongst other information an embedded SCSI command data block CDB for the read write operation and addresses. The VF driver places the Block on the first request queue for access by the VF . The VF driver notifies the VF of the addition of the new Dispatch Control Block on the first request queue via the first HSA PCI memory space causing register to increment a request state value e.g. the index of the first metadata to point to an address indicative of the location of the newly added Block within the first request queue and causing the VF of the adapter to de queue the Block and send it to the storage target . Subsequently the target storage provides a corresponding SCSI IO response.

In some embodiments input of a SCSI IO response received from storage to the first SCSI IO response queue proceeds as follows. If the SCSI IO request involves a Read then data is retrieved from physical storage . If the SCSI IO request involves a write then data is written to physical storage . After the SCSI IO request Read or Write has been successfully performed the VF inserts the new Completion Control Block to the first response queue at a second slot address location indicated by a state value e.g. the index of the second metadata and then increments the state value e.g. the index of the second metadata to indicate a next slot location in the first response queue where a next received completion is to be inserted. The VF notifies the VF driver of the addition of the new Completion Control Block to the first response queue . Commonly assigned patent application Ser. No. 12 689 152 identified above and incorporated by reference describes dispatch of a virtual SCSI IO storage request and processing of a response to a virtual IO request.

Commonly assigned patent application Ser. No. 12 687 999 filed Jan. 15 2010 invented by Subramanian et al. entitled Guest Hypervisor Interrupt Coalescing for Storage adapter Virtual Function in Guest Passthrough Mode which is expressly incorporated herein by this reference discloses an interrupt coalescing technique that can be employed in the course of access to the first queue . For example when interrupt coalescing is utilized the VF may be delayed in notifying the VF driver of the presence of new completions in the first response queue until multiple completions have been added to the first completion queue . As explained in application Ser. No. 12 687 999 interrupts that would have caused the VF driver to process the completions as they were received are coalesced for some delay duration. However even though interrupts are being coalesced the response state value e.g. index is incremented upon the arrival of each additional completion added to the first response queue to keep track of the number of completions received. When an interrupt finally is sent to the VF driver to notify it of the presence of one or more new completions in the first response queue to be consumed it will consume all completions in the first response queue up to the most recently incremented index value of the response state value .

The actual data that is read from or written to storage may comprise a plurality of data packets. During a read the completion queue of the VF receives the data packets from the storage region and causes the received data to be sent to the DMA logic within the adapter . The DMA logic cooperates with IO MMU logic within the host machine to read the data directly into a memory space of host machine physical memory not shown that has been allocated to the virtual machine for the application that originally requested the data.

Commonly assigned U.S. Pat. No. 7 680 919 invented by M. Nelson issued Mar. 16 2010 entitled Virtual Machine Migration describes a farm of host machines referred to as servers each of which may host one or more virtual machines VMs as well as mechanisms for migrating a VM from one server the source server to another the destination server while the VM is still running is an illustrative drawing of a farm of interconnected servers a server farm in accordance with some embodiments with each server hosting a plurality of virtual machines. The general configuration of the server farm includes a plurality of user machines and . . . that access a farm of host machines i.e. servers . . . via a network . The server labeled in corresponds to the identically labeled system of in that it comprises VM that is coupled for direct access to the SR IOV storage adapter . The resources of the servers and to are aggregated into one common resource pool. From the perspective of a user device therefore the server farm will appear to be one big machine with a large quantity of resources.

As indicated by arrow VM which runs on source machine is relocated to destination machine where it is instantiated as VM . In some embodiments VMs can be migrated only between machines that share storage where the VMs disks reside. In the example server farm of in order to allow for inter server migration the host machines and to therefore either share an external common storage system or can access each other s internal storage. This assumption eliminates the need to migrate entire disks. One way to arrange this is for all of the servers in the farm to be connected via a system such as Fibrechannel. This is illustrated in as the channel .

Referring again to the adapter resources are further configured to provide PF application programming interface API that comprises PF API modules used by the PF to control the VF in course of migration of the VM from the source machine to the destination machine shown in in accordance with some embodiments.

PF API module sets a maximum number of outstanding IO requests for the VF . In some embodiments this limit on the number of outstanding IO requests is enforced by the physical adapter by not de queuing a request from the first request queue if the maximum number of requests for the VF is already outstanding. Enforcing this limitation upon the maximum number can eliminate the need for an IO response queue overflow buffer on the migration destination machine.

PF API modules and enable disable either the first request queue or the first response queue . Enabling the first request queue causes the VF to service the first request queue if that queue is non empty. Enabling the first response queue causes the VF to add responses to the first response queue if the first SCSI IO queue is not full. The enable disable capabilities of modules and are utilized to manage these queues during live migration when transitioning the first HSA PCI memory space from direct storage access to emulated storage access on the migration source machine.

In some embodiments the VF includes a VF request queue not shown in which IO requests are queued prior to being sent to the storage target . PF API module flushes such IO requests from that queue not shown . However in other embodiments such as the one illustrated herein the VF sends IO requests immediately and therefore IO requests are resident only momentarily on the VF and as a result in many cases there will be no IO requests to flush from the VF .

PF API module obtains from the VF the number of outstanding SCSI IO requests. A SCSI IO request is considered to be outstanding until a corresponding SCSI IO response containing a corresponding SCSI IO control block is queued to the first response queue and any necessary DMA is complete for the IO pages referred to by the IOCB. The outstanding SCSI IO request count is used during the live migration to determine when it is safe to re map the first HSA PCI memory space from direct storage access to emulated storage access on the migration source machine.

PF API module saves a snapshot of the metadata device state from the registers of the VF . An error is returned if the VF is enabled to access either the first request queue or the first response queue . These capabilities are utilized during live migration in order to maintain the state of the first request queue and the first response queue when re map the first HSA PCI memory space from direct storage access to emulated storage access in the course of migrating a virtual machine from migration source to migration destination machines.

Referring again to live migration of a virtual machine such as VM from the source host machine to the destination host machine involves temporarily suspending all operation of the VM in the course of the migration. Graceful migration of the VM involves suspending its operation in a manner such that operations of the VM are not disrupted despite the temporary suspension of its operation. In accordance with some embodiments when a VM is relocated in the course of a direct storage access via a VF precautions are taken to avoid loss of SCSI IO requests and or SCSI IO responses that may be in flight during the time interval when the VM operations are temporarily suspended.

As mentioned above PF includes module that sets a maximum number of outstanding SCSI IO requests for the VF .

The process calls module which disables the VF from attempting to de queue any SCSI IO requests from the first SCSI IO request queue . Thus operation of the first SCSI IO request queue is suspended. The process calls module which flushes SCSI IO requests from the VF request queue i.e. transmits the SCSI IO request to storage to ensure that the VF request queue is empty. The process calls decision module which determines whether the VF has sent to physical storage SCSI IO storage requests for which it has not yet received IO completions. If there are SCSI IO requests that the VF previously de queued from the first request queue and sent to storage but for which it has not yet received completions then the decision module awaits completions for all such outstanding SCSI IO storage requests. If there are no more outstanding SCSI IO requests then process control passes to module which disables the IOs from being completed into queue . Next module saves a snapshot of the VF queue metadata device state from the registers of the VF .

As indicated by arrow a stun control module within the hypervisor kernel sends a message to the virtual machine that stuns the VM i.e. halts execution of instructions so that it does not generate new SCSI requests for delivery to the VF and so that it does not consume SCSI responses received by the VF from persistent storage . As indicated by arrow a state retrieval module within the hypervisor kernel sends a message to the PF driver to request that the PF obtain queue metadata state information for the first request queue and the first response queue from registers of the VF . In response to the message from the module the PF driver sends a request to the PF to retrieve the metadata state information . In response to the request from the PF driver the PF which is a component of the adapter runs the process of to obtain a snapshot of the state information from the registers associated with the VF . The PF passes the state information snapshotted from registers to the PF driver . As indicated by arrow the PF driver in turn passes the state information to the state retrieval module .

Also in response to the stun control message indicated by arrow by stun control module a memory region particularly a device state page is allocated within VMM i.e. within a virtualization intermediary to store the state information snapshotted i.e. copied from the VF registers . As indicated by arrow module saves to the device memory i.e. state page the device state information snapshotted from the VF registers . Moreover in response to the stun control message indicated by arrow the hypervisor kernel causes a re mapping of the first HSA PCI memory space to point to the newly allocated device state page as indicated by line .

Once the state information has been saved to the device state page in the VMM i.e. within the virtualization intermediary module sends an unsuspend or unstun message indicated by arrow to cause the VM to resume execution of instructions. It will be appreciated that alternatively the stun and unstun of the VM optionally may not be performed since disabling the request queue ensures that no additional requests can be sent in the course of the re mapping of the first HSA PCI memory space to point to the newly allocated device state page as indicated by line . In that alternative embodiment snappshotting and re mapping follow disabling of the VF from attempting to de queue SCSI IO requests from the first SCSI IO request queue .

Following re mapping and resumption of operation in the case of use of stun unstun of the VM however the first HSA PCI memory space which has been re mapped maps SCSI IO requests responses to the device state page which is within the VMM and which contains the snapshotted state information and . After the transfer of the state information to the device state page the hypervisor intervenes in the processing of such SCSI IO requests so as to provide emulated processing of such requests requests.

The hypervisor part of the virtualization intermediary intervenes in de queuing of requests from the first request queue and in transmitting the de queued requests to the physical storage . The hypervisor also intervenes in inserting responses received from the physical storage to the first response queue . In the course of intervening the hypervisor may trap IO requests and IO responses and related instructions so as to be able to stop the VM from operating on the source machine at any instruction boundary and save processing state so that the processing state can be transferred to the destination machine transparently and seamlessly without loss of information. More particularly since the hypervisor can trap instructions and save state information it can in effect control the flow of guest OS initiated IO during live migration e.g. track and quiesce guest OS initiated on the source machine and also can resume the IO on the destination machine . Thus it will be appreciated that through the process described with reference to the hypervisor kernel becomes interposed between the VM and storage so that it can readily control the process of migrating the VM from the source machine to the destination machine .

Following storage of the device state and to page and remapping of the first HSA PCI memory space to that page indicated by arrow VM migration then proceeds generally as described in U.S. Pat. No. 7 680 919 which is expressly incorporated herein by this reference. In preparation for VM migration in order to minimize the down time for the running source VM during the migration process the destination VM shown in is powered on in the destination machine and is put into a wait state. Memory pages are iteratively copied from the source VM on the source host machine to the destination VM on the destination machine while the source VM is still running on the source machine .

In particular once the destination VM is ready the source VM is suspended long enough using known techniques that its non memory state information can be transferred to and saved in the destination VM. The physical memory of the source VM that is the contents of the virtual memory which the VM views as physical memory is dealt with differently than all other non memory state information. This is because memory is too large to transfer all at once after the source VM has suspended execution.

In some embodiments as much physical memory as possible is transferred asynchronously while the source VM is running. A thread not shown is created preferably in the source VM s VMM whose job it is to push that is pre copy all of the source VM s memory over to the destination machine . The thread iterates through all physical pages and does at least the following a The physical page number associated with the physical page is write protected by the source VMM using known procedures and b The thread writes the page over to a destination machine kernel. The transferred pages include one or more pages not shown containing contents of the first request queue and the first response queue and the page containing the state information pertaining to those queues .

If any of the pages that were transferred are modified the VMM detects this because a write protect fault will be taken on the page then the page is marked as modified. Marking may be done as simply and compactly as setting a bit in a table or vector for the modified page. Note that the first time the destination VM touches any page that has not been copied to the destination machine it will require a network page fault to the source machine this fault is then used as a signal to immediately transfer the needed page. Once all pages have been transferred the source VM sends a list of modified pages to a kernel of the destination VM so it knows that these pages need to be paged in from the source VM .

Thus it also is possible to pre copy the source VM s physical memory to the destination iteratively that is over multiple passes before the non memory state is transferred. According to this iterative memory transfer procedure a first set preferably all of pages of the source VM s memory is pre copied to the destination VM . During the time the memory is being transferred however the source VM which is allowed to continue running may modify some of the transferred pages. Modifications to the source VM s memory may be detected and tracked using any known method such as a write protection mechanism or a separate table indicating memory modifications. These modified pages are then re transferred to the destination VM . While they are being transferred however the source VM may modify other pages or even modify a previously modified page again . The newly modified pages are then retransferred and so on.

The system repeats the iterative memory transfer procedure until the number of pages left to be transferred is less than some threshold or the system notices that no forward progress is being made i.e. no reduction in the number of newly modified pages still to be transferred . The threshold which may be zero may be determined as an absolute or relative number of pages either ahead of time or according to any known adaptive routine.

Each subsequent iteration should take less time because fewer pages will need to be transferred the transfer process should therefore converge towards a number of modified pages that is small enough that they can transferred rapidly. Any newly modified pages remaining to be copied over after the threshold has been reached may then be transferred after the source VM is suspended and before non memory state is copied over alternatively these remaining pages may be paged in by the destination VM either on demand or asynchronously after the source VM is suspended and the destination VM is resumed from the suspended source state from where the source VM was stopped.

It will be appreciated that during the iterative transfer of pages from the source VM to the destination VM the source VM which has been unstunned may continue to conduct IO using the first request queue and using the first response queue . The transfer of the snapshotted of the state information and which was created originally in the VF ensures that processing of IOs through those queues and picks up after remapping and unstunning of the VM where it left off prior to the stunning and remapping of the VM . Moreover since following remapping processing of the queues and involves the VMM and the hypervisor kernel the state information and of page can be transferred over to the destination VM just like any other memory page.

In the course of the migration process described above assuming that an SR IOV capable adapter exists on the destination machine and that the virtual function has been provisioned for the destination VM and further assuming that the saved VF device interface response queue is empty then following snapshotting restoring and re mapping the VF device interface emulation via a virtualization intermediary not shown on the destination machine is dismantled and the first HSA PCI memory space not shown of the destination VM is re mapped for direct access to physical storage via a VF on the SR IOV storage adapter of the destination machine .

However if the destination machine has an SR IOV adapter but the saved VF device interface response queue is not empty then the destination VM continues to use a VF device interface emulation on the destination machine until the response queue becomes empty and then dismantles the device emulation. Once the VF device emulation has been dismantled and the destination VM is configured for direct IO storage access over the VF the VF device interface is activated using a checkpointed snapshot from the emulated device interface of the migration source VM .

On the other hand if the destination machine has no SR IOV capable adapter then the destination VM continues using the VF device emulation on the destination machine .

The foregoing description and drawings of embodiments in accordance with the present invention are merely illustrative of the principles of the invention. Therefore it will be understood that various modifications can be made to the embodiments by those skilled in the art without departing from the spirit and scope of the invention which is defined in the appended claims.

