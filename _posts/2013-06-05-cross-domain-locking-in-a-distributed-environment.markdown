---

title: Cross domain locking in a distributed environment
abstract: In a distributed system, multiple nodes of a cluster update target data using a cross-domain lock. In the distributed system, data is separated into different domains, where some data elements are part of multiple domains. Multiple nodes each store a copy of the target data, which can be part of a single domain, or part of multiple domains. Where at least one element of the target data is part of two different domains, the nodes use cross-domain locks to lock both domains for at least a portion of the data update, and update the data while the lock is active. After updating the data, the nodes can release the cross-domain lock.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09152687&OS=09152687&RS=09152687
owner: NetApp, Inc.
number: 09152687
owner_city: Sunnyvale
owner_country: US
publication_date: 20130605
---
Embodiments described are related generally to data coherency and embodiments described are more particularly related to using cross domain locking for data updates in a distributed environment.

Portions of the disclosure of this patent document can contain material that is subject to copyright protection. The copyright owner has no objection to the reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever. The copyright notice applies to all data as described below and in the accompanying drawings hereto as well as to any software described below. Copyright 2013 NetApp Inc. All Rights Reserved.

Distributed systems may include multiple nodes storing copies of the same data locally at the nodes. In some distributed systems there is a strong consistency requirement throughout the system making it important to implement procedures to maintain consistency of data stored by the nodes. Examples of distributed systems having consistency requirements can include data serving systems where multiple caches access the same storage. Each cache can be considered a node in the distributed system.

One approach to consistency requirements in a distributed system is the Two Phase Commit Protocol 2PC . 2PC is a blocking protocol that operates from two phases a vote phase and a commit phase. It will be understood that there are other ways to maintain consistency within the distributed system. Data consistency operations typically lock the data during updates to prevent access to the data. The locking of the data can prevent data becoming inconsistent while the updates occur but it can negatively affect system performance during the update process.

Descriptions of certain details and embodiments follow including a description of the figures which can depict some or all of the embodiments described below as well as discussing other potential embodiments or implementations of the inventive concepts presented herein.

In a distributed system multiple nodes of a cluster update target data using a cross domain lock. In the distributed system data is separated into different domains where some data elements are part of multiple domains. Multiple nodes each store a copy of the target data which can be part of a single domain or part of multiple domains. Where at least one element of the target data is part of two different domains the nodes use cross domain locks to lock both domains for at least a portion of the data update and update the data while the lock is active. After updating the data the nodes can release the cross domain lock.

By separating data into different domains the system can limit locking to a specific domain for a data update. However there are cases in which a data element is part of multiple domains which can be referred to as overlapping data. Cross domain locking can address circumstances where overlapping data exists or where data elements are part of multiple domains. A distributed system is a networked system of nodes or entities and data accessible by the nodes where copies of an element of data can be in one or more nodes within the system. An example of a distributed system is a system of networked caches that provides access to data from a central store or database of information. A node can be any type of computing device such as a server storage server or other networked device that can store or cache a copy of an element of the data. The data element can be an atomic value within the system for example a 32 bit number in a 32 bit system or a group of atomic values for example a file data block or array.

In one embodiment cross domain locking can be used with cache coherency techniques. For example a system can employ cross domain locking to implement 2 phase commit protocol 2PC with a locking hierarchy in an asynchronous distributed environment. It will be understood that 2PC provides for locking during its two phases as a mechanism for ensuring coherency. However cross domain locking can provide an additional layer of locking that covers a scenario in which protected data is overlapped between different domains. The system can implement a cross domain lock as a higher order 2PC lock to synchronize updates from 2PC domain based locks. In one embodiment the system can hold or maintain the locks to ensure that each node has committed an update transaction prior to releasing the lock.

 Cohorts may be understood to specifically refer to nodes that participate in a particular group and will be updated together. For example a cohort can refer to a node that caches data whereas another node may not cache certain data and thus not be in the cohort with respect to that data. As another example multiple databases can be distributed through a network and certain nodes cache one database but not another. Thus not all nodes will include all the same data elements and updating can be specific to a group or set to which a node belongs. Thus shared data is not necessarily identical in every node but is data which is shared with at least some other node in system .

In one embodiment one or more of nodes is a storage server with caching that has a distributed architecture. A storage server having a distributed architecture can be implemented with multiple distributed storage servers. The distributed architecture can include physically separate network modules N modules and data disk modules D modules not shown . The separate modules can communicate with other storage servers and with a data coordinator for purposes of cache coherency over an external interconnect. The N module acts as a front end of the storage server exporting services to clients while the D module acts as a backend managing the underlying storage of the storage server. The N and D modules can be contained in separate housings and communicate with each other via network connections. Thus the N module and the D module enable a storage server to be physically separated into multiple modules that can be distributed across a network where individual modules can be added according to need e.g. add more N modules for a very active front end or more D modules for heavy data access processing operations in a modular fashion.

Clients represent other systems and or programs that make requests for data stored on or managed by nodes . Nodes typically communicate with each other over network . Clients can also interact with nodes over network . Network can be any type or combination of local area networks and or wide area networks. Network includes hardware devices to switch and or route traffic from clients to nodes typically as sequences and or groups of data packets. The hardware devices communicate over network channels via one or more protocols as is understood in the art.

Each node includes corresponding update logic through N 1 respectively . Update logic enables nodes to perform a coordinated data update with one or more other nodes. The node that initiates the data update can be referred to as an originator or a coordinator while the other node or nodes are participant s . In one embodiment the originator processes a request from a client that causes the update to protected data that is kept coherent in distributed system . To maintain data consistency in the system the originator can propose the update to other participants. As described in more detail below for example with respect to update logic or nodes implement cross domain locks for shared data that is subject to an update. The data that is the subject of the update can be referred to as target data. 

In one embodiment update logic exists as code executed on node . In one embodiment update logic is a module that is part of an operating system of node . In one embodiment update logic is at least partially implemented in hardware within node . In one embodiment nodes use update logic to coordinate all coherency operations. Nodes can receive proposals or updates as participants via update logic . Nodes can generate proposals via update logic . Thus update logic represents mechanisms used to generate and or respond to update operations.

As mentioned above data stored in shared data and can be organized as multiple domains. For simplification only three data elements of protected data and are illustrated. Domain is shown including elements and . Domain is shown including elements and . Thus data element is shared among multiple domains and is shown as cross domain . The organization of data into domains through M is implementation specific and is generally determined by an administrator prior to a time in which data is updated. During data updates within domain update logic not specifically shown locks data elements within the domain including element and for update operations. Similarly during data updates within domain update logic locks data elements within the domain including data elements and for update operations.

When protected data is overlapped between different domains such as between domains and domain based update transactions could potentially overwrite each other and cause unpredictable results. Thus in addition to domain based locks for domain based update operations system implements higher order locks referred to as cross domain locks to synchronize domain based updates. A data updater or update logic can determine that at least a portion of the target data is part of multiple domains and implement cross domain locks.

The update logic can implement cross domain locks in one of several ways. In one embodiment when the update logic determines that a data domain includes overlapping data or data that is part of multiple domains the update logic temporarily redefines the data domain boundaries. Thus for example domains and can be combined into a single domain for purposes of the data updating. Nodes and can coordinate data updates with the two domains defined as a single domain and then release the lock on both domains and restore the data domain boundaries. Such an approach benefits from simplicity even though it will reduce overall throughput by penalizing data updates of non overlapping data. For example while system can avoid unpredictable results by locking domains and while updating element the system causes a performance penalty by locking domain while updating element .

In one embodiment the update logic separates data updates into two groups of updates. One group of updates includes updates for non overlapped data which are coordinated using domain based update transactions. The other group of updates includes updates for overlapped data which are coordinated using cross domain transactions. Cross domain transactions can be reserved for use only when overlapping data exists in the target data. Such an approach can separate operations that need a cross domain lock from those which do not even though such an implementation would likely require update logic in both types of update transactions. For example nodes and would include separate update logic for domain based transaction and cross domain transactions.

In one embodiment the update logic coordinates all data updates through domain based update transactions or local domain transactions local domain referring to a single domain for the target data meaning no overlapping data is present . Whenever overlapping data is affected by the data update the update logic first acquires a cross domain transaction before initiating a domain based transaction to perform the update. Such an implementation can maximize parallelism potential by allowing updates to non overlapping data without initiating a cross domain lock but using the cross domain lock when needed. Such an approach also centralizes updates to update logic of domain based transactions.

Coordinating all data updates through domain based update transactions can further be separated into different sub approaches. In one embodiment all nodes participate in the cross domain transactions for the duration of the cross domain lock. Such an approach can be referred to as a consensus transaction and is illustrated in . In another embodiment only the originator node participates in the cross domain transaction synchronously and the participant s perform the cross domain operations asynchronously. Such an approach can be referred to as a targeted transaction and is illustrated in .

It will be understood that the target data that is the subject of the update transaction includes overlapping data. Thus originator enters a cross domain lock prior to updating the data. System specifically illustrates a vote phase and a commit phase. It will be understood that such phases are used in 2 Phase Commit 2PC but that other phases of other protocols could be used or simply other operations of update coordination could be used. For the sake of example the update transactions of will be assumed to be 2PC transactions even though such examples are not limiting to the general principle of using cross domain locking.

In one embodiment originator enters cross domain vote and performs the vote with participant participating. Thus both nodes are locked synchronously for the vote. When participant and originator both agree to enter the cross domain transaction in vote the nodes enter cross domain commit . Participant participates with originator in cross domain commit . Thus both nodes are locked synchronously for the commit.

During cross domain commit nodes and perform domain based transactions or local domain transactions vote and commit which can be 2PC vote and 2PC commit phases . The cross domain transactions can be considered of a higher order than the domain based transactions. Thus in one embodiment system can be said to have a hierarchy of locks or a hierarchy of transactions. In one embodiment 2PC or another consistency protocol can be made hierarchical.

With cross domain consensus transactions participant is not aware of when to release a cross domain transaction. The inability of participant to know when to release the cross domain 2PC transaction can result in undesirable behavior in the case of a failure. If originator encounters catastrophic failure between domain based vote and domain based commit participant would theoretically wait forever for completion of the domain based transaction. It will be understood that if participant attempts to implement a timer to release cross domain transaction domain based commit could arrive after the timer expires because of the asynchronous nature of the distributed environment of system . Thus participant cannot rely on timers to release the transactions and there is a possibility of hanging the system if originator fails during cross domain commit .

Whereas in system both the originator and the participant participated together in the cross domain portions of the update in system only originator is aware of the cross domain locks. As long as originator maintains the cross domain lock it will only propose transactions or update operations that are consistent with the cross domain locking of overlapping data. Thus even without participant being aware of the cross domain lock or participating directly in the cross domain transaction the coordination of originator and participant ensures the same protection for data updates as the interaction in system .

In the targeted cross domain transaction originator enters cross domain vote but participant does not enter cross domain vote . Originator then enters targeted cross domain commit . Similarly participant does not enter cross domain commit . Thus only originator needs to participate in the higher level lock e.g. the cross domain lock . Originator proposes the actual update transaction to participant while the cross domain lock is active. Originator can propose domain based vote to participant .

It will be understood that while participant does not necessarily need to participate directly in the cross domain lock the asynchronous nature of operations among the nodes could result in an atomicity violation of a data update e.g. a 2PC update for overlapped data without an ordering mechanism. The atomicity of operations can be maintained when participant implements processing queue . Processing queue allows participant to receive and queue transaction operations received from originator as well as other nodes in the cluster. When implemented in order e.g. by executing the queue via a FIFO first in first out principle participant executes the first operations prior to execution of subsequent operations. Thus all operations can occur in order as they occur within originator under the control of the cross domain lock.

As illustrated participant can queue each domain based transaction e.g. domain based vote received from originator and continue to execute whatever transaction s may already be in the queue. After completion of the transaction s in the queue that are ahead of the transaction proposed by originator participant can initiate the next transaction . Thus originator proposes domain based vote and subsequently domain based commit which participant queues. At any point when currently executing transactions complete participant can execute the transaction s proposed by originator . In one embodiment originator awaits an acknowledgement by participant of domain based commit prior to release targeted cross domain commit .

In one embodiment participant only queues overlapping data with processing queue . Thus participant can determine when a domain based transaction received from originator is directed to targeted data for which a transaction already exists in participant . If a domain based transaction is not directed to overlapping data participant can simply execute the transaction in any order and or in parallel with other transactions. If the domain based transaction is directed to overlapping data as indicated by the fact that another transaction is directed to the same target data participant can queue the data to ensure it is executed subsequently to the previous transaction.

If the data update does cross data domains block YES branch the node will use cross domain locks to perform the data update. The specific implementation of the cross domain lock can be dependent on a configuration of the system and a configuration of the data updater of the receiving node which is the originator node for purposes of the data update operations. If system does not implement targeted updating block NO branch the originator proposes a cross domain transaction to the participant node s block . In one embodiment the cross domain transaction includes initiating and performing a cross domain vote with the participant s block . The participant s can respond affirmatively to the vote to affirm the cross domain transaction. In one embodiment the originator and the participant s perform a cross domain commit block .

If the system implements targeted updating block YES branch the originator proposes the update transaction to itself block . In one embodiment the originator performs a cross domain vote locally or without the participant nodes s block . The originator can thus enter a voting phase for the cross domain transaction and send or generate a YES vote for the cross domain transaction. In one embodiment the originator then performs a cross domain commit block which is also local to itself without the participation of other nodes. Thus the originator can enter a commit phase for the cross domain transaction without sending or generating an acknowledgement. The acknowledgement would close the cross domain transaction and so it should not be sent prior to the nodes performing domain based transactions for the update.

Whether the cross domain transaction is executed locally to the originator or by consensus with the originator and participant s when the cross domain lock is active the originator can propose a domain based transaction e.g. a 2PC transaction to update the overlapping data block . Both the originator and the participant s perform the domain based vote block . Thus the nodes can enter a voting phase and vote YES for the domain based transaction. Both the originator and the participant s perform the domain based commit block . Thus for example the nodes can enter a commit phase for the domain based transaction and update the target data. Typically the nodes send an acknowledgement after completing the domain based transaction which can operate to close out the domain based transaction block . In alternative implementations another form of closing the transaction can be performed. Similarly the originator can close the cross domain transaction by sending or generating an acknowledgement or through use of another mechanism block .

If the system does not implement targeted updating block NO branch the participant will participate with the originator in the cross domain transaction and corresponding cross domain lock. It will be understood that the system implements a cross domain lock in conjunction with or as part of a cross domain transaction just as the system implements a domain based lock in conjunction with or as part of a domain based transaction. The participant performs a cross domain vote with the originator block . The participant then receives a cross domain commit proposal from the originator block .

If the system implements targeted updating block YES branch the participant processes the update transaction including determining if another transaction is in process block . In an implementation where targeted updating is used the participant will be unaware of the cross domain transaction which is only known to the originator. In one embodiment the participant only checks to see if another transaction on the same target data is in process. If the participant has another transaction pending or running block YES branch the participant queues the transaction block . The participant can then proceed to execute the in process transaction prior to executing the transaction for overlapping target data.

When the participant terminates the in process transaction block NO branch the participant then processes the next queued transaction block . It will be understood that there could be multiple transactions queued but for purposes of simplicity assume that the processing of the transaction in block refers to the transaction received in block . The process will work the same for each new transaction received queued and subsequently processed. In one embodiment the participant creates a new registry entry for the new transaction.

The participant performs a domain based vote block . The domain based vote will be for any new transaction received where targeted updating is used and will be performed with the originator in the cross domain commit when consensus updating is used. For example the participant can enter a vote phase of a domain based 2PC transaction in response to processing the proposal from the originator and send a YES vote to the domain based transaction. The participant performs a domain based commit after the vote block . For example the participant can enter a commit phase for the domain based transaction and update the target data locally block . In one embodiment the participant sends an acknowledgement to the originator block . In one embodiment the acknowledgement closes out domain based transaction. If other transactions are pending in the queue the participant can then commence the next queued domain based transaction.

Storage of data in storage units is managed by storage servers which receive and respond to various read and write requests from clients directed to data stored in or to be stored in storage units . Storage units constitute mass storage devices which can include for example flash memory magnetic or optical disks or tape drives illustrated as disks disk A B . Storage devices can further be organized into arrays not illustrated implementing a Redundant Array of Inexpensive Disks Devices RAID scheme whereby storage servers access storage units using one or more RAID protocols known in the art.

Storage servers can provide file level service such as used in a network attached storage NAS environment block level service such as used in a storage area network SAN environment a service which is capable of providing both file level and block level service or any other service capable of providing other data access services. Although storage servers are each illustrated as single units in a storage server can in other embodiments constitute a separate network element or module an N module and disk element or module a D module . In one embodiment the D module includes storage access components for servicing client requests. In contrast the N module includes functionality that enables client access to storage access components e.g. the D module and the N module can include protocol components such as Common Internet File System CIFS Network File System NFS or an Internet Protocol IP module for facilitating such connectivity. Details of a distributed architecture environment involving D modules and N modules are described further below with respect to and embodiments of a D module and an N module are described further below with respect to .

In one embodiment storage servers are referred to as network storage subsystems. A network storage subsystem provides networked storage services for a specific application or purpose and can be implemented with a collection of networked resources provided across multiple storage servers and or storage units.

In the embodiment of one of the storage servers e.g. storage server A functions as a primary provider of data storage services to client . Data storage requests from client are serviced using disks A organized as one or more storage objects. A secondary storage server e.g. storage server B takes a standby role in a mirror relationship with the primary storage server replicating storage objects from the primary storage server to storage objects organized on disks of the secondary storage server e.g. disks B . In operation the secondary storage server does not service requests from client until data in the primary storage object becomes inaccessible such as in a disaster with the primary storage server such event considered a failure at the primary storage server. Upon a failure at the primary storage server requests from client intended for the primary storage object are serviced using replicated data i.e. the secondary storage object at the secondary storage server.

It will be appreciated that in other embodiments network storage system can include more than two storage servers. In these cases protection relationships can be operative between various storage servers in system such that one or more primary storage objects from storage server A can be replicated to a storage server other than storage server B not shown in this figure . Secondary storage objects can further implement protection relationships with other storage objects such that the secondary storage objects are replicated e.g. to tertiary storage objects to protect against failures with secondary storage objects. Accordingly the description of a single tier protection relationship between primary and secondary storage objects of storage servers should be taken as illustrative only.

In one embodiment system includes data updater updater A B which include logic to perform data updates in a distributed system. Updater is configured to perform a cross domain transaction and or cross domain lock to perform updates for overlapping data. Updater can be implemented in hardware and or software of storage server .

Nodes can be operative as multiple functional components that cooperate to provide a distributed architecture of system . To that end each node can be organized as a network element or module N module A B a disk element or module D module A B and a management element or module M host A B . In one embodiment each module includes a processor and memory for carrying out respective module operations. For example N module can include functionality that enables node to connect to client via network and can include protocol components such as a media access layer Internet Protocol IP layer Transport Control Protocol TCP layer User Datagram Protocol UDP layer and other protocols known in the art.

In contrast D module can connect to one or more storage devices via cluster switching fabric and can be operative to service access requests on devices . In one embodiment the D module includes storage access components such as a storage abstraction layer supporting multi protocol data access e.g. Common Internet File System protocol the Network File System protocol and the Hypertext Transfer Protocol a storage layer implementing storage protocols e.g. RAID protocol and a driver layer implementing storage device protocols e.g. Small Computer Systems Interface protocol for carrying out operations in support of storage access operations. In the embodiment shown in a storage abstraction layer e.g. file system of the D module divides the physical storage of devices into storage objects. Requests received by node e.g. via N module can thus include storage object identifiers to indicate a storage object on which to carry out the request.

Also operative in node is M host which provides cluster services for node by performing operations in support of a distributed storage system image for instance across system . M host provides cluster services by managing a data structure such as a relational database RDB RDB A B which contains information used by N module to determine which D module owns services each storage object. The various instances of RDB across respective nodes can be updated regularly by M host using conventional protocols operative between each of the M hosts e.g. across network to bring them into synchronization with each other. A client request received by N module can then be routed to the appropriate D module for servicing to provide a distributed storage system image.

Similar to what is described above system includes data updater updater A B which include logic to perform data updates in a distributed system. Updater is configured to perform a cross domain transaction and or cross domain lock to perform updates for overlapping data. Updater can be implemented in hardware and or software of storage server .

It will be noted that while shows an equal number of N and D modules constituting a node in the illustrative system there can be different number of N and D modules constituting a node in accordance with various embodiments. For example there can be a number of N modules and D modules of node A that does not reflect a one to one correspondence between the N and D modules of node B. As such the description of a node comprising one N module and one D module for each node should be taken as illustrative only.

Memory includes storage locations addressable by processor network adapter and storage adapter for storing processor executable instructions and data structures associated with a multi tiered cache with a virtual storage appliance. A storage operating system portions of which are typically resident in memory and executed by processor functionally organizes the storage server by invoking operations in support of the storage services provided by the storage server. It will be apparent to those skilled in the art that other processing means can be used for executing instructions and other memory means including various computer readable media can be used for storing program instructions pertaining to the inventive techniques described herein. It will also be apparent that some or all of the functionality of the processor and executable software can be implemented by hardware such as integrated currents configured as programmable logic arrays ASICs and the like.

Network adapter comprises one or more ports to couple the storage server to one or more clients over point to point links or a network. Thus network adapter includes the mechanical electrical and signaling circuitry needed to couple the storage server to one or more client over a network. Each client can communicate with the storage server over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

Storage adapter includes a plurality of ports having input output I O interface circuitry to couple the storage devices e.g. disks to bus over an I O interconnect arrangement such as a conventional high performance FC or SAS Serial Attached SCSI Small Computer System Interface link topology. Storage adapter typically includes a device controller not illustrated comprising a processor and a memory for controlling the overall operation of the storage units in accordance with read and write commands received from storage operating system . As used herein data written by a device controller in response to a write command is referred to as write data whereas data read by device controller responsive to a read command is referred to as read data. 

User console enables an administrator to interface with the storage server to invoke operations and provide inputs to the storage server using a command line interface CLI or a graphical user interface GUI . In one embodiment user console is implemented using a monitor and keyboard.

In one embodiment computing device includes data updater which includes logic to perform data updates in a distributed system. While shown as a separate component in one embodiment data updater is part of other components of computer . Data updater is configured to perform a cross domain transaction and or cross domain lock to perform updates for overlapping data.

When implemented as a node of a cluster such as cluster of the storage server further includes a cluster access adapter shown in phantom having one or more ports to couple the node to other nodes in a cluster. In one embodiment Ethernet is used as the clustering protocol and interconnect media although it will be apparent to one of skill in the art that other types of protocols and interconnects can by utilized within the cluster architecture.

Multi protocol engine includes a media access layer of network drivers e.g. gigabit Ethernet drivers that interface with network protocol layers such as the IP layer and its supporting transport mechanisms the TCP layer and the User Datagram Protocol UDP layer . The different instances of access layer IP layer and TCP layer are associated with two different protocol paths or stacks. A file system protocol layer provides multi protocol file access and to that end includes support for the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A VI virtual interface layer implements the VI architecture to provide direct access transport DAT capabilities such as RDMA as required by the DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the storage server. In certain cases a Fibre Channel over Ethernet FCoE layer not shown can also be operative in multi protocol engine to receive and transmit requests and responses to and from the storage server. The FC and iSCSI drivers provide respective FC and iSCSI specific access control to the blocks and thus manage exports of luns logical unit numbers to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing blocks on the storage server.

The storage operating system also includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on storage devices. Information can include data received from a client in addition to data accessed by the storage operating system in support of storage server operations such as program application data or other system data. Preferably client data can be organized as one or more logical storage objects e.g. volumes that comprise a collection of storage devices cooperating to define an overall logical arrangement. In one embodiment the logical arrangement can involve logical volume block number vbn spaces wherein each volume is associated with a unique vbn.

File system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustrated as a SCSI target module . SCSI target module is generally disposed between drivers and file system to provide a translation layer between the block lun space and the file system space where luns are represented as blocks. In one embodiment file system implements a WAFL write anywhere file layout file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using a data structure such as index nodes or indirection nodes inodes to identify files and file attributes such as creation time access permissions size and block location . File system uses files to store metadata describing the layout of its file system including an inode file which directly or indirectly references points to the underlying data blocks of a file.

Operationally a request from a client is forwarded as a packet over the network and onto the storage server where it is received at a network adapter. A network driver such as layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to file system . There file system generates operations to load retrieve the requested data from the disks if it is not resident in core i.e. in memory . If the information is not in memory file system accesses the inode file to retrieve a logical vbn and passes a message structure including the logical vbn to the RAID system . There the logical vbn is mapped to a disk identifier and device block number disk dbn and sent to an appropriate driver of disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the storage server. Upon completion of the request the node and operating system returns a reply to the client over the network.

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the storage server adaptable to the teachings of the invention can alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path can be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware embodiment increases the performance of the storage service provided by the storage server in response to a request issued by a client. Moreover in another alternate embodiment of the invention the processing elements of adapters can be configured to offload some or all of the packet processing and storage access operations respectively from processor to increase the performance of the storage service provided by the storage server. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

When implemented in a cluster data access components of the storage operating system can be embodied as D module for accessing data stored on disk. In contrast multi protocol engine can be embodied as N module to perform protocol termination with respect to a client issuing incoming access over the network as well as to redirect the access requests to any other N module in the cluster. A cluster services system can further implement an M host e.g. M host to provide cluster services for generating information sharing operations to present a distributed file system image for the cluster. For instance media access layer can send and receive information packets between the various cluster services systems of the nodes to synchronize the replicated databases in each of the nodes.

In addition a cluster fabric CF interface module CF interface modules A B can facilitate intra cluster communication between N module and D module using a CF protocol . For instance D module can expose a CF application programming interface API to which N module or another D module not shown issues calls. To that end CF interface module can be organized as a CF encoder decoder using local procedure calls LPCs and remote procedure calls RPCs to communicate a file system command between D modules residing on the same node and remote nodes respectively.

In one embodiment data updater includes logic layers for the operating system to perform data updates in a distributed system. Data updater is configured to perform a cross domain transaction and or cross domain lock to perform updates for overlapping data in accordance with any embodiment described above. In one embodiment tracing engine is implemented on existing functional components of a storage system in which operating system executes.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and can implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

As used herein instantiation refers to creating an instance or a copy of a source object or source code. The source code can be a class model or template and the instance is a copy that includes at least some overlap of a set of attributes which can have different configuration or settings than the source. Additionally modification of an instance can occur independent of modification of the source.

Flow diagrams as illustrated herein provide examples of sequences of various process actions. Although shown in a particular sequence or order unless otherwise specified the order of the actions can be modified. Thus the illustrated embodiments should be understood only as an example and the process can be performed in a different order and some actions can be performed in parallel. Additionally one or more actions can be omitted in various embodiments thus not all actions are required in every embodiment. Other process flows are possible.

Various operations or functions are described herein which can be described or defined as software code instructions configuration and or data. The content can be directly executable object or executable form source code or difference code delta or patch code . The software content of the embodiments described herein can be provided via an article of manufacture with the content stored thereon or via a method of operating a communications interface to send data via the communications interface. A machine readable medium or computer readable medium can cause a machine to perform the functions or operations described and includes any mechanism that provides i.e. stores and or transmits information in a form accessible by a machine e.g. computing device electronic system or other device such as via recordable non recordable storage media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices or other storage media or via transmission media e.g. optical digital electrical acoustic signals or other propagated signal . A communication interface includes any mechanism that interfaces to any of a hardwired wireless optical or other medium to communicate to another device such as a memory bus interface a processor bus interface an Internet connection a disk controller. The communication interface can be configured by providing configuration parameters and or sending signals to prepare the communication interface to provide a data signal describing the software content.

Various components described herein can be a means for performing the operations or functions described. Each component described herein includes software hardware or a combination of these. The components can be implemented as software modules hardware modules special purpose hardware e.g. application specific hardware application specific integrated circuits ASICs digital signal processors DSPs etc. embedded controllers hardwired circuitry etc.

Besides what is described herein various modifications can be made to the disclosed embodiments and implementations without departing from their scope. Therefore the illustrations and examples herein should be construed in an illustrative and not a restrictive sense.

