---

title: Snapping of object features via dragging
abstract: Example systems and methods of performing a snapping operation in a graphical user interface are presented. In one example, a first user input indicating an initiation of a dragging operation in the graphical user interface is received, wherein the first user input indicates a first location. A source feature corresponding to a source object is determined based on the first location. Also received is a second user input during the dragging operation in the graphical user interface, in which the second user input indicates a current location. A current target feature corresponding to a current target object is determined based on the current location. The source feature corresponding to the source object is snapped to the current target feature corresponding to the current target object.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09250786&OS=09250786&RS=09250786
owner: Adobe Systems Incorporated
number: 09250786
owner_city: San Jose
owner_country: US
publication_date: 20130716
---
This application relates generally to data processing and in an example embodiment to the graphical snapping of object features by way of a single dragging operation in a graphical user interface.

In using many graphical drawing software applications such as Adobe After Effects or Microsoft Visio users often desire to connect abut or otherwise couple two or more different graphical shapes objects or layers together in some precise manner. For example a user may want to position an end of an arrow with a box in a flow diagram such that the end of the arrow makes contact with a particular point on an edge of the box. In many cases positioning a specific part of the arrow accurately such as by use of a mouse or touch input may be extremely difficult possibly requiring multiple user attempts.

To facilitate the positioning of these objects many drawing applications provide an automatic aligning or snapping function whereby a user may position two or more objects by dragging one of the objects in close proximity to another using a mouse or similar input device. In response the application may automatically position the object being dragged to some location predetermined by the application relative to the other object. The location may be for example some point on the object itself or some reference point in the graphical user interface such as a grid point or guide point. As a result the user may possess little or no control over which aspects or features of the objects are snapped together.

Some applications such as Trimble SketchUp Autodesk Maya Autodesk AutoCAD and Cinema 4D by MAXON Computer allow more user controllable selection of features or aspects of the multiple objects being aligned or snapped. To access this additional control users typically are directed to provide multiple input actions or pre selection of snapping features in addition to the actual dragging operation. These input actions may include for example selection of special menu items movement of special purpose graphical objects selection of object features of interest and the like.

The description that follows includes illustrative systems methods techniques instruction sequences and computing machine program products that exemplify illustrative embodiments. In the following description for purposes of explanation numerous specific details are set forth in order to provide an understanding of various embodiments of the inventive subject matter. It will be evident however to those skilled in the art that embodiments of the inventive subject matter may be practiced without these specific details. In general well known instruction instances protocols structures and techniques have not been shown in detail.

The display device may be any device or component capable of presenting a graphical user interface GUI to a user. Examples of the display device may include but are not limited to a cathode ray tube CRT monitor a liquid crystal display LCD monitor a touchscreen and the like. The user input interface may be any device or component that facilitates user input to the GUI such as for example a mouse a joystick a touchpad a touchscreen a gesture recognition component e.g. the Microsoft Kinect or a similar device. In some implementations a single device or component such as a touchscreen may be employed as both the display device and the user input interface . The user input interface may also include a keyboard or similar device to allow the user to submit alphanumeric input via the GUI.

The processor may be one or more processing components such as microprocessors digital signal processors DSPs graphical processing units GPUs or any other component capable of executing at least a portion of the graphics application A. The memory may be any data storage component including but not limited to dynamic random access memory DRAM static random access memory SRAM flash memory magnetic disk memory optical disc memory and the like capable of storing the graphics application A for retrieval and execution by the processor .

The graphics application A may be any application capable of presenting multiple graphical objects shapes layers or the like in a GUI to a user via the display device and receiving user input via the user input interface to manipulate the graphical objects including the dragging of objects across at least a portion of the GUI. Examples of the graphics application A include but are not limited to drawing applications diagramming applications vector graphics applications digital motion graphics applications visual effects applications digital compositing applications graphical modeling applications computer aided design CAD applications and computer aided engineering CAE applications involving either or both two dimensional and three dimensional objects.

The user system B includes for example the display device the user input interface the one or more processors and the memory described above in conjunction with the user system A of . Instead of storing the graphics application A the memory may include a browser such as for example Microsoft Internet Explorer Mozilla Firefox or another web browser capable of communicating with the server system . In another example the memory of the user system B may include a programmatic client that communicates with the server system via an application programming interface API not shown in provided in the server system .

The server system may include at least one processor and a memory that may be similar to the processor and the memory of the user system B. The memory may store a graphics application B which may be a server version of the graphics application A of in one implementation. Thus the server system executes the graphics application B with which a user of the user system B may interact via the display device and the user input interface .

Other device and system arrangements may be employed to provide the functionality described herein as well. Accordingly the various implementations discussed herein are not limited by the particular computing structure or system utilized.

As illustrated in the graphics application may include a user input module a source feature selection module a target feature selection module a snap generation module a display module and a configuration module . Further some modules of may be combined with others or subdivided into further modules. Also some of the modules shown in the graphics application may be omitted while others may be added in some implementations. In many arrangements the graphics application includes additional modules to facilitate a range of functions typically associated with a graphics application such as for example creation of graphical objects in a GUI rearrangement of object locations duplication or deletion of objects editing of objects e.g. resizing color alteration etc. and so forth.

The user input module may receive input from a user of the graphics application such as input that might be entered via a mouse joystick touchscreen keyboard gesture recognition component or other user input device. In one example the user input module receives input as part of a dragging operation with such input being processed by the source feature selection module and the target feature selection module . In some implementations the user input module may also receive input that may enable or disable the snapping functionality described herein during a dragging operation. The user input module may also receive user input for other functions provided by the graphics application not associated with the dragging operation but such functionality is not discussed in detail herein.

The source feature selection module may determine based on user input a particular feature of an object shape or layer to be dragged and snapped to a target feature of the same object or another object. Examples of object features include but are not limited to a corner of the object a side of the object a side midpoint of the object and an area or volume midpoint of the object. Possible object features are discussed more fully below in connection with . In one example the source feature selection module performs the selection in response to a user selecting a location within the source object as an initiation of a dragging operation to be applied to the source object or feature. In one example a user may indicate the initiation of the dragging operation by way of a pressing of a mouse button making contact with a touchscreen and the like. In addition the selection of the source feature may depend on the distance between the initial location of the dragging operation and each possible source feature of the source object so that the source feature is automatically determined or computed as part of the dragging operation.

The target feature selection module may determine again based on user input a particular feature of an object shape or layer to which the selected source feature is to be snapped or aligned. In one example the target feature selection module performs its selection on an ongoing or continual basis during the dragging of the source object or feature based on the current location of the source object or feature being dragged. In some implementations the target feature selection module may only select a particular target feature when the dragged source object or feature encroaches within some predetermined distance of a potential target feature in the GUI. Moreover in some implementations the source feature once snapped to a particular target feature will remain snapped thereto within a certain threshold of movement or distance to prevent rapid switching or snapping of the source feature to multiple closely spaced potential target features.

The snap generation module may perform the snap operation indicated in which the selected source feature of the source object is aligned or co located with the selected target feature of the target object based on input from both the source feature selection module and the target feature selection module . In some examples the orientation for both the source object and the target object remains unchanged as a result of the snap operation. However in other implementations the snapping operations described herein may also be applied to snap features together as a result of a dragging operation for a rotation scaling or other non uniform transformation of an object such as a deformation operation applied to the object. Further the snap generation module may track the current cursor position or other present location indicated via user input within the GUI so that a previously snapped source object may be unsnapped from a target object if the user continues the dragging operation by dragging the source feature away from the previous target feature before completing the dragging operation.

The display module may communicate with one or more of the user input module the source feature selection module the target feature selection module and the snap generation module to display the results of the dragging operation in the GUI including any snapping of object features as is discussed more fully below. The display module may also modify the display in response to other operations of the graphics application but such operations are not discussed in detail herein to focus on dragging operations and the snapping of object features that may result therefrom.

The configuration module may provide configuration information that affects the operation of the source feature selection module the target feature selection module and the snap generation module . For example information in the configuration module may indicate which features of a source object and or a target object may be snapped together in a dragging operation. The configuration information may also include one or more preset distances across the GUI within which source features will snap to target features or threshold distances that prevent source features from snapping onto other nearby target features while currently snapped to another target feature. The configuration information may also control or otherwise affect other aspects of the snapping operation in some embodiments and may be preset within the graphics application or be adjusted to at least some degree by a user.

In the method the user input module may receive a first user input that may indicate the possible initiation of a dragging operation operation . Based on the first user input the source feature selection module may determine a source feature to be snapped to an as yet undetermined target feature operation . The user input module may also receive second user input that may indicate the dragging operation being performed operation . Based on the second user input the target feature selection module may determine a current target feature to which the source target feature is to be snapped operation . Based on the determination of the current target feature the snap generation module may then snap the source feature to the current target feature operation .

While operations through of the method of are shown in a specific order other orders of operation including possibly concurrent or continual execution of at least portions of one or more operations may be possible in some implementations of method as well as other methods discussed herein. In fact each of the operations through may be performed in a continual repetitive or ongoing manner in response to the user continuing to perform the dragging operation in some embodiments possibly resulting in the source feature being snapped to one source feature unsnapped from that feature snapped to another source feature and so on until the user terminates the dragging operation.

In addition to three dimensional objects such as the objects and of two dimensional objects also may exhibit snappable features similar to those described above.

The user input module may then receive second user input indicating the dragging operation operation the second user input indicating a second location in the GUI. The target feature selection module may determine a current target feature in the GUI that is closest to the second location operation . The snap generation module may then determine whether the current target feature lies within some predetermined distance in the GUI from the current source feature operation . Also operation may also include a determination as to whether the current location exceeds a threshold distance from a previously snapped feature. If the current feature does not lie within the predetermined distance or does not exceed the threshold distance from a currently snapped feature if applicable the user input module may check to see if the user has terminated the dragging operation operation such as by releasing a mouse button ending contact with a touchscreen or the like. In one example the user may continue dragging the source object mentioned above in the GUI thus indicating a multiple number of second locations.

If instead the snap generation module determines that the current target feature lies within the predetermined distance in the GUI from the current source feature operation and if applicable exceeds the threshold distance from a currently snapped feature the snap generation module may snap or align the source feature to the current target feature operation which the display module may then represent on the GUI being presented to the user. The user input module may then determine whether the dragging operation has been terminated operation such as by way of a specific user input provided by the user for that purpose as described above. If the dragging operation is ended the user input module may then await the next possible initiation of a dragging operation by the user operation . Otherwise if the dragging operation is not being terminated the user input module may then await user input associated with a continuing of the dragging operation operation thus indicating another second location in the GUI.

In two graphical objects A and B are shown in a GUI along with a cursor . A user by way of a mouse joystick or other user input device may move the cursor within the GUI and more specifically may use the cursor to perform a dragging operation to snap a feature of one of the graphical objects A B to another. While the example of specifically involve the use of a cursor other embodiments such as those in which a touchscreen is employed for user input may not explicitly display a cursor or similar indication in the GUI. An indication of snap features on both of the graphical objects A B may be displayed in the GUI even in the absence of a cursor.

In reference to presuming the user desires to snap a feature of one graphical object B to a feature of the other object A the user may move the cursor over the graphical object B as a source object for the snapping operation to come. As shown in the available features e.g. corners and side midpoints are displayed on the GUI as small solid boxes. Further one of the features e.g. the bottom edge midpoint of the object B is designated as a selected source feature by way of a box surrounding the feature. Other designations for the available features and the selected source feature in the GUI are possible in other implementations. In some examples the available features and the source feature may be indicated in the GUI as a result of the user hovering the cursor over the intended source object B. In other implementations both the available features and the source feature are indicated in the GUI in response to the user selecting the source object B such as by way of pressing a mouse button or other user input action while the cursor is located over the object B. In yet another embodiment the available features may be designated in response to the cursor hovering over the object B while the source feature may be indicated in response to the user selecting the source object B. In addition such a selection may serve as an initial input for a dragging operation of the source object B across the GUI.

In one example the source feature is selected from the available features by way of the user positioning the cursor closest to the intended source feature compared to other available features and then selecting the source object B. Accordingly the source feature selection module may calculate distances between the cursor and each of the available features in the GUI to determine which of the available features is the source feature . This determination may be performed continually or repetitively as the user maneuvers the cursor over the source object B or each time the user initially selects the source object B prior to dragging the source object B.

In the user has moved the cursor to another location within the source object B causing the left edge midpoint of the source object B to be selected as the new source feature . In one example the user selects the location of the cursor as the initiation of a dragging operation e.g. by pressing a mouse button to cause the GUI to present the indication of the selection of the new source feature e.g. an encompassing box . In another implementation the user need only hover the cursor close to the new source feature to cause the indication of the selection of the new source feature to be provided in the GUI.

In reference to the user then drags the source object B toward the target object A to snap the current source feature to a yet to be determined target feature of the target object A. In one example the user may drag the source object B by moving the cursor while continuing to press a mouse button. As the user drags the source object B the target feature selection module determines whether the selected source feature is within some predetermined minimum distance of one of the features of the target object A. In the selected source feature is dragged within the predetermined distance to one of the available features of the target object A e.g. the upper right corner of the target object A resulting in the target feature selection module indicating the upper right corner of the target object A as the selected target feature via the display module . Also as result of the selection the snap generation module relocates the source object B aligning or snapping the source feature of the source object B with the target feature of the target object A. The snapping of the features and is further emphasized by way of a second encompassing box displayed in the GUI via the display module .

In the user continues to utilize the cursor to drag the source object B causing the source feature to be unsnapped from the previous target feature of and to be snapped instead to a point of the right edge of the target object A. As shown in the right edge is highlighted e.g. thickened changed to a different color etc. to indicate the snapping operation. In one example the user may snap the source feature to any of a plurality of points along the right edge of the target object A by sliding the source object B along the right edge of the target object A.

As the user continues to drag the source object B the source feature approaches another available feature the left edge midpoint of the target object A. As depicted in in response to the source feature encroaching within a predetermined distance of the left edge midpoint the snap generation module snaps the source feature to the left edge midpoint designated by the target feature selection module as the current target feature by way of a second encompassing box.

The user may then end the dragging operation such as for example by releasing the button of the mouse that was activated to perform the dragging operation. Accordingly the snapping of the selected source feature to the selected target feature remains in effect as shown in . In one example in response to the termination of the dragging operation the various designations for the available source and target features of the source object B and the target object A including the selected source feature and the selected target feature are removed from the GUI. The user may then perform any other operations provided by the graphical application including the snapping of objects as described herein.

While the example snapping operation of involves two dimensional objects A and B other embodiments of the snapping operation described herein may also be applied to three dimensional objects. For example is a graphical representation of example three dimensional objects A and B prior to being snapped together via dragging. More specifically in a user employs the cursor to drag a source object B with a selected source feature e.g. an upper left corner of the source object B toward one or more features of a target object A. The user may accomplish the selection of the source feature and the dragging of the source object B in a fashion similar to that discussed above in conjunction with . In the graphical representation of the two objects A and B essentially appear to contact each other. However as illustrated in which is a top view of the arrangement of the graphical representation of the appearance of close proximity of the two objects A B is due to the orthographic projection onto the viewer with respect to the objects A B in the GUI. Instead in three dimensional space the two objects A B are offset not only vertically as seen directly in but also both laterally and depth wise as seen in the top view of . In other examples the objects A B may be presented in a perspective projection as opposed to the orthographic projection of in which case the objects A B may still appear to be aligned while actually being offset.

In using the graphical representation of as a reference the user may attempt to snap the selected source feature of the source object B to the target object A by dragging the source object B downward in a direction parallel to the camera film plane even though the two objects A B are vertically laterally and depth wise offset from each other. As the user drags the source object B downward toward the target object A as presented on the display device the source feature is moved to within some predetermined distance of the nearest available target feature in two dimensional screen space as depicted in resulting in the source feature being snapped to a target feature as shown in . Further as can be seen in the top view of corresponding to the graphical representation of the vertical offset the lateral offset and the depth offset in relation to the viewer between the source feature and the target feature of the objects A and B is eliminated with the two objects A and B abutting both on the display device and within the virtual three dimensional space representation as a result of the snapping operation. Consequently in dragging a source object B to a target object A in a two dimensional view from the viewpoint of the user the two objects A B may be snapped together in three dimensional space regardless of the actual distance between the objects A and B in the GUI.

Each of the examples discussed above involves separate source and target objects. However in other embodiments the source feature and target feature to be snapped together may belong to the same graphical object. For example a mesh point defining the shape of a three dimensional model in an object may be dragged and snapped to another mesh point or snap feature within that same object. In another example an anchor point of an object may be dragged and snapped to another feature of the same object in order to relocate the anchor point relative to its associated object. This relocating and snapping of the anchor point may allow the user to precisely locate the point about which the object may be rotated to a corner edge or other available feature of the object. In another example an anchor point of one object may be snapped to a target feature of another object.

In one example the snapping operations described may be enabled or disabled by a user in the graphics application via a menu selection checkbox or other user accessible input of the graphics application with the configuration module of the graphics application maintaining the current enabled or disabled state of the snapping function. Moreover the configuration module may allow the user to select which types of source and or target features e.g. corners edges edge midpoints face midpoints volume midpoints anchor points and so forth will be available for the snapping operation. Furthermore as some objects including their bounding boxes may be nested within other objects or bounding boxes the configuration module may allow the user to select whether the features of nested and or encompassing objects may be available as source or target features.

As indicated in the figures described above the GUI may graphically or visually indicate e.g. via surrounding boxes or other visual markers such as those shown relative to the source feature of or the target feature of the identity of the source and or target features to be snapped together. In addition these feature markers may be further enhanced to indicate the particular objects with which these features are associated. In one example the color of the markers may match an identifying color of the object or layer with which the identified feature corresponds. For example a selected source feature of a source object identified with the color yellow may be surrounded with a yellow box or marker.

In yet other examples some of the feature markers may be altered in some way to identify the particular feature type with that marker. Such visual alteration may help the user distinguish between numerous or closely located features of a complex drawing or graphical display. In one implementation a volume midpoint may be designated with a double box as opposed to a single box for other available features of the same object. Further the feature markers for two separate features when snapped together may be combined to indicate the successful snapping of those features. For instance a volume midpoint of a source object snapped to a volume midpoint of a target object may result in the combined feature point being surrounding by four boxes. Other ways of distinguishing between the various types of features of both source and target objects are also contemplated.

As a result of at least some of the embodiments described above a user of a graphical application may precisely snap or align a specific user selected feature of one object shape or layer to another user selected feature of the same or different object by employing the same type of object dragging operation facilitated by many applications and operating systems.

The machine is capable of executing a set of instructions sequential or otherwise that specify actions to be taken by that machine. Further while only a single machine is illustrated the term machine shall also be taken to include any collection of machines that individually or jointly execute a set or multiple sets of instructions to perform any one or more of the methodologies discussed herein.

The example of the processing system includes a processor e.g. a central processing unit CPU a graphics processing unit GPU or both a main memory e.g. random access memory and static memory e.g. static random access memory which communicate with each other via bus . The processing system may further include video display unit e.g. a plasma display a liquid crystal display LCD or a cathode ray tube CRT . The processing system also includes an alphanumeric input device e.g. a keyboard a user interface UI navigation device e.g. a mouse a disk drive unit a signal generation device e.g. a speaker and a network interface device .

The disk drive unit a type of non volatile memory storage includes a machine readable medium on which is stored one or more sets of data structures and instructions e.g. software embodying or utilized by any one or more of the methodologies or functions described herein. The data structures and instructions may also reside completely or at least partially within the main memory the static memory and or within the processor during execution thereof by the processing system with the main memory the static memory and the processor also constituting machine readable tangible media.

The data structures and instructions may further be transmitted or received over a computer network via network interface device utilizing any one of a number of well known transfer protocols e.g. HyperText Transfer Protocol HTTP .

Certain embodiments are described herein as including logic or a number of components modules or mechanisms. Modules may constitute either software modules e.g. code embodied on a machine readable medium or in a transmission signal or hardware modules. A hardware module is a tangible unit capable of performing certain operations and may be configured or arranged in a certain manner. In example embodiments one or more computer systems e.g. the processing system or one or more hardware modules of a computer system e.g. a processor or a group of processors may be configured by software e.g. an application or application portion as a hardware module that operates to perform certain operations as described herein.

In various embodiments a hardware module may be implemented mechanically or electronically. For example a hardware module may include dedicated circuitry or logic that is permanently configured for example as a special purpose processor such as a field programmable gate array FPGA or an application specific integrated circuit ASIC to perform certain operations. A hardware module may also include programmable logic or circuitry for example as encompassed within a general purpose processor or other programmable processor that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a hardware module mechanically in dedicated and permanently configured circuitry or in temporarily configured circuitry for example configured by software may be driven by cost and time considerations.

Accordingly the term hardware module should be understood to encompass a tangible entity be that an entity that is physically constructed permanently configured e.g. hardwired or temporarily configured e.g. programmed to operate in a certain manner and or to perform certain operations described herein. Considering embodiments in which hardware modules are temporarily configured e.g. programmed each of the hardware modules need not be configured or instantiated at any one instance in time. For example where the hardware modules include a general purpose processor that is configured using software the general purpose processor may be configured as respective different hardware modules at different times. Software may accordingly configure a processor for example to constitute a particular hardware module at one instance of time and to constitute a different hardware module at a different instance of time.

Modules can provide information to and receive information from other modules. For example the described modules may be regarded as being communicatively coupled. Where multiples of such hardware modules exist contemporaneously communications may be achieved through signal transmissions such as for example over appropriate circuits and buses that connect the modules . In embodiments in which multiple modules are configured or instantiated at different times communications between such modules may be achieved for example through the storage and retrieval of information in memory structures to which the multiple modules have access. For example one module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled. A further module may then at a later time access the memory device to retrieve and process the stored output. Modules may also initiate communications with input or output devices and may operate on a resource for example a collection of information .

The various operations of example methods described herein may be performed at least partially by one or more processors that are temporarily configured e.g. by software or permanently configured to perform the relevant operations. Whether temporarily or permanently configured such processors may constitute processor implemented modules that operate to perform one or more operations or functions. The modules referred to herein may in some example embodiments include processor implemented modules.

Similarly the methods described herein may be at least partially processor implemented. For example at least some of the operations of a method may be performed by one or more processors or processor implemented modules. The performance of certain of the operations may be distributed among the one or more processors not only residing within a single machine but deployed across a number of machines. In some example embodiments the processors may be located in a single location e.g. within a home environment within an office environment or as a server farm while in other embodiments the processors may be distributed across a number of locations.

While the embodiments are described with reference to various implementations and exploitations it will be understood that these embodiments are illustrative and that the scope of claims provided below is not limited to the embodiments described herein. In general the techniques described herein may be implemented with facilities consistent with any hardware system or hardware systems defined herein. Many variations modifications additions and improvements are possible.

Plural instances may be provided for components operations or structures described herein as a single instance. Finally boundaries between various components operations and data stores are somewhat arbitrary and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the claims. In general structures and functionality presented as separate components in the exemplary configurations may be implemented as a combined structure or component. Similarly structures and functionality presented as a single component may be implemented as separate components. These and other variations modifications additions and improvements fall within the scope of the claims and their equivalents.

