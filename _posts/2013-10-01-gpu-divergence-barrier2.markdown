---

title: GPU divergence barrier
abstract: A device includes a memory, and at least one programmable processor configured to determine, for each warp of a plurality of warps, whether a Boolean expression is true for a corresponding thread of each warp, pause execution of each warp having a corresponding thread for which the expression is true, determine a number of active threads for each of the plurality of warps for which the expression is true, sort the plurality of warps for which the expression is true based on the number of active threads in each of the plurality of warps, swap thread data of an active thread of a first warp of the plurality of warps with thread data of an inactive thread of a second warp of the plurality of warps, and resume execution of the at least one of the plurality of warps for which the expression is true.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09652284&OS=09652284&RS=09652284
owner: QUALCOMM Incorporated
number: 09652284
owner_city: San Diego
owner_country: US
publication_date: 20131001
---
This disclosure relates to graphics processing and more particularly to techniques for managing the execution of threads on a graphics processing unit GPU .

Recently there has been a move toward so called general purpose GPUs GPGPUs . Unlike traditional GPUs which perform graphics rendering GPGPUs may be configured to execute a general purpose task or program often referred to as a kernel. Some types of tasks may be better suited to particular a type of processor such as a central processing CPU or GPU. CPUs may be better suited for tasks with more branches jumps and conditional logic while GPUs may be suited to highly parallel tasks and or tasks with many floating point calculations. GPUs may also include the capability to execute SIMD Single Instruction multiple Data instructions as many GPUs have a SIMD hardware architecture. When a GPU executes a SIMD instruction the GPU may execute the same operation indicated by the instruction on multiple data values. Typically a GPU has multiple execution units which are capable of executing the operations indicated by the SIMD instruction in parallel.

The techniques of this disclosure provide techniques for reducing divergence among threads executing on a graphics processing unit GPU . The GPU may include support for an instruction referred to as a divergence barrier instruction. The divergence barrier instruction attempts to group divergent threads from multiple warps into new warps such that the threads are executing the same instruction thereby improving GPU performance.

In one example this disclosure describes a method comprising determining for each warp of a plurality of warps whether a Boolean expression is true for a corresponding thread of each warp pausing execution of each warp having a corresponding thread for which the expression is true determining a number of active threads for each of the plurality of warps for which the expression is true sorting the plurality of warps for which the expression is true based on the number of active threads in each of the plurality of warps swapping thread data of an active thread of a first warp of the plurality of warps with thread data of an inactive thread of a second warp of the plurality of warps and resuming execution of the at least one of the plurality of warps for which the expression is true.

In another example this disclosure describes a device that includes a memory and at least one programmable processor configured to determine for each warp of a plurality of warps whether a Boolean expression is true for a corresponding thread of each warp pause execution of each warp having a corresponding thread for which the expression is true determine a number of active threads for each of the plurality of warps for which the expression is true sort the plurality of warps for which the expression is true based on the number of active threads in each of the plurality of warps swap thread data of an active thread of a first warp of the plurality of warps with thread data of an inactive thread of a second warp of the plurality of warps and resume execution of the at least one of the plurality of warps for which the expression is true.

In another example this disclosure describes an apparatus that includes means for determining for each warp of a plurality of warps whether a Boolean expression is true for a corresponding thread of each warp means for pausing execution of each warp having a corresponding thread for which the expression is true means for determining a number of active threads for each of the plurality of warps for which the expression is true means for sorting the plurality of warps for which the expression is true based on the number of active threads in each of the plurality of warps means for swapping thread data of an active thread of a first warp of the plurality of warps with thread data of an inactive thread of a second warp of the plurality of warps and means for resuming execution of the at least one of the plurality of warps for which the expression is true.

In another example this disclosure describes a non transitory computer readable storage medium storing instructions that when executed cause at least one programmable processor to determine for each warp of the plurality of warps for which the expression is true an associated divergence barrier of a plurality of divergence barriers group each warp of the plurality of warps into a plurality of compaction pools based on the associated divergence barrier of each warp wherein the instructions that cause the at least one processor to sort the plurality of warps comprise instructions the at least one processor to sort the plurality of warps belonging to a same one of the plurality of compaction pools wherein the first warp and the second warp comprise warps belong to the same one of the plurality of compaction pools and wherein the instructions that cause the at least one processor to resume execution of the at least one of the plurality of warps for which the condition is true comprises resuming execution of at least one warp of the same one compaction pool.

The details of one or more examples of the disclosure are set forth in the accompanying drawings and the description below. Other features objects and advantages of the disclosure will be apparent from the description and drawings and from the claims.

This disclosure is directed to techniques for reducing divergence of threads executing on a graphics processing unit GPU . A GPU may include multiple execution units referred to as processing elements PEs . A program referred to as a kernel may execute on one or more PEs of the GPU. An application may divide the kernel into multiple threads which constitute the basic unit of work of the GPU. The GPU scheduler may further group the threads together into a thread group referred to as a warp. A warp may include a certain number of threads for example 32 threads on some graphics architectures.

A driver or scheduler for the GPU creates threads to execute the kernel on the GPU. A thread is the basic unit of data to be processed on the GPU and should not be confused with a CPU thread. The Scheduler may assign each thread to an execution unit of the GPU. The execution units also referred to as processing elements PEs or shaders are SIMD units capable of parallel execution of the same instruction on multiple data values.

In general each thread of a warp executes the same instruction. A program counter PC stores the memory address of the instruction that each thread is to execute. Generally there may be a single PC for each of the threads of a warp. Having a single PC for each warp allows each of the threads to execute simultaneously as long as each of the threads do not need to execute different instructions.

Many GPUs now include the ability to execute flow control instructions e.g. to execute branch jump goto and other flow control instructions. Flow control instructions may alter the flow of program execution in a number of ways. In a program or kernel without flow control instructions a PE may execute instructions of the kernel from start to finish. After a PE finishes executing an instruction the GPU sets the value of the PC to the address of the next instruction in memory typically by incrementing the PC value by one and the PE executes the next instruction. The process of executing the program continues in this fashion a program without flow control instructions until the program reaches an exit point at which point execution terminates.

Executing a flow control instruction may cause a PE to execute a subsequent instruction at an address other than the incremented PC value. Instead of executing a subsequent instruction at the address of the incremented PC value a PE that executes a flow control instruction may execute a subsequent instruction that has a different PC address such as the address of a subroutine etc. Thus a flow control instruction is said to alter the execution flow of a program.

Examples of flow control instructions include subroutine calls branches returns jumps etc. In various examples the instruction address to which a PE jumps i.e. the address that is assigned to the PC may be based on the value of data that varies between threads at run time. Flow control instructions may also be associated with a Boolean expression that each PE evaluates separately. A Boolean expression is an expression that produces a Boolean value that evaluates to either true or false. A Boolean expression may include Boolean operators such as and or not exclusive or XOR etc. A Boolean expression may also include arithmetic tests such as greater than less than equal to not equal to greater than or equal to less than or equal to etc. The truth or falsity of the Boolean expression may depend on data or values that vary from one thread to another.

Thus it may be possible for one thread to jump to and execute a different instruction than another thread within the same warp. However as stated above there is only one PC for the warp. The condition where two or more threads of a warp execute different instructions is referred to as divergence. When divergence occurs some sets threads may continue to execute the same instructions. However there may be multiple sets of threads that execute different instructions as well.

As an example of thread divergence a first thread and a second thread of a warp may execute a flow control instruction such as an if else statement or loop statement. The subsequent instruction that the first thread executes may be based on the values of data stored in a register of the first thread. Similarly the subsequent instruction of the second thread may be based on the value of data stored in a register of the second thread. If the first and second threads have different register data the first and second thread may jump to different subsequent instructions which are associated with different instruction addresses.

In the case where warp threads are divergent threads may take different branches of control flow blocks such as an if else statement. In the case of a loop statement warp threads may also exit the loop statement at different times e.g. after executing different numbers of iterations of the loop.

When warp threads become divergent e.g. due to taking different branches of an if else statement or performing different numbers of iterations of a loop the GPU serializes each of the different execution paths caused by the divergence. That is the GPU determines threads that are active and are executing the same instruction. The active threads continue to execute on PEs associated with each thread until the threads finish execution or reach a barrier such as a divergence barrier instruction discussed in greater detail below.

During serialization the GPU also determines threads that are not currently executing and sets those inactive threads and their associated PEs to idle. While the PEs are set to idle the inactive threads do note execute which hurts GPU performance. In some cases divergent threads may further diverge i.e. there may be multiple levels or nested divergence. To handle nested divergence a GPU uses a convergence stack to track nested branches and loops. The GPU handle the deepest or innermost layer of divergence first and executes the threads with the deepest level of divergence until execution completes or pauses. The GPU then removes that level of divergence from the convergence stack and repeats the process of executing the innermost remaining thread on the convergence stack and removing completed threads from the convergence stack. Once a thread finishes executing a branch or loop the GPU may recombine or converge the threads back together to form warps that are no longer divergent.

The techniques of this disclosure introduce an instruction which a GPU may support referred to as a divergence barrier instruction. In various examples an application programming interface API may include support for the divergence barrier instruction. Such APIs may include the Open Compute Language OpenCL Open Graphics Language OpenGL and Microsoft DirectX APIs. When programming a GPU with a particular API a programmer may insert divergence barrier function calls which cause the GPU to execute the divergence barrier instruction at code points where divergence is likely to significantly impact performance. A GPU driver or compiler may also automatically detect code points where divergence is likely to significantly impact performance and may insert divergence barrier instructions at those code points.

A CPU then transmits the code of a kernel that includes the divergence barrier instructions to the GPU for execution. The GPU then executes the kernel code until it encounters a divergence barrier instruction. Each divergence barrier instruction causes the GPU to evaluate a Boolean expression. If the GPU evaluates the Boolean expression as true the GPU pauses execution of the warp. The GPU switches to and begins execution of another warp. The GPU continues the process of executing warps until all the warps of the kernel either finish execution or are paused e.g. due to executing a divergence barrier instruction . Once all the warps finish execution or are paused the GPU attempts to eliminate divergence amongst the warps that are currently paused as a result of executing a divergence barrier instruction.

When a GPU executes the divergence barrier instruction and pauses execution of a warp the GPU inserts the warp into a queue of warps that are currently paused due to having executed the divergence barrier instruction. Upon being placed into the queue the GPU sorts the warps in the queue based on the number of active threads in each warp using an insertion sort and sorts each of the paused warps in the queue using an insertion sort. After all warped are paused and sorted in queue or finished the GPU then attempts to eliminate divergence among threads of warps executing the kernel. The process of eliminating divergence amongst threads of warps is referred to as thread compaction. 

During thread compaction the GPU attempts to form warps having threads that have no divergence or less divergence by swapping currently active warps having more inactive threads with inactive threads from warps having more active threads. The GPU uses a warp sort queue in order to minimize the data amount of exchanged when swapping threads from different warps. During GPU thread compaction which results in the formation of new warps the GPU may continue execution of each new warp as soon as a warp having all active threads is formed. In this manner a GPU configured to support a divergence barrier instruction may reduce warp thread divergence and improve GPU performance.

As illustrated in the example of computing device includes a CPU a system memory a graphics processing unit GPU and a compiler driver . CPU may execute various types of applications. Examples of the applications include web browsers e mail applications spreadsheets video games or other applications that generate viewable objects for display. Instructions for execution of the one or more applications may be stored within system memory .

CPU may also execute compiler driver . Compiler driver may comprise a compiler and or a driver that controls the interaction of GPU . Compiler driver may take program code such as code written in a particular graphics application programming interface API and translate the code into kernel . Kernel is comprised of native code e.g. binary instructions that GPU is capable of executing. Compiler driver may also manage run time execution of GPU . As described in greater detail below compiler driver may insert divergence barrier instructions into kernel at run time in accordance with the techniques of this disclosure. CPU may transmit kernel to GPU for further processing.

GPU may be specialized hardware that allows for massively parallel processing which is well suited well for processing graphics data. In this way CPU offloads graphics processing that is better handled by GPU . CPU may communicate with GPU in accordance with a particular application processing interface API . Examples of such APIs include the DirectX API by Microsoft and the OpenGL by the Khronos group however aspects of this disclosure are not limited to the DirectX and the OpenGL APIs and may be extended to other types of APIs that have been developed are currently being developed or are to be developed in the future.

In addition to defining the manner in which GPU is to receive graphics data from CPU the APIs may define a particular graphics processing pipeline that GPU is to implement. GPU in illustrates the graphics processing pipeline defined by the Direct3D API. As described in more detail illustrates the graphics processing pipeline of the OpenGL 4.x API.

Examples of CPU and GPU include but are not limited to a digital signal processor DSP general purpose microprocessor application specific integrated circuit ASIC field programmable logic array FPGA or other equivalent integrated or discrete logic circuitry. In some examples GPU may be specialized hardware that includes integrated and or discrete logic circuitry that provides GPU with massive parallel processing capabilities suitable for graphics processing. In some instances GPU may also include general purpose processing and may be referred to as a general purpose GPU GPGPU . The techniques described in this disclosure may also be applicable to examples where GPU is a GPGPU.

System memory may comprise one or more computer readable storage media. Examples of system memory include but are not limited to a random access memory RAM a read only memory ROM an electrically erasable programmable read only memory EEPROM flash memory or any other medium that can be used to carry or store desired program code in the form of instructions and or data structures and that can be accessed by a computer or a processor.

In some aspects system memory may include instructions that cause CPU and or GPU to perform the functions ascribed to CPU and GPU in this disclosure. Accordingly system memory may be a computer readable storage medium comprising instructions that cause one or more processors e.g. CPU and GPU to perform various functions.

System memory may in some examples be considered as a non transitory storage medium. The term non transitory may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. However the term non transitory should not be interpreted to mean that system memory is non movable. As one example system memory may be removed from device and moved to another device. As another example a system memory substantially similar to system memory may be inserted into device . In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM .

CPU may also generate commands and data for GPGPU applications for example commands and scene data for a ray tracing application a physics simulation or data for any other type of GPGPU kernel. GPGPU applications e.g. kernel may also be compiled using a graphics API such as DirectX or OpenGL or using a more general purpose compute API such as Open Compute Language OpenCL or OpenCompute or DirectCompute. CPU may transmit the data for the kernel to a command buffer for processing. In various examples the command buffer may be part of system memory or part of GPU . In some examples CPU may transmit the commands and data of kernel for GPU to process via a special purpose bus such as a PCI Express bus or another general purpose serial or parallel bus.

To perform the operations stored of kernel in the command buffer GPU may implement a graphics processing pipeline. The graphics processing pipeline includes performing as defined by software or firmware executing on GPU and performing functions by fixed function units that are hardwired to perform very specific functions. The software or firmware executing on the GPU may be referred to as shaders e.g. shader . Shaders may execute on one or more processing elements also referred to as shader cores or PEs of GPU . Shaders provide users with functional flexibility because a user can program the shaders to execute desired tasks in any conceivable manner as with any other processor. The fixed function units however are hardwired for the manner in which the fixed function units perform tasks. Accordingly the fixed function units may not provide much functional flexibility. The techniques of this disclosure are directed toward execution of a kernel such as kernel on GPU shaders .

Once CPU transmits the data and or commands associated with rendering a graphical scene or executing a kernel to the command buffer GPU begins execution of the commands through the graphics pipeline of GPU . Scheduler of GPU creates threads which perform the basic unit of work associated with the kernel. Scheduler assigns the threads to a particular processing element of shaders . Scheduler also groups the threads into warps for execution and begins execution of the warps.

As discussed above if different threads jump to different instructions as the result of executing a flow control instruction the threads of a warp diverge. In the case of a divergent warp the scheduler executes serially each set of threads. That is GPU no longer executes all of the warp threads in parallel but serially in groups which hurts GPU performance.

To improve GPU performance when warps are divergent a programmer or compiler driver may insert a divergence barrier instruction into kernel . The divergence barrier is associated with a Boolean expression which GPU evaluates at run time. A Boolean expression is an expression that evaluates as either true or false. A Boolean expression may include arithmetic operators bitwise logical operators and or logical operators in various examples. By determining whether to execute a divergence barrier instruction based on a Boolean expression the Boolean expression provides flexibility in controlling when the GPU should execute the divergence barrier. The Boolean expression evaluation is one way in which a divergence barrier instruction differs from a traditional barrier instruction. That is unlike executing a traditional divergence barrier instruction in which a GPU always stops execution of warps when executing the barrier instruction warps do not have to stop at each divergence barrier because divergence barriers are associated with a Boolean condition and divergence barriers are often located in control flow blocks that are also associated with the Boolean expression. An example of pseudocode for the divergence barrier instruction is 

The divergence barrier instruction causes the GPU to determine whether the Boolean expression associated with the divergence barrier instruction is true for at least one thread in each of a warp that reaches the divergence barrier instruction. If the condition is true for at least one thread GPU pauses execution of each of the plurality of the warps sorts the warps based on the number of active threads and then swaps inactive threads with active threads to form new active inactive warps. GPU continues to swap inactive threads with active threads until no inactive warps having all inactive threads can be created. Once no inactive warps can be created GPU resumes execution of the warps. If GPU forms a warp having all active threads GPU may also immediately release from the queue and begin execution of that warp.

As one example in accordance with the techniques of this disclosure GPU of computing device may be configured to perform a method comprising determining for each warp of a plurality of warps whether a Boolean expression is true for a corresponding thread of each warp pausing execution of each warp having a corresponding thread for which the expression is true and determining a number of active threads for each of the plurality of warps for which the expression is true. The method may further comprise sorting the plurality of warps for which the expression is true based on the number of active threads in each of the plurality of warps swapping thread data of an active thread of a first warp of the plurality of warps with thread data of an inactive thread of a second warp of the plurality of warps and resuming execution of the at least one of the plurality of warps for which the expression is true.

Warp also includes instructions that a scheduler of GPU assigns PEs for execution. In some examples instructions may be stored in a command buffer. Instructions may include a set of instructions of a kernel that each PE is configured to execute. Program counter PC indicates the current instruction that one or more of PEs are to execute. After an instruction finishes executing on PEs the value of PC may be incremented to the address of the next instruction of kernel . Warp also includes registers . Registers A N registers may be general purpose registers capable of holding multiple data values or a single value. Registers may be banked that is may load and store data for particular PE. As an example register A may be limited to storing data for PE A and may not load or store data for other PEs. Each of registers may supply data to and or from one of PEs which PEs may then process. Warp may also include warp context data . Warp context data may include data that is common or shared amongst the different threads of warp . As an example context data may include data of a predication register which may include data for each thread that executes on PEs of warp .

Warp PEs instructions registers context and PC may comprise a core or part of a core of shaders of GPU . In various examples warp may comprise part of a shader such as a geometry shader pixel shader and or a vertex shader which may be part of a graphics pipeline of GPU . In some examples GPU may feed the results generated by a warp into another stage of the graphics pipeline for additional processing.

During execution of the kernel on warp one or more of PEs executes one of instructions located at the address indicated by PC . During execution of an instruction PEs may read one or more data values from registers . PEs may perform one or more operations on the data values and store new values back to registers . PEs may execute flow control instructions such as branches jumps gotos etc. The flow control instructions may cause one PE e.g. PE A to jump to a different one of instructions than PE B i.e. the threads executing on the PEs may become divergent due to different evaluations of flow control. Because there is a single PC however PEs may only execute one of instructions indicated by PC at one particular at a given time.

Once the threads of a warp diverge PEs may still only execute one instruction indicated by the value of PC at a particular time. To support divergent execution warp maintains state such as a bitmask that indicates which of PEs should execute the instruction at the address of PC . As an example PE A and B may be scheduled to execute different instructions resulting from taking different branches of an if else statement. In this example PE A executes a first instruction of instructions and PE B executes a second different instruction of instructions at a later time. When PE A executes the first instruction warp sets the bitmask to indicate that PE A is active during the execution of the instruction while PE B is inactive. PE A then continues to execute instructions until the thread of PE A finishes execution or pauses executes divergence barrier instruction and pauses execution of the thread. Once PE A finishes execution warp changes the bitmask to indicate that only PE B is active changes the value of PC to the address of the instruction that PE B should execute and then PE B executes the instructions specified by PC until the thread pauses or finishes execution.

As stated above the techniques of this disclosure include a divergence barrier instruction that when executed may improve performance of GPU when threads of multiple warps such as warp diverge. The divergence barrier instruction may comprise part of an application programming interface API such as the DirectX 11 API the OpenGL API OpenCL and or DirectCompute etc. A program written in such an API may insert a call to a divergence barrier function into kernel that causes GPU execute the divergence barrier instruction.

Compiler driver or an operating system may also insert calls to the divergence barrier instruction into the code of kernel . In various examples a user may compile kernel using compiler driver . During compilation compiler driver may analyze kernel and determine at least one of a location the program where divergence is likely to occur and a location that would significantly impact performance and may insert divergence barrier instructions at least one of those locations. Compiler driver may insert divergence barrier instructions into the instructions of kernel at run time also referred to as bind time at least one of a location where thread divergence is likely to occur and a location that would significantly impact performance.

One example of code that may be likely to diverge may be code of a raytracing application which is included below. In this example a divergence barrier instruction is inserted e.g. by a compiler or a user to reduce divergence when executing the following raytracing pseudocode 

The above pseudocode is an example of a loop that multiple threads and warps of GPU may execute. Each thread may execute the loop a different number of times e.g. based on a number of bounces that a ray makes in a raytracing scene. Therefore some threads may end after performing a few iterations of the loop while other threads may continue execution of the loop for as many as thirty iterations of the loop.

In this example GPU executes the divergence barrier instruction during each loop iteration. The divergence barrier instruction includes a Boolean expression which the GPU evaluates with each iteration of the loop. GPU only executes the operations associated with the divergence barrier instruction e.g. warp sorting and thread compaction if the Boolean expression evaluates to true for at least one thread of a warp. In this example the Boolean expression i 10 0 evaluates to true during every tenth iteration of the loop. When the Boolean expression is true for one thread of a warp GPU may swap threads from different warps in order to form new warps having more active threads a process referred to as thread compaction. 

Whenever the Boolean expression associated with a divergence barrier of one warp thread evaluates to true GPU puts the warp associated with that thread e.g. warp into a queue or a buffer. Once the warp is placed into the queue GPU stops warp from executing and sorts the warps in the queue.

Sorting a warp based on the number of active threads of each warp is illustrated in greater detail in . GPU may sort each of the warps based on the number of active threads in each warp using an insertion sort. GPU sorts the warps such that warps with more active threads are sorted to the front of the queue and warps with fewer active threads are at the back of the queue.

After all warps either added into the queue or have completed without being paused at the barrier GPU then performs thread compaction on warps in the queue i.e. swaps inactive threads from warps having a greater number of active threads with warps having a smaller number of greater number of threads. GPU continues to swap threads from warps having a greater number of active threads with warps having a smaller number of active threads until GPU cannot create an inactive warp. An inactive warp is a warp having all inactive threads. GPU may also swap per thread context data if any when swapping an inactive thread data with active thread. Once a fully active warp having all active threads is created by swapping threads GPU removes the fully active warp from the queue and sets its state to active and resumes execution of the fully active warp from the current instruction. After GPU finishes thread compaction all warps including partially active warps and fully inactive warps are set to the ready or active state. Partially active warps are also resumed from current instruction. Fully inactive threads can fast forward to the end of current control flow block and if no instructions follow the current control block the fully inactive warp can finish execution immediately. The process of swapping threads among warps is illustrated in greater detail with respect to .

To swap an active thread with an inactive thread GPU may store the register data stored of the inactive thread and the active thread in register swap buffer in some examples. GPU then stores the register data of the formerly inactive thread in the corresponding register of the formerly active thread. GPU also stores the register data of the formerly active thread in the corresponding register of the formerly inactive thread using multiplexer MUX . More particularly for each register associated with each thread multiplexer MUX multiplexes between the stored register values of the inactive and active threads and stores the values back to the register files of the warps that are to be swapped. During the swap process DBS may also swap per thread context data from the first and second warps. In some examples GPU may not utilize register swap buffer to swap register data. Rather GPU may swap register values in parallel rather than storing the values in a buffer.

In some examples each warp may refer to a set of registers associated with a particular thread referred to as a bank using a register pointer. GPU may store a mapping table of pointers. Each row or column of the table may correspond to a particular warp and each entry within the row or column corresponding to the warp depending on the table layout may store a pointer value that maps a particular thread to a register bank within registers . GPU may store the mapping of pointers to register banks for the threads of a warp in context data . In some examples if registers are referenced by per thread register bank pointers GPU may swap per thread register data by simply swapping the per thread register bank pointer values of two threads rather than swapping each of the corresponding register values of two threads using register swap buffer and mux .

In some examples executing kernels may frequently access global memory e.g. of GPU and or system memory or perform other operations that have a high amount of access time or latency. In this case barrier operations including divergence barrier operations may pause too many warps to hide these long latency operations and execution performance may suffer. In order to speed the execution of kernels with long latency operations GPU may perform thread compaction immediately once the number of active warps the active warp pool reaches a certain threshold.

Some kernels may include a mixture of traditional barrier operations and divergence barrier operations. Traditional barrier operations cause all warps that reach the barrier to pause and unlike divergence barriers are not associated with a Boolean condition that GPU evaluates at runtime. Traditional divergence barrier operations also do not cause GPU to perform thread sorting and thread compaction. For kernels that include a mix of traditional barriers and divergence barriers divergence barrier instructions should yield to traditional barrier operations. In a kernel having a mixture of both traditional and divergence barriers GPU may perform thread compaction without waiting for warps to pause due to executing a traditional barrier operation.

Some kernels may also include subroutine calls. During a subroutine call a GPU may swap threads data with a warp having a different call stacks associated with the called subroutine. Subroutine calls may be problematic when divergence barrier operations are included within such a call. For example a thread of a first warp may call a subroutine at a first line e.g. line of a kernel. A second warp may call the same subroutine at a later execution point e.g. line of the kernel. The subroutine includes a divergence barrier instruction.

Due to the execution intervening instructions and or other factors the stacks of the first warp and the second warp may differ from each other when the first and second warps execute the divergence barrier instruction inside the subroutine. In one example solution to the problem of having divergence barriers inside subroutines GPU may prohibit having divergence barriers inside subroutines entirely. In another example solution GPU may implement logic to ensure that warps executing subroutines having divergence barrier instructions have the same stacks when executing the divergence barrier instructions inside the subroutine calls.

GPU sorts unsorted warps based on the number of active threads in each warp. The resulting sorted warps are illustrated in as sorted warps . Of unsorted warps warp has the most active threads all active followed in order by warp warp warp warp warp warp warp and warp all inactive . As illustrated in GPU sorts unsorted warps using an insertion sort. The result of the insertion sort based on the number of active threads in each warp is illustrated in as sorted warp . In various examples GPU may store unsorted warps in a queue then sort the warps in place in the queue which results in sorted warps being the queue. In various examples the queue may be implemented as a linked list of pointers. Each pointer may point to a particular warp. To sort the linked list GPU may swap pointers associated with the warps in the linked list.

GPU swaps inactive threads with active threads based on the number of active and inactive threads in the two warps. In the example of GPU swaps threads from warps having more active threads with threads from warps having fewer active threads. In GPU swaps threads of a leftmost warp having an inactive thread with a rightmost warp having an active thread. GPU continues to swap threads from different warps from the outside in i.e. swapping inactive threads from warps having more active threads with active threads from warps having more inactive threads until no more inactive warps can be created. Scheduler resumes execution of any and all warps that still remain in the queue at that time. Additionally whenever the warp at the head of the queue contains all active threads scheduler releases the warp having all active threads located at the head of the queue and begins execution of that warp.

By swapping inactive threads with active threads the techniques of this disclosure form warps that have a larger number of active threads as well as warps that have all inactive threads. Warps having a greater number of active threads increase the utilization and throughput of GPU . Warps having all inactive threads may also increase the throughput of GPU because inactive warps may fast forward to the end of the current control flow block or finish execution if no instructions follow the current control block. Thus warps having all inactive threads may finish execution immediately in some cases. Thus GPU may reduce execution time or stop execution of such an inactive warp and utilize the PEs associated with the inactive warp to execute a different warp that scheduler determines can execute on those PEs.

In the example of each of DB1 DB2 and DB3 are located at different points in the kernel. One example pseudocode of sample that contains three divergence barriers that correspond to DB1 DB2 and DB3 is included below 

The kernel pseudocode includes multiple divergence barrier instructions DB1 DB2 and DB3. Each of the divergence barrier instructions occurs in an branch statement or loop statement. Warps executing kernel may reach the different divergence barriers depending on which control blow block they enter and the evaluation of the Boolean conditions e.g. barrier conditions associated with the divergence barrier instructions. Threads executing kernel may encounter DB1 first in the execution of kernel followed by DB2 or DB3.

GPU may handle the process of sorting warps and performing thread compaction similarly when multiple divergence barrier instructions are present in a kernel as opposed to a single divergence barrier instruction. In particular scheduler may group warps together that reach the same divergence barrier into what is referred to as a compaction pool. GPU may compact the threads of warps in the compaction pool which have reached the same divergence barrier instruction.

More particularly GPU associates a prefix associated with the divergence barrier that a warp has reached with each warp. As an example warps that reach the first divergence barrier may have a prefix 1 warps that reach a second divergence barrier may have a prefix 2 etc. Each warp is also assigned a second number e.g. a suffix that indicates the number of active threads in that warp. As an example if a warp has three active warps the warp is assigned the suffix three 3 .

The combination of the prefix and the suffix forms a number that GPU uses to sort the warps in the warp queue. As an example there may be three threads in the warp queue for GPU to sort. A first warp has reached divergence barrier 2 and has four 4 active threads. GPU may assign the first warp the number 24 for sorting purposes. A second warp may have reached divergence barrier 1 and have one 1 active thread. GPU may assign the second warp the value 11. A third warp may have reached divergence barrier 1 and has 3 three active threads. GPU may assign 13 as the sorting value for the warp. GPU sorts the warps in the queue by values of each warp. The result of the sort may be that such that the third warp having sort value 11 is at the head of the queue the second warp having sort value 13 is second in the queue and the first warp having sort value 24 is at the tail of the queue. Because the warps having sort values 11 and 13 have the same prefix 1 GPU may form a compaction group.

After GPU pauses all warps and inserts the warps in the queue or finishes execution if the warps are not paused on a barrier GPU performs thread compaction on warps in the first warp group by swapping active threads with inactive threads. In other words GPU eliminates all divergence on the first divergence barrier before eliminating divergence on subsequent divergence barriers i.e. GPU eliminates divergence on divergence barrier DB1 before moving onto divergence barriers DB2 DB3 etc.

To eliminate divergence associated with a particular divergence barrier GPU detaches a first warp group from the queue forms a compaction pool and performs compaction on the warps in the pool. As GPU executes compaction GPU releases warps from the compaction pool and resumes the warps upon execution such that GPU may pause the released warps again upon reaching any subsequent divergence barriers. Meanwhile the queue containing the remaining warps continues to receive additional warps paused on any divergence barriers. GPU may pause the resumed warps if they reach barriers DB2 DB3 or even DB1 again in the case of a loop. GPU adds the warps to the queue and sorts the warps with other warps in the queue as described above.

When all those warps are paused and inserted in the queue i.e. the queue gets full again GPU repeats the same compaction process on the current first group in the queue which may be for DB2 for example. Note that before GPU completes the previous compaction and releases all warps from previous compaction pool GPU may not have all warps in the queue and start another around of compaction. Thus there is not a conflict between consecutive compaction processes. Once all warps are paused on same barrier which forms only one group in the queue GPU may detach all of the paused warps from the queue and empty the queue.

Because warps paused on a barrier at the front of the queue for example DB1 are likely to hit subsequent barriers e.g. DB2 DB3 later on GPU may utilize the technique of compacting only the first warp group in order to be able to group as many divergent warps together into compaction pools as possible when performing compaction for subsequent barriers e.g. DB2 DB3 etc. . By compacting one barrier at a time this technique may improve efficiency of thread compaction by enabling compaction of a greater number of warps in a compaction pool during compaction of subsequent divergence barriers.

In case of multiple barriers GPU may begin performing compaction on divergence barriers earlier i.e. when queue is not full in same fashion and under the same conditions described above. Those conditions may include for example a kernel program contains traditional barriers or incurs frequent long latency operations.

When multiple divergence barriers are present in a kernel and the Boolean expression associated with the warp evaluates to true for at least one warp thread GPU places the warp into a queue and associates a prefix with the warp. The prefix indicates the particular divergence barrier that the warp has reached. As one example scheduler may append a prefix such as 1 to an identifier associated with each of warps and to indicate that those warps have reached divergence barrier DB1. Scheduler may add similar prefixes e.g. 2 3 to warps and to indicate that those warps have reached divergence barriers DB3 and DB2 respectively.

DBM stores each of warps and in a queue. Warps are initially unsorted and are associated with prefixes based on which divergence barrier the warps have reached. DBM initially sorts warps and based on the prefix associated with each of the threads and groups the warps together into compaction groups based on the prefix number.

The group of warps having a prefix corresponding to the earliest divergence barrier e.g. DB1 is referred to as the compaction pool. In the example of compaction pool includes warps and all of which have reached divergence barrier DB1 and therefore include the same prefix.

As described above GPU sorts the warps of compaction pool based on the prefix which is derived based on the divergence barrier number reached and the suffix which is related to the number of active threads in each warp. After GPU pauses all warps except those that have finished execution on barriers and inserts the paused warps into the queue and sorts the warps in the queue GPU detaches the first warp group from the queue which represents the front most barrier in the queue and forms a compaction pool with this group. GPU then performs thread compaction by swapping inactive threads of warps in the compaction pool having a larger number of active threads with active threads of warps in the compaction pool having a larger number of inactive threads until no more inactive warps can be created from the warps of the compaction pool. Once GPU finishes thread compaction of any new warps DBM releases the warps from compaction pool for execution.

At the same time the queue may continue to receive warps that are paused on any barriers after they resume on execution as described above. GPU may sort the newly received warp and sort them in the queue along with the existing warps in the queue using insertion sort as described above. Once all the warps have either paused on a divergence barrier and move into the queue or finish execution and then exit GPU detaches current first warp group from the queue to form a compaction pool and performs compaction on the compaction pool.

Some kernel applications may require even pacing of threads executing the kernel. Kernels that are sensitive to the pace at which threads execute may also complicate the use of divergence barriers. For example when executing such a pace sensitive kernel to which divergence barriers have been added some warps may reach the divergence barriers and pause while other warps may not be paused at a divergence barriers until much later in the instruction sequence of the kernel. Thus divergence barriers may cause uneven thread and warp pacing. To even thread pacing around a first divergence barrier having a first associated Boolean condition a programmer may insert a second divergence barrier instruction having a second associated Boolean expression that is the Boolean complement of the first Boolean condition.

In the above pseudocode GPU executes a loop that includes a first divergence barrier instruction associated with a first Boolean condition. The code includes a second divergence barrier instruction outside the loop that has a second Boolean condition that is the complement of the first Boolean condition. Because the second Boolean condition is the complement of the first GPU will pause each warp at either the first or second divergence barrier instruction thereby ensuring consistent thread pacing

In various examples the method of may further comprise swapping per thread context data for the plurality of threads for which the expression is true before resuming execution of the at least one of the plurality of warps. The thread data of the active thread may comprise register data of the active thread and the thread data of the inactive thread may comprise register data of the inactive thread. In some examples sorting the plurality of warps may comprise sorting the plurality of warps using an insertion sort.

In some examples GPU may further determine for each warp of the plurality of warps for which the expression is true an associated divergence barrier of a plurality of divergence barriers and group each warp of the plurality of warps into a plurality of compaction pools based on the associated divergence barrier of each warp. To sort the plurality of warps GPU may be further configured to sort the plurality of warps comprises sorting the plurality of warps belonging to a same one of the plurality of compaction pools. In various examples the first warp and the second warp comprise warps belong to the same one of the plurality of compaction pools and to resume execution of the at least one of the plurality of warps for which the condition is true GPU may be configured to resume execution of at least one warp of the same one compaction pool.

In some examples GPU may further assign a prefix to each of the plurality of warps based on the divergence barrier associated with each of the plurality of warps and to group the plurality of warp into the at least one compaction pool GPU may group the plurality of warps into at least one compaction pool based on the assigned prefix.

In still other examples GPU may further determine that the plurality of warps includes a warp having all active threads and resuming execution of the warp having all active threads. In yet another example to sort the plurality of warps for which the expression is true GPU may be configure to store the plurality of warps in a queue sort the plurality of warps for which the expression is true based on the number of active threads and store the sorted plurality of warps in the queue.

In yet another example compiler driver may be further configured to determine at least one of a location where divergence is likely to occur and a location that would significantly impact performance within kernel that executes on a plurality of warps. Compiler driver may insert a divergence barrier instruction into the kernel at the at least one location. The Boolean expression of the method of may be associated with the divergence barrier instruction in this example.

The techniques described in this disclosure may be implemented at least in part in hardware software firmware or any combination thereof. For example various aspects of the described techniques may be implemented within one or more processors including one or more microprocessors digital signal processors DSPs application specific integrated circuits ASICs field programmable gate arrays FPGAs or any other equivalent integrated or discrete logic circuitry as well as any combinations of such components. The term processor or processing circuitry may generally refer to any of the foregoing logic circuitry alone or in combination with other logic circuitry or any other equivalent circuitry such as discrete hardware that performs processing.

Such hardware software and firmware may be implemented within the same device or within separate devices to support the various operations and functions described in this disclosure. In addition any of the described units modules or components may be implemented together or separately as discrete but interoperable logic devices. Depiction of different features as modules or units is intended to highlight different functional aspects and does not necessarily imply that such modules or units must be realized by separate hardware or software components. Rather functionality associated with one or more modules or units may be performed by separate hardware firmware and or software components or integrated within common or separate hardware or software components.

The techniques described in this disclosure may also be stored embodied or encoded in a computer readable medium such as a computer readable storage medium that stores instructions. Instructions embedded or encoded in a computer readable medium may cause one or more processors to perform the techniques described herein e.g. when the instructions are executed by the one or more processors. Computer readable storage media may include random access memory RAM read only memory ROM programmable read only memory PROM erasable programmable read only memory EPROM electronically erasable programmable read only memory EEPROM flash memory a hard disk a CD ROM a floppy disk a cassette magnetic media optical media or other computer readable storage media that is tangible.

Computer readable media may include computer readable storage media which corresponds to a tangible storage medium such as those listed above. Computer readable media may also comprise communication media including any medium that facilitates transfer of a computer program from one place to another e.g. according to a communication protocol. In this manner the phrase computer readable media generally may correspond to 1 tangible computer readable storage media which is non transitory and 2 a non tangible computer readable communication medium such as a transitory signal or carrier wave.

Various aspects and examples have been described. However modifications can be made to the structure or techniques of this disclosure without departing from the scope of the following claims.

