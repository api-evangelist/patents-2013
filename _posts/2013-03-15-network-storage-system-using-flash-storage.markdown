---

title: Network storage system using flash storage
abstract: A system can comprise an I/O circuitry, a processor, reconfigurable circuitry, an array of flash storage devices, and a serial interconnect network that is coupled to transfer data between the I/O circuitry, the processor, the reconfigurable circuitry and the flash storage devices. The processor can be configured to designate an interconnect address space for use in communication over the interconnect network among the I/O circuitry, the processor, the reconfigurable circuitry and the flash storage devices. The reconfigurable circuitry can be configured to translate data addresses during transfers of data between the I/O circuitry and the array of flash storage devices. A method to access an array of flash storage devices that are coupled to I/O circuitry over a serial interconnect network can comprise using reconfigurable circuitry to capture data during transfers of data over the serial interconnect network.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09304902&OS=09304902&RS=09304902
owner: Saratoga Speed, Inc.
number: 09304902
owner_city: San Jose
owner_country: US
publication_date: 20130315
---
This patent document pertains generally to data processing and more particularly but not by way of limitation to assembly and storage of large data sets on network storage systems using flash storage.

The large amounts of information generated daily challenge data handling facilities as never before. In the context of today s information generation data is being generated at rates perhaps thousands or tens of thousands of times greater than was the data generation rate in the 1990s. Historically large volumes of data sparked explosive growth in data communications. Responses to growing amounts of data generation centered on improving the movement of data based in increased transmission data rates to enhance throughput in communication channels. For instance transmission pipelines grew from a few tens of megabits per second Mb s transmission rates to several tens of gigabits per second Gb s rates during the 1990s.

In the same period typical storage devices such as hard disk drives HDDs when amassed in sufficient numbers might accommodate large volumes of data but the rates at which data could be stored and retrieved have not scaled at the same rate as the volume of data stored on the devices has increased. Data access rates for HDDs are at similar orders of magnitude today as they were in the 90s.

Fundamental storage subsystems have not integrated technology to enable scaling of effective data storage at the same rate that data generation is growing. Hence the challenge to systems handling large volumes of data is not likely to be alleviated by the combination of contemporary HDD technology with high speed data transmission channels. In order to handle and manage big data information processing facilities will be pressured to utilize larger volumes of storage with higher performance rates for capturing and accessing data.

In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of some example embodiments. It will be evident however to one skilled in the art that the present invention may be practiced without these specific details.

Much of the drawback of current storage subsystems has to do with limitations created by bottlenecks in data transport bandwidth inherent in the storage devices themselves. For example disk drives have significant limitations in latency seek times and data transfer rates which for the purposes of this application these quantities will be cumulatively referred to as the access time. The total amount of storage that may be provided by HDD may approximate the expectations of storage capacities required by big data users but the ability to provide high performance data throughput at those storage capacities is limited by the use of HDD technology and the inherent data access limitations.

Although HDD devices can store large quantities of data with high reliability typical HDD devices lack access speeds high enough to directly service the storage requirements of client devices having high data rate requirements. When high performance network data transmission technologies are coupled to a large capacity storage subsystems based on HDD memory there is a performance mismatch between the data rates sustainable by the network attached devices and the HDDs that caching alone does not remedy to fulfill the performance levels required by big data.

Redundant paths may be created by coupling crossover paths from point to point links between two fabric instances such that some or all of the first fabric instance may be coupled to some or all of a second fabric instance . In this way components at endpoints and or root complexes associated with the first fabric instance may conduct transactions with components associated with endpoints and or further root complexes associated with the second fabric instance even though certain point to point links endpoints and root complexes may become unavailable due to failure or transaction congestion.

In continuing accord with example embodiments the root complex can denote the root of an interconnected I O hierarchy that connects a CPU and local memory not shown to I O components coupled by the interconnection fabric. A root complex may support transactions to or from one or more ports where each port defines an interface to a portion of the hierarchical interconnect fabric. Accordingly one or more root complexes RC may have redundant connections to a first set of switches which are coupled to each other through a first crossover path . The first set of switches can be coupled to a first set of endpoints and a second set of switches . The second set of switches are coupled to one another by a second crossover path as well as to one or more cache modules containing cache memory. The second set of switches can also be coupled to a second set of endpoints and a third set of endpoints .

According to certain example embodiments an interconnection scheme is based on the hierarchical interconnection fabric can interconnect components or devices such as a cache module for example with further devices not shown as endpoints of each connection. An endpoint can be a type of device that may be a requester or completer of a transaction within the interconnection scheme. According to the placement of point to point links and the interconnection of certain types of endpoints a hierarchy of component connections may be established at the endpoints.

For example the first set of endpoints may be I O circuits that can be connected to external networks and the second set of switches may be reconfigurable data transfer circuits. The first set of switches may be connection points establishing connections for transactions between I O circuits associated with the first set of endpoints and the reconfigurable data transfer circuits of the second set of switches . By maintaining certain types of components at certain levels of endpoint groupings a hierarchy of endpoints may be established. By way of further example the second set of endpoints and the third set of endpoints may be storage devices that when coupled to the second set of switches form a storage tier at a further level of an endpoint hierarchy.

The first crossover path provides a redundant path capability between endpoints in the first set of endpoints and the second set of endpoints . For instance a first endpoint in the first set of endpoints may connect through a first redundant path to a completer endpoint in the second set of endpoints . Alternately the first endpoint in the first set of endpoints may alternatively connect through a second redundant path to the same completer endpoint in the second set of endpoints as was the case for the first redundant path . By utilizing the first crossover path to couple the first and second switches SW in the first set of switches the second redundant path couples between the second switch of the first set of switches and the second switch SW of the second set of switches to complete a connection to the completer endpoint in the second set of endpoints .

The second redundant path may be utilized in the case for example that the first switch SW in the second set of switches were to become unavailable due to either a component failure or perhaps from congestion due to high volumes of data stemming from a previous transaction. By a simple application of symmetry the first redundant path and the second redundant path might be thought of as being rotated or reflected about a vertical axis down the center of the figure not shown such that the second endpoint in the first set of endpoints and a first completer endpoint in the fifth set of endpoints would be coupled to one another by either the mirrored first redundant path or the mirrored second redundant path not shown . Application of this symmetric case of redundant paths would provide a coupling of the second endpoint in the first set of endpoints to the completer endpoint in the third set of endpoints in the event that the second switch in the second set of switches were to become unavailable.

A similar situation for providing redundant paths may be visualized by applying the second crossover path to provide alternative connections between the first endpoint in the first set of endpoints to the completer endpoint in the third set of endpoints . The second crossover path would alternatively provide a path for coupling the first endpoint in the third set of endpoints through either of the switches in the second set of switches . By application of alternate path connections similar to that described above for the first redundant path and the second redundant path a similar pair of redundant paths would support the coupling the second endpoint in the first set of endpoints to the first endpoint in the third set of endpoints .

These redundant paths also allow for data to be routed to or from any root complex to any endpoint or cache memory module even if there is a path or endpoint that becomes unavailable due to a component failure or congestion for example. For example if the first switch in the first set of switches were to become unavailable the first root complex may take advantage of an alternate point to point link to the second switch in the first set of switches and gain access to either the second set of endpoints or the third set of endpoints through use of either of the switches in the second set of switches and the second crossover path in a fashion similar to that described above in regard to the first redundant path and the second redundant path . In addition if a first path from an endpoint to a further endpoint or from a root complex to any endpoint is operating at a full capacity a redundant path may be incorporated in order to boost performance by balancing the load between interconnection fabrics.

Various groupings of endpoints and switches have been described as forming a hierarchy within the interconnection fabric. Enumeration is a process initiated by a master processor or server to set up a description of a tree hierarchy corresponding to the interconnection fabric. The enumeration process may start from the tree root root complex and progress down to the leaf nodes of the interconnection fabric. According to some example embodiments a serial communication and connection specification or a serial bus standard such as the peripheral component interconnect express PCIe a configuration process may be implemented as boot software is executed to discover endpoints and assign identifiers to discovered endpoints using a depth first sequence. Once discovered these endpoints can be configured along with further PCIe components such as fabric switches and FPGAs not shown and associated cache modules so that packets can be routed per the enumerated identifiers as well as the address map described below . All levels of switches or endpoints within the tree are named according to the level they reside at. The enumerated hierarchy descriptors are stored in tables. Only the root complexes RC have a map of the tree hierarchy. The identities of data packets may have to be translated from the TCP IP space to this address naming space of the interconnection fabric e.g. PCIe address configuration space .

A further component of redundancy in accordance with some embodiments is the use of a mechanism known as a nontransparent bridge NTB . The NTB allows packets of data to traverse from the tree of one interconnection fabric to another tree in a parallel interconnection fabric. NTBs typically include registers that map the address space of one tree to the address space of another tree and translate relevant fields in packets as they traverse from one tree to another. Since each CPU die typically has its own root complex and there can be multiple CPU die in a subsystem NTBs can be used to allow multiple CPU die to connect to devices throughout the subsystem. The first root complex and the second root complex are coupled to inputs of the NTB . An output of the NTB is coupled to the first switch of the first set of switches .

According to example embodiments the NTB may be programmed to gate block the second output coupled to the second root complex in nominal operation. In the event of a CPU failure in the first root complex and in association with the first fabric instance software running on a CPU in the second root complex can re configure the NTB connection to become transparent thus making the previous nontransparent connection for the second root complex to become viable for managing the routing of data packets through the first fabric instance . Any components in the first fabric instance that were coupled to the first root complex become part of the interconnection fabric managed by the second root complex after reconfiguring the NTB .

According to some example embodiments a switch may appear as a bridge at an input port and have one or more outputs each appearing as a further bridge connected to the bridge at the input. By incorporating this array of bridges switches can create multiple endpoints out of one endpoint and thus allow one endpoint to be shared with multiple devices.

An interconnection fabric can be configured with a peer to peer P2P connectivity scheme according to example embodiments. Given the point to point nature of PCIe switches are used to allow a single device to communicate with multiple devices. Switches route packets either by address or by destination identifier described below . The switches within the switch fabric establish data communication paths between endpoints connected at the boundaries of the switch fabric in a manner similar to the connectivity description of endpoints in . This point to point communication between endpoints may be accomplished with a serial communication and connection specification or the PCIe standard as the management layer. PCIe is a high speed serial computer expansion bus standard that implements improvements over predecessor connection standards such as the peripheral component interconnect PCI standard and the accelerated graphics port AGP standard.

I O circuits can couple the switches to external networks not shown . The switches may be coupled to the reconfigurable data transfer components DATA TRANSFER as well as to one another. The coupling between the switches may be provided by a first crossover path similar to that described for the first set of switches . Each of the switches couples to both reconfigurable data transfer components . The first crossover path and the cross coupling of the switches to each of the two reconfigurable data transfer components can establish the cross coupling and redundant paths as well as the ensuing capabilities as described above in relation to first set of switches and the second set of switches .

The reconfigurable data transfer components are each coupled to a respective dynamic random access memory DRAM as well as to one another through a second crossover path . The reconfigurable data transfer components are also each cross coupled to a first switch array and a second switch array . The cross coupling of the reconfigurable data transfer components to the DRAMs and to one another provides the redundant paths capabilities described above in . The first switch array and the second switch array are coupled to a first flash array and a second flash array respectively.

Either of the external networks above may provide a transfer request to the I O circuits which can be propagated to the root complexes as requests . The requests are in turn propagated by the root complexes to the switch fabric as respective data transfer requests. According to an example embodiment a data transfer request made by the root complex can establish a point to point connection across the P2P switch fabric by establishing a path through particular switch elements in the fabric. The root complex may be involved in initiating a path through the switch fabric and thereafter data transfers from endpoint to endpoint may be conducted without direct intervention by the CPU in each data packet of the transfer. For example a data sourcing endpoint such as the first I O circuit may be coupled to a data receiving endpoint such as the first flash array by a transfer request initiated with the first root complex . The transfer request initiated by the first root complex may establish a memory mapped input output MMIO connection between the two endpoints and thereafter large amounts of data may be transferred between the endpoints without further CPU involvement.

Within the network interface tier of certain embodiments I O circuits provide high speed connections from external networks not shown to the interconnect layer or switch fabric . The I O circuits are coupled to external networks such as InfiniBand Fibre Channel serial attached SCSI SAS and or Ethernet for example. The I O circuits connect these external networks to the switch fabric . The I O circuit can provide a protocol conversion and still produce high speed data communication between the external networks and the switch fabrics .

The I O circuits may be coupled by the switch fabric to the storage tier or the cache tier . The storage tier may be composed of flash circuits arranged on flash boards . The switch fabrics may be implemented with an array of switches including I O circuit switches coupled to the I O circuits and flash switches coupled to the flash boards . The cache tier can be implemented with reconfigurable circuitry. In some embodiments the reconfigurable circuitry comprises field programmable gate arrays FPGA which are interposed in the switch fabric to couple cache modules to various endpoints under the command and direction of the FPGAs .

A server layer can be coupled to the switch fabric and includes CPUs or equivalently servers within the root complexes that act as a control and management agent for respective portions of the switch fabrics . The server layer may be for example a server board .

In some embodiments each server in the server layer may be coupled to a respective switch fabric . Each server and each respective switch fabric may be cross coupled to one another. Each server board may include first crossover paths coupling the servers to one another. A second crossover path can couple the first switch in the first switch fabric to the second switch in the second switch fabric . A third crossover path can couple the first FPGA in the first switch fabric to the second FPGA in the second switch fabric . Redundant paths as described above with reference to can provide access to a parallel or mirrored paths according to cross coupling between the switch fabrics

In redundant switch fabrics such as the first switch fabric and the second switch fabric there may be a server element or server within the root complex associated with each portion of the redundant switch fabric. According to certain embodiments of redundant switch fabric two servers may may be coupled to the switch fabric and reside on a server board and be directly coupled to one another by direct connections implemented on the server board . The direct connections between servers are primarily for communication and management considerations between the servers and additionally may operate as a cross coupled transmission path for data throughput. Various elements within the switch fabric associated with respective servers may be connected to a corresponding element in the complementary parallel switch fabric and thus provide redundancy by establishing paths in another switch fabric when a portion of a path is not available in the first switch fabric

For example the first crossover path or second crossover path can connect a first I O circuit switch in the first switch fabric with a second I O circuit switch in the second switch fabric and a third crossover path may connect a first FPGA to a second FPGA located in the first switch fabric and the second switch fabric respectively. Cross coupling may occur between the respective first and second I O circuit switches and the first and second FPGAs as well as between the first and second FPGAs and the first and second flash boards of the storage tier . Each of these cross coupling connections establish a further crossover point between the first switch fabric and the second switch fabric . The crossover paths can cross coupling described here operates in a similar fashion to the same elements described above in relation to and the establishing of redundant paths.

The storage tier can be composed of storage modules e.g. memory elements and implemented with flash memory or any persistent solid state memory technology that provides data access performance suitable for direct or speed buffered connection to data from external network environments. According to some embodiments flash modules containing flash memory technology can be organized in the storage tier . Yet the flash module connects to the switch fabric with a single connection and operates like a single ended source and sink receiver of data. Flash module access times may also exceed the access performance of HDD technology by several decimal orders of magnitude.

The cache tier can be positioned between the network interface tier and the storage tier . The cache tier can be connected through the switch fabric to both the I O circuits in the network interface tier and the storage modules in the storage tier . The cache tier can be considered as an intermediate storage layer to temporarily store data that ultimately transitions from the I O circuit to storage modules or vice versa. In certain embodiments the cache tier includes individual cache modules each having DRAM for cache storage and flash memory for backup in case of power loss for example.

Reconfigurable circuits which may be FPGAs in typical example embodiments are used to offload the software based processing performed by the servers in the root complexes during data transfers in read and write operations. The FPGAs use reconfigurable hardware circuits for read and write data transfers that are faster than the equivalent data transfer software executing on a processor. The CPU associated with the FPGA in the interconnection fabric or a Master CPU may be involved in providing instructions to configure the FPGA to handle the offloading of data transfers. The FPGA is configured by code executing on the CPU at boot up and may be reconfigured anytime the component configuration in the system is altered i.e. anytime there is a change in the configuration of memory devices . The FPGAs also manage data caching below .

The offloading of data transfer software typically executing on any CPU to the FPGA involves the FPGA being configured to perform translations of logic unit number LUN and logic block address LBA in the iSCSI domain into PCIe configuration addresses to transfer data to flash memory. Data addresses such as those involved in the iSCSI protocol and maintained in the Internet environment are managed in terms of LUNs and LBAs. However in the domain of storage devices maintained in the PCIe P2P connectivity environment addresses corresponding to the storage devices are managed according to the PCIe address configuration space below . A master server or CPU in the system executes boot up software that determines the size and location of all data structures utilized in an address translation and configures the FPGA with this information. The configuration process of FPGAs includes populating tables with address conversion information for establishing LUN maps and LBA maps to convert data addresses from the iSCSI standard to flash memory addresses below for example. Address configuration space for is determined according to the enumeration process above . Once the enumeration process has discovered endpoints in the switch fabric the FPGA can be configured with the tables and mappings that provide LUN and LBA translation to PCIe addresses and will.

More particularly a system in accordance with some embodiments reduces processor i.e. CPU and software involvement and intervention in the control and throughput of dataflow between an external network environment and the storage system. The system receives I O requests from the external network. Typically a basic amount of software must execute in a CPU before a given I O request may be directed to an appropriate read or write hardware procedure e.g. PCI read or write cycles to be carried out. According to particular embodiments most if not all of the typical software execution on the CPUs in the common I O request types may be bypassed through offloading the processing of these requests with the FPGAs . In common cases of reads or writes to blocks or data objects the CPU is bypassed entirely.

For instance the iSCSI approach to data transfer between remote devices requires that the fundamental SCSI commands for controlling the I O requests each be encapsulated in various levels of IP or TCP layers. Each one of these layers of encapsulation must be unwrapped by the CPU and DRAM in the software intensive approach to I O protocol servicing. This iSCSI approach further exacerbates the problem under the software intensive approach to I O processing. In typical example embodiments iSCSI read and write commands are offloaded to the FPGA for processing. All other command types are processed in the server according to the iSCSI target software.

The storage subsystem offloads read and write commands to the FPGA . The I O circuit sends all other command types to the iSCSI target software that has its own dedicated descriptor rings in server memory. The iSCSI target software executes in the CPU. Descriptor rings according to typical example embodiments are a circular natured portion of memory that may be shared between a processor and a PCIe device to buffer information handed off between the two components during data transfers. Particular descriptor rings may be receive rings and transmit rings for example. ISCSI allows multiple PDUs to be placed in a single TCP payload. The NIC places the entire TCP payload on the FPGA s descriptor ring only if all PDUs contained in that payload encapsulate either a read or a write command if any other command types are inter mixed with read or write commands then the NIC will place the entire payload on the iSCSI target descriptor ring.

The FPGA manages the caching of data involved in read and write transactions. The instructions provided to the FPGA from the execution of configuration programs operating in the CPU above can configures the FPGA to implement cache policies. Following the indexing of the LUN map and the LBA map the translated PCIe address is used to determine an address match in cache tags maintained within the FPGA . If there is a cache tag match the data sought in the data transfer request is resident in the cache and may be provided from cache. The access performance for cash is significantly greater than the access times for flash memory. Each cache hit cache tag match in a data transaction significantly improves performance compared to accessing flash memory directly. In this way data transactions may be completed entirely from cache and accomplished significantly faster than would acquiring the data from flash memory. In this way as much of the data involved in read and write transactions as possible is provided from cache and cached respectively. In certain example embodiments it may be possible to accomplish nearly all data transactions from cache.

Thus the cache module is accessible by any other peer component through memory to memory transfers utilizing the interconnect address space . This is possible due to the enumeration process including the cache modules associated with the FPGA in the same manner that all other peer devices associated with the switch fabric are enumerated.

The reconfigurable circuitry i.e. FPGAs store the following offload and cache management data structures in the cache module 

The resulting cache management information is written to configuration registers within the FPGA . The parameter block contains all boundaries sizes and versions of data structures necessary for correct re construction of all data structures when a cache module is moved to another system.

A TCP IP packet including a read command arrives 1 at the network interface tier and more specifically at the input of a NIC. In some embodiments the I O circuits comprise network interface cards NICs . The NIC reads 2 the descriptor and forwards that to the iSCSI receiver ring refer to descriptor rings above and packet buffer . The NIC writes 3 the payload into the packet buffer . The NIC writes 4 the receive status into the iSCSI receiver ring . The NIC sends 5 a new packet received interrupt. Next the receive status is checked 6 and if there is a receive error that is sent to the iSCSI target. Next the payload is read 7 and the header fields parsed.

The LUN field is read and used to index into the LUN map . The LUN base is mapped to a fixed state of the FPGA. If there is a LUN map miss send 8A an error indicator to the iSCSI target. Next the logic block address LBA field is read. The LBA field is used to index 8B into the LBA map which was determined according to the LUN map. If there is an LBA map miss and error indicator is sent to the iSCSI target. LBA map entries are regions with a base LBA and region length indicated in units of blocks. The PCI is the starting address of the region.

From the SCSI LBA map determined by the LUN the PCI address plus block offsets are used 9 for a tag lookup in the cache tags table . A single read command may require multiple cache lookups. If there is a cache hit check 10 whether the data is located in the left or right FPGA . If the data is located in the opposing FPGA send the PDU and frame number over the inter cache bus . If the data is located in the instant FPGA set the lock bit in the priority field .

Next the cache frame address is written 11 into a descriptor at the head of the transmit ring . The head pointer is incremented and the cache frame may serve as packet buffers. The head pointer for the transmit ring is updated 12 . The NIC reads 13 the descriptor from the iSCSI transmit ring and cache data . Next the NIC reads 14 the packet buffer from the cached data .

TCP IP and ethernet headers are prepended 15 to the data read from storage in the headers and retrieved data are transmitted as a packet containing the requested data. The NIC writes 16 a transmit status to the iSCSI transmit ring . The status is checked 17 and if a transmit error is detected and error indication is sent to the iSCSI target. If no transmit error is detected the lock bit is cleared from the priority field in the tags cache.

The iSCSI target software may need to be kept informed of the existence and progress of offloaded read and write commands so it can maintain its data structures including performance counters.

This method of disclosure is not to be interpreted as reflecting an intention that the claimed embodiments require more features than are expressly recited in each claim. Rather as the following claims reflect inventive subject matter lies in less than all features of a single disclosed embodiment. Thus the following claims are hereby incorporated into the Detailed Description with each claim standing on its own as a separate embodiment.

