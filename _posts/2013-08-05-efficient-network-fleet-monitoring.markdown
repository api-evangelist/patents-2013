---

title: Efficient network fleet monitoring
abstract: Methods and apparatus for efficient monitoring of network fleets are described. A list of network addresses of a set of hosts at which resources are to be monitored from a monitoring server of a provider network may be received at the monitoring server. The monitoring server may initiate establishment of a persistent network connection to a monitoring agent installed at a monitored host. A plurality of health messages from the monitoring agent may be obtained via the connection, including a host status entry for the monitored host and a resource status entry for at least one resource configured at the monitored host. A representation of the health messages may be saved in a repository for analysis.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09450700&OS=09450700&RS=09450700
owner: Amazon Technologies, Inc.
number: 09450700
owner_city: Reno
owner_country: US
publication_date: 20130805
---
Many companies and other organizations operate computer networks that interconnect numerous computing systems to support their operations such as with the computing systems being co located e.g. as part of a local network or instead located in multiple distinct geographical locations e.g. connected via one or more private or public intermediate networks . For example data centers housing significant numbers of interconnected computing systems have become commonplace such as private data centers that are operated by and on behalf of a single organization and public data centers that are operated by entities as businesses to provide computing resources to customers. Some public data center operators provide network access power and secure installation facilities for hardware owned by various customers while other public data center operators provide full service facilities that also include hardware resources made available for use by their customers. However as the scale and scope of typical data centers has increased the tasks of provisioning administering and managing the physical computing resources have become increasingly complicated.

The advent of virtualization technologies for commodity hardware has provided benefits with respect to managing large scale computing resources for many customers with diverse needs allowing various computing resources to be efficiently and securely shared by multiple customers. For example virtualization technologies may allow a single physical computing machine to be shared among multiple users by providing each user with one or more virtual machines hosted by the single physical computing machine with each such virtual machine being a software simulation acting as a distinct logical computing system that provides users with the illusion that they are the sole operators and administrators of a given hardware computing resource while also providing application isolation and security among the various virtual machines. Furthermore some virtualization technologies are capable of providing virtual resources that span two or more physical resources such as a single virtual machine with multiple virtual processors that spans multiple distinct physical computing systems.

As the functionality and features supported by providers of virtualized compute storage and networking resources grows and as the fleet of hardware platforms that are used by large scale providers grows the task of implementing administrative control operations such as monitoring the status of the platforms and or the status of virtualized resources and applications executing on the platforms has become more complicated. In addition the speed with which services and systems can be brought back online after outages especially large scale outages has become a critical issue influencing provider reputation and customer satisfaction.

While embodiments are described herein by way of example for several embodiments and illustrative drawings those skilled in the art will recognize that embodiments are not limited to the embodiments or drawings described. It should be understood that the drawings and detailed description thereto are not intended to limit embodiments to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope as defined by the appended claims. The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description or the claims. As used throughout this application the word may is used in a permissive sense i.e. meaning having the potential to rather than the mandatory sense i.e. meaning must . Similarly the words include including and includes mean including but not limited to.

Various embodiments of methods and apparatus for implementing efficient network fleet monitoring are described. Networks set up by an entity such as a company or a public sector organization to provide one or more network accessible services such as various types of cloud based computing storage or database services accessible via the Internet and or other networks to a distributed set of clients may be termed provider networks herein. In the remainder of this document the term client when used as the source or destination of a given communication may refer to any of the computing devices processes hardware modules or software modules that are owned by managed by or allocated to an entity such as an organization a group with multiple users or a single user that is capable of accessing and utilizing at least one network accessible service of the provider network. A given provider network may include numerous data centers which may be distributed across different geographical regions hosting various resource pools such as collections of physical and or virtualized computer servers storage servers with one or more storage devices each networking equipment and the like needed to implement configure and distribute the infrastructure and services offered by the provider. A number of different hardware and or software components some of which may be instantiated or executed at different data centers or in different geographical regions may collectively be used to implement each of the services in various embodiments.

As the number of resources managed by the provider network increases the probability of errors or failures occurring during any given time interval increases correspondingly and as a result the complexity and cost of ensuring that resources remain available may increase as well. In some embodiments one or more sets of dedicated monitoring servers may be designated to track the states of some or all of the remaining resources of a provider network e.g. as part of an effort to maintain high overall levels of resource availability and rapid responsiveness to failures. Lightweight i.e. low overhead monitoring agents may be installed at the resources to be monitored. Persistent or long lived connections may be established from each of the monitoring servers to some set of monitoring agents that is the monitoring servers instead of the monitoring agents or the monitored resources may be responsible for initiating the process of connection establishment. The connections may remain in place for long terms e.g. weeks or months in at least some embodiments e.g. until some maintenance operation or failure occurs. Any appropriate network protocol or combination of protocols may be used for the connections in various implementations such as TCP IP Transmission Control Protocol Internet Protocol UDP User Datagram Protocol and the like. In some embodiments the data transmitted may be encrypted using additional cryptographic protocols e.g. SSL Secure Sockets Layer or TLS Transport Layer Security . In at least one implementation network protocol parameter settings and or network device settings at one or both endpoints of the connections may be tuned to better support connection longevity.

Instead of relying upon traditional network address discovery mechanisms such as the Domain Name Service DNS or other similar services to determine the addresses of the monitored resources to which the persistent connections are to be established in at least some embodiments a monitoring server may be the recipient of a list of network addresses of the monitored resources. For example an administrative service of the provider network such as a host information service responsible for keeping up with mappings between network addresses and various resources may provide the list of network addresses. In at least one embodiment such an administrative service may be configured to periodically and or based on detected events or conditions provide an updated list of network addresses of monitored resources to the monitoring servers. In one embodiment of such a list is not received at a monitoring server within a configurable time period an alarm may be raised or a corrective action may be taken such as a request for an updated list being sent to the administrative service.

Once a persistent monitoring server initiated connection to a given monitored resource is established the flow of health state information may begin. For example in an embodiment in which the resources being monitored include virtualization hosts at which virtualized compute servers are instantiated health state information messages regarding the virtualization host as well as the virtualized compute servers or other resources associated with the virtualization hosts may be transmitted back to the monitoring server. A number of different health states may be defined for a given type of resource in some embodiments for example a virtualized compute resource may be deemed to be in a normal or responsive state if it responds within a threshold time to a configuration request submitted via a network in an overloaded state if it responds with a larger than expected delay or in a disconnected state if it does not respond to network request. Different sets of health states with corresponding state transition rules indicating when a given state is entered from another state may be defined for each type of monitored resource.

In some embodiments the monitoring servers may be configured to store received health state information e.g. in the form of health state snapshots corresponding to health messages received during respective time periods in a persistent storage repository. In at least one embodiment the repository used may comprise one or more nodes of a network accessible storage service implemented at the provider network. Thus in such embodiments health state about resources being used by one service of the provider network such as a virtualized compute service may be stored using resources of another service the storage service . Various kinds of analysis of collected health state information may be performed e.g. covering different time intervals and different state information granularities either at the monitoring servers or by downstream servers that are provided with access to the repository. If an unexpected health state is detected based on the collected information e.g. if a virtualization host appears to be unresponsive or overloaded one or more corrective actions may be initiated in some embodiments. For example a decision to restart or reboot the host may be taken or a decision to direct workload away from the host may be taken. The corrective actions may be determined and or initiated at the monitoring servers and or at analysis servers to which the collected health state information is provided by the monitoring servers.

The number of monitoring servers to be configured to collect health state information may be determined based on a number of factors in various embodiments. For example in some embodiments various performance related capabilities of the types of servers to be configured as monitoring servers may be considered. Connection establishment rate capabilities e.g. how quickly a given type of server can establish outbound connections to large numbers of monitoring agents health message analysis rate capabilities e.g. how many health state messages can be analyzed per unit time storage bandwidth capabilities e.g. how many snapshots of health messages can be written per unit time from the server to a storage service node and the like may be taken into account in such embodiments. In addition to or instead of the performance capabilities availability requirements for the monitoring service itself may play a role in determining the size of the monitoring server fleet for example in one implementation each monitored resource may be connected to at least K distinct monitoring servers under normal operating conditions. The total number of network accessible resources of the provider network for which health information is to be collected or the hierarchical arrangement of such resources may also influence the number of monitoring servers to be deployed. For example if the provider network includes N data centers in one embodiment at least 3N monitoring servers may be set up so that each data center has at least three monitoring servers. In at least some embodiments health state messages may be transmitted across datacenter boundaries or even geographical region boundaries e.g. resources at a given datacenter DC1 may be monitored by servers at a different datacenter DC2.

A number of different mechanisms may be used in various embodiments to ensure that health states are monitored efficiently without consuming excessive resources. For example in one embodiment notification enabled caches may be used by the monitoring servers to store health state records locally e.g. in addition to storing aggregated health state information in a repository . A cache e.g. in volatile memory comprising respective cache entries indicative of health messages received from a plurality of monitoring agents at a plurality of monitored hosts may be instantiated. The cache may have an associated listener mechanism or interface such that a notification is generated by the listener if it is determined that a particular entry has not been updated i.e. that a health message from a particular monitored host or resource has not been received for a configurable time period. Such a notification may be interpreted e.g. by the monitoring server as an indication that the corresponding resource may be in an unexpected or undesired state and corrective actions may be initiated. A notification enabled cache mechanism may require significantly fewer computing resources at the monitoring server to determine unexpected states than alternatives such as polling based schemes. The monitoring agents may be implemented as lightweight threads or processes that run within a management software stack at the monitored resources in some embodiments. For example in embodiments in which the monitored resources include virtualization hosts a monitoring agent process or thread may run within a virtualization manager or in a management virtual machine not accessible from guest virtual machines run on behalf of clients. In at least one implementation health state records may be written to respective files in specified directories by the monitored resources and the files may be read by the monitoring agent before the health state information is transmitted to the monitoring servers. In some embodiments a monitoring server may use respective data structures to store entries for monitored resources in different health states e.g. a hash table or hash map may be used for each state defined for a given type of resource and a record for a given resource may be moved from one data structure to another as the resource state changes. Using this approach the task of finding out exactly how many and which specific resources are in each state may be simplified.

In at least one embodiment multiple monitoring servers may be configured to obtain health state information from a given monitored resource. Such an approach may improve the reliability and availability of the monitoring service itself for example if three monitoring servers establish persistent connections to a given monitoring agent at a virtualization host and one or two of the monitoring servers fail health state of the virtualization host may still be processed successfully at the remaining monitoring server. In some embodiments employing multiple monitoring servers a quorum based policy may be used to make health state decisions e.g. if any one of the three monitoring servers is able to receive a health message during a given time interval indicating that a particular virtualization host is in a healthy responsive state the monitored host may be deemed to be healthy.

In at least some embodiments the monitoring servers may be designed with a goal of rapid recovery in the event of large scale failures such as when a data center loses power or when a large subset of the provider network s resources suffer from a simultaneous network outage. In some such embodiments each monitoring server may be responsible for implementing a carefully designed schedule of connection re establishment with a designated set of monitored resources. An overload avoidance criterion may be used to generate the schedule so that for example instead of repeatedly re trying to establish a connection with a given monitored resource a configurable delay which may be termed a back off delay is introduced between successive connection attempts. The schedule may indicate a sequence in which persistent connections are to be initiated to the monitored resources from a given monitoring server. In some implementations the schedules to be used by different monitoring servers may be coordinated in such a way that the a large number of monitored resources are contacted within a targeted time interval after a failure event is detected. For example the set of monitored resources MR may be partitioned intelligently into subsets MR sub1 MR sub2 MR subN with each of N monitoring servers being responsible for establishing connections with the resources of a respective subset first using respective lists of network addresses provided by the administrative service of the provider network. Using such a systematic approach to recovering monitoring capabilities especially one in which queries to obtain the destination addresses for network connections are not required and in which the monitored resources are not required to try to establish connections from their end may be especially effective in reducing the time taken to resume normal operations after large scale failure events.

Clients that utilize various services whose resources are monitored may be enabled to view or query health state information for resources of interest according to some embodiments. For example in one embodiment one or more programmatic interfaces such as web pages application programming interfaces APIs command line tools or graphical user interfaces may be implemented to support client queries regarding resource health states. In response to a request received via such an interface a representation of the health information available for a specified resource may be provided to the client e.g. by the appropriate monitoring server and or by a downstream analysis server configured to receive aggregated health status from one or more monitoring servers.

In the depicted embodiment each availability container has a respective monitoring server MS group e.g. MS group A in availability container A MS group B in availability container B and MS group C in availability group C. The MS groups may collectively be referred to as an MS fleet. Each MS group may include one or more monitoring servers e.g. MS group A includes MS A and B MS group B includes MS F and G MS group C includes MS K and L. Each monitoring server may be configured to initiate the establishment of at least one persistent connection e.g. a connection expected to last for weeks months or until a maintenance operation or a failure results in a disconnection with each of one or more monitored resources for which the monitoring server is responsible. Some of the resources monitored by a given monitoring server may be located in the same availability container as the monitoring server in the depicted embodiment while other monitored resources may reside in a different availability container thus the persistent connections may cross availability container boundaries and or data center boundaries.

Each availability container may comprise a respective set of monitored resources such as MR set A with MRs A B and C in availability container A MR set B with MRs G H and in availability container B and MR set C with MRs P Q and R in availability container C. Several different types of resources may be included in a given MR set in some embodiments such as virtualization hosts on which virtual compute servers are instantiated the virtual compute servers themselves storage resources networking resources and the like. In at least some embodiments respective monitoring agents such as lightweight low overhead threads or processes may be instantiated at some or all of the MRs . A given monitoring agent may be responsible for responding to connection establishment requests from one or more monitoring servers collecting state information locally at the MRs and transmitting it to the monitoring server s over the connections. Different approaches may be taken regarding the completeness or extent of state information that is transferred to the monitoring servers in various embodiments. Consider an example scenario in which the health state of a particular monitored entity E can be expressed completely in accordance with some agreed upon definition of completeness using N health indicators e.g. as a vector with N elements. In some embodiments each time that a given monitoring agent transmits a health message regarding E to a monitoring server all N health indicators may be included. In other embodiments a differential or delta based approach may be taken to transmitting health information in which only those health indicators that have changes since the last transmission may be included e.g. a particular health message may include N1 health indicators but the next health message may include a different number N2 of health indicators representing only the health related changes at E since the previous health message was sent. Using such a delta based or differential approach may have the benefit of reducing the total amount of health information transmitted since in many cases at most a few health metrics may change between one message and the next . Using a complete set of N indicators in every message may have its own advantages e.g. the inclusion of all the indicators may serve as an affirmative confirmation of the status of all the factors assumed to contribute to the health of the entity E.

As part of its suite of network accessible services the provider network may implement one or more storage services such as a key value data store in some embodiments. At least some monitoring servers may store aggregated representations of the health state information received from the monitored resources at one or more storage service nodes such as storage service nodes A B or C. In one implementation for example a snapshot of the health state of a given set of MRs as of a given point in time T may be generated at an MS based on health messages received during a time interval leading up to T and stored as a storage service node . Such snapshots may be consumed or processed by aggregated status analyzers such as aggregated status analyzers A B or C. In a manner similar to the comprehensive versus differential approaches that may be taken to construct health messages as described above the aggregated representations may also be stored using comprehensive approaches or differential approaches in various embodiments. In a comprehensive approach for example a full list of health indicators may be stored while in the differential approach only changes that have occurred since the last aggregated representation was generated may be included. Instead of batching information over a time period and then saving the collected information to a storage service node in some embodiments the health information may be streamed directly to a storage service node or a different storage destination as soon as it is received at the monitoring servers. In at least some embodiments analysis of collected health state information may be performed initially at the monitoring servers and if necessary e.g. if an unexpected or anomalous state is encountered additional analysis may be performed by the aggregated status analyzers . In one embodiment health status information may be collected from a plurality of MSs at an given aggregated status analyzer and if a discrepancy is found between the health state for a given MR as reported by different MSs a quorum based determination of health state may be made at an aggregated status analyzer as described below with reference to . In some embodiments the aggregated status analyzers may also or instead be configured to perform trend analysis using data collected over days weeks or months e.g. to identify longer term failure trends and or to find ways to improve overall provider network reliability. In some embodiments aggregated status analyzers may not be used. In at least some implementations in addition to or instead of using storage service nodes as repositories of health state information in memory or persistent storage caches at the MSs may be used to store health state entries at least temporarily as described below in further detail.

According to at least some embodiments an administrative service of the provider network such as a host information service may be responsible for maintaining an up to date database of the network addresses of at least a subset of the monitored resources . Thus in scenarios in which the monitored resources include virtualization hosts at which instances of virtual compute servers are launched on behalf of clients the administrative service may track the network addresses e.g. private and or public Internet Protocol addresses assigned to the virtualization hosts and or to the virtual compute server instances. In the depicted embodiment nodes of such a host information service may provide lists of the network addresses of MRs to the MSs . For example host information service node A in availability container A may provide a list of MR network addresses periodically or on request to MSs A and B host information service node B may provide MR network addresses to MSs F and G and host information service node C may provide MR network addresses to MSs K and L. In some embodiments MR network addresses for the entire fleet e.g. for multiple availability zones may be made available to each MS while in other embodiments addresses of only a subset of the MRs may be provided to each MS. In one implementation if an MS does not receive a list of MR addresses from the administrative service during a time period e.g. if the MS determines that an address list has not been received in the last 30 minutes the MS may submit a request to one or more nodes of the administrative service to obtain an up to date address list. The use of the administrative service to proactively provide network addresses of the monitored resources in some embodiments may eliminate the need for the MSs to waste resources trying to obtain addresses from DNS servers or similar address discovery mechanisms.

In some embodiments the administrative service nodes may be configured to provide the MR address lists at a high priority to the MSs in the event that a large scale failure event occurs e.g. if a substantial number of resources go offline as a result of a power outage or a network link disruption so that the MSs can quickly re establish persistent connections to their respective sets of monitored resources during recovery. In such embodiments each MS may determine and implement a respective schedule for connection re establishment and may utilize overload avoidance techniques such as adding delays between attempts to reconnect to a given monitored resource when implementing the schedule.

In at least some embodiments a plurality of monitoring servers may be used to monitor a given resource of the provider network. Such an approach may increase the resiliency of the monitoring service itself in that even if a given monitoring server becomes overloaded or inaccessible resource health monitoring can continue without interruption. illustrates an example of a system in which decisions regarding the health status of virtualization hosts and virtualized resources instantiated at the virtualization hosts are made using a quorum based protocol according to at least some embodiments. Two monitored virtualization hosts A and B are shown. Each monitored virtualization host includes a respective virtualization management software stack and a plurality of virtual compute resources which may also be referred to as virtual machines or compute instances . For example virtualization host A includes management stack A and virtual compute resources A and B while virtualization host B includes management stack B and virtual compute resources K and L.

A virtualization management stack may include for example a hypervisor providing virtualization functionality on top of the bare hardware of the host. A hypervisor may organize the resources of the virtualization host into a plurality of domains in the depicted embodiment with one domain which may be called domain zero being used for administration and the other domains being used for resource instances. An administrative instance of an operating system e.g. as opposed to client accessible operation systems set up on the virtual compute resources may be set up in domain zero in some implementations. In the depicted embodiment a respective monitoring agent e.g. agent A or B may be instantiated within each virtualization management stack e.g. as a daemon process or thread within the administrative instance of the operation system or in the hypervisor. The monitoring agents may each receive connection establishment requests from a plurality of monitoring servers e.g. each of the three monitoring servers A B and C may establish a respective persistent connection to agent A and another respective persistent connection to agent B in the depicted embodiment.

The monitoring agent may be configured to read health status information regarding the virtualization host and or the virtual compute resources from a variety of sources in different embodiments. For example in one embodiment each virtual compute resource e.g. an operating system component at the virtual compute resource may write some set of health status information to a respective file such as status file A B K or L for virtual compute resource A B K and L respectively and the agent may read the contents of the file. Other sources of health state information may also be used in various embodiments such as hypervisor or operating system log files event monitors performance tools application log files and the like. Collected health state information from a given virtualization host including information about the virtualization host itself as well as information about additional resources such the virtualized compute resources may be transmitted by the local agent via the connections to each of the monitoring servers A C in the depicted embodiment.

In some embodiments for any given monitored resource a quorum based health status decision may be obtained using a subset of the plurality of MSs . For example in one implementation if there is a discrepancy between the health status of a given virtualization host or compute resource as detected by the different MSs such that two of the MSs detect state S1 while the third detects state S2 the state detected by the majority of the MSs may be selected as the valid state. In one implementation an optimistic approach to quorum based health determination may be used whereby even if only a single MS receives information from an agent indicating that a particular monitored resource R is healthy at time T while other MSs are either inaccessible or do not have information regarding the state of R as of time T the monitored resource R may be deemed healthy.

A notification enabled cache mechanism may be used in some embodiments in which for example a listener interface or interfaces may be configured to transmit a timeout message to a specified destination such as health message collector if any given status entry is not updated within a configurable time window. Such a notification enabled cache which keeps track of how recently various entries were last modified and proactively informs specified targets when any given entry has not been modified for a specified time may be referred to as an LRU least recently updated least recently used timeout cache . Notification mechanisms other than listener interfaces may be used in other embodiments. In some implementations caches similar to that illustrated in may be implemented using any of various classes defined in an object oriented programming language library such as the Java concurrent utilities package java.util.concurrent .

In some embodiments during its lifetime a monitored resource such as a virtualization host may pass through a number of health states with respect to its connectivity with monitoring resources . illustrates examples of data structures that may be used for tracking state changes of monitored resources according to at least some embodiments. As shown a given monitored resource may be in one of four states disconnected connecting connected and responsive and a respective data structure DS such as a map or hash table may be maintained for each state. Entries in each data structure may represent the monitored resources that are currently in the corresponding state as determined from the perspective of the particular monitoring resource at which the data structures are established. Each of the data structures may represent a notification enabled cache of the type illustrated in in some embodiments. Thus for monitored resources to which a persistent connection is currently not established entries such as A and B may be created in disconnected data structure A. In some implementations when a monitored virtualization host is being initialized or has not yet come online an entry for the host may be created in the disconnected data structure A. When the monitoring server e.g. using health message collector determines that a monitored resource is in the process of responding to a connection establishment request from the monitoring server the entry for that monitored resource may be logically transferred from the disconnected data structure A and to connecting data structure B as indicated by arrow . Thus in the depicted example MR entry K represents a monitored resource that is currently in the process of connecting to the monitoring resource. In some cases an MR may become disconnected before a connection is successfully established in which case its entry may be moved back from connecting data structure B to disconnected data structure as indicated by arrow .

From a connecting state monitored resources may typically move fairly quickly to connected state indicating that the persistent connection is successfully established as indicated by arrow and thence to responsive state indicating that one or more health state messages have been received successfully at the monitoring server as indicated by arrow . Each state transition of a given resource may be reflected by the logical transfer of a corresponding entry for the resource from one data structure to another. In the example shown entries P or Q may be moved from connected data structure C to responsive data structure D as soon as the monitoring server determines that a threshold number of health messages have been received from the corresponding resource for example. At any given time during normal operation the majority of entries may be expected in the responsive data structure D with only a few entries typically in the other states. In the depicted embodiment transitions may occur from any of the other states to the disconnected state as indicated by arrows and . Other states than those shown in may be defined for monitored resources in some embodiments. In at least one embodiment different categories of monitored resources may have respective sets of state transitions defined e.g. virtualization hosts may be in any of one set of states while virtual compute resources may be in any of a different set of states.

The size and distribution of the monitoring server fleet e.g. how many monitoring servers are to be set up and where the monitoring servers are to be located with respect to the monitored resources may be determined based on a variety of factors in different embodiments. For example in one embodiment a number of different performance related capabilities of the servers to be used as monitoring servers may be taken into consideration. As shown in element connection establishment rate capabilities e.g. how quickly network connections can be established from the monitoring servers to the agents which may be a function of the computing power and or networking components at the server storage bandwidth e.g. how many health state snapshots can be written to storage server nodes per unit time from the monitoring servers network bandwidth and the like may be considered. The total size of the monitored resource fleet may influence the size of the monitoring fleet. In some embodiments the monitored resource fleet may be partitioned or sharded into subsets e.g. with an approximately equal number of monitored resources in each partition and a monitoring resource group with N monitoring servers may be set up for each partition. The computing capabilities of the downstream aggregated analysis servers and or availability redundancy requirements for the monitoring servers themselves may also impact the size of the monitoring sever fleet in various embodiments. The number and placement of the monitoring servers may then be determined element e.g. how many monitoring servers are to be established in each data center or availability container and specifically where e.g. in which network subnet the monitoring servers should be set up.

The appropriate number of monitoring servers may then be instantiated or launched element . A list of network addresses of the monitored resources for which a given monitoring server is responsible may then be received at the monitoring server e.g. from an administrative service such as a host information service of the provider network element . The monitoring server may then initiate establishment of persistent or long lived connections to the monitoring agents at some or all of the monitored resources element . Using the persistent connections set up from the monitoring servers health messages may be collected from the monitoring agents element comprising for example status information for virtualization hosts as well as other resources instantiated on the virtualization hosts. Health message contents may include a comprehensive list of health indicators in some implementations while in other implementations a differential approach may be used in which only those health indicators that have changed since the previous message was generated are included. In at least some embodiments decisions regarding the health state of a monitored resource e.g. a virtualization host a resource instantiated at a virtualized host or an application running in a virtualized host may be made using a quorum based protocol element . For example if N monitoring servers are configured to receive health messages regarding a given resource some quorum number less than N of the monitoring servers may be sufficient to determine the health state of the given resource. If an unexpected health state is identified e.g. either at one monitoring server by a plurality of monitoring servers involved in a quorum protocol or at a downstream analysis server corrective actions may be initiated element such as restarts alert generation and the like. In some embodiments health status information received from the agents may be passed on to analysis servers via a storage service of the provider network e.g. a storage service node may be used as the destination of health state information snapshots collected generated at the monitoring server and as a source from which the analysis servers obtain input for their analysis. Such snapshots may also be either comprehensive e.g. each snapshot includes the complete set of state indicators available or differential e.g. at least some snapshots only include changes since the previous snapshot in different implementations. In some embodiments instead of collecting health messages at the monitoring servers for a time period and then generating snapshots the health information may be streamed in real time to the storage service nodes.

In addition to aggregating and storing the data in the repository a local cache may be maintained of the health information extracted from the messages in the depicted embodiment. For example a respective entry may be maintained in a notification enabled cache for each monitored resource element and the entry may be updated whenever a health message regarding the monitored resource is received. An LRU timeout cache may be used in some implementations with a listener configured to generate a notification if any given entry is not updated within a configurable time window. If a notification is received e.g. at the message collector indicating that a health message from a given agent has not been received during a time interval in the depicted embodiment an attempt may be made to contact the agent element . If the agent sends an updated health message the cache entry may be updated accordingly. If the agent remains unresponsive element the resource s associated with the agent may be deemed to be in an unexpected or unhealthy state and corrective actions such as raising an alarm or restarting the resources may be taken.

As shown in element of an occurrence of a large scale failure event may be detected e.g. by a loss of connectivity to a threshold number of monitored resources from one or more monitoring servers . The set of resources to which connectivity is to be re established from a given monitoring server may be determined e.g. by examining the contents of a disconnected data structure similar to A element . A schedule for re establishing the connections may be determined element according to which the resources are contacted in a pre determined order. Appropriate back off delays may be included in the schedule for various eventualities to avoid thrashing or overload. For example delays may be introduced between connection establishment retries in case a given monitored resource does not respond in a timely fashion to a connection establishment request or between other recovery related operations such as restart commands being issued if a monitored resource remains in a connecting or connected state for a duration longer than a threshold. The connections from the monitoring servers to the monitored resources may be re established in accordance with the schedule element . The sequence of operations specified in the schedules of different monitoring servers may be selected and coordinated in some embodiments in such a way that the work associated with recovery from the large scale failure is balanced across the affected portions of the provider network e.g. that not all the monitoring servers attempt to utilize the same network paths or links simultaneously or near simultaneously.

In some embodiments at least a portion of the health state information collected from the monitored resources may be provided to clients of the services being implemented in the provider network. For example clients to whom virtual compute resources have been allocated may be enabled to query the state of the allocated resources via a programmatic interface. In some embodiments in addition to or instead of the health states of the virtual resources and hosts the health states of various types of client applications may be monitored using the kinds of techniques described above e.g. using persistent connections whose establishment is initiated by the monitoring servers . is a flow diagram illustrating aspects of operations that may be performed to enable clients of a provider network s services to view resource health state information according to at least some embodiments. As shown in element one or more programmatic interfaces such as web pages or web sites application programming interfaces APIs command line tools or graphical user interfaces may be implemented to enable clients to submit resource status queries and or to specify the resources of interest and the kinds of status information of interest. In some embodiments at least a subset of the status information collected by the monitoring servers may not be exposed to clients e.g. while state information about virtualized resources allocated to the clients may be provided to the clients state information about physical hosts or servers at which the virtualized resources are instantiated may not be available to clients.

As shown in element the metrics of interest may be collected from monitored resources of interest to the client e.g. using files that are written to by the resources or applications and read by the monitoring agents in a manner similar to that illustrated in . When a client request for health status of a specified resource is received element the request may be validated e.g. to ensure that the client is authorized to view the requested information . If the client has the appropriate permissions an indication of the requested information may be provided element .

It is noted that in various embodiments operations other than those illustrated in the flow diagrams of may be implemented to support network fleet monitoring and that some of the operations shown may not be implemented or may be implemented in a different order or in parallel rather than sequentially.

The techniques described above of monitoring resource states using long lasting server initiated connections and lightweight monitoring agents may be useful in a variety of different scenarios. For example for large provider networks with tens of thousands of monitored virtualization hosts alternative approaches in which the monitored entities are required to try to initiate connection establishment with monitoring servers may sometimes result in thrashing or overload situations especially after large scale failures while the systematic schedule driven approaches to connection establishment described above may avoid such problematic scenarios. In addition using low overhead monitoring agents at virtualization hosts may increase the fraction of the hosts computing capabilities that can be allocated for client use. The ability to use service provided network addresses for the monitored resources instead of relying on dynamically querying for network addresses may also reduce the variability of the time taken to respond to certain types of outages.

In at least some embodiments a server that implements a portion or all of one or more of the technologies described herein including the techniques to implement monitoring resources monitored resources storage service nodes aggregated status analyzers and host information service nodes may include a general purpose computer system that includes or is configured to access one or more computer accessible media. illustrates such a general purpose computing device . In the illustrated embodiment computing device includes one or more processors coupled to a system memory via an input output I O interface . Computing device further includes a network interface coupled to I O interface .

In various embodiments computing device may be a uniprocessor system including one processor or a multiprocessor system including several processors e.g. two four eight or another suitable number . Processors may be any suitable processors capable of executing instructions. For example in various embodiments processors may be general purpose or embedded processors implementing any of a variety of instruction set architectures ISAs such as the x86 PowerPC SPARC or MIPS ISAs or any other suitable ISA. In multiprocessor systems each of processors may commonly but not necessarily implement the same ISA.

System memory may be configured to store instructions and data accessible by processor s . In various embodiments system memory may be implemented using any suitable memory technology such as static random access memory SRAM synchronous dynamic RAM SDRAM nonvolatile Flash type memory or any other type of memory. In the illustrated embodiment program instructions and data implementing one or more desired functions such as those methods techniques and data described above are shown stored within system memory as code and data .

In one embodiment I O interface may be configured to coordinate I O traffic between processor system memory and any peripheral devices in the device including network interface or other peripheral interfaces such as various types of persistent and or volatile storage devices used to store physical replicas of data object partitions. In some embodiments I O interface may perform any necessary protocol timing or other data transformations to convert data signals from one component e.g. system memory into a format suitable for use by another component e.g. processor . In some embodiments I O interface may include support for devices attached through various types of peripheral buses such as a variant of the Peripheral Component Interconnect PCI bus standard or the Universal Serial Bus USB standard for example. In some embodiments the function of I O interface may be split into two or more separate components such as a north bridge and a south bridge for example. Also in some embodiments some or all of the functionality of I O interface such as an interface to system memory may be incorporated directly into processor .

Network interface may be configured to allow data to be exchanged between computing device and other devices attached to a network or networks such as other computer systems or devices as illustrated in through for example. In various embodiments network interface may support communication via any suitable wired or wireless general data networks such as types of Ethernet network for example. Additionally network interface may support communication via telecommunications telephony networks such as analog voice networks or digital fiber communications networks via storage area networks such as Fibre Channel SANs or via any other suitable type of network and or protocol.

In some embodiments system memory may be one embodiment of a computer accessible medium configured to store program instructions and data as described above for through for implementing embodiments of the corresponding methods and apparatus. However in other embodiments program instructions and or data may be received sent or stored upon different types of computer accessible media. Generally speaking a computer accessible medium may include non transitory storage media or memory media such as magnetic or optical media e.g. disk or DVD CD coupled to computing device via I O interface . A non transitory computer accessible storage medium may also include any volatile or non volatile media such as RAM e.g. SDRAM DDR SDRAM RDRAM SRAM etc. ROM etc. that may be included in some embodiments of computing device as system memory or another type of memory. Further a computer accessible medium may include transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as a network and or a wireless link such as may be implemented via network interface . Portions or all of multiple computing devices such as that illustrated in may be used to implement the described functionality in various embodiments for example software components running on a variety of different devices and servers may collaborate to provide the functionality. In some embodiments portions of the described functionality may be implemented using storage devices network devices or special purpose computer systems in addition to or instead of being implemented using general purpose computer systems. The term computing device as used herein refers to at least all these types of devices and is not limited to these types of devices.

Various embodiments may further include receiving sending or storing instructions and or data implemented in accordance with the foregoing description upon a computer accessible medium. Generally speaking a computer accessible medium may include storage media or memory media such as magnetic or optical media e.g. disk or DVD CD ROM volatile or non volatile media such as RAM e.g. SDRAM DDR RDRAM SRAM etc. ROM etc. as well as transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as network and or a wireless link.

The various methods as illustrated in the Figures and described herein represent exemplary embodiments of methods. The methods may be implemented in software hardware or a combination thereof. The order of method may be changed and various elements may be added reordered combined omitted modified etc.

Various modifications and changes may be made as would be obvious to a person skilled in the art having the benefit of this disclosure. It is intended to embrace all such modifications and changes and accordingly the above description to be regarded in an illustrative rather than a restrictive sense.

