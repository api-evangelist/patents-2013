---

title: Auto-commit memory
abstract: Apparatuses, systems, methods, and computer program products are disclosed. A method includes processing metadata for data saved from a volatile memory buffer to a non-volatile storage medium. The data may be saved in response to a trigger event for a volatile memory buffer. A method includes locating saved data on a non-volatile storage medium. A method includes providing access to saved data after a trigger event based on processed metadata.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09218278&OS=09218278&RS=09218278
owner: SanDisk Technologies, Inc.
number: 09218278
owner_city: Plano
owner_country: US
publication_date: 20130315
---
This disclosure relates to auto commit memory and more particularly to exposing auto commit memory management operations.

Volatile memory such as random access memory RAM typically has faster access times than non volatile storage such as NAND flash magnetic hard disk drives or the like. While the capacities of volatile memory continue to increase as the price of volatile memory decreases volatile memory remains more expensive per unit of capacity than most non volatile storage. This often leads to design tradeoffs between the speed and performance of volatile memory and the lower price of non volatile storage at larger capacities.

Methods for auto commit memory are presented. In one embodiment a method includes processing metadata for data saved from a volatile memory buffer to a non volatile storage medium. Data in certain embodiments is saved in response to a trigger event for a volatile memory buffer. A method in a further embodiment includes locating saved data on a non volatile storage medium. In another embodiment a method includes providing access to saved data after a trigger event based on processed metadata.

Apparatuses for auto commit memory are presented. In one embodiment a volatile memory buffer is within an isolation zone of a non volatile memory device. A memory buffer in certain embodiments is configured to store data in a non volatile memory device in response to a trigger event. A commit agent in one embodiment is configured to provide access to data from a non volatile memory device in response to recovery from a trigger event.

In one embodiment an auto commit memory is configured to present one or more volatile auto commit buffers to a host computing device. One or more auto commit buffers in certain embodiments are configured to write data from the one or more auto commit buffers to a non volatile recording medium. A storage management layer in one embodiment is configured to allocate one or more auto commit buffers to a user. In a further embodiment a commit agent is configured to provide a user with access to data from a non volatile recording medium after the data has been written to the non volatile recording medium.

Apparatuses for auto commit memory management are presented. In one embodiment an apparatus includes an auto commit memory for a non volatile memory device. An auto commit memory in certain embodiments is configured to commit stored data from the auto commit buffer to a non volatile memory medium of a non volatile memory device in response to a restart event. In a further embodiment a populate module is configured to transfer data from a non volatile memory medium to an auto commit memory within a non volatile memory device in response to a populate request. A destage module in one embodiment is configured to transfer data from an auto commit memory to a non volatile memory medium within a non volatile memory device in response to a destage request.

Systems for auto commit memory management are presented. In one embodiment a system includes a device driver for a non volatile storage device. In certain embodiments a device driver is configured to execute on a processor of a host computing device. In one embodiment a device driver includes an auto commit request module configured to receive a destage request for a memory buffer within an isolation zone of a non volatile recording device. A memory buffer in certain embodiments is configured to automatically commit stored data from an auto commit buffer to a non volatile recording medium of a non volatile recording device. A device driver in another embodiment includes a flush module configured to execute a serializing instruction to flush data from a cache of a processor to a memory buffer in response to a destage request. A system in one embodiment includes a storage controller disposed on a non volatile recording device. In a further embodiment a storage controller includes a destage module configured to transfer data from a memory buffer to a non volatile recording medium within a non volatile recording device in response to a destage request.

Reference throughout this specification to features advantages or similar language does not imply that all of the features and advantages that may be realized with the present disclosure should be or are in any single embodiment of the disclosure. Rather language referring to the features and advantages is understood to mean that a specific feature advantage or characteristic described in connection with an embodiment is included in at least one embodiment of the present disclosure. Thus discussion of the features and advantages and similar language throughout this specification may but do not necessarily refer to the same embodiment.

Furthermore the described features advantages and characteristics of the disclosure may be combined in any suitable manner in one or more embodiments. One skilled in the relevant art will recognize that the disclosure may be practiced without one or more of the specific features or advantages of a particular embodiment. In other instances additional features and advantages may be recognized in certain embodiments that may not be present in all embodiments of the disclosure. These features and advantages of the present invention will become more fully apparent from the following description and appended claims or may be learned by the practice of the disclosure as set forth hereinafter.

Many of the functional units described in this specification have been labeled as modules in order to more particularly emphasize their implementation independence. For example a module may be implemented as a hardware circuit comprising custom VLSI circuits or gate arrays off the shelf semiconductors such as logic chips transistors or other discrete components. A module may also be implemented in programmable hardware devices such as field programmable gate arrays programmable array logic programmable logic devices or the like.

Modules may also be implemented in software for execution by various types of processors. An identified module of executable code may for instance comprise one or more physical or logical blocks of computer instructions which may for instance be organized as an object procedure or function. Nevertheless the executables of an identified module need not be physically located together but may comprise disparate instructions stored in different locations which when joined logically together comprise the module and achieve the stated purpose for the module.

Indeed a module of executable code may be a single instruction or many instructions and may even be distributed over several different code segments among different programs and across several memory devices. Similarly operational data may be identified and illustrated herein within modules and may be embodied in any suitable form and organized within any suitable type of data structure. The operational data may be collected as a single data set or may be distributed over different locations including over different storage devices and may exist at least partially merely as electronic signals on a system or network. Where a module or portions of a module are implemented in software the software portions are stored on one or more computer readable media.

Reference throughout this specification to one embodiment an embodiment or similar language means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present disclosure. Thus appearances of the phrases in one embodiment in an embodiment and similar language throughout this specification may but do not necessarily all refer to the same embodiment.

Reference to a computer readable medium may take any form capable of storing machine readable instructions on a digital processing apparatus. A computer readable medium may be embodied by a compact disk digital video disk a magnetic tape a Bernoulli drive a magnetic disk a punch card flash memory integrated circuits or other digital processing apparatus memory device.

Furthermore the described features structures or characteristics of the disclosure may be combined in any suitable manner in one or more embodiments. In the following description numerous specific details are provided such as examples of programming software modules user selections network transactions database queries database structures hardware modules hardware circuits hardware chips etc. to provide a thorough understanding of embodiments of the disclosure. One skilled in the relevant art will recognize however that the disclosure may be practiced without one or more of the specific details or with other methods components materials and so forth. In other instances well known structures materials or operations are not shown or described in detail to avoid obscuring aspects of the disclosure.

The schematic flow chart diagrams included herein are generally set forth as logical flow chart diagrams. As such the depicted order and labeled steps are indicative of one embodiment of the presented method. Other steps and methods may be conceived that are equivalent in function logic or effect to one or more steps or portions thereof of the illustrated method. Additionally the format and symbols employed are provided to explain the logical steps of the method and are understood not to limit the scope of the method. Although various arrow types and line types may be employed in the flow chart diagrams they are understood not to limit the scope of the corresponding method. Indeed some arrows or other connectors may be used to indicate only the logical flow of the method. For instance an arrow may indicate a waiting or monitoring period of unspecified duration between enumerated steps of the depicted method. Additionally the order in which a particular method occurs may or may not strictly adhere to the order of the corresponding steps shown.

The host stores data in the storage device and communicates data with the storage device via a communications connection not shown . The storage device may be internal to the host or external to the host . The communications connection may be a bus a network or other manner of connection allowing the transfer of data between the host and the storage device . In one embodiment the storage device is connected to the host by a PCI connection such as PCI express PCI e . The storage device may be a card that plugs into a PCI e connection on the host .

The storage device also has a primary power connection that connects the storage device with a primary power source that provides the storage device with the power that it needs to perform data storage operations such as reads writes erases etc. The storage device under normal operating conditions receives the necessary power from the primary power source over the primary power connection . In certain embodiments such as the embodiment shown in the primary power connection connects the storage device to the host and the host acts as the primary power source that supplies the storage device with power. In certain embodiments the primary power connection and the communications connection discussed above are part of the same physical connection between the host and the storage device . For example the storage device may receive power over a PCI connection.

In other embodiments the storage device may connect to an external power supply via the primary power connection . For example the primary power connection may connect the storage device with a primary power source that is a power converter often called a power brick . Those in the art will appreciate that there are various ways by which a storage device may receive power and the variety of devices that can act as the primary power source for the storage device .

The storage device provides nonvolatile storage memory and or recording media for the host . shows the storage device comprising a write data pipeline a read data pipeline nonvolatile memory a storage controller a power management apparatus and a secondary power supply . The storage device may contain additional components that are not shown in order to provide a simpler view of the storage device .

The nonvolatile memory stores data such that the data is retained even when the storage device is not powered. Examples of nonvolatile memory include flash memory nano random access memory nano RAM or NRAM nanocrystal wire based memory silicon oxide based sub 10 nanometer process memory graphene memory Silicon Oxide Nitride Oxide Silicon SONOS Resistive random access memory RRAM programmable metallization cell PMC conductive bridging RAM CBRAM magneto resistive RAM MRAM dynamic RAM DRAM phase change RAM PRAM or other non volatile solid state storage media. In other embodiments the non volatile memory may comprise magnetic media optical media or other types of non volatile storage media. For example in those embodiments the non volatile storage device may comprise a hard disk drive an optical storage drive or the like.

While the non volatile memory is referred to herein as memory media in various embodiments the non volatile memory may more generally comprise a non volatile recording media capable of recording data the non volatile recording media may be referred to as a non volatile memory media a non volatile storage media or the like. Further the non volatile storage device in various embodiments may comprise a non volatile recording device a non volatile memory device a non volatile storage device or the like.

The storage device also includes a storage controller that coordinates the storage and retrieval of data in the nonvolatile memory . The storage controller may use one or more indexes to locate and retrieve data and perform other operations on data stored in the storage device . For example the storage controller may include a groomer for performing data grooming operations such as garbage collection.

As shown the storage device in certain embodiments implements a write data pipeline and a read data pipeline an example of which is described in greater detail below with regard to . The write data pipeline may perform certain operations on data as the data is transferred from the host into the nonvolatile memory . These operations may include for example error correction code ECC generation encryption compression and others. The read data pipeline may perform similar and potentially inverse operations on data that is being read out of nonvolatile memory and sent to the host .

The storage device also includes a secondary power supply that provides power in the event of a complete or partial power disruption resulting in the storage device not receiving enough electrical power over the primary power connection . A power disruption is any event that unexpectedly causes the storage device to stop receiving power over the primary power connection or causes a significant reduction in the power received by the storage device over the primary power connection . A significant reduction in power in one embodiment includes the power falling below a predefined threshold. The predefined threshold in a further embodiment is selected to allow for normal fluctuations in the level of power from the primary power connection . For example the power to a building where the host and the storage device may go out. A user action such as improperly shutting down the host providing power to the storage device a failure in the primary power connection or a failure in the primary power supply may cause the storage device to stop receiving power. Numerous varied power disruptions may cause unexpected power loss for the storage device .

The secondary power supply may include one or more batteries one or more capacitors a bank of capacitors a separate connection to a power supply or the like. In one embodiment the secondary power supply provides power to the storage device for at least a power hold up time during a power disruption or other reduction in power from the primary power connection . The secondary power supply in a further embodiment provides a power hold up time long enough to enable the storage device to flush data that is not in nonvolatile memory into the nonvolatile memory . As a result the storage device can preserve the data that is not permanently stored in the storage device before the lack of power causes the storage device to stop functioning. In certain implementations the secondary power supply may comprise the smallest capacitors possible that are capable of providing a predefined power hold up time to preserve space reduce cost and simplify the storage device . In one embodiment one or more banks of capacitors are used to implement the secondary power supply as capacitors are generally more reliable require less maintenance and have a longer life than other options for providing secondary power.

In one embodiment the secondary power supply is part of an electrical circuit that automatically provides power to the storage device upon a partial or complete loss of power from the primary power connection . Similarly the system may be configured to automatically accept or receive electric power from the secondary power supply during a partial or complete power loss. For example in one embodiment the secondary power supply may be electrically coupled to the storage device in parallel with the primary power connection so that the primary power connection charges the secondary power supply during normal operation and the secondary power supply automatically provides power to the storage device in response to a power loss. In one embodiment the system further includes a diode or other reverse current protection between the secondary power supply and the primary power connection to prevent current from the secondary power supply from reaching the primary power connection . In another embodiment the power management apparatus may enable or connect the secondary power supply to the storage device using a switch or the like in response to reduced power from the primary power connection .

An example of data that is not yet in the nonvolatile memory may include data that may be held in volatile memory as the data moves through the write data pipeline . If data in the write data pipeline is lost during a power outage i.e. not written to nonvolatile memory or otherwise permanently stored corruption and data loss may result.

In certain embodiments the storage device sends an acknowledgement to the host at some point after the storage device receives data to be stored in the nonvolatile memory . The write data pipeline or a sub component thereof may generate the acknowledgement. It is advantageous for the storage device to send the acknowledgement as soon as possible after receiving the data.

In certain embodiments the write data pipeline sends the acknowledgement before data is actually stored in the nonvolatile memory . For example the write data pipeline may send the acknowledgement while the data is still in transit through the write data pipeline to the nonvolatile memory . In such embodiments it is highly desirable that the storage device flush all data for which the storage controller has sent an acknowledgement to the nonvolatile memory before the secondary power supply loses sufficient power in order to prevent data corruption and maintain the integrity of the acknowledgement sent.

In addition in certain embodiments some data within the write data pipeline may be corrupted as a result of the power disruption. A power disruption may include a power failure as well as unexpected changes in power levels supplied. The unexpected changes in power levels may place data that is in the storage device but not yet in nonvolatile memory at risk. Data corruption may begin to occur before the power management apparatus is even aware or notified that there has been a disruption in power.

For example the PCI e specification indicates that in the event that a power disruption is signaled data should be assumed corrupted and not stored in certain circumstances. Similar potential corruption may occur for storage devices connected to hosts using other connection types such as PCI serial advanced technology attachment serial ATA or SATA parallel ATA PATA small computer system interface SCSI IEEE 1394 FireWire Fiber Channel universal serial bus USB PCIe AS or the like. A complication may arise when a power disruption occurs meaning that data received from that point to the present time may be presumed corrupt a period of time passes the disruption is sensed and signaled and the power management apparatus receives the signal and becomes aware of the power disruption. The lag between the power disruption occurring and the power management apparatus discovering the power disruption can allow corrupt data to enter the write data pipeline . In certain embodiments this corrupt data should be identified and not stored to the nonvolatile memory . Alternately this corrupt data can be stored in the nonvolatile memory and marked as corrupt as described below. For simplicity of description identifying corrupt data and not storing the data to the nonvolatile memory will be primarily used to describe the functions and features herein. Furthermore the host should be aware that this data was not stored or alternatively data for which integrity is a question is not acknowledged until data integrity can be verified. As a result corrupt data should not be acknowledged.

The storage device also includes a power management apparatus . In certain embodiments the power management apparatus is implemented as part of the storage controller . The power management apparatus may be for instance a software driver or be implemented in firmware for the storage device . In other embodiments the power management apparatus may be implemented partially in a software driver and partially in the storage controller or the like. In one embodiment at least a portion of the power management apparatus is implemented on the storage device as part of the storage controller or the like so that the power management apparatus continues to function during a partial or complete power loss using power from the secondary power supply even if the host is no longer functioning.

In one embodiment the power management apparatus initiates a power loss mode in the storage device in response to a reduction in power from the primary power connection . During the power loss mode the power management apparatus in one embodiment flushes data that is in the storage device that is not yet stored in nonvolatile memory into the nonvolatile memory . In particular embodiments the power management apparatus flushes the data that has been acknowledged and is in the storage device that is not yet stored in nonvolatile memory into the nonvolatile memory . In certain embodiments described below the power management apparatus may adjust execution of data operations on the storage device to ensure that essential operations complete before the secondary power supply loses sufficient power to complete the essential operations i.e. during the power hold up time that the secondary power supply provides.

In certain embodiments the essential operations comprise those operations for data that has been acknowledged as having been stored such as acknowledged write operations. In other embodiments the essential operations comprise those operations for data that has been acknowledged as having been stored and erased. In other embodiments the essential operations comprise those operations for data that have been acknowledged as having been stored read and erased. The power management apparatus may also terminate non essential operations to ensure that those non essential operations do not consume power unnecessarily and or do not block essential operations from executing for example the power management apparatus may terminate erase operations read operations unacknowledged write operations and the like.

In one embodiment terminating non essential operations preserves power from the secondary power supply allowing the secondary power supply to provide the power hold up time. In a further embodiment the power management apparatus quiesces or otherwise shuts down operation of one or more subcomponents of the storage device during the power loss mode to conserve power from the secondary power supply . For example in various embodiments the power management apparatus may quiesce operation of the read data pipeline a read direct memory access DMA engine and or other subcomponents of the storage device that are associated with non essential operations.

The power management apparatus may also be responsible for determining what data was corrupted by the power disruption preventing the corrupt data from being stored in nonvolatile memory and ensuring that the host is aware that the corrupted data was never actually stored on the storage device . This prevents corruption of data in the storage device resulting from the power disruption.

In one embodiment the system includes a plurality of storage devices . The power management apparatus in one embodiment manages power loss modes for each storage device in the plurality of storage devices providing a system wide power loss mode for the plurality of storage devices . In a further embodiment each storage device in the plurality of storage devices includes a separate power management apparatus that manages a separate power loss mode for each individual storage device . The power management apparatus in one embodiment may quiesce or otherwise shut down one or more storage devices of the plurality of storage devices to conserve power from the secondary power supply for executing essential operations on one or more other storage devices .

In one embodiment the system includes one or more adapters for providing electrical connections between the host and the plurality of storage devices . An adapter in various embodiments may include a slot or port that receives a single storage device an expansion card or daughter card that receives two or more storage devices or the like. For example in one embodiment the plurality of storage devices may each be coupled to separate ports or slots of the host . In another example embodiment one or more adapters such as daughter cards or the like may be electrically coupled to the host i.e. connected to one or more slots or ports of the host and the one or more adapters may each provide connections for two or more storage devices .

In one embodiment the system includes a circuit board such as a motherboard or the like that receives two or more adapters such as daughter cards or the like and each adapter receives two or more storage devices . In a further embodiment the adapters are coupled to the circuit board using PCI e slots of the circuit board and the storage devices are coupled to the adapters using PCI e slots of the adapters. In another embodiment the storage devices each comprise a dual in line memory module DIMM of non volatile solid state storage such as Flash memory or the like. In one embodiment the circuit board the adapters and the storage devices may be external to the host and may include a separate primary power connection . For example the circuit board the adapters and the storage devices may be housed in an external enclosure with a power supply unit PSU and may be in communication with the host using an external bus such as eSATA eSATAp SCSI FireWire Fiber Channel USB PCIe AS or the like. In another embodiment the circuit board may be a motherboard of the host and the adapters and the storage devices may be internal storage of the host .

In view of this disclosure one of skill in the art will recognize many configurations of adapters and storage devices for use in the system . For example each adapter may receive two storage devices four storage devices or any number of storage devices. Similarly the system may include one adapter two adapters three adapters four adapters or any supported number of adapters. In one example embodiment the system includes two adapters and each adapter receives four storage devices for a total of eight storage devices .

In one embodiment the secondary power supply provides electric power to each of a plurality of storage devices . For example the secondary power supply may be disposed in a circuit on a main circuit board or motherboard and may provide power to several adapters. In a further embodiment the system includes a plurality of secondary power supplies that each provide electric power to a subset of a plurality of storage devices . For example in one embodiment each adapter may include a secondary power supply for storage devices of the adapter. In a further embodiment each storage device may include a secondary power supply for the storage device . In view of this disclosure one of skill in the art will recognize different arrangements of secondary power supplies for providing power to a plurality of storage devices .

The systems methods and apparatus for power reduction management described above may be leveraged to implement an auto commit memory capable of implementing memory semantic write operations e.g. persistent writes at CPU memory write granularity and speed. By guaranteeing that certain commit actions for the write operations will occur even in the case of a power failure or other restart event in certain embodiments volatile memory such as DRAM SRAM BRAM or the like may be used as considered or represented as non volatile.

A restart event as used herein comprises an intentional or unintentional loss of power to at least a portion of the host computing device or system and or a non volatile storage device. A restart event may comprise a system reboot reset or shutdown event a power fault power loss or power failure event or another interruption or reduction of power. By guaranteeing certain commit actions the auto commit memory may allow storage clients to resume execution states even after a restart event may allow the storage clients to persist different independent data sets or the like.

As used herein the term memory semantic operations or more generally memory operations refers to operations having a granularity synchronicity and access semantics of volatile memory accesses using manipulatable memory pointers or the like. Memory semantic operations may include but are not limited to load store peek poke write read set clear and so on. Memory semantic operations may operate at a CPU level of granularity e.g. single bytes words cache lines or the like and may be synchronous e.g. the CPU waits for the operation to complete . In certain embodiments providing access at a larger sized granularity such as cache lines may increase access rates provide more efficient write combining or the like than smaller sized granularity access.

The ACM may be available to computing devices and or applications both local and remote using one or more of a variety of memory mapping technologies including but not limited to memory mapped I O MMIO port I O port mapped IO PMIO Memory mapped file I O and the like. For example the ACM may be available to computing devices and or applications both local and remote using a PCI e Base Address Register BAR or other suitable mechanism. ACM may also be directly accessible via a memory bus of a CPU using an interface such as a double data rate DDR memory interface HyperTransport QuickPath Interconnect QPI or the like. Accordingly the ACM may be accessible using memory access semantics such as CPU load store direct memory access DMA 3party DMA remote DMA RDMA atomic test and set and so on. The direct memory semantic access to the ACM disclosed herein allows many of the system and or virtualization layer calls typically required to implement committed operations to be bypassed e.g. call backs via asynchronous Input Output interfaces may be bypassed . In some embodiments an ACM may be mapped to one or more virtual ranges e.g. virtual BAR ranges virtual memory addresses or the like . The virtual mapping may allow multiple computing devices and or applications to share a single ACM address range e.g. access the same ACM simultaneously within different virtual address ranges . An ACM may be mapped into an address range of a physical memory address space addressable by a CPU so that the CPU may use load store instructions to read and write data directly to the ACM using memory semantic accesses. A CPU in a further embodiment may map the physically mapped ACM into a virtual memory address space making the ACM available to user space processes or the like as virtual memory.

The ACM may be pre configured to commit its contents upon detection of a restart condition or other pre determined triggering event and as such operations performed on the ACM may be viewed as being instantly committed. For example an application may perform a write commit operation on the ACM using memory semantic writes that operate at CPU memory granularity and speed without the need for separate corresponding commit commands which may significantly increase the performance of applications affected by write commit latencies. As used herein a write commit operation is an operation in which an application writes data to a memory location e.g. using a memory semantic access and then issues a subsequent commit command to commit the operation e.g. to persistent storage or other commit mechanism .

Applications whose performance is based on write commit latency the time delay between the initial memory write and the subsequent persistent commit operation typically attempt to reduce this latency by leveraging a virtual memory system e.g. using a memory backed file . In this case the application performs high performance memory semantic write operations in system RAM but in order to commit the operations must perform subsequent commit commands to persist each write operation to the backing file or other persistent storage . Accordingly each write commit operation may comprise its own separate commit command. For example in a database logging application each log transaction must be written and committed before a next transaction is logged. Similarly messaging systems e.g. store and forward systems must write and commit each incoming message before receipt of the message can be acknowledged. The write commit latency therefore comprises a relatively fast memory semantic write followed by a much slower operation to commit the data to persistent storage. Write commit latency may include several factors including access times to persistent storage system call overhead e.g. translations between RAM addresses backing store LBA etc. and so on. Examples of applications that may benefit from reduced write commit latency include but are not limited to database logging applications filesystem logging messaging applications e.g. store and forward semaphore primitives and so on.

The systems apparatus and methods for auto commit memory disclosed herein may be used to significantly increase the performance of write commit latency bound applications by providing direct access to a memory region at any suitable level of addressing granularity including byte level page level cache line level or other memory region level that is guaranteed to be committed in the event of a system failure or other restart event without the application issuing a commit command. Accordingly the write commit latency of an application may be reduced to the latency of a memory semantic access a single write over a system bus .

ACM may be built out of memory addressable non volatile memory. That non volatile memory may be emulated with battery or capacitor backed DRAM that is saved and restored across powercuts or it may be a true byte addressable non volatile memory. To use an NVM as ACM the NVM may also contain the arming meta data which describes how to crash recover or deferred commit the data to the right place. This meta data may include addressing information that describes how to interpret what the data represents so it can be committed later. The metadata may include a designation of what LBA the data is to be committed under the VSL it may be the designation of what LBA under a partition or virtual volume it may be the filesystem inode and offset or the like. The metadata may include therefore the fully qualified address from the root of the device down to the exact context e.g. SML VSU FS INODE . ACM data may include information that is found recorded in a non volatile medium designated to be committed automatically as per the associated ACM meta data should the system restart. ACM or auto commit memory may include a non volatile medium that stores ACM data and meta data that may be accessible using memory load store semantics may be processed automatically or committed after a system restart or the like.

Accordingly when data is written to the ACM it may not initially be committed per se is not necessarily stored on a persistent memory media and or state rather a pre configured process is setup to preserve the ACM data and its state if a restart event occurs while the ACM data is stored in the ACM . The pre configuring of this restart survival process is referred to herein as arming. The ACM may be capable of performing the pre configured commit action autonomously and with a high degree of assurance despite the system experiencing failure conditions or another restart event. As such an entity that stores data on the ACM may consider the data to be instantaneously committed or safe from loss or corruption at least as safe as if the data were stored in a non volatile storage device such as a hard disk drive tape storage media or the like.

In embodiments where the ACM comprises a volatile memory media the ACM may make the volatile memory media appear as a non volatile memory may present the volatile memory as a non volatile medium or the like because the ACM preserves data such as ACM data and or ACM metadata across system restart events. The ACM may allow a volatile memory media to be used as a non volatile memory media by determining that a trigger event such as a restart or failure condition has occurred copying the contents of the volatile memory media to a non volatile memory media during a hold up time after the trigger event and copying the contents back into the volatile memory media from the non volatile memory media after the trigger event is over power has been restored the restart event has completed or the like.

In one embodiment the ACM is at least byte addressable. A memory media of the ACM in certain embodiments may be natively byte addressable directly providing the ACM with byte addressability. In another embodiment a memory media of the ACM is not natively byte addressable but a volatile memory media of the ACM is natively byte addressable and the ACM writes or commits the contents of the byte addressable volatile memory media to the non byte addressable memory media of the ACM in response to a trigger event so that the volatile memory media renders the ACM byte addressable.

The ACM may be accessible to one or more computing devices such as the host . As used herein a computing device such as the host refers to a computing device capable of accessing an ACM. The host may be a computing device that houses the ACM as a peripheral the ACM may be attached to a system bus of the host the ACM may be in communication with the host over a data network and or the ACM may otherwise be in communication with the host . The host in certain embodiments may access the ACM hosted by another computing device. The access may be implemented using any suitable communication mechanism including but not limited to CPU programmed IO CPIO port mapped IO PMIO memory mapped IO MMIO a Block interface a PCI e bus Infiniband RDMA or the like. The host may comprise one or more ACM users . As used herein an ACM user refers to any operating system OS virtual operating platform e.g. an OS with a hypervisor a guest OS application process thread entity utility user or the like that is configured to access the ACM .

The ACM may be physically located at one or more levels of the host . In one embodiment the ACM may be connected to a PCI e bus and may be accessible to the host with MMIO. In another embodiment the ACM may be directly accessible to a CPU of the host via a memory controller. For example the ACM may be directly attached to and or directly e.g. Quick Path Interconnect QPI in communication with a CPU of the host or the like. Volatile media of the ACM and non volatile backing media of the ACM in certain embodiments may not be physically co located within the same apparatus but may be in communication over a communications bus a data network or the like. In other embodiments as described below hardware components of the ACM may be tightly coupled and integrated in a single physical hardware apparatus. Volatile memory media and or non volatile memory media of the ACM in one embodiment may be integrated with or may otherwise cooperate with a CPU cache hierarchy of the host to take advantage of CPU caching technologies such as write combining or the like.

One or more ACM buffers in certain embodiments may be mapped into an address range of a physical memory address space addressable by a CPU a kernel or the like of the host device such as the memory system described below. For example one or more ACM buffers may be mapped as directly attached physical memory as MMIO addressable physical memory over a PCI e bus or otherwise mapped as one or more pages of physical memory. At least a portion of the physically mapped ACM buffers in a further embodiment may be mapped into a virtual memory address space accessible to user space processes or the like as virtual memory.

Allowing ACM users to directly address the ACM buffers in certain embodiments bypasses one or more layers of the traditional operating system memory stack of the host device providing direct load store operation access to kernel space and or user space applications. An operating system using a kernel module an application programming interface the storage management layer SML described below or the like in one embodiment maps and unmaps ACM buffers to and from the memory system for one or more ACM users and the ACM users may directly access an ACM buffer once the operating system maps the ACM buffer into the memory system . In a further embodiment the operating system may also service system flush calls for the ACM buffers or the like.

The SML and or the SML API described below in certain embodiments provide an interface for ACM users an operating system and or other entities to request certain ACM functions such as a map function an unmap function a flush function and or other ACM functions. To perform a flush operation in response to a flush request the ACM may perform a commit action for each ACM buffer associated with the flush request. Each ACM buffer is committed as indicated by the ACM metadata of the associated ACM buffer . A flush function in various embodiments may be specific to one or more ACM buffers system wide for all ACM buffers or the like. In one embodiment a CPU an operating system or the like for the host may request an ACM flush operation in response to or as part of a CPU cache flush a system wide data flush for the host or another general flush operation.

An ACM user an operating system or the like may request a flush operation to maintain data consistency prior to performing a maintenance operation such as a data snapshot or a backup to commit ACM data prior to reallocating an ACM buffer to prepare for a scheduled restart event or for other circumstances where flushing data from an ACM buffer may be beneficial. An ACM user an operating system or the like in certain embodiments may request that the ACM map and or unmap one or more ACM buffers to perform memory management for the ACM buffers to reallocate the ACM buffers between applications or processes to allocate ACM buffers for new data applications or processes to transfer use of the ACM buffers to a different host in shared ACM embodiments or to otherwise manipulate the memory mapping of the ACM buffers . In another embodiment the SML may dynamically allocate map and or unmap ACM buffers using a resource management agent as described below.

Since the ACM is guaranteed to auto commit the data stored thereon in the event of a trigger event the host or ACM user may view data written to the ACM as being instantaneously committed or non volatile as the host or ACM user may access the data both before and after the trigger event. Advantageously while the restart event may cause the ACM user to be re started or re initialized the data stored in the ACM is in the same state condition after the restart event as it was before the restart event. The host may therefore write to the ACM using memory write semantics and at CPU speeds and granularity without the need for explicit commit commands by relying on the pre configured trigger of the ACM to commit the data in the event of a restart or other trigger event .

The ACM may comprise a plurality of auto commit buffers each comprising respective ACM metadata . As discussed below the ACM metadata may include data to facilitate committing of ACM data in response to a triggering event for the auto commit buffer such as a logical identifier for data in the ACM buffer an identifier of a commit agent instructions for a commit process or other processing procedure security data or the like. The auto commit buffers may be of any suitable size from a single sector page byte or the like to a virtual or logical page size e.g. 80 to 400 kb . The size of the auto commit buffers may be adapted according to the storage capacity of the underlying non volatile storage media and or hold up time available from the secondary power supply .

In one embodiment the ACM may advertise or present to the host to ACM users or the like a storage capacity of the ACM buffers that is larger than an actual storage capacity of memory of the ACM buffers . To provide the larger storage capacity the ACM may dynamically map and unmap ACM buffers to the memory system and to the non volatile backing memory of the ACM such as the non volatile memory described above. For example the ACM may provide virtual address ranges for the ACM buffers and demand page data and or ACM buffers to the non volatile memory as ACM buffer accesses necessitate. In another embodiment for ACM buffers that are armed to commit to one or more predefined LBAs of the non volatile memory the ACM may dynamically move the ACM data and ACM metadata from the ACM buffers to the associated LBAs of the non volatile memory freeing storage capacity of the ACM buffers to provide a larger storage capacity. The ACM may further return the ACM data and ACM metadata back to one or more ACM buffers as ACM buffers become available certain addresses outside the data of currently loaded ACM buffers is requested or the like managing storage capacity of the ACM buffers .

The ACM is pre configured or armed to implement one or more triggered commit actions in response to a restart condition or other pre determined condition . As used herein a restart condition or event may include but is not limited to a software or hardware shutdown restart of a host a failure in a host computing device a failure of a component of the host e.g. failure of the bus a software fault e.g. an fault in software running on the host or other computing device a loss of the primary power connection an invalid shutdown or another event that may cause the loss of data stored in a volatile memory.

In one embodiment a restart event comprises the act of the host commencing processing after an event that can cause the loss of data stored within a volatile memory of the host or a component in the host . The host may commence resume processing once the restart condition or event has finished a primary power source is available and the like.

The ACM is configured to detect that a restart event condition has occurred and or respond to a restart event by initiating a recovery stage. During a recovery stage the ACM may restore the data of the ACM to the state prior to the restart event. Alternatively or in addition during the recovery stage the ACM may complete processing of ACM data or ACM metadata needed to satisfy a guarantee that data in the ACM is available to ACM users after the restart event. Alternatively or in addition during the recovery stage the ACM may complete processing of ACM data or ACM metadata needed to satisfy a guarantee that data in the ACM is committed after the restart event. As used herein commit means data in the ACM is protected from loss or corruption even after the restart event and is persisted as required per the arming information associated with the data. In certain embodiments the recovery stage includes processing ACM data and ACM metadata such that the ACM data is persisted even though the restart event occurred.

As used herein a triggered commit action is a pre configured commit action that is armed to be performed by the ACM in response to a triggering event e.g. a restart event a flush command or other pre determined event . In certain embodiments the triggered commit action persists at least enough ACM data and or ACM metadata to make data of the ACM available after a system restart to satisfy a guarantee of the ACM that the data will be accessible to an ACM user after a restart event in certain embodiments this guarantee is satisfied at least in part by committing and or persisting data of the ACM to non volatile memory media. A triggered commit action may be completed before during and or after a restart event. For example the ACM may write ACM data and ACM metadata to a predefined temporary location in the nonvolatile memory during a hold up time after a restart event and may copy the ACM data back into the ACM buffers to an intended location in the nonvolatile memory or perform other processing once the restart event is complete.

A triggered commit action may be armed when the ACM is requested and or a particular ACM buffer is allocated for use by a host . In some embodiments an ACM may be configured to implement a triggered commit action in response to other non restart conditions. For example an operation directed to a particular logical address e.g. a poke may trigger the ACM a flush operation may trigger the ACM or the like. This type of triggering may be used to commit the data of the ACM during normal operation e.g. non restart or non failure conditions .

The arming may occur when an auto commit buffer is mapped into the memory system of the host . Alternatively arming may occur as a separate operation. As used herein arming an auto commit buffer comprises performing the necessary configuration steps needed to complete the triggered action when the action is triggered. Arming may include for example providing the ACM metadata to the ACM or the like. In certain embodiments arming further includes performing the necessary configuration steps needed to complete a minimal set of steps for the triggered action such that the triggered action is capable of completing after a trigger event. In certain embodiments arming further includes verifying the arming data e.g. verifying that the contents of the auto commit buffer or portion thereof can be committed as specified in the ACM metadata and verifying that the ACM is capable and configured to properly perform the triggered action without error or interruption.

The verification may ensure that once armed the ACM can implement the triggered commit action when required. If the ACM metadata cannot be verified e.g. the logical identifier or other ACM metadata is invalid corrupt unavailable or the like the arming operation may fail memory semantic operations on the auto commit buffer may not be allowed unit the auto commit buffer is successfully armed with valid ACM metadata . For example an auto commit buffer that is backed by a hard disk having a one to one mapping between LBA and physical address may fail to arm if the LBA provided for the arming operation does not map to a valid and operational physical address on the disk. Verification in this case may comprise querying the disk to determine whether the LBA has a valid corresponding physical address and or using the physical address as the ACM metadata of the auto commit buffer .

The armed triggered commit actions are implemented in response to the ACM or other entity detecting and or receiving notification of a triggering event such as a restart condition. In some embodiments an armed commit action is a commit action that can be performed by the ACM and that requires no further communication with the host or other devices external to the isolation zone of the ACM discussed below . Accordingly the ACM may be configured to implement triggered commit actions autonomously of the host and or other components thereof. The ACM may guarantee that triggered commit actions can be committed without errors and or despite external error conditions. Accordingly in some embodiments the triggered commit actions of the ACM do not comprise and or require potentially error introducing logic computations and or calculations. In some embodiments a triggered commit action comprises committing data stored on the volatile ACM to a persistent storage location. In other embodiments a triggered commit action may comprise additional processing of committed data before during and or after a triggering event as described below. The ACM may implement pre configured triggered commit actions autonomously the ACM may be capable of implementing triggered commit actions despite failure or restart conditions in the host loss of primary power or the like. The ACM can implement triggered commit actions independently due to arming the ACM as described above.

The ACM metadata for an ACM buffer in certain embodiments identifies the data of the ACM buffer . For example the ACM metadata may identify an owner of the data may describe the data itself or the like. In one embodiment an ACM buffer may have multiple levels of ACM metadata for processing by multiple entities or the like. The ACM metadata may include multiple nested headers that may be unpackaged upon restart and used by various entities or commit agents to determine how to process the associated ACM data to fulfill the triggered commit action as described above. For example the ACM metadata may include block metadata file metadata application level metadata process execution point or callback metadata and or other levels of metadata. Each level of metadata may be associated with a different commit agent or the like. In certain embodiments the ACM metadata may include security data such as a signature for an owner of the associated ACM data a pre shared key a nonce or the like which the ACM may use during recovery to verify that a commit agent an ACM user or the like is authorized to access committed ACM metadata and or associated ACM data. In this manner the ACM may prevent ownership spoofing or other unauthorized access. In one embodiment the ACM does not release ACM metadata and or associated ACM data until a requesting commit agent ACM user or the like provides valid authentication such as a matching signature or the like.

One or more commit agents such as the commit management apparatus described below with regard to in certain embodiments process ACM data based on the associated ACM metadata to execute a triggered commit action. A commit agent in various embodiments may comprise software such as a device driver a kernel module the SML a thread a user space application or the like and or hardware such as the controller described below that is configured to interpret ACM metadata and to process the associated ACM data according to the ACM metadata . In embodiments with multiple commit agents the ACM metadata may identify one or more commit agents to process the associated ACM data. The ACM metadata may identify a commit agent in various embodiments by identifying a program function of the commit agent to invoke e.g. a file path of the program by including computer executable code of the commit agent e.g. binary code or scripts by including a unique identifier indicating which of a set of registered commit agents to use and or by otherwise indicating a commit agent associated with committed ACM metadata . The ACM metadata in certain embodiments may be a functor or envelope which contains the information such as function pointer and bound parameters for a commit agent to commit the ACM data upon restart recovery.

In one embodiment a primary commit agent processes ACM metadata and hands off or transfers ACM metadata and or ACM data to one or more secondary commit agents identified by the ACM metadata . A primary commit agent in one embodiment may be integrated with the ACM the controller or the like. An ACM user or other third party in certain embodiments may provide a secondary commit agent for ACM data that the ACM user or other third party owns and the primary commit agent may cooperate with the provided secondary commit agent to process the ACM data. The one or more commit agents for ACM data in one embodiment ensure and or guarantee that the ACM data remains accessible to an owner of the ACM data after a restart event. As described above with regard to triggered commit actions a commit agent may process ACM metadata and associated ACM data to perform one or more triggered commit actions before during and or after a trigger event such as a failure or other restart event.

In one embodiment a commit agent in cooperation with the ACM or the like may store the ACM metadata in a persistent or non volatile location in response to a restart or other trigger event. The commit agent may store the ACM metadata at a known location may store pointers to the ACM metadata at a known location may provide the ACM metadata to an external agent or data store or the like so that the commit agent may process the ACM metadata and associated ACM data once the restart or other trigger event has completed. The known location may include one or more predefined logical block addresses or physical addresses of the non volatile memory a predefined file or the like. In certain embodiments hardware of the ACM is configured to cooperate to write the ACM metadata and or pointers to the ACM metadata at a known location. In one embodiment the known location may be a temporary location that stores the ACM data and ACM metadata until the host has recovered from a restart event and the commit agent may continue to process the ACM data and ACM metadata . In another embodiment the location may be a persistent location associated with the ACM metadata .

In response to completion of a restart event or other trigger event during recovery in one embodiment a commit agent may locate and retrieve the ACM metadata from the non volatile memory from a predefined location or the like. The commit agent in response to locating and retrieving the ACM metadata locates the ACM data associated with the retrieved ACM metadata . The commit agent in certain embodiments may locate the ACM data in a substantially similar manner as the commit agent locates the ACM metadata retrieving ACM data from a predefined location retrieving pointers to the ACM data from a predefined location receiving the ACM data from an external agent or data store or the like. In one embodiment the ACM metadata identifies the associated ACM data and the commit agent uses the ACM metadata to locate and retrieve the associated ACM data. For example the commit agent may use a predefined mapping to associate ACM data with ACM metadata e.g the Nth piece of ACM data may be associated with the Nth piece of ACM metadata or the like the ACM metadata may include a pointer or index for the associated ACM data or another predefined relationship may exist between committed ACM metadata and associated ACM data. In another embodiment an external agent may indicate to the commit agent where associated ACM data is located.

In response to locating and retrieving the ACM metadata and associated ACM data the commit agent interprets the ACM metadata and processes the associated ACM data based on the ACM metadata . For example in one embodiment the ACM metadata may identify a block storage volume and LBA s where the commit agent is to write the ACM data upon recovery. In another embodiment the ACM metadata may identify an offset within a file within a file system where the commit agent is to write the ACM data upon recovery. In a further embodiment the ACM metadata may identify an application specific persistent object where the commit agent is to place the ACM data upon recovery such as a database record or the like. The ACM metadata in an additional embodiment may indicate a procedure for the commit agent to call to process the ACM data such as a delayed procedure call or the like. In an embodiment where the ACM advertises or presents volatile ACM buffers as nonvolatile memory the ACM metadata may identify an ACM buffer where the commit agent is to write the ACM data upon recovery.

In certain embodiments the ACM metadata may identify one or more secondary commit agents to further process the ACM metadata and or associated ACM data. A secondary commit agent may process ACM metadata and associated ACM data in a substantially similar manner to the commit agent described above. Each commit agent may process ACM data in accordance with a different level or subset of the ACM metadata or the like. The ACM metadata may identify a secondary commit agent in various embodiments by identifying a program function of the secondary commit agent to invoke e.g. a file path of the program by including computer executable code of the secondary commit agent by including a unique identifier indicating which of a set of registered secondary commit agents to use and or by otherwise indicating a secondary commit agent associated with committed ACM metadata .

In one embodiment a secondary commit agent processes a remaining portion of the ACM metadata and or of the ACM data after a previous commit agent has processed the ACM metadata and or the ACM data. In a further embodiment the ACM metadata may identify another non volatile medium separate from the ACM for the secondary commit agent to persist the ACM data even after a host experiences a restart event. By committing the ACM metadata and the associated ACM data from the ACM buffers in response to a trigger event such as a failure or other restart condition and processing the ACM metadata and the associated ACM data once the trigger event has completed or recovered the ACM may guarantee persistence of the ACM data and or performance of the triggered commit action s defined by the ACM metadata .

The ACM is communicatively coupled to a host which like the host described above may comprise operating systems virtual machines applications a processor complex a central processing unit CPU and the like. In the example these entities are referred to generally as ACM users . Accordingly as used herein an ACM user may refer to an operating system a virtual machine operating system e.g. hypervisor an application a library a CPU fetch execute algorithm or other program or process. The ACM may be communicatively coupled to the host as well as the ACM users via a bus such as a system bus a processor s memory exchange bus or the like e.g. HyperTransport QuickPath Interconnect QPI PCI bus PCI e bus or the like . In some embodiments the bus comprises the primary power connection e.g. the non volatile storage device may be powered through the bus . Although some embodiments described herein comprise solid state storage devices such as certain embodiments of the non volatile storage device the disclosure is not limited in this regard and could be adapted to use any suitable recording memory storage device and or recording memory storage media .

The ACM may be tightly coupled to the device used to perform the triggered commit actions. For example the ACM may be implemented on the same device peripheral card or within the same isolation zone as the controller and or secondary power source . The tight coupling of the ACM to the components used to implement the triggered commit actions defines an isolation zone which may provide an acceptable level of assurance based on industry standards or other metric that the ACM is capable of implementing the triggered auto commit actions in the event of a restart condition. In the example the isolation zone of the ACM is provided by the tight coupling of the ACM with the autonomous controller and secondary power supply discussed below .

The controller may comprise an I O controller such as a network controller e.g. a network interface controller storage controller dedicated restart condition controller or the like. The controller may comprise firmware hardware a combination of firmware and hardware or the like. In the example the controller comprises a storage controller such as the storage controller and or non volatile storage device controller described above. The controller may be configured to operate independently of the host . As such the controller may be used to implement the triggered commit action s of the ACM despite the restart conditions discussed above such as failures in the host and or ACM users and or loss of the primary power connection .

The ACM is powered by a primary power connection which like the primary power connection described above may be provided by a system bus bus external power supply the host or the like. In certain embodiments the ACM also includes and or is coupled to a secondary power source . The secondary power source may power the ACM in the event of a failure to the primary power connection . The secondary power source may be capable of providing at least enough power to enable the ACM and or controller to autonomously implement at least a portion of a pre configured triggered commit action s when the primary power connection has failed. The ACM in one embodiment commits or persists at least enough data e.g. ACM data and ACM metadata while receiving power from the secondary power source to allow access to the data once the primary power connection has been restored. In certain embodiments as described above the ACM may perform at least a portion of the pre configured triggered commit action s after the primary power connection has been restored using one or more commit agents or the like.

The ACM may comprise volatile memory storage. In the example the ACM includes one or more auto commit buffers . The auto commit buffers may be implemented using a volatile Random Access Memory RAM . In some embodiments the auto commit buffers may be embodied as independent components of the ACM e.g. in separate RAM modules . Alternatively the auto commit buffers may be implemented on embedded volatile memory e.g. BRAM available within the controller a processor complex an FPGA or other component of the ACM .

Each of the auto commit buffers may be pre configured armed with a respective triggered commit action. In some embodiments each auto commit buffer may comprise its own respective ACM metadata . The ACM metadata in some embodiments identifies how and or where the data stored on the auto commit buffer is to be committed. In some examples the ACM metadata may comprise a logical identifier e.g. an object identifier logical block address LBA file name or the like associated with the data in the auto commit buffer . The logical identifier may be predefined. In one embodiment when an auto commit buffer is committed the data therein may be committed with the ACM metadata e.g. the data may be stored at a physical storage location corresponding to the logical identifier and or in association with the logical identifier . To facilitate committing of ACM data during a hold up time after a restart event the ACM may write ACM data and ACM metadata in a single atomic operation such as a single page write or the like. To permit writing of ACM and ACM metadata in a single atomic operation the ACM buffers may be sized to correspond to a single write unit for a non volatile storage media that is used by the ACM . In some embodiments the ACM metadata may comprise a network address an LBA or another identifier of a commit location for the data.

In a further embodiment a logical identifier may associate data of an auto commit buffer with an owner of the data so that the data and the owner maintain the ownership relationship after a restart event. For example the logical identifier may identify an application an application type a process ID an ACM user or another entity of a host device so that the ACM data is persistently associated with the identified entity. In one embodiment a logical identifier may be a member of an existing namespace such as a file system namespace a user namespace a process namespace or the like. In other embodiments a logical identifier may be a member of a new or separate namespace such as an ACM namespace. For example a globally unique identifier namespace as is typically used in distributed systems for identifying communicating entities may be used as an ACM namespace for logical identifiers. The ACM may process committed ACM data according to a logical identifier for the data once a restart event has completed. For example the ACM may commit the ACM data to a logical identifier associated with a temporary location in response to a restart event and may write the ACM data to a persistent location identified by another logical identifier during recovery after the restart event.

As described above the ACM may be tightly coupled with the components used to implement the triggered commit actions e.g. the ACM is implemented within an isolation zone which ensures that the data on the ACM will be committed in the event of a restart condition. As used herein a tight coupling refers to a configuration wherein the components used to implement the triggered commit actions of the ACM are within the same isolation zone or two or more distinct trusted isolation zones and are configured to operate despite external failure or restart conditions such as the loss of power invalid shutdown host failures or the like. illustrates a tight coupling between the ACM the auto commit buffers the controller which is configured to operate independently of the host and the secondary power source which is configured to power the controller and the ACM including the auto commit buffers while the triggered commit actions are completed. Examples of a tight coupling include but are not limited to including the controller the secondary power source and the auto commit buffers on a single printed circuit board PCB within a separate peripheral in electronic communication with the host and the like. In other embodiments the ACM may be tightly coupled to other a different set of components e.g. redundant host devices redundant communication buses redundant controllers alternative power supplies and so on .

The ACM may be accessible by the host and or ACM users running thereon. Access to the ACM may be provided using memory access semantics such as CPU load store commands DMA commands 3rd party DMA commands RDMA commands atomic test and set commands manipulatable memory pointers and so on. In some embodiments memory semantic access to the ACM is implemented over the bus e.g. using a PCI e BAR as described below .

In a memory semantic paradigm ACM users running on the host may access the ACM via a memory system of the host . The memory system may comprise a memory management unit virtual memory system virtual memory manager virtual memory subsystem or similar memory address space implemented by an operating system a virtualization system e.g. hypervisor an application or the like. A portion of the ACM e.g. one or more auto commit buffers may be mapped into the memory system such that memory semantic operations implemented within the mapped memory address range ACM address range are performed on the ACM .

The SML in certain embodiments allocates and or arbitrates the storage capacity of the ACM between multiple ACM users using a resource management agent or the like. The resource management agent of the SML may comprise a kernel module provided to an operating system of the host device a device driver a thread a user space application or the like. In one embodiment the resource management agent determines how much storage capacity of the ACM buffers to allocate to an ACM user and how long the allocation is to last. Because in certain embodiments the ACM commits or persists data across restart events the resource management agent may allocate storage capacity of ACM buffers across restart events.

The resource management agent may assign different ACM buffers to different ACM users such as different kernel and or user space applications. The resource management agent may allocate ACM buffers to different usage types may map ACM buffers to different non volatile memory locations for destaging or the like. In one embodiment the resource management agent may allocate the ACM buffers based on commit agents associated with the ACM buffers by the ACM metadata or the like. For example a master commit agent may maintain an allocation map in ACM metadata identifying allocation information for ACM buffers of the ACM and identifying in one embodiment one or more secondary commit agents and the master commit agent may allocate a portion of the ACM buffers to each of the secondary commit agents . In another embodiment commit agents may register with the resource management agent may request resources such as ACM buffers from the resource management agent or the like. The resource management agent may use a predefined memory management policy such as a memory pressure policy or the like to allocate and arbitrate ACM buffer storage capacity between ACM users .

In some embodiments establishing an association between an ACM address range within the memory system and the ACM may comprise pre configuring arming the corresponding auto commit buffer s with a triggered commit action. As described above this pre configuration may comprise associating the auto commit buffer with a logical identifier or other metadata which may be stored in the ACM metadata of the buffer . As described above the ACM may be configured to commit the buffer data to the specified logical identifier in the event of a restart condition or to perform other processing in accordance with the ACM metadata .

Memory semantic access to the ACM may be implemented using any suitable address and or device association mechanism. In some embodiments memory semantic access is implemented by mapping one or more auto commit buffers of the ACM into the memory system of the host . In some embodiments this mapping may be implemented using the bus . For example the bus may comprise a PCI e or similar communication bus and the mapping may comprise associating a Base Address Register BAR of an auto commit buffer of the ACM on the bus with the ACM address range in the memory system e.g. the host mapping a BAR into the memory system .

The association may be implemented by an ACM user e.g. by a virtual memory system of an operating system or the like through an API of a storage layer such as the storage management layer SML . The SML may be configured to provide access to the auto commit memory to ACM users . The storage management layer may comprise a driver kernel level application user level application library or the like. One example of an SML is the Virtual Storage Layer of Fusion io Inc. of Salt Lake City Utah. The SML may provide a SML API comprising inter alia an API for mapping portions of the auto commit memory into the memory system of the host for unmapping portions of the auto commit memory from the memory system of the host for flushing the ACM buffers and the like. The SML may be configured to maintain metadata which may include a forward index comprising associations between logical identifiers of a logical address space and physical storage locations on the auto commit memory and or persistent storage media. In some embodiments ACM may be associated with one or more virtual ranges that map to different address ranges of a BAR or other addressing mechanism . The virtual ranges may be accessed e.g. mapped by different ACM users . Mapping or exposing a PCI e ACM BAR to the host memory may be enabled on demand by way of a SML API call.

The SML API may comprise interfaces for mapping an auto commit buffer into the memory system . In some embodiments the SML API may extend existing memory management interfaces such as malloc calloc or the like to map auto commit buffers into the virtual memory range of ACM user applications e.g. a malloc call through the SML API may map one or more auto commit buffers into the memory system . Alternatively or in addition the SML API may comprise one or more explicit auto commit mapping functions such as ACM alloc ACM free or the like. Mapping an auto commit buffer may further comprise configuring a memory system of the host to ensure that memory operations are implemented directly on the auto commit buffer e.g. prevent caching memory operations within a mapped ACM address range .

The association between the ACM address range within the host memory system and the ACM may be such that memory semantic operations performed within a mapped ACM address range are implemented directly on the ACM without intervening system RAM or other intermediate memory in a typical write commit operation additional layers of system calls or the like . For example a memory semantic write operation implemented within the ACM address range may cause data to be written to the ACM on one or more of the auto commit buffers . Accordingly in some embodiments mapping the ACM address range may comprise disabling caching of memory operations within the ACM address range such that memory operations are performed on an ACM and are not cached by the host e.g. cached in a CPU cache in host volatile memory or the like . Disabling caching within the ACM address range may comprise setting a non cacheable flag attribute associated with the ACM range when the ACM range is defined.

As discussed above establishing an association between the host memory system and the ACM may comprise arming the ACM to implement a pre determined triggered commit action. The arming may comprise providing the ACM with a logical identifier e.g. a logical block address a file name a network address a stripe or mirroring pattern or the like . The ACM may use the logical identifier to arm the triggered commit action. For example the ACM may be triggered to commit data to a persistent storage medium using the logical identifier e.g. the data may be stored at a physical address corresponding to the logical identifier and or the logical identifier may be stored with the data in a log based data structure . Arming the ACM allows the host to view subsequent operations performed within the ACM address range and on the ACM as being instantly committed enabling memory semantic write granularity e.g. byte level operations and speed with instant commit semantics.

Memory semantic writes such as a store operation for a CPU are typically synchronous operations such that the CPU completes the operation before handling a subsequent operation. Accordingly memory semantic write operations performed in the ACM memory range can be viewed as instantly committed obviating the need for a corresponding commit operation in the write commit operation which may significantly increase the performance of ACM users affected by write commit latency. The memory semantic operations performed within the ACM memory range may be synchronous. Accordingly ACM may be configured to prevent the memory semantic operations from blocking e.g. waiting for an acknowledgement from other layers such as the bus or the like . Moreover the association between ACM address range and the ACM allow memory semantic operations to bypass system calls e.g. separate write and commit commands and their corresponding system calls that are typically included in write commit operations.

Data transfer between the host and the ACM may be implemented using any suitable data transfer mechanism including but not limited to the host performing processor IO operations PIO with the ACM via the bus the ACM or other device providing one or more DMA engines or agents data movers to transfer data between the host and the ACM the host performing processor cache write flush operations or the like.

As discussed above an ACM may be configured to automatically perform a pre configured triggered commit action in response to detecting certain conditions e.g. restart or failure conditions . In some embodiments the triggered commit action may comprise committing data stored on the ACM to a persistent storage media. Accordingly in some embodiments an ACM such as the ACM described above may be comprise persistent storage media. is a block diagram of a system depicting an embodiment of an ACM configured to implement triggered commit actions which may include committing data to a persistent solid state and or non volatile storage.

The ACM of the example may be tightly coupled to the non volatile storage device which comprises a controller . The controller may comprise a write data pipeline and a read data pipeline which may operate as described above. The non volatile storage device may be capable of persisting data on a non volatile memory such as solid state storage media.

A commit management apparatus is used to commit data to the non volatile memory in response to a trigger event such as loss of primary power connection or other pre determined trigger event. Accordingly the commit management apparatus may comprise and or be configured to perform the functions of the power management apparatus described above. The commit management apparatus may be further configured to commit data on the ACM e.g. the contents of the auto commit buffers to the non volatile memory in response to a restart condition or on request from the host and or ACM users and in accordance with the ACM metadata . The commit management apparatus is one embodiment of a commit agent .

The data on the ACM may be committed to the persistent storage in accordance with the ACM metadata such as a logical identifier or the like. The ACM may commit the data to a temporary location for further processing after a restart event may commit the data to a final intended location or the like as described above. If the non volatile memory is sequential storage device committing the data may comprise storing the logical identifier or other ACM metadata with the contents of the auto commit buffer e.g. in a packet or container header . If the non volatile memory comprises a hard disk having a 1 1 mapping between logical identifier and physical address the contents of the auto commit buffer may be committed to the storage location to which the logical identifier maps. Since the logical identifier or other ACM metadata associated with the data is pre configured e.g. armed the ACM implements the triggered commit action independently of the host . The secondary power supply supplies power to the volatile auto commit buffers of the ACM until the triggered commit actions are completed and or confirmed to be completed or until the triggered commit actions are performed to a point at which the ACM may complete the triggered commit actions during recovery after a restart event.

In some embodiments the ACM commits data in a way that maintains an association between the data and its corresponding logical identifier per the ACM metadata . If the non volatile memory comprises a hard disk the data may be committed to a storage location corresponding to the logical identifier which may be outside of the isolation zone e.g. using a logical identifier to physical address conversion . In other embodiments in which the non volatile memory comprises a sequential media such as solid state storage media the data may be stored sequentially and or in a log based format as described in above and or in U.S. Provisional Patent Application Publication No. 61 373 271 entitled APPARATUS SYSTEM AND METHOD FOR CACHING DATA and filed 12 Aug. 2010 which is hereby incorporated by reference in its entirety. The sequential storage operation may comprise storing the contents of an auto commit buffer with a corresponding logical identifier as indicated by the ACM metadata . In one embodiment the data of the auto commit buffer and the corresponding logical identifier are stored together on the media according to a predetermined pattern. In certain embodiments the logical identifier is stored before the contents of the auto commit buffer . The logical identifier may be included in a header of a packet comprising the data or in another sequential and or log based format. The association between the data and logical identifier may allow a data index to be reconstructed as described above.

As described above the auto commit buffers of the ACM may be mapped into the memory system of the host enabling the ACM users of access these buffers using memory access semantics. In some embodiments the mappings between logical identifiers and auto commit buffers may leverage a virtual memory system of the host .

For example an address range within the memory system may be associated with a memory mapped file. As discussed above a memory mapped file is a virtual memory abstraction in which a file portion of a file or block device is mapped into the memory system address space for more efficient memory semantic operations on data of the non volatile storage device . An auto commit buffer may be mapped into the host memory system using a similar abstraction. The ACM memory range may therefore be represented by a memory mapped file. The backing file must be stored on the non volatile memory within the isolation zone See below or another network attached non volatile storage device also protected by an isolation zone . The auto commit buffers may correspond to only a portion of the file the file itself may be very large exceeding the capacity of the auto commit buffers and or the non volatile memory . When a portion of a file is mapped to an auto commit buffer the ACM user or other entity may identify a desired offset within the file and the range of blocks in the file that will operate with ACM characteristics e.g. have ACM semantics . This offset will have a predefined logical identifier and the logical identifier and range may be used to trigger committing the auto commit buffer s mapped within the file. Alternatively a separate offset for a block or range of blocks into the file may serve as a trigger for committing the auto commit buffer s mapped to the file. For example anytime a memory operation load store poke etc. is performed on data in the separate offset or range of blocks may result in a trigger event that causes the auto commit buffer s mapped to the file to be committed.

The underlying logical identifier may change however e.g. due to changes to other portions of the file file size changes etc. . When a change occurs the SML via the SML API an ACM user or other entity may update the ACM metadata of the corresponding auto commit buffers . In some embodiments the SML may be configured to query the host operating system hypervisor or other application for updates to the logical identifier of files associated with auto commit buffers . The queries may be initiated by the SML API and or may be provided as a hook callback mechanism into the host . When the ACM user no longer needs the auto commit buffer the SML may de allocate the buffer as described above. De allocation may further comprise informing the host that updates to the logical identifier are no longer needed.

In some embodiments a file may be mapped across multiple storage devices e.g. the storage devices may be formed into a RAID group may comprise a virtual storage device or the like . Associations between auto commit buffers and the file may be updated to reflect the file mapping. This allows the auto commit buffers to commit the data to the proper storage device. The ACM metadata of the auto commit buffers may be updated in response to changes to the underlying file mapping and or partitioning as described above. Alternatively the file may be locked to a particular mapping or partition while the auto commit buffers are in use. For example if a remapping repartitioning of a file is required the corresponding auto commit buffers may commit data to the file and then be re associated with the file under the new mapping partitioning scheme. The SML API may comprise interfaces and or commands for using the SML to lock a file release a file and or update ACM metadata in accordance with changes to a file.

Committing the data to solid state non volatile storage may comprise the storage controller accessing data from the ACM auto commit buffers associating the data with the corresponding logical identifier e.g. labeling the data and injecting the labeled data into the write data pipeline as described above. In some embodiments to ensure there is a page program command capable of persisting the ACM data the storage controller maintains two or more pending page programs during operation. The ACM data may be committed to the non volatile memory before writing the power loss identifier power cut fill pattern described above.

In some embodiments the ACMs A and B may implement a striping scheme e.g. a RAID scheme . In this case different portions of the host data may be sent to different ACMs A and or B. Driver level software such as a volume manager implemented by the SML and or operating system may map host data to the proper ACM per the striping pattern.

In some configurations the memory access semantics provided by the ACMs may be adapted according to a particular storage striping pattern. For example if host data is mirrored from the ACM A to the ACM B a memory semantic write may not complete and or an acknowledgement may not be returned until the ACM A verifies that the data was sent to the ACM B under the instant commit semantic . Similar adaptations may be implemented when ACMs are used in a striping pattern e.g. a memory semantic write may be not return and or be acknowledged until the striping pattern for a particular operation is complete . For example in a copy on write operation the ACM A may store the data of an auto commit buffer and then cause the data to be copied to the ACM B. The ACM A may not return an acknowledgment for the write operation or allow the data to be read until the data is copied to the ACM B.

The use of mirrored ACM devices A and B may be used in a high availability configuration. For example the ACM devices A and B may be implemented in separate host computing devices. Memory semantic accesses to the devices A and B are mirrored between the devices as described above e.g. using PCI e access . The devices may be configured to operate in high availability mode such that device proxying may not be required. Accordingly trigger operations as well as other memory semantic accesses may be mirrored across both devices A and B but the devices A and B may not have to wait for a acknowledge from the other before proceeding which removes the other device from the write commit latency path.

The commit management apparatus includes a monitor module which may be configured to detect restart conditions such as power loss or the like. The monitor module may be configured to sense triggering events such as restart conditions e.g. shutdown restart power failures communication failures host or application failures and so on and in response to initiate the commit module to initiate the commit loss mode of the apparatus failure loss mode and or to trigger the operations of other modules such as modules and or . The commit module includes an identification module terminate module corruption module and completion module which may operate as described above.

The identification module may be further configured to identify triggered commit actions to be performed for each ACM buffer of the ACM . As discussed above the identification module may prioritize operations based on relative importance with acknowledged operations being given a higher priority than non acknowledged operations. The contents of auto commit buffers that are armed to be committed may be assigned a high priority due to the instant commit semantics supported thereby. In some embodiments the ACM triggered commit actions may be given a higher priority than the acknowledged contents of the write data pipeline . Alternatively the contents of armed auto commit buffers may be assigned the next highest priority. The priority assignment may be user configurable via an API IO control IOCTRL or the like .

The termination module terminates non essential operations to allow essential to continue as described above. The termination module may be configured to hold up portions of the ACM that are armed to be committed e.g. armed auto commit buffers and may terminate power to non armed unused portions of the auto commit memory . The termination module may be further configured to terminate power to portions of the ACM individual auto commit buffers as the contents of those buffers are committed.

The corruption module identifies corrupt or potentially corrupt data in the write data pipeline as described above. The module may be further configured to identify corrupt ACM data data that was written to the ACM during a power disturbance or other restart condition . The corruption module may be configured to prevent corrupt data on the ACM from being committed in a triggered commit action.

An ACM module is configured to access armed auto commit buffers in the auto commit memory identify the ACM metadata associated therewith e.g. label the data with the corresponding logical identifier per the ACM metadata and inject the data and metadata into the write data pipeline of the non volatile storage controller . In some embodiments the logical identifier or other ACM metadata of the auto commit buffer may be stored in the buffer itself. In this case the contents of the auto commit buffer may be streamed directly into a sequential and or log based storage device without first identifying and or labeling the data. The ACM module may inject data before or after data currently in the write data pipeline . In some embodiments data committed from the ACM is used to fill out the remainder of a write buffer of the write data pipeline after removing potentially corrupt data . If the remaining capacity of the write buffer is insufficient the write buffer is written to the non volatile storage and a next write buffer is filled with the remaining ACM data.

As discussed above in some embodiments the non volatile storage controller may maintain an armed write operation logical page write to store the contents of the write data pipeline in the event of power loss. When used with an ACM two or more armed write operations logical page writes may be maintained to ensure the contents of both the write data pipeline and all the armed buffers of the ACM can be committed in the event of a restart condition. Because a logical page in a write buffer may be partially filled when a trigger event occurs the write buffer is sized to hold at least one more logical page of data than the total of all the data stored in all ACM buffers of the ACM and the capacity of data in the write data pipeline that has been acknowledged as persisted. In this manner there will be sufficient capacity in the write buffer to complete the persistence of the ACM in response to a trigger event. Accordingly the auto commit buffers may be sized according to the amount of data the ACM is capable of committing. Once this threshold is met the SML may reject requests to use ACM buffers until more become available.

The completion module is configured to flush the write data pipeline regardless of whether the certain buffers packets and or pages are completely filled. The completion module is configured to perform the flush and insert the related padding data after data on the ACM if any has been injected into the write data pipeline . The completion module may be further configured to inject completion indicator into the write data pipeline which may be used to indicate that a restart condition occurred e.g. a restart condition fill pattern . This fill pattern may be included in the write data pipeline after injecting the triggered data from the ACM .

As discussed above the secondary power supply may be configured to provide sufficient power to store the contents of the ACM as well as data in the write data pipeline . Storing this data may comprise one or more write operations e.g. page program operations in which data is persistently stored on the non volatile storage media . In the event a write operation fails another write operation on a different storage location may be attempted. The attempts may continue until the data is successfully persisted on the non volatile storage media . The secondary power supply may be configured to provide sufficient power for each of a plurality of such page program operations to complete. Accordingly the secondary power supply may be configured to provide sufficient power to complete double or more page program write operations as required to store the data of the ACM and or write data pipeline .

The host may be communicatively coupled to the ACM via a bus which may comprise a PCI e bus or the like. Portions of the ACM are made accessible to the host may mapping in auto commit buffers into the host . In some embodiments mapping comprises associating an address range within the host memory system with an auto commit buffer of the ACM . These associations may be enabled using the SML API and or SML available on the host .

The SML may comprise libraries and or provide interfaces e.g. SML API to implement the memory access semantics described above. The API may be used to access the ACM using memory access semantics via a memory semantic access module . Other types of access such as access to the non volatile storage may be provided via a block device interface .

The SML may be configured to memory map auto commit buffers of the ACM into the memory system via the SML API . The memory map may use a virtual memory abstraction of the memory system . For example a memory map may be implemented using a memory mapped file abstraction. In this example the operating system or application designates a file to be mapped into the memory system . The file is associated with a logical identifier LID e.g. logical block address which may be maintained by a file system an operating system or the like.

The memory mapped file may be associated with an auto commit buffer of the ACM . The association may be implemented by the SML using the bus . The SML associates the address range of the memory mapped file in the memory system with a device address of an auto commit buffer on the ACM . The association may comprise mapping a PCI e BAR into the memory system . In the example the ACM address range in the memory system is associated with the auto commit buffer .

As discussed above providing memory access semantics to the ACM may comprise arming the ACM to commit data stored thereon in the event of failure or other restart. The pre configured arming ensures that in the event of a restart data stored on the ACM will be committed to the proper logical identifier. The pre configuration of the trigger condition enables applications to access the auto commit buffer using instant commit memory access semantics. The logical identifier used to arm the auto commit buffer may be obtained from an operating system the memory system e.g. virtual memory system or the like.

The SML may be configured to arm the auto commit buffers with a logical identifier e.g. automatically by callback and or via the SML API . Each auto commit buffer may be armed to commit data to a different logical identifier different LBA persistent identifier or the like which may allow the ACM to provide memory semantic access to a number of different concurrent ACM users . In some embodiments arming an auto commit buffer comprises setting the ACM metadata with a logical identifier. In the example the ACM address range is associated with the logical identifier and the ACM metadata of the associated auto commit buffer is armed with the corresponding logical identifier .

The SML may arm an auto commit buffer using an I O control IOCTL command comprising the ACM address range the logical identifier and or an indicator of which auto commit buffer is to be armed. The SML through the SML API may provide an interface to disarm or detach the auto commit buffer . The disarm command may cause the contents of the auto commit buffer to be committed as described above e.g. committed to the non volatile storage device . The detach may further comprise disarming the auto commit buffer e.g. clearing the ACM metadata . The SML may be configured to track mappings between address ranges in the memory system and auto commit buffers so that a detach command is performed automatically.

Alternatively or in addition the SML may be integrated into the operating system or virtual operating system e.g. hypervisor of the host . This may allow the auto commit buffers to be used by a virtual memory demand paging system. The operating system may through the SML API or other integration technique map arm auto commit buffers for use by ACM users . The operating system may issue commit commands when requested by an ACM user and or its internal demand paging system. Accordingly the operating system may use the ACM as another generally available virtual memory resource.

Once an ACM user has mapped the ACM address range to an auto commit buffer and has armed the buffer the ACM user may access the resource using memory access semantics and may consider the memory accesses to be logically committed as soon as the memory access has completed. The ACM user may view the memory semantic accesses to the ACM address range to be instantly committed because the ACM is configured to commit the contents of the auto commit buffer to the logical identifier regardless of experiencing restart conditions. Accordingly the ACM user may not be required to perform separate write and commit commands e.g. a single memory semantic write is sufficient to implement a write commit . Moreover the mapping between the auto commit buffer and the ACM disclosed herein removes overhead due to function calls system calls and even a hypervisor if the ACM user is running in a virtual machine that typically introduce latency into the write commit path. The write commit latency time of the ACM user may therefore be reduced to the time required to access the ACM itself.

As described above in certain embodiments the host may map one or more ACM buffers into an address range of a physical memory address space addressable by a CPU a kernel or the like of the host device such as the memory system as directly attached physical memory as MMIO addressable physical memory over a PCI e bus or otherwise mapped as one or more pages of physical memory. The host may further map at least a portion of the physically mapped ACM buffers into a virtual memory address space accessible to user space processes or the like as virtual memory. The host may map the entire capacity of the physically mapped ACM buffers into a virtual memory address space a portion of the physically mapped ACM buffers into a virtual memory address space or the like.

In a similar manner the host may include a virtual machine hypervisor host operating system or the like that maps the physically mapped ACM buffers into an address space for a virtual machine or guest operating system. The physically mapped ACM buffers may appear to the virtual machine or guest operating system as physically mapped memory pages with the virtual machine hypervisor or host operating system spoofing physical memory using the ACM buffers . A resource management agent as described above may allocate arbitrate storage capacity of the ACM buffers among multiple virtual machines guest operating systems or the like.

Because in certain embodiments virtual machines guest operating systems or the like detect the physically mapped ACM buffers as if they were simply physically mapped memory the virtual machines can sub allocate arbitrate the ACM buffers into one or more virtual address spaces for guest processes or the like. This allows processes within guest operating systems in one embodiment to change ACM data and or ACM metadata directly without making guest operating system calls without making requests to the hypervisor or host operating system or the like.

In another embodiment instead of spoofing physical memory for a virtual machine and or guest operating system a virtual machine hypervisor a host operating system or the like of the host device may use para virtualization techniques. For example a virtual machine and or guest operating system may be aware of the virtual machine hypervisor or host operating system and may work directly with it to allocate arbitrate the ACM buffers or the like. When the ACM is used in a virtual machine environment in which one or more ACM users operate within a virtual machine maintained by a hypervisor the hypervisor may be configured to provide ACM users operating within the virtual machine with access to the SML API and or SML .

The hypervisor may access the SML API to associate logical identifiers with auto commit buffers of the ACM as described above. The hypervisor may then provide one or more armed auto commit buffers to the ACM users e.g. by mapping an ACM address range within the virtual machine memory system to the one or more auto commit buffers . The ACM user may then access the ACM using memory access semantics e.g. efficient write commit operations without incurring overheads due to inter alia hypervisor and other system calls. The hypervisor may be further configured to maintain the ACM address range in association with the auto commit buffers until explicitly released by the ACM user e.g. the keep the mapping from changing during use . Para virtualization and cooperation in certain embodiments may increase the efficiency of the ACM in a virtual machine environment.

In some embodiments the ACM user may be adapted to operate with the instant commit memory access semantics provided by the ACM . For example since the armed auto commit buffers are triggered to commit in the event of a restart without an explicit commit command the order in which the ACM user performs memory access to the ACM may become a consideration. The ACM user may employ memory barriers complier flags and the like to ensure the proper ordering of memory access operations.

For example read before write hazards may occur where an ACM user attempts to read data through the block device interface that is stored on the ACM via the memory semantic interface . In some embodiments the SML may maintain metadata tracking the associations between logical identifiers and or address ranges in the memory system and auto commit buffers . When an ACM user or other entity attempts to access a logical identifier that is mapped to an auto commit buffer e.g. through the block device interface the SML directs the request to the ACM via the memory semantic interface preventing a read before write hazard.

The SML may be configured to provide a consistency mechanism for obtaining a consistent state of the ACM e.g. a barrier snapshot or logical copy . The consistency mechanism may be implemented using metadata maintained by the SML which as described above may track the triggered auto commit buffers in the ACM . A consistency mechanism may comprise the SML committing the contents of all triggered auto commit buffers such that the state of the persistent storage is maintained e.g. store the contents of the auto commit buffers on the non volatile storage or other persistent storage .

As described above ACM users may access the ACM using memory access semantics at RAM granularity with the assurance that the operations will be committed if necessary in the event of restart failure power loss or the like . This is enabled by inter alia a mapping between the memory system of the host and corresponding auto commit buffers memory semantic operations implemented within an ACM memory range mapped to an auto commit buffer are implemented directly on the buffer . As discussed above data transfer between the host and the ACM may be implemented using any suitable data transfer mechanism including but not limited to the host performing processor IO operations PIO with the ACM via the bus e.g. MMIO PMIO and the like the ACM or other device providing one or more DMA engines or agents data movers to transfer data between the host and the ACM the host performing processor cache write flush operations or the like. Transferring data on the bus may comprise issuing a bus write operation followed by a read. The subsequent read may be required where the bus e.g. PCI bus does not provide an explicit write acknowledgement.

In some embodiments an ACM user may wish to transfer data to the ACM in bulk as opposed to a plurality of small transactions. Bulk transfers may be implemented using any suitable bulk transfer mechanism. The bulk transfer mechanism may be predicated on the features of the bus . For example in embodiments comprising a PCI e bus bulk transfer operations may be implemented using bulk register store CPU instructions.

Similarly certain data intended for the ACM may be cached in processor cache of the processor complex . Data that is cached in a processor cache may be explicitly flushed to the ACM to particular auto commit buffers using a CPU cache flush instruction or the like such as the serializing instruction described below.

The DMA engines described above may also be used to perform bulk data transfers between an ACM user and the ACM . In some embodiments the ACM may implement one or more of the DMA engines which may be allocated and or accessed by ACM users using the SML through the SML API . The DMA engines may comprise local DMA transfer engines for transferring data on a local system bus as well as RDMA transfer engines for transferring data using a network bus network interface or the like.

In some embodiments the ACM may be used in caching applications. For example the non volatile storage device may be used as cache for other backing store such as a hard disk network attached storage or the like not shown . One or more of the ACM auto commit buffers may be used as a front end to the non volatile storage cache a write back cache by configuring one or more of the auto commit buffers of the ACM to commit data to the appropriate logical identifiers in the non volatile storage . The triggered buffers are accessible to ACM users as described above e.g. by mapping the buffers into the memory system of the host . A restart condition causes the contents of the buffers to be committed to the non volatile storage cache. When the restart condition is cleared the cached data in the non volatile storage committed by the auto commit buffers on the restart condition will be viewed as dirty in the write cache and available for use and or migration to the backing store. The use of the ACM as a cache front end may increase performance and or reduce wear on the cache device.

In some embodiments auto commit buffers of the ACM may be leveraged as a memory write back cache by an operating system virtual memory system and or one or more CPUs of the host . Data cached in the auto commit buffers as part of a CPU write back cache may be armed to commit as a group. When committed the auto commit buffers may commit both data and the associated cache tags. In some embodiments the write back cache auto commit buffers may be armed with an ACM address or armed with a predetermined write back cache address . When the data is restored logical identifier information such as LBA and the like may be determined from a log or other data.

In some embodiments the SML may comprise libraries and or publish APIs adapted to a particular set of ACM users . For example the SML may provide an Instant Committed Log Library ICL adapted for applications whose performance is tied to write commit latency such as transaction logs database file system and other transaction logs store and forward messaging systems persistent object caching storage device metadata and the like.

The ICL provides mechanisms for mapping auto commit buffers of the ACM into the memory system of an ACM user as described above. ACM users or the ICL itself may implement an efficient supplier consumer paradigm for auto commit buffer allocation arming and access. For example a supplier thread or process in the application space of the ACM users may be used to allocate and or arm auto commit buffers for the ACM user e.g. map auto commit buffers to address ranges within the memory system of the host arm the auto commit buffers with a logical identifier and so on . A consumer thread or process of the ACM user may then accesses the pre allocated auto commit buffers . In this approach allocation and or arming steps are taken out of the write commit latency path of the consumer thread. The consumer thread of the ACM user may consider memory semantic accesses to the memory range mapped to the triggered auto commit buffers the ACM memory range as being instantly committed as described above.

Performance of the consumer thread s of the ACM user may be enhanced by configuring the supplier threads of an Instant Committed Log Library ICL or ACM user to allocate and or arm auto commit buffers in advance. When a next auto commit buffer is needed the ACM user have access a pre allocated armed buffer from a pool maintained by the supplier. The supplier may also perform cleanup and or commit operations when needed. For example if data written to an auto commit buffer is to be committed to persistent storage a supplier thread or another thread outside of the write commit path may cause the data to be committed using the SML API . Committing the data may comprise re allocating and or re arming the auto commit buffer for a consumer thread of the ACM user as described above.

The supplier consumer approach described above may be used to implement a rolling buffer. An ACM user may implement an application that uses a pre determined amount of rolling data. For example an ACM user may implement a message queue that stores the last 20 inbound messages and or the ACM user may manage directives for a non volatile storage device e.g. persistent trim directives or the like . A supplier thread may allocate auto commit buffers having at least enough capacity to hold the rolling data needed by the ACM user e.g. enough capacity to hold the last 20 inbound messages . A consumer thread may access the buffers using memory access semantics load and store calls as described above. The SML API or supplier thread of the ACM user may monitor the use of the auto commit buffers . When the consumer thread nears the end of its auto commit buffers the supplier thread may re initialize the head of the buffers by causing the data to be committed if necessary mapping the data to another range within the memory system and arming the auto commit buffer with a corresponding logical identifier. As the consumer continues to access the buffers the consumer stores new data at a new location that rolls over to the auto commit buffer that was re initialized by the supplier thread and continues to operate. In some cases data written to the rolling buffers described above may never be committed to persistent storage unless a restart condition or other triggering condition occurs . Moreover if the capacity of the auto commit buffers is sufficient to hold the rolling data of the ACM user the supplier threads may not have to perform re initialize re arming described above. Instead the supplier threads may simply re map auto commit buffers that comprise data that has rolled over and or discard the rolled over data therein .

In its simplest form a rolling buffer may comprise two ACM buffers and the SML may write to one ACM buffer for an ACM user while destaging previously written data from the other ACM buffer to a storage location such as the non volatile memory or the like. In response to filling one ACM buffer and completing a destaging process of the other ACM buffer the SML may transparently switch the two ACM buffers such that the ACM user writes to the other ACM buffer during destaging of the one ACM buffer in a ping pong fashion. The SML may implement a similar rolling process with more than two ACM buffers . The ICL in certain embodiments includes and or supports one or more transactional log API functions. An ACM user may use the ICL in these embodiments to declare or initialize a transactional log data structure.

As a parameter to a transactional log API command to create a transactional log data structure in one embodiment the ICL receives a storage location such as a location in a namespace and or address space of the non volatile storage or the like to which the SML may commit empty and or destage data of the transactional log from two or more ACM buffers in a rolling or circular manner as described above. Once an ACM user has initialized or declared a transactional log data structure in one embodiment the use of two or more ACM buffers to implement the transactional log data structure is substantially transparent to the ACM user with the performance and benefits of the ACM . The use of two or more ACM buffers in certain embodiments is transparent when the destage rate for the two or more ACM buffers is greater than or equal to the rate at which the ACM user writes to the two or more ACM buffers . The ICL in one embodiment provides byte level writes to a transactional log data structure using two or more ACM buffers .

In another example a supplier thread may maintain four 4 or more ACM buffers . A first ACM buffer may be armed and ready to accept data from the consumer as described above. A second ACM buffer may be actively accessed e.g. filled by a consumer thread as described above. A third ACM buffer may be in a pre arming process e.g. re initializing as described above and a fourth ACM buffer may be emptying or destaging e.g. committing to persistent storage as described above .

In some embodiments the ICL and or rolling log mechanisms described above may be used to implement an Intent Log for Synchronous Writes for a filesystem e.g. the ZFS file system . The log data ZIL may be fairly small 1 to 4 gigabytes and is typically write only. Reads may only be performed for file system recovery. One or more auto commit buffers may be used to store filesystem data using a rolling log and or demand paging mechanism as described above.

The ICL library may be configured to operate in a high availability mode as described above in conjunction with . In a high availability mode the SML and or bus sends commands pertaining to memory semantic accesses to two or more ACM each of which may implement the requested operations and or be triggered to commit data in the event of a restart condition.

The ACM disclosed herein may be used to enable other types of applications such as durable synchronization primitives. A synchronization primitive may include but is not limited to a semaphore mutex atomic counter test and set or the like.

A synchronization primitive may be implemented on an auto commit buffer . ACM users or other entities that wish to access the synchronization primitive may map the auto commit buffer into the memory system . In some embodiments each ACM user may map the synchronization primitive auto commit buffer into its own respective address range in the memory system . Since the different address ranges are all mapped to the same auto commit buffer all will show the same state of the synchronization primitive. ACM users on remote computing devices may map the synchronization primitive auto commit buffer into their memory system using an RDMA network or other remote access mechanism e.g. Infiniband remote PCI etc. .

In some embodiments the SML may comprise a Durable Synchronization Primitive Library DSL to facilitate the creation of and or access to synchronization primitives on the ACM . The DSL may be configured to facilitate one to many mappings as described above one auto commit buffer to many address ranges in the memory system .

The ACM users accessing the semaphore primitive may consider their accesses to be durable since if a restart condition occurs while the synchronization primitive is in use the state of the synchronization primitive will be persisted as described above the auto commit buffer of the synchronization primitive will be committed to the non volatile storage or other persistent storage .

As described above the SML may be used to map a file into the memory system virtual address space of the host . The file may be mapped in an Instant Committed Memory ICM mode. In this mode all changes made to the memory mapped file are guaranteed to be reflected in the file even if a restart condition occurs. This guarantee may be made by configuring the demand paging system to use an auto commit buffer of the ACM for all dirty pages of the ICM file. Accordingly when a restart condition occurs the dirty page will be committed to the file and no data will be lost.

In some embodiments the SML may comprise an ICM Library ICML to implement these features. The ICML may be integrated with an operating system and or virtual memory system of the host . When a page of an ICM memory mapped file is to become dirty the ICML prepares an auto commit buffer to hold the dirty page. The auto commit buffer is mapped into the memory system of the host and is triggered to commit to a logical identifier associated with the memory mapped file. As described above changes to the pages in the memory system are implemented on the auto commit buffer via the memory semantic access module .

The ICML may be configured to commit the auto commit buffers of the memory mapped file when restart conditions occur and or when the demand paging system of the host needs to use the auto commit buffer for another purpose. The determination of whether to detach the auto commit buffer from a dirty page may be made by the demand paging system by the SML e.g. using a least recently used LRU metric or the like or by some other entity e.g. an ACM user . When the auto commit buffer is detached the SML may cause its contents to be committed. Alternatively the contents of the auto commit buffer may be transferred to system RAM at which point the virtual memory mapping of the file may transition to use a RAM mapping mechanisms.

In some embodiments the SML or ICML may be configured to provide a mechanism to notify the operating system virtual memory system or the like that a page of a memory mapped file is about to become dirty in advance of an ACM user writing the data. This notification may allow the operating system to prepare an auto commit buffer for the dirty page in advance and prevent stalling when the write actually occurs while the auto commit buffer is mapped and armed . The notification and preparation of the auto commit buffer may implemented in a separate thread e.g. a supplier thread as described above .

The SML and or ICML may provide an API to notify the operating system that a particular page that is about to be written has no useful contents and should be zero filled. This notification may help the operating system to avoid unnecessary read operations.

The mechanisms for memory mapping a file to the ACM may be used in log type applications. For example the ICL library may be implemented to memory map a log file to one or more auto commit buffers as described above. A supplier thread may provide notifications to the operating system regarding which pages are about to become dirty and or to identify pages that do not comprise valid data.

Alternatively or in addition the ICML may be implemented without integration into an operating system of the host . In these embodiments the ICML may be configured to monitor and or trap system signals such as mprotect mmap and manual segmentation fault signals to emulate the demand paging operations typically performed by an operating system.

At step an auto commit buffer of the ACM may be mapped into the memory system of a computing device e.g. the host . The mapping may comprise associating a BAR address of the auto commit buffer with an address range in the memory system.

At step the auto commit buffer may be armed with ACM metadata configured to cause the auto commit buffer to be committed to a particular persistent storage and or at a particular location in the persistent storage in the event of a restart condition. In some embodiments the ACM metadata may comprise a logical identifier such as a LBA object identifier or the like. Step may comprise verifying that the ACM metadata is valid and or can be used to commit the contents of the auto commit buffer.

At step an ACM user such as an operating system application or the like may access the armed auto commit buffer using memory access semantics. The ACM user may consider the accesses to be instantly committed due to the arming of step . Accordingly the ACM user may implement instant committed writes that omit a separate and or explicit commit command. Moreover since the memory semantic accesses are directly mapped to the auto commit buffer via the mapping of step the memory semantic accesses may bypass systems calls typically required in virtual memory systems.

At step an auto commit buffer of an ACM is mapped into the memory system of a computing device e.g. the host and is armed as described above.

At step an ACM user accesses the auto commit buffer using memory access semantics e.g. by implementing memory semantic operations within the memory range mapped to the auto commit buffer at step .

At step a restart condition is detected. As described above the restart condition may be a system shutdown a system restart a loss of power a loss of communication between the ACM and the host computing device a software fault or any other restart condition that precludes continued operation of the ACM and or the host computing device.

At step the ACM implements the armed triggered commit actions on the auto commit buffer. The triggered commit action may comprise committing the contents of the auto commit buffer to persistent storage such as a solid state or other non volatile storage or the like.

At step the method ends until a next auto commit buffer is mapped and or armed or a restart condition is detected.

At step the method accesses armed auto commit buffers on the ACM if any . Accessing the armed auto commit buffer may comprise the method determining whether an auto commit buffer has been armed by inspecting the triggered ACM metadata thereof. If no triggered ACM metadata exists or the ACM metadata is invalid the method may determine that the auto commit buffer is not armed. If valid triggered ACM metadata does exist for a particular auto commit buffer the method identifies the auto commit buffer as an armed buffer and continues to step .

At step the triggered commit action for the armed auto commit buffers is performed. Performing the triggered commit action may comprise persisting the contents of the auto commit buffer to a sequential and or log based storage media such as a solid state or other non volatile storage media. Accordingly the triggered commit action may comprise accessing a logical identifier of the auto commit buffer labeling the data with the logical identifier and injecting the labeled data into a write data pipeline. Alternatively the triggered commit action may comprise storing the data on a persistent storage having a one to one mapping between logical identifier and physical storage address e.g. a hard disk . The triggered commit action may comprise storing the contents of the armed auto commit buffer to the specified physical address.

Performing the triggered commit action at step may comprise using a secondary power supply to power the ACM solid state storage medium and or other persistent non volatile storage medium until the triggered commit actions are completed.

In certain embodiments instead of or in addition to using a volatile memory namespace such as a physical memory namespace a virtual memory namespace or the like and or instead of or in addition to using a storage namespace such as a file system namespace a logical unit number LUN namespace or the like one or more commit agents as described above may implement an independent persistent memory namespace for the ACM . For example a volatile memory namespace which is typically accessed using an offset in physical and or virtual memory is not persistent or available after a restart event such as a reboot failure event or the like and a process that owned the data in physical and or virtual memory prior to the restart event typically no longer exists after the restart event. Alternatively a storage namespace is typically accessed using a file name and an offset a LUN ID and an offset or the like. While a storage namespace may be available after a restart event a storage namespace may have too much overhead for use with the ACM . For example saving a state for each executing process using a file system storage namespace may result in a separate file for each executing process which may not be an efficient use of the ACM .

The one or more commit agents and or the controller in certain embodiments provide ACM users with a new type of persistent memory namespace for the ACM that is persistent through restart events without the overhead of a storage namespace. One or more processes such as the ACM user in one embodiment may access the persistent memory namespace using a unique identifier such as a globally unique identifier GUID universal unique identifier UUID or the like so that data stored by a first process for an ACM user prior to a restart event is accessible to a second process for the ACM user after the restart event using a unique identifier without the overhead of a storage namespace a file system or the like.

The unique identifier in one embodiment may be assigned to an ACM user by a commit agent the controller or the like. In another embodiment an ACM user may determine its own unique identifier. In certain embodiments the persistent memory namespace is sufficiently large and or ACM users determine a unique identifier in a predefined known manner e.g. based on a sufficiently unique seed value nonce or the like to reduce limit and or eliminate collisions between unique identifiers. In one embodiment the ACM metadata includes a persistent memory namespace unique identifier associated with an owner of an ACM buffer an owner of one or more pages of an ACM buffer or the like.

In one embodiment the one or more commit agents and or the controller provide a persistent memory namespace API to ACM users over which the ACM users may access the ACM using the persistent memory namespace. In various embodiments the one or more commit agents and or the controller may provide a persistent memory namespace API function to transition convert map and or copy data from an existing namespace such as a volatile memory namespace or a storage namespace to a persistent memory namespace a persistent memory namespace API function to transition convert map and or copy data from a persistent memory namespace to an existing namespace such as a volatile memory namespace or a storage namespace a persistent memory namespace API function to assign a unique identifier such as a GUID a UUID or the like a persistent memory namespace API function to list or enumerate ACM buffers associated with a unique identifier a persistent memory namespace API function to export or migrate data associated with a unique identifier so that an ACM user such as an application and or process may take its ACM data to a different host to a different ACM or the like and or other persistent memory namespace API functions for the ACM . For example an ACM user in one embodiment may use a persistent memory namespace API function to map one or more ACM buffers of a persistent memory namespace into virtual memory of an operating system of the host or the like and the mapping into the virtual memory may end in response to a restart event while the ACM user may continue to access the one or more ACM buffers after the restart event using the persistent memory namespace. In certain embodiments the SML may provide the persistent memory namespace API in cooperation with the one or more commit agents and or the controller .

The persistent memory namespace in certain embodiments is a flat non hierarchical namespace of ACM buffers and or associated ACM pages indexed by the ACM metadata . The one or more commit agents and or the controller in one embodiment allow the ACM buffers to be queried by ACM metadata . In embodiments where the ACM metadata includes a unique identifier in certain embodiments an ACM user may query or search the ACM buffers by unique identifier to locate ACM buffers and or stored data associated with a unique identifier. In a further embodiment the one or more commit agents and or the controller may provide one or more generic metadata fields in the ACM metadata such that an ACM user may define its own ACM metadata in the generic metadata field or the like. The one or more commit agents and or the controller in one embodiment may provide access control for the ACM based on unique identifier or the like.

In one embodiment an ACM buffer may be a member of a persistent memory namespace and one or more additional namespaces such as a volatile namespace a storage namespace or the like. In a further embodiment the one or more commit agents and or the controller may provide multiple ACM users with simultaneous access to the same ACM buffers . For example multiple ACM users of the same type and or with the same unique identifier multiple instances of a single type of ACM user multiple processes of a single ACM user or the like may share one or more ACM buffers . Multiple ACM users accessing the same ACM buffers in one embodiment may provide their own access control for the shared ACM buffers such as a locking control turn based control moderator based control or the like. In a further embodiment using a unique identifier a new ACM user an updated ACM user or the like on the host may access

In certain embodiments the ACM may comprise a plurality of independent access channels buses and or ports and may be at least dual ported e.g. dual ported triple ported quadruple ported . In embodiments where the ACM is at least dual ported the ACM is accessible over a plurality of independent buses . For example the ACM may be accessible over redundant bus connections with a single host may be accessible to a plurality of hosts over separate buses with the different hosts or the like. In embodiments where the ACM is at least dual ported if one node and or access channel fails e.g. a host a bus one or more additional nodes and or access channels to the ACM remain functional obviating the need for redundancy replication or the like between multiple hosts .

In one embodiment the ACM comprises a PCI e attached dual port device and the ACM may be connected to and in communication with two hosts over independent PCI e buses . For example the ACM may comprise a plurality of PCI e edge connectors for connecting to a plurality of PCI e slot connectors or the like. In a further embodiment the power connection may also be redundant with one power connection per bus or the like. At least one of the plurality of connections in certain embodiments may comprise a data network connection such as a NIC or the like. For example the ACM may comprise one or more PCI e connections and one or more data network connections.

In one embodiment the controller may arbitrate between a plurality of hosts to which the ACM is coupled such that one host may access the ACM buffers at a time. The controller in another embodiment may accept a reservation request from a host and may provide the requesting host with access to the ACM buffers in response to receiving the reservation request. The ACM may natively support a reservation request as an atomic operation of the ACM . In other embodiments the ACM may divide ACM buffers between hosts may divide ACM buffers between hosts but share backing non volatile memory between hosts or may otherwise divide the ACM buffers the non volatile memory and or associated address spaces between hosts .

In one embodiment the controller the one or more commit agents and or other elements of the ACM may be dual headed split brained or the like each head or brain being configured to communicate with a host and with each other to provide redundant functions for the ACM . By being at least dual ported in certain embodiments the ACM may be redundantly accessible without the overhead of replication duplication or the like which would otherwise reduce I O speeds of the ACM especially if such replication duplication were performed over a data network or the like.

In general the ACM module services auto commit requests from an ACM user or other client for the ACM . As described above with regard to the ACM users as used herein a client may comprise one or more of an operating system OS virtual operating platform e.g. an OS with a hypervisor guest OS application process thread entity utility user or the like that is configured to access or use the ACM . In certain embodiments the ACM module supports one or more auto commit memory management operations such as a populate operation a destage operation a barrier operation a checkpoint operation or the like. In the depicted embodiment the ACM module includes an auto commit request module and a transfer module . The transfer module in the depicted embodiment includes a populate module and a destage module . By supporting a populate destage barrier and or checkpoint operation in certain embodiments the ACM module provides an interface whereby an ACM user or other client may manage what data is stored in and or ensure persistence and consistency for the byte addressable ACM buffers whether the ACM buffers are natively volatile or non volatile regardless of the type of media used for the ACM buffers .

As described above in certain embodiments the ACM module and or the ACM enable clients such as the ACM users to access fast byte addressable persistent memory combining benefits of volatile memory and non volatile storage. Auto commit logic inside the hardware of the storage device such as the power management apparatus described above in certain embodiments provides power cut protection for data written to the auto commit buffers of the ACM .

In certain embodiments a storage capacity of the auto commit buffers may be less than a storage capacity of the non volatile memory media of the non volatile storage device less than a storage capacity of a recording memory storage medium located external to the non volatile storage device such as on the host device with an ACM user over a data network or the like. For example a recording memory storage medium of the auto commit buffers may be more expensive per unit of storage capacity than the non volatile memory media or the like. In order to manage storage capacity differences to provide flexibility to ACM users or the like in one embodiment the ACM module may allow an ACM user or other client to dynamically load or populate data from other locations into the auto commit buffers using a populate command and or to dynamically unload or destage data from the auto commit buffers to another location using a destage command thereby freeing storage capacity of the auto commit buffers for other use. By allowing ACM users to dynamically manage and move data into and out of auto commit buffers in certain embodiments the ACM module may minimize an impact of a smaller storage capacity of the auto commit buffers allowing the ACM users to dynamically store certain data in other locations.

The ACM may be accessible to applications operating system components and or other ACM users as byte addressable memory mapped to a virtual address space of a memory system of a processor complex . Updates to data of the ACM by ACM users may be stored in one or more processor caches of the processor complex and or the memory system from which the data may be written back lazily to the underlying ACM buffers . A processor cache may include a write combine buffer an L1 processor cache an L2 processor cache an L3 processor cache a processor cache hierarchy and or another type of processor cache.

Caching data of the ACM buffers in a processor cache may improve performance e.g. decrease an amount of time it takes for the processor complex to access data of the ACM buffers . However in certain embodiments caching data of the ACM buffers may increase a risk of losing updates to the data in response to a restart event such as a power failure of the host . For example a processor cache may be weakly ordered not guaranteeing or ensuring that the processor cache will maintain an order of operations for cached data but instead trickling data down to the auto commit buffers arbitrarily without a guaranteed order or timeframe.

In certain embodiments the ACM module in cooperation with the SML or the like makes the ACM available to one or more ACM users using an API such as the SML API described above. The ACM module and or the SML may provide the SML API and or another ACM API to ACM users at a user level and or a kernel level.

To make the ACM usable for ACM users even across restart events the ACM module may provide persistence and or consistency for data of the auto commit buffers despite the weak ordering of a processor cache of the processor complex . The ACM module in certain embodiments may guarantee or ensure that application data residing in processor caches of the processor complex has been flushed or destaged to the ACM and will be persisted across restart events as described above.

The ACM module may provide consistency of data of the auto commit buffers to ensure that the data is meaningful to the ACM user after a restart event e.g. the data may be accessed recovered interpreted by an ACM user to recover application data and or state . As described above the memory system may flush destage or otherwise move data from the one or more processor caches of the processor complex to the auto commit buffers at arbitrary times without strict ordering.

Further ACM users may perceive consistency across multiple updates to data of the auto commit buffers . For example a transaction of an ACM user may change multiple attributes within a data structure and the ACM module may atomically update each change to preserve application consistency for the ACM user . By managing consistency for data of the auto commit buffers for the ACM users in certain embodiments the ACM module may obviate the need for ACM users to manage consistency themselves thereby simplifying application development use of the ACM and the like.

The ACM module may provide guarantee or otherwise manage persistence and or consistency using one or more management and or synchronization operations such as a populate operation a destage operation a barrier operation a checkpoint operation or the like. As used herein a populate operation comprises an ACM operation that transfers copies moves and or loads data into one or more auto commit buffers of the ACM . A destage operation as used herein comprises an ACM operation that transfers copies moves and or loads data from one or more auto commit buffers of the ACM to another location. As used herein a barrier operation comprises an ACM operation that synchronizes flushes and or destages dirty data from one or more processor caches to one or more auto commit buffers of the ACM . A checkpoint operation as used herein comprises an ACM operation that creates a copy or snapshot of the data contents of one or more pages of the auto commit buffers .

In one embodiment a checkpoint operation synchronizes flushes and or destages dirty data from one or more processor caches to the auto commit buffers e.g. performs a barrier operation prior to copying the data contents of the pages of the auto commit buffers to create a snapshot to service a destage request or the like. The ACM module in various embodiments may use the auto commit request module the transfer module e.g. the populate module and or the destage module a barrier module a flush module a completion module a checkpoint module and or a lock module to execute populate operations destage operations barrier operations checkpoint operations and or other management operations for the ACM .

In one embodiment the auto commit request module is configured to monitor detect or otherwise receive auto commit requests from clients such as the ACM users described above another module a host computing device or the like. The auto commit request module in one embodiment receives auto commit requests from clients over an ACM API such as the SML API described above or the like. The auto commit request module may receive auto commit requests directly or indirectly may receive a notification or report of an auto commit request or the like.

An auto commit request that the auto commit request module receives in certain embodiments may comprise a populate request a destage request a barrier request a checkpoint request or another ACM management request. The auto commit request module in certain embodiments determines a request type for a received auto commit request for example determining whether a received auto commit request is a populate request a destage request a barrier request or a checkpoint request so that the ACM module can service the received auto commit request. The auto commit request module may provide a different function method and or channel for different types of auto commit requests may support a type indicator or field in an auto commit request indicating a request type may parse an auto commit request to determine a request type or the like.

As described in greater detail below with regard to the transfer module the auto commit request module may be configured to receive auto commit requests to copy move write load or otherwise transfer data between two namespaces two locations or the like such as a populate request a destage request or the like. As used herein a namespace comprises a container or range of logical or physical identifiers that index or identify data data locations or the like. As described above examples of namespaces may include a file system namespace a LUN namespace a logical address space a storage namespace a virtual memory namespace a persistent ACM namespace a volatile memory namespace an object namespace a network namespace a global or universal namespace a BAR namespace or the like.

A populate request and or a destage request may include one or more logical identifiers such as a source logical identifier and a destination or target logical identifier indicating the locations and or namespaces between which the data is to be transferred. As described above logical identifiers may include a file identifier and or an offset from a file system namespace a LUN ID and an offset from a LUN namespace an LBA or LBA range from a storage namespace one or more virtual memory addresses from a virtual memory namespace an ACM address from a persistent ACM namespace a volatile memory address from a volatile memory namespace of the host device an object identifier a network address a GUID UUID or the like a BAR address or address range from a BAR namespace or another logical identifier. As described above a commit agent or the like may include a logical identifier for data in the ACM metadata for the data. The logical identifier for the data may be known to a client such as an ACM user and the commit agent or the like may make the data available to the client after a restart event so that the client can retrieve the data based on the known logical identifier even after the restart event.

In one embodiment the ACM module uses the transfer module to service auto commit requests to transfer or move data between different locations different namespaces or the like such as populate and or destage requests. The transfer module may use the populate module to service populate requests the destage module to service destage requests or the like.

The transfer module in cooperation with the populate module and or the destage module or the like may determine whether a namespace associated with an auto commit request is associated with the non volatile storage device such as a namespace for the non volatile memory media a namespace for the ACM or the like e.g. is the data of the request stored by and or to be transferred to the non volatile storage device . In certain embodiments at least one of the namespaces of a populate request and or a destage request is associated with an auto commit buffer of the ACM such as a namespace of a source logical identifier for a destage request of a destination logical identifier for a populate request or the like. For example the transfer module may analyze or parse a source logical identifier a destination logical identifier or the like from a populate request and or a destage request to determine whether the logical identifiers are for the ACM or the non volatile memory media .

If the transfer module the populate module and or the destage module determines that both the source and the destination for the data of a transfer are within the non volatile storage device e.g. the data is being populated from the non volatile memory media to the ACM the data is being destaged from the ACM to the non volatile memory media or the like the transfer may be performed internally within the non volatile storage device . In certain embodiments data is transferred internally within a non volatile storage device when the data remains within the physical confines or footprint of the non volatile storage device when the data is transferred without entering a system communications bus between the non volatile storage device and a host device when the data is transferred over an internal bus within the non volatile storage device or the like. For example in one embodiment the transfer module the populate module and or the destage module may transfer data between an integrated circuit or other storage element of the ACM and an integrated circuit or other storage element of the non volatile memory media over a communications bus of the non volatile storage device or the like on the same printed circuit board PCB or interconnected group of PCBs e.g. between a daughter card and a host or mother card between peer or sister cards on the same bus or the like .

By servicing transfer auto commit requests such as populate requests and or destage requests internally within a single device interconnected devices or the like without transferring the data to a host device and or a host processor over a system communications bus in certain embodiments the ACM module may reduce traffic and increase available bandwidth on the system communications bus reduce processing overhead for the host processor or the like in comparison to data transfers that use a system communications bus and or a host processor to complete the transfer. The transfer module using the populate module and or the destage module may optimize the transfer of data for the ACM by determining whether the source and or destination for the data is associated with the non volatile storage device and transferring the data internally when possible. If a source location or a destination location for a transfer auto commit request such as a populate and or destage request is external to the non volatile storage device such as another non volatile storage device of the host device volatile memory of the host device a network storage location or the like the transfer module may transfer move copy populate and or destage the data between the ACM and the external location associate the data with a namespace of the external location or the like.

In response to a populate request and or a destage request the transfer module may transfer or change a physical storage location for data and in certain embodiments may transfer or change a namespace and or logical address space for the data to reflect a new location for the data. For example based on a source location and or namespace of data of a populate request the populate module may transfer data from a file system namespace a LUN namespace a block storage namespace a virtual memory namespace a volatile memory namespace an object namespace a BAR namespace or another namespace to a namespace of the ACM such as the persistent ACM namespaces described above. In a further embodiment the populate module may associate transferred data in the ACM with ACM metadata such as a logical address for the data to be committed to in the non volatile memory media as described above.

In certain embodiments the transfer module may preserve a logical identifier for transferred data such as a logical identifier from ACM metadata for the data so that a client who knows the logical identifier may retrieve the data using the known logical identifier before or after transferring destaging and or populating the data. For example in response to a destage request for data the transfer module may write an LBA or LBA range a filename a LUN ID a persistent ACM address or the like for data to an append point of a sequential log of the non volatile memory media with the data or to another predefined location to preserve the association of the logical identifier and the data. In response to a populate request for data the transfer module may store a known logical identifier such as an LBA or LBA range a filename a LUN ID a persistent ACM address or the like for data to the auto commit buffer with the data as ACM metadata or the like to preserve the association between the known logical identifier and the data.

The populate module may transfer data between address spaces by preserving adding updating and or changing a logical to physical mapping for transferred data in a mapping structure index forward map or the like as described above with regard to the metadata and or forward index maintained by the SML . For example to transfer data from an address space of the non volatile memory media to an address space of the ACM in response to a populate request to transfer the data into the ACM the populate module may remove an entry for the data from a forward index or other mapping structure for the non volatile memory media and add an entry for the data in an index or other mapping structure for the ACM . For data the populate module transfers from a location external to the non volatile storage device the populate module may cooperate with a storage controller device driver or the like for the external location to remove the data from a namespace or address space of the external location or the like. In other embodiments the populate module may leave the data at the external location mapped by the namespace and or address space of the external location.

The destage module based on a destination location and or namespace of data of a destage request may transfer data from a namespace of the ACM to a file system namespace a LUN namespace a block storage namespace a virtual memory namespace a volatile memory namespace an object namespace a BAR namespace or another namespace of a destination or target location of the destage request. The destage module may transfer data between address spaces by preserving adding updating and or changing a logical to physical mapping for transferred data in a mapping structure index forward map or the like as described above with regard to the metadata and or forward index maintained by the SML . For example to transfer or destage data from an address space of the ACM to an address space of the non volatile memory media in response to a destage request to transfer the data to the non volatile memory media the destage module may remove an entry for the data from an index or other mapping structure for the ACM and add an entry for the data in a forward index or other mapping structure for the non volatile memory media . For data the destage module transfers or destages from the ACM to a location external to the non volatile storage device the destage module may cooperate with a storage controller device driver or the like for the external location to add the data to a namespace or address space of the external location as part of a write process for the external location or the like.

In one embodiment the transfer module may leave a copy of transferred data in the previous location accessible using one or more logical identifiers from the previous namespace. In a further embodiment the transfer module may delete remove trim invalidate erase or otherwise clear transferred data from the previous or original location to reclaim storage capacity of the previous or original location for use to store other data. In certain embodiments whether a transfer e.g. a populate a destage or the like is a copy with data remaining in a previous location or a move with data removed from a previous location may be selectable by a requesting user or client as a parameter to a populate and or destage request or the like.

In one embodiment the transfer module uses the populate module to service populate requests from ACM users or other clients. The populate module in certain embodiments is configured to service populate requests by transferring copying or loading data into one or more auto commit buffers of the ACM associating data with a namespace or address space of the one or more auto commit buffers or the like as described above.

The populate module may determine based on a populate request whether the data of the populate request is already located in the non volatile storage device in the non volatile memory media or the like and may transfer the data internally within the non volatile storage device as described above. For example the populate module may determine whether a source namespace or address space of the data of a populate request is associated with the non volatile storage device based on a source logical identifier for the data of the populate request and transfer or load the data into the ACM and or to a namespace or address space of the ACM internally within the non volatile storage device if the source namespace or address space is associated with the non volatile storage device . If the data of a populate request is not already located in the non volatile storage device the populate module may transfer the data from a location external to the non volatile storage device using a programmed input output PIO operation a DMA operation a 3party DMA operation an RDMA operation a block device interface an operating system storage stack or the like transferring the data over a system communications bus using a processor of the host device or the like.

In one embodiment the transfer module uses the destage module to service destage requests from ACM users or other clients. The destage module in certain embodiments is configured to service destage requests by transferring copying or destaging data from one or more auto commit buffers of the ACM to another location associating data with a namespace or address space of the other location as described above. In certain embodiments the destage module transfers or destages an entire range or region of data identified in a destage request regardless of whether a portion of the data may already be stored at the destination location. In another embodiment the destage module may transfer or destage just dirty data data that is not yet stored at the destination location without transferring or destaging clean data that is already stored at the destination location. In certain embodiments the destage module transfers copies or destages data from one or more auto commit buffers to another location such as the non volatile memory media or the like that may have a larger capacity a slower response time or the like than the one or more auto commit buffers .

The destage module may determine based on a destage request whether the destination for the destage request is within the non volatile storage device in the non volatile memory media or the like and may transfer or destage the data internally within the non volatile storage device as described above. For example the destage module may determine whether a destination namespace or address space of the data of a destage request is associated with the non volatile storage device such as the non volatile memory media based on a destination logical identifier for the data of the destage request and transfer or destage the data from the ACM internally within the non volatile storage device if the destination namespace or address space is associated with the non volatile storage device e.g. the data is being transferred copied or moved within the non volatile storage device to another location in the ACM to the non volatile memory media or the like .

If the destination for data of a destage request is not located in the non volatile storage device the populate module may transfer or destage the data from the ACM to a location external to the non volatile storage device using a PIO operation a DMA operation a 3party DMA operation an RDMA operation a block device interface an operating system storage stack or the like transferring the data over a system communications bus using a processor of the host device or the like. In response to transferring or destaging data of a destage request from one or more auto commit buffers of the ACM the destage module may delete remove trim invalidate erase or otherwise clear the data from the ACM and reuse the storage capacity associated with the data in the one or more auto commit buffers .

As described below with regard to the flush module prior to transferring or destaging data from the ACM the destage module may cooperate with the flush module to ensure consistency of the data e.g. that data is flushed from a processor complex of the host device to the one or more auto commit buffers . For example the flush module described below may issue a serializing instruction that flushes data from the processor complex to the one or more auto commit buffers may place a destage identifier or other marker in the processor complex associated with the non volatile storage device e.g. storing the destage identifier or other marker to a virtual memory address mapped to a control status register or other predefined location within the non volatile storage device may issuing a second serializing instruction to flush the destage identifier or other marker from the processor complex or the like. The destage module may transfer destage or otherwise write data of a destage request from the ACM to a destination location in response to receiving a destage identifier or other marker from the flush module in the non volatile storage device . In certain embodiments the flush module may place in the processor complex an indicator for the data of a destage request prior to issuing a serializing instruction so that the serializing instruction flushes the indicator to the non volatile storage device where it is received by the destage module .

In one embodiment the auto commit request module is substantially similar to the auto commit request module described above with regard to receiving detecting or otherwise monitoring auto commit requests and the like. The transfer module in one embodiment uses the populate module to service populate requests and or uses the destage module to service destage requests substantially as described above. The barrier module in certain embodiments services barrier auto commit requests using the flush module and the completion module and may ensure consistency of data for the transfer module .

In one embodiment the flush module to service a populate request a destage request a barrier request a checkpoint request or the like is configured to issue perform or otherwise execute a serializing instruction for a processor cache of a processor complex in response to the auto commit request module receiving an auto commit request. A serializing instruction flushes destages or otherwise synchronizes data from a processor cache of the processor complex and or the memory system to underlying memory such as an auto commit buffer . One or more auto commit buffers receive data that the flush module flushes or destages from the processor complex . An auto commit buffer or other underlying memory device to which data is flushed or destaged from the processor complex in response to a serializing instruction in one embodiment is selected by a memory manager for the processor complex or the like based on which underlying memory device is mapped into a logical address range for the data in virtual memory of the processor complex .

Examples of serializing instructions include an MFENCE instruction an SFENCE instruction an xchg instruction e.g. compare and swap compare and swap double CMPXCHG CMPXCHNG8B CMPXCHNG16B and or CMP8XCHG16 or the like. In certain embodiments a serializing instruction ensures and or guarantees that operations such as memory semantic load and store operations that precede the serializing instruction are flushed destaged or otherwise synchronized prior to operations that follow the serializing instruction.

In one embodiment the flush module issues performs or otherwise executes a serializing instruction for an entire processor cache or set of processor caches in response to an auto commit request. In another embodiment the flush module may issue perform or otherwise execute a serializing instruction just for data of one or more auto commit buffers stored in a processor cache or set of processor caches so that data associated with other memory devices is not necessarily flushed destaged and or synchronized. For example the flush module may include a memory address range for pages of one or more auto commit buffers in the serializing instruction so that the processor cache or set of caches flushes destages or otherwise synchronizes just the indicated memory address range .

In one embodiment the completion module is configured to determine completion of the serializing instruction flushing destaging or otherwise synchronizing data to one or more auto commit buffers . The completion module in certain embodiments determines that a serializing instruction has completed by placing a predefined completion identifier in the processor cache after the flush module issues the serializing instruction issuing a second serializing instruction and determining that the serializing instruction is complete once the completion identifier is received at the non volatile storage device of the ACM . In certain embodiments the completion identifier comprises a destage identifier or other marker which also indicates to the destage module that a destage operation has been requested. Because a serializing instruction ensures or guarantees that operations performed prior to the serializing instruction are synchronized prior to operations performed after the serializing instruction the synchronization of the completion identifier to the ACM in response to the second serializing instruction indicates that the first serializing instruction has completed. In other embodiments the completion module may determine completion of the serializing instruction without issuing a second serializing instruction. For example the processor complex may notify the completion module of completion of the serializing instruction by sending an interrupt writing a completion identifier to a control status register which the completion module may poll or the like.

The completion module in certain embodiments may place a completion identifier a destage identifier or another predefined marker in the processor cache by writing or storing the completion identifier to a virtual memory address of the host device mapped to a control status register of the ACM . In another embodiment the completion module may place a completion identifier in the processor cache by writing or storing the completion identifier to a virtual memory address mapped to a page of an auto commit buffer or the like. The completion module in various embodiments may detect arrival of a completion identifier a destage identifier or another marker in the ACM by polling a control status register of the ACM by polling a predefined location in the auto commit buffers by receiving an interrupt from the ACM or the like.

The completion module in certain embodiments may use different completion identifiers depending on the type of the auto commit request. For example the completion module may write a BARRIER COMPLETE completion identifier for a barrier request may write a CHECKPOINT BEGIN completion identifier for a checkpoint request may write a DESTAGE BEGIN completion identifier for a destage request or the like. A completion identifier in one embodiment comprises a predefined sequence string pattern flag or other indicator that the completion module may write or store to a processor cache to the ACM or the like.

For checkpoint requests as described below with regard to the checkpoint module in response to a CHECKPOINT BEGIN completion identifier or another checkpoint indicator the checkpoint module may create a snapshot copy of one or more pages of the auto commit buffers . The completion module and or the checkpoint module in certain embodiments may write a CHECKPOINT COMPLETE completion identifier to a control status register of the ACM in response to the checkpoint module completing the snapshot copy.

In one embodiment the completion module indicates to a requesting client completion of an auto commit request. The completion module in various embodiments may indicate or notify a client of completion of an auto commit request by returning execution control to the client by sending a return value to the client by sending an interrupt to the client by writing a completion identifier to a control status register polled or poked by the client or the like.

The ACM module in certain embodiments may guarantee or ensure persistence of data flushed to one or more auto commit buffers in response to the completion module determining that a serializing instruction for the flushed data has completed. In a further embodiment the ACM module may guarantee or ensure persistence of operations received for one or more auto commit buffers prior to a received auto commit request in response to the completion module determining completion of a serializing instruction for the auto commit request. As described above once data has been synchronized or stored in the auto commit buffers the ACM preserves or persists the data in non volatile memory media and provides the data from the non volatile memory media to clients such as ACM users after recovery from the restart event. In this manner in certain embodiments the ACM module can provide persistence and consistency of data for ACM users even if a processor cache does not guarantee an order of data an order of operations or the like.

As described above in certain embodiments the ACM is coupled to the host device using a communications bus such as a PCI e bus or the like. In one embodiment the communications bus supports strong operation ordering at least for transactions within a similar traffic class or the like so that the communications bus maintains an order in which data is flushed from the processor cache to one or more auto commit buffers . For example PCI e 2.0 PCI e 3.0 and the like support strong ordering semantics for transactions within the same traffic class. By flushing or destaging data from a processor cache to an auto commit buffer over a communications bus that supports strong operation ordering in certain embodiments the completion module may ensure that a serializing instruction has actually completed in response to receiving a completion identifier at the ACM because the data of the serializing instruction and the completion identifier are received in operation order. In embodiments where the communications bus does not support operation ordering the flush module may act as an intermediary between the processor complex and the communications bus coordinating with the controller to provide strong operation ordering over the communications bus or the like. For example the flush module may queue commands in a FIFO queue and manage and confirm the exchange of each command with the controller or the like to enforce strong or strict operation ordering. The communications bus may be in communication with the processor complex through a northbridge device a root complex or the like of the processor complex .

In one embodiment the checkpoint module cooperates with the barrier module to service checkpoint requests for one or more pages of the auto commit buffers . A checkpoint operation in certain embodiments comprises a barrier operation to ensure that pending writes residing in processor caches for the auto commit buffers are written back to the auto commit buffers followed by a snapshot operation to create a copy of one or more pages of data.

In one embodiment the checkpoint module sends to the ACM a checkpoint indicator identifying one or more pages of the auto commit buffers to be checkpointed e.g. the one or more pages that are to comprise the requested snapshot copy . The auto commit request module may receive a checkpoint indicator with a checkpoint request from a client or the like and may provide the checkpoint indicator to the checkpoint module . A checkpoint indicator in various embodiments may comprise a set a list a bitmap an array an address range a page index or another data structure indicating which pages of the one or more auto commit buffers the checkpoint module is to checkpoint.

The checkpoint module in certain embodiments sends a checkpoint indicator to the ACM by writing storing or otherwise placing the checkpoint indicator in a processor cache of the processor complex prior to the flush module issuing a first serializing instruction so that the first serializing instruction flushes or destages the checkpoint indicator to the ACM . For example the checkpoint module may store a checkpoint indicator to a virtual memory address that is mapped to a predefined control status register of the ACM that is mapped to a predefined location within an auto commit buffer or the like. In certain embodiments the checkpoint module comprises a driver checkpoint module disposed in a device driver on the host computing device such as the SML which places a checkpoint indicator in a processor cache of the processor complex and a cooperating controller checkpoint module disposed in a storage controller to receive directly or indirectly the checkpoint indicator at the ACM . By placing a checkpoint indicator in a processor cache prior to a first serializing instruction in certain embodiments the checkpoint module may ensure that the checkpoint indicator reaches the ACM prior to a completion identifier from the completion module .

In response to the completion module determining that the first serializing instruction has completed in one embodiment the checkpoint module copies one or more pages of the auto commit buffers creating a snapshot of the one or more pages or the like. For example the checkpoint module may copy the one or more pages in response to a completion identifier from the completion module comprising a checkpoint indicator such as CHECKPOINT BEGIN or the like. In one embodiment the checkpoint module copies the one or more pages to a second location within the auto commit buffers so that the snapshot copy is separately preserved in response to a restart event. In another embodiment the checkpoint module copies the one or more pages directly to non volatile memory media of the ACM creating the snapshot in the non volatile memory media . In a further embodiment the checkpoint module may copy the one or more pages to a separate non volatile memory device to a separate ACM B or the like that is independent from the ACM copying the one or more pages over a communications bus over a data network or the like.

A commit agent of the ACM in certain embodiments makes the snapshot copy of the one or more pages available to an ACM user after recovery from a restart event. In one embodiment the ACM only makes snapshot copies of checkpointed data available to ACM users after recovery from a restart event to provide the ACM users with a consistent known state or the like. Writes or other changes to ACM data that occur between a checkpoint operation and a restart event in certain embodiments may be lost and unavailable after the restart event. In another embodiment a commit agent of the ACM may make a raw un checkpointed version of data available to ACM users in addition to one or more snapshot copies or checkpointed versions of the data after a restart event.

In one embodiment the lock module is configured to stall operations for one or more auto commit buffers during execution of an auto commit request so that the ACM module can guarantee or ensure persistence and or consistency of data after the auto commit request or the like. For example the lock module may stall operations between the flush module issuing a serializing instruction and the completion module determining completion of the serializing instruction. The lock module in certain embodiments may stall operations by unmapping one or more auto commit buffers from the virtual address space of an ACM user in response to an auto commit request and remapping the one or more auto commit buffers into the virtual address space in response to completion of the auto commit request.

In certain embodiments the checkpoint module may track the progress of a checkpoint operation and the lock module may allow operations on portions of auto commit buffers that have already been copied. The checkpoint module in a further embodiment may cooperate with the lock module to copy or move existing data from an auto commit buffer to a snapshot so that the lock module can allow a pending operation for the auto commit buffer to execute.

The ACM module and its various sub modules as described above may be disposed in a device driver for the ACM executing on a processor of the host device such as the SML may be disposed in a storage controller for the ACM and or may comprise portions in each of a device driver and a storage controller or the like. In one embodiment a device driver for a non volatile storage device comprises the auto commit request module and the flush module and a storage controller of the non volatile storage device comprises at least a portion of the populate module the destage module the completion module and or the checkpoint module or the like.

For example in one embodiment the completion module comprises two portions a driver completion module disposed in a device driver on the host device and a controller completion module disposed in a storage controller . The driver completion module may store a completion identifier to a processor cache and issue a second serializing instruction from the host while the controller completion module may determine completion of the serializing instruction in response to receiving the completion identifier flushed or destaged from the processor cache to the non volatile storage device or the like in response to a second serializing instruction.

The lock module stalls operations for one or more auto commit buffers during execution of the received auto commit request. The flush module issues a serializing instruction to flush destage and or synchronize data from a processor cache to the one or more auto commit buffers . If the completion module determines that the serializing instruction is not complete the completion module continues to determine whether the serializing instruction is complete. If the completion module determines that the serializing instruction is complete the completion module writes a completion identifier to a control status register on the non volatile storage device of the ACM . The completion module issues a second serializing instruction to flush the completion identifier to the non volatile storage device .

If the auto commit request module determines that the auto commit request is a checkpoint request the checkpoint module copies one or more pages identified by the checkpoint indicator to create a snapshot copy within the non volatile storage device of the ACM . The method continues with step . If the auto commit request module determines that the auto commit request is a barrier request the checkpoint module may not copy pages of the auto commit buffers. The lock module allows operations for the one or more auto commit buffers and the auto commit request module and or the completion module indicates completion of the auto commit request to a client. The method continues and the auto commit request module monitors for additional auto commit requests.

If the auto commit request module determines that the auto commit request is a destage request the lock module stalls operations for one or more auto commit buffers during execution of the received auto commit request. The flush module issues a serializing instruction to flush destage and or synchronize data from a processor cache of a processor complex to the one or more auto commit buffers . If the completion module determines that the serializing instruction is not complete the completion module continues to determine whether the serializing instruction is complete. If the completion module determines that the serializing instruction is complete the completion module and or the destage module writes a destage identifier or another marker to a control status register on the non volatile storage device of the ACM . The completion module and or the destage module issues a second serializing instruction to flush the destage identifier or another marker to the non volatile storage device .

If the transfer module using the destage module or the like determines that the data is associated with the non volatile memory device such as the non volatile memory medium or the like the destage module writes destages moves copies or otherwise transfers the data from the auto commit buffer to the non volatile memory medium or other location within the non volatile memory device . The destage module associates the data with a namespace of the non volatile memory medium . If the transfer module using the destage module or the like determines that the data is not associated with the non volatile memory device e.g. is stored elsewhere outside the non volatile memory device the destage module writes destages moves copies or otherwise transfers the data from the auto commit buffer to a location external to the non volatile memory device and associates the data with a namespace of the external location. The lock module allows operations for the one or more auto commit buffers and the auto commit request module and or the completion module indicates completion of the auto commit request to a client. The method continues and the auto commit request module monitors for additional auto commit requests.

If the auto commit request module determines that the auto commit request is a populate request the transfer module using the populate module or the like determines whether the data is associated with the non volatile memory device e.g. stored in the non volatile memory media or the like . If the transfer module determines that the data is associated with the non volatile memory device the populate module writes populates moves copies or otherwise transfers the data from the non volatile memory medium or other location within the non volatile memory device into the auto commit buffer within the non volatile memory device . If the transfer module determines that the data is not associated with the non volatile memory device the populate module writes populates moves copies or otherwise transfers the data from a location external to the non volatile memory device into the auto commit buffer . The populate module associates the data with a namespace of the auto commit buffer . The auto commit request module and or the completion module indicates completion of the auto commit request to a client. The method continues and the auto commit request module monitors for additional auto commit requests.

A means for receiving an auto commit request for an auto commit buffer in various embodiments may include a storage management layer a device driver a storage controller an ACM API such as a SML API an auto commit request module a processor complex other logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for receiving an auto commit request for an auto commit buffer .

A means for populating an auto commit memory and or an auto commit buffer with data in response to an auto commit request comprising a populate request in various embodiments may include a storage management layer a device driver a storage controller an ACM module a transfer module a populate module a processor complex other logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for populating an auto commit memory and or an auto commit buffer with data.

A means for destaging data from an auto commit memory and or an auto commit buffer in response to an auto commit request comprising a destage request in various embodiments may include a storage management layer a device driver a storage controller an ACM module a transfer module a destage module a processor complex other logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for destaging data from an auto commit memory and or an auto commit buffer .

A means for flushing data from a processor complex to an auto commit memory buffer in response to an auto commit request comprising a destage request in various embodiments may include a storage management layer a device driver a storage controller a transfer module a destage module a flush module a processor complex other logic hardware and or other executable code stored on a computer readable storage medium. Other embodiments may include similar or equivalent means for issuing a serializing instruction in response to an auto commit request.

The present disclosure may be embodied in other specific forms without departing from its spirit or essential characteristics. The described embodiments are to be considered in all respects only as illustrative and not restrictive. The scope of the disclosure is therefore indicated by the appended claims rather than by the foregoing description. All changes which come within the meaning and range of equivalency of the claims are to be embraced within their scope.

