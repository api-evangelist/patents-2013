---

title: Hybrid fibre channel storage with end-to-end storage provisioning and external connectivity in a storage area network environment
abstract: An example method for hybrid Fiber Channel (FC) storage with end-to-end storage provisioning and external connectivity in a storage area network (SAN) environment is provided and includes partitioning a SAN into an internal virtual storage area network (VSAN) for connectivity to an internal storage element located in a first portion of the SAN implemented in a unified computing system (UCS), where a second portion of the SAN is external to the UCS, partitioning the SAN into an external VSAN for connectivity to an external storage element located in the second portion of the SAN, and facilitating communication with the internal storage element over the internal VSAN and with the external storage element over the external VSAN. In one embodiment, border ports on a FI in FC switching mode are configured as N-ports for the external VSAN, and the external storage element is attached to the UCS through an N-port.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09503523&OS=09503523&RS=09503523
owner: CISCO TECHNOLOGY, INC.
number: 09503523
owner_city: San Jose
owner_country: US
publication_date: 20130920
---
This disclosure relates in general to the field of communications and more particularly to a hybrid fibre channel FC storage with end to end storage provisioning and external connectivity in a storage area network SAN environment.

A SAN transfers data between computer systems and storage elements through a specialized high speed network. The SAN consists of a communication infrastructure which provides physical connections. It also includes a management layer which organizes the connections storage elements and computer systems so that data transfer is secure and robust. The SAN allows an any to any connection across the network by using interconnect elements such as switches. The SAN introduces the flexibility of networking to enable one server or many heterogeneous servers to share a common storage utility. The SAN might include many storage devices including disks tapes and optical storage. Additionally the storage utility might be located far from the servers that use it.

An example method for hybrid Fibre Channel FC storage with end to end storage provisioning and external connectivity in a storage area network SAN environment is provided and includes partitioning e.g. subdividing allocating configuring dividing sectioning etc. a SAN into an internal virtual storage area network VSAN for connectivity to an internal storage element located in a first portion of the SAN implemented in a unified computing system UCS where a second portion of the SAN is external to the UCS partitioning the SAN into an external VSAN for connectivity to an external storage element located in the second portion of the SAN and facilitating communication with the internal storage element over the internal VSAN and with the external storage element over the external VSAN. Storage Area Network corresponds to a network connecting various network elements e.g. servers computers etc. with one or more storage devices e.g. hard disk drives tape drives etc. which include devices used exclusively for storing e.g. recording information e.g. data . In one embodiment border ports on a FI in FC switching mode are configured as N ports for the external VSAN and the external storage element is attached to the UCS through an N port.

UCSM may provide a unified embedded management of software and hardware components of UCS across multiple chassis rack servers and virtual machines. UCSM can participate in server provisioning device discovery inventory configuration diagnostics monitoring fault detection auditing and statistics collection among other functionalities . The architecture of UCSM can include multiple software layers with well defined boundaries. Suitable application gateways executing in UCSM may act as hardware abstraction layers between a data management engine DME and managed endpoints e.g. servers storage elements etc. . The DME stores and maintains states for the managed endpoints including configuration information. Configuration changes and settings may be propagated to the endpoints appropriately by UCSM .

One or more external storage array e.g. located in second portion of SAN may be connected to FI through a FC Fibre Channel over Ethernet FCoE switch . UCS may further comprise an internal storage blade e.g. located in the first portion of SAN comprising UCS . Substantially all servers and storage blades attached to FI over appropriate ports become part of the single highly available management domain of UCSM and are considered to be internal to UCS as the term is used in this disclosure. External devices are connected to FI over specific ports that are distinct from the ports used for internal devices. In general each network element has one or more ports that connect it to the SAN. Ports are identified in a number of ways. For example World Wide Name WWN is a globally unique identifier for a port that allows certain applications to access the port. UCSM can discover the WWN of a device or host and assign a port address to the device . As used herein the term storage array and storage blade can be used interchangeably to refer to storage elements configured to store data and permit input output access to the data using suitable protocols such as Small Computer System Interface SCSI Internet Small Computer System Interface iSCSI FC and FCoE.

A user not shown may log into UCS through a service profile to access storage array and storage blade through FI within the broad scope of the embodiments. Service profile which is a logical description of a physical server can contain values for the specific server s property settings including virtual Network Interface Cards vNICs or virtual Host Bus Adaptors vHBAs Media Access Control MAC addresses boot policies firmware policies and other elements as appropriate.

According to various embodiments SAN may be partitioned into a plurality of distinct virtual e.g. logical partitions called virtual SANs VSANs . In particular external storage array may be included in VSAN e.g. VSAN and internal storage blade may be included in VSAN e.g. VSAN . The various storage elements may communicate with FI over appropriate interfaces. For example external storage array may communicate with FI over a N port internal storage blade and service profile may communicate with FI over virtual FC interfaces VFC .

For purposes of illustrating certain example techniques of communication system it is important to understand how typical communications may traverse the network. The following foundational information may be viewed as a basis from which the present disclosure may be properly explained. Such information is offered earnestly for purposes of explanation only and accordingly should not be construed in any way to limit the broad scope of the present disclosure and its potential applications.

A SAN can be used to bypass traditional network bottlenecks. It facilitates direct high speed data transfers between servers and storage devices potentially in any of the following three ways server to storage where the same storage device is accessed serially or concurrently by multiple servers server to server where the SAN facilitates high speed high volume communication between servers storage to storage providing outboard data movement capability that enables data to be moved without server intervention thus freeing up server processor cycles for other activities like application processing e.g. a disk device that backs up its data to a tape device without server intervention or a remote device mirroring across the SAN. A conventional enterprise data center typically uses Ethernet for LAN communications and FC for the SAN. FC is a high speed network technology commonly running at 2 4 8 and 16 gigabit speeds primarily used in SANs. FC includes a topology where devices are connected to FC switches conceptually similar to Ethernet switches. FC traffic generally requires a lossless transport layer because losing a single data packet in the data storage context may be unacceptable.

Blade center deployments and top of rack aggregation devices are being increasingly used in SAN environments. As edge switch e.g. SAN switches on the network edge population grows with increasing network size the number of domain IDs is of concern. Because the total number of domain identifiers IDs which has a one to one correspondence with the number of SAN switches that can exist in a physical fabric in FC SANs is limited to 256 the storage elements have to be judiciously allocated to the limited number of available domains. The SAN switches use the FC domain ID to route frames from a specific initiator e.g. source such as a server or customer end device to any target e.g. destination such as a storage element in the SAN fabric. The 1 byte domain ID allows up to 256 possible addresses according to FC standards. As used herein the term SAN switch refers to a FC or FCoE enabled network switch. SAN switches can also include Fabric Interconnects used in UCS architecture.

Another concern is interoperability with third party switches. In the past different SAN fabric vendors interpreted the FC addressing standard differently. In addition some vendor specific attributes used for switch to switch connectivity in expansion ports E Ports made connection of SAN switches from different vendors challenging leading customers to implement edge switch technology that matched the core director type in the fabric. The E port functions as a fabric expansion port. One E port on a specific switch e.g. FI may be connected to another E port on another switch to create an Inter Switch Link ISL between the two switches. E ports generally carry frames between switches for configuration and fabric management.

In contrast to E ports N ports present FC host interfaces to the SAN switches. When the SAN switch is configured in N port virtualization NPV mode the SAN switch appears as a server to the switch that connects to it over the N port. NPV allows a single N Port to be assigned multiple N Port IDs or FC IDs FCIDs over a single link. The FCIDs can be managed on a FC fabric as unique entities on the same physical host. In a virtual machine environment where many host operating systems or applications are running on a physical host each virtual machine can be managed independently from zoning aliasing and security perspectives. Moreover because the connection from the edge switch to the core switch is treated as an N Port and not an E Port the edge switch shares the domain ID of the core switch as FCIDs are being allocated. The edge NPV enabled switch no longer requires a separate domain ID to receive connectivity to the fabric.

Typically the UCS system in the SAN appears to the outside world e.g. outside the portion of the SAN comprising the UCS as a collection of Network Interface Cards NICs and Fibre Channel Host Bus Adapters FC HBAs . The FI in the UCS architecture serves as a controller for the array of network resources managed by the UCSM executing in the FI. The FI typically runs in the NPV mode allowing the FI to act like a server for example allowing multiple hosts to login to the upstream fabric on the same number of FC uplink ports.

Each internal storage element e.g. disk drive storage partition or volumes etc. may be identified by a unique logical unit number LUN in the UCS. As used herein the term storage element comprises a network element configured to store data substantially exclusively and to permit input output access to the data. Network element encompasses computers network appliances servers routers switches gateways bridges load balancers firewalls processors modules or any other suitable device component element or object operable to exchange information in a network environment. Moreover the network elements may include any suitable hardware software components modules interfaces or objects that facilitate the operations thereof. This may be inclusive of appropriate algorithms and communication protocols that allow for the effective exchange of data or information. Examples of storage elements include storage blades storage arrays direct attached storage hard disk drives magnetic tapes storage partitions storage volumes etc.

In some UCS systems the LUNs may be exposed through FC or FCoE targets with suitable FC zoning configured in the FC fabric. FC zoning is the partitioning of a FC fabric into smaller subsets to restrict interference add security and to simplify management. To restrict server access to storage arrays not allocated to that server the SAN uses zoning. Typically zones are created for each group of servers that access a shared group of storage devices and LUNs. zones define connectivity between HBAs and service profiles. Devices outside a zone are not visible to the devices inside the zone . In the UCS UCSM can be used to configure FC zoning however FC services should be enabled in the FI to facilitate the configuration. FC services include functions like encryption role based access control RBAC redundant array of independent disks RAID redundancy controller services etc. To permit FC services the FI should be placed in a FC switching mode rather than the NPV mode FC zoning is not possible in NPV mode because the FI cannot run fabric services in the NPV mode .

However if the FI is placed in the FC switching mode rather than the NPV mode connectivity problems may arise when the FI is also connected to an upstream external FC FCoE switch e.g. not all switches may be compatible with E ports which are the uplink ports in FC switching mode moreover some switches may not work with the N port which appears as a server port . Moreover in the FC switching mode the UCS border port e.g. external network facing port is configured as an E port and the FC fabric connected to the external storage array is extended to the UCS which can mean that fabric failure domain could also be extended to the UCS. Additionally the UCS may have to be included in the fabric management instead of being treated as a server. Extending the FC fabric can also cause some instability in the SAN. Because the FC switching mode and the NPV mode are mutually exclusive the UCSM cannot configure and provide connectivity for both internal storage blades and external storage arrays from end to end in currently existing systems.

Communication system can resolve the aforementioned issues and potentially others associated with hybrid FC storage and storage provisioning among other features in a SAN environment. In various embodiments SAN may be partitioned into an internal VSAN e.g. for connectivity to an internal storage element e.g. storage blade located in UCS and an external VSAN e.g. for connectivity to an external storage element e.g. storage array located in portion of SAN . Embodiments of communication system may facilitate communication with the internal storage element e.g. storage blade over the internal VSAN e.g. VSAN and with the external storage element e.g. storage array over the external VSAN e.g. VSAN . In a specific example embodiment border ports on FI connecting to the external storage element e.g. storage array may be configured as N port for the external VSAN e.g. even though FI is in FC switching mode. Thus FI may behave as an N Port Virtualizer although it is configured in FC switching mode.

According to some embodiments the internal storage element e.g. storage blade may be attached to UCS through vFC interface of FI configured in FC switching mode. In other embodiments the internal storage element may comprise a direct attached storage attached to UCS through an appliance port of FI configured in FC switching mode. In yet other embodiments the internal storage element may comprise a storage array similar to storage array attached via another SAN switch similar to FC FCoE switch to UCS through an E port of FI configured in FC switching mode. Note that the internal VSAN e.g. and the external VSAN e.g. may be distinguished from each other by distinct VSAN identifiers IDs .

In some embodiments UCSM may receive storage requirements associated with an initiator e.g. service profile and one or more targets e.g. storage blade storage array in SAN . UCSM may generate configuration settings for the initiator and the targets and generate FC services in FI . The configuration settings for the initiator may comprise appropriate NICs at the initiator to facilitate LUNs of the targets appearing as local disks at the initiator. The configuration settings for the targets may comprise suitable configurations for target ports disk groups volumes LUNs and LUN mappings on the corresponding targets. In various embodiments generating the FC services can comprise retrieving initiator to target connectivity requirements for internal storage elements using WWNs of the initiator and the targets computing FC zone sets FC zones and FC zone members without user intervention performing UCSM managed zoning on the internal VSAN e.g. for internal storage connectivity and facilitating fabric based zoning on the external VSAN e.g. for external storage connectivity .

According to various embodiments UCS may be configured in FC switching mode to enable the FC fabric services. The user can configure external VSANs for connectivity to external storage elements and internal VSANs for connectivity to internal storage elements. Internal VSANs may be automatically managed by UCS through UCSM including FC zoning. External VSANs may be managed outside UCS by appropriate owners. Internal VSANs may be distinct from external VSANs using appropriate VSAN IDs or domain IDs in some embodiments . Uplink FC FCoE ports in FI may be configured as FC N ports to enable connectivity with external storage despite FI being in the FC switching mode.

UCS may implement a hybrid mode where uplink FC FCoE N Ports and FC services can coexist in the same UCS instantiation. One FC domain ID may be assigned for external storage and another FC domain ID may be assigned for internal storage elements. The external domain ID may be managed by a storage administrator outside UCS . The internal domain ID may be managed entirely by UCSM and may not be exposed externally. Internal VSANs including internal zoning may not be advertised to any border port of UCS .

External and internal VSANs may be configured such that external VSANs are distinct from internal VSANs. For example the same VSAN ID may not be used for both internal and external storage ensuring that internal FC zoning is separate from external FC zoning. UCS may maintain the separation between external and internal VSANs. In particular a specific VSAN cannot be used for both external and internal storage connectivity. The VSAN ID allocation can be enforced by UCSM which has access to substantially all FC ports VSANs and vHBA configuration in UCS .

FC FCoE border ports can be configured as N ports for external VSANs even though FI is configured in FC switching mode. From the upstream switch e.g. FC FCoE switch perspective UCS may appear the same as if UCS had been configured in FC NPV mode with uplink ports configured as N ports in NPV mode. Internal VSANs may use FC switching capabilities including FC zoning of UCSM . External FC FCoE switches may distribute the FC zoning configuration for the external VSANs and UCSM may assume ownership of the FC zoning for internal VSANs.

In some embodiments the internal VSAN may be limited to one UCS domain. In such embodiments UCS may not use global domain IDs. UCSM may automatically assign an internal domain ID to the internal storage elements. Because internal domain IDs are independent of external domain IDs there may not be any conflict in IDs which can be a concern as only 255 domain IDs are allowed in FC SANs . Further as the assignment is automatic e.g. without user intervention user input and management may be minimized.

In some embodiments the internal VSANs may extend to two or more UCS domains. For example internal storage elements can be shared among two or more UCS instances. In such embodiments an E port on one FI may be connected directly to another E port on another FI in an internal VSAN. In such cases the user may manually assign domain IDs on the internal VSAN such that separate domain IDs are assigned to connected UCS instances using the internal storage elements. The domain ID used for internal storage elements may be different from the domain ID used for external storage.

In various embodiments a host e.g. server represented by service profile can be configured with vHBAs for external and internal storage elements. One vHBA may be configured with an external VSAN and another vHBA on the same host may be configured with an internal VSAN. In some embodiments the host can be configured for external storage connectivity only or internal storage connectivity only or none or both. In a user interface e.g. graphical user interface GUI command line interface CLI or application programming interface API of UCSM the user can specify storage requirements using a simple logical representation. For example the user may specify a logical server requires a 20 GB boot LUN and 50 GB shared LUN. The user may not have to enter other details about FC FCoE and FC zoning.

UCSM may process the high level storage requirements and automatically configure both the initiator and targets. On the host side the initiator can be automatically configured by UCSM with FCoE and or SCSI NICs with vDAS FCoE such that LUNs appear as local disks. SCSI NICs may simplify the deployment as LUNs appear as local devices. On the targets UCSM may automatically configure FC or FCoE target ports disk groups volumes LUNs and LUN mappings. UCSM may also automatically configure the FC fabric services. UCSM may have substantially complete knowledge of the initiator to target connectivity requirements for the internal storage elements. For example UCSM may know the WWNs of the initiator and corresponding targets. Hence UCSM can automatically compute FC zone sets FC zones and FC zone members without any user intervention. Embodiments of communication system can perform UCSM managed zoning on internal VSANs while supporting Fabric based zoning on external VSANs.

In various embodiments higher level management functions may be included to simplify the operational model. Such management functions may include by way of examples and not as limitations FC zoning policies automatic assignment of domain IDs for internal storage elements automatic validation of VSAN IDs e.g. determining that VSAN IDs do not overlap between external and internal storage elements deferred deployment role based access control RBAC etc. In addition the user can decide whether uplink FC FCoE ports are configured as E ports or N ports for each uplink port.

Embodiments of communication system can provide a hybrid storage model in which an integrated computer system e.g. UCS provides access to both external and internal FC FCoE storage elements. The internal storage elements may be provisioned using high level requirements and can be fully automated. Embodiments of communication system can provide substantially complete automation of internal storage provisioning while maintaining existing operational management model for external storage elements.

Turning to the infrastructure of communication system SAN represents a series of points or nodes of interconnected communication paths for receiving and transmitting packets of information that are delivered to communication system . SAN offers a communicative interface between storage elements and or hosts and may include in addition any local area network LAN wireless local area network WLAN metropolitan area network MAN Intranet Extranet WAN virtual private network VPN or any other appropriate architecture or system that facilitates communications in a network environment and can provide lossless service for example similar to or according to FC FCoE or other SAN protocols. SAN may implement any suitable communication protocol for transmitting and receiving data packets within communication system . The architecture of the present disclosure may include a configuration capable of transmission control protocol internet protocol TCP IP communications for the transmission and or reception of packets in a network. The architecture of the present disclosure may also operate in conjunction with a user datagram protocol IP UDP IP or any other suitable protocol where appropriate and based on particular needs.

Turning to is a simplified block diagram illustrating certain details of an example embodiment of communication system . FC zoning may be configured in UCSM for internal VSANs e.g. VSAN . In some embodiments as illustrated herein the NIC e.g. vHBA SCSI vNIC of the initiator host e.g. service profile may be configured exclusively for internal storage connectivity to storage blade . Upon receiving the storage requirements for the initiator to target connectivity UCSM may automatically configure the internal VSAN FC zoning and other parameters and details to implement the storage requirements.

Turning to is a simplified block diagram illustrating certain details of an example embodiment of communication system . Internal storage elements e.g. storage blade may span multiple UCS domains. FI and FI may be connected to each other over E ports and . Service profile may access storage blade over internal VSAN . The user may manually assign separate domain IDs on internal VSAN when the UCS instances are connected over E port on internal VSAN e.g. one UCS instance implemented in FI and another UCS instance implemented in FI using internal storage blade . The domain ID used for internal storage blade may be different from the domain ID used for external storage not shown .

Turning to is a simplified block diagram illustrating certain details of an example embodiment of communication system . Direct attached storage array may be directly connected to FI over an appliance port or equivalent port . Direct attached storage array can be handled as an internal storage element by UCSM according to an embodiment of communication system . UCSM can discover direct attached storage array during configuration. During the discovery UCSM can perform an inventory of the FC FCoE storage ports in direct attached storage array for automatic configuration of FC zoning.

Turning to is a simplified block diagram illustrating certain details of an example embodiment of communication system . In some embodiments external storage elements e.g. storage array can be connected to UCS over E port . Consequently UCS may become part of the switch fabric of FC FCoE switch . Storage array may be treated as part of internal VSAN with appropriate FC zoning domain IDs etc.

Turning to is a simplified block diagram illustrating certain details of an example embodiment of communication system . Network element may execute an instance of UCSM . An example of network element includes FI . A hybrid storage module may execute in network element . Hybrid storage module may include a processor a memory element a user interface and a configuration module . In various embodiments user interface may include any suitable interface configured to permit the user to input storage requirements in a simple logical representation. Examples of user interface include GUI CLI and API. Configuration module may include a domain ID module a VSAN module a port module a FC zoning module an initiator module a target module and a FC fabric module .

Storage requirements may be input into user interface by the user. Storage requirements may be input in a simple logical representation without details about FC zoning FC zone members etc. For example the user may specify a logical server requires a 20 GB boot LUN and 50 GB shared LUN. Based on storage requirements configuration module may generate initiator configuration target configuration and FC fabric services . Domain ID module may determine whether external storage elements and internal storage elements are specified in storage requirements . Accordingly domain ID module may assign an internal domain ID to the internal storage elements and an external domain ID to the external storage elements. VSAN module may generate an internal VSAN for internal storage connectivity and an external VSAN for external storage connectivity. The internal VSAN may be assigned to the internal domain ID and the external VSAN may be assigned to the external domain ID. Port module may configure border ports and other ports accordingly. For example border ports on the external VSAN may be configured to be N ports. FC zoning module may configure FC zones in the internal VSAN appropriately.

Initiator module may generate initiator configuration which can include by way of examples and not as limitations appropriate NICs at the initiator to facilitate LUNs of the one or more targets appearing as local disks at the initiator. Target module may generate target configuration which can include by way of examples and not as limitations configuration of target ports disk groups volumes LUNs and LUN mappings. FC fabric module may retrieve initiator to target connectivity requirements for the internal storage elements using WWNs of the initiator and the targets. In some embodiments the WWNs and other details may be retrieved from a database based on specifications in storage requirements . In other embodiments the WWNs and other details may be specified in storage requirements . FC zoning module may compute FC zone sets FC zones and FC zone members without user intervention and provide the zoning configuration to FC fabric module . FC zoning module may perform UCSM managed zoning on the internal VSAN and facilitate fabric based zoning on the external VSAN.

Turning to is a simplified flow diagram illustrating example operations that may be associated with embodiments of communication system . At UCS may be configured in FC switching mode to enable FC fabric services. At SAN may be partitioned into internal VSANs e.g. and external VSANs e.g. . At internal VSANs may be configured for internal storage connectivity. In one example embodiment at internal VSANs may be limited to one domain UCSM may assign internal domain IDs to internal storage elements.

In another example embodiment at internal VSANs may span more than one domain the user may manually assign internal domain IDs to internal storage elements. At internal VSANs may be automatically managed by UCSM including FC zoning without user intervention. At external VSANs may be configured for external storage connectivity. External VSANs may be managed externally e.g. outside UCS at . At UCSM may ensure that internal VSANs are distinct from external VSANs for example internal VSANs and external VSANs use different VSAN IDs. At uplink FC FCoE ports may be configured as N ports e.g. based on user preferences .

Turning to is a simplified flow diagram illustrating example operations that may be associated with embodiments of communication system . At UCSM may read storage requirements on user interface . At UCSM may configure internal domain IDs internal and external VSANs and ports e.g. N ports vFCs etc. . At UCSM may create FCOE and or SCSI NICs at initiators with VDAS FCoE to facilitate LUNS appearing as local disks. At UCSM may configure FC or FCoE target ports disk groups volumes LUNS and LUN mappings on targets. At UCSM may retrieve initiator to target connectivity requirements for internal storage using WWNs of initiators and corresponding targets. At UCSM may compute FC zone sets FC zones and FC zone members without user intervention. At UCSM may perform UCSM managed zoning on internal VSANs and facilitate fabric based zoning on external VSANs.

Note that in this Specification references to various features e.g. elements structures modules components steps operations characteristics etc. included in one embodiment example embodiment an embodiment another embodiment some embodiments various embodiments other embodiments alternative embodiment and the like are intended to mean that any such features are included in one or more embodiments of the present disclosure but may or may not necessarily be combined in the same embodiments.

In example implementations at least some portions of the activities outlined herein may be implemented in software in for example FI . In some embodiments one or more of these features may be implemented in hardware provided external to these elements or consolidated in any appropriate manner to achieve the intended functionality. The various network elements may include software or reciprocating software that can coordinate in order to achieve the operations as outlined herein. In still other embodiments these elements may include any suitable algorithms hardware software components modules interfaces or objects that facilitate the operations thereof.

Furthermore FI and other SAN switches described and shown herein and or their associated structures may also include suitable interfaces for receiving transmitting and or otherwise communicating data or information in a network environment. Additionally some of the processors and memory elements associated with the various nodes may be removed or otherwise consolidated such that a single processor and a single memory element are responsible for certain activities. In a general sense the arrangements depicted in the FIGURES may be more logical in their representations whereas a physical architecture may include various permutations combinations and or hybrids of these elements. It is imperative to note that countless possible design configurations can be used to achieve the operational objectives outlined here. Accordingly the associated infrastructure has a myriad of substitute arrangements design choices device possibilities hardware configurations software implementations equipment options etc.

In some example embodiments one or more memory elements e.g. memory element can store data used for the operations described herein. This includes the memory element being able to store instructions e.g. software logic code etc. in non transitory computer readable media such that the instructions are executed to carry out the activities described in this Specification. A processor can execute any type of instructions associated with the data to achieve the operations detailed herein in this Specification. In one example processors e.g. processor could transform an element or an article e.g. data from one state or thing to another state or thing.

In another example the activities outlined herein may be implemented with fixed logic or programmable logic e.g. software computer instructions executed by a processor and the elements identified herein could be some type of a programmable processor programmable digital logic e.g. a field programmable gate array FPGA an erasable programmable read only memory EPROM an electrically erasable programmable read only memory EEPROM an ASIC that includes digital logic software code electronic instructions flash memory optical disks CD ROMs DVD ROMs magnetic or optical cards other types of machine readable mediums suitable for storing electronic instructions or any suitable combination thereof.

These devices may further keep information in any suitable type of non transitory computer readable storage medium e.g. random access memory RAM read only memory ROM field programmable gate array FPGA erasable programmable read only memory EPROM electrically erasable programmable ROM EEPROM etc. software hardware or in any other suitable component device element or object where appropriate and based on particular needs. The information being tracked sent received or stored in communication system could be provided in any database register table cache queue control list or storage structure based on particular needs and implementations all of which could be referenced in any suitable timeframe. Any of the memory items discussed herein should be construed as being encompassed within the broad term memory element. Similarly any of the potential processing elements modules and machines described in this Specification should be construed as being encompassed within the broad term processor. 

It is also important to note that the operations and steps described with reference to the preceding FIGURES illustrate only some of the possible scenarios that may be executed by or within the system. Some of these operations may be deleted or removed where appropriate or these steps may be modified or changed considerably without departing from the scope of the discussed concepts. In addition the timing of these operations may be altered considerably and still achieve the results taught in this disclosure. The preceding operational flows have been offered for purposes of example and discussion. Substantial flexibility is provided by the system in that any suitable arrangements chronologies configurations and timing mechanisms may be provided without departing from the teachings of the discussed concepts.

Although the present disclosure has been described in detail with reference to particular arrangements and configurations these example configurations and arrangements may be changed significantly without departing from the scope of the present disclosure. For example although the present disclosure has been described with reference to particular communication exchanges involving certain network access and protocols communication system may be applicable to other exchanges or routing protocols. Moreover although communication system has been illustrated with reference to particular elements and operations that facilitate the communication process these elements and operations may be replaced by any suitable architecture or process that achieves the intended functionality of communication system .

Numerous other changes substitutions variations alterations and modifications may be ascertained to one skilled in the art and it is intended that the present disclosure encompass all such changes substitutions variations alterations and modifications as falling within the scope of the appended claims. In order to assist the United States Patent and Trademark Office USPTO and additionally any readers of any patent issued on this application in interpreting the claims appended hereto Applicant wishes to note that the Applicant a does not intend any of the appended claims to invoke paragraph six 6 of 35 U.S.C. section 112 as it exists on the date of the filing hereof unless the words means for or step for are specifically used in the particular claims and b does not intend by any statement in the specification to limit this disclosure in any way that is not otherwise reflected in the appended claims.

