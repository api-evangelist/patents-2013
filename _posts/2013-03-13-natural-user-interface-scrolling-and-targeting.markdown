---

title: Natural user interface scrolling and targeting
abstract: A user interface is output to a display device. If an element of a human subject is in a first conformation, the user interface scrolls responsive to movement of the element. If the element is in a second conformation, different than the first conformation, objects of the user interface are targeted responsive to movement of the element without scrolling the user interface.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09342230&OS=09342230&RS=09342230
owner: MICROSOFT TECHNOLOGY LICENSING, LLC
number: 09342230
owner_city: Redmond
owner_country: US
publication_date: 20130313
---
User interfaces are typically controlled with keyboards mice track pads and other peripheral devices. Recently natural user interfaces that are controlled by human gestures have been developed to provide a more natural user experience.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter. Furthermore the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure.

A user interface is output to a display device. If an element of a human subject is in a first conformation the user interface scrolls responsive to movement of the element. If the element is in a second conformation different than the first conformation objects of the user interface are targeted responsive to movement of the element without scrolling the user interface.

Display device may be operatively connected to entertainment system via a display output of the entertainment system. For example entertainment system may include an HDMI or other suitable wired or wireless display output. Display device may receive video content from entertainment system and or it may include a separate receiver configured to receive video content directly from a content provider.

The depth camera may be operatively connected to the entertainment system via one or more interfaces. As a non limiting example the entertainment system may include a universal serial bus to which the depth camera may be connected. Depth camera may be used to recognize analyze and or track one or more human subjects and or objects within a physical space such as user . Depth camera may include an infrared light to project infrared light onto the physical space and a depth camera configured to receive infrared light.

Entertainment system may be configured to communicate with one or more remote computing devices not shown in . For example entertainment system may receive video content directly from a broadcaster third party media delivery service or other content provider. Entertainment system may also communicate with one or more remote services via the Internet or another network for example in order to analyze image information received from depth camera .

While the embodiment depicted in shows entertainment system display device and depth camera as separate elements in some embodiments one or more of the elements may be integrated into a common device.

One or more aspects of entertainment system and or display device may be controlled via wireless or wired control devices. For example media content output by entertainment system to display device may be selected based on input received from a remote control device computing device such as a mobile computing device hand held game controller etc. Further in embodiments elaborated below one or more aspects of entertainment system and or display device may be controlled based on natural user input such gesture commands performed by a user and interpreted by entertainment system based on image information received from depth camera .

At shows user from the perspective of a tracking device. The tracking device such as depth camera may include one or more sensors that are configured to observe a human subject such as user .

At shows a schematic representation of the observation data collected by a tracking device such as depth camera . The types of observation data collected will vary depending on the number and types of sensors included in the tracking device. In the illustrated example the tracking device includes a depth camera a visible light e.g. color camera and a microphone.

The depth camera may determine for each pixel of the depth camera the depth of a surface in the observed scene relative to the depth camera. A three dimensional x y z coordinate may be recorded for every pixel of the depth camera. schematically shows the three dimensional x y z coordinates observed for a DPixel v h of a depth camera. Similar three dimensional x y z coordinates may be recorded for every pixel of the depth camera. The three dimensional x y z coordinates for all of the pixels collectively constitute a depth map. The three dimensional x y z coordinates may be determined in any suitable manner without departing from the scope of this disclosure.

The visible light camera may determine for each pixel of the visible light camera the relative light intensity of a surface in the observed scene for one or more light channels e.g. red green blue grayscale etc. . schematically shows the red green blue color values observed for a V LPixel v h of a visible light camera. Red green blue color values may be recorded for every pixel of the visible light camera. The red green blue color values for all of the pixels collectively constitute a digital color image. The red green blue color values may be determined in any suitable manner without departing from the scope of this disclosure.

The depth camera and visible light camera may have the same resolutions although this is not required. Whether the cameras have the same or different resolutions the pixels of the visible light camera may be registered to the pixels of the depth camera. In this way both color and depth information may be determined for each portion of an observed scene by considering the registered pixels from the visible light camera and the depth camera e.g. V LPixel v h and DPixel v h .

One or more microphones may determine directional and or non directional sounds coming from user and or other sources. schematically shows audio data recorded by a microphone. Audio data may be recorded by a microphone of depth camera . Such audio data may be determined in any suitable manner without departing from the scope of this disclosure.

The collected data may take the form of virtually any suitable data structure s including but not limited to one or more matrices that include a three dimensional x y z coordinate for every pixel imaged by the depth camera red green blue color values for every pixel imaged by the visible light camera and or time resolved digital audio data. User may be continuously observed and modeled e.g. at frames per second . Accordingly data may be collected for each such observed frame. The collected data may be made available via one or more Application Programming Interfaces APIs and or further analyzed as described below.

The depth camera entertainment system and or a remote service optionally may analyze the depth map to distinguish human subjects and or other targets that are to be tracked from non target elements in the observed depth map. Each pixel of the depth map may be assigned a user index that identifies that pixel as imaging a particular target or non target element. As an example pixels corresponding to a first user can be assigned a user index equal to one pixels corresponding to a second user can be assigned a user index equal to two and pixels that do not correspond to a target user can be assigned a user index equal to zero. Such user indices may be determined assigned and saved in any suitable manner without departing from the scope of this disclosure.

The depth camera entertainment system and or remote service optionally may further analyze the pixels of the depth map of user in order to determine what part of the user s body each such pixel is likely to image. A variety of different body part assignment techniques can be used to assess which part of the user s body a particular pixel is likely to image. Each pixel of the depth map with an appropriate user index may be assigned a body part index . The body part index may include a discrete identifier confidence value and or body part probability distribution indicating the body part or parts to which that pixel is likely to image. Body part indices may be determined assigned and saved in any suitable manner without departing from the scope of this disclosure.

At shows a schematic representation of a virtual skeleton that serves as a machine readable representation of user . Virtual skeleton includes twenty virtual joints head shoulder center spine hip center right shoulder right elbow right wrist right hand left shoulder left elbow left wrist left hand right hip right knee right ankle right foot left hip left knee left ankle and left foot. This twenty joint virtual skeleton is provided as a nonlimiting example. Virtual skeletons in accordance with the present disclosure may have virtually any number of joints.

The various skeletal joints may correspond to actual joints of user centroids of the user s body parts terminal ends of the user s extremities and or points without a direct anatomical link to the user. Each joint may have at least three degrees of freedom e.g. world space x y z . As such each joint of the virtual skeleton is defined with a three dimensional position. For example a right hand virtual joint is defined with an x coordinate position a y coordinate position and a z coordinate position . The position of the joints may be defined relative to any suitable origin. As one example the depth camera may serve as the origin and all joint positions are defined relative to the depth camera. Joints may be defined with a three dimensional position in any suitable manner without departing from the scope of this disclosure.

A variety of techniques may be used to determine the three dimensional position of each joint. Skeletal fitting techniques may use depth information color information body part information and or prior trained anatomical and kinetic information to deduce one or more skeleton s that closely model a human subject. As one non limiting example the above described body part indices may be used to find a three dimensional position of each skeletal joint.

A joint orientation may be used to further define one or more of the virtual joints. Whereas joint positions may describe the position of joints and virtual bones that span between joints joint orientations may describe the orientation of such joints and virtual bones at their respective positions. As an example the orientation of a wrist joint may be used to describe if a hand located at a given position is facing up or down.

Joint orientations may be encoded for example in one or more normalized three dimensional orientation vector s . The orientation vector s may provide the orientation of a joint relative to the depth camera or another reference e.g. another joint . Furthermore the orientation vector s may be defined in terms of a world space coordinate system or another suitable coordinate system e.g. the coordinate system of another joint . Joint orientations also may be encoded via other means. As non limiting examples quaternions and or Euler angles may be used to encode joint orientations.

A joint conformation may be used to further define one or more of the virtual joints. Whereas joint positions may describe the position of joints and virtual bones that span between joints and joint orientations may describe the orientation of such joints and virtual bones at their respective positions joint conformations may describe other aspects of the joints virtual bones and or body parts associated therewith. As an example the conformation of a hand joint may be used to describe whether a corresponding hand is in an open grip or closed grip conformation.

Joint conformations may be encoded for example as a one dimensional number or vector. As a non limiting example a joint conformation number or variable may be assigned to a joint.

While the above description discusses potential conformations for a virtual hand joint it is to be understood that other joints may be assigned one or more conformation variables suitable for the different possible conformations of those particular joints. For example a head joint could be assigned a value based on the head joint being in either an open mouthed or closed mouthed conformation.

In some examples a conformation variable may be assigned to different possible conformations of a plurality of joints. For example two hand joints in close proximity may adopt a plurality of possible conformations e.g. hands clasped clapping etc and each conformation variable may be assigned a value.

Joint positions orientations conformations and or other information may be encoded in any suitable data structure s . Furthermore the position orientation and or other parameters associated with any particular joint may be made available via one or more APIs.

As seen in virtual skeleton may optionally include a plurality of virtual bones e.g. a left forearm bone . The various skeletal bones may extend from one skeletal joint to another and may correspond to actual bones limbs or portions of bones and or limbs of the user. The joint orientations discussed herein may be applied to these bones. For example an elbow orientation may be used to define a forearm orientation.

The virtual skeleton may be used to recognize one or more gestures performed by user . As a non limiting example one or more gestures performed by user may be used to trigger scrolling of the user interface responsive to movements of the user and the virtual skeleton may be analyzed over one or more frames to determine if the one or more gestures have been performed. For example a conformation of a hand joint of the virtual skeleton may be determined and virtual pointer may be moved based on the position of the hand joint. It is to be understood however that a virtual skeleton may be used for additional and or alternative purposes without departing from the scope of this disclosure.

At method may include outputting a user interface to a display device. For example user interface may be output to display device as depicted in . The display device may be separate from the entertainment device or may be integrated in the entertainment device. The scrollable user interface may only present a subset of information in a viewable content area at one time and thus a user may scroll through the presented information to reach a desired portion of the scrollable user interface.

At method may include determining if an element of a human subject is in a first or second conformation. For example method may include determining if hand of user is in a closed grip or open grip conformation. Determining if an element of a human subject is in a first or second conformation may include receiving a virtual skeleton of a human subject modeled from a depth image from a depth camera the virtual skeleton including a virtual hand joint modeling a three dimensional position of a hand of the human subject. For example depth camera may take depth image of user . Depth image may be used to generate virtual skeleton including virtual hand joint modeling a three dimensional position of hand of user .

Further virtual hand joint may then be used to determine if hand of user is in a closed grip conformation or an open grip conformation. Determining if hand is in a closed grip conformation or an open grip conformation may further include analyzing a portion of the depth infrared and or color images corresponding to a position of the virtual hand joint. In other words the position of the hand joint may be used to locate which portions of other image data should be analyzed. Such portions may be analyzed using machine learning techniques to determine the conformation.

In some examples a virtual skeleton may include a suitable number of hand and or finger joints to facilitate a determination of whether the hand of the human subject is in a closed grip conformation or open grip conformation.

In some examples determining if the hand of the human subject is in a closed grip conformation or open grip conformation further includes determining the conformation of the hand by recognizing a transition of the hand from one conformation to another conformation. For example it may be difficult to conclusively determine if a user s hand is in an open grip conformation when the hand is moving rapidly but it may be possible to determine that the user s hand has transitioned from a closed grip conformation to a conformation that is not a closed grip conformation.

If the element of the human subject is determined to be in a first conformation method may proceed to . If the element of the human subject is determined to be in a second conformation method may proceed to .

At method may include entering a scrolling mode. For example method may enter a scrolling mode when a hand of the user is in a closed grip conformation. At method may include scrolling the user interface responsive to movement of the element. For example if the hand of the user is in a closed grip conformation the user interface may scroll responsive to movement of the virtual hand joint modeling a three dimensional position of the hand. Scrolling the user interface in this manner is discussed further below and with regards to .

In some examples scrolling the user interface responsive to movement of the element includes scrolling the user interface in three dimensions responsive to movement of the element in three dimensions. For example the user interface may be a graphic representation of a landscape or other three dimensional game playing area. The user by moving the user s hand while in a closed grip conformation may be able to pan scroll and zoom across and through the landscape depicted on the display device in three dimensions.

It should be understood that the user interface may be scrolled in an unrestricted manner. In other words a user may scroll the user interface to any desired position. The user interface need not scroll by increments the same width as the display. Further such scrolling may be presented as a visually smooth movement of the user interface across the display. Additionally one or more sections of the user interface may be weighted so that the user interface may be scrolled with an unrestricted flow but at the same time naturally favors stopping on weighted sections of particular interest. Weighted sections may be used to decrease the likelihood that a user will stop scrolling on a less useful section of the user interface.

At method may include identifying a transition from the first conformation while the element is moving. For example the user may be holding a hand in a closed grip conformation then move the hand and release the grip into an open grip conformation or otherwise transition out of the closed grip conformation. Other examples for transitions from the first conformation are discussed further below and with regards to . If such a transition is identified method may include entering an inertial scrolling mode as shown at . An example sub method for inertial scrolling a user interface is shown by method in . If no such transition is identified at method may include maintaining the user interface in a scrolling mode as shown at .

Returning to if the element of the human subject is determined to be in a second conformation method may proceed to . At method may include entering a targeting mode. In some examples the targeting mode may allow the user to target objects in the user interface that appear on the display device. In some embodiments if the element is in a second conformation different than the first conformation the user may target objects of the user interface responsive to movement of the element without scrolling the user interface. For example while right hand joint is in an open grip conformation the user may target objects of the user interface responsive to movement of the virtual hand joint without scrolling the user interface. In some examples the user interface includes a cursor that moves as a targeting function of a position of the element when targeting.

At method may include moving a cursor as a targeting function of a position of the element. In other words the user interface may include a cursor that changes position on the user interface to reflect movement of the user s hand while the hand is in an open grip conformation. The cursor may be a display object such as virtual pointer as shown in . In some examples the cursor may not be shown on the display device. For example the appearance of display objects on the user interface may be altered to reflect the positioning of an unseen cursor on or near a display object.

At method may include determining whether a cursor is positioned over an object on the user interface. As described above in some examples the appearance of the object may be altered to reflect the positioning of a cursor over the object. In some examples display objects may be configured to be selectable by the user when the cursor is positioned over the object. Some display objects may not be selectable even if the cursor is positioned over the object. As nonlimiting examples selectable objects may include icons buttons items on a list display objects involved in gameplay and or virtually any other displayable object.

If the cursor is not positioned over a selectable object on the user interface method may include maintaining a targeting mode as shown at . If the cursor is determined to be positioned over an object on the user interface e.g. within a threshold proximity method may include determining if there is motion of the element towards the display device greater than a threshold distance as shown at . If such motion is detected at method may include selecting the targeted object as shown at . If movement of the element towards the display device greater than a threshold distance is not detected method may include maintaining a targeting mode as shown at . In other words when a cursor is over a selectable object on the user interface the user may select the object by moving an element in the second conformation towards the display device. For example the user may extend a hand in an open grip conformation towards the display device to select an object targeted by a cursor. The user may also decline to select a targeted object and move the cursor to target a different object displayed on the user interface. The targeting and selecting of objects on the user interface is described further below and with regard to .

As introduced above a scrollable user interface may be controlled by the movements of a human target via the skeletal modeling of depth maps. For example schematically show virtual skeleton modeling different gestures of a human subject at different moments in time e.g. time t time t and time t . As discussed above virtual skeleton can be derived from depth information acquired from a depth camera observing the human target. While virtual skeleton is illustrated as a jointed stick figure it is to be understood that the virtual skeleton may be represented by any suitable machine readable data structure. For example the joints illustrated as dots in may be represented by positional coordinates and or other machine readable information. As such a logic subsystem of a computing system may receive the virtual skeleton i.e. data structure s representing the virtual skeleton in machine readable form and process the position and or other attributes of one or more joints. In this way the skeletal position movement and therefore the conformations of the modeled human subject may be interpreted as different gestured controls for controlling aspects of the user interface.

At time tof virtual skeleton moves the position of right hand joint upwards from the position at t while the human subject maintains right hand in a closed grip conformation. As discussed with regard to moving an element while in a first conformation e.g. moving a hand in a closed grip conformation may cause user interface to scroll as a function of the movement of the element. shows the user interface scrolling upwards as a function of the movement of the element in the first conformation as indicated by the upwards arrow and the upwards scrolling of list . It is to be understood that an upward hand movement may cause scrolling in the opposite direction without departing from the scope of this disclosure.

As indicated at time tof virtual skeleton moves the position of right hand joint downwards from the position at t while the human subject maintains right hand in a closed grip conformation. shows the user interface scrolling downwards as a function of the movement of the element in the first conformation as indicated by the downwards arrow and the downwards scrolling of list . If the position of right hand joint at time tis equivalent to the position of right hand joint at time t as shown in user interface may display the same information as at time t as shown by the visible content of list in . However scrolling may be based on hand velocity and or other parameters than absolute hand position. Further it is to be understood that a downward hand movement may cause scrolling in the opposite direction without departing from the scope of this disclosure.

In some embodiments the absolute movement of a hand joint away from a default hand joint position is used as the basis for determining the amount of scrolling of the user interface. In some embodiments the relative position of a hand joint to a shoulder joint or other reference joint is used as the basis for determining the amount of scrolling. Translation of hand joint movement to user interface scrolling does not have to be in a 1 1 ratio. In some embodiments an observed hand joint position may be translated into a scrolling distance using a linear or nonlinear translation function. In other words a certain amount of hand movement may cause different amounts of scrolling depending on the position of the hand velocity of the hand acceleration of the hand and or parameters of the scrollable interface.

While depict scrolling in one dimension responsive to hand joint moving in an up and down direction it is to be understood that scrolling of the user interface may occur in two or three dimensions responsive to hand joint moving in two or three dimensions. In some examples the scrolling of user interface may be constrained to scrolling in one or two dimensions and thus movement of hand joint may be constrained or projected to one or two dimensions. For example list may be scrollable in one dimension. As virtual skeleton moves right hand in an up or down direction e.g. along a y axis right hand may actually be moving closer to or further from the display device e.g. along a z axis along with movement of the right arm joint. The movement of the hand along the z axis may be de emphasized or constrained to a projection along the y axis. In some examples the user interface may be scrollable in two dimensions for example a grid of objects scrollable along the x and y axes. Thus the movement along the z axis may be de emphasized or constrained to a projection along the x and y axes.

At time tof virtual skeleton moves the position of right hand joint upwards from the position at t while the human subject maintains right hand in an open grip conformation. As discussed with regard to moving an element while in a second conformation e.g. moving a hand in an open grip conformation may cause virtual pointer to move on user interface as a function of the movement of the element. shows virtual pointer moving upwards as a function of the movement of the element in the second conformation at time t. As discussed with regard to while right hand joint is in an open conformation the user may target objects on the user interface for example object . Positioning virtual pointer on or near object may cause object to change in appearance as an indication that object may be selected by the user. Virtual pointer is depicted as an arrow but may take on other appearances or may be configured to not be displayed on user interface . While in targeting mode the user interface may not scroll as a function of movement of hand joint as depicted by list remaining stationary on user interface .

In some embodiments the absolute movement of a hand joint away from a default hand joint position is used as the basis for determining the targeting of virtual pointer on the user interface. In some embodiments the relative position of a hand joint to a shoulder joint or other reference joint is used as the basis for determining the amount of cursor movement. Translation of hand joint movement to cursor targeting does not have to be in a 1 1 ratio. In some embodiments an observed hand joint position may be translated into a virtual pointer movement distance using a linear or nonlinear translation function. In other words a certain amount of hand movement may cause different amounts of virtual pointer movement depending on the position of the hand and or parameters of the user interface.

While depict targeting in one dimension reflective of hand joint moving in an up and down direction it is to be understood that targeting of virtual pointer may occur in two or three dimensions reflective of hand joint moving in three dimensions. In some examples the targeting of virtual pointer may be constrained to targeting in one or two dimensions and thus movement of hand joint may be constrained or projected to one or two dimensions.

At time tof virtual skeleton moves the position of right hand joint forwards from the position at t while the human subject maintains right hand in an open grip conformation. depicts object being selected by the user in response to the movement of right hand joint towards display device .

In some examples an object of the user interface is selected responsive to movement of the element over a threshold distance in the direction of the display device when the cursor engages the object. In some examples an object of the user interface may be selected responsive to movement of the element over a certain velocity or acceleration. In some examples an object of the user interface may be selected responsive to movement of the element along a distance compared to distances along other axes. In some examples the targeting of an object may result in constraining or projecting movement of an element along an axis or axes. In some examples a user may select an object by moving an element towards a display device and may de select the object by moving the element away from the display device.

At method may include entering an inertial scrolling mode. As a sub routine of method may be the equivalent of . As described with regards to an inertial scrolling mode may be entered upon detecting a transition of an element from a first conformation while the element is moving.

At method may include inertial scrolling the user interface as a scrolling function of a velocity of the element. In other words if the user interface depicts a virtual globe the user may fling the globe around a virtual axis with a velocity. In some examples this may include inertial scrolling the user interface as a scrolling function of a velocity of the element during a time period that includes a transition of the element from the first conformation. In some examples where the element is a hand of a human subject method may include inertial scrolling the user interface as a scrolling function of a velocity of the hand of the human subject during a time period that includes a transition of the hand from the first conformation.

At method may include determining if there is movement of the element in the current scroll direction greater than a threshold. In other words if the user interface is scrolling in an upwards direction method may include determining if there is movement of the element in an upwards direction greater than a threshold. If movement of the element in the current scroll direction greater than a threshold is detected method may proceed to .

At method may include increasing the scrolling speed as a function of the velocity of the element. In the example where the user interface is a virtual globe while the virtual globe is rotating around a virtual axis the user may increase the rotation speed of the globe by flinging the globe in the direction of rotation. In some examples increasing the scrolling speed as a function of the velocity of the element may include inertial scrolling the user interface with increased speed in a current scroll direction responsive to subsequent movements of the element in a direction that is substantially similar to the current scroll direction. For example the user interface may inertial scroll with increased speed in a current scroll direction responsive to subsequent movements of a hand of a human subject over a threshold distance in a direction that is substantially similar to the current scroll direction.

If movement of the element in the current scroll direction greater than a threshold is not detected method may proceed to . At method may include determining if there is motion of the element towards the display device greater than a threshold. If motion of the element towards the display device greater than a threshold is not detected method may maintain the inertial scrolling mode as shown at . If motion of the element towards the display device greater than a threshold is detected method may proceed to .

At method may include decreasing the scrolling speed as a function of the distance the element moved. In the example where the user interface is a virtual globe while the globe is rotating around a virtual axis the user may slow rotation of the globe by reaching out and touching the globe. In some examples this may include reducing the speed of inertial scrolling as a function of a distance the element moves along an axis perpendicular to the display device. For example the speed of inertial scrolling may decrease as a function of a distance a hand of a human subject moves along an axis perpendicular to the display device. In other words while the user interface is inertial scrolling the human subject may slow or stop the inertial scrolling by extending a hand towards the display screen. In some examples the user may not select objects on the screen by extending a hand towards the display screen during inertial scrolling. Rather the user may stop the inertial scrolling when the user identifies an object of interest and then select the object in a subsequent targeting and selection mode. Examples for entering inertial scrolling mode are discussed further below and with regards to . Examples for increasing inertial scrolling speed are discussed further below and with regards to . Examples for decreasing inertial scrolling speed are discussed further below and with regards to .

At time tof virtual skeleton moves the position of right hand joint upwards from the position of the right hand joint at t while the human subject maintains right hand in a closed grip conformation. As discussed with regard to moving an element while in a first conformation e.g. moving a hand in a closed grip conformation may cause user interface to scroll as a function of the movement of the element. shows the user interface scrolling upwards as a function of the movement of the element in the first conformation as indicated by the upwards arrow and the upwards scrolling of list .

At time tof virtual skeleton moves the position of right hand joint upwards from the position of the right hand joint at t while the human subject transitions right hand from a closed grip conformation to an open grip conformation. As described with regards to detection of a transition from the first conformation to a conformation that is not the first conformation while the element is moving may cause inertial scrolling of user interface . shows the user interface inertial scrolling upwards as a function of the velocity of the element as indicated by upwards arrow and the inertial scrolling of list . In other words the user interface may scroll with inertia even while the hand stops movement upwards responsive to the previous movement and conformational transition of the hand of the human subject.

In some examples inertial scrolling of the user interface may include generating an inertial vector as an inertial function of movement of the element during a time period that includes a transition of the element from the first conformation and inertial scrolling the user interface in the direction of the inertial vector. A storage machine may be configured to store instructions to generate an inertial vector as an inertial function of a direction of movement of the hand of the human subject during a time period that includes a transition of the hand from the first conformation and inertial scroll the user interface in the direction of the inertial vector.

In other words a vector may be generated based on the movement of the element to determine the direction of the inertial scrolling. A logic machine may record the movements of virtual hand joint . Upon transition of hand joint from a closed grip to an open grip conformation the logic machine may analyze data regarding the position of hand joint retrospectively from the moment in time hand joint began moving e.g. tof until at least the moment in time when hand joint transitions out of the closed grip conformation e.g. tof . An inertial vector may then be generated as a function of the movement of hand joint and user interface may inertial scroll in that direction.

In some examples the data regarding movement of the element may be filtered to generate an inertial vector in a direction corresponding to a dominant direction of movement by the element. For example a human subject may move a hand along the z axis during the transition from the closed grip conformation despite moving the hand primarily along the y axis before the transition from the closed grip conformation. The data may be filtered to de emphasize the data surrounding the transition.

While depict inertial scrolling in one dimension reflective of hand joint moving in an up and down direction it is to be understood that inertial scrolling of the user interface may occur in three dimensions responsive to hand joint moving in three dimensions. In some examples the inertial scrolling of user interface may be constrained to scrolling in one or two dimensions and the movement of hand joint may be projected to one or two dimensions. For example list may be scrollable in one dimension. As virtual skeleton moves right hand in an up or down direction e.g. along a y axis right hand may actually be moving closer to or further from the display device e.g. along a z axis along with movement of the right arm joint. The data along the z axis may be de emphasized constrained or projected onto the y axis in the generation of the inertial vector.

In some examples the user interface may be inertial scrollable in two dimensions for example a grid of objects inertial scrollable along the x and y axes. Thus the movement along the z axis may be disregarded or constrained to a projection along the x and y axes. In some examples the inertial vector may be a one dimensional vector with coordinates in a three dimensional space.

In some examples inertial scrolling may be initiated with the inertial scrolling occurring at a predetermined speed. In other examples the initial inertial scrolling speed may be a function of a magnitude of a velocity of the element. As described for the generation of an inertial vector data collected regarding movement of the element may be filtered to determine a magnitude of the velocity of the element. For example a human subject may move a hand at a constant speed before the transition from the closed grip conformation and then move the hand with decreased speed during and or after the transition. The data may be filtered to de emphasize the data surrounding the transition. The resulting magnitude may then be used to generate an inertial scrolling speed.

In some examples the speed of the inertial scrolling of the user interface may decrease over time after initiation for example with a virtual coefficient of friction. The virtual coefficient of friction may be set to a value equal to 0 in which case the speed of the inertial scrolling would remain constant until additional input is received.

As described above it may be difficult to conclusively determine if a user s hand is in an open grip conformation when the hand is moving rapidly but it may be possible to determine that the user s hand has transitioned from a closed grip conformation to a conformation that is not a closed grip conformation. In some examples determining if the hand of the human subject has transitioned from the closed grip conformation may include determining that the hand of the human subject is no longer in a closed grip conformation. In some examples recognizing the transition of the hand from one conformation to another conformation includes determining that the hand has reached a virtual boundary. In some embodiments the virtual boundary may include positions where the hand is no longer visible by a depth camera.

In some embodiments where the user interface includes a cursor that moves as a targeting function of a position of the element the transition of the element may be characterized by the cursor moving to a boundary of the user interface. In some embodiments where the user interface includes a cursor that moves as a targeting function of a position of a hand of a human subject the transition of the hand from one conformation to another may be characterized by the cursor moving to a boundary of the user interface.

At time tof virtual skeleton moves the position of right hand joint upwards from the position at time t while the human subject maintains right hand in a closed grip conformation. As discussed with regard to and moving an element while in a first conformation e.g. moving a hand in a closed grip conformation may cause user interface to scroll as a function of the movement of the element. shows the user interface scrolling upwards as a function of the movement of the element in the first conformation as indicated by upwards arrow and the upwards scrolling of list . In this example virtual pointer is also depicted as moving as a function of the movement of the element in the first conformation. Virtual pointer is shown moving upwards along with list .

As indicated at time tof virtual skeleton moves the position of right hand joint upwards from the position at t while the human subject transitions right hand while remaining in a closed grip conformation. In this example the movement of right hand joint causes virtual pointer to reach a boundary of display device . The action of the cursor reaching a boundary is recognized as being representative of hand joint transitioning out of a closed grip conformation. This may cause the user interface to scroll with inertia responsive to the movement and transition of the hand of the human subject as depicted in by arrow and the inertial scrolling of list .

As described above with regards to when the user interface is inertial scrolling the user may increase the speed of the inertial scrolling by moving an element in a current scrolling direction. In other words the user may move a hand in the same direction as the user interface is inertial scrolling in order to increase the speed of the inertial scrolling.

As indicated at time tof virtual skeleton moves the position of right hand joint upwards from the position at t while the human subject maintains right hand in a closed grip conformation. As the movement of right hand joint is in substantially the same direction as the inertial scrolling direction the speed of inertial scrolling may thus be increased as a function of the velocity of right hand joint as indicated by arrow and the increased speed of inertial scrolling of list as depicted in .

Increasing the speed of inertial scrolling by moving an element in the direction of the inertial scrolling may not be dependent on the conformation of the element. As indicated at time tof the human subject transitions right hand to an open grip conformation but this transition does not impact the speed of inertial scrolling as indicated by arrow and list shown in . At time tof virtual skeleton moves the position of right hand joint upwards from the position at t while the human subject maintains right hand in an open grip conformation. As the movement of right hand joint is in substantially the same direction as the inertial scrolling direction the speed of inertial scrolling may thus be increased as a function of the velocity of right hand joint as indicated by arrow and the increased speed of inertial scrolling of list shown in .

Determining the inertial scrolling speed as a function of the velocity of movement of an element may include collecting data regarding movement of the element from the point in time where the element begins moving to the point in time where the element stops moving and further include filtering the collected data in order to determine the direction and magnitude of the velocity of the element. If the direction of the velocity of the element is substantially similar to the direction of inertial scrolling the speed of the inertial scrolling user interface may be increased proportionate to the magnitude of the velocity of the movement of the element.

Although depicted as inertial scrolling in one dimension in user interface may inertial scroll in two or three dimensions responsive to movement of the element in two or three dimensions. In some embodiments when an inertial scroll direction has been determined subsequent movement by the user may be constrained or projected onto an axis of the inertial scroll direction. As depicted in movement of a hand in an upwards motion may further include movement of the hand along the z axis as the hand joint may not maintain a single axis of movement when the user moves an arm joint. This motion along the z axis may be constrained projected onto an axis of the inertial scroll direction or may be de emphasized when determining the velocity of the hand.

As described above with regards to when the user interface is inertial scrolling the user may decrease the speed of the inertial scrolling by moving an element along an axis perpendicular to the display device. In other words the user may move a hand toward the display device in order to decrease the speed of the inertial scrolling.

At time tof virtual skeleton has moved right hand joint into a position partially extended away from the body of the virtual skeleton. In an example where the user is facing the display device right hand joint is moved along an axis perpendicular to display device . This action may cause the speed of inertial scrolling to decrease as depicted by arrow and list as shown in . In some embodiments the hand joint may need to extend at least a threshold distance to decrease the inertial scrolling.

At time tof virtual skeleton moves right hand joint into a position fully extended away from the body of the virtual skeleton. In an example where the user is facing the display device right hand joint is moved along an axis perpendicular to display device . This action may cause the inertial scrolling to stop as depicted by list as shown in . In some embodiments the speed of inertial scrolling may decrease proportionate to the distance traveled by the hand joint towards the display device. In some embodiments the hand joint may need to extend at least a threshold distance to stop the inertial scrolling. In some examples the speed of inertial scrolling may decrease proportionate to the velocity or acceleration of the hand towards the display device.

In some embodiments the methods and processes described above may be tied to a computing system of one or more computing devices. In particular such methods and processes may be implemented as a computer application program or service an application programming interface API a library and or other computer program product.

Computing system includes a logic machine and a storage machine . Computing system may optionally include a display subsystem input subsystem communication subsystem and or other components not shown in .

Logic machine includes one or more physical devices configured to execute instructions. For example the logic machine may be configured to execute instructions that are part of one or more applications services programs routines libraries objects components data structures or other logical constructs. Such instructions may be implemented to perform a task implement a data type transform the state of one or more components achieve a technical effect or otherwise arrive at a desired result.

The logic machine may include one or more processors configured to execute software instructions. Additionally or alternatively the logic machine may include one or more hardware or firmware logic machines configured to execute hardware or firmware instructions. Processors of the logic machine may be single core or multi core and the instructions executed thereon may be configured for sequential parallel and or distributed processing. Individual components of the logic machine optionally may be distributed among two or more separate devices which may be remotely located and or configured for coordinated processing. Aspects of the logic machine may be virtualized and executed by remotely accessible networked computing devices configured in a cloud computing configuration.

Storage machine includes one or more physical devices configured to hold instructions executable by the logic machine to implement the methods and processes described herein. When such methods and processes are implemented the state of storage machine may be transformed e.g. to hold different data.

Storage machine may include removable and or built in devices. Storage machine may include optical memory e.g. CD DVD HD DVD Blu Ray Disc etc. semiconductor memory e.g. RAM EPROM EEPROM etc. and or magnetic memory e.g. hard disk drive floppy disk drive tape drive MRAM etc. among others. Storage machine may include volatile nonvolatile dynamic static read write read only random access sequential access location addressable file addressable and or content addressable devices.

It will be appreciated that storage machine includes one or more physical devices. However aspects of the instructions described herein alternatively may be propagated by a communication medium e.g. an electromagnetic signal an optical signal etc. that is not held by a physical device for a finite duration.

Aspects of logic machine and storage machine may be integrated together into one or more hardware logic components. Such hardware logic components may include field programmable gate arrays FPGAs program and application specific integrated circuits PASIC ASICs program and application specific standard products PSSP ASSPs system on a chip SOC and complex programmable logic devices CPLDs for example.

The terms module program and engine may be used to describe an aspect of computing system implemented to perform a particular function. In some cases a module program or engine may be instantiated via logic machine executing instructions held by storage machine . It will be understood that different modules programs and or engines may be instantiated from the same application service code block object library routine API function etc. Likewise the same module program and or engine may be instantiated by different applications services code blocks objects routines APIs functions etc. The terms module program and engine may encompass individual or groups of executable files data files libraries drivers scripts database records etc.

When included display subsystem may be used to present a visual representation of data held by storage machine . This visual representation may take the form of a graphical user interface GUI . As the herein described methods and processes change the data held by the storage machine and thus transform the state of the storage machine the state of display subsystem may likewise be transformed to visually represent changes in the underlying data. Display subsystem may include one or more display devices utilizing virtually any type of technology. Such display devices may be combined with logic machine and or storage machine in a shared enclosure or such display devices may be peripheral display devices.

When included input subsystem may comprise or interface with one or more user input devices such as a keyboard mouse touch screen or game controller. In some embodiments the input subsystem may comprise or interface with selected natural user input NUI componentry. Such componentry may be integrated or peripheral and the transduction and or processing of input actions may be handled on or off board. Example NUI componentry may include a microphone for speech and or voice recognition an infrared color stereoscopic and or depth camera for machine vision and or gesture recognition a head tracker eye tracker accelerometer and or gyroscope for motion detection and or intent recognition as well as electric field sensing componentry for assessing brain activity.

When included communication subsystem may be configured to communicatively couple computing system with one or more other computing devices. Communication subsystem may include wired and or wireless communication devices compatible with one or more different communication protocols. As non limiting examples the communication subsystem may be configured for communication via a wireless telephone network or a wired or wireless local or wide area network. In some embodiments the communication subsystem may allow computing system to send and or receive messages to and or from other devices via a network such as the Internet.

It will be understood that the configurations and or approaches described herein are exemplary in nature and that these specific embodiments or examples are not to be considered in a limiting sense because numerous variations are possible. The specific routines or methods described herein may represent one or more of any number of processing strategies. As such various acts illustrated and or described may be performed in the sequence illustrated and or described in other sequences in parallel or omitted. Likewise the order of the above described processes may be changed.

The subject matter of the present disclosure includes all novel and nonobvious combinations and subcombinations of the various processes systems and configurations and other features functions acts and or properties disclosed herein as well as any and all equivalents thereof.

