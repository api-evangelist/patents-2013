---

title: Method for scheduling with deadline constraints, in particular in Linux, carried out in user space
abstract: A method for scheduling tasks with deadline constraints, based on a model of independent periodic tasks and carried out in the user space by means of API POSIX is provided.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09582325&OS=09582325&RS=09582325
owner: Centre National De La Recherche Scientfique
number: 09582325
owner_city: Paris
owner_country: FR
publication_date: 20131105
---
The invention concerns a method allowing the performance of monoprocessor multiprocessor or multikernel scheduling of tasks with deadline constraints. A scheduler of tasks with deadline constraints means that each task has a termination time that it must not exceed. The scheduling performed supports execution on a single processor or a plurality of processors. It is performed under Linux or any other operating system that supports the POSIX standard and more particularly its extension POSIX.1c but is not integrated in the kernel this is because it works in user space.

User space is a mode of operation for user applications as opposed to kernel space which is a supervisor mode of operation that has advanced functionalities having all rights particularly the one for accessing all the resources of a microprocessor. Manipulation of resources is nevertheless made possible in user space by what are known as API Application Programming Interface functionalities which themselves rely on the use of peripheral pilots. The development of a solution in user space makes possible and facilitates development of schedulers with time constraints in relation to direct integration into the kernel of the operating system which is made very difficult by its complexity. Another advantage is that it brings increased stability because an execution error in user space does not have serious consequences for the integrity of the rest of the system.

In order to perform scheduling in user space the method of the invention relies particularly on the mechanisms defined by the POSIX Portable Operating System Interface X the X expressing the Unix heritage standard and its extension POSIX.1c and notably on the POSIX task structure the POSIX threads pthreads and the POSIX task management APIs. It should be remembered that a process is a program instance that is being executed a task is a division of a process and a thread is the accomplishment of a task under Linux by means of the POSIX APIs a thread or task cannot exist without a process but there may be a plurality of threads or tasks per process. In this regard reference may be made to the work by Robert Love O Reilly 2007.

Linux is a system in which the management of time is called shared time as opposed to realtime management. In this type of management several factors linked to the operation of the system such as resource sharing input output management interrupts influence on virtual memory etc. give rise to temporal uncertainties about the execution times of the tasks. Therefore it is not possible to guarantee a hard realtime solution that is to say one in which the execution of each task is accomplished within strict and inviolable time limits. The realization of the proposed scheduler is nevertheless suitable for what are known as soft realtime systems or systems with deadline constraints which accept variations in the processing of the data of the order of no more than one second. This is the case with a large number of realtime applications for example multimedia applications.

As it is not natively a realtime system several technical solutions are however possible for making the Linux kernel compatible with realtime constraints. The most common solution involves associating therewith an auxiliary realtime kernel having a genuine realtime scheduler RT Linux RTAI XENOMAI . This auxiliary realtime kernel has priority and processes realtime tasks directly. Other non realtime tasks are delegated to the standard Linux kernel which is considered to be a background task or job with lower priority. However the development of a specific scheduler integrated into the kernel is very difficult because it requires in depth knowledge of the kernel and involves heavy kernel development with all the complexity and instability that that brings. Moreover realtime Linux extensions are supported by a smaller number of platforms. The proposed solution allows implementation in user space that is therefore simpler more stable and can be applied to any platform supporting a standard Linux kernel. This approach does not allow hard realtime constraints to be guaranteed but considerably simplifies the development of a scheduler while being suitable for soft realtime applications the great majority of applications .

A method for scheduling according to the invention is particularly suitable for the implementation of low consumption oriented scheduling policies for example an EDF earliest deadline first policy which is a particular type of scheduling with deadline constraints with dynamic voltage and frequency scaling DVFS in order to minimize consumption which is important in onboard applications. In this regard see the following articles 

There are numerous works on low consumption scheduling but these essentially remain theoretical and are practically never integrated into an operating system owing to the complexity of this work. The proposed solution since implemented entirely in user space greatly facilitates this integration.

An object of the invention is therefore a method for scheduling tasks with deadline constraints which is based on a model of independent periodic tasks and performed in user space in which 

The method can be executed under an operating system that is compatible with a POSIX.1c standard which may particularly be a Linux system. In this case 

Another object of the invention is a computer program product for implementing a method for scheduling as claimed in one of the preceding claims.

The principle of implementation for a scheduler in user space according to an embodiment of the invention which is based on a Linux operating system is illustrated in . An application is seen as a set of tasks to be scheduled . The scheduler controls the execution of these tasks by means of three specific functions reference preempt Task resume Task and run on Task CPU . These functions the names of which are arbitrary and given solely by way of nonlimiting example allow the preemption of a task the resumption of a task and the allocation of a task to a processor respectively. They rely on the use of functionalities provided by the Linux APIs reference allowing control of the tasks and of the processors from user space.

To operate the scheduler needs to have specific information that comes from the model of tasks that is used. For a model of periodic and independent tasks this is at least information relating to the deadline of each task the period thereof and the state of activity thereof other time information can likewise be provided worst case execution time WCET subsequent deadline that is to say current time plus deadline to avoid confusion with the subsequent deadline the deadline is sometimes called absolute deadline etc. In the case of a multiprocessor platform the scheduler likewise requires an information item that is indicative of the processor to which the task is assigned. Moreover each task is associated with a POSIX thread pthread which likewise needs to be known to the scheduler.

Other information likely to be necessary to the scheduler is a MUTEX associated with the task a condition associated with the task a Linux identifier for the task.

The best case execution time BCET and the actual execution time AET is information that without being indispensible for performing scheduling is very useful for development because it allows particularly the AET the execution time of a task to be fixed the execution time of a task normally varies from one execution to the other . This facilitates verification of the scheduling by allowing it to be compared with simulation results with tasks having the same parameters and overall exactly the same execution times .

In the standard Linux POSIX task structure pthread this information is not accessible in user space. For this reason the implementation of the invention requires extension of this task structure through the creation of a type personalized using a data structure.

The application model is made up of periodic and independent tasks. A model of periodic tasks refers to the specification of homogeneous sets of jobs that are repeated at periodic intervals a job is a particular form of task and more precisely an independent task whose execution is strictly independent of the results of the other jobs . A model of independent tasks refers to a set of tasks for which the execution of one task is not subordinate to the situation of another. This model supports synchronous and asynchronous task execution and can be used in a large number of real applications that need to comply with time constraints. Knowledge of the characteristics and time constraints of the tasks deadline period worst execution time etc. is therefore necessary in order to use this type of scheduling. As explained above this information is added to a specific structure that extends the standard task type under Linux.

The example of refers particularly to a video application in which each of the four tasks corresponds to particular processing of a frame. These four tasks are therefore repeated every 40 ms period in order to be able to process 25 frames per second.

One or more queues are used to store the tasks. At least one queue is necessary in order to produce scheduling with deadline constraints in order to store the list of tasks that are ready. A priority is likewise associated with each task in order to define the order of execution thereof. The priority criterion is dependent on the scheduling policy. For the EDF Earliest Deadline First policy for example the priority criterion is the proximity of the deadline the task having the closest deadline has the highest priority. The queue of tasks that are ready is generally sorted in order of decreasing priority. The priority criterion under consideration here is different than the priority criteria that are used in native Linux schedulers shared time . The scheduler is called at precise instants called scheduling instants which are triggered by the tasks themselves at moments that are characteristic of their execution. These moments are called task events reference in corresponding by way of example to their activation onActivate their end of execution onBlock their reactivation onUnBlock or their termination onTerminate these names are arbitrary and given solely by way of nonlimiting example.

The events are triggered at the appropriate moments of the execution of an application task by calls to onActivate onBlock onUnBlock onTerminate functions which are inserted into its code. Each task event updates the fields of the task structure for the task in question state subsequent deadline period etc. and calls the scheduler if necessary. Whether or not the scheduler is called by a task event depends on the scheduling policy. For an EDF policy for example the scheduler is called on onActivate onBlock and onUnBlock events.

The onActivate event corresponds to the creation of a task. It marks the change from the unexisting state to ready . For reasons of synchronization to the creation of tasks explained at . the onActivate event increments at the end of execution thereof a rendez vous variable and then prompts the task to await an activation condition from the scheduler for example by calling the pthread cond wait function from the Pthread Condition Variable API of the POSIX.1c standard.

The onBlock event is triggered when a task terminates its actual execution AET . It then changes to the waiting state in which it begins to wait until it reaches its period. When it reaches its period the task triggers the onUnBlock event which makes it change to the ready state then prompts it to await a resumption condition for example by calling the POSIX function pthread cond wait. This condition will be signaled by the scheduler by means of the pthread cond broadcast function which itself belongs to the Pthread Condition Variable API of the POSIX.1c standard.

At any moment in its execution a task can be preempted by another task that has become a higher priority in the case of an EDF algorithm because its deadline has become closer to that of the task that is being executed . The preemption of a task makes it change from the running state to ready . Conversely the task that resumes following a preemption changes from the ready state to the running state.

To avoid interbiocking phenomena the scheduler is not called directly by task events but rather is called by means of a pthread . In other words the function that performs a task event onActivate onBlock onUnBlock onTerminate calls another function call scheduler name given by way of nonlimiting example which creates a scheduling pthread in order to execute the scheduler.

On each call the scheduler performs the following actions by means of a main function select  name given by way of nonlimiting example setup of a queue for the tasks that are ready sorting of the queue in order of decreasing priority highest priority task at the start of the list preemption of nonpriority tasks that are being executed allocation of eligible tasks to free processors and starting of eligible tasks by sending a resumption condition. Determination of eligible tasks requires the scheduler to know the state of all existing tasks. The list of input actions in this case corresponds to EDF scheduling but can be modified and in particular enriched in order to implement other scheduling policies.

Owing to the multitask and multiprocessor execution a plurality of instances of the scheduler could be called so as to be executed at the same time. In order to prevent such an eventuality a mutex from MUTual Exclusion device is used in order to protect some shared variables like the queue of tasks that are ready. The mutex is locked at the beginning of execution of the scheduler and then released at the end of the call to the scheduler. This method guarantees that only a single instance of the scheduler is executed at a time which allows no risk modification of the shared variables.

The first execution of the scheduler takes place just after the tasks are created. In order to guarantee control of the tasks by the scheduler synchronization needs to be performed in order to be sure that all the tasks have indeed finished being created before executing the scheduler and that the tasks do not start being executed until after the scheduler has given them the order. To this end first of all a global variable of rendez vous type is initialized to 0 before the tasks are created. Next all the tasks are created and put onto the first processor of the system. Right at the beginning of their creation in other words upon execution of the onActivate event each tasks increments the rendez vous variable and then immediately suspends its execution by starting to await a resumption condition use of the POSIX function pthread cond wait . When the value of the rendez vous variable is equal to the number of tasks the scheduler can be executed. During this execution the scheduler resumes execution of the eligible tasks by signaling to them their resumption condition pthread cond broadcast function .

In order to control the execution of application tasks the scheduler requires the use of two specific functions for suspension or resumption of a task called preempt Task and resume Task respectively in reference these names being given solely by way of nonlimiting example. The preempt Task function is based on the use of the Signal API of the POSIX standard. In order to preempt a task the SIGUSRI signal is sent to the corresponding pthread POSIX function pthread kill . The associated signal manager sigusr prompts the task to await a resumption condition pthread cond wait upon reception of the signal. There is a resumption condition for each task of the application. The resume Task function signals the appropriate resumption condition to the task in question in order to resume its execution. Said function is therefore strictly equivalent to calling the Linux function pthread cond broadcast in fact the creation of a specific function is justified essentially for reasons of legibility of the code. This mechanism makes use of the fact that in practice the signal manager SIGUSR is executed by the Linux pthread that receives the signal. Thus the signal manager can identify the pthread to be suspended necessary for sending the wait condition to the pthread concerned by the pthread cond wait function for example by comparing its own process identifier tid with that of all the pthreads of the application. It should be noted that the scheduler does not use the task preemption and resumption mechanisms provided by the kernel since these are not accessible in user space.

In order to control the execution of application tasks in a multiprocessor platform the scheduler requires an explicit function allowing the execution of a task to be fixed on a given processor. Although there is an API for control of the Linux pthread by the processors of a multiprocessor platform CPU affinity there is not a specific function for allocating a task to a processor. The performance of this function indicated by run on in reference this name being given by way of nonlimiting example assigns the processor CPU to the execution of the task Task by relying on the Linux API CPU affinity . This is based on the specification of the processor CPU solely in the affinity mask of the task Task this affinity mask then being assigned to the task by the pthread setaffinity np function of the CPU affinity API . Next in order to guarantee that the scheduler never executes more than one task per processor all other application tasks are prevented from using the processor CPU. It is moreover verified that a task is never assigned to more than a single processor.

A specific function change freq name given solely by way of nonlimiting example is used in the case of a scheduler using DVFS techniques in order to control the dynamic voltage and frequency scaling of the processors reference . This function uses a Linux API called CPUFreq which allows the frequency of each processor to be changed by means of the virtual file system sysfs . By way of example it suffices to write the desired frequency to a file the file sys devices system cpu cpu0 cpufreq scaling setspeed under Linux in order to modify the frequency of the processor . The scheduler can use the change freq function in order to allow modification of the frequency by writing it to the file system. It is moreover necessary to use the Userspace governor first which is the only DVFS mode under Linux that allows a user or an application to change the processor frequency at will. Userspace is one of the five Unix DVFS governors and its name indicates that the frequency and therefore the voltage can be modified at will by the user.

A possible low consumption scheduling technique involves making use of the dynamic slack time provided by a task following execution thereof in order to adjust the operating frequency and voltage of the processor s DVFS so as to save power while providing time guarantees. By way of example the scheduler of the invention has been used for performing scheduling of DSF Deterministic Stretch to Fit type in which the frequency of the processors and therefore the voltage which is dependent thereon is recalculated for each scheduling event using the actual execution time AET for accomplished tasks and the worst execution time WCET for the others in order to allocate the turnaround time of the previous task to the next task which allows the operating frequency of the processor in question to be reduced.

The annex contains source code written in C language for a computer program allowing the implementation of a method for scheduling according to the invention using the DSF DVFS technique described above. This code is given solely by way of nonlimiting example.

The DSF Scheduler directory contains the kernel of the code of the DSF scheduler. It notably comprises the prototype file N scheduler.h which contains the definition of the necessary types task structure processor structure etc. the global variables list of tasks list of processors parameters of the scheduler etc. and the prototypes of the exported functions.

The DSF Scheduler.c file then describes the main functions of the scheduler. The most important is the select function this describing the whole scheduling process performed at each scheduling instant. The task events onActivate onBlock onUnBlock and on Terminate that trigger the scheduling instants are likewise described in this file. The scheduling function relies on a slow down function that computes and applies the change of frequency. Finally the distinctive features of the scheduler when the application starts have required the development of a specific function start sched. This function is executed just a single time right at the beginning of the application being launched and is based largely on the code of the main scheduler select  . For all other scheduling instants it is the select function that is called via the call scheduler function. The Makefile file is used for compilation.

The application to be scheduled is described in the file N application.c. This is also the file containing the hand of the program. This performs the various initializations that are required creates the POSIX tasks launches the scheduler and synchronizes everything. It should be stated here that an application is seen as a set of tasks each having temporal characteristics such as the worst case execution time WCET the period the deadlines etc. The tasks used perform simple processing multiplication of the elements in a table of integers up to a certain execution time AET which is described by the function usertask actualexec and then put themselves into a mode for awaiting their reactivation usertask sleepexec when they reach their period.

Finally in order to keep a structure for the code that is as clear as possible by keeping only the essential functions of the scheduler in the N Scheduler.c file the secondary functions that are required have themselves been grouped into another in a file N utils.c. By way of example this contains functions for initialization task preemption allocation of tasks to processors sorting display etc.

The pm drivers directory contains the functions from the lowest level on which the scheduler relies. These are more precisely the prototype files intel kernel i5 m520.h pm typedef.h and the file pm cpufreq.c.

The intel kernel i5 m520.h prototype file contains the description of the target platform whose name it bears. This is information about the number of processors on the platform and about the possible frequencies for each of the kernels which are represented in the form of states. The types used e.g. the processor states are defined in the pm typedef.h file.

The pm cpufreq.c file contains particularly the functions that allow the dynamic effective change of frequency on the execution platform taking the Linux API CPUfreq as a basis. The dynamic voltage and frequency scaling is managed by policies called governors under Linux which are known per se. More precisely in order to be able to change the processor frequency frequencies it is first of all necessary to use a governor called userspace. A first set governor function is present for this. Next the interaction with the Linux frequency scaling pilot is effected by means of exchange files located in sys devices system cpu cpuX cpufreq where X represents the number of the processor in question. The open cpufreq and close cpufreq functions allow the scaling setspeed exchange file to be opened and closed. The frequency scaling is carried out by the function CPU state manager apply state.

The invention has been described in detail with reference to a particular embodiment multiprocessor system under Linux EDF scheduling policy with dynamic voltage and frequency scaling DVFS . These are not essential limitations however.

Thus the method for scheduling of the invention can be applied to a monoprocessor platform and or is able not to implement consumption reduction mechanisms.

Implementation of the invention is particularly easy under LINUX because the APIs described above are available. However a scheduler according to the invention can be provided under another operating system that is compatible with the POSIX standard and more precisely with the IEEE POSIX 1.c or equivalently POSIX 1003.1c standard provided that it supports an equivalent of the CPU affinity for multiprocessor applications and CPUfreq for applications making use of the dynamic voltage and frequency scheduling APIs.

Other scheduling policies with deadline constraints can be implemented such as Rate Monotonic RM Deadline Monotonic DM or Latest Laxity First LLF policies for example.

Equally various techniques for reducing consumption can be implemented. By way of nonlimiting example it is possible to cite the DSF Deterministic Stretch to Fit technique making use of dynamic voltage and frequency scaling this is the technique used in the example that has just been described and AsDPM Assertive Dynamic Power Mangement making use of processor rest modes.

It should be noted that the reproduction of a task scheduler under deadline constraints in user space as described above can be implemented only under certain conditions that are dependent on the operating system OS . By way of example these conditions are as follows. The method according to the invention must support the notion of preemptive scheduling in user space. The possibility of explicit preemption of a task from user space is a feature that cannot be implemented in all Os. The task model needs to integrate deadline and state constraint attributes. The possibility of extension of the task model with these attributes which are accessible in user mode is dependent on the OS.

It should likewise be noted that the implementation of low consumption oriented scheduling policies in user space can be effected only under certain conditions that are dependent on the OS. The possibility of temporarily putting aside or dynamically scaling the frequency of a processor in user space cannot be implemented in all operating systems notably under Windows.

It should likewise be noted that the method according to the invention does not require any explicit intervention of the supervisor mode and is based exclusively on mechanisms in user mode in the form of APIs.

The standard Linux POSIX task structure pthread which is described in the particular embodiment of can be generalized as a standard POSIX task structure thread under an OS that allows it to be implemented. Like the standard Linux POSIX task structure pthread of the implementation of the invention for this generalized task structure requires them to be extended by parameters that are accessible in user space. Equally like the application model described for the information concerning the characteristics and time constraints of the tasks is added to a specific structure that extends the standard POSIX task type by parameters that are made accessible in user space.

It should be noted that the method for preemption of POSIX tasks in user space that is implemented in the invention does not exist in what are known as consumer OSs that is to say those devoid of realtime constraints. According to the invention and generally in order to control the execution of application tasks the scheduler needs to perform a functionality of explicit preemption of POSIX tasks in user space which is based on the use of two specific functions for the suspension and resumption of a task.

