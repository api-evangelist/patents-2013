---

title: Systems and methods for a de-duplication cache
abstract: A de-duplication is configured to cache data for access by a plurality of different storage clients, such as virtual machines. A virtual machine may comprise a virtual machine de-duplication module configured to identify data for admission into the de-duplication cache. Data admitted into the de-duplication cache may be accessible by two or more storage clients. Metadata pertaining to the contents of the de-duplication cache may be persisted and/or transferred with respective storage clients such that the storage clients may access the contents of the de-duplication cache after rebooting, being power cycled, and/or being transferred between hosts.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09116812&OS=09116812&RS=09116812
owner: Intelligent Intellectual Property Holdings 2 LLC
number: 09116812
owner_city: Wilmington
owner_country: US
publication_date: 20130125
---
This application claims priority to U.S. Provisional Patent Application No. 61 591 822 entitled Systems and Methods for a De Duplication Cache to Vikram Joshi et al. filed Jan. 27 2012 which is hereby incorporated by reference.

A host may comprise a plurality of virtual machines deployed in a Virtual Desktop Infrastructure VDI environment. The virtual machines may use similar operating systems and applications. These commonalities may present I O performance and redundancy challenges. For example when a large number of the virtual machines boot the virtual machines may accesses a similar files stored on similar storage systems which can result in creating a boot storm that can overwhelm I O resources. Moreover the I O may result in caching multiple duplicative copies of the files in cache storage resulting in reduced cache availability and wasted cache resources.

Disclosed herein are systems apparatus and methods for efficient I O using cache storage. The cache storage may comprise various memory devices such as flash memory devices or RAM random access memory that may or may not be block oriented. The systems and methods disclosed herein do not differentiate between Flash memory RAM or other types of memory and further envision new types of memory developed in the future that will utilize various embodiments disclosed herein.

In some embodiments a de duplication cache may be configured to de duplicate cache data for a plurality of virtual machines. As used herein de duplicating cache data refers to caching data for use by two or more different storage clients. De duplication may reduce the I O overhead to primary storage resources and improve the efficiency of the cache.

Disclosed herein are embodiments of an apparatus comprising a de duplication cache manager configured to admit data into a de duplication cache in response to admission requests from one or more of a plurality of virtual machines. The apparatus may further comprise a cache interface module configured to provide access to a single copy of data admitted into the de duplication cache to two or more of the virtual machines.

In some embodiments the apparatus may comprise a virtual machine de duplication module configured to identify files suitable for admission into the de duplication cache. Suitable files may include files that are infrequently modified files that are never modified are read only files that are accessed multiples times by a single virtual machine and or are likely to be accessed by multiple virtual machines within a given time period and so on. The de duplication cache may be configured to index data admitted into the de duplication cache using context independent identifiers which may be derived from the data itself. Accordingly the context independent identifiers may be consistent across the virtual machines.

The de duplication cache manager may be configured to index data admitted into the de duplication cache using context independent identifiers. The de duplication cache manager may be configured to determine whether data has already been admitted by use of the context independent identifiers. The de duplication cache manager may be configured to verify a match between context independent data identifiers by a byte by byte comparison of data corresponding to the data identifiers. The de duplication cache manager may be configured to restrict access to data admitted into the de duplication cache to virtual machines that have previously requested admission of the data.

The apparatus may further comprise a cache retention module configured to retain data admitted into the de duplication cache by a virtual machine in response to determining that the virtual machine is being transferred to a different host. The apparatus may further comprise a cache transfer module configured to transfer de duplication cache tags of a virtual machine to another host.

Disclosed herein are embodiments of an apparatus comprising a monitoring module configured to monitor input output I O operations of a virtual machine a de duplication cache policy module configured to identify I O requests pertaining to data that satisfy a de duplication policy for admission into a de duplication cache and a virtual machine de duplication module configured to service the identified I O requests using a de duplication cache shared by a plurality of virtual machines.

The virtual machine de duplication module may be configured to admit a file into the de duplication cache by deriving a data identifier of the file from data of the file and providing the data and the data identifier to the de duplication cache. The virtual machine de duplication cache module may be configured to admit the file into the de duplication cache in response to an I O request pertaining to the file and the operations to admit the file into the de duplication cache may be performed on a separate thread from a thread performing operations to satisfy the I O request pertaining to the file.

In some embodiments the virtual machine de duplication module is configured associate names of files admitted into the de duplication cache with respective data identifiers of the files to indicate that the files have been admitted into the de duplication cache. The virtual machine de duplication module may be further configured to request data of a file from the de duplication cache by use of the data identifier associated with the file. The virtual machine de duplication module may be configured to indicate that a file is no longer admitted in response to detecting an I O request to modify the file.

The virtual machine de duplication module may be configured to store associations between file names of files admitted into the de duplication cache and data identifiers of the files on a persistent storage medium and to load the stored associations in response to one or more of restarting the virtual machine rebooting the virtual machine power cycling the virtual machine and migrating the virtual machine to a different host.

The apparatus may further comprise file selection criteria configured to identify files that are suitable for admission into the de duplication cache based on one or more of file name file extension file path file volume file attribute and or hint e.g. de duplication configuration information received via a configuration interface I O request or the like .

Disclosed herein are methods comprising caching a file in a de duplication cache shared by multiple virtual machines in response to a request to admit the file into the de duplication cache from a first one of the virtual machines associating the cached file with a context free identifier providing data of the cached file from the de duplication cache to a second one of the virtual machines in response to a request from the second virtual machine wherein the request comprises the context free identifier of the cached file.

The method may further comprise determining whether a file has already been admitted into the de duplication cache by comparing a context free identifier derived from data of the file with context free identifiers of files that have been admitted into the de duplication cache. In some embodiments the method further comprises comparing data of the file to existing file data in the de duplication cache in response to the context free identifier of the file matching a context free identifier of a file that is already admitted into the de duplication cache and providing a modified context free identifier for the file data to the first virtual machine in response to the file data differing from file data of the file already admitted into the de duplication cache.

The disclosed methods may further comprise verifying that the second virtual machine is authorized to access the cached file which may comprise determining that the second virtual machine has access to the file on a primary storage.

Disclosed herein are embodiments of machine readable storage media comprising instructions configured to cause a computing device to perform a method the method comprising identifying a file for admission into a de duplication cache in response to monitoring file I O requests within a storage stack of a virtual machine determining whether the file has been admitted into a de duplication cache and servicing the I O request by use of the de duplication cache in response to determining that the file has been admitted wherein servicing the I O request comprises requesting data of the file from the de duplication cache using a universal identifier of the file.

In response to determining that the file has not been admitted into the de duplication cache the method may further comprise calculating the universal identifier of the file based on data of the file and generating a request to admit the file into the de duplication cache the request comprising the data of the file and the universal identifier.

In some embodiments the method further comprises associating a name of the file with the universal identifier in a de duplication index of the virtual machine in response to receiving an indication that the file has been admitted into the de duplication cache. The disclosed methods may further include identifying an I O request pertaining to the file determining that the file was admitted into the de duplication cache by use of the de duplication index and requesting data of the file from the de duplication cache by use of the universal identifier associated with the name of the file in the de duplication index. The entry of a file in the de duplication index may be invalidated and or removed in response to detecting an I O request to modify the corresponding file.

The disclosed methods may comprise persisting a snapshot of the de duplication index populating the de duplication index using the snapshot subsequent to one or more of rebooting the virtual machine power cycling the virtual machine and transferring the virtual machine to a different host.

The host may comprise one or more computing devices capable of hosting the virtual machines A N. The host may comprise for example one or more processors memory devices persistent storage devices communication devices e.g. I O interfaces network interfaces human machine interfaces etc. and so on. Although depicts three virtual machines A N the disclosure is not limited in this regard the virtualized environment could include any number of hosts comprising any number of virtual machines A N.

The virtualization kernel may be configured to manage the operation of the virtual machines A N operating on the host as well as other components and services provided by the host . For example the virtualization kernel may be configured to handle various I O operations associated with a primary storage system or other I O devices. The primary storage system may be shared among the multiple virtual machines A N and or multiple hosts. The primary storage system may include but is not limited to one or more disk drives and or other storage devices one or more storage arrays such as an Array of Inexpensive Disks RAID Just a Bunch of Disks JBOD or the like network attached storage such as a network area storage NAS a storage area network SAN or the like.

The host may further comprise a virtual machine cache which may be configured to provide caching services to the virtual machines A N deployed on the host computing device . The virtual machine cache may comprise a cache provisioner module and cache storage . The cache storage may comprise one or more storage devices including but not limited solid state memory devices Random Access Memory RAM devices volatile memory battery backed RAM or the like. As used herein a solid state memory device refers to a non volatile persistent memory that can be repeatedly erased and reprogrammed. Accordingly a solid state memory device may comprise a solid state storage device and or solid state storage drive SSD e.g. a Flash storage device . The cache provisioner module may be configured to provision resources of the cache storage to the virtual machines A N which may comprise dynamically provisioning cache resources and or I O operations IOPS to the virtual machines A N. The cache provisioner module may be configured to provide for sharing resources of the cache storage between multiple virtual machines A N.

In some embodiments one or more of the virtual machines A N may comprise an I O driver A N and a cache management system CMS A N. The I O driver A N may be configured to intercept I O operations of the associated virtual machine A N and to direct the I O operations to the corresponding CMS A N for processing selected I O operations may be serviced using the virtual machine cache . In some embodiments and as depicted in the I O driver may be in close proximity to the source of I O operations of the virtual machines A N e.g. the I O driver A N may be deployed within the virtual machine A N and as such does not have to access the virtualization kernel and or cross a virtual machine boundary to access information pertaining to virtual machine A N I O operations . In some embodiments the I O driver A N may comprise and or be implemented as a device driver e.g. a device driver of respective guest operating systems of the virtual machines A N . The I O driver A N may comprise a generic component that forms part of an operating system and a device specific component. The I O driver A N may leverage I O Application Programming Interfaces APIs published by the guest operating system e.g. may be in the I O path of the virtual machines A N . The I O driver A N may comprise a filter driver A N configured to monitor I O request packets IRP of a Microsoft Windows operating system. The disclosure is not limited in this regard however and may be applied to any suitable I O framework of any operating system e.g. Unix LINUX OSX Solaris or the like and or virtualization environment .

In some embodiments the virtual machines A N may be configured to be transferred and or relocated between hosts . The systems apparatus and methods disclosed herein may provide for transferring a cache operating state between hosts . As used herein cache operating state or cache state refers to a current working state of a cache which may include but is not limited to cache metadata such as cache admission information e.g. cache tags access metrics and so on cache data e.g. the contents of a cache storage and the like. Transferring a cache operating state may therefore comprise transferring cache metadata and or cache data. The virtualization kernel or other virtualization layer may be configured to prevent virtual machines that reference local resources of the host such as local disk storage or the like from being transferred. Accordingly virtual machines A N may be configured to access the virtual machine cache as if the virtual machine cache were a shared storage resource and or in a way that does not prevent the virtual machines A N from being transferred between hosts .

One or more of the virtual machines A N may comprise a CMS A N which may be configured to manage cache resources provisioned to the virtual machine A N. The CMS A N may be configured to maintain cache metadata such as cache tags to represent data that has been admitted into the virtual machine cache . The cache tags may be maintained within memory resources of the virtual machine A N such that the cache tags are transferred with the virtual machine between hosts A N . In other embodiments and as depicted in cache tags B N of one or more of the virtual machines B N may be maintained within the virtualization kernel e.g. within the virtual machine cache .

The cache provisioner module may be configured to dynamically provision cache resources to the virtual machines A N. Cache allocation information associated with a particular virtual machine e.g. Virtual Machine A may be communicated to the corresponding virtual machine CMS A N via the I O driver and or using another communication mechanism.

In some embodiments the cache provisioner module is configured to maintain mappings between virtual machines A N and respective cache storage locations allocated to the virtual machines A N. The mappings may be used to secure cache data of the virtual machines A N e.g. by limiting access to the virtual machine A N mapped to the cached data and or to provide for retaining and or transferring cache data of one or more virtual machines A N transferred from the host to other remote hosts.

The CMS A N may be configured to maintain cache metadata which may comprise cache tags A N in accordance with the cache storage that has been allocated to the virtual machine A N. As used herein a cache tag refers to an association between an identifier and a cache resource e.g. a page or other cache storage location in the cache storage . Accordingly the cache tags A N may represent cache resources that have been allocated to a particular virtual machine A N by the cache provisioner module . As used herein an identifier of a cache tag A N refers to an identifier used by the virtual machine A N to reference data that has been or will be stored in the cache storage . A cache tag identifier may include but is not limited to an address e.g. a memory address physical storage address logical block address etc. such as an address on the primary storage system a name e.g. file name directory name volume name etc. a logical identifier a reference or the like.

In some embodiments the cache tags A N represent a working set a virtual machine A N cache. As used herein a working set of cache tags A N refers to a set of cache tags corresponding to cache data that has been admitted and or retained in the cache storage by the CMS A N through inter alia the application of one or more cache policies such as cache admission policies cache retention and or eviction policies e.g. cache aging metadata cache steal metadata least recently used LRU hotness and or coldness and so on cache profiling information file and or application level knowledge and the like. Accordingly the working set of cache tags A N may represent the set of cache data that provides optimal I O performance for the virtual machine A N under certain operating conditions.

In some embodiments the CMS A N may be configured to preserve a snapshot of cache state which may comprise persisting the cache tags A N and or related cache metadata in a non volatile storage medium such as the primary storage system persistent cache storage device e.g. cache storage or the like. A snapshot may comprise all or a subset of the cache metadata of the CMS A N e.g. cache state which may include but is not limited to the cache tags A N related cache metadata such as access metrics and so on. In some embodiments a snapshot may further comprise pinning data in the cache storage which may cause data referenced by the one or more cache tags to be retained in the cache storage . Alternatively the snapshot may reference only the data identifiers e.g. cache tags A N and may allow the underlying cache data to be removed and or evicted from the cache storage .

The CMS A N may be configured to load a snapshot from persistent storage and to use the snapshot to populate the cache tags A N. A snapshot may be loaded as part of an initialization operation e.g. cache warm up and or in response to configuration and or user preference. For example the CMS A N may be configured to load different snapshots that are optimized for particular application s and or service s . Loading a snapshot may further comprise requesting cache storage from the cache provisioner module as disclosed herein. In some embodiments the CMS A N may load a subset of a snapshot if the virtual machine A N cannot allocate sufficient cache space for the full snapshot.

The CMS A N may be further configured to retain the cache tags A N in response to relocating and or transferring the virtual machine A N to another host. Retaining the cache tags may comprise maintaining the cache tags A N in the memory of the virtual machine A N and or not invalidating the cache tags A N. Retaining the cache tags A N may further comprise requesting cache storage from the cache provisioner module of the destination host in accordance with the retained cache tags A N and or selectively adding and or removing cache tags A N in response to being allocated more or less cache storage on the destination host. In some embodiments the CMS A N may retain the cache tags A N despite the fact that the cache data referenced by the cache tags A N does not exist in the cache storage of the new destination host. As disclosed in further detail below the virtual machine cache may be configured to populate the cache storage with cache data from a previous host of the virtual machine A N e.g. via a network transfer and or from a shared primary storage system.

The cache storage may comprise one or more non volatile storage resources such as a solid state storage device and or a portion thereof. The virtual machine cache may logically partition the cache storage into multiple chunks. As used herein a chunk refers to an arbitrarily sized portion of cache storage capacity the cache storage may be divided into any number of chunks having any size. Each cache chunk may comprise a plurality of pages each of which may comprise one or more storage units e.g. sectors . In a particular embodiment each chunk may comprise 256 MB megabytes of storage capacity a 2 TB terabyte cache storage device divided into 256 MB chunks may comprise 8384 chunks.

The cache provisioner module may provision cache resources to virtual machines A N based upon inter alia the cache requirements of the virtual machines A N availability of cache resources and so on. The cache resources allocated to a particular virtual machine A N may change over time in accordance with the operating conditions of the virtual machine A N. The cache provisioner module may provision cache chunks to a virtual machine A N which may determine the cache capacity of that virtual machine A N. For example if two 256 MB chunks are assigned to a specific virtual machine A N that virtual machine s cache capacity is 512 MB. The cache provisioner module may be further configured to provision cache resources to other entities such as the de duplication cache e.g. cache resources .

In some embodiments cache resources are provisioned using a thin provisioning approach. A thin provisioning approach may be used where the virtual machines A N are configured to operate with fixed size storage resources and or changes to the reported size of a storage resource would result in error condition s . The cache storage device may be represented within the virtual machines A N as a fixed size resource e.g. through a virtual disk or other I O interface such as the I O driver of . The cache provisioner module may dynamically allocate cache resources to the virtual machines A N in accordance with changing I O conditions. Regardless of the number of cache chunks actually allocated to a particular virtual machine A N the cache storage interface may appear to remain at a constant fixed size which may allow for dynamic cache reallocation without causing error conditions within the virtual machines A N.

The virtual machine cache may comprise a cache interface module configured to manage access to the virtual machine cache . The cache interface module may provide one or more communication links and or interfaces through which the virtual machine cache may service I O requests for the virtual machines A N communicate configuration and or allocation information and so on. In some embodiments the cache interface module is configured to communicate with the virtual machines A N through a virtual disk and or using Virtual Logical Unit Number VLUN driver . The VLUN driver may be further configured to provide a communication link between the virtual machines A N and the virtual machine cache .

The VLUN driver may be further configured to provide for representing dynamically provisioned cache resources as fixed size VLUN disks A N within the virtual machines A N. In an exemplary embodiment the cache storage may comprise two terabytes 2 TB of storage capacity. The cache provisioner may allocate four gigabytes 4 GB to the virtual machine A one gigabyte 1 GB to virtual machine B three gigabytes 3 GB to virtual machine N and so on. As disclosed above other virtual machines B N on the host may be allocated different amounts of cache resources in accordance with the I O requirements of the virtual machines B N and or the availability of cache resources. The VLUN driver and VLUN disk A N may be configured to represent the entire capacity of the cache device to the virtual machines A N e.g. 2 TB regardless of the actual allocation to the particular virtual machine A N by the cache provisioner module . In addition and as disclosed in further detail below the physical cache resources A N allocated to the virtual machine A may be discontiguous within the physical address space of the cache storage . The virtual machine cache may further comprise a map module configured to present the cache resources allocated to the virtual machines A N as a contiguous range of virtual cache addresses regardless of the location of the underlying physical storage resources.

In some embodiments the CMS A N comprises an I O driver A N configured to monitor and or filter I O requests of the corresponding virtual machine A N. The I O driver A N may be configured to forward the I O requests to the CMS A N which may selectively service the I O requests by use of the virtual machine cache . The I O driver A N may comprise a storage driver such as a Windows Driver or other storage driver adapted for use an operating system and or operating environments. The I O driver A N may be configured to monitor requests within an I O and or storage stack of the virtual machine A N. In some embodiments the I O driver A N may further comprise an I O filter A N configured to monitor and or service I O requests directed to primary storage and or other storage resources . I O requests directed to the primary storage system may be serviced directly at the primary storage system non cached or may be serviced using the virtual machine cache as disclosed herein.

The I O filter A N may comprise a SCSI filter configured to manage data transfers between physical and virtual entities e.g. primary storage system VLUN disk A N and or the virtual machine cache . The I O filter A N may be configured to identify the VLUN disk A N within the virtual machine A N and manage capacity changes implemented by inter alia the cache provisioning module via the VLUN driver . As disclosed above the VLUN disk A N may be a virtual disk configured to represent dynamically allocated cache resources within the virtual machines A N as fixed size storage resources. The VLUN disk A N may be configured to report a fixed storage capacity to the operating system of the virtual machine A N rather than the actual dynamic cache capacity allocated to the virtual machine A. Accordingly the cache provisioner may be configured to dynamically provision cache storage to from the virtual machines A N through the VLUN disks A N without adversely affecting the virtual machines A N.

As disclosed above virtual machines A N may be transferred between hosts without powering down and or resetting the virtual machine A N. Such transfer operations may be simplified when the virtual machines A N reference shared resources since the virtual machines A N will be able to access the same resources when transferred. However virtual machines A N that reference local resources e.g. resources only available on the particular host may be prevented from being transferred.

In the embodiment the CMS A N may be configured to access the virtual machine cache through the VLUN disk A N configured to appear as a shared device to the virtualization kernel and or a device that does not prevent virtual machines A N from being transferred between hosts . The VLUN disk A N be provided in a Virtual Machine Disk Format VMDK supported by the host and or virtualization kernel . The I O filter may further provide for communicating other data such as configuration command and or control data e.g. performing a handshake protocol with the virtual machine cache . The virtual disk may be represented as a VLUN disk implemented according to the VMDK format of the host and or virtualization kernel . The virtual disk may be relatively small e.g. a few megabytes since the virtual disk is not used for storage but as a conduit for communication between the virtual machine and the virtual machine cache in the virtualization kernel . Alternatively or in addition the VLUN disk A N may be hidden from other applications and or operating systems of the virtual machine A N and or may be presented to the virtual machine A N as a read only storage resource and as such the operating system of the virtual machine A N may prevent other applications from attempting to write data thereto.

The virtual machines A N may be configured to emulate shared storage in other ways. For example in some embodiments the virtual machines A N may be configured to replicate one or more shared VLUN disks across a plurality of hosts such that to the hosts the VLUN disks appear to be shared devices. For instance the VLUN disks may share the same serial number or other identifier. The host and or the virtualization kernel may therefore treat the VLUN disks as shared devices and allow virtual machines A N to be transferred to from the host . The VDMK approach disclosed above may provide advantages over this approach however since a smaller number of shared disks need to be created which may prevent exhaustion of limited storage references e.g. a virtual machine may be limited to referencing storage devices .

The cache provisioner module may report the actual physical cache storage allocated to the virtual machine A via a communication link . The communication link may operate separately from I O data traffic between the VLUN driver and the I O filter A N. Thus asynchronous out of band messages may be sent between the VLUN driver and the I O filter A N. The cache provisioner module may use the communication path to dynamically re provision and or reallocate cache resources between the virtual machines A N e.g. inform the virtual machines A N of changes to cache resource allocations . The I O driver A N may report the allocation information to the CMS A N which may use the allocation information to determine the number of cache tags A N available to the virtual machine A N and so on.

As disclosed above the cache resources allocated to a virtual machine A N may be represented by cache tags A N. The cache tags A N may comprise inter alia mappings between I O addresses of a virtual machine A N and storage locations within the cache storage e.g. physical addresses of cache pages . A cache tag may therefore comprise a translation and or mapping between an identifier e.g. a storage I O address logical identifier LBA or the like used by a virtual machine A N and a cache resource e.g. a cache chunk page or the like . In some embodiments cache tags A N are configured to have a linear 1 1 correspondence with physical cache pages such that each cache tag A N represents a respective page within the cache storage . The cache tags A N may be organized linearly in RAM or other memory within the virtual machines A N as in and or virtualization kernel as in disclosed in further detail below . The linear organization may allow the memory address of a cache tag A N to be used to derive an identifier and or address of a corresponding storage location within the cache storage . Alternatively or in addition cache tags A N may be organized into other data structures such as hashtables indexes trees or the like and or may comprise separate cache address metadata.

Cache tags A N may comprise cache metadata which may include but is not limited to a next cache tag index cache state access metrics checksum valid map a virtual machine identifier VMID and so on. The next tag index may comprise a link and or reference to a next cache tag A N. The cache state may indicate a current state of the cache tag A N. As disclosed in further detail below the state of a cache tag A N may indicate whether the cache tag A N corresponds to valid data is dirty and so on. The access metrics metadata may indicate usage characteristics of the cache tag A N such as a last access time access frequency and so on. A checksum may be used to ensure data integrity the checksum may comprise a checksum of the cache data that corresponds to the cache tag A N. The size of the checksum of the cache tags A N may vary based on the size of the cache pages and or the level of integrity desired e.g. a user can obtain a higher level of integrity by increasing the size of the checksum . The valid unit metadata may identify portions of a cache page that comprise valid cache data. For example a cache page may comprise a plurality of sectors and the valid unit may indicate which sectors comprise valid cache data and which correspond to invalid and or non cached data.

In some embodiments cache tags A N may further comprise a VMID which may be configured to identify the virtual machine A N to which the cache tag A N is allocated. Alternatively ownership of the cache tag A N may be determined without an explicit VMID. As depicted in ownership of cache tags may be determined by the virtual machine A N in which the cache tags are stored. Referring to cache tags of one or more virtual machines B N may be maintained outside of the respective virtual machines B N e.g. within the virtualization kernel . In this embodiment CMS may be configured to associate cache tags B N and or ranges and or groups of cache tags B N with particular virtual machines B N by use of inter alia a VMID field.

A cache tag A N may be in one of a plurality of different states as indicated by the cache tag state field of the cache tag A N which may include but are not limited to a free state an invalid state a valid state a read pending state a write pending state and a depleted state. A cache tag A N may be initialized to a free state which indicates that the cache tag A N is not currently in use. The cache tag A N transitions from a free state to a write pending state in response to a cache write and or cache read update operation a write to the cache caused by a read miss or the like . The cache tag A N transitions to a valid state in response to completion of the cache write. The cache tag may revert to the write pending state in response to a subsequent write and or modify operation. The cache tag A N transitions to a read pending state in response to a request to read data of the cache tag and reverts to the valid state in response to completion of the read. The cache tag A N may transition to the invalid state in response to an attempt to perform a write operation while the cache tag A N is in the read pending or write pending state. The cache tag A N transitions from the invalid state to the free state in response to completing the write or read update. A cache tag A N transitions to the depleted state in response to failure of a read or write operation e.g. from the read pending or write pending state .

In some embodiments cache tags A N may further comprise a pinned state indicator. Cache tags A N that are pinned may be protected from being evicted from the cache storage allocated to another virtual machine A N or the like. Pinning cache tags A N may also be used to lock a range of cache addresses. In certain situations a portion of data associated with a read operation is available in the cache storage but a portion is not available or not valid resulting in a partial cache hit. The CMS A N may determine whether to retrieve all of the data from the primary storage system or retrieve a portion from the cache storage and the remainder from the primary storage system which may involve more than one I O to the primary storage system .

In certain embodiments the CMS A N is configured to manage a partial cache miss to minimize the number of I O requests forwarded on to the primary storage system . In addition to managing partial cache miss I O requests the CMS A N mitigates the amount of fragmentation of I Os to primary storage based on I O characteristics of the I O requests. Fragmentation of I Os also known as I O splitting refers to an I O request that crosses a cache page boundary or is divided between data that resides in the cache and data that resides on the primary storage. The I O characteristics may include whether the I O is contiguous the size of the I O request the relationship of the I O request size to the cache page size and the like. In affectively managing partial cache hits and fragmentation of I O requests the CMS A N may coalesce I O requests for non contiguous address ranges and or generate additional I O requests to either the virtual machine cache or the primary storage .

As disclosed above the CMS A N may be configured to snapshot a group of cache tags A N which may comprise storing the cache tags A N to persistent storage. The cache tags A N may be retrieved from the persistent storage when the virtual machine A N warms up e.g. reboots power cycles etc. . The cache data associated with the cache tags A N may have been pinned within the cache storage and as such may be immediately available. Alternatively the cache storage may be populated from the primary storage system or other data source to thereby recreate the full working set.

In some embodiments the address space translator is configured to correlate cache tag identifiers of a virtual machine A N with cache storage locations e.g. cache addresses cache pages etc. . In embodiments in which the CMS is implemented within a virtual machine A N as depicted in the cache tag identifier may comprise logical addresses and or identifiers of the data e.g. the address of the data in the primary storage system . In embodiments in which the CMS is implemented within the virtualization kernel as depicted in the cache tag identifier may comprise a block address associated with the data and or a storage address as identified within the storage stack of the virtualization kernel .

The cache tag manager may be configured to manage the cache tags allocated to one or more virtual machines A N which may comprise maintaining associations between virtual machine identifiers e.g. logical identifiers address etc. and data in the cache storage . The cache tag manager may be configured to dynamically add and or remove cache tags in response to allocation changes made by the cache provisioner module . In some embodiments the cache tag manager is configured to manage cache tags of a plurality of different virtual machines A N. The different sets of cache tags may be maintained separately e.g. within separate datastructures and or in different sets of cache tags and or in a single data structure.

The access metrics module may be configured to determine and or maintain cache access metrics using inter alia one or more clock hand sweep timers or the like. The steal candidate module may be configured to identify cache data and or cache tags that are candidates for eviction based on access metrics and or other cache policy e.g. least recently used stateness sequentiality etc. or the like.

The cache page management module may be configured to manage cache resources e.g. cache page data and related operations. The valid unit map module may be configured to identify valid data stored in cache storage and or a primary storage system . The page size management module may be configured to perform various page size analysis and adjustment operations to enhance cache performance as disclosed herein. The interface module may be configured to provide one or more interfaces to allow other components devices and or systems to interact with the CMS which may include but is not limited to modifying the number and or extent of cache tags allocated to a virtual machine A N querying and or setting one or more configuration parameters of the CMS accessing cache tags e.g. for a snapshot checkpoint or other operation or the like.

The cache state retention module may be configured to retain the portions of the cache state of the CMS which may include the cache tags de duplication index disclosed below and so on in response to transferring the virtual machine A N to a different host. As disclosed above the cache tags may represent a working set of the cache of a particular virtual machine A N which may be developed through the use of one or more cache admission and or eviction policies e.g. the access metrics module steal candidate module and so on in response to the I O characteristics of the virtual machine and or the applications running on the virtual machine A N.

The CMS may develop and or maintain a working set for the cache using inter alia a file system model. The cache storage may comprise one or more solid state storage devices which may provide fast read operations but relatively slow write and or erase operations. These slow write operations can result in significant delay when initially developing the working set for the cache. Additionally the solid state storage devices comprising the cache storage may have a limited lifetime a limited number of write erase cycles . After reaching the write lifetime of a solid state storage device portions of the device become unusable. These characteristics may be taken into consideration by the CMS in making cache admission and or eviction decisions.

The cache state transfer module may be configured to transfer portions of the cache state of the virtual machine A N between hosts and or to persistent storage e.g. in a snapshot operation . The cache state transfer module may comprise transferring cache tags maintained in the virtualization kernel to a remote host and or non volatile storage.

The cache tag snapshot module may be configured to maintain one or more snapshots of the working set of the cache of a virtual machine A N. As disclosed above a snapshot refers to a set of cache tags and or related cache metadata at a particular time. The snapshot module may be configured to store a snapshot of the cache tags on a persistent storage medium and or load a stored snapshot into the CMS .

The cache provisioner module may be configured to maintain mappings between virtual machines and the cache resources allocated to the virtual machines A N. The cache provisioner module may implement mappings that can be dynamically changed to re allocate cache resources between various virtual machines A N. The mappings may be further configured to allow the cache provisioner to represent dynamically allocated cache resources to the virtual machines A N as contiguous ranges of virtual cache resources independent of the underlying physical addresses of the cache storage .

As illustrated in the cache provisioner module may be configured to allocate cache resources to the virtual machines A N within the cache storage . Resources A may be allocated to virtual machine A resources B may be allocated to virtual machine B resources N may be allocated to virtual machine N and so on. The cache provisioner may be further configured to allocate cache resources for de duplication caching services which may comprise allocating cache resources to the de duplication cache . As disclosed in further detail herein the de duplication cache may be configured to cache data accessible to two or more of the virtual machines A N. Although the cache resources A N allocated to the virtual machines A N and the cache resources allocated to the de duplication cache are depicted as contiguous ranges of physical addresses within the cache storage the disclosure is not limited in this regard. As illustrated in below the cache resources A N and or may be interleaved fragmented and or discontiguous within the physical address space of the cache storage . The map module may be configured to provide for representing the resources A N and or as contiguous ranges of virtual cache resources comprising inter alia contiguous ranges of virtual cache addresses.

Referring to in some embodiments the cache provisioner module may be configured to allocate virtual cache storage resources to the virtual machines A N. As used herein a virtual cache resource refers to an indirect logical and or virtual reference to a physical cache address. Virtual cache resources may be mapped to actual physical cache storage locations by a map module which may comprise mappings and or associations between dynamically allocated virtual cache resources e.g. virtual cache addresses and physical storage locations within the cache storage . The map module may enable the cache provisioner to allocate contiguous ranges of virtual cache resources to virtual machines A N despite the fact that the underlying physical storage resources are discontiguous within the physical address space of the cache storage .

In the embodiment virtual cache storage is allocated to virtual machine A VM . The virtual cache storage may comprise a contiguous range of cache addresses or identifiers. As depicted in the virtual cache storage comprises a contiguous range of cache chunks including VM VM VM through VM N. The physical cache storage resources actually allocated to VM A may not be contiguous and or may be interleaved with cache resources that are allocated to other virtual machines B N. As illustrated in the actual physical cache chunks allocated to VM A comprise a discontiguous set of chunks VM VM VM VM N within the physical address space of the cache storage . The virtual address space of the virtual cache storage may be independent of the underlying physical address space of the cache storage . The chunks in the physical address space may be discontiguous and or interleaved with chunks that are allocated to other virtual machines B N. Although shows some of the different locations in a physical order the cache chunks allocated to the VM A may be located in a random order in accordance with the availability of physical cache resources e.g. available chunks . Moreover the chunks allocated to the VM A may be interleaved and or fragmented with chunks allocated to other virtual machines.

The map module may be configured to map virtual cache resources e.g. virtual cache addresses to physical cache resources in the physical address space of the cache storage . In some embodiments the map module may comprise an any to any index of mappings between virtual cache addresses allocated to the virtual machines A N and the physical cache addresses within the cache storage . Accordingly the virtual cache addresses may be independent of the underlying physical addresses of the cache storage . The translation layer implemented by the map module may allow cache tags A N to operate within a contiguous virtual address space despite the fact that the underlying physical allocations A may be non contiguous within the cache storage . Alternatively in some embodiments the mapping module may be omitted and the CMS A N may be configured to directly manage physical cache addresses within the cache storage .

The map module may be leveraged to secure data in the cache storage . In some embodiments the virtual machine cache may restrict access to data in the cache storage to particular virtual machines A N and or may prevent read before write conditions. The cache provisioner module may be configured to restrict access to physical cache chunks to the virtual machine A N to which the chunk is allocated. For example the cache chunk labeled VM may only be accessible to the virtual machine A based on inter alia the mapping between VM A and the cache chunk VM in the map module . Moreover the indirect addressing of the map module may prevent virtual machines A N from directly referencing and or addressing physical cache chunks allocated to other virtual machines A N.

As disclosed above the virtual machine cache may be configured to control access to data stored within the cache storage by use of inter alia the cache provisioner module and or map module . In some embodiments the CMS A N and virtual machines A N reference cache data by use of virtual cache addresses rather than physical addresses of the cache storage . Accordingly the virtual machines A N may be incapable of directly referencing the data of other virtual machines A N. The cache provisioner module may be further configured to allocate different incompatible virtual cache addresses to different virtual machines A N such as virtual cache addresses in different non contiguous address ranges and or address spaces. The use of different incompatible ranges may prevent the virtual machines A N from inadvertently or intentionally referencing virtual and or physical cache resources of other virtual machines A N.

Securing data may comprise preventing read before write conditions that may occur during dynamic cache resource provisioning. For example a first virtual machine A may cache sensitive data within a cache chunk that is dynamically reallocated to another virtual machine B. The virtual machine cache may be configured to prevent the virtual machine B from reading data from the chunk that were not written by the virtual machine B. In some embodiments the cache provisioner may be configured to erase cache chunks in response to reassigning the chunks to a different virtual machine A N or removing the association between a virtual machine A N and the cache chunk . Erasure may not be efficient however due to the characteristics of the cache storage erasing solid state storage may take longer than other storage operations 100 to 1000 times longer than read and or write operations and may increase the wear on the storage medium. Accordingly the virtual machine cache may be configured to prevent read before write conditions in other ways. In some embodiments for example the virtual machine cache may be configured to TRIM reallocated chunks e.g. logically invalidate the data stored on the chunks . Cache chunks that are erased and or invalidated prior to be reallocated may be referred to as unused chunks. By contrast a chunk comprising data of another virtual machine A N and was not erased or TRIMed is referred to as a used or dirty chunk which may be monitored to prevent read before write security hazards.

Referring to the virtual machine cache may be configured to maintain monitoring state metadata pertaining to the cache chunks . The monitoring state metadata may be persisted for use after a power cycle event. The monitoring state metadata may comprise a bitmask. In some embodiments each 4 kb sub portion of a used chunk is monitored to determine whether there has been a corresponding write. Monitoring metadata may be generated in response to reallocating a used or dirty chunk between virtual machines A N. After reallocation each sub portion of the chunk may be tested prior to read operations to ensure that the used chunk has been written by the virtual machine A N attempting to perform the read.

In the embodiment a chunk A is reallocated. The sub portions of the chunk A are represented by references m through mN. An indication of a write operation may be reflected by a 1 in the monitoring metadata . The virtual machine cache may be configured to prevent read operations on sub portions that have not been written e.g. are not marked with a 1 .

Referring back to in some embodiments the CMS A N is configured to operate within the virtual machines A N and cache tags A N and or other cache metadata are maintained within the memory space of the respective virtual machines A N. Storing the cache tags and other cache metadata within the associated virtual machine A N may allow the virtual machine A N to easily determine whether data is available in the virtual machine cache without having to access a different system or process e.g. access the virtualization kernel . In such embodiments the CMS may manage cache operations using locally stored cache tags which may increase the speed and efficiency of I O operations. Additionally the virtual machine A N typically has available more detailed information regarding access characteristics than other external processes and or systems and as such may be in a better position to make cache management decisions. For example the virtual machine A N may have access to contextual information pertaining to I O requests such as application and or file level knowledge which may be used to develop an effective working set of cache tags . Other systems that are external to the virtual machine A N e.g. operating within the virtualization kernel may only have access to low level I O information. Thus having the cache tags stored locally in the virtual machine A N may improve cache and or I O performance.

The virtual machine cache may provide caching services to the virtual machine A through the cache interface module as disclosed above which may comprise representing cache resources as a VLUN disk A within the virtual machine A monitoring I O requests of the virtual machine A by use of the I O driver A and or filter A and selectively servicing the monitored I O requests by use of the VM cache via the communication link . The standard virtual machines B N may access cache services differently. In some embodiments I O requests of the virtual machines B N are handled within a storage stack . The storage stack may comprise an I O framework of the host and or virtualization kernel . The storage stack may define a storage architecture in which storage services such as file system drivers volume drivers disk drivers and the like are deployed. Storage services may be configured to interoperate by issuing and or consuming I O requests within various layers of the I O stack . The cache interface module may comprise an I O driver X and or filter driver X configured to monitor I O requests of the virtual machines B N in the storage stack . Selected I O requests of the virtual machines B N may be serviced using the virtual machine cache .

The virtual machine cache may comprise a CMS X operating within the host and or virtualization kernel . The I O driver X and or filter driver X may be configured to direct I O requests of the virtual machines B N to the CMS X which may selectively service the I O requests as disclosed herein. The CMS X may be configured to maintain cache metadata for the virtual machines B N including inter alia cache tags B N. In some embodiments the CMS X maintains the cache tags B N in a single data structure. Alternatively the cache tags B N may be maintained separately and or may be managed by separate instances of the CMS X.

As disclosed above the cache provisioner may be configured to provision cache storage resources to the virtual machines A N. The cache provisions may be configured to dynamically re provision and or reallocate cache resources in accordance with user preferences configuration and or I O requirements of the virtual machines A N. The virtual machines A N may have different I O requirements which may change over time due to inter alia changes in operating conditions usage characteristics and or patterns application behavior and the like. The cache resources available to the virtual machines A N may vary as well due to inter alia virtual machines A N being migrated to and or from the host virtual machines A N coming on line virtual machines A N becoming inactive e.g. shut down suspended etc. or the like. The cache provisioner may therefore be configured to adjust the allocation of cache resources in response to I O requirements of particular virtual machines A N and or the I O characteristics and or I O load on the host due to other virtual machines A N other processes and or services running on the host and so on .

Step may comprise detecting a request to perform a data read operation. The data read operation may be requested by a particular storage client such as a virtual machine A N. The request may be detected by an I O driver A N and or I O filter A N operating within the virtual machine A N e.g. in close proximity to the virtual machine as depicted in . Alternatively the request may be detected by an I O driver X and or I O filter X operating within the virtualization kernel . Step may further comprise communicating the request to the CMS A N configured to operate within the virtual machine A N and or a CMS X operating within the virtualization kernel .

Step may comprise determining whether data of the read operation is available in the cache storage . In some embodiments step comprises identifying a cache tag A N that corresponds to the read request e.g. identifying a cache tag A N having an identifier that matches an identifier or address associated with the read request . If a cache tag A N is available and the cache tag A N is valid and readable e.g. in the valid state the flow may continue at step otherwise the flow may continue at step .

Step may comprise retrieving data of the read request from cache storage . Step may therefore comprise servicing the read request by the virtual machine cache . Step may further comprise updating cache metadata such as clock hands data access metrics or the like. Retrieving the data may further comprise determining a physical address of the data within the cache storage using the cache tag A N identified at step . Step may comprise mapping and or translating a virtual cache address to a physical cache address by use of a map module as disclosed herein.

Step may comprise retrieving the cache data from primary storage e.g. from the primary storage system . Step may further comprise determining whether the data should be admitted into the cache. This determination may be based on cache availability admission policy eviction policy or the like. The CMS A N may determine whether admitting the data would improve I O performance of the virtual machine A N and if so may admit the data into the cache storage . Admitting the data may comprise allocating one or more cache tags A N storing the data in the cache storage and or associating the physical storage location of the data with an identifier of the data by use of the allocated cache tags A N.

Step may comprise determining whether the write request pertains to data in the cache which may comprise identifying a cache tag associated with the storage I O address or other identifier . If a cache tag A N is identified at step the flow continues to step which may comprise determining a physical address of the data within the cache storage using inter alia the identified cache tag A N. Step may comprise writing data of the write request to the identified physical storage location s . Step may further comprise writing the data to primary storage system in a write through operation . In some embodiments data is written to the cache storage and the primary storage system simultaneously in a write through operation. Writing data to the primary storage system may comprise allowing storage services of the virtualization layer and or host to write the data to the primary storage system . Step may comprise acknowledging completion of the write request in response to writing the data to the primary storage system .

As disclosed above cache may be cached in a write through cache mode in which data is written and or modified on both the primary storage system and the cache storage . A write completion is acknowledged after the write operation to the primary storage system is completed regardless of whether a corresponding write operation to the cache storage has completed. In specific embodiments cache write operations can be queued and completed as the cache speed allows. Thus a cache storage with a slow write speed or a queue of pending write operations does not degrade overall I O performance. Cache tags associated with incomplete or queued write operations are identified as pending e.g. are set to a write pending state as disclosed above . After the write operation completes the associated cache tag transitions to a valid state. In some embodiments attempts to read data of a cache tag that is in a pending state results in a cache miss causing retrieval of the requested data from the pending memory buffer associated with the I O or from the primary storage system as described above.

Although a write through cache mode is described herein the disclosure is not limited in this regard and could be adapted to operate in any suitable cache mode including but not limited to write back cache mode read through write behind refresh ahead or the like. The embodiments disclosed herein may be further configured to cache data in a write never cache mode as disclosed in U.S. Provisional Patent Application Ser. No. 61 696 126 to Vikram Joshi et al. filed Aug. 31 2012 and entitled Systems Methods and Interfaces for Adaptive Persistence which is hereby incorporated by reference.

Step may comprise stalling cache I O operations by the corresponding CMS A N which may comprise stopping I O traffic between the virtual machine A N and the virtual machine cache . Step may comprise the VLUN driver issuing a message to the CMS A N through the communication link to stop sending I O data traffic pertaining to the cache while the cache allocation is modified. Alternatively or in addition step may comprise the I O driver X and or I O filter X ignoring and or holding I O requests pertaining to the virtual machine B N.

In some embodiments step comprises stalling the CMS A N which allows applications operating on the corresponding virtual machine A N to continue to perform I O operations independently of the virtual machine cache e.g. operate directly with the primary storage system and or other storage resources . The CMS A N may be configured to invalidate cache tags A N in response to write operations that occur while the CMS A N is stalled. Step may further comprise flushing any outstanding I O requests directed to the virtual machine cache before halting cache operations e.g. waiting for any outstanding I O data traffic to and from the virtual machine cache to complete and or notifying the cache provisioner module that the cache traffic has been halted.

Step may comprise modifying the cache resources allocated to the CMS A N. Step may comprise modifying cache resource mappings implemented by the mapping module which may include allocating additional physical cache storage space to the virtual machine A in the cache device associating physical cache resources with corresponding virtual cache addresses and or resource identifiers by use of the map module removing associations between the virtual machine A and physical cache resources e.g. if the cache allocation is being decreased and so on. Step may further comprise informing the CMS A N that the cache resources allocated thereto have been resized which may comprise providing an indication of the cache resources that have been allocated providing identifiers of the cache resources allocated with the CMS A N e.g. a set of one or more virtual cache addresses address range s or the like and so on.

Step may comprise updating cache metadata of the CMS A N in accordance with the modifications of step . Step may comprise modifying the cache tags of the CMS A N in accordance with the modifications of step step may comprise allocating additional cache tags A N in response to being allocated additional cache resources and or removing cache tags A N in response to being allocated fewer cache resources. As described above additional cache tags A N may be allocated contiguously within a virtual cache address space and or contiguous memory. Therefore additional cache tags A N may be appended to an existing contiguous range of cache tags A N thereby preserving the existing working set of the virtual machine A N during the resizing operation. Cache tags may A N may be removed in contiguous ranges which may allow the working set of the remaining cache tags to be preserved.

Step may comprise resuming cache I O operations which may comprise indicating that the modification s of steps and or are complete and instructing the CMS A N to resume cache I O operations. Step may therefore comprise selectively servicing I O operations of the virtual machine A N using the virtual machine cache as described herein.

As illustrated in the host may comprise a large number of virtual machines A N. The virtual machines A N may be deployed in a Virtual Desktop Infrastructure VDI environment. As such the virtual machines A N may use many of the same files such as operating system files application files data files user profile information and so on. The virtual machines A N may access these files in a similar way. These commonalities can present I O performance and redundancy problems. For example when a large number of the virtual machines A N boot each may accesses a similar set of operating system files stored on the primary storage system or some other persistent storage . The resulting boot storm may overwhelm the primary storage system which may significantly degrade the performance. Similar file access storms may occur in response to the virtual machines A N loading applications accessing shared data accessing user profile information executing a login process and so on. Moreover because the virtual machines A N use identical or similar operating systems applications and or files the virtual machines A N may cache duplicate data in the virtual machine cache resulting in reduced cache availability and wasted cache resources.

The systems and methods for file level de duplication disclosed herein may be used to improve the I O performance of the virtual machines A N by inter alia caching a single copy of data for access by a plurality of virtual machines A N. As depicted in one or more of the virtual machines A N may comprise a virtual machine de duplication module VMDM A N which may be configured to identify data suitable for admission into a de duplication cache . Data suitable for admission may include stable data that is used or is likely to be used by two or more virtual machines A N. The two or more virtual machines may access a single copy of the cached data within the de duplication cache . As used herein stable data refers to data that is infrequently modified. Examples of stable data include but are not limited to read only data e.g. static configuration data operating system files e.g. .sys files .dll files .so files and so on application files e.g. .exe files etc. static data files content files e.g. .mpeg files .html files .jpg and so on and the like. Data admitted into the de duplication cache by the de duplication cache manager may be accessible to multiple virtual machines A N. As such in certain embodiments the de duplication cache may be configured to operate in a read only mode meaning that after data is initially admitted into the de duplication cache the data may not be modified within the de duplication cache or modified infrequently . Accordingly data subject to frequent updates and or changes may not be suitable for admission into the de duplication cache .

The virtual machines A N may access data in the de duplication cache in lieu of accessing the primary storage system and or admitting separate copies of the data into the cache storage via respective CMS A N of the virtual machines A N which may result in reduced I O load on the primary storage increased I O performance and more efficient use of the cache storage . In some embodiments the virtual machines A N may be configured to persist and or transfer cache metadata pertaining to the de duplication cache such that the virtual machines A N can access data admitted into the de duplication cache after reboot restart power cycle and or migration operations which may significantly ameliorate boot storm issues.

In some embodiments the de duplication cache is deployed outside of the virtual machines A N e.g. within the virtualization kernel and or host . Accordingly the de duplication cache may comprise a module of the virtual machine cache a service and or process operating within the virtualization kernel and or host e.g. on a bare metal operating system of the host or the like.

The cache provisioner may be configured to allocate cache storage resources within the cache storage for the de duplication cache which may comprise allocating one or more chunks to the de duplication cache as disclosed herein. The map module may be configured to provide mappings between virtual cache resources managed by the de duplication cache manager and physical cache resources allocated to the de duplication cache by the cache provisioner as disclosed herein.

The cache provisioner may allocate cache storage resources to the de duplication cache in the same way that cache resources are allocated to the virtual machines A N. In some embodiments the de duplication cache comprises a VLUN disk the cache provisioner module may be configured to dynamically re provision and or reallocate cache resources to from the de duplication cache through the VLUN driver and VLUN disk as disclosed herein. Alternatively the de duplication cache may be configured to access cache resources directly via the map module and or on the cache storage .

As depicted in the host may be configured to host a plurality of virtual machines A N. One or more of the virtual machines A N may comprise a CMS A N. The CMS A N may be configured to monitor I O requests within the virtual machine A N and selectively service the monitored I O requests by use of the virtual machine cache as disclosed herein. The CMS A N may be configured to monitor I O requests within a storage stack of the virtual machine A N. The CMS A N may monitor I O requests at various layers A N of the storage stack using the I O driver A N filter driver A N or the like. The CMS A N may comprise a multi level and or file level cache configured to monitor I O requests at various levels A N of the storage stack of the virtual machine A N and to selectively service the I O requests by use of the virtual machine cache . The CMS A N may be configured to maintain layer specific cache metadata pertaining to cache operations at each layer A N. Further embodiments of file level and multi level caching are disclosed in U.S. patent application Ser. No. 13 287 998 to Joshi Vikram et al. filed Nov. 2 2011 entitled Systems and Methods for a File Level Cache and which is hereby incorporated by reference.

The system may comprise a VMDM A N which may be configured to identify data suitable for admission into the de duplication cache . Data suitable for admission into the de duplication cache may include data that is used within multiple virtual machines A N e.g. operating system files application files and so on and is stable e.g. the data is rarely updated and or modified . The VMDM A N may be configured to identify data suitable for admission by monitoring I O requests within the storage stack of the virtual machine A N using inter alia the I O driver A N filter driver A N and or other dedicated file I O monitor modules not shown . In some embodiments the VMDM A N may operate at a file level of A the storage stack and as such may be configured to monitor I O requests pertaining to file operations.

The VMDM A N may comprise a de duplication policy module A N configured to identify files suitable for admission into the de duplication cache based on inter alia de duplication admission policy A N. The de duplication admission policy A N may include file selection criteria which may include but is not limited to file name matching extension matching volume matching disk matching inclusion lists exclusion lists and the like. File selection criteria may further comprise dynamic and or learned criteria. For example in some embodiments the de duplication policy module A N may be configured to monitor file I O access patterns within the virtual machine A N access by various storage clients to dynamically identify files suitable for admission into the de duplication cache . For example the de duplication policy module A N may be configured to monitor file access frequencies for one or more of write operations and read operations. Files subject to multiple read operations and few if any write operations may be selected for admission into the de duplication cache. In some embodiments the de duplication admission policy A N may select files for admission in response to properties of the I O request API calls and or other messages. For example the I O request issued by the storage client may include flags and or other metadata indicating that the corresponding data should be admitted into the de duplication cache . Alternatively or in addition the VMDM A N may be configured to receive de duplication cache admission configuration and or hints through the configuration interface . The configuration interface may be configured to receive de duplication cache admission policy information through one or more of dedicated APIs block device interface calls I O requests fadvise calls IOCTL calls and the like.

The de duplication policy module A N and or CMS A N may be configured to prevent data from being redundantly cached in multiple cache layers which may comprise preventing data that has been admitted into the de duplication cache from being admitted into the virtual machine cache by the CMS A N. In some embodiments the de duplication policy module A N may inform the CMS A N of files that have been admitted into the de duplication cache and in response the CMS A N may be configured to prevent data of the identified files from being admitted into the CMS A N e.g. the CMS A N may be configured to ignore I O requests pertaining to the identified files .

Referring back to the VMDM A N may be configured to maintain de duplication cache metadata pertaining to data that has been admitted into the de duplication cache . The metadata may include a de duplication index A N. In some embodiments files may be identified by use of a unique file identifier UFID which may uniquely identify the file with respect to the virtual machine A N e.g. uniquely identify the file within the namespace of the file system and or operating system of the virtual machine A N . The UFID may comprise a combination of the name of the file and a volume identifier VID which comprise a volume GUID volume name or the like e.g. VID windows system32 kernel32.dll . Files may be further identified by use of a context independent identifier. As used herein a context independent or context free identifier refers to an identifier that is independent of the namespace of the particular virtual machine A N. The context independent and or context free identifier may be used to allow different virtual machines A N to share access to data in the de duplication cache . In some embodiments the context independent identifier comprises a Data Identifier DID which may be derived from the contents of the file itself. A DID may include but is not limited to a hash e.g. SHA 1 MD5 or the like a Cyclic Redundancy Check CRC value CRC32 a signature or the like. Accordingly a context independent identifier may comprise and or be referred to as a file signature. Generating the DID of a file may comprise reading at least a portion of the file data e.g. contents of the file and using the file data to generate the DID e.g. by hashing the file data signing the file data processing the file data or the like . The DID of a file may be common to the virtual machines A N and or de duplication cache . Accordingly the DID of a file may comprise a context free and or universal identifier of the file which may be used to reference the file data by the de duplication module and or any virtual machine A N of any host .

The de duplication cache may index files admitted thereto by use of context independent identifiers such as DIDs which may allow different types of virtual machines A N having different file naming conventions and or file paths to access file data within the de duplication cache . For example the UFID of kernel32.dll on the virtual machine A may be referenced by VID1 windows system32 kernel32.dll which may differ from the UFID on other virtual machines e.g. the UFID of kernel32.dll on virtual machine B may be VID2 windows install system32 kernel32.dll . However the DID used by the virtual machines A and B to reference kernel32.dll may be the same since the DID is derived from the contents of the file.

The de duplication index A N may be configured to associate the UFID of files that have been admitted into the de duplication cache with a respective DID . The de duplication index A N may be implemented using any suitable data structure including but not limited to a tree hash table linked list lookup table content addressable map CAM or the like. depicts one embodiment of a de duplication index . As depicted in the de duplication index associates virtual machine specific UFIDs with corresponding context independent DIDs represented in Hex format . As disclosed above the UFIDs may correspond to a unique file identifier of a particular virtual machine A N e.g. a fully qualified file name and the DIDs may comprise a context independent identifier of the files which may be derived from the contents of the file. The VMDM A N may be configured to identify files that have been admitted by the de duplication cache manger by use of the UFIDs and to reference the data by use of the context independent DIDs .

As disclosed above the de duplication cache may comprise a de duplication cache manager which may be configured to manage the contents of the de duplication cache and or the cache resources allocated to the de duplication cache by the cache provisioner . In some embodiments the de duplication cache manager is configured to represent de duplication cache resources using de duplication cache tags . Like the cache tags A N disclosed herein the de duplication cache tags may be maintained in any suitable data structure including but not limited to contiguous memory a table tree or the like. depicts one embodiment of de duplication cache tags . As illustrated in the de duplication cache tags may be configured to associate DIDs of files admitted into the de duplication cache with respective storage locations of the file data within cache storage . The storage locations may comprise references to virtual cache resources that are translated to physical cache addresses by the map module as disclosed herein. Alternatively the storage locations may comprise physical cache addresses that directly reference the cache storage . Like the cache tags A N disclosed herein the de duplication cache tags may comprise additional cache metadata not shown such as access metrics timer data and so on which may be used to manage admission to and or eviction from the de duplication cache . The de duplication cache tags may further comprise identifiers of virtual machines A N associated with the files that have been admitted into the de duplication cache . As disclosed in further detail herein the virtual machine identifiers may be used to secure data admitted into the de duplication cache and or prevent read before write hazards.

Step may comprise determining whether to admit the file into the de duplication cache . Step may therefore comprise determining whether the file is suitable for admission into the de duplication cache by use of a de duplication cache policy module A N. Step may comprise determining whether the file satisfies a de duplication cache admission criteria such as a file selection criteria . Step may further comprise receiving and or accessing de duplication cache admission policy via the configuration interface which may include but is not limited to one or more dedicated APIs block device interface commands and or extensions fadvise calls IOCTRL calls I O requests file attributes and or the like.

Step may further comprise determining whether data of the file has already been admitted into the de duplication cache by use of the de duplication index A N e.g. determining whether the deduplication index A N includes a valid entry corresponding to the UFID of the file . Step may further comprise determining whether the file is suitable for admission into the de duplication cache by use of the deduplication policy module A N e.g. applying file selection criteria A N or the like . If the I O request pertains to data that has been admitted into the de duplication cache and or is suitable for admission the flow continues at step otherwise the flow continues at step .

Step may comprise servicing the I O request by use of the de duplication cache . If data of the I O request has already been admitted into the de duplication cache step may comprise requesting the data from the de duplication cache via the communication link e.g. through the VLUN disk A N . The request may comprise the DID of the file as indicated by the de duplication index A N. If data of the I O request has not been admitted into the de duplication cache by the virtual machine A N step may comprise requesting admission for the data as disclosed in further detail herein. Step may further comprise indicating to the CMS A N and or other cache layers that data of the I O request is being cached in the de duplication cache .

Step may comprise servicing the I O request by use of another cache layer such as the CMS A N as disclosed herein and or servicing the I O request within the storage stack of the virtual machine A N and or virtualization kernel .

Referring back to as disclosed herein the VMDM A N may be configured to determine whether data of a file I O request has been admitted into the de duplication cache by use of the de duplication index A N. If the de duplication index A N comprises a valid entry corresponding to the UFID of the file the VMDM A N may attempt to service the request using the de duplication cache which may comprise requesting the data from the de duplication cache using the corresponding DID in the de duplication index A N. Referring to the VMDM A N may request file data of c windows system32 kernel32.dll from the de duplication cache using the corresponding DID EA733BA0. 

In response to a request to access file data the de duplication cache may determine whether data corresponding to the request is available e.g. has not been evicted by use of the de duplication cache manager . The de duplication cache manager may attempt to reference a de duplication cache tag that corresponds to the requested DID . If a valid de duplication cache tag exists the de duplication cache may read the data from the cache storage and provide the data to the VMDM A N via the communication link and or other cache interface mechanism . The VMDM A N may use the data received from the de duplication cache to service the I O request.

If the de duplication cache does not have the requested data e.g. the de duplication cache manager cannot locate a valid de duplication cache tag associated with the DID of the request the de duplication cache may signal a cache miss indication or other error code . In response the VMDM A N may attempt to admit the file data into the de duplication cache . Admitting the data may comprise reading data of the file from primary storage or another storage resource determining a DID of the file data and issuing a request to admit the data to the de duplication cache via the communication link or other cache interface . The admission request may include the file data and the corresponding DID. The VMDM A N may be configured to generate the admission request in a separate thread and or process that is outside of the critical path of the original I O request in the storage stack A N. As used herein the critical path of an I O request refers to the sequence of operations that contribute to the latency of the I O request. Since admitting the data into the de duplication cache is performed in a separate thread and or process the I O request can be serviced normally and may not significantly impact the performance of the I O request.

In some embodiments the VMDM A N may be configured to admit portions of a file into the de duplication cache . A de duplication cache policy module A N may determine the maximum size for files admitted into the de duplication cache . The VMDM A N may determine whether the file exceeds the maximum file size and if so may attempt to admit only a portion and or chunk of the file. For example a large file large.dll may be segmented into 16 chunks which may be identified by a relative index large1.dll large2.dll and so on. The de duplication index A N may comprise separate entries and corresponding DIDs for each segment of the file. The VMDM A N may attempt to admit the various portions of the file as needed e.g. in response to I O requests pertaining to various portions of the file .

The de duplication cache may comprise a de duplication policy module configured to selectively admit file data into the de duplication cache in response to requests from the VMDMs A N. The de duplication policy module may determine whether to admit data into the de duplication cache based on various cache policy factors which may include but are not limited to cache resource availability access metrics e.g. how many different virtual machines A N have requested the file data and or access metrics of the requests and so on.

In response to determining to admit data of the admission request the de duplication cache may be configured to allocate one or more de duplication cache tag s for the request associate the allocated de duplication cache tags with the DID s of the request and provide for storing the data of the admission request in the cache storage in association with the virtual cache addresses of the corresponding de duplication cache tags . Admitting the data may further comprise providing an indication to the VMDM A N that the data was successfully admitted into the de duplication cache . In response to the indication the VMDM A N may update the de duplication index A N which may comprise adding an entry to associate the UFID of the file with the corresponding DID . The VMDM A N may be further configured to indicate that the file is being cached in the de duplication cache so that other cache layers such as the CMS A N do not redundantly cache data of the file.

If the admission request is not satisfied due to cache policy or an error condition the de duplication cache may return an error code or other indication to the VMDM A N. In response the VMDM A N may remove entries corresponding to the DID s of the admission request from the de duplication index A N if any and or indicate to other cache layers e.g. CMS A N that the file has not been admitted into the de duplication cache .

As disclosed herein each virtual machine A N may reference file data using the same context free DID values. In the embodiment each virtual machine A N may reference the file data of kernel32.dll using the same DID EA733BA0. Accordingly each virtual machine A N that accesses kernel32.dll and determines that kernel32.dll is suitable for admission into the de duplication cache will attempt to admit the file using the same DID EA733BA0. When each virtual machine A N attempts to access kernel32.dll for the first time the de duplication index A N of the virtual machine A N will not include an entry for the file. Therefore the VMDM A N will attempt to admit the file into the de duplication cache which as disclosed above may comprise reading the file kernel32.dll calculating the DID EA733BA0 and issuing a request to admit the file data to the de duplication cache . Accordingly the de duplication cache may receive many different requests to admit the same data e.g. data associated with the same DID .

In response to a request to admit file data corresponding to a particular DID the de duplication cache manager may determine whether data corresponding to the DID has already been admitted by a different virtual machine A N by inter alia reference to the de duplication cache tags . If a valid de duplication cache tag associated with the DID exists the de duplication cache manager may determine that the file data of the admission request has already been admitted. The de duplication cache manager may acknowledge the admission request without re admitting the file data. The de duplication cache manager may however update the de duplication cache tag s to indicate that corresponding virtual machine A N is authorized to read the file data from the de duplication cache e.g. update the virtual machine identifier information of the cache tag s .

In some embodiments de duplication cache may be configured to verify that the existing file data in the cache storage matches the file data the admission request. This verification may be used to prevent errors due to DID collisions. As used herein a DID collision refers to a situation in which different file data results in the same DIDs . DID collisions may be a result from using shorter less complex DIDs . The size and or complexity of the DIDs used in the system may be a tradeoff between a the overhead needed to calculate communicate and or store the DIDs and b the probability of DID collisions. Although large complex DIDs may significantly reduce the likelihood of DID collisions the overhead of such DIDs may be unacceptable. In some embodiments the size and or complexity of the DIDs may be based on inter alia the number of unique files and or file versions that may be admitted into the de duplication cache . Embodiments that require fewer unique files and or file versions may be configured to use less complex and or smaller DIDs . Embodiments that require a relatively large number of unique files and or file versions may be configured to use large more complex DIDs . The size of the DIDs and or other configuration information may be managed by use of the configuration interface module and or cache interface .

In some embodiments the de duplication cache may be configured to verify that matches between two or more DIDs are not due to a DID collision. Verifying DIDs may comprise comparing the file data from which the DIDs were determined e.g. a byte by byte comparison . According in response to determining that the DID of an admission request matches the DID associated with a valid de duplication cache tag the de duplication cache may be configured to verify that the underlying file data matches by inter alia comparing the data in the admission request to the data in cache storage comparing stronger hash signature and or CRC values or the like. If the match is due to a collision the de duplication cache may be configured to return an error to the VMDM A N indicating that the admission request could not be satisfied.

Alternatively the de duplication cache may be configured to admit the data using an alternative DID and may provide the alternative DID to the VMDM A N in response to successfully admitting the data into the de duplication cache . The alternative DID may be generated in a deterministic manner such that subsequent requests from VMDM A N of other virtual machines A N will also result in the same alternative DID . The alternative DID may be derived from the original DID and or may be derived from the file data itself. For example the alternative DID may be calculated using an alternative hash signature or other algorithm. Alternatively or in addition the alternative DID may comprise a two part identifier comprising a first portion derived from the original DID and a second portion generated by the de duplication cache and or VMDM A N.

As disclosed above in response to receiving an acknowledgement that the admission request was satisfied the VMDM A N may update the de duplication index A N to associate the file UFID with the corresponding DID or alternative DID . The VMDM A N may service subsequent I O requests pertaining to the file by use of the de duplication cache which may comprise a determining that the file has been admitted into the de duplication cache in reference to the de duplication index A N matching the UFID of the file to a valid entry in the index A N b requesting data to satisfy the I O request from the de duplication cache and c satisfying the I O request using data provided by the de duplication cache .

In some embodiments VMDM A N is configured to store a snapshot A N of the de duplication index A N on a persistent storage medium e.g. the primary storage or the like . As disclosed above a snapshot refers to current state information. Accordingly storing a snapshot of the VMDM A N may comprise storing a snapshot of the de duplication index A N e.g. persisting entries comprising the UFIDs and DIDs of the data that has been admitted into the de duplication cache by the virtual machine A N . The snapshot A N may be persisted in any suitable format including but not limited to a file a configuration repository such as a registry or persistent settings a database cache storage or the like.

In some embodiments the VMDM A N is configured to load the snapshot A N of the de duplication index A N after the corresponding virtual machine A N undergoes a reboot restart power cycle and or transfer operation e.g. vMotion operation . Loading the snapshot A N may comprise populating the de duplication index A N with the UFIDs and corresponding DIDs of the files that have been admitted into the de duplication cache by the virtual machine A N. In some embodiments when the virtual machine A N restarts the VMDM A N is configured to populate the de duplication index A N with the contents of the corresponding snapshot A N which may allow the VMDM A N to access data of files admitted into the de duplication cache prior to the interruption. Accordingly the effects of the boot storm caused by multiple virtual machines A N attempting to simultaneously access the primary storage may be significantly reduced.

As disclosed above the contents of the de duplication cache may be shared between multiple virtual machines A N. As such the de duplication cache and or VMDM A N may be configured to manage modifications to files admitted to the de duplication cache since such modifications could adversely affect other virtual machines A N that are accessing the files. Accordingly in some embodiments the de duplication cache may be configured to operate in a read only mode such that data cannot be modified after admission.

In some embodiments VMDM A N is configured to monitor I O requests within the storage stack of the virtual machine A N to identify requests to modify files that have been admitted into the de duplication cache . In response to identifying such a request the VMDM A N may be configured to invalidate the de duplication entry corresponding to the file in the de duplication index A N. The VMDM A N may be further configured to update the snapshot A N of the de duplication index A N. Accordingly subsequent I O requests pertaining to the file may operate against primary storage . The VMDM A N may be further configured to inform the de duplication cache that the file is no longer in use by the virtual machine A N. In response the de duplication cache manager may remove the VMID of the virtual machine A N from the corresponding de duplication cache tag remove the VMID from the VMID field of the corresponding entry .

After invalidating the entry for kernel32.dll in the de duplication index A subsequent requests to access data of the file may result in cache misses at the VMDM A since the entry for kernel32.dll is removed and or marked invalid . In response to the miss the VMDM A may attempt to admit the updated data of kernel32.dll into the de duplication cache as described above which may comprise a reading the updated contents of kernel32.dll b determining a DID of the file data and c issuing one or more admission requests comprising the file data and DID to the de duplication cache . Since the DID of the modified kernel32.dll is based on different file data than the original version the DID will be different from the original DID of the file unless a DID collision exists . The de duplication cache may admit the updated file data in accordance with the de duplication policy module which may comprise a allocating one or more de duplication cache tags b associating the allocated de duplication cache tags with the DID provided in the admission request and c providing for storing the file data of the admission request in cache storage .

As illustrated above since the de duplication cache references data using context free DIDs the de duplication cache may be capable of caching multiple versions of files that have the same and or similar names. Accordingly other virtual machines B N may continue to access the original version of kernel32.dll referenced by DID EA733BA0 while the virtual machine A uses the updated version of kernel32.dll. In response to determining that the updated version of kernel32.dll was successfully admitted into the de duplication cache the VMDM A may be configured to update the de duplication index A to associate kernel32.dll with the new DID 90EAF331 as depicted in . The VMDM A may be further configured to update the snapshot and or inform other cache layers e.g. CMS A that kernel32.dll has been admitted into the de duplication cache .

The cache provisioner module may be configured to dynamically allocate cache storage resources to the de duplication cache . As described herein cache resources may be dynamically allocated in response to the I O requirements of the virtual machines A N. The de duplication cache manager may be configured to manage the contents of the de duplication cache in response to the cache resources allocated thereto. Managing the de duplication cache may include but is not limited to a selectively admitting and or denying admission to the de duplication cache b maintaining cache access metrics such as least recently used LRU steal timer or the like and or c evicting files from the de duplication cache . Evictions may occur due to cache capacity issues aging cache resource allocation changes or the like. Alternatively or in addition file data may be evicted in response to a request from an external entity via the cache interface and or configuration interface . For example file data in the de duplication cache may be corrupt comprise a virus Trojan or the like. In response to detecting a problem with cache data the de duplication cache or other entity may request removal of the data. Removing the data may include but is not limited to invalidating and or erasing the data from the cache storage removing de duplication cache tags corresponding to the data and so on. Subsequent requests for the data may therefore result in a cache miss which may result in readmission of the data into the de duplication cache as disclosed herein.

In some embodiments the de duplication cache may be configured to secure data stored therein. Securing the data may comprise preventing read before write hazards. As used herein a read before write hazard refers to an attempt by an entity to read data that was not written by the thereby. In the embodiment a read before write hazard may comprise a virtual machine B attempting to read a file that was admitted into the de duplication cache by a different virtual machine A. The de duplication cache may be configured to maintain an access list configured to identify the virtual machines A N that are authorized to access particular files. Referring to the VMID field of the de duplication cache tags may identify the virtual machines A N that are authorized to access particular files. The de duplication cache may determine that a virtual machine A N is authorized to access a file in response to determining that the virtual machine A N has access to the underlying file data e.g. access to the file on the primary storage . The de duplication cache may therefore determine that a virtual machine A N is authorized to access a particular file in response to the virtual machine A N admitting and or attempting to admit the file into the de duplication cache .

The de duplication cache may restrict access to file data of the DIDs to the virtual machines A N identified in the corresponding access list . Referring to the de duplication cache may restrict access to file 45AD342E to virtual machine B the other virtual machines A and N may be prevented from accessing this data until the other virtual machines A and or N attempt to admit data of DID 45AD342E into the de duplication cache .

In some embodiments the read before write security restrictions may be lifted or relaxed. For example when admitting file data into the cache a virtual machine A N may indicate that the file data is public and may be accessed without restriction. For example system files such as kernel32.dll may be specified as public. Public access may be denoted with a wildcard indicator in the access list or the like e.g. data of DID 20AE45EA may be publicly accessible . Allowing unrestricted access to file data may be advantageous in certain situations. For example and as disclosed above after being transferred to another host the virtual machine A N may load a snapshot A N of the de duplication index A N. However the virtual machine A N may not have attempted to admit the referenced files on the destination host and as such may not appear on the access list s maintained by the de duplication cache of the destination host meaning that the virtual machine A N would have to attempt to re admit the file s at the destination host before having access to the file s . If the files are marked as publicly accessible the virtual machine A N may immediately access the files at the destination host without further accesses to the primary storage which as disclosed above may ameliorate boot storm issues when the virtual machine A N reboots restarts and or is power cycled as part of the transfer. Alternatively or in addition when a virtual machine A N is migrated to another host the previous host may provide de duplication cache state of the virtual machine to the destination host. The destination host may use the transferred de duplication cache state data to allow the virtual machine A N to access data in the de duplication cache of the destination host. The de duplication cache state may include the DIDs of file data the virtual machine A N is authorized to access e.g. the de duplication cache tags pertaining to the virtual machine .

Step may comprise determining whether the file should be admitted into the de duplication cache e.g. determine whether the file is suitable for de duplication as disclosed herein. Step may comprise determining whether the file is referenced in the de duplication index A N and or evaluating de duplication policy of a de duplication policy module A N such as file selection criteria . If the file is not suitable for de duplication the flow may continue to step where the I O request may be serviced by a CMS A N primary storage or the like otherwise the flow continues to step .

Step may comprise determining whether the file has been admitted into the de duplication cache . Step may comprise determining whether the de duplication index A N comprises a valid entry associated with the UFID of the file. If so the flow may continue to step otherwise the flow continues to step .

Step may comprise requesting the file data from the de duplication cache . Step may comprise requesting the data using the DID of the file as indicated by the de duplication index A N. The request of step may be issued to the de duplication cache via a communication link e.g. via a VLUN disk A N and or by use of other mechanisms of the cache interface . In response to the request the de duplication cache may be configured to identify a de duplication cache tag associated with the DID and provide the corresponding data from cache storage . The VMDM A N may be further configured to service the I O request detected at step using the data provided by the de duplication cache .

Step may comprise attempting to admit data of the file into the de duplication cache . Accordingly step may comprise reading the file data and or portion thereof generating a DID for the file data and providing the file data and DID to the de duplication cache for admission as described above. The file data may be read in a slow path thread or processes that is separate from the I O thread s and or processes used to service the original I O request. Accordingly while the operation s of step are performed the original I O request may be serviced from primary storage another cache level or the like.

As illustrated in admitting data into the de duplication cache may further comprise forking the admission process or thread from the process or thread used to service the I O request. Accordingly the operations involved in admitting the data into the de duplication cache may be performed separately from and or independently of the fast path operations involved in servicing the I O request in the storage stack of the virtual machine A N and or storage stack of the host .

Step may further comprise updating the de duplication index A N in response to acknowledgement that the data was successfully admitted. Step may further comprise updating a snapshot A N and or informing other cache layers e.g. CMS A N that data of the file is being cached in the de duplication cache .

Step may comprise determining if a valid de duplication cache tags associated with the DID exists. If so the flow continues at step otherwise the flow continues at step .

Step may comprise selectively admitting the data into the de duplication cache . As disclosed herein admitting data into the de duplication cache may comprise a determining whether to admit the data by use of a de duplication policy module A N b allocating one or more de duplication cache tags c storing data of the admission request within the cache storage and or d associating the de duplication cache tags allocated for the data with the DID of the data and or the storage location s of the data on the cache storage . Step may comprise acknowledging that the data was successfully admitted into the de duplication cache .

Step may comprise determining whether to verify the DID match identified at step e.g. determine whether the match of step was due to a DID collision . The determination of step may be based on the strength size and or complexity of the DIDs the number of unique files being handled by the de duplication cache and so on. Step may further comprise comparing data of the admission request to the data stored in the cache storage to inter alia verify the DID match. If step indicates that a DID collision has occurred the flow may continue at step otherwise the flow may continue to step .

Step may comprise returning an indication that the request to admit the data into the cache could not be satisfied. The indication may specify that the request could not be satisfied due to a DID collision.

Step may comprise determining whether a DID collision exists as disclosed herein. If no DID collision exists the flow continues to step which may comprise acknowledging that the data is admitted into the de duplication cache without actually re admitting the data. Step may further comprise updating one or more file access list s on the de duplication cache tags to indicate that the virtual machine A N is authorized to access the corresponding data.

If step indicates that a DID collision exists the flow may continue to step . Step may comprise generating an alternative DID for the data. As disclosed above the alternative DID may be generated deterministically such that subsequent DID collisions involving the same data will result in the same alternative DID which may allow data subject to the DID collision to be de duplicated between multiple virtual machines A N. The flow may continue to step which may comprise admitting the data into the de duplication cache as disclosed herein.

Step may comprise acknowledging that the data was admitted into the de duplication cache. The acknowledgement may comprise the alternative DID of the data generated at step if applicable .

Step may comprise determining whether the file should be admitted into the de duplication cache and or whether the file is excluded from admission. Step may comprise the de duplication policy module A N evaluating de duplication admission policy A N. The de duplication admission policy A N may comprise file selection criteria such as the file selection criteria of disclosed above. The de duplication admission policy A N may further comprise an exclusion list identifying files that should not be admitted into the de duplication cache e.g. are unsuitable for de duplication and or are being cached by other layers of the CMS A N . The exclusion list may be dynamically populated in response to cache configuration and or activity of the CMS A N. If the file is suitable for admission to the de duplication cache and is not otherwise excluded the flow may continue to step otherwise the flow may continue at step . Step may comprise allowing the I O request to be serviced using another cache layer e.g. CMS A N primary storage or the like. Accordingly step may comprise ignoring the I O request at the VMDM A N.

Step may comprise attempting to access the requested file data at the de duplication cache and servicing the I O request at step as disclosed herein.

Step may comprise admitting the file data into the de duplication cache as disclosed herein. Step may comprise servicing the I O request using primary storage and reading the file data and generating a corresponding DID in a separate thread or processes. Step may further comprise providing the file data and DID to de duplication cache for admission at step . If the file data is successfully admitted into the de duplication cache the flow may end otherwise the flow may continue at step .

Step may comprise adding an identifier of the file e.g. the UFID of the file to an exclusion list or other data structure which may allow other cache services such as the CMS A N to cache the file and may prevent the VMDM A N from repeatedly attempting to admit the file into the de duplication cache . In some embodiments step may only include the file in the exclusion list in response to certain failure conditions from the de duplication cache . For example the de duplication cache may indicate that the file data may be admitted later e.g. it cannot be admitted now but may be admitted if when more cache space is available . In this situation the file may not be included on the exclusion list. Alternatively the de duplication cache may indicate that the file is unlikely to ever be admitted e.g. due to a DID collision or the like . In response to such an indication the file may be added to the exclusion list to prevent repeated error conditions and or to allow other caching services to attempt to handle the file. In other embodiments step may comprise evaluating one or more thresholds such as a retry count latency metric or the like to determine whether to add the file to the exclusion list.

Step may comprise providing access to the cache file data to the virtual machines A N such that subsequent requests for the files generated by the virtual machines A N are serviced using a single copy of the corresponding files within the cache storage .

Step may comprise one or more of the virtual machines A N storing a snapshot of the de duplication index A N as disclosed above phish25

Step may further comprise loading the snapshot A N following one or more of a reboot restart power cycle and or migration operation. Loading the snapshot A N may comprise populating the de duplication index A N with the contents of the snapshot A N. As disclosed above loading the snapshot A N may allow the virtual machines to identify files that have been admitted into the de duplication cache without re reading and or re admitting the files. For example at step the virtual machines may admit operating system files into the de duplication cache which may result in a boot storm as described above. At step and upon rebooting the virtual machines A N may access their respective snapshots A N populate the de duplication index A N and access the operating system files from the de duplication cache which may significantly reduce boot storm issues.

The operations of the fast path may include but are not limited to servicing the I O request in the storage stack by inter alia reading the file data from primary storage or other storage resource. The operations of the slow path may include but are not limited to a reading the file data from the primary storage by use of inter alia the storage stack b determining a DID of the file data c communicating a cache admission request to the de duplication cache that includes the file data and the DID via the communication link provided by the DC interface and cache interface d allocating de duplication cache tag s for the file data and e storing the file data in the cache storage.

As disclosed above in some embodiments virtual machines A N may be configured to be transferred between hosts . Transferring a virtual machine A N may comprise retaining and or transferring cache state of the virtual machine A N which may include but is not limited to the de duplication index A N de duplication cache tags and or data admitted into the de duplication cache .

Each virtual machine may be assigned a respective VMID. The VMID may be assigned when the virtual machine is instantiated on a host A N e.g. during an initialization and or handshake protocol . The VMID may comprise a process identifier thread identifier or any other suitable identifier. In some embodiments the VMID may uniquely identify the virtual machine on a particular host A N and or within a within a group of hosts A N. For example the hosts A N may operate within the same namespace such as a cluster and the VMID of each virtual machine may be unique within the namespace of the cluster unique across the virtual machines A N deployed on hosts A N in the cluster . In some embodiments the VMID may comprise a host identifier such as a Media Access Control MAC address network address distinguished name or the like. The VMID may comprise an identifier assigned by the virtualization kernel hypervisor host A N or the like. Accordingly in some embodiments a VMID may uniquely identify a virtual machine in a particular namespace and may identify the host A N upon which the virtual machine is currently deployed or was previously deployed . Alternatively or in addition each virtual machine may be configured to maintain a current host identifier and a previous host identifier.

In some embodiments one or more of the virtual machines may be capable of being relocated and or transferred between the hosts A N. For example a virtual machine X may be migrated from the host A to the host B e.g. in a VMotion or similar operation . The systems apparatus and methods disclosed herein may provide for migrating the cache state of the virtual machine X from the host A to the host B including cache state pertaining to the de duplication cache A N. Migrating the cache state of the virtual machine X may comprise migrating cache metadata e.g. cache tags X A and or de duplication index A A to the host B migrating data of the virtual machine X that has been admitted into the cache storage A of the host A cache data X A and or de duplication data A and the like. Accordingly transferring the virtual machine X from host A to host B may comprise retaining and or transferring portions of the cache state of the virtual machine X to the destination host B.

In the embodiment the virtual machine X comprises a CMS X which as disclosed herein may be configured to selectively service I O operations of the virtual machine X by use of the virtual machine cache A of the host A and or in accordance with cache resources dynamically allocated to the virtual machine X on the host A e.g. cache storage X A . The CMS X may comprise an I O driver and or filter X which may be configured to monitor I O operations within the virtual machine X and or provide a communication link not shown between the CMS X and the virtual machine cache A of the host A. The CMS X may be configured to maintain cache metadata including the cache tags X A in accordance with the cache resources allocated to the virtual machine X by the cache provisioner module A. As depicted in the cache tags X A may be maintained within the virtual machine X e.g. within the local memory space of the virtual machine X .

The cache tags X A may correspond to cache data X A stored in physical storage locations of the cache storage A e.g. cache chunks and or pages . The cache data X A may be associated with identifiers of the cache tags X A and or the VMID of the virtual machine X by a map module as disclosed above.

The virtual machine X may further comprise a VMDM X which may be configured to identify files suitable for admission into the de duplication cache A of the hosts A as disclosed herein. The VMDM X may be further configured to maintain a de duplication index denoted X A on host A . The de duplication index X A may comprise metadata pertaining to the files that have been admitted into the de duplication cache A of the host A by VMDM X.

The virtual machine X may be transferred from the host A to the host B. Transferring the virtual machine X may comprise transferring a current operating state of the virtual machine X including a current memory image or state of the virtual machine X from the host A to the host B. The memory image of the virtual machine X may include but is not limited to contents of the memory stack heap virtual memory and so on. Accordingly in the embodiment the cache tags X A may be automatically transferred to the host B with the virtual machine X denoted X B on host B . In addition the contents of the de duplication index may be automatically transferred with the memory image of the virtual machine X denoted X B on host B .

Transferring the cache tags X A to host B may comprise incorporating the cache tags X B in accordance with cache resources allocated to the virtual machine X on the host B which may comprise adding and or removing portions of the cache tags X B on the host B in accordance with the cache resources allocated to the virtual machine X by the cache provisioner module B of host B.

Transferring the cache state of the virtual machine X may further comprise transferring the cache data X A to which the cache tags X B refer. Transferring the cache data X A may comprise retaining the cache data X A of the virtual machine X on the host A in response to the virtual machine X being transferred therefrom requesting portions of the retained cache data X A from the host A and or transferring portions of the cache data X A between the hosts A and B.

Transferring the cache state of the virtual machine X may further comprise transferring de duplication data X A of the virtual machine X. The data X A may correspond to file data admitted into the de duplication cache A and stored within the cache storage A in response to requests from the VMDM X of the virtual machine X as disclosed above. Transferring the cache state may further comprise retaining and or transferring de duplication cache state such as de duplication cache tags X A . As disclosed above the de duplication cache tags X A pertaining to the virtual machine X may identify files in the de duplication cache A that the virtual machine X is authorized to access e.g. files that are accessible to the virtual machine X per the access list fields of the de duplication cache tags X A . Accordingly the de duplication cache tags X A and or portions thereof may be transferred to the destination host B as part of the cache state of the virtual machine X. The cache state transferred to the host B may further include the data to which the de duplication cache tags X A refer.

In some embodiments the virtual machine cache B at the host B may be configured to transfer portions of the cache state of the virtual machine X in response to determining that the virtual machine X was transferred to the host B from another host A. The virtual machine cache B may be configured to identify the transferred virtual machine X and or determine that the virtual machine X is being transferred to the host B before the virtual machine X arrives thereon. In some embodiments the virtual machine cache B may be notified that the virtual machine X is being migrated to the host B. The notification may be generated by the previous host A the virtual machine cache A the virtualization kernel A or B a management process or entity or the like. The notification may comprise the VMID of the virtual machine X cache requirements of the virtual machine X and so on.

In some embodiments the virtual machine cache B identifies that the virtual machine X was transferred to the host B in response to receiving a cache request from the virtual machine X e.g. via the VLUN driver and or communication link . After being transferred to the host B the CMS X and or VMDM X may continue to attempt to service I O operations using the VM cache which may comprise monitoring I O within the virtual machine X using inter alia the I O driver X and or directing selected I O requests to the virtual machine cache B and or de duplication cache B via the VLUN disk X. The requests however may reference cache resources and or cache data X A X A within the cache storage A of the host A that are not available on the host B. The requests may further comprise the VMID of the transferred virtual machine X. The virtual machine cache B may determine that the virtual machine X was transferred to the host B in response to receiving such requests the virtual machine cache B may determine that no cache space has been allocated to a virtual machine X associated with the VMID provided in the request s the cache provisioner module B has not allocated cache storage X B for the virtual machine X and so on. In addition the de duplication cache B may determine that the VMID of the virtual machine X does not appear on any of the access list s and or de duplication cache tags X B of the host B. In some embodiments the virtual machine cache B may determine that the virtual machine X was transferred to the host B based on a host identifier of the VMID. The host identifier may reference the host A whereas the host identifier of a newly powered on virtual machine on the host B may comprise a host identifier of the host B or may be blank . Alternatively or in addition the virtual machine X may comprise a separate host identifier which may reference host A and may be accessed in the handshake protocol with the virtual machine cache B.

In response to identifying the transferred virtual machine X the virtual machine cache B may initiate a handshake protocol. The handshake protocol may comprise allocating cache storage resources to the virtual machine X e.g. cache space X B by the cache provisioner module B. The amount of cache storage to allocate to the virtual machine X may be based on the size of the cache storage allocated to the virtual machine X on the host A cache storage X A the size of the working set of the virtual machine X e.g. the number of cache tags X B available cache resources and so on. The cache provisioner module B may attempt to allocate sufficient cache storage X B to support the retained cache tags X A . If sufficient cache storage cannot be allocated the CMS X may be configured to modify the retained cache tags X B in accordance with the new cache storage allocation X B . If excess cache resources are available the CMS X may be configured to add new tags to the retained cache tags X B . The allocation may be communicated through a virtual disk X and or I O driver X e.g. SCSI filter driver as described above. Allocating cache storage may further comprise allocating cache resources for the de duplication cache B. The cache provisioner module B may be configured to attempt to allocation sufficient cache resources B to support the contents of the de duplication index X B of the virtual machine X.

Transferring the cache state of the virtual machine X may comprise transferring portions of the cache data stored within the cache storage A of the host A cache data X A to the host B. In some embodiments the virtual machine cache A may comprise a retention module A which may be configured to retain cache data X A of the virtual machine X after the virtual machine X is transferred from the host A. The cache data X A may be retained for a retention period and or until the virtual machine cache A determines that the retained cache data X A is no longer needed. The retention module A may determine whether to retain the cache data X A and or determine the cache data retention period based upon various retention policy considerations including but not limited to availability of cache storage A availability of cache storage B relative importance of the retained cache data X A as compared to cache requirements of other virtual machines whether the cache data X A is available in the primary storage system or other backing store a cache mode and or persistence level of the cache data X A and so on. For example cache data stored in a write never cache mode cache data that has not been written through to the primary storage system may only be available on the original virtual machine cache A. The cache retention module A may be configured to prioritize retention of write never cache data until the write never cache data is transferred to the new host B. By contrast cache data stored in different cache modes e.g. write through and or write back cache mode may have a lower retention priority since this data will also be available from the primary storage system . In some embodiments the retention policy comprises a transfer threshold the retained cache data X A may be retained until a threshold amount of the retained cache data X A has been transferred. Alternatively or in addition the retained cache data X A may be removed as it is transferred to the host B e.g. portions transferred to the host B may be immediately removed from the cache storage A of host A .

As disclosed above the CMS X of the virtual machine X may be configured to retain cache metadata the cache tags X B at the host B despite the fact that the cache storage B does not comprise the cache data to which the cache tags X B refer. Although the virtual machine X may have been allocated cache resources X B at the host B the newly allocated resources may not be populated with cache data X A of the virtual machine X. As described in further detail herein the virtual machine cache B may be configured to populate the cache storage X B with cache data X A transferred from the cache storage A and or from the primary storage system to reconstruct the working set of the virtual machine X at the host B.

The virtual machine cache B may comprise a cache transfer module B which may be configured to access cache data X A of the virtual machine X at the previous host A. The cache transfer module B may be configured to identify the previous host A by use of the VMID e.g. accessing a previous host identifier maintained by the virtual machine X by interrogating the virtual machine X querying the virtualization kernel B or other entity or the like. The cache transfer module B may use the host identifier and or host addressing information request portions of the retained cache data X A from the host A via the network . In some embodiments the cache transfer module B is configured to determine and or derive a network address and or network identifier network name or reference of the host A from the host identifier.

The virtual machine cache A may comprise a cache transfer module A that is configured to selectively provide access to retained cache data X A of the virtual machine X. In some embodiments the cache transfer module A is configured to secure the retained cache data X A . For example the cache transfer module A may be configured to verify that the requesting entity e.g. the virtual machine cache B is authorized to access the retained cache data X A which may comprise verifying that the virtual machine X has been deployed on the host B and or verifying that requests for the retained cache data X A are authorized by the virtual machine X or other authorizing entity . For example the cache transfer module A may request a credential associated with the transferred virtual machine X such as the VMID or the like. Alternatively or in addition the cache transfer module A may implement a cryptographic verification which may comprise verifying a signature generated by the transferred virtual machine X or the like.

The cache data X A may be transferred between the hosts A and B using various mechanisms including but not limited to push transfers demand paging transfers prefetch transfers bulk transfers or the like.

A push transfer may comprise the cache transfer module A pushing cache data X A of the virtual machine X to the host B without receiving a request for the cache data X A e.g. before the host B requests the cache data X A . The cache transfer module A may be configured to push cache data X A of the virtual machine X in response to determining that the virtual machine X is to be transferred to the host B. The cache data X A may be pushed to the new host B before the transfer actually takes place before the transfer is complete and or before the virtual machine X initiates a handshake protocol at the new host B. Pushing the cache data X A may serve to notify the virtual machine cache B that the virtual machine B is being transferred thereto. In response the virtual machine cache B may preemptively allocate cache resources X B for the virtual machine X and or begin populating the cache with the cache data X A pushed from the host A.

A demand paging transfer may comprise transferring retained cache data X A in response to I O requests generated by the virtual machine X after deployment on the host B e.g. on demand . The transferred cache data X A may be used to service the I O requests. In addition the transferred cache data X A may be admitted into the cache storage B of the new host B. Alternatively the transferred cache data X A may be admitted at a later time or not at all in accordance with cache policy and or cache resource allocations at the host B.

A prefetch transfer may comprise transferring cache data X A according to a prefetch cache policy e.g. by proximity or the like . The amount and or extent of cache data X A to prefetch from the host A may be determined by inter alia cache metadata of the CMS X e.g. cache aging metadata hotness and so on . Accordingly in some embodiments the cache transfer module B may be configured to query the CMS X to identify cache data X A for prefetch if any and or prioritize prefetch operations.

A bulk transfer may comprise transferring cache data X A in bulk independent of I O operations of the virtual machine X. A bulk transfer may comprise populating the entire cache storage X B allocated to the virtual machine X at host B. Alternatively a bulk transfer may comprise populating a subset of the cache storage X B which as disclosed above may be selected based upon cache metadata of the virtual machine CMS X and or determined by differences in cache resources allocated to the virtual machine X at the hosts A and B.

The cache transfer module B may be further configured to prioritize cache transfers e.g. prefetch and or bulk transfers in accordance with the cache mode and or state of the cache data X A . For example data that is cached in a write never cache mode or write back cache mode and is not yet backed in primary storage may only be available from the previous host A and as such may be prioritized over data that may be available from alternative sources e.g. primary storage system . Therefore the cache transfer module B may be configured to prefetch and or bulk transfer certain portions of the cache data X A rather than waiting for on demand paging or the like.

The cache storage module B may be configured to selectively admit cache data X A into the cache X B . The cache storage module B may be further configured to populate the cache data X B from other sources such as the primary storage system other hosts N or the like. The cache storage module B may be configured to associate the cache data X B with the identifiers of the retained cache tags X B such that the references in the retained cache tags X B remain valid per the mappings implemented by the map module as disclosed above.

In response to requests for cache data X A of the virtual machine X the cache transfer module A may be configured to identify the requested cache data using inter alia the VMID of the transferred virtual machine X by use of the map module . The cache transfer module A may transfer the requested cache data X A if available to the cache transfer module B via the network .

The cache transfer module B may be configured to populate the cache data X B from various other sources such as the primary storage system or other shared storage resources. The cache transfer module B may select the source of the cache data based upon various policy considerations e.g. a cache transfer policy which may include a network policy bandwidth policy host resource policy primary storage resource policy and the like. For example in response to determining that the network is highly congested the cache transfer module B may be configured to reduce the amount of data to transfer defer a bulk transfer and or transfer the cache data from another source that is independent of the network . Similarly the cache transfer module B may direct requests to the host A as opposed to the primary storage system in response to determining that the primary storage system is heavily loaded and or has limited available bandwidth. Certain types of data however may only be available from the host A. For instance write never and or write back cache data that has not yet been written through to the primary storage system may only be available from the host A. The cache transfer module B may be configured to identify such data and to prioritize such data transfers to reduce the chance of data loss.

The retained cache data X A may represent cache resources that cannot be used by the other virtual machines A N operating on the host A. As such the cache retention module A may be configured to selectively remove the retained cache data X A when it is no longer needed and or according to a retention policy. The retention policy may be determined based upon the retention policy factors described above. In some embodiments the cache transfer module B is configured to inform the host A of cache data that has been transferred to the host B from other sources so that the cache retention module A can remove the corresponding retained cache data X A from the cache storage A. The cache transfer module B may be further configured to inform the host A of other conditions in which the cache data X A no longer needs to be retained such as when the data is modified overwritten deleted e.g. TRIMed and or evicted from the cache storage B at the host B. For example upon being transferred to the host B the virtual machine X may perform a storage operation to delete or TRIM data corresponding to cache data X A retained at host A. In response the cache transfer module B may inform the host A that the corresponding cache data X A no longer needs to be retained within the cache storage A.

As disclosed above in some embodiments the cache transfer module A may be configured to push cache data X A to the host B. Pushing cache data may comprise transferring retained cache data X A to the cache transfer module B and or cache storage module B without receiving a request independent of requests for the cache data X A . The cache transfer module A may determine the host identifier of the new host B through user configuration the verification process described above active polling by the cache transfer module A a call back implemented by the transferred virtual machine X or the like. In some embodiments the virtual machine cache B of the host B may identify that the virtual machine X was transferred from the host A in response to receiving cache data pushed from the host A as described above. The cache transfer module A may be configured to selectively push high priority cache data such as write never cache data to prevent data loss.

Cache state pertaining to the de duplication cache A de duplication cache state may be transferred between hosts A B as disclosed above. The retention module A may be configured to retain de duplication cache tags X A and or de duplication data A at the host A in response to determining that the virtual machine X has transferred or is being transferred to the host B. The cache transfer module A may be configured to selectively transfer the de duplication cache state to the host B. The de duplication cache state may include but is not limited to the DIDs of files accessed by the virtual machine X de duplication cache tags X A pertaining to the virtual machine X and or contents of the de duplication cache X A de duplication cache data X A . Transferring the de duplication cache state may comprise one or more push transfers demand paging transfers prefetch transfers and or bulk transfers.

As disclosed above transferring de duplication cache state to the host B may allow the virtual machine X to access data in the de duplication cache B immediately. For example transferring the DIDs of files accessed by the virtual machine X may allow the de duplication cache B to determine which files in the de duplication cache B the virtual machine X is authorized to access without first requiring the virtual machine X to re admit the data. Accordingly transferring the de duplication cache state may reduce the overhead on I O resources primary storage and network and or ameliorate boot storm issues.

In some embodiments the cache transfer module A and or B is configured to transfer de duplication cache tags X A before transferring the corresponding de duplication data A . The de duplication cache B of the host B may use the de duplication cache tags X A to determine whether the corresponding file data has already be admitted into the de duplication cache B based on the DIDs of the de duplication cache tags X A . Determining whether the corresponding file data has already been admitted may further comprise verifying that purported matches between the DIDs is not due to a DID collision as disclosed above. In some embodiments the cache transfer modules A and or B may be configured to perform a byte by byte comparison between the portions of the de duplication cache data A and the de duplication cache data B . Alternatively the cache transfer modules A and or B may be configured to determine and or exchange a larger higher security DID values in lieu of a byte by byte comparison. Files already admitted into the de duplication cache of the host B may not need to be transferred to the host B and or read from the primary storage . The de duplication cache B may be configured to update the de duplication cache tags X B in response to the de duplication cache tags X A transferred from the host A which may comprise updating access list information and so on to indicate that the virtual machine X is authorized to access particular file data in the de duplication cache B as disclosed above. Accordingly the virtual machine X may be able to access such files without first attempting to re admit data of the files into the de duplication cache B of the host B.

Further embodiments of systems and methods for transferring cache state are disclosed in U.S. patent application Ser. No. 13 687 979 to Vikram Joshi et al. filed Nov. 28 2012 and entitled Systems Methods and Apparatus for Cache Transfers and which is incorporated by reference.

Reference throughout this specification to features advantages or similar language does not imply that all of the features and advantages that may be realized are included any single embodiment. Rather language referring to the features and advantages is understood to mean that a specific feature advantage or characteristic described in connection with an embodiment is included in at least one embodiment. Thus discussion of the features and advantages and similar language throughout this specification may but do not necessarily refer to the same embodiment.

The embodiments disclosed herein may involve a number of functions to be performed by a computer processor such as a microprocessor. The microprocessor may be a specialized or dedicated microprocessor that is configured to perform particular tasks according to the disclosed embodiments by executing machine readable software code that defines the particular tasks of the embodiment. The microprocessor may also be configured to operate and communicate with other devices such as direct memory access modules memory storage devices Internet related hardware and other devices that relate to the transmission of data in accordance with various embodiments. The software code may be configured using software formats such as Java C XML Extensible Mark up Language and other languages that may be used to define functions that relate to operations of devices required to carry out the functional operations related to various embodiments. The code may be written in different forms and styles many of which are known to those skilled in the art. Different code formats code configurations styles and forms of software programs and other means of configuring code to define the operations of a microprocessor in accordance with the disclosed embodiments.

Within the different types of devices such as laptop or desktop computers hand held devices with processors or processing logic and also possibly computer servers or other devices that utilize the embodiments disclosed herein there exist different types of memory devices for storing and retrieving information while performing functions according to one or more disclosed embodiments. Cache memory devices are often included in such computers for use by the central processing unit as a convenient storage location for information that is frequently stored and retrieved. Similarly a persistent memory is also frequently used with such computers for maintaining information that is frequently retrieved by the central processing unit but that is not often altered within the persistent memory unlike the cache memory. Main memory is also usually included for storing and retrieving larger amounts of information such as data and software applications configured to perform functions according to various embodiments when executed by the central processing unit. These memory devices may be configured as random access memory RAM static random access memory SRAM dynamic random access memory DRAM flash memory and other memory storage devices that may be accessed by a central processing unit to store and retrieve information. During data storage and retrieval operations these memory devices are transformed to have different states such as different electrical charges different magnetic polarity and the like. Thus systems and methods configured disclosed herein enable the physical transformation of these memory devices. Accordingly the embodiments disclosed herein are directed to novel and useful systems and methods that in one or more embodiments are able to transform the memory device into a different state. The disclosure is not limited to any particular type of memory device or any commonly used protocol for storing and retrieving information to and from these memory devices respectively.

Embodiments of the systems and methods described herein facilitate the management of data input output operations. Additionally some embodiments may be used in conjunction with one or more conventional data management systems and methods or conventional virtualized systems. For example one embodiment may be used as an improvement of existing data management systems.

Although the components and modules illustrated herein are shown and described in a particular arrangement the arrangement of components and modules may be altered to process data in a different manner. In other embodiments one or more additional components or modules may be added to the described systems and one or more components or modules may be removed from the described systems. Alternate embodiments may combine two or more of the described components or modules into a single component or module.

