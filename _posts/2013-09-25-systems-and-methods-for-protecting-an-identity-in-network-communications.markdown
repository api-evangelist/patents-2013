---

title: Systems and methods for protecting an identity in network communications
abstract: In some embodiments, a method includes sending a first data unit, received from a source device, to a destination device via a first data unit path. The first data unit path includes (1) a first virtual machine and a second virtual machine that are included in a first network, and (2) a third virtual machine that is included in a second network. Furthermore, the first data unit path includes the first virtual machine, the second virtual machine, and the third virtual machine in a first order. The method includes sending a second data unit, received from the source device, to the destination device via a second data unit path from the source device to the destination device. The second data unit path includes each of the first virtual machine, the second virtual machine, and the third virtual machine in a second order different from the first order.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08984138&OS=08984138&RS=08984138
owner: Cutting Edge Consulting Associates, Inc.
number: 08984138
owner_city: Chantilly
owner_country: US
publication_date: 20130925
---
The present invention is a continuation of U.S. patent application Ser. No. 13 961 379 filed on Aug. 7 2013 entitled Systems and Methods for Protecting an Identity in Network Communications which claims priority to U.S. Provisional Application No. 61 732 664 filed on Dec. 3 2012 entitled Systems and Methods for Protecting an Identity in Network Communications the contents of both which are herein incorporated by reference in their entirety.

Known methods of communications within and across networks having commercial clouds is typically through routing at the network layer based on an external internet protocol IP address. The network will forward the packets to the requested IP address via the most expedient path available. This communications method however fails to provide for the ability to route via one or more intermediate locations to mask the source or identity of the communications.

Thus a need exists for additional routing techniques that provide an automated ability to control the routing of communications within and across networks having commercial clouds and networks.

In some embodiments a method includes sending a first data unit received from a source device to a destination device via a first data unit path. The first data unit path includes 1 a first virtual machine and a second virtual machine that are included in a first network and 2 a third virtual machine that is included in a second network. Furthermore the first data unit path includes the first virtual machine the second virtual machine and the third virtual machine in a first order. The method includes sending a second data unit received from the source device to the destination device via a second data unit path from the source device to the destination device. The second data unit path includes each of the first virtual machine the second virtual machine and the third virtual machine in a second order different from the first order.

In some embodiments an apparatus includes a network layout module configured to be operatively coupled to a first network that includes a first virtual machine and a second virtual machine and that is configured to be operatively coupled to a source device and a destination device. The network layout module is configured to be operatively coupled to a second network different from the first network that includes a third virtual machine and a fourth virtual machine and that is configured to be operatively coupled to the source device and the destination device. The network layout module configured to define at a first time a first data unit path from the source device to the destination device. The first data unit path includes each of the first virtual machine the second virtual machine the third virtual machine and the fourth virtual machine in a first order. The network layout module configured define at a second time after the first time and in response to an event a second data unit path from the source device to the destination device. The second data unit path includes each of the first virtual machine the second virtual machine the third virtual machine and the fourth virtual machine in a second order different from the first order.

In some embodiments a method includes defining at a first time a first data unit path from a source device to a destination device the first data unit path includes 1 a first virtual machine and a second virtual machine that are included in a first network and 2 a third virtual machine that is included in a second network. The first data unit path includes the first virtual machine the second virtual machine and the third virtual machine in a first order. The method includes modifying in response to defining the first data unit path a routing table such that a first data unit sent from the source device to the destination device prior to a second time after the first time follows the first data unit path. The method includes defining after the second time a second data unit path from the source device to the destination device. The second data unit path includes each of the first virtual machine the second virtual machine and the third virtual machine in a second order different from the first order. The method includes modifying in response to defining the second data unit path the routing table such that a second data unit sent from the source device to the destination device after the second time follows the second data unit path.

In some embodiments a method includes sending a first data unit received from a source device to a destination device via a first data unit path. The first data unit path includes 1 a first virtual machine and a second virtual machine that are included in a first network and 2 a third virtual machine that is included in a second network. Furthermore the first data unit path includes the first virtual machine the second virtual machine and the third virtual machine in a first order. The method includes sending a second data unit received from the source device to the destination device via a second data unit path from the source device to the destination device. The second data unit path includes each of the first virtual machine the second virtual machine and the third virtual machine in a second order different from the first order.

Some embodiments provide cloud based secure virtual private networks VPNs and identity persona protected access to the Internet. Some embodiments provide support for customer migration to the cloud a range of identity management and persona attribution options secure network connectivity and control of customer information held within commercial cloud providers databases. Some embodiments can distribute the communications activity data storage and work dynamically within and across multiple clouds simultaneously also collectively referred to as the network . Some embodiments can also provide a mix of cloud transport application location and data storage options. Some embodiments can be implemented to provide each customer with the ability to define their own dedicated network s within a service infrastructure. In addition some embodiments can augment the multi layered security capabilities provided by the commercial cloud providers with an additional layer of encryption and implementation of dynamic security policies that prohibit malicious traffic from entering the network. With such embodiments companies can take advantage of all the benefits of cloud technology while improving the overall resiliency security and exposure on the Internet.

In one embodiment user identity protected access to the Internet can be provided using commercial clouds for transport purposes. This embodiment can provide enhanced capabilities for customer access to the Internet with a range of identity persona management options geolocation sites and control of the customer information held within commercial providers databases. This embodiment can distribute the communications activity dynamically within and across multiple clouds simultaneously and regularly churn the underlying network infrastructure. The dynamic transport of communications for Internet access across multiple commercial providers make actual user information and origination identities a very difficult target for hackers search engine optimization companies and other privacy threats.

In some embodiments users can have the option to connect into an identity protection system system within a network via a standard IP browser session OSI Layer 3 or though a Virtual Private Network connection OSI Layer 2 . Separate identities can be established to acquire infrastructure information e.g. servers bandwidth IP addresses from Cloud 1 and Cloud 2 providers. Layer 2 Internet Protocol Security IPSEC tunnels can be established between the service equipment Cloud 1 infrastructure and Cloud 2 infrastructure to ensure that the IP traffic is routed through the network as architected to appropriately protect the user information and origination identities.

To efficiently manage the complexity of establishing and maintaining the network for the service a virtualization platform for example the Nicira Network Virtualization Platform NVP can be used. NVP is a distributed software suite that defines scalable fully featured isolated virtual networks that are completely decoupled and independent from the physical network. In some embodiments other virtualization platforms can be used.

Known uses of the Nicira NVP product address the barriers of scaling network infrastructure in data centers to keep pace with the rapid expansion of cloud technology. Network virtualization decouples network services from the underlying physical hardware enabling programmatic creation of agile virtual networks to complement the implementation of storage and compute virtualization.

Embodiments described herein can use features of the standard Nicira NVP product. Such embodiments can establish a logical Layer 2 network across the physical infrastructure the system Cloud 1 and Cloud 2 through use of NVP adding Virtual Machines VMs connected to the NVP logical networks on a hypervisor at each point of the network. Such embodiments can develop specific routing instructions for each VM to force the communications to follow the intended path.

As discussed above embodiments of the system can provide identity protection by using a virtualization platform for example the Nicira Network Virtualization Platform NVP . A virtualization platform can be a network control plane that can enable programmatic control of networking capabilities within multi tenant cloud data centers. Just as server virtualization can provide flexible control of virtual machines VMs running on a pool of server hardware NVP s network virtualization and other virtualization platforms can provide a centralized application programming interface API to provision and configure one or more isolated logical networks on top of a single physical network or more than one physical networks. Logical networks can decouple VM connectivity and network services from the physical network and can give cloud providers the flexibility to place or migrate VMs anywhere within data centers or spanning multiple data centers while still supporting Layer 2 L2 connectivity and other enterprise class network capabilities.

NVP can provide two network views 1 the logical network view and 2 the transport network view. The logical network view can describe the connectivity and network services a VM identifies in the cloud. This view can be independent of the underlying physical devices of the data center s and can be independent of the most expedient path between a user and a network. The transport network view can describe the physical devices the hypervisors that host VMs and the physical network hardware that can connect the hypervisor machines that can handle the actual packet processing and forwarding that underlie the logical network.

In some embodiments the logical network view and the transport network view can exist simultaneously. NVP can act as a network hypervisor that can virtualize the transport network view devices to faithfully implement the network view. This task can be analogous to how an x86 based hypervisor server can provide each VM with a logical view of resources e.g. virtual CPU memory and disks by mapping virtual resources to physical server hardware.

When changes in the logical network view occur NVP can modify the forwarding state of all relevant Open Virtual Switch OVS devices in the transport network view to reflect the changes. Similarly when changes in the transport network view occur NVP can update the forwarding rules and state in the transport network view so the connectivity identified by the VMs remains constant with the logical network view.

At its core NVP can expose a centralized API that can be used to define the logical network view and to control it across the transport network view. This can make packet forwarding consistent with the logical description.

As described above the NVP can include a control plane and a data plane. With regard to the control plane a controller cluster module can provide the NVP API which can be exposed as a web service used to define the logical network view and can manage the forwarding state at all transport nodes that the controller cluster module manages that are running OVS. The controller cluster module can be a distributed system that runs on a set of x86 servers. The data plane can be implemented by access layer switching devices hypervisor based Open Virtual Switches that can be managed by the controller cluster module. Known network hardware not managed by the cluster can provide the bandwidth to interconnect these access layer devices enabling them to define network overlays that can implement the logical network view.

The virtualization platform can include NVP access switches that can support the Open vSwitch OVS management interface. OVS management interface can be an open protocol that enables control and visibility of packet forwarding by a switch. Endpoints connected to OVS enabled access switches can be operatively coupled to a logical network to send and receive data e.g. packets cells etc . Endpoints can include for example hypervisors and or gateways. A hypervisor can include a vSwitch that can be OVS enabled which can allow VM endpoints running on the hypervisor to be attached to a logical network. A gateway can attach a logical network to a physical network that is not managed by NVP e.g. the Internet. Each gateway can operate as an L2 gateway service that send data to a physical L2 segment or can operate as an L3 gateway service mapped to a physical router port.

In the data plane logical switches can be implemented using different encapsulation mechanisms which result in two different types of logical networks 1 overlay logical networks and 2 bridged logical networks. Overlay logical networks can use L2 in L3 tunneling such that logical network topology can be completely decoupled from transport network topology. Bridged logical networks may not use encapsulation and alternatively VM traffic on bridged logical networks can be sent directly on the physical network similar to traditional hypervisor networking. Overlay logical networks can include service nodes. A service node can be an OVS enabled x86 appliance that can be managed by the controller cluster module and can be used to provide extra packet processing capacity to implement logical networks.

The virtualization platform can separate management traffic e.g. traffic within the system that instantiates virtual machines defines and or updates routing tables etc from data traffic. Management traffic can be sent and received between OVS devices and the controller cluster module while data traffic can be sent and received directly between the OVS devices. The controller cluster module can remain outside the data path.

The virtualization platform can manage and or otherwise control the path that data takes through an identity protection system system . Said another way the virtualization platform can define a network layout of the system and can therefore control the path data takes through the defined network layout. An OVS management connection can use SSL for security and can provide the controller cluster module with an interface to 1 view the set of endpoints VMs or external networks present on a given OVS device 2 distribute the logical network forwarding state down to the OVS devices and 3 manipulate OVS packet forwarding to implement the logical network view.

The virtualization platform can be configured such that data traffic to from the VM or external network endpoints can be handled by OVS devices hypervisors gateways and service nodes . For example a hypervisor in one data center can tunnel packets to another hypervisor in a different data center which can provide L2 connectivity between two VMs that are on the same logical network. For another example a hypervisor can tunnel a packet to a service node which can encapsulate that packet and forward it using a secure VPN tunnel to a gateway in a remote customer premises.

As discussed above the virtualization platform can be used to define one or more networks included in a system. A network defined by the virtualization platform can include a transport network view and a logical network view.

The transport network view can include entities that communicate via an API and that can inform the controller cluster module about the set of OVS devices the controller cluster module should manage and how these OVS devices can communicate to create network overlays. These entities shown in the transport network view can includes transport nodes transport zones and transport connectors. A transport node can be any OVS device managed by the controller cluster module including hypervisors gateways and service nodes. A transport zone can be a physical network that can connect a set of transport nodes. Finally a transport connector can include the IP address es and encapsulation type s define by the controller cluster as attachments to be used by transport nodes to communicate with each other when implementing overlay logical networks. For bridged logical networks transport connectors can indicate the OVS Bridge a hypervisor should use when sending receiving decapsulated traffic.

The logical network view can include entities that communicate via an API and that can describe the network connectivity identified by endpoints VMs and external networks that are plugged into OVS access switches. API entities shown in the logical network view include logical switches logical switch ports logical routers and attachments. A logical switch can be an L2 switching overlay implemented using one of the supported encapsulation mechanisms GRE IPsecGRE STT IPsecSTT or Bridge . A logical switch port can be similar to a physical switch port and can enable the configuration of network services e.g. security profiles QoS policies and can expose data for monitoring or troubleshooting. A logical router e.g. a virtual L3 router can be implemented using one of the supported encapsulation mechanisms generic routing encapsulation GRE IPsecGRE spin transfer torque STT STT IPsecSTT or Bridge . Finally an attachment can include a logical wire e.g. a logical connection that associates a VM interface VIF attachment or external network L2 gateway attachment L3 gateway attachment with a logical port.

Identity protection systems described herein can be configured to protect identities and or locations herein identity identities in and accessing a network. For example identity protection systems can protect an identity of a user and or an organization and or the location of data stored and or executable . Identity protection systems can protect an identity of a private source from a public destination e.g. a destination on a wide area network such as the internet can protect the identities of a private source and or a private destination from a third party malicious or otherwise for example via a secure private network can protect the location of stored and or executable data and or can define isolated private networks.

As shown in cloud can include one or more core modules A B and one or more aggregation module A C. Similarly cloud can include one or more core modules A B and one or more aggregation module A C. While cloud and cloud are shown in as including two core modules and 3 aggregation modules each in other embodiments either of cloud and or cloud can include more or fewer core and or aggregation switches.

In some embodiments a virtualization platform e.g. NVP can be installed on one or more physical servers within each of cloud and cloud . In such embodiments the virtualization platform can be used to instantiate multiple virtual machines . In some embodiments a hypervisor transport node module can be installed to facilitate the data transport within each cloud. A hypervisor transport node module can include a hypervisor and Open vSwitch OVS . In such embodiments at least a portion of the multiple virtual machines can be operatively coupled to the hypervisor to facilitate system communications. In such embodiments routing instructions for system can be installed on each of the multiple virtual machines .

In some embodiments a gateway transport node module not shown can be installed to provide external connectivity to the Internet. The gateway transport node module can be used for routing between the logical networks and can allow for transport between cloud and or cloud and the Internet. In some embodiments physical connectivity between the hypervisor transport node module and gateway transport node module can be via internal IP addresses assigned by cloud and cloud .

In some instances control cluster module also referred to herein as network layout module can manage hypervisor transport node modules and gateway transport node modules. In such instances when a new hypervisor transport node module and or a gateway transport node module are instantiated control cluster can be notified to begin managing the respective module.

Transport node modules e.g. hypervisor transport node module and or a gateway transport node module can communicate with each other to implement logical networks. Transport connectors not shown in can define an encapsulation mechanism and identifiers used for those connections.

In some embodiments OVSs can expose bridge each of virtual machines A B A B to the hypervisor s physical network interface. In some embodiments each logical port can include an attachment that can act as a source sink of traffic sent in and out of the logical network. In some embodiments virtual machines A B A B can use a virtual interface Vif Attachment such that communications across the entire network are identified as being associated with one logical switch. This attachment can allow the virtual machines A B A B to move data from one port to another across the network. Each of virtual machines A B A B can include routing instructions to facilitate communication through the logical network of system . In some embodiments gateway transport node modules can use an L3 gateway attachment that can allow a connection of a logical switch port to a physical network interface exposed via a virtualization platform gateway service e.g. a node configured to allow communications between networks using the same and or different protocols .

As described above the path a packet takes through system can be dynamically changed based on a perceived threat on a predetermined schedule etc. In such instances the path can travel between any number of virtual machines across any number of clouds. In some such embodiments for example the path can include a first virtual machine in a first cloud then a first virtual machine in a second cloud then back to a second virtual machine in the first cloud then back to a second virtual machine in the second cloud. In such embodiments the routing tables can be updated to ensure proper routing within the network for legitimate traffic. In some embodiments because the path changes an intruder seeking to discern the location of a client website and or otherwise uncover a source of a packet would be unable to trace the packet via the original path because that path may change.

Systems and described above can be used in a variety of ways to protect identities as described herein. depict several embodiments of identity protection systems that can use systems similar to systems to protect identities. Specifically systems described herein can dynamically change a data path between a source and a destination 1 to protect an identity see e.g. 2 to protect the location and or contents of a storage module see e.g. 3 to define secure virtual private networks see e.g. and or 4 to isolate identity protected networks see e.g. .

Communication between entity and any of network elements of network can include one or more data paths via cloud cloud and or network and via one or more virtual machines VM not shown instantiated in cloud cloud and or network . For example communication between entity and any of network elements can be routed through a first VM in cloud a first VM in cloud a second VM in cloud a second VM in cloud and then to any of network elements . Furthermore as described herein the data path between entity and any of network elements can dynamically change on a schedule randomly pseudo randomly upon detection of a threat by an administrator of system etc.

In some embodiments the locations of network elements and or data paths between any of network elements can be dynamically changed. For example as shown in network element is associated with first cloud network element is associated with second cloud and network element is associated with network . In such an embodiment by way of example network element specifically the data program process stored data etc can be moved e.g. instantiated in second cloud .

As described above an attempt to identify the source location of a data transmitter and or a network element along an initial path may be prevented because a new path has been established within system . In such an example entity can access data stored at network entity . In the event that the data path between entity and network element is compromised and or if unauthorized to access network entity is attempted either the data path can have changed and or the location of network entity can have changed. In such embodiments network layout module can facilitate the updating and distribution of routing tables within system .

Communication between entity and any of network elements of network and between any of any of network elements and any of another network elements can include one or more data paths via cloud and or cloud network and via one or more virtual machines VM not shown instantiated in cloud and or cloud network . For example communication between entity and any of network elements can be routed through a first VM in cloud a first VM in cloud a second VM in cloud a second VM in cloud and then to any of network elements of network . Furthermore as described herein the data path between entity and any of network elements can dynamically change on a schedule randomly pseudo randomly upon detection of a threat by an administrator of system etc.

In some embodiments the locations of network elements and or data paths between any of network elements can be dynamically changed. For example as shown in network element is associated with first cloud network element is associated with second cloud and network element is associated with network . In such an embodiment by way of example network element specifically the data program process stored data etc can be moved e.g. instantiated in second cloud .

As described above entity can access data stored or use a program and or other process located at network entity . In the event that the data path between entity and network element is compromised and or if unauthorized access to network entity is attempted either the data path can change and or the location of network entity can change. In such embodiments network layout module can facilitate the updating and distribution of routing tables within system .

While shown and or described herein as including a number of cloud network other networks network elements entities etc in some embodiments there can be more or fewer network elements and or entities. For example while system is shown as including clouds and network elements in other instances system can include more or fewer clouds and or network elements.

While the networks e.g. local and legacy network wide area network clouds etc are shown and described herein in various logical and or physical configurations in some embodiments the networks can include different configuration. For example while depicts first cloud second cloud as overlapping in some embodiments first cloud and second cloud can be mutually exclusive logically and or physically. Similarly while depicts first cloud second cloud and network as mutually exclusive in some embodiments any of first cloud second cloud and network can be logically and or physically overlapping and or mutually exclusive with any other of first cloud second cloud and network .

Some embodiments described herein relate to devices e.g. wireless access points mobile communication devices with a non transitory computer readable medium also can be referred to as a non transitory processor readable medium or memory having instructions or computer code thereon for performing various computer implemented operations. The computer readable medium or processor readable medium is non transitory in the sense that it does not include transitory propagating signals per se e.g. a propagating electromagnetic wave carrying information on a transmission medium such as space or a cable . The media and computer code also can be referred to as code may be those designed and constructed for the specific purpose or purposes. Examples of non transitory computer readable media include but are not limited to magnetic storage media such as hard disks floppy disks and magnetic tape optical storage media such as Compact Disc Digital Video Discs CD DVDs Compact Disc Read Only Memories CD ROMs and holographic devices magneto optical storage media such as optical disks carrier wave signal processing modules and hardware devices that are specially configured to store and execute program code such as Application Specific Integrated Circuits ASICs Programmable Logic Devices PLDs Read Only Memory ROM and Random Access Memory RAM devices. Other embodiments described herein relate to a computer program product which can include for example the instructions and or computer code discussed herein.

While various embodiments have been described above it should be understood that they have been presented by way of example only and not limitation. Where methods and steps described above indicate certain events occurring in certain order the ordering of certain steps may be modified. Additionally certain of the steps may be performed concurrently in a parallel process when possible as well as performed sequentially as described above. Although various embodiments have been described as having particular features and or combinations of components other embodiments are possible having any combination or sub combination of any features and or components from any of the embodiments described herein.

