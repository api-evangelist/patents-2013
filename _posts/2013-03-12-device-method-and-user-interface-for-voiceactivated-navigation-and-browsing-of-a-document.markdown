---

title: Device, method, and user interface for voice-activated navigation and browsing of a document
abstract: The electronic device with one or more processors and memory receives a first document including a plurality of links. The electronic device outputs a voice reading of at least a portion of the first document, and outputs audible information identifying a link of the plurality of links. In response to outputting the audible information identifying the link, the electronic device receives from the user a voice command regarding the link, and, in response to receiving from the user the voice command, outputs a voice reading of at least a portion of a second document associated with the link.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09495129&OS=09495129&RS=09495129
owner: Apple Inc.
number: 09495129
owner_city: Cupertino
owner_country: US
publication_date: 20130312
---
This application claims priority to U.S. Provisional Patent Application Ser. No. 61 666 659 filed Jun. 29 2012 entitled Device Method and User Interface for Voice Activated Navigation and Browsing of a Document which is incorporated by reference herein in its entirety.

The disclosed embodiments relate generally to digital assistant systems and more specifically digital assistant systems that perform voice activated navigation and browsing of documents.

Just like human personal assistants digital assistant systems can perform requested tasks and provide requested advice information or services. A digital assistant system s ability to fulfill a user s request is dependent on the digital assistant system s correct comprehension of the request or instructions. Recent advances in natural language processing have enabled users to interact with digital assistant systems using natural language in spoken or textual forms. Such digital assistant systems can interpret the user s input to deduce the user s intent translate the deduced intent into actionable tasks and parameters execute operations or deploy services to perform the tasks and produce output that is intelligible to the user.

Such digital assistant systems may be configured to assist users who have limited accessibility to interact with electronic devices. For example people with impaired vision such as low vision users and blind users dyslexic users or others with learning disabilities or even sighted users who simply want or need to use a device without looking at the device during operation can benefit from digital assistant systems that read information to users. In another example for electronic devices with touch screens people with limited motor skills such as those with certain finger or hand impairments may find performing touch gestures on the touch screens difficult if not impossible. However digital assistant systems may receive voice commands thereby eliminating the need for touch gestures.

However navigation and browsing of documents remain cumbersome and inefficient thereby creating a significant cognitive burden on a user with impaired vision and or limited motor skills.

As described above there is a need for digital assistant systems to provide an improved user interface for navigation and browsing of documents to users with impaired vision and or limited motor skills. This enables users to efficiently navigate through and browse documents.

The embodiments disclosed herein provide methods systems and computer readable storage media that provide voice activated navigation and browsing of a document.

Some embodiments provide a method for navigating through documents performed at an electronic device with one or more processors and memory. This method includes receiving a first document comprising a plurality of links outputting a voice reading of at least a portion of the first document outputting audible information identifying a link of the plurality of links and in response to outputting the audible information identifying the link receiving from the user a voice command of a first type regarding the link. The method also includes in response to receiving from the user the voice command outputting a voice reading of at least a portion of a second document associated with the link.

In accordance with some embodiments a method for browsing a document performed at an electronic device with one or more processors and memory includes receiving a document having a plurality of portions where at least some of the portions are associated with respective metadata. The method also includes outputting a voice reading of respective portions of the document including audibly distinguishing the respective portions based on the respective metadata. The method further includes receiving from a user a voice command requesting navigation to a particular portion associated with particular metadata and in response to receiving the voice command outputting a voice reading of the particular portion associated with the particular metadata.

In accordance with some embodiments a method for identifying a set of documents performed at an electronic device with one or more processors and memory includes outputting a voice reading of at least a portion of a document of a plurality of documents. The method also includes while outputting the voice reading receiving from a user a voice command requesting a document corresponding to a particular criteria. The method further includes in response to receiving from the user the voice command identifying one or more documents of the plurality of documents that correspond to the particular criteria and outputting a voice reading of at least a portion of a respective document of the one or more identified documents.

In accordance with some embodiments an electronic device includes one or more processors and memory storing one or more programs for execution by the one or more processors. The one or more programs include instructions for performing the operations of any of the methods described above. In accordance with some embodiments a graphical user interface on an electronic device with a display memory and one or more processors to execute one or more programs stored in the memory includes one or more of the elements displayed in any of the methods described above which are updated in response to inputs as described in any of the methods above. In accordance with some embodiments a computer readable storage medium has stored therein instructions which when executed by an electronic device with one or more processors and memory cause the device to perform the operations of any of the methods described above. In accordance with some embodiments an electronic device includes means for performing the operations of any of the methods described above. In accordance with some embodiments an information processing apparatus for use in an electronic device includes means for performing the operations of any of the methods described above. In accordance with some embodiments an electronic device includes a processing unit configured to perform the operations of any of the methods described above.

In accordance with some embodiments an electronic device includes an audio input unit configured to receive audio inputs. The electronic device also includes an audio output unit configured to output audible information. The electronic device includes a processing unit coupled to the audio input unit and the audio output unit. The processing unit is configured to receive a first document comprising a plurality of links. The processing unit is configured to output a voice reading of at least a portion of the first document. The processing unit is configured to output audible information identifying a link of the plurality of link. The processing unit is configured to in response to outputting the audible information identifying the link receive from the user a voice command regarding the link. The processing unit is configured to in response to receiving from the user the voice command output a voice reading of at least a portion of a second document associated with the link.

In accordance with some embodiments an electronic device includes an audio input unit configured to receive audio inputs. The electronic device also includes an audio output unit configured to output audible information. The electronic device includes a processing unit coupled to the audio input unit and the audio output unit. The processing unit is configured to receive the document having a plurality of portions wherein at least some of the portions are associated with respective metadata. The processing unit is configured to output a voice reading of respective portions of the document including audibly distinguishing the respective portions based on the respective metadata. The processing unit is configured to receive from a user a voice command requesting navigation to a particular portion associated with particular metadata. The processing unit is configured to in response to receiving the voice command output a voice reading of the particular portion associated with the particular metadata.

In accordance with some embodiments an electronic device includes an audio input unit configured to receive audio inputs. The electronic device also includes an audio output unit configured to output audible information. The electronic device includes a processing unit coupled to the audio input unit and the audio output unit. The processing unit is configured to output a voice reading of at least a portion of a document of a plurality of documents. The processing unit is configured to while outputting the voice reading receive from a user a voice command requesting a document corresponding to a particular criteria. The processing unit is configured to in response to receiving from the user the voice command identify one or more documents of the plurality of documents that correspond to the particular criteria and output a voice reading of at least a portion of a respective document of the one or more identified documents.

Thus digital assistant systems are provided with new and improved methods that enable navigation and browsing of documents thereby improving the user interfaces for users with limited accessibility. Such methods and systems may complement or replace existing methods and systems.

Specifically a digital assistant system is capable of accepting a user request at least partially in the form of a natural language command request statement narrative and or inquiry. Typically the user request seeks either an informational answer or performance of a task by the digital assistant system. A satisfactory response to the user request is generally either provision of the requested informational answer performance of the requested task or a combination of the two. For example a user may ask the digital assistant system a question such as Where am I right now Based on the user s current location the digital assistant may answer You are in Central Park near the west gate. The user may also request the performance of a task for example by stating Please invite my friends to my girlfriend s birthday party next week. In response the digital assistant may acknowledge the request by generating a voice output Yes right away and then send a suitable calendar invite from the user s email address to each of the user friends listed in the user s electronic address book. There are numerous other ways of interacting with a digital assistant to request information or performance of various tasks. In addition to providing verbal responses and taking programmed actions the digital assistant can also provide responses in other visual or audio forms e.g. as text alerts music videos animations etc. .

As shown in in some embodiments a digital assistant system is implemented according to a client server model. The digital assistant system includes a client side portion e.g. and hereafter digital assistant DA client executed on a user device e.g. and and a server side portion hereafter digital assistant DA server executed on a server system . The DA client communicates with the DA server through one or more networks . The DA client provides client side functionalities such as user facing input and output processing and communications with the DA server . The DA server provides server side functionalities for any number of DA clients each residing on a respective user device also called a client device .

In some embodiments the DA server includes a client facing I O interface one or more processing modules data and models and an I O interface to external services . The client facing I O interface facilitates the client facing input and output processing for the digital assistant server . The one or more processing modules utilize the data and models to determine the user s intent based on natural language input and perform task execution based on the deduced user intent.

In some embodiments the DA server communicates with external services e.g. navigation service s messaging service s information service s calendar service telephony service etc. through the network s for task completion or information acquisition. The I O interface to the external services facilitates such communications.

Examples of the user device include but are not limited to a handheld computer a personal digital assistant PDA a tablet computer a laptop computer a desktop computer a cellular telephone a smartphone an enhanced general packet radio service EGPRS mobile phone a media player a navigation device a game console a television a remote control or a combination of any two or more of these data processing devices or any other suitable data processing devices. More details on the user device are provided in reference to an exemplary user device shown in .

Examples of the communication network s include local area networks LAN and wide area networks WAN e.g. the Internet. The communication network s may be implemented using any known network protocol including various wired or wireless protocols such as Ethernet Universal Serial Bus USB FIREWIRE Global System for Mobile Communications GSM Enhanced Data GSM Environment EDGE code division multiple access CDMA time division multiple access TDMA Bluetooth Wi Fi voice over Internet Protocol VoIP Wi MAX or any other suitable communication protocol.

The server system can be implemented on at least one data processing apparatus and or a distributed network of computers.

Although the digital assistant system shown in includes both a client side portion e.g. the DA client and a server side portion e.g. the DA server in some embodiments a digital assistant system refers only to the server side portion e.g. the DA server . Alternatively in some embodiments the functions of a digital assistant can be implemented as a standalone application installed on a user device. In addition the divisions of functionalities between the client and server portions of the digital assistant can vary in different embodiments. For example in some embodiments the DA client is a thin client that provides only user facing input and output processing functions and delegates all other functionalities of the digital assistant to the DA server . In some other embodiments the DA client is configured to perform or assist one or more functions of the DA server .

For example in some embodiments a motion sensor a light sensor and a proximity sensor are coupled to the peripherals interface to facilitate orientation light and proximity sensing functions. In some embodiments other sensors such as a positioning system e.g. GPS receiver a temperature sensor a biometric sensor and the like are connected to the peripherals interface to facilitate related functionalities.

In some embodiments the user device includes a camera subsystem coupled to the peripherals interface . In some embodiments an optical sensor of the camera subsystem facilitates camera functions such as taking photographs and recording video clips. In some embodiments the user device includes one or more wired and or wireless communication subsystems provide communication functions. The communication subsystems typically includes various communication ports radio frequency receivers and transmitters and or optical e.g. infrared receivers and transmitters. In some embodiments the user device includes an audio subsystem coupled to one or more speakers and one or more microphones to facilitate voice enabled functions such as voice recognition voice replication digital recording and telephony functions.

In some embodiments an I O subsystem is also coupled to the peripheral interface . In some embodiments the user device includes a touch screen and the I O subsystem includes a touch screen controller coupled to the touch screen . When the user device includes the touch screen and the touch screen controller the touch screen and the touch screen controller are typically configured to for example detect contact and movement or break thereof using any of a plurality of touch sensitivity technologies such as capacitive resistive infrared surface acoustic wave technologies proximity sensor arrays and the like. In some embodiments the user device includes a display that does not include a touch sensitive surface. In some embodiments the user device includes a separate touch sensitive surface. In some embodiments the user device includes other input controller s . When the user device includes the other input controller s the other input controller s are typically coupled to other input control devices such as one or more buttons rocker switches thumb wheel infrared port USB port and or a pointer device such as a stylus.

The memory interface is coupled to memory . In some embodiments the memory includes a non transitory computer readable medium such as high speed random access memory and or non volatile memory e.g. one or more magnetic disk storage devices one or more flash memory devices one or more optical storage devices and or other non volatile solid state memory devices .

In some embodiments the memory stores an operating system a communications module a graphical user interface module a sensor processing module a phone module and applications and a subset or superset thereof. The operating system includes instructions for handling basic system services and for performing hardware dependent tasks. The communications module facilitates communicating with one or more additional devices one or more computers and or one or more servers. The graphical user interface module facilitates graphic user interface processing. The sensor processing module facilitates sensor related processing and functions e.g. processing voice input received with the one or more microphones . The phone module facilitates phone related processes and functions. The application module facilitates various functionalities of user applications such as electronic messaging web browsing media processing navigation imaging and or other processes and functions. In some embodiments the application module includes or interacts with a web browser application . In some embodiments the application module includes or interacts with an electronic messaging application.

As described above in some embodiments the memory also stores client side digital assistant instructions e.g. in a digital assistant client module and various user data e.g. user specific vocabulary data preference data and or other data such as the user s electronic address book to do lists shopping lists etc. to provide the client side functionalities of the digital assistant.

In various embodiments the digital assistant client module is capable of accepting voice input text input touch input and or gestural input through various user interfaces e.g. the I O subsystem of the user device . The digital assistant client module is also capable of providing output in audio visual and or tactile forms. For example output can be provided as voice sound alerts text messages menus graphics videos animations vibrations and or combinations of two or more of the above. During operation the digital assistant client module communicates with the digital assistant server e.g. the digital assistant server using the communication subsystems .

In some embodiments the digital assistant client module utilizes various sensors subsystems and peripheral devices to gather additional information from the surrounding environment of the user device to establish a context associated with a user input. In some embodiments the digital assistant client module provides the context information or a subset thereof with the user input to the digital assistant server e.g. the digital assistant server to help deduce the user s intent.

In some embodiments the context information that can accompany the user input includes sensor information e.g. lighting ambient noise ambient temperature images or videos of the surrounding environment etc. In some embodiments the context information also includes the physical state of the device e.g. device orientation device location device temperature power level speed acceleration motion patterns cellular signals strength etc. In some embodiments information related to the software state of the user device e.g. running processes installed programs past and present network activities background services error logs resources usage etc. of the user device is also provided to the digital assistant server e.g. the digital assistant server as context information associated with a user input.

In some embodiments the DA client module selectively provides information e.g. at least a portion of the user data stored on the user device in response to requests from the digital assistant server. In some embodiments the digital assistant client module also elicits additional input from the user via a natural language dialogue or other user interfaces upon request by the digital assistant server . The digital assistant client module passes the additional input to the digital assistant server to help the digital assistant server in intent deduction and or fulfillment of the user s intent expressed in the user request.

In some embodiments the memory may include additional instructions or fewer instructions. Furthermore various functions of the user device may be implemented in hardware and or in firmware including in one or more signal processing and or application specific integrated circuits and the user device thus need not include all modules and applications illustrated in . For example in some embodiments the user device does not include the touch screen .

The digital assistant system includes memory one or more processors an input output I O interface and a network communications interface . These components communicate with one another over one or more communication buses or signal lines .

In some embodiments the memory includes a non transitory computer readable medium such as high speed random access memory and or a non volatile computer readable storage medium e.g. one or more magnetic disk storage devices one or more flash memory devices one or more optical storage devices and or other non volatile solid state memory devices .

The I O interface couples input output devices of the digital assistant system such as displays a keyboard touch screens and microphones to the user interface module . The I O interface in conjunction with the user interface module receives user inputs e.g. voice input keyboard inputs touch inputs etc. and process them accordingly. In some embodiments when the digital assistant is implemented on a standalone user device the digital assistant system includes any of the components and I O and communication interfaces described with respect to the user device in e.g. one or more microphones . In some embodiments the digital assistant system represents the server portion of a digital assistant implementation and interacts with the user through a client side portion residing on a user device e.g. the user device shown in .

In some embodiments the network communications interface includes wired communication port s and or wireless transmission and reception circuitry . The wired communication port s receive and send communication signals via one or more wired interfaces e.g. Ethernet Universal Serial Bus USB FIREWIRE etc. The wireless circuitry typically receives and sends RF signals and or optical signals from to communications networks and other communications devices. The wireless communications may use any of a plurality of communications standards protocols and technologies such as GSM EDGE CDMA TDMA Bluetooth Wi Fi VoIP Wi MAX or any other suitable communication protocol. The network communications interface enables communication between the digital assistant system with networks such as the Internet an intranet and or a wireless network such as a cellular telephone network a wireless local area network LAN and or a metropolitan area network MAN and other devices.

In some embodiments the non transitory computer readable storage medium of memory stores programs modules instructions and data structures including all or a subset of an operating system a communications module a user interface module one or more applications and a digital assistant module . The one or more processors execute these programs modules and instructions and reads writes from to the data structures.

The operating system e.g. Darwin RTXC LINUX UNIX OS X WINDOWS or an embedded operating system such as VxWorks includes various software components and or drivers for controlling and managing general system tasks e.g. memory management storage device control power management etc. and facilitates communications between various hardware firmware and software components.

The communications module facilitates communications between the digital assistant system with other devices over the network communications interface . For example the communication module may communicate with the communications module of the device shown in . The communications module also includes various software components for handling data received by the wireless circuitry and or wired communications port .

In some embodiments the user interface module receives commands and or inputs from a user via the I O interface e.g. from a keyboard touch screen and or microphone and provides user interface objects on a display.

The applications include programs and or modules that are configured to be executed by the one or more processors . For example if the digital assistant system is implemented on a standalone user device the applications may include user applications such as games a calendar application a navigation application a web browser application or an email application. If the digital assistant system is implemented on a server farm the applications may include resource management applications diagnostic applications or scheduling applications for example.

The memory also stores the digital assistant module or the server portion of a digital assistant . In some embodiments the digital assistant module includes the following sub modules or a subset or superset thereof an input output processing module a speech to text STT processing module a natural language processing module a dialogue flow processing module a task flow processing module and a service processing module . Each of these processing modules has access to one or more of the following data and models of the digital assistant or a subset or superset thereof ontology vocabulary index user data task flow models and service models .

In some embodiments using the processing modules e.g. the input output processing module the STT processing module the natural language processing module the dialogue flow processing module the task flow processing module and or the service processing module data and models implemented in the digital assistant module the digital assistant system performs at least some of the following identifying a user s intent expressed in a natural language input received from the user actively eliciting and obtaining information needed to fully deduce the user s intent e.g. by disambiguating words names intentions etc. determining the task flow for fulfilling the deduced intent and executing the task flow to fulfill the deduced intent. In some embodiments the digital assistant also takes appropriate actions when a satisfactory response was not or could not be provided to the user for various reasons.

In some embodiments the I O processing module interacts with the user through the I O devices or with a user device e.g. a user device in through the network communications interface to obtain user input e.g. a speech input and to provide responses to the user input. The I O processing module optionally obtains context information associated with the user input from the user device along with or shortly after the receipt of the user input. The context information includes user specific data vocabulary and or preferences relevant to the user input. In some embodiments the context information also includes software and hardware states of the device e.g. the user device in at the time the user request is received and or information related to the surrounding environment of the user at the time that the user request was received. In some embodiments the I O processing module also sends follow up questions to and receives answers from the user regarding the user request. In some embodiments when a user request is received by the I O processing module and the user request contains a speech input the I O processing module forwards the speech input to the speech to text STT processing module for speech to text conversions.

In some embodiments the speech to text processing module receives speech input e.g. a user utterance captured in a voice recording through the I O processing module . In some embodiments the speech to text processing module uses various acoustic and language models to recognize the speech input as a sequence of phonemes and ultimately a sequence of words or tokens written in one or more languages. The speech to text processing module is implemented using any suitable speech recognition techniques acoustic models and language models such as Hidden Markov Models Dynamic Time Warping DTW based speech recognition and other statistical and or analytical techniques. In some embodiments the speech to text processing can be performed at least partially by a third party service or on the user s device. Once the speech to text processing module obtains the result of the speech to text processing e.g. a sequence of words or tokens it passes the result to the natural language processing module for intent deduction.

The natural language processing module natural language processor of the digital assistant takes the sequence of words or tokens token sequence generated by the speech to text processing module and attempts to associate the token sequence with one or more actionable intents recognized by the digital assistant. As used herein an actionable intent represents a task that can be performed by the digital assistant and or the digital assistant system and has an associated task flow implemented in the task flow models . The associated task flow is a series of programmed actions and steps that the digital assistant system takes in order to perform the task. The scope of a digital assistant system s capabilities is dependent on the number and variety of task flows that have been implemented and stored in the task flow models or in other words on the number and variety of actionable intents that the digital assistant system recognizes. The effectiveness of the digital assistant system however is also dependent on the digital assistant system s ability to deduce the correct actionable intent s from the user request expressed in natural language.

In some embodiments in addition to the sequence of words or tokens obtained from the speech to text processing module the natural language processor also receives context information associated with the user request e.g. from the I O processing module . The natural language processor optionally uses the context information to clarify supplement and or further define the information contained in the token sequence received from the speech to text processing module . The context information includes for example user preferences hardware and or software states of the user device sensor information collected before during or shortly after the user request prior interactions e.g. dialogue between the digital assistant and the user and the like.

In some embodiments the natural language processing is based on ontology . The ontology is a hierarchical structure containing a plurality of nodes each node representing either an actionable intent or a property relevant to one or more of the actionable intents or other properties . As noted above an actionable intent represents a task that the digital assistant system is capable of performing e.g. a task that is actionable or can be acted on . A property represents a parameter associated with an actionable intent or a sub aspect of another property. A linkage between an actionable intent node and a property node in the ontology defines how a parameter represented by the property node pertains to the task represented by the actionable intent node.

In some embodiments the ontology is made up of actionable intent nodes and property nodes. Within the ontology each actionable intent node is linked to one or more property nodes either directly or through one or more intermediate property nodes. Similarly each property node is linked to one or more actionable intent nodes either directly or through one or more intermediate property nodes.

An actionable intent node along with its linked concept nodes may be described as a domain. In the present discussion each domain is associated with a respective actionable intent and refers to the group of nodes and the relationships therebetween associated with the particular actionable intent. In some embodiments the ontology is made up of many domains. Each domain may share one or more property nodes with one or more other domains.

In some embodiments the ontology includes all the domains and hence actionable intents that the digital assistant is capable of understanding and acting upon. In some embodiments the ontology may be modified such as by adding or removing domains or nodes or by modifying relationships between the nodes within the ontology .

In some embodiments nodes associated with multiple related actionable intents may be clustered under a super domain in the ontology . For example a travel super domain may include a cluster of property nodes and actionable intent nodes related to travels. The actionable intent nodes related to travels may include airline reservation hotel reservation car rental get directions find points of interest and so on. The actionable intent nodes under the same super domain e.g. the travels super domain may have many property nodes in common. For example the actionable intent nodes for airline reservation hotel reservation car rental get directions find points of interest may share one or more of the property nodes start location destination departure date time arrival date time and party size. 

In some embodiments each node in the ontology is associated with a set of words and or phrases that are relevant to the property or actionable intent represented by the node. The respective set of words and or phrases associated with each node is the so called vocabulary associated with the node. The respective set of words and or phrases associated with each node can be stored in the vocabulary index in association with the property or actionable intent represented by the node. For example the vocabulary associated with the node for the property of restaurant may include words such as food drinks cuisine hungry eat pizza fast food meal and so on. For another example the vocabulary associated with the node for the actionable intent of initiate a phone call may include words and phrases such as call phone dial ring call this number make a call to and so on. The vocabulary index optionally includes words and phrases in different languages.

In some embodiments the natural language processor receives the token sequence e.g. a text string from the speech to text processing module and determines what nodes are implicated by the words in the token sequence. In some embodiments if a word or phrase in the token sequence is found to be associated with one or more nodes in the ontology via the vocabulary index the word or phrase will trigger or activate those nodes. When multiple nodes are triggered based on the quantity and or relative importance of the activated nodes the natural language processor will select one of the actionable intents as the task or task type that the user intended the digital assistant to perform. In some embodiments the domain that has the most triggered nodes is selected. In some embodiments the domain having the highest confidence value e.g. based on the relative importance of its various triggered nodes is selected. In some embodiments the domain is selected based on a combination of the number and the importance of the triggered nodes. In some embodiments additional factors are considered in selecting the node as well such as whether the digital assistant system has previously correctly interpreted a similar request from a user.

In some embodiments the digital assistant system also stores names of specific entities in the vocabulary index so that when one of these names is detected in the user request the natural language processor will be able to recognize that the name refers to a specific instance of a property or sub property in the ontology. In some embodiments the names of specific entities are names of businesses restaurants people movies and the like. In some embodiments the digital assistant system can search and identify specific entity names from other data sources such as the user s address book a movies database a musicians database and or a restaurant database. In some embodiments when the natural language processor identifies that a word in the token sequence is a name of a specific entity such as a name in the user s address book that word is given additional significance in selecting the actionable intent within the ontology for the user request.

For example when the words Mr. Santo is recognized from the user request and the last name Santo is found in the vocabulary index as one of the contacts in the user s contact list then it is likely that the user request corresponds to a send a message or initiate a phone call domain. For another example when the words ABC Caf are found in the user request and the term ABC Caf is found in the vocabulary index as the name of a particular restaurant in the user s city then it is likely that the user request corresponds to a restaurant reservation domain.

User data includes user specific information such as user specific vocabulary user preferences user address user s default and secondary languages user s contact list and other short term or long term information for each user. The natural language processor can use the user specific information to supplement the information contained in the user input to further define the user intent. For example for a user request invite my friends to my birthday party the natural language processor is able to access user data to determine who the friends are and when and where the birthday party would be held rather than requiring the user to provide such information explicitly in his her request.

Once the natural language processor identifies an actionable intent or domain based on the user request the natural language processor generates a structured query to represent the identified actionable intent. In some embodiments the structured query includes parameters for one or more nodes within the domain for the actionable intent and at least some of the parameters are populated with the specific information and requirements specified in the user request. For example the user may say Make me a dinner reservation at a sushi place at . In this case the natural language processor may be able to correctly identify the actionable intent to be restaurant reservation based on the user input. According to the ontology a structured query for a restaurant reservation domain may include parameters such as Cuisine Time Date Party Size and the like. Based on the information contained in the user s utterance the natural language processor may generate a partial structured query for the restaurant reservation domain where the partial structured query includes the parameters Cuisine Sushi and Time 7pm . However in this example the user s utterance contains insufficient information to complete the structured query associated with the domain. Therefore other necessary parameters such as Party Size and Date are not specified in the structured query based on the information currently available. In some embodiments the natural language processor populates some parameters of the structured query with received context information. For example if the user requested a sushi restaurant near me the natural language processor may populate a location parameter in the structured query with GPS coordinates from the user device .

In some embodiments the natural language processor passes the structured query including any completed parameters to the task flow processing module task flow processor . The task flow processor is configured to perform one or more of receiving the structured query from the natural language processor completing the structured query and performing the actions required to complete the user s ultimate request. In some embodiments the various procedures necessary to complete these tasks are provided in task flow models . In some embodiments the task flow models include procedures for obtaining additional information from the user and task flows for performing actions associated with the actionable intent.

As described above in order to complete a structured query the task flow processor may need to initiate additional dialogue with the user in order to obtain additional information and or disambiguate potentially ambiguous utterances. When such interactions are necessary the task flow processor invokes the dialogue processing module to engage in a dialogue with the user. In some embodiments the dialogue processing module determines how and or when to ask the user for the additional information and receives and processes the user responses. In some embodiments the questions are provided to and answers are received from the users through the I O processing module . For example the dialogue processing module presents dialogue output to the user via audio and or visual output and receives input from the user via spoken or physical e.g. touch gesture responses. Continuing with the example above when the task flow processor invokes the dialogue processing module to determine the party size and date information for the structured query associated with the domain restaurant reservation the dialogue flow processor generates questions such as For how many people and On which day to pass to the user. Once answers are received from the user the dialogue processing module populates the structured query with the missing information or passes the information to the task flow processor to complete the missing information from the structured query.

In some cases the task flow processor may receive a structured query that has one or more ambiguous properties. For example a structured query for the send a message domain may indicate that the intended recipient is Bob and the user may have multiple contacts named Bob. The task flow processor will request that the dialogue processor disambiguate this property of the structured query. In turn the dialogue processor may ask the user Which Bob and display or read a list of contacts named Bob from which the user may choose.

Once the task flow processor has completed the structured query for an actionable intent the task flow processor proceeds to perform the ultimate task associated with the actionable intent. Accordingly the task flow processor executes the steps and instructions in the task flow model according to the specific parameters contained in the structured query. For example the task flow model for the actionable intent of restaurant reservation may include steps and instructions for contacting a restaurant and actually requesting a reservation for a particular party size at a particular time. For example using a structured query such as restaurant reservation restaurant ABC Caf date Mar. 12 2012 time 7pm party size 5 the task flow processor may perform the steps of 1 logging onto a server of the ABC Caf or a restaurant reservation system that is configured to accept reservations for multiple restaurants such as the ABC Caf 2 entering the date time and party size information in a form on the website 3 submitting the form and 4 making a calendar entry for the reservation in the user s calendar.

In some embodiments the task flow processor employs the assistance of a service processing module service processor to complete a task requested in the user input or to provide an informational answer requested in the user input. For example the service processor can act on behalf of the task flow processor to make a phone call set a calendar entry invoke a map search invoke or interact with other user applications installed on the user device and invoke or interact with third party services e.g. a restaurant reservation portal a social networking website a banking portal etc. . In some embodiments the protocols and application programming interfaces API required by each service can be specified by a respective service model among the services models . The service processor accesses the appropriate service model for a service and generates requests for the service in accordance with the protocols and APIs required by the service according to the service model.

For example if a restaurant has enabled an online reservation service the restaurant can submit a service model specifying the necessary parameters for making a reservation and the APIs for communicating the values of the necessary parameter to the online reservation service. When requested by the task flow processor the service processor can establish a network connection with the online reservation service using the web address stored in the service models and send the necessary parameters of the reservation e.g. time date party size to the online reservation interface in a format according to the API of the online reservation service.

In some embodiments the natural language processor dialogue processor and task flow processor are used collectively and iteratively to deduce and define the user s intent obtain information to further clarify and refine the user intent and finally generate a response e.g. provide an output to the user or complete a task to fulfill the user s intent.

In some embodiments after all of the tasks needed to fulfill the user s request have been performed the digital assistant formulates a confirmation response and sends the response back to the user through the I O processing module . If the user request seeks an informational answer the confirmation response presents the requested information to the user. In some embodiments the digital assistant also requests the user to indicate whether the user is satisfied with the response produced by the digital assistant .

In some embodiments the electronic device pauses for a predefined period e.g. one two three four or five seconds etc. after outputting the voice signal the audio signal information about the link and or a voice reading of the text associated with the link.

In some embodiments the electronic device receives a voice command from a user. In some embodiments the electronic device receives the voice command from the user while the electronic device outputs the voice signal the audio signal information about the link and or a voice reading of the text associated with the link during the pause or while the electronic device outputs a voice reading of a portion of the document subsequent to the text associated with the link. In some embodiments the electronic device receives the voice command from the user prior to outputting a voice signal and or an audio signal about a second link distinct from the link.

In some embodiments the voice command is a request to navigate to the linked document e.g. follow the link . In some embodiments the electronic device stores a last voiced out portion of the document. In some embodiments the electronic device stores information about the link e.g. the position and or identification of the link in lieu of the last voiced out portion of the document.

In some embodiments while the electronic device outputs a voice reading of one or more portions of the linked document the electronic device receives a voice command from the user requesting navigation back to the first document. illustrates that in response the electronic device displays one or more portions of the first document and outputs a voice reading of a portion of the first document.

In some embodiments the electronic device outputs a voice inquiry about navigating back to the first document. In some embodiments the electronic device outputs a voice inquiry about navigating back to the first document after outputting a voice reading of the last sentence of the linked document. When the user provides a voice command requesting navigation back to the first document the electronic device outputs a voice reading of a portion of the first document as explained above i.e. navigates back to the first document . Alternatively in some embodiments the electronic device automatically navigates back to the first document without receiving a voice command from the user for example when the electronic device completes outputting a voice reading of the linked document.

In some embodiments when the electronic device resumes outputting a voice reading of the first document the electronic device starts outputting a voice reading of a portion of the first document that corresponds to the last voiced out portion of the first document. In some embodiments the electronic device resumes by outputting a voice reading of the text associated with the link. In some embodiments the electronic device resumes by outputting a voice reading of a portion of the first document subsequent to the text associated with the link. In some embodiments the electronic device resumes by outputting a voice reading of a sentence that includes the text associated with the link. In some embodiments the electronic device resumes by outputting a voice reading of a paragraph that includes the text associated with the link.

In some embodiments the electronic device receives a voice command from the user requesting one or more documents corresponding to particular criteria. For example in some embodiments the particular criteria are that the one or more documents be authored by one or more authors e.g. find email messages from David find email messages from David John Karen and Paul find email messages from addresses of this email etc. . In other embodiments the particular criteria are that the one or more documents be associated with a particular document e.g. find email messages in this thread find replies to this email etc. . In yet other embodiments the particular criteria are that the one or more documents be replies to a particular document. In still other embodiments the particular criteria are that the one or more documents include a last message from a respective author e.g. find a last email from David . In some embodiments the particular criteria are that the one or more documents include a first message from the respective author.

In some embodiments the particular criteria are that the one or more documents correspond to a particular date range e.g. find emails received last week find email messages received from January 1 to March 31 etc. . In some embodiments the particular date range corresponds to a single date.

In some embodiments the particular criteria includes one or more of the above mentioned criteria e.g. find emails from David and Karen received last week .

In some embodiments the electronic device outputs a voice reading of email messages in the list e.g. the list illustrated in or . In some embodiments when the electronic device outputs a voice reading of a last email in the list the electronic device outputs audible information indicating that the email that is read out is the last email in the list.

Although illustrate exemplary user interfaces displayed on the electronic device in some embodiments the electronic device outputs audible information e.g. a voice reading and or audible signals without displaying the exemplary user interfaces on the electronic device. In some embodiments the electronic device does not include a display at all.

These operations are merely exemplary and fewer or less operations may be performed by the electronic device in various embodiments.

In some embodiments the electronic device determines whether the portion of the document includes a link. If the portion of the document includes a link the electronic device outputs audible information identifying a link of the plurality of links e.g. a voice output of the word link or an audio signal beep and or information about the link such as the title author date and source of a linked document as described with respect to .

In some embodiments the electronic device receives a voice command from a user. In some embodiments the electronic device determines whether the electronic device has received a voice command.

In some embodiments when the electronic device receives a voice command the electronic device determines whether the received voice command is a navigation command e.g. whether the received voice command includes a request for navigation to a second document associated with the link i.e. a linked document . If the received voice command includes a request for navigation to a linked document the electronic device retrieves the linked document. The electronic device then outputs a voice reading of one or more portions of the linked document e.g. .

In some embodiments when the electronic device receives a voice command the electronic device determines whether the received voice command is a navigation command e.g. whether the received voice command includes a request for navigation to a second document associated with the link i.e. a linked document . If the received voice command includes a request for navigation to a linked document the electronic device retrieves the linked document. The electronic device then outputs a voice reading of one or more portions of the linked document e.g. .

In some embodiments when the electronic device receives a voice command the electronic device determines whether the received voice command is an information command e.g. whether the received voice command includes a request for information about the linked document . If the received voice command includes a request for information about the linked document e.g. title author date source abstract summary first sentence first paragraph etc. such as who is the author the electronic device retrieves the requested information. In some embodiments at least a portion of the requested information is obtained from the linked document e.g. title author date and abstract . In some embodiments at least a portion of the requested information is obtained by processing the linked document e.g. summarizing the linked document . In some embodiments at least a portion of the requested information is provided by a third party server e.g. review of the linked document . The electronic device outputs a voice reading of the requested information. In other embodiments the electronic device receives at least a portion of the requested information from metadata of the linked document or retrieves information from other sources associated with the linked document to get details.

In some embodiments when a user provides a voice command between two adjacent links or immediately after two adjacent links the electronic device identifies the two links as candidate links corresponding to the voice command. Because the user s voice command may be directed to a first or second link of the two adjacent links there may be a need to clarify with respect to which link the user s voice command should be performed. In some embodiments the electronic device outputs audible information about the candidate links and or an audible inquiry about which link the user wants to perform the voice command on. For example while the electronic device outputs a voice reading of a sentence the next game between School A and School B is on March 3 a user provides a voice command follow the link. In some embodiments the electronic device outputs an audible inquiry which link and receives a subsequent voice command from the use. When the subsequent voice command is School B the electronic device retrieves a document linked with the text School B and outputs a voice reading of a portion of the linked document.

In some embodiments after outputting a voice reading of a respective portion of the document the electronic device determines whether the end of the document has been reached e.g. whether the electronic device has output a voice reading of the entire document . If the end of the document has not been reached the electronic device outputs a voice reading of a subsequent portion of the document. In some embodiments if the end of the document has been reached the electronic device waits for a navigation command from the user. When the electronic device receives a navigation command from the user requesting navigation to a previous document or a new document the electronic device retrieves the previous document or the new document. As described previously these operations e.g. operations and etc. may be performed by a server or a mobile device. In some embodiments prior to receiving the navigation command from the user the electronic device outputs a voice inquiry about whether the user wants to navigate to another document e.g. as described with respect to . In some embodiments when the user does not want to navigate to another document the electronic device stops outputting a voice reading.

These operations are merely exemplary and fewer or less operations may be performed by the electronic device in various embodiments.

In some embodiments the electronic device receives a document having a plurality of portions. At least some of the portions are associated with respective metadata. In some embodiments the respective metadata indicate the structure of the document e.g. paragraphs sentences headings styles etc. 

The electronic device outputs a voice reading of a respective portion of the document. While outputting the voice reading of the respective portion of the document the electronic device audibly distinguishes each respective portion based on that portion s respective metadata. For example the electronic device determines whether the respective portion corresponds to a signpost. As used herein a signpost refers to a predefined location or a predefined type of locations in the document. In some embodiments when the respective portion corresponds to a signpost the electronic device outputs a voice signal or an audio signal to indicate whether the portion corresponds to a signpost. For example in some embodiments at the start of a new paragraph the electronic device outputs a voice signal or an audio signal to indicate the beginning of the new paragraph. Similarly in some embodiments a voice signal or an audio signal is used to indicate that a beginning or end of a section and or to indicate a bookmark.

Subsequent to outputting a voice signal or an audio signal to indicate whether the portion corresponds to a signpost e.g. after outputting a voice reading of a couple of paragraphs after outputting a voice signal or an audio signal to indicate whether the portion corresponds to a signpost the electronic device receives a voice command from the user and determines whether the voice command is a navigation command. When the voice command is a navigation command requesting navigation to the signpost the electronic device navigates to the portion corresponding to the signpost and outputs a voice reading of the portion corresponding to the signpost.

In some embodiments after outputting a voice reading of a respective portion of the document the electronic device determines whether the end of the document has been reached e.g. whether the electronic device has output a voice reading of the entire document . If the end of the document has not been reached the electronic device outputs a voice reading of a subsequent portion of the document.

These operations are merely exemplary and fewer or less operations may be performed by the electronic device in various embodiments.

In some embodiments the electronic device outputs a voice reading of at least a portion of a document.

The electronic device receives from a user a voice command requesting a document corresponding to a particular criteria. In some embodiments the electronic device receives at least a portion of the voice command while outputting the voice reading. In some embodiments the particular criteria requires that the one or more identified documents be authored by one or more authors identified by the user. In some embodiments the particular criteria requires that the one or more identified documents be associated with a particular document. In some embodiments the particular criteria requires that the one or more identified documents be replies to a particular document. In some embodiments the particular criteria requires that the one or more identified documents include a last message from a respective author. In some embodiments the particular criteria requires that the one or more identified documents correspond to a particular date range identified by the user.

In some embodiments prior to receiving the voice command requesting a document corresponding to the particular criteria the electronic device receives from the user a request for information regarding the document e.g. author word count date of last update a number of sections in the document etc. . In some embodiments in response to the request for the information regarding the document the electronic device outputs audible information includes the requested information regarding the document. In some embodiments the electronic device retrieves the requested information from one or more remote computer systems e.g. one or more search engines and or database servers . In some embodiments the electronic device determines the requested information from the document.

In some embodiments the electronic device receives from the user a request to store the requested information and in response stores the requested information e.g. on the electronic device or remotely . In some embodiments the electronic device stores the requested information without a request from the user to store the requested information.

In one example the request is for a length of the document e.g. how long is this article and in response to the request for the length of the document the electronic device determines the length of the document e.g. word count or page count and outputs audible information that includes the length of the document. In another example the request is for whether the document was updated e.g. was this article updated and in response to the request for whether the document was updated the electronic device determines whether the document has been updated and outputs audible information that indicates whether the document has been updated e.g. there are corrections to the article. . In yet another example the request is for an author of the document e.g. who wrote this article . In some embodiments in response to the request for the author of the document the electronic device outputs audible information that includes the author of the document. In some cases the author information is extracted from the document e.g. in the text portion as authored by or in an author field in a markup language . In some cases the author information is extracted from a web page that links to the document.

In some embodiments the electronic device receives a request for additional information based on the requested information obtains the additional information and outputs the additional information. In some of the examples described above with respect to the author information after outputting audible information that includes the author of the document the electronic device receives a request for other documents authored by the same author e.g. what other articles did this author write and in response obtains information that includes other documents authored by the author of the document and outputs the information. In some embodiments the electronic device obtains the information that includes other documents authored by the author of the document by sending a request to one or more remote computer systems e.g. one or more search engines and or database servers receiving at least a portion of the information from at least a subset of the one or more remote computer systems.

In some embodiments while outputting the voice reading of at least a portion of the document the electronic device receives from the user a request to store information regarding the document. In some embodiments in response to receiving the request to store information regarding the document the electronic device stores the information regarding the document. For example in some embodiments the request is to bookmark the document e.g. bookmark this article and the electronic device stores access information for the document e.g. a universal resource locator for the document as a bookmark. In some embodiments the electronic device stores the access information for the document on the electronic device. In some embodiments the electronic device stores the access information at a remote server.

It would be obvious to a person having ordinary skill in the art that in some embodiments the electronic device provides the requested information without performing subsequent operations e.g. operations and described below .

The electronic device identifies one or more documents that correspond to the particular criteria e.g. .

The electronic device outputs a voice reading of at least a portion of a respective document of the one or more identified documents.

In some embodiments after outputting a voice reading of a respective portion of the document the electronic device determines whether the end of the document has been reached e.g. whether the electronic device has output a voice reading of the entire document . If the end of the document has not been reached the electronic device outputs a voice reading of a subsequent portion of the document.

In some embodiments if the end of the document has been reached the electronic device determines whether all documents in the one or more identified documents have been read. If all documents in the one or more identified documents have not been read the electronic device outputs a voice reading of a portion of a next document in the one or more identified documents.

In accordance with some embodiments shows a functional block diagram of electronic device configured in accordance with the principles of the invention as described above. The functional blocks of the device may be implemented by hardware software or a combination of hardware and software to carry out the principles of the invention. It is understood by persons of skill in the art that the functional blocks described in may be combined or separated into sub blocks to implement the principles of the invention as described above. Therefore the description herein may support any possible combination or separation or further definition of the functional blocks described herein.

As shown in electronic device includes audio input unit and audio output unit . In some embodiments electronic device includes display unit configured to display one or more portions of an electronic document. In some embodiments electronic device includes touch sensitive surface unit configured to detect one or more gestures on touch sensitive surface unit . Electronic device further includes processing unit coupled to audio input unit and audio output unit . In some embodiments processing unit is also coupled to display unit and touch sensitive surface unit . In some embodiments processing unit includes document receiving unit outputting unit voice command receiving unit storing unit retrieving unit identifying unit and user selection receiving unit .

Processing unit is configured to receive a first document comprising a plurality of links e.g. with document receiving unit . Processing unit is configured to output a voice reading of at least a portion of the first document e.g. with outputting unit through audio output unit . Processing unit is configured to output audible information identifying a link of the plurality of links e.g. with outputting unit through audio output unit . Processing unit is configured to in response to outputting the audible information identifying the link receive from the user a voice command regarding the link e.g. with voice command receiving unit through audio input unit . Processing unit is configured to in response to receiving from the user the voice command output a voice reading of at least a portion of a second document associated with the link e.g. with outputting unit through audio output unit .

In some embodiments processing unit is configured to in response to receiving from the user the voice command regarding the link store information about the link e.g. with storing unit and output a voice reading of one or more portions of the second document e.g. with outputting unit through audio output unit .

In some embodiments processing unit is configured to after outputting the voice reading of at least a portion of the second document receive from the user a voice command requesting navigation back to the first document e.g. with voice command receiving unit through audio input unit and in response to receiving from the user the voice command requesting navigation back to the first document output a voice reading of one or more portions of the first document subsequent to text in the first document associated with the link e.g. with outputting unit through audio output unit .

In some embodiments processing unit is configured to after outputting the voice reading of at least a portion of the second document output a voice inquiry about navigating back to the first document e.g. with outputting unit through audio output unit in response to outputting the voice inquiry about navigating back to the first document receive from the user a voice command requesting navigation back to the first document e.g. with voice command receiving unit through audio input unit and in response to receiving from the user the voice command requesting navigation back to the first document output a voice reading of one or more portions of the first document including a text in the first document associated with the link e.g. with outputting unit through audio output unit .

In some embodiments processing unit is configured to after outputting the voice reading of at least a portion of the second document in response to receiving from the user the voice command requesting navigation back to the first document automatically output a voice reading of one or more portions of the first document subsequent to text in the first document associated with the link e.g. with outputting unit through audio output unit .

In some embodiments the voice command is a voice command of a first type. In some embodiments processing unit is configured to receive from the user a voice command of a second type distinct from the first type regarding the link e.g. with voice command receiving unit through audio input unit and in response to receiving from the user the voice command of the second type regarding the link retrieve information from the second document e.g. with retrieving unit and output voice information corresponding to the voice command of the second type based on the information from the second document e.g. with outputting unit through audio output unit .

In some embodiments the voice command of the second type includes a request for information about an author of the second document.

In some embodiments the voice command of the second type includes a request for a summary of the second document.

In some embodiments processing unit is configured to identify two or more links as candidate links corresponding to a voice command from the user e.g. with identifying unit output audible information about the candidate links e.g. with outputting unit through audio output unit and receive from the user a selection of a single link of the candidate links e.g. with user selection receiving unit through audio input unit or touch sensitive surface unit .

In accordance with some embodiments shows a functional block diagram of electronic device configured in accordance with the principles of the invention as described above. The functional blocks of the device may be implemented by hardware software or a combination of hardware and software to carry out the principles of the invention. It is understood by persons of skill in the art that the functional blocks described in may be combined or separated into sub blocks to implement the principles of the invention as described above. Therefore the description herein may support any possible combination or separation or further definition of the functional blocks described herein.

As shown in electronic device includes audio input unit and audio output unit . In some embodiments electronic device includes display unit configured to display one or more portions of an electronic document. In some embodiments electronic device includes touch sensitive surface unit configured to detect one or more gestures on touch sensitive surface unit . Electronic device further includes processing unit coupled to audio input unit and audio output unit . In some embodiments processing unit is also coupled to display unit and touch sensitive surface unit . In some embodiments processing unit includes document receiving unit outputting unit and voice command receiving unit .

Processing unit is configured to receive the document having a plurality of portions e.g. with document receiving unit wherein at least some of the portions are associated with respective metadata. Processing unit is configured to output a voice reading of respective portions of the document including audibly distinguishing the respective portions based on the respective metadata e.g. with outputting unit through audio output unit . Processing unit is configured to receive from a user a voice command requesting navigation to a particular portion associated with particular metadata e.g. with voice command receiving unit through audio input unit . Processing unit is configured to in response to receiving the voice command output a voice reading of the particular portion associated with the particular metadata e.g. with outputting unit through audio output unit .

In some embodiments processing unit is configured to output a voice reading of the respective portions of the text to the user in accordance with respective styles of the respective portions by outputting a voice reading of a first portion of the text in the document to the user using a first set of voice characteristics e.g. with outputting unit through audio output unit and outputting a voice reading of a second portion of the text in the document to the user using a second set of voice characteristics distinct from the first set of voice characteristics e.g. with outputting unit through audio output unit .

In some embodiments processing unit is configured to receive at least a portion of the voice command from the user e.g. with voice command receiving unit through audio input unit while outputting the voice reading of one or more portions of the document.

In some embodiments at least one of the respective portions of the document is a respective paragraph of the document.

In some embodiments at least one of the respective portions of the document is a respective heading of the document.

In some embodiments at least one of the respective portions of the document is a respective sentence of the document.

In some embodiments a first portion of the document has a first style and a second portion of the document has a second style distinct from the first style.

In accordance with some embodiments shows a functional block diagram of electronic device configured in accordance with the principles of the invention as described above. The functional blocks of the device may be implemented by hardware software or a combination of hardware and software to carry out the principles of the invention. It is understood by persons of skill in the art that the functional blocks described in may be combined or separated into sub blocks to implement the principles of the invention as described above. Therefore the description herein may support any possible combination or separation or further definition of the functional blocks described herein.

As shown in electronic device includes audio input unit and audio output unit . In some embodiments electronic device includes display unit configured to display one or more portions of an electronic document. In some embodiments electronic device includes touch sensitive surface unit configured to detect one or more gestures on touch sensitive surface unit . Electronic device further includes processing unit coupled to audio input unit and audio output unit . In some embodiments processing unit is also coupled to display unit and touch sensitive surface unit . In some embodiments processing unit includes outputting unit voice command receiving unit and identifying unit .

Processing unit is configured to output a voice reading of at least a portion of a document of a plurality of documents e.g. with outputting unit through audio output unit . Processing unit is configured to while outputting the voice reading receive from a user a voice command requesting a document corresponding to a particular criteria e.g. with voice command receiving unit through audio input unit . Processing unit is configured to in response to receiving from the user the voice command identify one or more documents of the plurality of documents that correspond to the particular criteria e.g. with identifying unit and output a voice reading of at least a portion of a respective document of the one or more identified documents e.g. with outputting unit through audio output unit .

In some embodiments the particular criteria require that the one or more identified documents be authored by one or more authors identified by the user.

In some embodiments the particular criteria require that the one or more identified documents be associated with a particular document.

In some embodiments the particular criteria require that the one or more identified documents be replies to a particular document.

In some embodiments the particular criteria require that the one or more identified documents include a last message from a respective author.

In some embodiments the particular criteria require that the one or more identified documents correspond to a particular date range identified by the user.

In some embodiments processing unit is configured to output audible information indicating a number of the one or more identified documents e.g. with outputting unit through audio output unit .

In some embodiments processing unit is configured to output audible information indicating that the respective document is a last document of the one or more identified documents e.g. with outputting unit through audio output unit .

The foregoing description for purpose of explanation has been described with reference to specific embodiments. However the illustrative discussions above are not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications to thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated.

For example one or more aspects of the operations described with respect to may be used with operations described with respect to e.g. outputting an audible inquiry after reaching an end of a document . Similarly one or more aspects of the operations described with respect to may be used with operations described with respect to and one or more aspects of the operations described with respect to may be used with operations described with respect to . For brevity these details are not repeated.

