---

title: Invoking application programming interface calls using voice commands
abstract: Technologies are described herein for invoking API calls through voice commands. An annotated API description is received at a voice API interface. The annotated API description comprises descriptions of one or more APIs and speech annotations for the one or more APIs. The voice API interface further receives a voice API command from a client. By utilizing the annotated API description and the speech annotations contained therein, the voice API interface converts the voice API command into an API call request, which is then sent to the corresponding service for execution. Once the service returns an API call result, the voice API interface interprets the API call result and further converts it into an audio API response based on the information contained in the annotated API description and the speech annotations. The audio API response is then sent to the client.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09589578&OS=09589578&RS=09589578
owner: Amazon Technologies, Inc.
number: 09589578
owner_city: Reno
owner_country: US
publication_date: 20131029
---
An application programming interface API is a set of programming instructions and standards for accessing functionality provided by a program. For example an API may specify how other software components should interact with the program being accessed. Developers of various software components can generate requests to an API by including API calls in the code of their software components. The syntax of the API calls may be described in the documentation of the API that is being called. Traditionally APIs only support machine to machine communications and are software to software interfaces. Using APIs applications may communicate with one another without any user knowledge or intervention.

Certain types of services like Web services typically expose their functionality almost exclusively through APIs. Consequently an end user may have to utilize a software module to invoke API calls to utilize functionality provided by a service. Sometimes however end users may find it difficult to access or build such a software module. For example some users may not have access to a computing device at the time they wish to make an API call. Others might only have access to a computing device that is not equipped with traditional input output mechanisms like a monitor a keyboard a mouse a touchscreen etc. such as the GOOGLE GLASS wearable computer developed by GOOGLE INC. of Mountain View Calif. In these situations traditional mechanisms for invoking API calls typically would not enable an end user to access functionality provided through an API.

The following detailed description is directed to technologies for invoking API calls through voice commands. Utilizing the technologies described herein an end user can make an API call using a voice command. The results of the API call can then be returned as an audio response or a spoken output. Thus the functionality provided by various APIs can be accessed and utilized without the end user creating a program for calling the APIs and without any computing device.

According to one aspect presented herein a computer implemented mechanism is disclosed for invoking API calls through voice commands. In order to provide this functionality API descriptions describing one or more APIs may be annotated with speech annotations to facilitate API calls through voice commands. The annotated API descriptions may be sent to a voice API interface that may also receive a voice API command from a user for invoking one or more API calls. The voice API interface may then convert the voice API command into a text API command utilizing the speech annotations contained in the annotated API descriptions. The text API command may further be translated into an API call request based on the annotated API description. An API call may then be invoked by sending the API call request to the corresponding service application.

Once the service application returns an API call result a text result interpretation may be generated based on the API call results and the annotated API description. The text result interpretation may then be converted into an audio API response by utilizing the information contained in the speech annotations of the annotated API description. The audio API response may then be sent to the user. Additional details regarding the various components and processes described above for invoking API calls through voice commands will be presented below with regard to .

It should be appreciated that the subject matter presented herein may be implemented as a computer process a computer controlled apparatus a computing system or an article of manufacture such as a computer readable storage medium. While the subject matter described herein is presented in the general context of program modules that execute on one or more computing devices those skilled in the art will recognize that other implementations may be performed in combination with other types of program modules. Generally program modules include routines programs components data structures and other types of structures that perform particular tasks or implement particular abstract data types.

Those skilled in the art will also appreciate that aspects of the subject matter described herein may be practiced on or in conjunction with other computer system configurations beyond those described herein including multiprocessor systems microprocessor based or programmable customer electronics minicomputers mainframe computers handheld computers personal digital assistants e readers cellular telephone devices special purposed hardware devices network appliances and the like. As mentioned briefly above the embodiments described herein may be practiced in distributed computing environments where tasks may be performed by remote computing devices that are linked through a communications network. In a distributed computing environment program modules may be located in both local and remote memory storage devices.

In the following detailed description references are made to the accompanying drawings that form a part hereof and that show by way of illustration specific embodiments or examples. The drawings herein are not drawn to scale. Like numerals represent like elements throughout the several figures which may be referred to herein as a FIG. or FIGS. .

Each of the services A N may expose an API A N respectively which may be referred to herein individually as an API or collectively as APIs . As discussed above an API exposed by a service defines the interface for accessing functionality provided by the service . An API may be a library for example that includes specifications for routines data structures object classes and variables. When used in the context of web development an API may be a web service API containing a set of programming instructions and standards for accessing the web service.

Each of the APIs A N may be published along with an annotated API description A N which may be referred to herein individually as an annotated API description or collectively as annotated API descriptions . In some implementations each API may publish its own annotated API description while in other implementations one annotated API description may contain information for multiple APIs. The annotated API description provide a machine readable description of how the service defined by the API may be called including but not limited to information such as the format or syntax of the API calls operations methods and resources involved in the API calls input parameters and or outputs of the API calls properties of the input parameters and output results and others. Terms or words in the annotated API description that pertain to information such as operations methods resources input parameters output results and other information of the API are collectively referred to herein as keywords of the API .

The system may also include a voice API interface that provides functionality for invoking API calls through voice commands. The voice API interface may receive a voice API command from a user and convert the voice API command into an API call request for a corresponding service . The returned API call result from the corresponding API or the service may then be converted by the voice API interface into an audio API response to be presented to the user . In some embodiments the voice API interface may be provided together with each service which may allow a software development kit SDK including the service and the functionality provided by the voice API interface to be provided to a customer of the service . In another embodiment the voice API interface may be offered as a separate service with one voice API interface managing calls to one or multiple APIs .

According to embodiments the communication between the user and the voice API interface may be realized through a client . A user may provide a voice API command to the client which in turn may submit the voice API command to the voice API interface . Likewise the returned audio API response from the voice API interface may be sent to the client which may then present the audio API response to the user .

The client may be a user device or a module executing on a user device. The user device may be a PC a laptop a notebook a personal digital assistant PDA a game console a set top box an e reader a consumer electronics device a smartphone a tablet computing device a server computer a traditional telephone device an Internet telephony device or any other device capable of taking a voice API command as a voice input from the user sending the voice API command to the voice API interface receiving audio API response and presenting it to the user . The client may be connected to the voice API interface through a network which may be a LAN a WAN the Internet or any other networking topology known in the art that connects the client to the voice API interface .

While illustrates the use of a network and a network in order to enable the communications between the client the voice API interface and the services it should be understood that any number of entities illustrated in may be able to directly communicate with each other without the use of a network. For example the voice API interface and the services may be located on the same service host. In another example both the voice API interface and the services may reside on the same device as the client .

The voice API command may be submitted by the user through the client as a complete command. Alternatively or additionally the user may interact with the voice API interface to obtain guidance from the voice API interface for inputting the voice API command . For example for the operations or parameters involved in the API call the voice API interface may provide a list of candidates for the user to choose from or the voice API interface may ask the user to input one by one the parameters or resources required for the API call.

In some embodiments the user may also record the voice API command as an audio file and store the audio file in a storage medium accessible to the client . The client may retrieve the audio file containing the voice API command at a predetermined time and send it to the voice API interface . For example user may want to know the usage of a particular resource on a given day. Rather than wait until midnight of that day the user may record the voice API command as an audio file and instruct the client to submit the voice API command to the voice API interface around midnight.

In other scenarios the user may need to invoke the same API call multiple times to check the status of a resource at different time points. The user could record the voice API command once and configure the client to submit the recorded voice API command at specified points in time. Furthermore if the service is unavailable at the time when the user attempts to invoke the API calls the user may also record the voice API command and instruct the client to submit the voice API command when the corresponding service becomes available. In another embodiment a library of the recorded voice API commands from the user may be maintained. The user may send the recorded voice API commands in the library individually for simple tasks as described above. Alternatively or additionally the user may send the recorded voice API commands in the library as a group and or in a certain order to achieve more complicated tasks.

Similarly upon receiving the audio API response the client may present the audio API response to the user immediately. Alternatively or additionally the audio API response may be stored as an audio file such as in a voice mail system accessible to the client or any other device used by the user . The user may retrieve the audio file at any time to listen to the audio API response or to delete or archive the audio API response .

As discussed above once the voice API interface receives the voice API command from a user it may convert the voice API command into an API call request for the corresponding API of service . To facilitate the conversion each annotated API description may contain speech annotation that may be utilized by the voice API interface to perform the conversion.

The speech annotation may include hints on how to recognize words terms referencing various resources methods or parameters of a service in the voice API command . According to embodiments the speech annotation might also include acoustic models for the keywords and or phonemes contained in the keywords of the API . The acoustic models may be represented as stochastic models modeling the speech signals of the keywords and or the phonemes of the keywords along with parameters for each of the stochastic models or as audio feature vectors calculated in the time domain and or the frequency domain of the speech signals. Other representations of the acoustic models that capture the acoustic features of the speech signal may also be employed and included in the speech annotation .

Returned API call results from the corresponding API may also be received and processed by the voice API interface . The API call result may be in an abstract format that is human unreadable. According to embodiments the voice API interface may interpret those abstract representations of the API call result into a human understandable language and further convert it into an audio API response to present the API call result to the user . Information that can facilitate the interpretation of the API call result may also be included in the annotated API description and utilized by the voice API interface . Additional details regarding the voice API interface and the annotated API description will be presented below with regard to .

As known in the art spoken words may be characterized using acoustic models represented by sequences of feature vectors. A spoken word in an utterance or an input speech signal may be identified by comparing the sequence of feature vectors of the spoken word with one or more sequences of prototype feature vectors which represent the words to be recognized called vocabulary. The speech to text engine may select from the vocabulary a word whose sequence of prototype feature vectors has the smallest distance to the sequence of feature vectors of the spoken word as the recognized word. Alternatively rather than relying on the prototype feature vectors of words in the vocabulary the speech to text engine may employ stochastic models for the generation of feature vectors corresponding to words in the vocabulary to recognize words. For instance an utterance may be recognized as or assigned to a word if the utterance best fits the stochastic model of that word. In one embodiment the stochastic models for modeling the spoken words speech are Hidden Markov Models HMMs .

While prototype feature vectors and stochastic models are described herein as acoustic models for speech to text conversion or speech recognition it should be appreciated that other models or other methods of recognizing speech signals may also be employed such as speech recognition based on neural networks. It is intended that this application encompass the use of any techniques for speech to text conversion.

According to one embodiment the speech to text engine may further employ the speech annotation and or other information contained in the annotated API description during the speech recognition of the voice API command . As briefly mentioned above the speech annotation contained in the annotated API description may include hints on how to recognize command referencing various resources methods or parameters of an API . In embodiments the speech annotation may contain annotations for keywords of the API along with their corresponding phonemes and include acoustic models for the keywords and or the phonemes of the keywords of the API . For example an API may be exposed by a service that can attach a storage volume to a virtual machine instance running on a certain device. The annotated API description of the API may describe the functionality of the service and may further define the format or syntax of a call to the service as attach volume ID instance ID device ID where attach is the name of the operation method volume ID instance ID and device ID are three input parameters required for the operation attach representing a storage volume an instance and a device respectively.

The call request described above will instruct the service to attach a storage volume identified by volume ID to a virtual machine instance identified by instance ID running on a host device with identification device ID. The annotated API description may further define the properties of each of the parameters. For instance the annotated API description may describe that the parameter volume ID ID is a four digit number within a range of 0000 4999 the parameter instance ID is also a four digit number but within a range of 5000 9000 and the parameter device ID is a string consisting of three characters.

In this example the keywords of the API may include attach volume instance device digits 0 through 9 and characters a through z. The speech annotation of the annotated API description may thus include phonemes for these keywords and acoustic models for one or more of the keywords and or the phonemes contained in the keywords. As discussed above the acoustic models may be represented as sequences of feature vectors stochastic models along with their parameters or any other format that is suitable for representing acoustic models for speech signals.

The speech annotation may further include a command model that captures the properties of command language and allows prediction of subsequent words for a given word in the voice API command . Such a command model may be built for example using formats of API calls described in the annotated API description . When a name of an operation such as attach is recognized the command model may be utilized to predict that subsequent words would be the parameters of the operation such as volume ID instance ID and device ID as illustrated in the above example. It should be appreciated that the examples given above are merely illustrative and that other formats of API calls and or other ways of identifying keywords representing acoustic models and command models may be employed in a similar manner.

According to various embodiments the speech annotation may include additional information that may be useful for the speech recognition performed by the speech to text engine . For example the speech annotation may include acoustic models for alternative expressions of keywords of an API . In the example discussed above some users may refer to a storage volume 1234 as volume 1234 or storage volume 1234 while other users might refer to the storage volume as vol 1234. To accommodate different expressions of a keyword the speech annotation may also include acoustic models for each of these alternative expressions. Furthermore the speech annotation may also take into account languages spoken or preferred by the user and include acoustic models that are adapted to the preferred language of the user such as the native language of the user . In some implementations the speech annotation may include pronunciation information following the pronunciation lexicon specification PLS . It will be appreciated that additional information may be included in the speech annotation beyond those described herein and that not all the information described will be present in the speech annotation of every annotated API description for every API .

Once the speech to text engine has converted the voice API command into a text API command the API call interpreter may further translate the text API command into an API call request according to the format defined in the annotated API description . The API call request may then be sent to the corresponding service for execution.

Some of the services may have security requirements. For example a policy controlling the execution of a service may require authentication of the user before executing the requested service . In such a situation a biometric feature may be generated from the voice API command and be utilized to authenticate the user . A biometric profile may have been built for each registered user and the user may be authenticated if the biometric feature generated from the voice API command matches the biometric profile of the user . In one implementation the biometric feature may be utilized along with a private key of the user to sign the API call request . In another implementation the biometric feature may be appended to the API call request and a signature may be generated for both the API call request and the biometric feature using the private key of the user . Additionally or alternatively a portion of the voice API command may be sent along with or in place of the biometric feature as a voice sample of the user . The voice sample may be utilized for the authentication of the user through for example voice recognition. Similarly the user may also provide a password for accessing the API in the voice API command if a password is required for authentication. The speech signal corresponding to the password may also be converted into text and sent together with the API call request .

The returned API call result from the corresponding API of the service may be received at the API call interpreter and translated into a text result interpretation . Depending on the nature of the services the returned API call result may be represented using an abstract format that is human unreadable such as abstract numbers strings or characters. For example a service may return an API call result as a number of value 0 or value 1 with 0 representing a failure of the call and 1 representing a success. Such an abstract representation may not be meaningful to the user if the user is not provided with further explanation of the results. Sometimes such abstract representation of the API call result may also be confusing because different services may return the same result but with different meanings. For instance one service may use a number with value 0 to represent a failure of the service call while another service may use the same value 0 to represent a success of the service call.

According to embodiments the API call interpreter may be employed to interpret the API call result into a text result interpretation using a human understandable language or in a human readable format. Such a text result interpretation when converted into the audio API response and presented to the user may be readily understood by the user without referring to further details of the API . In one implementation the interpretations of the API call result or a mapping of the API call result to a text result may be included in the annotated API description . In the example discussed above an API call result for an API call attach volume ID instance ID device ID may be returned containing a value 0 if the operation fails or a value 1 if the operation succeeds. The annotated API description may contain an interpretation for a returned value 1 as volume ID has been successfully attached to instance ID associated with device ID and an interpretation for a returned value 0 as attaching volume ID to instance ID associated with device ID has failed. 

Based on the annotated API description and the actual API call request the API call interpreter may generate the text result interpretation corresponding to the API call result . For example for a API call of attach 1234 5678 sdh the generated text result interpretation may be volume 1234 has been successfully attached to instance 5678 associated with device sdh if value 1 is returned or attaching volume 1234 to instance 5678 associated with device sdh has failed if value 0 is returned.

According to another embodiment rather than obtaining interpretation from the annotated API description the API call interpreter may utilize natural language generation NLG technology to convert the abstract representation of the API call result into a natural language representation that can be understood by the user . Furthermore the user may also specify a preference on how much detail he she wants to hear in the results such as through user account setup configuration or through the voice API command . The API call interpreter may address such a requirement by processing the text result interpretation and or the API call result based on the preference of the user such as by filtering transforming or simplifying the API call result or adding or removing interpretations. It should be appreciated that the examples presented above for generating interpretations of the API call result are for illustration only and should not be construed as limiting.

The generated text result interpretation may then be converted by the text to speech engine using speech synthesis technology into the corresponding audio format and output as an audio API response to the client . As known in the art speech synthesis may be realized by assigning phonetic transcriptions to each word and dividing and marking the input text into prosodic units. Phonetic transcriptions and prosody information collectively called symbolic linguistic representation may then be converted into sound by a synthesizer.

In some implementations speech synthesis may be achieved by concatenating pieces of recorded speech that are stored in a database. Apart from these speech synthesis technologies the text to speech engine may also benefit from speech synthesis information contained in the speech annotation . By way of example and not limitation the speech synthesis information may include phonetic transcriptions for words that may be contained in the text result interpretation prosody information for the text result interpretation and or access information for recorded speech of elements in the text result interpretation if there is any.

In one embodiment the speech annotation may contain prosody information for the text result interpretation along with phonetic transcriptions for a subset of the words included in the text result interpretation . The subset of words may include words contained in the text result interpretation that may not be understood by the text to speech engine . For example the text result interpretation may contain terms such as abbreviations that are specific to the service and may have unique pronunciations. In such a scenario the speech annotation may be annotated with phonetic transcriptions for these specific terms. Some of these terms may have corresponding recorded speech stored in a database that is accessible to the text to speech engine . The access information including the location of the recorded speech may also be contained in the speech annotation . The text to speech engine may then choose to synthesize the speech using the phonetic transcriptions or to use the recorded speech depending on the situations of the text to speech engine such as workload network bandwidth storage availability etc. The generated audio API response may then be sent to the client for presentation to the user .

It should also be appreciated that the logical operations described herein with respect to and the other FIGS. may be implemented 1 as a sequence of computer implemented acts or program modules running on a computing system and or 2 as interconnected machine logic circuits or circuit modules within the computing system. The implementation of the various components described herein is a matter of choice dependent on the performance and other requirements of the computing system. Accordingly the logical operations described herein are referred to variously as operations structural devices acts or modules. These operations structural devices acts and modules may be implemented in software in firmware in special purpose digital logic and any combination thereof. It should also be appreciated that more or fewer operations may be performed than shown in the FIGS. and described herein. These operations may also be performed in parallel or in a different order than those described herein.

The routine begins at operation where one or more annotated API descriptions are received from one or more APIs of the services . As discussed above an annotated API description may contain information for one API or for multiple APIs . The annotated API description may provide a machine readable description indicating how the service defined by the API may be called including but not limited to information such as the format or syntax of the API calls operations methods and resources involved in the API calls input parameters and or outputs of the API calls properties of the input parameters and output results and others. The annotated API description may further include interpretations of API call result .

The annotated API description may also contain speech annotation that may be utilized in processing API calls through voice commands. The speech annotation may contain but is not limited to hints on how to recognize words terms referencing various resources methods or parameters of a service in the voice API command . In embodiments the speech annotation may include acoustic models for the keywords and or the phonemes contained in the keywords of the API . The acoustic models may be represented as for example stochastic models modeling the speech signals for the keywords and or the phonemes of the keywords along with parameters for each of the stochastic models or as audio feature vectors calculated in the time domain and or the frequency domain of the speech signals. The speech annotation may further include a command model that captures the properties of command language and allows prediction of subsequent words for a given word in the voice API command . In one implementation the command model may be specified according to speech recognition grammar specification SRGS and semantic interpretation for speech recognition SISR .

The speech annotation may also include acoustic models for alternative expressions of keywords of an API and acoustic models that are adapted to the native or preferred language of the user . Information to facilitate the speech synthesis may be further included in the speech annotation . The speech synthesis information may include phonetic transcriptions for words that may be contained in the text result interpretation prosody information for the text result interpretation and or access information for recorded speech of elements in the text result interpretation if there is any. In some implementations the speech annotation or a portion of it may be in the format of a voice extensible markup language VoiceXML and or be generated by an interactive voice response IVR server.

From operation the routine then proceeds to operation where a voice API command may be received from a client . As discussed above the voice API command may be sent by the user through the client instantly or may be retrieved from a storage medium accessible to the client which stores pre recorded voice API commands by the user . The pre recorded voice API commands may be retrieved and sent to the voice API interface at a predetermined time specified by the user or periodically according to rules set up by the user .

The routine then proceeds from operation to operation where the received voice API command may be converted into a text API command through speech recognition. The speech annotation contained in the annotated API description may be utilized in the conversion. For instance the acoustic models for the keywords and or the phonemes contained in the keywords and the command model of the API may be utilized to establish accurate acoustic models and or to reduce the search space of recognized words for the conversion of the corresponding text API command .

From operation the routine then proceeds to operation where a determination may be made as to whether the converted text API command needs to be verified with the user . Such a determination may be made based on the speech recognition results. For example some words may be recognized with low confidence and verification with the user may help to confirm the recognized words and or correct any error in the converted text API command . In these situations the operation may decide that verification is needed for the converted text API command if the confidence score of the recognition is below a certain threshold. There may also be situations wherein some parameters or keywords that should be included in an API call are missing from the recognized text API command and thus verification or extra information may be needed from the user . In other scenarios the user may prefer to verify every text API command that has been converted to ensure the accuracy of the submission of the API command in which case the operation may always decide that verification is needed.

Upon determining that the converted text API command needs to be verified the routine proceeds to operation where the text API command is verified with the user . In one implementation the text API command may be converted into a speech signal and presented to the user . The user may then provide his her feedback through voice input. Depending on the nature of the verification the feedback may be a correction of terms in the voice API command a resubmission of the voice API command or simply a spoken word such as a yes to confirm the correctness of the text API command . In some embodiments the feedback provided by the user may also be employed to improve the speech to text engine . For example a positive confirmation from the user may be utilized to modify the calculation of the confidence score to increase the confidence score of the same words in future speech recognitions. A correction of words by the user may be incorporated into the current acoustic model for those words so that the future recognition would output the correct words. It should be appreciated that these examples are only illustrative and should not be construed as limiting. Other ways of incorporating the feedback provided by the user to improve the speech to text engine may be utilized by following the same principles described above.

From operation or if it is determined at operation that no verification is needed from the user the routine proceeds to operation where the text API command may be translated into an API call request . The translation may be performed by extracting keywords from the text API command and generating an API call request in a format defined in the annotated API description based on those keywords. The routine may then proceed to operation where the generated API call request may be sent to the corresponding API of the service for execution. From operation the routine proceeds to operation where it ends.

While illustrates that the verification of the voice API command from the user occurs after the generation of the text API command it should be understood that the verification can be performed at any operation of the routine . For example the verification can be conducted during the conversion of the voice API command to the text API command at operation or during or after the generation of the API call request at operation .

The routine begins at operation where an API call result may be received at the voice API interface from a corresponding API of a service . The routine then proceeds to operation where a text result interpretation may be generated from the received API call result . As discussed above the text result interpretation may be generated by translating abstract representations of the API call result to a human understandable language. Such a translation may be provided or indicated in the corresponding annotated API description or may be generated by the voice API interface based on the annotated API description . From operation the routine proceeds to operation where the text result interpretation may be converted into an audio API response through speech synthesis. The speech synthesis information contained in the speech annotation of the annotated API description may be utilized to facilitate the speech synthesis such as the phonetic transcriptions of words contained in the text result interpretation prosody information of the text result interpretation and or access information for recorded speech of elements in the text result interpretation . Other information may also be included in the speech annotation and be utilized in the speech synthesis process.

Once the audio API response is generated the routine may then proceed to operation where the audio API response may be sent to the client for presentation to the user . The audio API response may be presented to the user instantly or may be stored in a storage medium accessible to the user . The user may then retrieve and listen to the audio API response at his her leisure. The routine then proceeds to operation where it ends.

The computer includes a baseboard or motherboard which is a printed circuit board to which a multitude of components or devices may be connected by way of a system bus or other electrical communication paths. In one illustrative embodiment one or more central processing units CPUs operate in conjunction with a chipset . The CPUs may be standard programmable processors that perform arithmetic and logical operations necessary for the operation of the computer .

The CPUs perform operations by transitioning from one discrete physical state to the next through the manipulation of switching elements that differentiate between and change these states. Switching elements may generally include electronic circuits that maintain one of two binary states such as flip flops and electronic circuits that provide an output state based on the logical combination of the states of one or more other switching elements such as logic gates. These basic switching elements may be combined to create more complex logic circuits including registers adders subtractors arithmetic logic units floating point units and the like.

The chipset provides an interface between the CPUs and the remainder of the components and devices on the baseboard . The chipset may provide an interface to a random access memory RAM used as the main memory in the computer . The chipset may further provide an interface to a computer readable storage medium such as a read only memory ROM or non volatile RAM NVRAM for storing basic routines that help to startup the computer and to transfer information between the various components and devices. The ROM or NVRAM may also store other software components necessary for the operation of the computer in accordance with the embodiments described herein.

The computer may operate in a networked environment using logical connections to remote computing devices and computer systems through a network such as the local area network . The chipset may include functionality for providing network connectivity through a network interface controller NIC such as a gigabit Ethernet adapter. The NIC is capable of connecting the computer to other computing devices over the network . It should be appreciated that multiple NICs may be present in the computer connecting the computer to other types of networks and remote computer systems.

The computer may be connected to a mass storage device that provides non volatile storage for the computer. The mass storage device may store system programs application programs other program modules and data which have been described in greater detail herein. The mass storage device may be connected to the computer through a storage controller connected to the chipset . The mass storage device may consist of one or more physical storage units. The storage controller may interface with the physical storage units through a serial attached SCSI SAS interface a serial advanced technology attachment SATA interface a fiber channel FC interface or other type of interface for physically connecting and transferring data between computers and physical storage units.

The computer may store data on the mass storage device by transforming the physical state of the physical storage units to reflect the information being stored. The specific transformation of physical state may depend on various factors in different implementations of this description. Examples of such factors may include but are not limited to the technology used to implement the physical storage units whether the mass storage device is characterized as primary or secondary storage and the like.

For example the computer may store information to the mass storage device by issuing instructions through the storage controller to alter the magnetic characteristics of a particular location within a magnetic disk drive unit the reflective or refractive characteristics of a particular location in an optical storage unit or the electrical characteristics of a particular capacitor transistor or other discrete component in a solid state storage unit. Other transformations of physical media are possible without departing from the scope and spirit of the present description with the foregoing examples provided only to facilitate this description. The computer may further read information from the mass storage device by detecting the physical states or characteristics of one or more particular locations within the physical storage units.

In addition to the mass storage device described above the computer may have access to other computer readable storage media to store and retrieve information such as program modules data structures or other data. It should be appreciated by those skilled in the art that computer readable storage media can be any available media that provides for the storage of non transitory data and that may be accessed by the computer .

By way of example and not limitation computer readable storage media may include volatile and non volatile removable and non removable media implemented in any method or technology. Computer readable storage media includes but is not limited to RAM ROM erasable programmable ROM EPROM electrically erasable programmable ROM EEPROM flash memory or other solid state memory technology compact disc ROM CD ROM digital versatile disk DVD high definition DVD HD DVD BLU RAY or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium that can be used to store the desired information in a non transitory fashion.

The mass storage device may store an operating system utilized to control the operation of the computer . According to one embodiment the operating system comprises the LINUX operating system. According to another embodiment the operating system comprises the WINDOWS SERVER operating system from MICROSOFT Corporation. According to further embodiments the operating system may comprise the UNIX or SOLARIS operating systems. It should be appreciated that other operating systems may also be utilized. The mass storage device may store other system or application programs and data utilized by the computer such as the voice API interface the annotated API description and or any of the other software components and data described above. The mass storage device might also store other programs and data not specifically identified herein.

In one embodiment the mass storage device or other computer readable storage media is encoded with computer executable instructions which when loaded into the computer transforms the computer from a general purpose computing system into a special purpose computer capable of implementing the embodiments described herein. These computer executable instructions transform the computer by specifying how the CPUs transition between states as described above. According to one embodiment the computer has access to computer readable storage media storing computer executable instructions which when executed by the computer perform the routines described above with regard to . The computer might also include computer readable storage media for performing any of the other computer implemented operations described herein.

The computer may also include one or more input output controllers for receiving and processing input from a number of input devices such as a keyboard a mouse a touchpad a touch screen an electronic stylus or other type of input device. Similarly the input output controller may provide output to a display such as a computer monitor a flat panel display a digital projector a printer a plotter or other type of output device. It will be appreciated that the computer may not include all of the components shown in may include other components that are not explicitly shown in or may utilize an architecture completely different than that shown in .

Based on the foregoing it should be appreciated that technologies for invoking API calls through voice commands has been presented herein. Moreover although the subject matter presented herein has been described in language specific to computer structural features methodological acts and computer readable media it is to be understood that the invention defined in the appended claims is not necessarily limited to the specific features acts or media described herein. Rather the specific features acts and mediums are disclosed as example forms of implementing the claims.

The subject matter described above is provided by way of illustration only and should not be construed as limiting. Furthermore the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure. Various modifications and changes may be made to the subject matter described herein without following the example embodiments and applications illustrated and described and without departing from the true spirit and scope of the present invention which is set forth in the following claims.

