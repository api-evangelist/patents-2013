---

title: Providing virtual storage processor (VSP) mobility with induced file system format migration
abstract: A technique provides virtual storage processor (VSP) mobility with induced file system format migration. The technique involves receiving a command specifying (i) a source VSP which includes source file systems which store data in a source file system format, (ii) a destination, and (iii) a destination file system format. The technique further involves forming, in response to the command, a destination VSP which includes destination file systems. The technique further involves copying, while the source VSP is accessed by a set of host computers, data from the source file systems of the source VSP to the destination file systems of the destination VSP. The destination file systems store the copied data in the destination file system format. The technique further involves synchronizing, after the data is copied from the source file systems to the destination file systems, state changes between the source VSP and the destination VSP.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09305071&OS=09305071&RS=09305071
owner: EMC Corporation
number: 09305071
owner_city: Hopkinton
owner_country: US
publication_date: 20130930
---
Data storage systems typically include one or more physical storage processors SPs accessing an array of disk drives and or electronic flash drives. Each SP is connected to a network such as the Internet and or a storage area network SAN and receives transmissions over the network from host computing devices hosts . Each SP can support network attached storage NAS as well. The transmissions from the hosts include IO requests also called host IOs. Some IO requests direct the SP to read data from an array whereas other IO requests direct the SP to write data to the array. Also some IO requests perform block based data requests where data are specified by LUN Logical Unit Number and offset values whereas others perform file based requests where data are specified using file names and paths. Block based IO requests typically conform to a block based protocol such as Fibre Channel or iSCSI Internet SCSI where SCSI is an acronym for Small Computer System Interface for example. File based IO requests typically conform to a file based protocol such as NFS Network File System CIFS Common Internet File System or SMB Server Message Block for example.

In some data storage systems an SP may operate one or more virtual data movers. As is known a virtual data mover is a logical grouping of file systems and servers that is managed by the SP and provides a separate context for managing host data stored on the array. A single SP may provide multiple virtual data movers for different users or groups. For example a first virtual data mover may organize data for users in a first department of a company whereas a second virtual data mover may organize data for users in a second department of the company. Each virtual data mover may include any number of host file systems for storing user data.

Improved techniques involve providing virtual storage processor VSP mobility with induced file system format migration by migrating all file systems i.e. host file systems as well as hidden configuration file systems file systems which are not directly accessible to hosts and which store control data personality data metadata etc. of source VSPs to destinations. Such migration may be handled by a migration tool which is within the input output I O stack but above block based storage and replication. Accordingly all data is migrated i.e. host data as well as configuration data . Furthermore file system and file characteristics such as file system identifiers and inode numbers assigned to the files of the file systems can be preserved i.e. identically assigned to the files storing the copied data at the destination so that migration is transparent to the hosts e.g. file handles in use by the hosts remain usable after migration .

One embodiment is directed to a method of providing VSP mobility and concurrent file system format migration. The method includes receiving by processing circuitry a VSP move command from a user the VSP move command specifying i a source VSP which includes multiple source file systems which store data in a source file system format ii a destination and iii a destination file system format which is different than the source file system format. The method further includes forming by the processing circuitry and in response to the VSP move command a destination VSP which includes multiple destination file systems. The method further includes copying by the processing circuitry and while the source VSP is accessed by a set of host computers data from the multiple source file systems of the source VSP to the multiple destination file systems of the destination VSP. The multiple destination file systems store the copied data in the destination file system format. The method further includes synchronizing by the processing circuitry and after the data is copied from the multiple source file systems to the multiple destination file systems state changes between the source VSP and the destination VSP.

In some arrangements the source VSP includes i a first set of configuration file systems storing source VSP configuration data and ii a first set of host file systems storing host data. In these arrangements forming the destination VSP includes providing i a second set of configuration file systems to store destination VSP configuration data and ii a second set of host file systems to store a copy of the host data. The first and second sets of configuration file systems are transparent to the set of host computers i.e. hidden from the set of host computer and the first and second sets of host file systems being visible to the set of host computers i.e. being accessible by the set of host computers .

In some arrangements which involve migration between formats having different block pointer sizes the first set of configuration file systems and the first set of host file systems include existing inode structures to store the data using block pointers having a first block pointer size defined by the source file system format. In these arrangements providing the second set of configuration file systems and the second set of host file systems includes provisioning the second set of configuration file systems and the second set of host file systems with new inode structures to store the copied data using block pointers having a second block pointer size defined by the destination file system format. The second block pointer size e.g. 64 bits is different than the first block pointer size e.g. 32 bits .

It should be understood that in the cloud context the processing circuitry is formed by remote computer resources distributed over a network. Such a computing environment is capable of providing certain advantages such as enhanced fault tolerance load balancing processing flexibility etc.

Other embodiments are directed to electronic systems and apparatus processing circuits computer program products and so on. Some embodiments are directed to various methods electronic components and circuitry which are involved in providing VSP mobility and concurrent file system format migration.

One should appreciate that in a conventional virtual data mover arrangement the SP has a root file system with mount points to which the host file systems of each virtual data mover are mounted. Thus the SP and all its virtual data movers form a single large directory and all share a common namespace. Hosts can access their virtual data mover managed data by connecting to the SP over the network logging on and specifying paths relative to the SP s root where their data are kept. The typical arrangement thus requires hosts to access data of a virtual data mover using paths that are referenced to and dependent upon the root of the SP.

In addition settings for prescribing virtual data mover operations are conventionally stored in the root file system of the SP. Many of these settings are global to all virtual data movers operating on the SP others may be specific to particular virtual data movers.

Unfortunately the intermingling of virtual data mover content within an SP s root file system impairs the ease of mobility and management of virtual data movers. For example administrators wishing to move a virtual data mover e.g. its file systems settings and servers from one SP to another SP must typically perform many steps on a variety different data objects. File systems server configurations and other settings may need to be moved one at a time. Also as the contents of different virtual data movers are often co located care must be taken to ensure that changes affecting one virtual data mover do not disrupt the operation of other virtual data movers.

In contrast with the conventional approach an improved technique for managing host data in a data storage apparatus provides VSPs as substantially self describing and independent entities. Each VSP has its own namespace which is independent of the namespace of any other VSP and is independent of the namespace of the SP s i.e. the physical SP s root file system. Each VSP also has its own network address. Hosts may thus access VSPs directly without having to include path information relative to the root of the SP on which the VSPs are operated. VSPs can thus be moved from one physical SP to another with little or no disruption to hosts which may in many cases continue to access the VSPs on the new SPs using the same paths as were used to access the VSPs on the original SPs.

In some examples each VSP includes within its namespace a configuration file system storing configuration settings for operating the VSP. These configuration settings include for example network interface settings and internal settings that describe the VSPs personality i.e. the manner in which the VSP interacts on the network. By providing these settings as part of the VSP itself e.g. within the file systems of the VSP the VSP can be moved from one physical SP to another substantially as a unit. The increased independence of the VSP from its hosting SP promotes many aspects of VSP management including for example migration replication failover trespass multi tenancy load balancing and gateway support.

In some examples the independence of VSPs is further promoted by storing data objects of VSPs in the form of respective files. These data objects may include for example file systems LUNs virtual storage volumes vVols and virtual machine disks VMDKs . Each such file is part of a set of internal file systems of the data storage apparatus. Providing data objects in the form of files of a set of internal file systems promotes independence of VSPs and unifies management of file based objects and block based objects.

In accordance with improvements hereof certain embodiments are directed to a method of managing host data on a data storage apparatus connected to a network. The method includes storing a network address and a set of host data objects accessible within a namespace of a virtualized storage processor VSP operated by a physical storage processor of the data storage apparatus. The namespace includes only names of objects that are specific to the VSP. The method further includes receiving by the physical storage processor a transmission over the network from a host computing device. The transmission is directed to a network address and includes an IO request designating a pathname to a host data object to be written or read. The method still further includes identifying the host data object designated by the IO request by i matching the network address to which the transmission is directed with the network address stored for the VSP to identify the VSP as the recipient of the IO request and ii locating the host data object within the namespace of the VSP using the pathname. The IO request is then processed to complete the requested read or write operation on the identified host data object.

Other embodiments are directed to computerized apparatus and computer program products. Some embodiments involve activity that is performed at a single location while other embodiments involve activity that is distributed over a computerized environment e.g. over a network .

Embodiments of the invention will now be described. It is understood that such embodiments are provided by way of example to illustrate various features and principles of the invention and that the invention hereof is broader than the specific example embodiments disclosed.

An improved technique involves providing virtual storage processor VSP mobility with induced file system format migration by migrating all file systems i.e. host file systems as well as hidden configuration file systems file systems which are not directly accessible to hosts and which store control data personality data metadata etc. of source VSPs to destinations. Such migration may be handled by a migration tool which is within the input output I O stack but above block based storage and replication. Accordingly all data is migrated i.e. host data as well as configuration data . Furthermore file system and file characteristics such as file system identifiers and inode numbers assigned to the files of the file systems can be preserved i.e. identically assigned to the files storing the copied data at the destination so that mobility as well as migration are transparent to the hosts e.g. file handles currently in use by the hosts remain usable after migration .

An improved technique for managing host data in a data storage apparatus provides virtualized storage processors VSPs as substantially self describing and independent constructs.

The network can be any type of network such as for example a storage area network SAN local area network LAN wide area network WAN the Internet some other type of network and or any combination thereof. In an example the hosts N connect to the SP using various technologies such as Fibre Channel iSCSI NFS SMB 3.0 and CIFS for example. Any number of hosts N may be provided using any of the above protocols some subset thereof or other protocols besides those shown. As is known Fibre Channel and iSCSI are block based protocols whereas NFS SMB 3.0 and CIFS are file based protocols. The SP is configured to receive 10 requests N in transmissions from the hosts N according to both block based and file based protocols and to respond to such IO requests N by reading or writing the storage .

The SP is seen to include one or more communication interfaces control circuitry e.g. a set of processors and memory . The communication interfaces include for example adapters such as SCSI target adapters and network interface adapters for converting electronic and or optical signals received from the network to electronic form for use by the SP . The set of processors includes one or more processing chips and or assemblies. In a particular example the set of processors includes numerous multi core CPUs. The memory includes both volatile memory e.g. RAM and non volatile memory such as one or more ROMs disk drives solid state drives SSDs and the like. The set of processors and the memory are constructed and arranged to carry out various methods and functions as described herein. Also the memory includes a variety of software constructs realized in the form of executable instructions. When the executable instructions are run by the set of processors the set of processors are caused to carry out the operations of the software constructs. Although certain software constructs are specifically shown and described it is understood that the memory typically includes many other software constructs which are not shown such as various applications processes and daemons.

As shown the memory includes an operating system such as Unix Linux or Windows for example. The operating system includes a kernel . The memory is further seen to include a container . In an example the container is a software process that provides an isolated userspace execution context within the operating system . In various examples the memory may include multiple containers like the container with each container providing its own isolated userspace instance. Although containers provide isolated environments that do not directly interact and thus promote fault containment different containers can be run on the same kernel and can communicate with one another using inter process communication IPC mediated by the kernel . Containers are well known features of Unix Linux and other operating systems.

In the example of only a single container is shown. Running within the container is an IO stack and multiple virtualized storage processors VSPs . The IO stack provides an execution path for host IOs e.g. N and includes a front end and a back end . The VSPs each run within the container and provide a separate context for managing host data. In an example each VSP manages a respective set of host file systems and or other data objects and uses servers and settings for communicating over the network with its own individual network identity. Although three VSPs are shown it is understood that the SP may include as few as one VSP or as many VSPs as the computing resources of the SP and storage resources of the storage allow.

Although the VSPs each present an independent and distinct identity it is evident that the VSPs are not in this example implemented as independent virtual machines. Rather all VSPs operate in userspace and employ the same kernel of the SP . Although it is possible to implement the VSPs as independent virtual machines each including a virtualized kernel it has been observed that VSPs perform faster when the kernel is not virtualized.

Also it is observed that the VSPs all run within the container i.e. within a single userspace instance. Again the arrangement shown reflects a deliberate design choice aimed at optimizing VSP performance. It is understood though that alternative implementations could provide different VSPs in different containers or could be provided without containers at all.

The memory is further seen to store a configuration database . The configuration database stores system configuration information including settings related to the VSPs and their data objects. In other implementations the configuration database is stored elsewhere in the data storage apparatus such as on a disk drive separate from the SP but accessible to the SP e.g. over a backplane or network.

In operation the hosts N issue IO requests N to the data storage apparatus . The IO requests N may include both block based requests and file based requests. The SP receives the IO requests N at the communication interfaces and passes the IO requests to the IO stack for further processing.

At the front end of the IO stack processing includes associating each of the IO requests N with a particular one of the VSPs . In an example each VSP stores a network address e.g. an IP address in a designated location within its file systems. The front end identifies the network address to which each IO request is directed and matches that address with one of the network addresses stored with the VSPs . The front end thus uses the network address to which each IO request is sent to identify the VSP to which the IO request is directed. Further processing of the IO request is then associated e.g. tagged with an identifier of the matching VSP such that the IO request is processed within a particular VSP context. Any data logging metrics collection fault reporting or messages generated while the IO request is being processed are stored with the associated VSP e.g. in a file system dedicated to the VSP . Also any path information provided with the IO request e.g. to a particular directory and file name is interpreted within the namespace of the identified VSP.

Processing within the front end may further include caching data provided with any write IOs and mapping host data objects e.g. host file systems LUNs vVols VMDKs etc. to underlying files stored in a set of internal file systems. Host IO requests received for reading and writing both file systems and LUNs are thus converted to reads and writes of respective files. The IO requests then propagate to the back end where commands are executed for reading and or writing the physical storage .

In an example processing through the IO stack is performed by a set of threads maintained by the SP in a set of thread pools. When an IO request is received a thread is selected from the set of thread pools. The IO request is tagged with a VSP identifier and the selected thread runs with the context of the identified VSP. Typically multiple threads from different thread pools contribute to the processing of each IO request there are many processing layers . Multiple threads from the thread pools can process multiple IO requests simultaneously i.e. in parallel on the data objects of any one VSP or multiple VSPs.

Although shows the front end and the back end together in an integrated form the front end and back end may alternatively be provided on separate SPs. For example the IO stack may be implemented in a modular arrangement with the front end on one SP and the back end on another SP. The IO stack may further be implemented in a gateway arrangement with multiple SPs running respective front ends and with a back end provided within a separate storage array. The back end performs processing that is similar to processing natively included in many block based storage arrays. Multiple front ends can thus connect to such arrays without the need for providing separate back ends. In all arrangements processing through both the front end and back end is preferably tagged with the particular VSP context such that the processing remains VSP aware.

The storage pool organizes elements of the storage in the form of slices. A slice is an increment of storage space such as 256 MB in size which is obtained from the storage . The pool may allocate slices to lower deck file systems for use in storing their files. The pool may also deallocate slices from lower deck file systems if the storage provided by the slices is no longer required. In an example the storage pool creates slices by accessing RAID groups formed from the storage dividing the RAID groups into FLUs Flare LUNs and further dividing the FLU s into slices.

Continuing with reference to the example shown in a user object layer includes a representation of a LUN and of an HFS host file system and a mapping layer includes a LUN to file mapping and an HFS to file mapping . The LUN to file mapping maps the LUN to a first file F and the HFS to file mapping maps the HFS to a second file F . Through the LUN to file mapping any set of blocks identified in the LUN by a host IO request is mapped to a corresponding set of blocks within the first file . Similarly through the HFS to file mapping any file or directory of the HFS is mapped to a corresponding set of blocks within the second file . The HFS is also referred to herein as an upper deck file system which is distinguished from the lower deck file systems which are for internal use.

In this example a first lower deck file system includes the first file and a second lower deck file system includes the second file . Each of the lower deck file systems and includes an inode table and respectively . The inode tables and provide information about files in respective lower deck file systems in the form of inodes. For example the inode table of the first lower deck file system includes an inode which provides file specific information about the first file . Similarly the inode table of the second lower deck file system includes an inode which provides file specific information about the second file . The information stored in each inode includes location information e.g. block locations where the respective file is stored and may thus be accessed as metadata to identify the locations of the files and in the storage .

Although a single file is shown for each of the lower deck file systems and it is understood that each of the lower deck file systems and may include any number of files each with its own entry in the respective inode table. In one example each lower deck file system stores not only the file F or F for the LUN or HFS but also snaps of those objects. For instance the first lower deck file system stores the first file along with a different file for every snap of the LUN . Similarly the second lower deck file system stores the second file along with a different file for every snap of the HFS .

As shown a set of slices is allocated by the storage pool for storing the first file and the second file . In the example shown slices S through S are used for storing the first file and slices S through S are used for storing the second file . The data that make up the LUN are thus stored in the slices S through S whereas the data that make up the HFS are stored in the slices S through S.

In some examples each of the lower deck file systems and is associated with a respective volume such as a sparse LUN. Sparse LUNs provide an additional layer of mapping between the lower deck file systems and the pool and allow the lower deck file systems to operate as file systems normally do by accessing underlying volumes. Additional details about sparse LUNs and their relation to lower deck file systems may be found in U.S. Pat. No. 7 631 155 which is hereby incorporated by reference in its entirety. The incorporated patent uses the term container file system to refer to a construct similar to the lower deck file system disclosed herein.

Although the example of shows storage of a LUN and a host file system in respective lower deck file systems and it is understood that other data objects may be stored in one or more lower deck file systems in a similar manner. These may include for example file based vVols block based vVols and VMDKs.

For example the VSP includes a first lower deck file system and a second lower deck file system . The first lower deck file system includes a file FA which provides a file representation of a first host file system . Similarly the second lower deck file system includes a file FB which provides a file representation of a second host file system . The host file systems and are upper deck file systems which may be made available to hosts N for storing file based host data. HFS to file mappings like the HFS to file mapping are understood to be present although not shown in for expressing the files FA and FB in the form of upper deck file systems. Although only two host file systems and are shown it is understood that the VSP may include any number of host file systems. In an example a different lower deck file system is provided for each host file system. The lower deck file system stores the file representation of the host file system and if snaps are turned on any snaps of the host file system. In a similar manner to that described in connection with each of the lower deck file systems and includes a respective inode table allowing the files FA and FB and their snaps to be indexed within the respective lower deck file systems and accessed within the storage .

In some examples the VSP also includes one or more lower deck file systems for storing file representations of LUNs. For example a lower deck file system stores a file FC which provides a file representation of a LUN . A LUN to file mapping not shown but similar to the mapping expresses the file FC in the form of a LUN which may be made available to hosts N for storing block based host data. In an example the lower deck file system stores not only the file FC but also snaps thereof and includes an inode table in essentially the manner described above.

The VSP further also includes a lower deck file system . In an example the lower deck file system stores file representations FD and FE of two internal file systems of the VSP a root file system and a configuration file system . In an alternative arrangement the files FD and FE are provided in different lower deck file systems. In an example the lower deck file system also stores snaps of the files FD and FE and files are accessed within the lower deck file system via file system to file mappings and using an inode table substantially as described above.

In an example the root file system has a root directory designated with the slash and sub directories as indicated. Any number of sub directories may be provided within the root file system in any suitable arrangement with any suitable file structure the example shown is merely illustrative. As indicated one sub directory Local stores for example within constituent files information about the local environment of the SP such as local IP sub net information geographical location and so forth. Another sub directory Rep stores replication information such as information related to any ongoing replication sessions. Another sub directory Cmd Svc stores command service information and yet another sub directory MPs stores mount points.

In the example shown the directory MPs of the root file system provides mount points e.g. directories on which data objects e.g. file systems and or LUNs are mounted. For example the host file systems and are respectively mounted on mount points MP and MP the LUN is mounted on the mount point MP and the configuration file system is mounted on the mount point MP. In an example establishment of the mount points MP MP and execution of the mounting operations for mounting the file systems and the LUN onto the mount points MP MP are provided in a batch file stored in the configuration file system e.g. in Host Objects .

The root file system has a namespace which includes the names of the root directory sub directories and files that belong to the root file system . The file systems and also each have respective namespaces. The act of mounting the file systems and onto the mount points MP MP and MP of the root file system serves to join the namespace of each of the file systems and with the namespace of the root file system to form a single namespace that encompasses all the file systems and . This namespace is specific to the VSP and is independent of namespaces of any other VSPs. Although the LUN is a block based host object that does not itself have a namespace it is nevertheless part of the namespace of the VSP and is accessible using a path within the namespace of the VSP .

Also it is understood that the root file systems of the VSPs are separate and distinct from any root file system of the physical SP . Thus if the SP has its own root file system that root file system is separate from the root file systems of the VSPs and does not share its namespace with the namespace of any of the root file systems of the VSPs.

Although the VSP is seen to include file systems and LUNs other host objects may be included as well. These include for example file based vVols block based vVols and VMDKs. Such host objects may be provided as file representations in lower deck file systems and mounted to mount points of the root file system .

As its name suggests the configuration file system stores configuration settings for the VSP . These settings include settings for establishing the personality of the VSP i.e. the manner in which the VSP interacts over the network . Although the configuration file system is shown with a particular directory structure it is understood that any suitable directory structure can be used. In an example the configuration file system stores the following elements 

Many configuration settings are established at startup of the VSP . Some configuration settings are updated as the VSP is operated. The configuration file system preferably does not store host data.

Although has been shown and described with reference to a particular VSP it is understood that all of the VSPs may include a root file system a configuration file system and at least one host file system or LUN substantially as shown. Particular host objects and configuration settings differ however from one VSP to another.

By storing the configuration settings of VSPs within the file systems of the VSPs themselves and providing a unique namespace for each VSP VSPs are made to be highly independent both of other VSPs and of the particular SPs on which they are provided. For example moving a VSP from one SP to another involves copying its lower deck file systems or some subset thereof from a source SP to a target SP starting the VSP s servers on the target SP in accordance with the configuration settings and resuming operation on the target SP. As the paths for accessing data objects on VSPs are not rooted to the SPs on which they are run hosts may often continue to access migrated VSPs using the same instructions as were used prior to moving the VSPs.

Although shows only a single record for a single VSP it is understood that the configuration database may store records like the record for any number of VSPs including all VSPs of the data storage apparatus . During start up of the data storage apparatus or at some other time a computing device of the data storage apparatus reads the configuration database and launches a particular VSP or a group of VSPs on the identified SPs. As a VSP is starting the SP that owns the VSP reads the configuration settings of the configuration file system to configure the various servers of the VSP and to initialize its communication protocols. The VSP may then be operated on the identified SP i.e. the SP may then be operated with the particular VSP s context.

It is understood that VSPs operate in connection with the front end of the IO stack . The VSPs thus remain co located with their respective front ends in modular and gateway arrangements.

At step a network address and a set of host data objects are stored in a data storage apparatus. The set of host data objects are accessible within a namespace of a virtualized storage processor VSP operated by a physical storage processor of the data storage apparatus. The namespace includes only names of objects that are specific to the VSP. For example an IP address of the VSP is stored in a file of a directory of the configuration file system . The VSP runs on the SP of the data storage apparatus . A set of host objects including host file systems and and LUN are also stored in the data storage apparatus . These host objects are made accessible within the namespace of the VSP by mounting these data objects to mount points MP MP within the root file system and thus merging their namespaces with that of the root file system . The resulting merged namespace includes only names of objects that are specific to the VSP .

At step a transmission is received by the physical storage processor over the network from a host computing device. The transmission is directed to a network address and includes an IO request designating a pathname to a host data object to be written or read. For example the SP receives a transmission over the network from one of the hosts N . The transmission is directed to a particular IP address and includes an IO request e.g. one of N . The IO request designates a pathname to a host data object to be written or read. The pathname may point to any of the host file systems or to the LUN or to any file or offset range accessible through the host file systems or or the LUN . The pathname may also point to a vVol or VMDK for example or to any other object which is part of the namespace of the VSP .

At step the host data object designated by the IO request is identified by i matching the network address to which the transmission is directed with the network address stored for the VSP to identify the VSP as the recipient of the IO request and ii locating the host data object within the namespace of the VSP using the pathname. For example each of the VSPs stores an IP address in its configuration file system . When an IO request is received an interface running within the front end of the IO stack checks the IP address to which the IO request is directed and matches that IP address with one of the IP addresses stored for the VSPs . The VSP whose IP address matches the IP address to which the IO request is directed is identified as the recipient of the IO request. The IO request arrives to the SP with a pathname to the host data object to be accessed. The front end looks up the designated pathname within the identified VSP to identify the particular data object to which the IO request is directed.

At step the IO request is processed to complete the requested read or write operation on the identified host data object. For example the front end and the back end process the IO request to perform an actual read or write to the designated host data object on the storage .

An improved technique has been described for managing host data in a data storage apparatus. The technique provides virtualized storage processors VSPs as substantially self describing and independent entities. Each VSP has its own namespace which is independent of the namespace of any other VSP and independent of the namespace of the SP s root file system. Each VSP also has its own network address. Hosts may thus access VSPs directly without having to include path information relative to the root of the SP on which the VSP is operated. VSPs can thus be moved from one physical SP to another with little or no disruption to hosts which may continue to access the VSPs on the new SPs using the same paths as were used when the VSPs were running on the original SPs.

As used throughout this document the words comprising including and having are intended to set forth certain items steps elements or aspects in an open ended fashion. Also and unless explicitly indicated to the contrary the word set as used herein indicates one or more of something. Although certain embodiments are disclosed herein it is understood that these are provided by way of example only and the invention is not limited to these particular embodiments.

Having described certain embodiments numerous alternative embodiments or variations can be made. For example embodiments have been shown and described in which host file systems LUNs vVols VMDKs and the like are provided in the form of files of underlying lower deck file systems. Although this arrangement provides advantages for simplifying management of VSPs and for unifying block based and file based operations the use of lower deck file systems is merely an example. Indeed host file systems LUNs vVols VMDKs and the like may be provided for VSPs in any suitable way.

Also although the VSPs are shown and described as userspace constructs that run within the container this is also merely an example. Alternatively different VSPs may be provided in separate virtual machines running on the SP . For example the SP is equipped with a hypervisor and a virtual memory manager and each VSP runs in a virtual machine having a virtualized operating system.

Also the improvements or portions thereof may be embodied as a non transient computer readable storage medium such as a magnetic disk magnetic tape compact disk DVD optical disk flash memory Application Specific Integrated Circuit ASIC Field Programmable Gate Array FPGA and the like shown by way of example as medium in . Multiple computer readable media may be used. The medium or media may be encoded with instructions which when executed on one or more computers or other processors implement the various methods described herein. Such medium or media may be considered an article of manufacture or a machine and may be transportable from one machine to another.

Further although features are shown and described with reference to particular embodiments hereof such features may be included in any of the disclosed embodiments and their variants. Thus it is understood that features disclosed in connection with any embodiment can be included as variants of any other embodiment whether such inclusion is made explicit herein or not. Those skilled in the art will therefore understand that various changes in form and detail may be made to the embodiments disclosed herein without departing from the scope of the disclosure.

It should be understood that the source location S and the destination location D may reside in the same data storage array such as in separate storage pools e.g. among storage drives in the same room on the same set of racks in the same enclosure or cabinet etc. also see the storage pool in and the separate storage pools in . Alternatively the source location S and the destination location D may reside in different storage arrays e.g. on different data storage apparatus in different rooms campuses cities etc. .

It should be further understood that the user may enter commands into the data storage system via a local control console or terminal e.g. a user interface or remotely from a client device. In either situation the user is able to enter commands effectively to control operation of the data storage system e.g. by simply typing into a command line interface by operating a pointing device via a specialized graphical user interface GUI combinations thereof and so on.

Initially and by way of example suppose that the user intends to move a source VSP S located in storage pool to a destination VSP D located in storage pool on the same data storage array or apparatus . In this situation suppose that the configuration database of the data storage system already includes a particular set of database records S for the source VSP S . Further suppose that the source VSP S has been operating for some period time to store host data on behalf of one or more hosts also see . To this end the particular set of database records S defines for the source VSP S a root file system and appropriate mount points on which to mount other file systems. The particular set of database records S further defines other file systems which are mounted to the root file system of the source VSP S . In this example and as shown in see dashed lines a VSP configuration file system VSP Config and two host or user file systems FS FS are mounted to the root file system of the source VSP S . The particular set of database records S may further define other information as well such as replication sessions which are currently ongoing and which involve these file systems hereinafter referred to as the source file systems of the source VSP S .

As described earlier in connection with the VSP configuration file system VSP Config includes VSP configuration data which defines an operating environment for the source VSP S e.g. a network address CIFS settings NFS settings other VSP personality information and so on . Accordingly the personality or identity of the source VSP S is defined by the VSP configuration data in the VSP configuration file system.

The host file systems FS FS include host data provided by host applications running on the hosts . Since the VSP configuration file system VSP Config defines the operating environment for the host file systems FS FS accessibility of any replicas of the host file systems FS FS requires presence of the VSP configuration file system VSP Config .

Now suppose that the user e.g. an administrator of the data storage system is ready to begin VSP movement. To direct the data storage apparatus to move the source VSP S the user enters a move command into the data storage system . If the user does not specify that he or she wishes to induce file system format migration the data storage system responds by moving the source VSP S without file system format migration. A suitable technique for performing such movement uses replication and can be found in U.S. patent application Ser. No. 13 837 053 entitled PROVIDING MOBILITY TO VIRTUAL STORAGE PROCESSORS the teachings of which are hereby incorporated by reference in their entirety.

However suppose that the user specifies that he or she wishes to induce file system format migration when moving the source VSP S . To start migration the user runs a specialized application e.g. dedicated business logic an inband migration tool a script combinations thereof etc. also see to perform VSP mobility with induced file system format migration. That is the processing circuitry of the data storage system when running the specialized application forms a specialized controller which carries out the movement process.

Along these lines the user may enter a single move command. A suitable syntax for such a migration command is 

In response to the move command the controller i.e. the processing circuitry running the application accesses the configuration database to identify which file systems are mounted to the root file system of the source VSP S . In the context of a set of processors executing software it should be understood that executable code for carrying out VSP movement may reside in the front end of the IO stack also see .

In this example the controller sees that there are at least three files systems i.e. the VSP configuration file system and two host files systems that are mounted to the root file system of the source VSP S which is to be migrated. Accordingly the controller automatically creates a new set of database records D in the configuration database to define the destination VSP D . Additionally the controller creates a destination root file system for the destination VSP D and appropriate mount points on which to mount other file systems. Furthermore the controller appropriately creates destination file systems i.e. VSP Config FS and FS which use the new file system format e.g. UFS 64 and mounts the destination file systems to the destination root file system of the destination VSP D . At this point the destination VSP D is formed and the new set of database records D accurately reflect the existence of the destination VSP D . In some arrangements parts of the root file system of the source VSP S are copied to the root file system of the destination VSP D e.g. files containing local configuration replication context etc. are copied .

Next the controller enters a primary copying phase and copies data from the source file systems to the destination file systems . Arrow C in illustrates copying of the data from VSP Config of the source VSP S and storing the copied data in VSP Config of the destination VSP D . Similarly arrow FS in illustrates copying of the data from FS of the source VSP S and storing the copied data in FS of the destination VSP D Likewise arrow FS in illustrates copying of the data from FS of the source VSP S and storing the copied data in FS of the destination VSP D . It should be understood that during this copy activity the copied data is stored in the new file system format e.g. UFS 64 .

As shown in during this primary copying phase all host IOs are serviced through the source VSP S . That is the hosts send their host IOs to the data storage apparatus and the upper level of the IO stack steers all host IO requests to the source VSP S .

However as shown in all host access that do not cause state changes are now directed to the destination VSP D . For example host read operations are now steered by the upper level of the IO stack to the destination VSP D . Such operation enables the hosts to test out operability of the destination VSP D before fully committing to the permanent use of the destination VSP D .

In some situations the user may wish to remain in compatibility mode for just a short period of time e.g. perhaps the user is anxious to take full advantage of the improvements available from the new file system format. However it should be understood that the data storage system is capable of continuing to operate in compatibility mode for an extended or indefinite period of time. During this period the source VSP S continues to process host write requests with state synchronization to the destination VSP D and the destination VSP D continues to process non state changing IOs.

It should be understood that if the user changes his or her mind and wishes to revert back to fully using the source VSP S the user can enter a rollback command to the data storage apparatus . In response to the rollback command the upper level of the IO stack stops steering certain IOs e.g. host read requests to the destination VSP D . Instead the upper level of the IO stack now steers all IOs to the source VSP S and the VSP movement process is terminated. As a result the storage consumed by copying the data to the second file system format can be reused for other purposes.

Operations with the hosts continue uninterrupted so that the VSP movement process is perfectly seamless to the hosts i.e. all file handles continue to work . The advantages of the new file system format can now be fully enjoyed.

In step the circuitry receives a VSP move command from a user. As mentioned earlier the VSP move command specifies i a source VSP which includes multiple source file systems which store data in a source file system format ii a destination or a destination VSP and iii a destination file system format which is different than the source file system format.

In step the circuitry forms in response to the VSP move command a destination VSP which includes multiple destination file systems. In particular the circuitry accesses the configuration database in a manner which is hidden from the hosts .

In step while the source VSP S is accessed by one or more of the hosts the circuitry copies data from the multiple source file systems of the source VSP S to the multiple destination file systems of the destination VSP D and stores the copied data in the destination file system format.

In step after the data is copied from the multiple source file systems to the multiple destination file systems the circuitry synchronizes state changes between the source VSP and the destination VSP. In particular any data which is changed at the source VSP is also changed at the destination VSP.

In step the circuitry receives a commit command and commits to using the destination VSP or a cancellation rollback command to cancel migration . Once the circuitry commits to the destination VSP all data accesses are processed by the destination VSP and the data corresponding to the source VSP can be removed and the storage space can be reused.

As described above improved techniques involve providing VSP mobility with induced file system format migration by migrating all file systems i.e. host file systems as well as hidden configuration i.e. control data personality data and metadata file systems of source VSPs to destinations. Such migration may be handled by a migration tool which is within the input output I O stack but above block based storage and replication. Accordingly all data is migrated i.e. host data as well as configuration data . Additionally file characteristics such as the file system identifier file identifiers e.g. inode numbers assigned to host files of host file systems can be preserved i.e. identically assigned at the destination .

While various embodiments of the present disclosure have been particularly shown and described it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the present disclosure as defined by the appended claims.

Along these lines the migration process was described above as being initiated via a user command and the particular user command was by way of example shown as a command line. Other forms of commands are suitable for use as well such as a series of subcommands and or parameters entered via a GUI or series of dialog windows.

Moreover the migration command was described above as being entered directly by a user by way of example only. In other arrangements the migration command can be entered automatically or indirectly e.g. via a script as part of a complex series of commands from a program as part of an application programming interface API and so on.

Additionally it should be understood VSP movement was described above as occurring between two different storage pools by way of example only. In some arrangements VSP movement with induced file system format migration is performed within the same storage pool .

Furthermore the VSP mobility was described above as occurring in the same data storage array by way of example only. In other arrangements VSP mobility induced file system format migration occurs across arrays. In these arrangements the configuration database of both data storage apparatus are updated during the migration process. Such modifications and enhancements are intended to belong to various embodiments of the disclosure.

