---

title: Secure multi-tenant virtual control server operation in a cloud environment using API provider
abstract: In a system that separates the control (management) layer from the data layer of a distributed storage system, an application programming interface (API) provider is presented that manages storage awareness of the virtual control servers with at least one virtual array and underlying physical arrays. The system described herein advantageously enables multi-tenant support to provide individual and/or isolated access to shared storage resources among multiple tenants and offers improved scalability and operations in a cloud deployment.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09602341&OS=09602341&RS=09602341
owner: EMC IP Holding Company LLC
number: 09602341
owner_city: Hopkinton
owner_country: US
publication_date: 20130619
---
This application is related to the field of data storage and more particularly to systems for managing data and resources in a virtualized environment.

In current storage networks and particularly storage networks including geographically distributed directors or nodes and storage resources preserving or reducing bandwidth between resources and directors while providing optimized data availability and access is highly desirable. Data access may be localized in part to improve access speed to pages requested by host devices. Caching pages at directors provides localization however it is desirable that the cached data be kept coherent with respect to modifications at other directors that may be caching the same data. An example of a system for providing distributed cache coherence is described in U.S. Pat. No. 7 975 018 to Unrau et al. entitled Systems and Methods for Providing Distributed Cache Coherence which is incorporated herein by reference. Other systems and techniques for managing and sharing storage array functions among multiple storage groups in a storage network are described for example in U.S. Pat. No. 7 266 706 to Brown et al. entitled Methods and Systems for Implementing Shared Disk Array Management Functions which is incorporated herein by reference.

Data transfer among storage devices including transfers for data replication or mirroring functions may involve various data synchronization processing and techniques to provide reliable protection copies of data among a source site and a destination site. In synchronous transfers data may be transmitted to a remote site and an acknowledgement of a successful write is transmitted synchronously with the completion thereof. In asynchronous transfers a data transfer process may be initiated and a data write may be acknowledged before the data is actually transferred to directors at the remote site. Asynchronous transfers may occur in connection with sites located geographically distant from each other. Asynchronous distances may be distances in which asynchronous transfers are used because synchronous transfers would take more time than is preferable or desired. Particularly for asynchronous transfers it is desirable to maintain a proper ordering of writes such that any errors or failures that occur during data transfer may be properly identified and addressed such that for example incomplete data writes be reversed or rolled back to a consistent data state as necessary.

Reference is made for example to U.S. Pat. No. 7 475 207 to Bromling et al. entitled Maintaining Write Order Fidelity on a Multi Writer System which is incorporated herein by reference that discusses features for maintaining write order fidelity WOF in an active active system in which a plurality of directors i.e. controllers and or access nodes at geographically separate sites can concurrently read and or write data in a distributed data system. Discussions of data ordering techniques for synchronous and asynchronous data replication processing for other types of systems including types of remote data facility RDF systems produced by EMC Corporation of Hopkinton Mass. may be found for example in U.S. Pat. No. 7 613 890 to Meiri entitled Consistent Replication Across Multiple Storage Devices U.S. Pat. No. 7 054 883 to Meiri et al. entitled Virtual Ordered Writes for Multiple Storage Devices and U.S. Pat. No. 8 335 899 to Meiri et al. entitled Active Active Remote Synchronous Mirroring which are all incorporated herein by reference.

In a virtualized environment a centralized management infrastructure henceforth referred to as a virtual control server may provide a central point of control for managing monitoring provisioning and migrating virtual machines. A virtual machine VM is a software implementation of a machine that executes programs like a physical machine. Virtualization software allows multiple VMs with separate operating systems to run in isolation on the same physical server. Each VM may have its own set of virtual hardware e.g. RAM CPU NIC etc. upon which an operating system and applications are loaded. The operating system may see a consistent normalized set of hardware regardless of the actual physical hardware components. The virtual control server may operate to control virtual machines in data centers and for example in connection with cloud computing. The virtual control server may further include a virtual data center that provides logical control and management of data storage in a data center and provides for sub dividing contents of virtual components into compute resources network resources and storage resources. Under various circumstances it may be advantageous to control access to resources among the virtual components including sharing of distributed resources among the virtual components.

A distributed storage system of a virtualized environment may include a grid and or other configuration of different storage nodes connected to each other over synchronous and or asynchronous latencies. Coordinating and controlling the sharing of resources distributed among these different storage nodes may in many cases be difficult or problematic using known technologies. In some cases known distributed storage systems of virtualized environments may not provide suitable multi tenancy support. Such systems may filter resources only based on physical attributes ports initiators etc. and thereby expose physical array details to the virtual control servers of the virtualized storage environment. In a large cloud deployment this may result in unnecessary details and information being reported to one or more virtual control servers. Such an approach is not efficient and may undesirably reduce or prevent multi tenant isolation of the underlying storage platform.

Accordingly it is desirable to provide an effective and efficient system to address issues like that noted above for a distributed system in a virtualized environment.

According to the system described herein a method of sharing resources in a virtualized environment includes managing distributed resources of the virtualized environment using at least one virtual array. A plurality of virtual control servers may access the at least one virtual array. An application programming interface API provider is provided that manages interfacing between the plurality of virtual control servers and the at least one virtual array. The API provider receives requests from the plurality of virtual control servers. The API provider identifies resources of the at least one virtual array that are responsive to the requests. The API provider reports relevant resources independently to each of the plurality of virtual control servers the relevant resources being separately responsive to each of the requests from the plurality of virtual control servers. The distributed resources may be provided using a plurality of storage devices that are remotely located from each other. An administrator of each of the plurality of virtual control servers may use corresponding credentials to access the at least one virtual array via the API provider. The at least one virtual array may include a plurality of virtual arrays each of the virtual arrays having distinct characteristics. The API provider may be used to maintain isolation between the plurality of virtual control servers while enabling shared access of the resources of the at least one virtual array.

According further to the system described herein a non transitory computer readable medium stores software for sharing resources in a virtualized environment. The software includes executable code that manages distributed resources of the virtualized environment using at least one virtual array. Executable code is provided that provides an application programming interface API provider that manages interfacing between a plurality of virtual control servers and the at least one virtual array. Executable code is provided that receives at the API provider requests from the plurality of virtual control servers. Executable code is provided that identifies at the API provider resources of the at least one virtual array that are responsive to the requests. Executable code is provided that reports relevant resources independently to each of the plurality of virtual control servers from the API provider the relevant resources being separately responsive to each of the requests from the plurality of virtual control servers. The distributed resources may be provided using a plurality of storage devices that are remotely located from each other. Executable code may be provided that receives credentials of an administrator of each of the plurality of virtual control servers that are used to access the at least one virtual array via the API provider. The at least one virtual array may include a plurality of virtual arrays each of the virtual arrays having distinct characteristics. Executable code may be provided that enables the API provider to be used to maintain isolation between the plurality of virtual control servers while enabling shared access of the resources of the at least one virtual array.

According to the system described herein a method of filtering information in a virtualized environment includes managing distributed resources of the virtualized environment using at least one virtual array. A plurality of virtual control servers may access the at least one virtual array. An application programming interface API provider is provided that manages interfacing between the plurality of virtual control servers and the at least one virtual array. Event or alert information concerning the distributed resources of the virtual array is received at the API provider. The event or alert information is filtered at the API to identify individual ones of the plurality of virtual control servers affected by the event or alert information. Relevant event or alert information is reported independently to each of the plurality of virtual control servers from the API provider the relevant event or alert information corresponding to affected ones of the virtual control servers separately affected by the relevant event or alert information. The distributed resources may be provided using a plurality of storage devices that are remotely located from each other. An administrator of each of the plurality of virtual control servers may use corresponding credentials to access the at least one virtual array via the API provider. The at least one virtual array may include a plurality of virtual arrays each of the virtual arrays having distinct characteristics. The API provider may be used to maintain isolation between the plurality of virtual control servers while enabling shared access of the resources of the at least one virtual array.

According further to the system described herein a non transitory computer readable medium stores software for filtering information in a virtualized environment. The software includes executable code that manages distributed resources of the virtualized environment using at least one virtual array. Executable code is provided that provides an application programming interface API provider that manages interfacing between the plurality of virtual control servers and the at least one virtual array. Executable code is provided that receives at the API provider event or alert information concerning the distributed resources of the virtual array. Executable code is provided that filters at the API provider the event or alert information to identify individual ones of the plurality of virtual control servers affected by the event or alert information. Executable code is provided that reports relevant event or alert information independently to each of the plurality of virtual control servers from the API provider the relevant event or alert information corresponding to affected ones of the virtual control servers separately affected by the relevant event or alert information. The distributed resources may be provided using a plurality of storage devices that are remotely located from each other. Executable code may be provided that receives credentials of an administrator of each of the plurality of virtual control servers that are used to access the at least one virtual array via the API provider. The at least one virtual array may include a plurality of virtual arrays each of the virtual arrays having distinct characteristics. Executable code may be provided that enables the API provider to be used to maintain isolation between the plurality of virtual control servers while enabling shared access of the resources of the at least one virtual array.

Each of the hosts may be communicably coupled to one or more of directors over one or more network connections . It is noted that host devices may be operatively coupled with directors over any of a number of connection schemes as required for the specific application and geographical location relative to each of the directors including for example a direct wired or wireless connection an Internet connection a local area network LAN type connection a wide area network WAN type connection a VLAN a proprietary network connection a Fibre channel FC network etc. Furthermore hosts may also be coupled to one another via the networks and or operationally via a different network and several of the hosts may be clustered together at one or more sites in which the sites are geographically distant from one another. It is also noted that in various embodiments the networks may be combined with the SAN networks .

Each of the directors may also include or be communicably coupled with one or more file systems such as a virtual machine file system VMFS a new technology file system NTFS and or other appropriate file system and may be communicably coupled with one or multiple storage resources each including one or more disk drives and or other storage volumes over one or more storage area networks SAN and or other appropriate network such as a LAN WAN etc. The directors may be located in close physical proximity to each other and or one or more may be remotely located e.g. geographically remote from other directors as further discussed elsewhere herein. It is possible for the SANs to be coupled together and or for embodiments of the system described herein to operate on the same SAN as illustrated by a dashed line between the SAN and the SAN . Each of the directors may also be able to intercommunicate with other directors over a network such as a public or private network a peripheral component interconnected PCI bus a Fibre Channel FC network an Ethernet network and or an InfiniBand network among other appropriate networks. In other embodiments the directors may also be able to communicate over the SANs and or over the networks . Several of the directors may be clustered together at one or more sites and in which the sites are geographically distant from one another. The system described herein may be used in connection with a vSphere product produced by VMware Inc. of Palo Alto Calif. and or VPLEX product produced by EMC Corporation of Hopkinton Mass. The system described herein may also be used in connection with storage products produced by EMC Corporation such as a Symmetrix VMAX VNX and or Isilon product. Although discussed and illustrated in connection with embodiment for a distributed storage system the system described herein may generally be used in connection with any appropriate distributed processing system.

Each distributed cache manager may be responsible for providing coherence mechanisms for shared data across a distributed set of directors. In general the distributed cache manager may include a module with software executing on a processor or other intelligence module e.g. ASIC in a director. The distributed cache manager may be implemented in a single director or distributed across multiple intercommunicating directors. In certain aspects each of the directors may be embodied as a controller device or blade communicably coupled to one or more of the SANs that allows access to data stored on the storage networks. However it may be appreciated that a director may also be embodied as an intelligent fabric switch a hub adapter and or other appropriate network device and may also be implemented as a virtual machine as further discussed elsewhere herein.

A distributed storage system may enable a storage device to be exported from multiple distributed directors which may be either appliances or arrays for example. In an active active storage system if there are multiple interfaces to a storage device each of the interfaces may provide equal access to the storage device. With an active active storage system hosts in different locations may have simultaneous write access to mirrored exported storage device s through a local front end thereof i.e. a director . The distributed storage system may be responsible for providing globally consistent and coherent data access. The system described herein may be used in connection with enabling the distributed storage system to meet consistency guarantees and maximize data access even in response to failures that may cause inconsistent data within the distributed storage system.

Using virtualization software one or more physical servers may be subdivided into a plurality of virtual machines. As further discussed elsewhere herein a virtual machine VM is a software implementation of a machine that executes programs like a physical machine. Virtualization software allows multiple VMs with separate operating systems to run in isolation on the same physical server. Each VM may have its own set of virtual hardware e.g. RAM CPU NIC etc. upon which an operating system and applications are loaded. The operating system may see a consistent normalized set of hardware regardless of the actual physical hardware components. The term virtualization software is used herein to generally refer to any and all software that supports the operation of one or more VMs. A number of virtualization software products exist including the VMware product family provided by VMware Inc. of Palo Alto Calif. A benefit of providing VMs is the ability to host multiple unrelated clients in a single physical server. The virtualization software may maintain separation of each of the clients and in which each of the clients separately access their own virtual server s . Other virtualization products that may be used in connection with the system described herein include Hyper V by Microsoft Corporation of Redmond Wash. public license virtualization products and or other appropriate virtualization software.

Configuring and deploying VMs is known in the field of computer science. For example U.S. Pat. No. 7 577 722 to Khandekar et al. entitled Provisioning of Computer Systems Using Virtual Machines which is incorporated herein by reference discloses techniques for configuring and deploying a VM according to user specifications. VMs may be provisioned with respect to any appropriate resource including for example storage resources CPU processing resources and or memory. Operations of VMs may include using virtual machine images. A VM image is the state of the virtual machine as it resides in the host s memory. The VM image may be obtained for an operating VM and transferred to another location where the VM continues execution from the state defined by the virtual machine image. In this way the VM image may be a snapshot of an execution state of a program by a VM that may be moved between different locations and processing thereafter continued without interruption.

As discussed in detail elsewhere herein in a virtualized environment a virtual center or virtual control server an example of which may be a vCenter product produced by VMware may provide a central point of control for managing monitoring provisioning and migrating virtual machines. A virtual control server may operate to control virtual machines in data centers and for example in connection with cloud computing. A virtual control server may further include a virtual data center that provides logical control and management of data storage in a data center. A virtual control sever may be used in connection with an infrastructure platform that provides an integrated package of components to provide network compute and or storage services for use in a virtualized environment. One example of an infrastructure platform is a Vblock product produced by VCE Company LLC of Richardson Tex. It is noted that the term Vblock used herein may also be generally understood as including and referring to any appropriate software and or component packages of a converged infrastructure product that provides network compute and or storage services for use in a virtualized computing environment. For example other suitable types of converged infrastructure products may include EMC Corporation s VMAX SP and or VSPEX products. Management of a Vblock and or other appropriate type of converged infrastructure product may be provided by an appropriate software element. For example EMC s Ionix Unified Infrastructure Manager UIM may be integrated with Vblock and provide a management console for management of the Vblock package.

It is also noted that the system described herein may be used in connection with thin provisioning processes and techniques. Thin provisioning also known as virtual provisioning allows for the creation of logical volumes of storage space where allocation of physical storage space occurs only when space is actually needed e.g. when data is actually written to the storage space . Logical storage space may be identified to a user as being available even though no physical storage space has been committed at least initially. When data is written to the logical storage space physical storage space is drawn for use from a pool of physical storage space as further described elsewhere herein. For a more detailed discussion of thin provisioning and uses thereof in connection with tiered storage systems and techniques reference is made for example to U.S. Pat. No. 7 949 637 to Burke entitled Storage Management for Fine Grained Tiered Storage with Thin Provisioning which is incorporated herein by reference.

The data centers may contain any number of processors and storage devices that are configured to provide the functionality described herein. In an embodiment herein the storage devices may be storage arrays provided by EMC Corporation of Hopkinton Mass. Other appropriate types of storage devices and different types of processing devices may also be used in connection with the system described herein. The data centers may be configured similarly to each other or may be configured differently. The network may be any network or similar mechanism allowing data communication between the data centers . In an embodiment herein the network may be the Internet and or any other appropriate network and each of the data centers may be coupled thereto using any appropriate mechanism. In other embodiments the network may represent a direct connection e.g. a physical connection between the data centers .

In various embodiments VMs may be migrated from a source one of the data centers to a destination one of the data centers . VMs may be transferred from one data site to another including VM mobility over geographical distances for example for reasons of disaster avoidance load balancing and testing among other reasons. For a discussion of migrating VMs reference is made to U.S. patent application Ser. No. 12 932 080 to Meiri et al. filed Feb. 17 2011 entitled VM Mobility Over Distance and U.S. patent application Ser. No. 13 136 359 to Van der Goot filed Jul. 29 2011 entitled Active Active Storage and Virtual Machine Mobility Over Asynchronous Distances which are incorporated herein by reference. A product such as EMC s VPLEX Metro and or VPLEX Geo may be used to enable the resources of disparate storage systems in dispersed data centers to be federated and or coordinated and utilized as a single pool of virtual storage. VPLEX allows for logical storage units e.g. logical unit numbers LUNs provisioned from various storage arrays to be managed through a centralized management interface. Products like VPLEX Metro or Geo provide for data mobility availability and collaboration through active active data over synchronous and asynchronous distances with provide for the ability to non disruptively move many VMs. It is noted that the term VPLEX used herein may also generally be understood to refer to and include any appropriate software and or component packages that provide for coordinating and or federating resources of disparate systems as a single pool of virtual resources in particular for example a single pool of virtual storage.

The sites may be coupled via SANs to storage resources . The storage resources may be located in proximity to the sites and or may be remotely located and accessed. In an embodiment the SANs may be separate networks. Alternatively in another embodiment the SANs may be part of the same network an embodiment shown represented by a dashed line connecting the SANs . In various embodiments joint operations performed by the sites may include multiple independent sub computations and may include operations of a clustered small computer system interface SCSI device corresponding to use of external storage nodes that may be accessed by one or more of the sites .

A distributed layer is shown schematically as a system of the sites that may be distributed across the sites in connection with processing of one or more access nodes. In a virtualization environment the distributed layer may function like a virtual control server that provides for control of managing monitoring provisioning and migrating virtual machines. The virtual control server may provide for managing deployment of virtual machines at one or more data centers like the sites and may operate to control virtual machines at the sites in connection with cloud computing including both internal and external cloud infrastructures and hybrids thereof.

An orchestration layer may be provided that provides policy driven orchestration for controlling access and or load balancing in connection with servicing I O requests among the sites in a cloud computing environment. I O requests from the hosts may be received by one or more of the sites over a communication link that may be a network such as the Internet and or other suitable communication link. The orchestration layer may be coupled to the sites including the distributed layer via a communication link that may be the same as or a different network than the communication link . The orchestration layer may control and implement policies and or other information for the servicing I O requests at one or more of the sites as further discussed elsewhere herein. In various embodiments the orchestration layer may be a software layer that is distributed across the sites like the distributed layer and or may be integrated in connection with an independent compute entity coupled to the sites . The orchestration layer orchestrates based for example on policies and or other information fed from manual and dynamic inputs where compute and storage processes may reside and provides non disruptive control for the servicing of I O requests that is fully enabled by a dynamic active active storage platform. For further discussion of operations and features of orchestration layers in a cloud environment reference is made to U.S. patent application Ser. No. 12 930 249 to Monden et al. filed Dec. 31 2010 entitled Fully Automated Cloud Tiering which is incorporated herein by reference.

In another embodiment one or more of the inputs may provide information obtained from one or more of the sites and or from one or more of the hosts with input information obtained from one or more users. The inputs may reflect specific information corresponding to service level agreements SLAs with respect to service agreements for one or more customers and service providers. The orchestration layer may maintain and update various types of policies for controlling and or otherwise governing processing requirements such as requirements of a particular SLA load balancing requirements requirements of minimizing processing costs and or maximizing processing speed for one or more I O requests and or other processing considerations. In various embodiments the orchestration layer may include a policy engine that makes determinations driven by business policies SLAs performance requirements utility or other rate schedules and or other appropriate type of dynamic service input or dynamic triggers to execute various orchestration events.

The orchestration layer may be used to control selection of sites for the servicing of I O requests according to the one or more policies and based on the inputs to the orchestration layer . In an embodiment the distributed layer of one or more of the sites may determine an appropriate one of the sites to handle any particular I O request based on the policy and or other information maintained by the orchestration layer and route the I O request to the appropriately determined site. The system described herein provides a system that may respond to a particular level of activity at one or more of the sites . For example a burst of data processing requests may occur for a particularly high usage period in the middle of the day for one or more of the sites e.g. site . Accordingly routing of I O requests to another site e.g. site may be controlled using the orchestration layer in response to a burst period.

In an embodiment each host cluster may include hosts such as ESX hosts in a vSphere cluster and each director cluster may include directors in a VPLEX cluster. It is noted that although ESX hosts and illustrated and discussed herein as examples any appropriate host may be used in connection with the system described herein. Front end networks may connect through host links to the host clusters and through front end links to the director clusters . One or more back end networks may connect through back end links to the director clusters and through array links to the disk arrays . In an embodiment the front and back end networks may be Fibre Channel networks. The front end networks allow the hosts or VMs running therein of the host clusters to perform I O operations with the director clusters while the back end networks allow the directors of the director clusters to perform I O on the disk arrays . One or more host networks such as vSphere Ethernet networks connect the ESX hosts in host clusters . One or more director networks connect the directors of the director clusters . It is noted that in other embodiments the host cluster may be implemented as a distributed processing layer operating across the multiple sites and or the front end networks may be external networks accessed by each of the sites .

Various types of failures including network failures within a cluster may result in behaviors that are further discussed elsewhere herein. It should be noted that the host cluster e.g. vSphere cluster may be connected in such a way that VMs can keep their network e.g. IP FC IB addresses when migrating between clusters for example by means of a vLan or an open vSwitch . In an embodiment VPLEX may be used and configured to expose one or more distributed volumes from both VPLEX director clusters. A VMFS may be created on top of these distributed volumes allowing VMs that migrate between the sites to see the same file system in either site. It is also noted that as illustrated and according to various embodiments each site may include redundancies in hosts directors and links therebetween.

In some embodiments the system described herein may be used in connection with a first set of one or more data centers that are relatively active primary data centers and a second set of one or more data centers that are relatively inactive failover data centers . The first set of data centers and second set of data centers may both be used for application reading and writing but the first set of data centers may be more active and or include more response time sensitive applications than the second set of data centers. Each of the relatively active data centers in the first set of data centers may use at least one corresponding data center in the second set of data centers for failover operations. It should also be noted that in addition to the active active system described herein the system described herein may also be used in active passive functioning as appropriate or desired. For further detailed discussion of specific system behaviors in connection with different types of failure scenarios reference is made for example to U.S. patent application Ser. No. 13 136 359 to Van Der Goot.

The system described herein may provide for advantageous sharing of resources using a virtual platform product that federates and or coordinates virtual storage. In various embodiments the resources may be resources supported by virtual data centers and including virtualized storage products such as EMC s VMAX and or VNX products and or converged infrastructure products such as VCE s Vblock products. The system described herein may enable migration of resources dynamically and non disruptively thereby allowing a user to move storage data and or compute processing. For example the system described herein may enable migration of the contents of one VMAX to another VMAX and or may enable migration of contents of a virtual data center to another virtual data center that may be contained in the same or different VMAX s and may provide for seamless migration within a data center and or among multiple data centers. By enabling use of dynamic policies the system described herein allows for dynamically load balanced among virtual platforms and infrastructure products.

According to an embodiment of the system described herein distributed resources may be shared between multi connected virtual platforms such as VPLEX clusters in a dynamic sharing arrangement. A virtual platform may include one or more pairs of directors that enables failover from one director of the pair s to the other director of the pair s in a cluster in the case of hardware or path failure. In an exclusive sharing arrangement between two virtual platforms e.g. between two VPLEX clusters the distributed resources are shared in an all or nothing arrangement in which either all or none of the distributed resources are shared between the two VPLEX clusters. In an embodiment the system described herein advantageously enables a dynamic sharing relationship between multiple virtual platforms that may be tied to specific resources.

The virtual platform may include one or more pairs of directors that enables failover from one director of the pair s to the other director of the pair s in a cluster in the case of hardware or path failure as further discussed elsewhere herein. In an embodiment the virtual platform may include a VPLEX cluster component e.g. VPLEX Metro for synchronous latencies and or VPLEX Geo for asynchronous latencies that may be used to provide high availability support for host clusters including ESX host cluster support and vSphere operations for the pairing of the resources for operations for active active storage operations and or site failure handling for disaster recovery operations. The illustrated embodiment may be used to advantageously reduce durations for a recovery point objective RPO and a recovery time objectives RTO in connection with recovery operations using multiple storage devices. It is further noted that although the virtual platform is shown as a separate component the virtual platform component may be implemented as a software layer distributed across one or more storage devices providing the resources . In the virtualization environment of the system the virtual platform may function to provide for control of managing monitoring provisioning and migrating virtual machines among storage devices of one or more data centers.

An interface may be provided to enable orchestration of the resource access and or other control functions among the resources including migration of virtual data centers VDCs . In an embodiment the interface may be provided by a common management layer such as a management layer see used to manage the storage device s . The interface may provide policy driven orchestration for controlling access and or operations in connection with VDC migration. The interface may be used in connection with controlling and implement policies and or other information for migration operations and or servicing of I O requests. The interface may be used to orchestrate the resource sharing based on policies and or other information fed from manual and dynamic inputs where compute and storage processes may reside and provides non disruptive control and or dynamic load balancing among the resources .

In various embodiments migration and or access of objects among the resources may be performed in connection with storage tiering between storage devices. For example resource may include storage arrays of a first type and resource may include storage arrays of a second type in which the first and second types of storage arrays having different characteristics. For example the first type may be of a type that has fast accessibility or more expensive whereas the second type of storage may be of a slower or less expensive accessibility type.

According to the system described herein in a system that separates the control management layer from the data layer of a distributed storage system it is advantageous to present an application programming interface API provider that manages storage awareness of the virtual control servers with at least one virtual array and underlying physical arrays. The system described herein further advantageously enables multi tenant support to provide individual and or isolated access to shared storage resources among multiple tenants and offers improved scalability and operations in a cloud deployment.

An API provider is illustrated that enables storage device management software to inform a virtual control server of how the file system e.g. VMFS storage is configured and protected. In an embodiment the API provider may be a vSphere Storage API for Storage Awareness VASA provider product from EMC Corporation. This information may be used to allow administration of server virtualization components such as vSphere of virtual control servers e.g. vCenter servers to make informed decisions as to VM placement. Although three virtual control servers are illustrated the system described herein may be used in connection with any appropriate number of virtual control servers. The API provider enhances the ability for the server virtualization technology e.g. vSphere administrators to track how storage devices are configured to meet performance and availability needs.

The API provider may be used to expose the virtual array to the virtual control servers . Advantageously as more arrays are brought under the control of virtual array the virtual control servers may not be affected. Specifically the virtual control servers do not have to rebuild their inventories of arrays and storage processors when the infrastructure changes. This is a significant advantage in a cloud deployment which is often dynamic.

Each administrator of the virtual control servers may add and or otherwise enable functionality of the API provider to one of the virtual control servers using credentials in the virtual array . The API provider uses the credentials to get the inventory information from the virtual array for each of the virtual control servers. In a multi tenant environment this allows each tenant to run a separate or independent virtual infrastructure which can securely share the storage from the instance of the virtual array . The API provider may maintain independent sessions for each virtual control server connection and maintain a separate state for each session. Advantageously this allows multiple virtual control servers to connect to one common API provider and each can get only their own resources reported to them.

In an embodiment the system described herein may be used to further control the events and alerts that are reported to each of the virtual control servers in the multi tenant environment. Specifically all events and alerts of the virtual array may be reported to the API provider while the API provider may then filter the events and alerts based on the credentials of each virtual control server admin and report only the relevant alerts and events to each virtual control server. This prevents unnecessary or irrelevant alerts from being reported to the virtual control servers . For example an alert to say that a volume is running out of space should logically be reported to only the virtual control server s to whom that volume is exported. The virtual array reports many alerts and it is advantageous to avoid inundation of any of the virtual control servers with irrelevant alerts. Security issues may also be advantageously controlled in that a virtual control server admin who does not have certain visibility into resources through the virtual array should not get that visibility indirectly through the API provider . For more detailed discussions of correlating events and alerts in a virtualized storage system in a cloud environment reference is made to U.S. patent application Ser. No. 13 561 340 to Jain et al. filed Jul. 30 2012 and entitled Scalable Codebook Correlation for Cloud Scale Topology which is incorporated herein by reference.

In another embodiment the API provider according to the system described herein may have the ability to restrict access of virtual control server admin to facilitate the controlled segregation of resources. For example virtual control server admins may be restricted to specific projects involving particular storage and VM resources. For example metering feeds may be project based and thus project based isolation may help in aligning the metering feed from the virtual array to the metering used by particular ones of the virtual control servers . In an embodiment there may be ways in which the API provider can perform access restriction and or project isolation. First by providing a project name while connecting to the API provider in the URL a virtual control server admin can be restricted to a particular project. Second by allowing the virtual control server admin a designated role PROJECT ADMIN for one project the virtual control server admin may see only the resources which belong to one project.

Various embodiments discussed herein may be combined with each other in appropriate combinations in connection with the system described herein. Additionally in some instances the order of steps in the flowcharts flow diagrams and or described flow processing may be modified where appropriate. Further various aspects of the system described herein may be implemented using software hardware a combination of software and hardware and or other computer implemented modules or devices having the described features and performing the described functions. The system may further include a display and or other computer components for providing a suitable interface with other computers and or with a user. Software implementations of the system described herein may include executable code that is stored in a computer readable medium and executed by one or more processors. The computer readable medium may include volatile memory and or non volatile memory and may include for example a computer hard drive ROM RAM flash memory portable computer storage media such as a CD ROM a DVD ROM a flash drive or other drive with for example a universal serial bus USB interface and or any other appropriate tangible or non transitory computer readable medium or computer memory on which executable code may be stored and executed by a processor. The system described herein may be used in connection with any appropriate operating system.

Other embodiments of the invention will be apparent to those skilled in the art from a consideration of the specification or practice of the invention disclosed herein. It is intended that the specification and examples be considered as exemplary only with the true scope and spirit of the invention being indicated by the following claims.

