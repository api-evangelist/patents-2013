---

title: Storage space reclamation on volumes with thin provisioning capability
abstract: Techniques for reclaiming storage space are disclosed herein. According to one embodiment, a storage space reclamation method includes a storage host creating at least one temporary logical container of data in a storage volume managed by a file system of a host so that a predetermined portion of storage capacity of the storage volume is occupied. Access to the storage volume is provided by a network storage controller to the storage host. The storage host translates a host address range for the file system of each temporary logical container of data into a storage controller address range for the network storage controller. The storage host requests the network storage controller to deallocate blocks the locations of which are indicated by the storage controller address range, and then deletes the at least one temporary logical container of data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09383924&OS=09383924&RS=09383924
owner: NETAPP, INC.
number: 09383924
owner_city: Sunnyvale
owner_country: US
publication_date: 20130227
---
At least one embodiment of the present invention pertains to data storage volumes with thin provisioning capabilities and more particularly to thinly provisioned volumes with capability of reclaiming storage space.

A network storage controller is a processing system that is used to store and retrieve data on behalf of one or more hosts on a network. A storage controller operates on behalf of one or more hosts to store and manage data in a set of mass storage devices such as magnetic or optical storage based disks tapes flash memory etc. Some storage controllers are designed to service file level requests from hosts as is commonly the case with file servers used in a network attached storage NAS environment. Other storage controllers are designed to service block level requests from hosts as with storage controllers used in a storage area network SAN environment. Still other storage controllers are capable of servicing both file level requests and block level requests as is the case with certain storage controllers made by NetApp Inc. of Sunnyvale Calif.

Some network storage controllers can provide so called thin provisioning capabilities to storage volumes of the storage devices of the controllers. Thin provisioning is a type of virtualization technology to give the appearance of a volume or a LUN unit having more storage space than the actual available storage space. Thin provisioning is a technique for optimizing utilization of available storage space. It relies on on demand allocation of blocks of data versus the traditional method of allocating all the blocks in advance in response to an allocation request. This methodology helps avoid poor space utilization rates that commonly occur in other storage allocation method where large pools of storage capacity are allocated to individual hosts but remain unused i.e. not written to . With thin provisioning storage capacity utilization efficiency can be automatically driven up towards 100 with little administrative overhead. The storage controller can first allocate relatively little storage capacity for the volume and then later increase storage capacity in accordance with actual space usage from the hosts.

A thinly provisioned volume i.e. a storage volume that is thinly provisioned by a storage controller is able to grow and also shrink its storage capacity when needed. A volume is a logical data set which is an abstraction of physical storage combining one or more physical mass storage devices e.g. disks or parts thereof into a single logical storage object. A volume can be for example a logical unit identified by a logical unit number LUN in a SAN environment. Thin provisioning allows storage space to be easily allocated to hosts on a just enough and just in time basis.

However when a storage controller assigns a volume to a host the host can create its own file system on the volume and do its own file system bookkeeping. As a result the host can have a very different idea of how much space it is currently using within the volume than what the storage controller has. illustrates an example scenario of inconsistent views of storage space usage from a storage host and a storage controller. A file system of the storage host manages the storage volume and the storage controller the data access requests for the storage volume. At step of the host writes two new files to the volume each consuming 25 of the total storage space of the volume. Both the host and the storage controller show that 50 of the storage space has been used. At step the host writes another new file of the same size. Again the host and the storage controller both show that 75 of the storage space has been used. At step the host deletes the first and second files. For hosts with most file systems e.g. New Technology File System also referred to as NTFS deleting a file causes the host to deallocate the blocks of the deleted files and record the references to these blocks in a free block data structure e.g. volume free space map . However in a SAN environment there is no mechanism for the host to notify the storage controller of the deletion of the file. The data stored inside the volume is opaque to the storage controller. Hence the views of the host and the storage system diverge at step . The host shows that the volume is only 25 full while the storage system shows that 75 of the volume is in use. The host is under no obligation to reuse the blocks it deallocated so if the host writes another file to the volume the fourth file may occupy previously unused space as shown in step . Then the storage controller shows that the volume is full while the host shows just 50 utilization of the volume.

These discrepancies in views between the host and the storage controller do not pose a serious problem in situations of volumes having fixed sizes. But for a thinly provisioned volume there is a big potential difference from the perspective of the storage controller between a volume that is considered to be 25 full and one that is considered to be 75 full. The host considers the extra 50 to be unallocated however the storage controller does not know that. Consequently the storage controller would not be able to adjust the thinly provisioned volume and assign the unallocated space for other purposes. Over a period of time the storage controller tends to allocate more storage space for the host while more files are deleted by the host without release from the storage controller s view. The benefits of thin provisioning therefore tend to disappear over time. Eventually all storage space of the volume is allocated and the storage controller can no longer provide thin provisioning capability to the volumes of the hosts.

Techniques introduced here provide an efficient mechanism for reclaiming storage space. According to one embodiment a storage space reclamation method includes a storage host creating one or more temporary files on a storage volume managed by a file system of a host so that a predetermined portion of storage capacity of the storage volume is occupied. A file system as the term is used herein is a structured set of logical containers of data which may be but are not necessarily in the form of files directories logical units and or other types of logical containers and software for managing the same. Access to the storage volume is provided by a network storage controller. For each temporary file the storage host converts a host address range for the storage host into a storage controller address range for the network storage controller. The storage host requests the network storage controller to deallocate blocks the locations of which are indicated by the storage controller address ranges and then deletes the temporary files. Blocks are the basic units of storage space used by a file system for storing user data. A block can be for example 4 KBytes although other block sizes are possible. Once the blocks are freed by the storage controller they are no longer retained and can be used for purposes such as thin provisioning of volumes and snapshot storage.

The space reclamation techniques introduced here help to prevent the benefits of thin provisioning from eroding over time. By running the disclosed space reclamation process a thinly provisioned volume may be maintained at high storage efficiency such that the amount of storage space consumed on a storage controller is no more than what is actually needed by the host volume s file system.

Furthermore the space reclamation techniques disclosed herein do not require relocating allocated blocks. This is helpful since some types of volumes such as Cluster Shared Volumes CSVs do not have a simple mechanism for relocating the allocated blocks. Further the space reclamation techniques do not force a CSV into a redirected I O mode. When in redirected I O mode all writes to the CVS are conducted indirectly via a single node that owns the CSV which poses a large performance penalty for I O requests from other nodes in the cluster.

The space reclamation techniques introduced here may be particularly useful in any environment that shows a large variance in the amount of data used. For instance systems used for queuing print jobs email or standard file sharing are excellent candidates for space reclamation on a periodic schedule. Any system that has experienced a major content change is also a good candidate for space reclamation.

Other aspects of the technology introduced here will be apparent from the accompanying figures and from the detailed description which follows.

References in this specification to an embodiment one embodiment or the like mean that the particular feature structure or characteristic being described is included in at least one embodiment of the present invention. Occurrences of such phrases in this specification do not all necessarily refer to the same embodiment however.

Refer now to which shows an example of a network storage system serving data storage requests from storage hosts in which one can implement the technique being introduced here. In a storage controller is coupled to a set of hosts through an interconnect . The interconnect may be for example a local area network LAN wide area network WAN metropolitan area network MAN global area network such as the Internet a Fibre Channel fabric or any combination of such interconnects. Each of the hosts may be for example a conventional personal computer PC server class computer workstation handheld computing communication device or the like.

Storage of data in the storage subsystem is managed by network storage controller hereinafter simply storage controller or controller . Storage controller and Storage subsystem are collectively referred to as the storage system. The storage controller receives and responds to various read and write requests from the hosts directed to data stored in or to be stored in storage subsystem . Storage subsystem includes a number of nonvolatile mass storage devices which can be for example conventional magnetic or optical disks or tape drives alternatively they can be non volatile solid state memory such as flash memory or any combination of such devices. The mass storage devices in storage subsystem can be organized as a Redundant Array of Inexpensive Disks RAID in which case the storage controller accesses the storage subsystem using a conventional RAID algorithm for redundancy.

The storage controller includes a storage operating system which is responsible for managing storage of data in the storage subsystem servicing requests from hosts and performing various other types of storage related operations. In one embodiment the storage operating system can include a file system to manage the storage of data. For instance the file system can be a Write Anywhere File Layout WAFL file system developed by NetApp Inc.

In certain embodiments the storage operating system is implemented entirely in the form of software. In other embodiments however the storage operating system may be implemented in pure hardware e.g. specially designed dedicated circuitry.

Storage controller may be for example a storage server which provides file level data access services to hosts such as commonly done in a NAS environment or block level data access services such as commonly done in a SAN environment or it may be capable of providing both file level and block level data access services to hosts. Further although the storage controller is illustrated as a single unit in it can have a distributed architecture. For example the storage controller can be designed as a physically separate network module e.g. N blade and disk module e.g. D blade not shown which communicate with each other over a physical interconnect. Such an architecture allows convenient scaling such as by deploying two or more N modules and two or more D modules all capable of communicating with each other through the interconnect.

Hosts are processing devices e.g. computers that send read and write requests to the storage controller directed to data stored in or to be stored in storage subsystem . is a high level block diagram showing an example of the architecture of a computer node . The computer node can be a host or a storage controller. The computer node includes one or more processors and memory coupled to an interconnect . The interconnect shown in is an abstraction that represents any one or more separate physical buses point to point connections or both connected by appropriate bridges adapters or controllers. The interconnect therefore may include for example a system bus a Peripheral Component Interconnect PCI bus or PCI Express bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB IIC I2C bus or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus also called Firewire .

The processor s is are the central processing unit CPU of the computer node and thus control the overall operation of the computer node . In certain embodiments the processor s accomplish this by executing software or firmware stored in memory . The processor s may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs trusted platform modules TPMs or the like or a combination of such devices.

The memory is or includes the main memory of the computer node . The memory represents any form of random access memory RAM read only memory ROM flash memory or the like or a combination of such devices. In use the memory may contain among other things code embodying at least a portion of an operating system of the computer node . For instance the operating system can be a Windows OS a Linux OS a Unix OS an android or an Apple OS. Code can also include a file system such as an NTFS file system FAT file system EXT file system or a HFS system.

Also connected to the processor s through the interconnect are a network adapter and a storage adapter . The network adapter provides the computer node with the ability to communicate with remote devices such as hosts over the interconnect and may be for example an Ethernet adapter or Fibre Channel adapter. The storage adapter allows the computer node to access a storage subsystem such as storage subsystem and may be for example a Fibre Channel adapter or SCSI adapter.

The hosts coupled to the storage system in can be different types of hosts utilizing different file systems. For instance a host can be a Cluster Shared Volume CSV using an NTFS file system. A CSV is an NTFS volume on a shared disk that is accessible for read and write operations by all nodes within a Windows Server Failover Cluster. CSVs operate by orchestrating metadata I O operations between the nodes in the cluster via the Server Message Block SMB protocol. Read and write operations are passed directly to the serial attached SCSI iSCSI Fibre Channel or Fibre Channel over Ethernet shared storage via block based protocols. CSVs are owned by one node at a time but multiple nodes may write directly to a CSV. CSV enables a virtual machine VM to have complete mobility throughout the cluster since any node can access the Virtual Hard Disk VHD files of the virtual machine on the shared CSV volume.

Node A includes a CSV redirector to intercept and redirect I O requests made by the process A . Unlike typical network shares data read and write I O requests may be sent by the CSV redirector directly to the storage device volume which is mounted on Node B through a local storage device driver that is without involving node B in this example. Note that cluster nodes are typically connected to the cluster s physical storage devices via SCSI Fibre Channel Internet Small Computer System Interface iSCSI or other high speed communication links. Similar to Node A Node C can also directly access storage device volume without involving node B . Node B can access the storage device volume via node B s redirector and a local storage device driver .

As shown in a CSV allows concurrent access to the volume from multiple cluster nodes. Each node includes a storage device driver for accessing local storage device volumes and remote storage device volumes mounted on other nodes. A storage device driver e.g. of a remote node with respect to the storage device volume can access the device volume without involving node B while the owning node s storage device driver maintains and protects volume ownership.

Thin provisioning can be applied to different types of volumes including CSVs of the storage hosts and can cause unused allocated storage space as described above. Techniques introduced here however provide a highly efficient mechanism for reclaiming the unused allocated storage space. A host determines which blocks on a volume are not in use based on a volume free space map of the volume that the storage host maintains in its memory or storage and creates temporary files and retrieves block addresses of the temporary files. The host then translates the addresses into address information for the storage controller and communicates the translated address information to the storage controller. In response the storage controller marks the blocks as free i.e. deallocated . Once the blocks are marked as free by the storage controller they can be used for other purposes such as thin provisioning and snapshot storage.

At step of the process the storage host stores files on a storage volume having thin provisioning capability. The storage capacity of the storage volume can be automatically adjusted by the network storage controller. The storage volume is managed by a file system of the storage host and is served by i.e. access to the volume is provided by the storage controller. In one embodiment the file system is an NTFS file system. In another embodiment the storage volume is a CSV that is accessible by a plurality of nodes within a cluster. In yet another embodiment the storage volume is a LUN controlled by the storage controller. In still another embodiment the storage volume stores files representing virtual hard disk drive for virtual machines.

When the storage host deletes a file the host does not directly notify the controller of the deletion of the file in a SAN environment. The storage volume contains blocks that are deallocated by the file system of the host and still remain allocated by the storage controller.

At step the storage host creates a temporary directory on the storage volume. At step the storage host identifies all free space on the storage volume based on a volume free space map of the storage volume that the storage host maintains in its memory or storage.

At step the storage host creates a temporary file in the temporary directory in the storage volume and set the size of the temporary file to a predetermined size. In one embodiment the predetermined size is the largest number of blocks that the storage controller can deallocate using a single SCSI command. In this respect the predetermined size may be for example 244 MB. The storage host may choose a predetermined size smaller than the amount of available free space in the storage volume.

At step the storage host checks whether there is an error in failing to create the temporary file. In a case where the storage volume is a CSV volume an error can be caused by another host of the cluster accessing e.g. writing the same free space of the storage volume while the host was attempting to create the temporary file. If there is an error for failing to create the temporary file at step the storage host requests a refreshed volume free space map of the storage volume. Then the process goes back to state in which the storage host creates the temporary file on the storage volume based on the refreshed volume free space map. Otherwise the process continues to step .

At step the storage host checks whether a predetermined relatively high percentage of storage capacity of the storage volume is occupied. In some embodiments the predetermined percentage is 90 95 98 99 or 100 . If less than the predetermined percentage of storage capacity of the storage volume is occupied the process goes back to step to create another temporary file until in step the occupied space is found to have reached the predetermined percentage of the storage capacity. Otherwise the process continues to step .

At step for each individual temporary file of the plurality of temporary files the storage host validates that individual temporary file in the file system by setting the valid data length of that individual temporary file without writing zeros or other values to the blocks of that individual temporary file. Thus the process takes less time than setting a size of the file because there is no need to write any data into that individual temporary file for it to be validated.

At step for each individual temporary file created in step the storage host locks the location of that individual temporary file so that no process or host can move any part of that individual temporary file for the remainder of the life of the file.

At step the storage host retrieves a mapping data structure from the network storage controller. The mapping data structure that the network storage controller generates includes a mapping between host block addresses for the file system of the host and controller addresses for the network storage controller. The file system is capable of using the host address range for specifying block locations on the file system. The network storage controller is capable of using the controller address range for specifying block locations on the storage controller. In one embodiment the host block addresses are NTFS sector addresses. The controller block addresses are WAFL block addresses. In another embodiment the controller block addresses are Logical Block Addressing LBA addresses for specifying locations of blocks served by the network storage controller.

At step for each individual temporary file created in step the storage host converts a host address range of that individual temporary file i.e. block address of its first logical block to block address of its last logical block into a controller address range based on the mapping data structure. In one embodiment the controller address range is a range of blocks addresses conforming to SCSI protocol.

At step for each individual temporary file created in step the storage host sends a request including the controller address range to the storage controller to deallocate blocks specified by the controller address range indicated in the request. In one embodiment the request is a network request message including a SCSI command to instruct the storage controller to deallocate blocks specified by the controller address range indicated in the network request message. In another embodiment the SCSI command is a SCSI UNMAP command.

At step after all requests to deallocate these temporary files are sent to the storage controller the storage host deletes the temporary directory and the plurality of temporary files from the storage volume managed by the file system of the host. In one embodiment the storage host deletes the temporary directory and the temporary files after the storage host receives confirmation from the storage controller that the blocks indicated in the requests are deallocated.

At step the storage controller has reclaimed the storage space and again allocates the deallocated blocks to the storage host for increasing storage capacity of a thinly provisioned volume or for storing a file system snapshot.

As shown in in one embodiment the space reclamation process can apply to a Windows CSV that is supported by an NTFS formatted LUN unit. A storage controller provides the read and write access to the CSV . In such embodiment a storage host of the Windows CSV creates temporary files on the CSV volume until the CSV volume is 98 full. The storage host sets a size of each temporary file to the largest size of blocks that the storage controller can deallocate i.e. unmap with a single SCSI UNMAP command which may be 244 MB for example. The storage host in one embodiment uses an NTFS application programming interface API e.g. SetFileValidData to notify the NTFS file system of the storage host that the temporary files are valid and need not be written with values such as zeroes before use. The storage host uses another NTFS API e.g. FSCTL MARK HANDLE to lock the temporary files so that the operating system e.g. Windows does not move any part of the files for the remainder of their lives. Once the temporary files are locked the storage host uses yet another NTFS API e.g. FSCTL GET RETRIEVAL POINTERS to determine the NTFS sector ranges of the blocks that contain the temporary files . The storage host uses a mapping data structure from the storage controller to convert the NTFS sector ranges for the temporary files into block ranges that correspond to the LUN s block address space. The storage host sends a series of SCSI commands to instruct the storage controller to deallocate each of the blocks that contain the temporary files specified by the converted block ranges . After the blocks are deallocated the blocks are available for any other client to use. Finally the storage host deletes all of the temporary files from the CSV.

The space reclamation process disclosed above does not use Windows disk defragmentation API or relocation API which is not available for CSV volumes. The process can perform on a CSV volume while the CSV volume is in normal I O mode. CSV is supported for use with Microsoft s Hyper V virtualization hypervisor. Hyper V virtual machines keep their data in fixed size VHD files stored on the CSV. Other than occasions of virtual machines being created or destroyed the amount of space consumed within a CSV is generally constant. Consequently during the disclosed space reclamation process filling the CSV for a few seconds or minutes with temporary files is unlikely to cause any issues for the virtual machines.

From the storage controller s perspective as the blocks of each temporary file are unmapped immediately after it is created there is little change in storage space consumption in a thinly provisioned LUN. The disclosed space reclamation process does not waste any time checking to see whether blocks are already unmapped or zeroing blocks before unmapping them.

Capabilities such as thin provisioning can greatly improve storage efficiency. The disclosed space reclamation techniques ensure the benefits of thin provisioning will not erode over time. By executing the disclosed space reclamation process on a regular basis e.g. on an automatic schedule a thinly provisioned volume may be maintained at high storage efficiency such that the amount of storage space that the storage controller sees as consumed in the volume is no more than what is actually needed by the volume s host file system.

In addition to the above mentioned examples various other modifications and alterations of the invention may be made without departing from the invention. Accordingly the above disclosure is not to be considered as limiting and the appended claims are to be interpreted as encompassing the true spirit and the entire scope of the invention.

